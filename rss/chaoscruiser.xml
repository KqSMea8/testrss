<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>从Q-learning的小游戏看阿尔法元技术</title>
<link>http://www.jintiankansha.me/t/Pzd7krtfkL</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/Pzd7krtfkL</guid>
<description>&lt;p&gt;机器学习的三个框架监督学习 ， 无监督学习， 强化学习， 唯强化学习最难理解 。 如果说监督学习是感知和预测（Perception &amp;amp; Prediction)， 强化学习就是决策，它赋予机器以动机和计划的能力，同时你也可以它身上学习你改怎么决策！&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGytCcNt1I0LPKhPnqPGJjesvSmhFofwgHwz91R8sBVHBcPG8dMOT5yw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;853&quot; /&gt;关键时刻，知道前面有老虎还是会被吃，你得会跑！ 后者就是强化学习
&lt;p&gt;阿尔法元用到的框架是深度强化学习，在那里深度学习其实只是起到提取特征的作用，而背后的核心框架正是强化学习的一套基本功。&lt;/p&gt;

&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.8583333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGEkv9kPiaOXFyTRAXibln4Q066UNc5D8wOvyReEicdM7hfpiaiabZgRXy5ng/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;802&quot; /&gt;任何强化学习都包含几个基本部分，状态， 行为， 和奖励
&lt;p&gt;铁哥在这里通过一个简单的例子给大家讲一下这个通向强人工智能最有可能的路径。&lt;/p&gt;
&lt;p&gt;首先， 强化学习比监督学习难很多， 因为监督学习， 随时有一个老师在那里告诉你怎么是对，怎么是错，错了多少。 而强化学习， 只有游戏结束曲终人散的时候， 得到一个奖励， 或者惩罚， 然而， 你怎么都回想不清楚你究竟哪一步对， 哪一步错误， 可谓是急死老师傅。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGibwlNbS6H2cCs9HQNKn9O9E1vicptvQLb6aXKGWApvVfk4swpuEOceOw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;2000&quot; /&gt;没有老师了这回！
&lt;p&gt;而且， 你更加直接的面对不确定性， 这种残酷的感觉，就是你需要预测的是n多步以后的时候， 那不确定性累加起来， 可以哭倒长城了。&lt;/p&gt;
&lt;p&gt;不过， 这才是真实的人生， 走出的步子永远不知道上帝给你什么 ，而且， 不是不报，时候未到， 虽然你不知道什么时候到，只能哀叹一声， 苍天绕过谁啊。&lt;/p&gt;
&lt;p&gt;这个问题， 术语叫做sparse， 也就是说可以用于当下决策的有关最终奖励的信号太弱太弱了。 如何解决这个问题？ 有个词，叫“趋势” ， 你需要从历史的经验里， 得到一个对未来趋势的判断，然后赋予此刻每个可能的行动一个值， 因为总有某个行动， 比其它的更容易导致良好的趋势， 哪怕优势只是一点点， 我们这样一步步走出， 总会得到一个帕累托最优吧 。 然后呢 ，假设你有无穷的生命， 你通过每一轮游戏的经历得到的信息不停迭代这个趋势。&lt;/p&gt;
&lt;p&gt;这就是强化学习的最简单的方法 所谓Q 学习（Q- learning）方法。 它自然的包含了两个部分， 一个是通过经历不停的强化对某个状态行为下趋势的认知 ，另一个则是根据这个趋势，每步走出当下最优的选择（这不就是理性人决策吗！）。&lt;/p&gt;
&lt;p&gt;我们再学究一点， 探究一下这个趋势的数学含义， 其实数学家的角度， 所谓的Q,  就是在当下选择之下未来收益的期望， 瞧，一句话把趋势是什么， 问题里的不确定性都囊括了。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.17222222222222222&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGGqwI7nk462eOhlIsc1mvRMRfevfqtVkOqNRxq7NJjBRwrasm33dic2Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1041&quot; /&gt;&lt;p&gt;我们再来从人类认知角度看这个问题， 人脑学习如何决策的原理其实是类似的， 你决策的依据， 也是未来的趋势。&lt;/p&gt;
&lt;p&gt;一向自由洒脱的小明问了， 我哪有那么理性？ 我平时的生活从没有思考过什么趋势，更不要说什么未来收益的期望， 我就是根据我的性情决策的。&lt;/p&gt;
&lt;p&gt;只是小明没有意识到。 虽然人都生活在情绪而不是理性之中，但情绪亦是一种一种朴拙的生存理性，正是某种藏在你基因（祖先的经历）或者过往经历中的Q - value，使你你冥冥中觉得大势不妙而产生了恐惧，反之则是欣喜。  这里印证了当下的某种经历与你祖先的某一次窘境的一致性，比如你祖宗在水边看到的一条蛇， 引起了你对水杯中蛇影的恐惧。&lt;/p&gt;
&lt;p&gt;你看到美女的兴奋是因为它预示了一种未来子孙满堂的丰盛（基因所致）， 这来自于你赶地铁的焦虑是它预示了一系列的停薪考试挂科的衰微（经历所致）。 情绪，使你在一秒钟内能够决策。 那些缺少情绪而只有理性的人， 通常无法在日常生活中正常决策。 未来虽然是不确定的， 但是你的喜怒哀乐还是让你做出大致适合生存的选择。 甚至更神奇的，你有时候不必经历后果， 就可以通过你的情绪反应学习，比如某次开车太快差点被撞，这种恐惧和后怕可以让你学到开慢点，而不是等撞死了再说（TD learning）。&lt;/p&gt;
&lt;p&gt;这个Q - learning  方法 ， 就是赋予机器以“情绪” 智能， 来克服这个缺乏“ 监督信号”问题的。&lt;/p&gt;
&lt;p&gt;你要给一个行为所能导致的所有未来趋势赋值，你就要面对一个问题， 即使此刻你做了一个决定， 你的行动导致的下一个状态你又需要决策，每个决策又导致不同的结果， 这样始终都是处于不确定的状态。&lt;/p&gt;
&lt;p&gt;为了简化这个问题， Q - learning做了一个大刀阔斧的假设， 就是我现在做个决策， 后面的决定也都是按照某个最优的法则走的（选择当时的可选行动力最优的）， 这样我忽略那些基于此刻行动导致的下个状态下那些不太好的选择 ，而是只考虑那时候最优选择的回报，这也就给出了一个很自洽的解。 也就是说， 此处行动导致的趋势， 就是未来可能回到的最大值，瞧， 一句话就把这个不确定难题搞定了， 我们基于此把趋势这个模糊的东西量化。&lt;/p&gt;
&lt;p&gt;另外一个难点是什么呢？  下一步就得到的回报没有人会把它和未来十年的回报等同视之， 也就是说，你要以某种方式对不同时刻到来的奖励加权， 这有点像银行的贴现，未来那虚无缥缈的金银宝藏，还是好好乘个贴现因子，说不定乘出来还不如此刻的一个蛋糕。   这个原理被藏在一个叫bellaman equation的方程里。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.2833333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGogAod8mBhGKy18KMutjiaKT4MbAw6pmMgSNIux7764LDkz16p57xqicQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;926&quot; /&gt;一个用迭代方法写出的数学表达， 不能再优美。
&lt;p&gt;这两点放在一块，我们就可以完美的定义Q value，它就是在理性人（没步下做最优选择）假设下每一步的收益按贴现率加和来的未来收益总期望。这么绕口，其实我还没说下半句， 那就是在此刻某个行为下的期望，太累了，分开讲。 这整个定义，就是充满智慧的Q - value。&lt;/p&gt;
&lt;p&gt;然后Q - learning呢？ 刚说过了， 学习，是通过既往的经验，  Q - learning的过程就是根据行为做出后得到的真实境遇（可以是奖励或是惩罚，也可以是新的位置的Q值）来更新这个期望的过程， that‘s  all。  当Q 值被更新， 你的行为也就被更新了 ，因为我每一次的行为无非选择最大的Q值。&lt;/p&gt;
&lt;p&gt;这里， 我们做一个更简单的例子， 把强化学习变成一个简单走迷宫的游戏，  在最经典的Q-learning下看看效果。 不要小看它简单， 小小的迷宫可以玩出深度学习的众多花样。&lt;/p&gt;
&lt;p&gt;游戏的框架被称为“冰湖”  ： 北方之王（king of north）统治的国度里有一个巨大的冰湖，冰湖里隐藏着一些深不可测的冰洞， 冰湖的某个角落藏着关系帝国龙脉的宝藏， 一群流亡士兵要在黑夜里穿越冰湖找到保藏， 而且不要掉到洞里。 而且冰湖上时刻管着凌琳的风， 你跑路的风向以某个概率被风吹偏。。士兵要决策自己跑路的方向，最终更快更安全的拿到宝藏。&lt;/p&gt;
&lt;p&gt;这个游戏的版本一是你手里有一个GPS， 士兵知道坐标，但没有记忆， 士兵手里有一个巨大的表格， 记下每个位置下不同方向的。Q值，士兵们一个接个去寻找， 通过Q叠代， 让后面的士兵更快更安全的走到地方。&lt;/p&gt;
&lt;p&gt;此处我们得到一个典型的马尔科夫决策过程，每一步的状态（位置）都可以决定你的全部未来， 你唯一需要迭代的是那个往不同方向迈一步可能得到的后果。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.2569444444444444&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGQJbJWLOkfws4H5OvicnpF55dYIHkpdPlgicYLzib0y9BibfteY7icwJ5T1A/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1118&quot; /&gt;经过几轮迭代以后，我们的这个小小冰湖世界的所有位置都被填进了这个 神秘的Q-value
&lt;p&gt;那么我说好，这个黑色的冰湖， 绝地大多说地方一个信号都没有， 你都收不到远方的奖励感知信号， 哪来的学习呢？ 那么好了 ， 我们这个Q - leanring的表格， 就是关键。 因为你第一步迭代的一定是离你和目标最近的那个点，这个点率先得到更高的Q值，当这个点被更新了，下一次走到它附近的agent就很可能会迭代它周边的这个Q值， 如此以往，Q值会被逐步的迭代出来，如同一个从不幸到幸福的梯度场， agent无论何时开始出发，都可以很快的循着这个梯度场（找到相邻位置里最幸福的那个位置）达到目标， 并且越来越聪明的躲开冰洞，无论它离目标和危险有多远， 无论这些士兵本身对冰湖并无概念，也对地理毫无了解， 他们的表现却让你觉得他们对远方的局势了如执掌。&lt;/p&gt;
&lt;p&gt;这一点对于这样一个状态数量非常有限， 当下的行为仅需要考虑当下状态 ，而且奖励非常确定的时候能是可以求解的。 上述这个框架又被称为马尔科夫决策框架，“马尔科夫”无论出现在哪里，都是描述当下状态可以完全决定下一刻状态的某种时间“离散”的游戏（典型的如围棋，象棋）&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.7991266375545851&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHG9dicEQGaLuCP7jiavia4o4icyu6zicu9jgMOsFpWD82bZRiaxNAM7ibib90sUw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;687&quot; width=&quot;687&quot; /&gt;
&lt;p&gt;一旦上面说的任何一条被打破，游戏就没有那么简单了。 例如：&lt;/p&gt;

&lt;p&gt;1，  当我的冰湖变得非常巨大， 可能达到的状态达到无穷多， 这时候你要计算一个通往“幸福”的梯度场变得没那么容易。 所以， 我用我深度学习的惯有招数， 我用一个带有先验信息的神经网络来假设这个梯度场的结构再学习！  此处可以有deep Q learning。&lt;/p&gt;
&lt;p&gt;2，  既然状态多不好搞，能不能更直接一点， 不学习Q值， 而是直接学习行为？ 于是我们有了policy learning。比如著名的policy-gradient  ，  actor-critic这些deepmind的家常菜。&lt;/p&gt;
&lt;p&gt;3，  士兵手里不再具有GPS定位信息， 而是只知道相邻位置的状态， 这时候他只能够像盲人摸象一样四处摸。  这时候你有一个非全局的马尔科夫决策过程。 因为你每一步的信息已经不足以让你做出智慧的决定。  那怎么办？  当下信息不足，我就用历史信息来补充， 来个循环神经网络给我， 再加一个能够自己生成定位的神经网络！  此处请看deepmind最新文章Vector-based navigation using grid-like representations in artificial agents。&lt;/p&gt;
&lt;p&gt;由此我们从一个简单的自己对世界环境毫无知觉的agent，一步步迈向了自己心中具有世界模型， 可以通过自己的学习生成世界模型的agent， 认知诞生了， 知识诞生了，而一切都提高了我们强化学习创造的虚拟生命的学习效率和生存可能。&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5166666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGQjXYib1oQaoUze8Ke5OuOicwf0OAaCWKBZtIGPRsR6yia31WGoC3kD2lg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1485&quot; /&gt;&lt;p&gt;有了这些更新的方法， 我们就从最简单的游戏迈向了围棋， 迈向了星际争霸这些你真正爱玩的游戏， 最终迈向真实的生活。&lt;/p&gt;

更多阅读&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382963&amp;amp;idx=1&amp;amp;sn=53c1f03208ca7a41285566b9bf8aa83d&amp;amp;chksm=84f3caf2b38443e40428d245046814ac4ca8883c4403a88f68980b86e57b4c754759edf6eece&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;铁哥谈AI： 浅析阿尔法元之元&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381615&amp;amp;idx=1&amp;amp;sn=04c5cc6cda0f7bd7032c1beb7aeea64d&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;阿尔法狗是怎么用机器学习做决策的&lt;/a&gt;&lt;br /&gt;&lt;span&gt;作者铁哥的微信号：ironcruiser。再今年7月，铁哥会推出深度强化学习的在线课程，敬请期待。&lt;/span&gt;&lt;p&gt;下面是广告时间，总时长超过16个小时，全面的涵盖常见深度学习模型架构，优化方法。本文提到的正则化技术都有详细的逐行代码详解。超值的良心价。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;9.821333333333333&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGGB3P7tjOcxTibLZ3J6CzkuzaMRoB3nUxdiavuQlnR6Rl6YDMUIU8t33g/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;750&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

</description>
<pubDate>Sun, 13 May 2018 18:42:21 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/Pzd7krtfkL</dc:identifier>
</item>
<item>
<title>放养的深度学习-浅谈自编码器</title>
<link>http://www.jintiankansha.me/t/npYi6vxlsI</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/npYi6vxlsI</guid>
<description>&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5559322033898305&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGeRlt2tPet7t9EFQs0zLjkwUOmkyuKL0ib9CTekpRN36BKrUbLcicHGicw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;590&quot; /&gt;&lt;/p&gt;
&lt;p&gt;说起非监督学习，总逃不过Yann LeCnn的这张图，他将强化学习比做蛋糕上的樱桃，将监督学习比做蛋糕的奶油，而将非监督学习比做蛋糕底座。这个比喻不止说明了这三种学习范式的江湖地位。更形象的指出了各自的特点，强化学习得到最多的关注，监督学习好吃但是相对价格更贵（需要人工进行数据标记），而非监督学习虽然潜力巨大，但却相对来说最默默无闻。&lt;/p&gt;

&lt;p&gt;除了不需要人工数据标记这个优点，非监督学习的另一个优点是不会引入人的偏见。举一个具体的例子，如果要开发一个能够自动对手机里照片按照内容分类的工具，那么监督学习的做法就是将图像进行分类，看看图片中的主体是人物，动植物，建筑，自然风景或是文字，然后按照每张照片的分类将照片放到不同的相册中。而非监督的方式要做的是将每一张图片放到一个二维空间中，不是随便的放，而是要让内容相近的图片都凑到一起，最终出现几个图片簇，而在每个簇之间的距离要尽可能的大。&lt;/p&gt;

&lt;p&gt;如何判断图片的语义是否相近了，可以假设同一个时间段拍摄的，在同一个地点拍摄的照片具有相似的内容。一旦训练好了这样一个将图片自动聚类（降维）的模型，那么你朋友第二天发给你的昨天聚会照片，即使系统显示的时间不在同一天，也会被归档在正确的相册中。&lt;/p&gt;

&lt;p&gt;在深度学习出现之前，聚类已经有了很多种成熟的方法了，从最简单的K means到不需要设定聚类数的affinity propagation，再到层次化的聚类。然而，这些聚类方法对于非结构化的数据，例如图片，声音处理的不好，如果能够将非结构化数据的维度降低，那就可以使用传统的聚类方法了。然而，线性的聚类方法，例如PCA或MDS，在图像上的表现不佳。这里展示的是使用PCA对MINST数据进行降维的结果。不同的数字并没有被明显的区分开。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceCicV64SRTQnx3BRq4k511ZSJETWx8QNbWf8wGiae19HVib8FHxHNg3HiczzmquQ7zbmj5jjUg6HoQZg/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;0.8078125&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而使用非监督学习在深度学习中最典型的架构-自编码器，就可以做到对图像等非结构数据进行降维。下面展示的是用深度学习对手写数字进行的降维，不同的数字区分要比上一副图好的多。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceCicV64SRTQnx3BRq4k511ZMfRMrSCIUnicFBDJalm7f8VmE2lzQ7gzySGuwGib2ChZjvI684DmiaiaBA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.9076376554174067&quot; data-w=&quot;563&quot; /&gt;&lt;/p&gt;

&lt;p&gt;任何深度学习的架构的构建，都是将想要达到的目标翻译成损失函数的过程，数据降维的目标是让降维后的数据能够更好的保持原有数据的区分度，让原来能分开的数据现在也能分开。但原始的没有标签的数据点之间距离都没有明确的定义，又如何用一个公式来量化降维后数据点之间的区分度。&lt;/p&gt;

&lt;p&gt;如果在横向上无法解决问题，那可以试试在纵向上进行探索。先假设问题已经解决了，我们找到了一种完美的将MINST图像进行降维的方案，那么我们能拿这个方案做什么？假设每个MINST数据集中的图像有一个唯一的6位ID，越靠前的位置表示对该图片的越大类的分类。那么我们完美的降维方案给出的结果将可以100%的预测出图片的ID。接着假设我们还没有找到这个完美的降维方案，但已经差的不多了，那么我们根据降维后预测出的图像ID应该只在最后几位有差距。到了这一步，待优化的损失函数已经呼之欲出了，即降维后的特征生成的图像和原始的图像差距有多大。接下来要做的就是先训练一个神经网络来降维，再训练一个网络进行升维。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.952755905511811&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGbP13WPHIY6l6ibxRAWng4EMBRGQZ5PI6STxTNlL6CjJdTF36jTuNVxg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;254&quot; /&gt;&lt;/p&gt;
&lt;p&gt;从最基本的自编码器出发，将损失函数进行改变，让输入加入误差，而待重构的是不带误差的原始数据，就可以得到变分自编码器（VAE）。而在网络结构上，也可以使用卷积网络或循环神经网络。然而不同于深度网络，自编码器的结构使得网络可以逐层训练。例如一个堆叠的自编码器，先将原始的100维数据降低为50维，再降低为10维，最后降低为3维。那么就可以先训练一个浅层的100-&amp;gt;50-&amp;gt;100的自编码器，训练好后将网络固定，再使用上次训练得出的降维数据训练50-&amp;gt;10&amp;gt;50的自编码器，依次类推，最后将训练好的编码器和解码器按顺序堆叠起来。这样的训练方式可以避免梯度消失的问题。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8387096774193549&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGKcTX2C9MmhZL4F8sL3I8NvlPia8JFdS7KCOcvbSZBSWOZ5Hbzn7xChg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;279&quot; /&gt;&lt;/p&gt;
&lt;p&gt;不同深度的网络学习率随着深度增加显著降低&lt;/p&gt;

&lt;p&gt;自编码器进行数据降维的目地，是为了进行方便聚类。使用Kmeans等算法，可以对原来的Minst数据进行聚类。在有数据标签的时候，使用Normalized Mutual Information (NMI)，可以来评价聚类的效果，这个数字越大，表示被正确的聚在一起的数字越多。如此，就可以评价使用原始的数据聚类和使用自编码器降维后的数据聚类的效果。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7507836990595611&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHG0BTBHIbcOeWoEN4gXMSvebAcOChVJVcicuasOlmImKia8ibKZicykh4pRQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;638&quot; /&gt;&lt;/p&gt;
&lt;p&gt;以下是Keras相关代码&lt;/p&gt;
&lt;pre readability=&quot;4&quot;&gt;
&lt;span class=&quot;c1&quot;&gt;# this is our input placeholder&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;784&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,))&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# &quot;encoded&quot; is the encoded representation of the input&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'sigmoid'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# &quot;decoded&quot; is the lossy reconstruction of the input&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;784&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# this model maps an input to its reconstruction&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autoencoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt;&lt;span class=&quot;p&quot; readability=&quot;2&quot;&gt;)&lt;p&gt;#  this model maps an input to its encoded representation&lt;/p&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;span class=&quot;n&quot;&gt;autoencoder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'adam'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'mse'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;span class=&quot;n&quot;&gt;pred_auto_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_auto&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;span class=&quot;n&quot;&gt;km&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_auto_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;km&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_auto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;br /&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;span class=&quot;n&quot;&gt;normalized_mutual_info_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;可以看到，使用自编码器+Kmeans 的方法，其MNI从0.50提升到了0.74，效果很明显。类似的，使用自编码器降维后的特征，也可以结合传统的机器学习算法，例如KNN，随机森林等进行分类任务，你可以训练多个层数不同自编码器，有的将原始数据（100维）降低为10维，有的降低为2维，再分别使用降维后的数据进行分类，这可以理解成集成弱分类器，也可以从神经网络的角度理解成残差网络，即在深度的网络中加入直接连接的短路层。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.546242774566474&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGcL3oPA5crOuGwNgd1S9eia4FsRyyMNlgMqdh9armExJB5oaM879iakuw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;346&quot; /&gt;&lt;/p&gt;
&lt;p&gt;残差网络的结构&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;p&gt;自编码器在监督学习中还有另一种应用。还是拿Minst数据分类举例，已知标签时，可以对每一个数字训练一个自编码器，然后一个待分类的数字来了，分别用在训练集中训练好的自编码器进行重构，根据那个自编码器的重构误差大小，使用softmax函数，将不同数字对应的自编码器的重构误差转化为该图片为那个数字的概率分布。&lt;/p&gt;

&lt;p&gt;这样的分类方式下，假设你拿一张写着字母A的图片，那么这十个用手写数字训练出来的自编码器，就会出现没有一个重构误差足够小的情况，这使得你的模型能够发现异常点，而传统的分类模型，你的图片总会被归到一类中。再假设现在你在Minst数据集之外，又拿到了一万张手写的8的照片，这时你不需要重新训练模型，只需要重新训练数字8对应的自编码器。&lt;/p&gt;

&lt;p&gt;假设你在训练中发现数字8和数字6总是容易区分不开，那你可以根据一个新的图片在数字6和8中对应的自编码器中的重构误差进行人为的调整，比对在经过softmax时放大其中的差异。这使的深度学习不在是一个黑盒，使得研究者可以看到预测过程中发生了什么。&lt;/p&gt;

&lt;p&gt;在模型训练的过程中，也可以针对原始图像和生成图像的距离进行类似Relu函数的截断。通过更改权重，让模型在重构误差小于一定cutoff的情况就不进行优化，而只关注那些重构误差还相对较大的样本。这里的思路就类似Xgboost中将预测结果不好的样本再放回重新训练的思路了。&lt;/p&gt;

&lt;p&gt;假设现在数字8的样本数已经是其他数字的100倍了，上述的使用自编码器的分类模型不会像传统的分类模型那样，受到样本数不均的负面影响。这对于医疗，金融这种正负样本量差距很大的应用场景，极其有用。同时，这样的方法进行的预测，更加稳健，不用担心图像修改一个像素，模型的预测结果就会改变。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;将自编码器用于图像生成，可以避免GAN中的模式塌缩问题（只生成黄色的猫）。分别用猫和狗的图像训练自编码器，将原始图像的降维表示进行微调，就可以用来生成新的猫和狗的照片。由于生成的图片都在原始图片降维后的空间附近， 自编码器生成的照片，不会出现GAN那样五条腿的猫这种明显不符合常识的图片。&lt;/p&gt;

&lt;p&gt;使用自编码器降维，还会有更好玩的应用。还是MINST数据集，将所有6的图片，分别转90，180，270度，这样一张图片就变成了4张，通过自编码器降维再进行Kmeans 聚类（K取4），是可以将图像按照转动的角度分成4类的。现在拿一张正常的9的图片交给自编码器去降维，这个图片会被分到旋转180度的那一簇6中，这说明模型可以学到图片的语义信息，这种在不同事物间进行关联的能力，是人类推理的基石。&lt;/p&gt;

&lt;p&gt;更哲学一些的论述，非监督学习做的是根据现有的数据去预测将来。MInst数据集的例子可以看成是将一个人写数字的过程拍了下来，假设一个人要花3秒写数字，自编码器做的可以看成根据第三秒后的笔迹预测前一秒之前的笔迹，最终能够达到在一个数字还没有写完的时候就预测出这个数字本身是什么。（空间的降维再升维对应时间上的历史匹配加预知）&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;总结一下，这篇小文介绍了自编码器的原理，训练方法及在分类和聚类和图像生成中多种的应用场景及优势。自编码器的变种很多，发展很快，代码相比于CNN，RNN来，实现起来也不难，是一个值得细致学习的探索深度学习框架，这篇小文要做的只是抛砖引玉。&lt;/p&gt;

&lt;p&gt;更多越多&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383201&amp;amp;idx=1&amp;amp;sn=d401d14056886ca4ffc747821ee3d4c1&amp;amp;chksm=84f3cbe0b38442f69e9bbb440e30aa6b3386601f7c597ded9200f73a46b0f24ddb775a684642&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;R 语言中的深度学习 Minst数据集下的聚类分析&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;原创不易，随喜赞赏&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9397590361445783&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd4icdSNQnx98Zicw7nnP3icPycqI1uRMvRxCezhYm4xQKdbiamvHVmC19a0o7YS03eqTrIL9QJS4wS4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;249&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;9.821333333333333&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGGB3P7tjOcxTibLZ3J6CzkuzaMRoB3nUxdiavuQlnR6Rl6YDMUIU8t33g/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;750&quot; /&gt;&lt;/p&gt;





</description>
<pubDate>Sat, 12 May 2018 11:32:55 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/npYi6vxlsI</dc:identifier>
</item>
<item>
<title>WTF阅读笔记-一个老实人对未来的理性乐观</title>
<link>http://www.jintiankansha.me/t/6nusn8iTBd</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/6nusn8iTBd</guid>
<description>&lt;p&gt;今天要说的是一本17年的书，书名很特别，WTF，副标题是未来会怎么样以及为什么未来取决于我们。WTF是英语世界中what the f××k的缩写，正好也是what‘s the future的首字母。这本书的作者是一位成功的企业家Tim O’Reilly，他创立的最出名的就是O‘Reilly出版公司了，在计算机领域，O’Reilly出版的技术类书籍是权威与易懂的保证。在WTF这本书中，作者描绘了两种以WTF开头的未来，一种是惊叹，另一种是诅咒，分别指出这样的未来会怎样到来，最后论证了未来是如何由当下的选择决定的。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.5085714285714287&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccm10icagn350KIrJfupicwdWqsBuOhkjRZbNEiczKXncmec1hicYfiaiaicVe0lEg7va7tnPZBG37CJP2PQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;525&quot; /&gt;&lt;/p&gt;

&lt;p&gt;先说好的一面，在WTF的第二部分Platform thinking（平台思维）中，作者讲述了将例如Urber等互联网公司的思维模式推广到社会中那些不那么有效率的部分的尝试。由于本书的作者的生活经历，书中所举的例子也都来自美国，但在这个全球化的年代，其中的道理是通用的。比如本书中提到的Gov 2.0计划，是一个帮助通过开发app美国地方政府提高办事效率的公益组织。&lt;/p&gt;

&lt;p&gt;而在国内，政务的电子化则是由互联网巨头推动的，不论是在微信上预约港澳通行证的办理，还是通过提供照片在线举报交通违法行为，一线城市的地方政务正在变得在线化。在WTF这本书中，作者列出了他从Gov 2.0中吸取的教训，第一是围绕着用户的需求，而不是政府部门之间相互制衡的需求来设计产品，第二是做最重要而不是全部的事（do less），第三个教训是根据收集到的数据，不断调整设计，第四是设立规则和设计产品一样重要。&lt;/p&gt;

&lt;p&gt;关于官僚主义，有句话是这样说的，唯一把我们从官僚主义的低效中拯救的就是它的低效。但当政府开始向互联网公司一样开始将自己当成一个平台，那么游戏规则的变化会带来激励的改变，之前官僚系统的低效是源自于缺少直接的反馈机制，官僚对上级而不是对他们服务的对象负责，而上级要了解真实的民情，接受信息的带宽太窄。&lt;/p&gt;

&lt;p&gt;而将政务电子化，不止是减少了其中的处理流程，提升了效率，更重要的是让数据的积累变得多维度高带宽。举学术界的例子，发表文章是每一个学者都要去做的，在封闭的出版环境，不止审稿的过程慢，还不可避免的带有个人偏见，而且最终评价文章好坏也只是依据第三方给出的影响因子，而将展示论文看成是一种服务，则会带来arxiv那样的论文预印平台，从而使得每一个参与者都能从中获益。&lt;/p&gt;

&lt;p&gt;在面对未来时，我们在意的是连接多数人平台还是富裕少数人的生意，决定了我们将要面对哪一种WTF的未来。说完了光明的一面，让我们看看作者是如何讨论悲观者眼中的未来的。这本书第三部分的标题是一个被算法统治的世界（A world ruled by algorithm），分别从企业管理，政府规章，媒体和金融业四个方面，指出算法正在如何深刻却又无声的影响着我们的生活。其中涉及到很多已经发生的未来，例如社交媒体中的filter bubble，隐私泄露，以及高频算法交易带来的监管落伍。这里就不再一一复述了。&lt;/p&gt;

&lt;p&gt;而WTF这本书厉害的是指出了这一切背后的根源在于当下的人们拿错了地图。企业的管理者的目地不应该是不断提升的股价，创业者的目地也不是拿到下一轮的投资。当金钱收益成为了唯一的评价指标，急功近利就会取代深耕细作，由公司资助的科研文章在近二十年来一直在减少，而由公司资助的专利申请则在增加。企业的管理者在当下的激励环境下会通过股票回购来维持股价的涨幅，而不愿意将公司的利润拿出来投资在人力资本和产品研发上。本书的作者感叹美国制造业的空心化，哀叹的不是工厂搬走了，而是技术工人不再成为一个有吸引力的岗位了。随着中国的制造业升级，越来越多的工厂也会搬走，这不意味着制造业的空心化，只要人们普遍尊重制造业的从业者，年轻人愿意在制造业上投入时间，这就不是问题。&lt;/p&gt;

&lt;p&gt;接下来说说这本书副标题中另一面，为什么未来是取决于我们的。这里先讲一个2000年的小故事，那时摇篮中的亚马逊有了一项影响深远的发明，即一键购买，不要小瞧这个技术上很容易实现的发明，正是这个不起眼的发明，革命性的改变硅谷从业者的认知地图，之后的facebook的点赞，urber的一键叫车等设计都受到了一键购买的启发。这个故事告诉人们，人们认识世界的时候，是通过前人构建的认知地图来展开的，一键购买的按钮在互联网人的认知地图中种下了操作便捷的种子，这启发了之后的种种发明，而一键购买还意味着不经过思考就冲动消费，这也影响了之后的互联网技术的发展。&lt;/p&gt;

&lt;p&gt;当下的细微选择影响着我们认知未来的地图，这是我们改变未来的第一种方法。而技术的进步使得做曾经昂贵的成本会变得接近免费，这使得企业家能够思考未来应该是怎样的问题，而不是去复制或者细调之前的商业模式。例如Urber的思考方式就不是说如何利用移动互联网让出租车更加高效，那样做出来的多半是给每一个司机做一个个人网页界面，做一个动态地图给用户展示公司属下的出租车现在在那里。而Urber直接从用户的角度去思考问题，而这样的跃迁式的新模式，将是我们改变未来的第二种方式。每一个子领域的跃迁将会带给每个人认知地图的范式迁移，从而使得影响不局限于一个领域。&lt;/p&gt;

&lt;p&gt;Tim O‘Reilly的志向是通过传播最新的技术资讯来影响他人，从而改变世界。为此，他不在乎有多少个亿万富翁是从他家的书中得到了灵感，也不愿意为了业务的扩张，引入外界的投资，从而影响到公司的独立决策。类似于钢铁侠马斯克，他们所做的都是在看准的领域点燃一把火，通过展示一个可以靠自己收入维持下去的企业是怎样运作的，从而吸引更多人来投入资金或注意力。在这本书的第四部分，这本书的作者用他自己的故事现身说法，对想改变未来的年轻人提供了几条建议。虽然这些建议听起来有些鸡汤，但这是我觉得本书最具有启发性的部分。作者拿自身经过的事情有一说一，没有一丝自夸，全是老实人的大白话，如果一定要喝鸡汤，这是最不油腻的一种了。&lt;/p&gt;

&lt;p&gt;第一条建议，不要将金钱看得太重，在你真正在意的领域工作。钱如同汽车油箱里的汽油，没油了汽车会抛锚，但人生不是从一个加油站到另一个加油站。你要关注的是那个你即使失败了也能让世界变得更好的目标，是那个当你胜利后让你变得渺小的目标，因为你的成功会带来更多更聪明的追随者，会让你面对更激励的竞争而不是更容易赚的钱。&lt;/p&gt;

&lt;p&gt;第二条建议是从你身边的环境中拿走的价值要少于你创造的价值。如今很多模仿式的创新，关注的只是如何从已有的平台中攥取价值，金融领域，互联网媒体中的诸多机制设计，也不关注如何为其所寄生的环境赋能。对于创业者来说，做真正在意的事是正确的起点，而选择给予多于还是少于付出决定了业务模式能不能持久。&lt;/p&gt;

&lt;p&gt;第三个建议是眼光放长远，通过对不同情境的未来提前准备来应对变化，同时关注如何让自己变得更强大。比如面对贸易战，要争辩的不是贸易战会不会来，而是讨论即使贸易战来了，而且我们吃亏了，如何能够将损失降低，如何能够不伤到元气。（原书中举的是气候变暖的例子）情境化的思考问题会关注那些眼前看起来很遥远的未来，准备最坏的情况下的应急预案，这样即使最坏的情况发生了，我们也不至于手足无措，而若是最坏的情景没有发生，我们也获得了技术的提升。就如同你买了重疾险，然后发现保险公司送了健身房的年卡。通过将眼光放远，早做准备，你变成了更好的自己。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6976744186046512&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccm10icagn350KIrJfupicwdWz3dx9cDoqEsh14P2A9iaH8lWHQIx6V3ibk3kicfozibAaicDUymKtAF8cqQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;645&quot; /&gt;&lt;/p&gt;


&lt;p&gt;在上面的图中，我们要关注的是左上方，右上方是好莱坞电影中的套路，一个新技术拯救了世界，但实际上多半是小聪明暂时的将问题隐藏了。左下方是因循守旧的官僚系统解决问题的模式，而右下方则是为什么人们会对未来感到悲观的理由。而左上方则是创业者要深耕的领域，在一个问题（需求）不存在或微小到巨头们看不上的细分领域内，以全新的方式快速的建立起新的模式。正是有一代代这样不断创新者，才使得我们对未来有理由保持乐观。&lt;/p&gt;

&lt;p&gt;再举一个大家都很关注的例子，人工智能会不会大规模的造成人类失业。作者也给出了类似的一张图，最好的情境是右上角，人们改变的认知地图，关注的不是资本所有者的利益的最大化，而是每个普通人的福祉。而且科技使人类能够以新的方式开展之前无法想象的工作，这样的未来就如同前工业时代的人来到了当代，会惊喜的留下眼泪。而右下方的场景，则如同石器时代的人来到了奴隶社会，他们会感到生产力的飞跃性提高，但是没有了自由，他们会对这样的未来感到失望。而在左上方，则是另一种次优的未来，缺少志向高远的企业家，AI只是替代了现有的人类劳动，而留给人类的是无尽的闲暇去从事艺术创造及情感交流。而左下方的未来，则是充满了无谓的争端，人类会毁掉机器，一次次的阻止一个注定要到来的未来，不管是否成功，都无法带给其中的人持久的幸福感。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.684931506849315&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccm10icagn350KIrJfupicwdWiaD7cqC88pd7ozEicPLg7ribZ8fRfX5f90bKHiccko2QLBzBckVcT6pv6Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;657&quot; /&gt;&lt;/p&gt;


&lt;p&gt;这样的分类，是我看到的关于AI加持下的未来这个话题最具洞见的讨论。而这幅地图的模板，则是之前出现的第一幅图。AI要解决的大问题不是如何复制人类的工作，而是如何带领人类更高效的工作，而人类要做出的反应则是该如何分配蛋糕。川普的上台使的被全球化忽略的人走到了前台，这是改变的第一步，本来历史将会走向右下方的，但现在看来，却像要走到左下方了。&lt;/p&gt;

&lt;p&gt;那该如何让未来变成右上方，至少是左上方那样了。取决于一少部分心智坚定的人关注的是金钱还是理想，思考问题时眼光有多久，愿不愿意付出多于收获。更取决于每个人每天做的是糊口的job还是为了一个愿景而从事的work。&lt;/p&gt;

&lt;p&gt;在WTF这本书的结尾，作者引用贝索斯在亚马逊年会上讲给全体员工的话，我们始终处在Day 1，这类似于乔帮主的Stay hunger，stay fooolish，接着有员工问Day 2 是怎样的，贝索斯的回答很精彩，他说Day 2是静止的漂浮在无关讯息之海，紧接着是漫长而痛苦的死去。（Day 2 is stasis. Followed by irrelevance. Followed by excruciating, painful decline. Followed by death）&lt;/p&gt;

&lt;p&gt;接着贝索斯给出了四条建议，来避免Day 2的到来。一是关注客户，持续尝试不惧失败，二是对既有的流程和捷径保持质疑（resisting proxies）而不是不在乎结果，只在乎符合规矩的做事；三是拥抱技术的变化，四是快速的进行决策。这四条建议，不管对于那个阶段的企业，都是适用的。&lt;/p&gt;

&lt;p&gt;在一本关于未来会怎样这样的大话题的书的结尾，最好是加上一些私人化的小故事。WTF这本书举的是Youtube上的免费在线教育是如何让作者的养女快速的学到做出星级饭店水平的甜点的。网络平台使得人们的生活变得丰富，使得人的爱好能够变得不止是爱好，使得更多人能够在更多领域能够零门槛的追求卓越，这是对未来乐观的理由。而我在这篇读书笔记的结尾，也要说说一件让我印象深刻的事，是在B站上看到的一部叫我的三体的MineCraft漫画，这部水平不亚于星级穿越这样商业大片的神作是由民间爱好者业余完成的，没有融资，全靠热爱，完成了《三体》这本书的电影改编，从第一季到第二季，其中展现的技术爆炸让人忍不住说一声WTF，正是牛X。正是这样机器和人类的共同创造，使得我对未来保有信心。&lt;/p&gt;


&lt;p&gt;原创不易，随喜赞赏,&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9397590361445783&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd4icdSNQnx98Zicw7nnP3icPycqI1uRMvRxCezhYm4xQKdbiamvHVmC19a0o7YS03eqTrIL9QJS4wS4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;249&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383440&amp;amp;idx=1&amp;amp;sn=3248aeb279deec49cdfdf89d1d67b738&amp;amp;chksm=84f3c8d1b38441c7125075dd204b1cda1c2bb4ca76021d40b0f787b34fc7fc61d525febb300c&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;社交网络视角下的大历史-读《The Square and the Tower》&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383140&amp;amp;idx=1&amp;amp;sn=b24d8d35a61587709c9a1f3ab11178d6&amp;amp;chksm=84f3cb25b3844233857cfa13b44e82f07a79cee89034cd95f29c581a07588934a1d85ecad16f&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;让互联网更有人情味-读《Technically wrong》&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;












</description>
<pubDate>Sat, 05 May 2018 11:10:07 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/6nusn8iTBd</dc:identifier>
</item>
</channel>
</rss>