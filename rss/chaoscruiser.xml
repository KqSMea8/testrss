<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>聊一聊机器学习在金融中的应用</title>
<link>http://www.jintiankansha.me/t/eSFV6NdBKc</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/eSFV6NdBKc</guid>
<description>&lt;p&gt;金融从业者少不了和数据打交道，因此天生希望能有工具帮助他们自动化数据处理的过程。这篇文章将从工程的角度，全面的梳理机器学习，尤其是深度学习应用在金融领域的机遇和挑战，通过展示在不同的应用场景下，当前的技术能够做些什么，帮助从业者系统化的了解AI+金融这个热点领域。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6978798586572438&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccUibfib97pQbTAPaTWWPp1lnfvEVMN8PghZOthmwRBmYadWG66ibt4PicgWSuurDTibOicagDN94tNN0Lw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;566&quot; /&gt;&lt;/p&gt;
&lt;p&gt;如何拆解AI+金融这个话题，从数据来源看，可以分为结构化与非结构化，后者主要指自然语言处理，从应用场景看，可以分为征信评级和行情预测 ，从模型来看，用的最多的是预测和回归，而其中最有挑战的是自动化交易中的强化学习，从数据处理流来看 预处理和模型构建过程中都能用到机器学习的方法。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;让我们一个个来看，首先是自然语言处理和AI会有哪些结合了，这里又可以分成五类，第一类是用于客户服务的聊天机器人，当然聊天机器人也可以用来向顾客进行产品推荐；第二类是通过对社交媒体中的内容进行情感分析来预测市场行情的；第三类则是通过生成模型，自动提取例如上市公司的财报等公开数据中的关键信息，例如文因互联这家创业公司开发的产品。第四类则是通过知识图谱做用户画像，给不同的用户打上对应的标签，从而优化产品策略和个性化推广方案的设计；第五个应用场景是在数据的预处理阶段，通过深度学习将自然语言转换成向量，从而消除句子间的歧义，并在指代同一个事物的不同语句见建立联系，从而方便多维度数据的组合。&lt;/p&gt;

&lt;p&gt;相比于无拘无束的非结构数据，结构化数据就是一张纸的表，而这也是传统的金融行业最经常处理的数据。每个人在一张表上是一条记录，而大数据则是让同一个人出现在不同的表上，再结合多张表上的弱信号，去预测这个人未来的行为。大数据不是指数据量有多大，而是指结合之前无法被有效利用的边角数据的一整套方法。对于金融行业来说，最经常做的就是借与贷。而为了控制风险，就需要征信模型了。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.1229357798165138&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccUibfib97pQbTAPaTWWPp1lnSeu56FAsxVt5Qoicsj9eOlOMV12pB2utjpqkMbTvT20yZtsPP5CrBicA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;545&quot; /&gt;&lt;/p&gt;


&lt;p&gt;征信模型可以看成是一颗决策树，要判断那些人有借有还。而&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383399&amp;amp;idx=1&amp;amp;sn=97153224277dd77dfec9b931a798fa30&amp;amp;chksm=84f3c826b38441305dbb8d04341ab3d69337d4862c2ae8e6ff37034444752bd08dd5f06bc653&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;树模型&lt;/a&gt;也是征信中常用的一种模型。相比与其他领域的应用，金融领域的每一次错误预测都需要付出真金白银的代价，因此需要模型具有绝对的高可用性，这意味着模型不仅不能花费太多的时间，也不能因为用户人数的增加就使用简化而效果稍差的模型（模型的运行时间稳定），模型还需要对极端案例的判别也不能有系统性的误差。另外，由于可以征信模型可以使用的数据有很多来源，例如信用卡消费数据，社交网络上的连接关系，甚至包括在阅读类APP留下的阅读记录，征信模型需要能灵活的引入新的特征，而不必每次都从头开始。&lt;/p&gt;

&lt;p&gt;正是由于如上的几个原因，深度学习在征信模型中还没有得到广泛的应用。深度学习训练出的模型相对来说是一个黑箱，人们难以理解模型为何做决策，而且深度学习的模型容易被恶意的攻破，而征信模型除了要对普通人进行信用打分，还需要应对欺诈风险和模型使用者的误操作风险。而例如随机森林 XGBoost这样的树模型，以及多重线性回归这样的传统方法，则能相对较好的满足上述的需求。&lt;/p&gt;

&lt;p&gt;征信模型不止是打一个分这样的分类问题，还包括授予TA多少信用额度这样的回归问题。而在借与贷的另一个链条上，则涉及如何向顾客推荐理财产品，以提高转化率和客户满意度，这同样是一个分类问题，可用类似的方法解决。而在保险，企业贷等领域的应用，也可以算成是征信问题的变种。&lt;/p&gt;

&lt;p&gt;接下来我们看看深度学习在金融市场中的应用，这里包括毫秒级进行自动化交易的高频交易算法和在更大的时间尺度上来预测市场未来的行情，例如neural finance（https://github.com/Metnew/neural-finance）。这方面学术界有很多探索，用到的既包括常用的处理时间序列的模型，例如&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381709&amp;amp;idx=1&amp;amp;sn=1697cba21960594b188cc5b53cbd108e&amp;amp;chksm=84f3f18cb384789a279a3c41e6d2c07fe7312f4bda82a1c0e5e1ff4243f34beea4ed0643056a&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;RNN&lt;/a&gt;（点击查看介绍文章）LSTM，还包括非监督学习中的自编码器。&lt;/p&gt;

&lt;p&gt;一个常见的工具包是bulbea，这个开源的python包不仅集成了股票市场的预测模型，还包括对模型效果的可视化工具。模型基于各股票的历史数据，计算股票未来的走势，模型使用起来很简单，还可以通过可视化展示模型的效果，下图展示了模型预测的误差是很低的。bulbea还集成了相应的API，可以对twiter中的句子进行情感分析，用社交媒体在的信息来指导交易。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6588628762541806&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccUibfib97pQbTAPaTWWPp1ln64QQ91ofiaMhVaBAVlu7wmc834Tyicz5iaYnHgPrX2pd2gbseeuD4gqrw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;897&quot; /&gt;&lt;/p&gt;

&lt;p&gt;接下来介绍&lt;span&gt;liquidity.ai（https://github.com/BenjiKCF/Neural-Network-with-Financial-Time-Series-Data），一个基于tensorflow的开源的工具，集成了训练数据，数据预处理及LSTM模型。该模型会根据当前的行情，实时的给出对每个资产，是应该买入卖出还是持有的概率。这类的模型在真实世界中，看重的不是其预测的有多准确，而是交易员该如何综合的使用该模型给出的概率，以及其他的机器学习模型给出的预测，最终这个结合了诸多机器和人类智能的系统究竟能否赚钱。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;总结一下，这篇小文介绍了机器学习在金融中的应用，部分例如反欺诈及数据安全方面的应用，这里没有介绍到，但也是一个有潜力的反向，而学界研究最热的，还是将强化学习的模式用在金融中，只是这方面的成果还无法走入工业界。另外需要提醒的是，深度学习不是万能药，在市场预测上，深度学习模型的表现不一定总会好于传统的方法，而且深度学习所需的时间资源和计算资源都远超传统方法，因此是一项高投入但高潜力的尝试。&lt;/p&gt;

&lt;p&gt;接下来是福利时间，去年特训营的课程中的精华部分限时免费啦！&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.77734375&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccUibfib97pQbTAPaTWWPp1lnSZyjIPSHd5dpibsdQFmbdmIIKWJvyZ7IQ7icxZ98VHibMo80iaDWAKbcIg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1280&quot; /&gt;&lt;/p&gt;



</description>
<pubDate>Sat, 17 Mar 2018 11:07:27 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/eSFV6NdBKc</dc:identifier>
</item>
<item>
<title>聊聊机器学习中的那些树</title>
<link>http://www.jintiankansha.me/t/ChsTXw4a6G</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/ChsTXw4a6G</guid>
<description>&lt;p&gt;树模型是机器学习领域内，除了深度学习之外，使用的最为广泛，也是变种特别多的一种模型了，树模型的好处是其很容易理解，且相对不容易过拟合，训练时对资源的消耗也更少。最常用树模型包括决策树，随机森林及XGBoos。而在去年，南大的周志华教授提出了deep forest，一种借鉴深度学习的树模型，树模型还有其他的更为冷门的变种，例如正则化贪心森林和。这篇文章将始简单的介绍下上述的几种树模型的原理，树模型是最容易理解的，请您放心，本文只有一个公式，是关于信息熵的。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7043650793650794&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccogNPcQ5WnNa6AdkIyxtnysA1u4drKZGG7VaNsawbhbH4PeB4PCE4ejFgAevhD7eSfNOgXYo28GQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;504&quot; /&gt;&lt;br /&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9223529411764706&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccogNPcQ5WnNa6AdkIyxtnyMw9nodQCiaXia2rSrHex5ERv0kNCPhdEXBuCofank9rc7ibJTyE71ibribw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;425&quot; /&gt;&lt;/p&gt;
&lt;p&gt;树模型主要用来做分类。最简单的一种叫做决策树，决策树是一个非常接近人类思维的模型。 它形象的说就是一个调查问卷， 把一个最终的决策转化为个若干问题， 每一步得到一个答案， 按照答案的正否来决定下一个问题是什么，如此形成一个树结构， 最后形成一个分类器。 比如经常被举出的例子， 你要买电脑， 要根据很多特征挑选电脑，比如cpu，gpu，硬盘，内存等， 你一定会问你自己一系列问题， 我要买那款cpu，gpu， 硬盘， 内存等，最后做出决策。决策树要做的是把这个过程自动化，最后给我们我们希望的判定结果。&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;table width=&quot;553&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;C&lt;/span&gt;&lt;span&gt;pu&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;G&lt;/span&gt;&lt;span&gt;pu&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;内存&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;决策&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;高&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;中&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;低&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;买&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;高&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;高&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;中&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;买&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;高&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;中&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;低&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;不买&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在一棵决策树上，其中的节点可以分成根节点（蓝色） 决策节点（红色）和终止节点（绿色），而图中的方框里包含的即是一颗子树，这么看来，树模型是不是特别好理解？树模型的第二个好处是可以方便的探索数据中那些维度更加重要（对做出正确的预测贡献更大），比如上述的买电脑的例子，你会发现对于大多数人来说，CPU的型号最关键。树模型的第三个好处是不怎么需要做数据清洗和补全，还用买电脑的例子，假设你拿到的数据部分中没有告诉GPU的型号，你不必要丢掉这部分数据，进入到相应的子树里，随机的让这条数据进入一个终止节点就好了，这样，你便能够利用缺失的数据了。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5506756756756757&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGia5Hb4EM9I3ice9oPfmgZWacJDocvMicthH9CmR8JVXcnErdmGOKQaoATw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;592&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;谈起树模型，就要说起基尼系数，这个指数最常见的场景是描述贫富差距，但也可以用来指导树模型在那里分叉。假设一颗最简单做二分类问题的决策树，拿到的数据分为两种特征，一个是性别，一个是班级，预测学生们愿不愿打板球，下面的图是两种不同的树模型，用性别来分，10个女生中有2个愿意打球，而20个男生中有13个愿意打球，而用班级分，效果则没有那么好，具体怎么计算了，先从左到右依次计算每个终止节点的基尼系数，(0.2)*(0.2)+(0.8)*(0.8)=0.68  (0.65)*(0.65)+(0.35)*(0.35)=0.55 (0.43)*(0.43)+(0.57)*(0.57)=0.51  (0.56)*(0.56)+(0.44)*(0.44)=0.51，之后对每棵树的基尼系数进行加权平均 ：10/30)*0.68+(20/30)*0.55 = 0.59（按性别分），(14/30)*0.51+(16/30)*0.51 = 0.51（按班级分），因此在该例子中，性别是一个更好的特征。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.35207823960880197&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGia5pP3W6ENljz9Vic1nPvicj6OVaojibwAJl0icom3R1aV8PRz3lQYibqf8gQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;818&quot; /&gt;&lt;/p&gt;


&lt;p&gt;理解决策树的下一个重要的概念是信息增益，信息可以看成是减少了多少系统中的无序，而描述系统的无序程度，可以用信息熵，对于二分类问题，计算公式是  &lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.13761467889908258&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaMD3iadJ2PO53rDHgltclFOtOZazLSmyYdnIiawYlNS6ic4JSISGAP5GLQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;218&quot; /&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccogNPcQ5WnNa6AdkIyxtnyGEibYlXRFjBvOGicGltricVQU2vZutNicZmy1vljdVwu8QL4YrpenrvbWA/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;0.7298245614035088&quot; data-w=&quot;570&quot; /&gt;&lt;/p&gt;
&lt;p&gt;对于每一次树上的分叉，先算下父节点的熵，再计算下子节点的熵的加权平均，就可以计算出决策树中的一个决策节点带来了多少信息增益了。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.17941952506596306&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaavfmHpWrxeTic90ArwS5nhSFhfgPbETKsXAhdVORwwpibfhqSdRNKwzA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;379&quot; /&gt;&lt;/p&gt;
&lt;p&gt;信息熵公式告诉我们的是，我们每次对所有特征都扫描一遍，选择那个让我们的信息增长最大的特征。 依次在这个特征的每个可能取值下，我们在寻找第二个关键特征，列出第二个特征选的可能取值并寻找第三个特征依次类推。 再对每一分支的操作里， 如果我们发现在某个特征组合下的样本均为一类， 则停止分叉的过程。 整个操作过程形似寻找一颗不断分叉的树木， 故名决策树。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  决策树能够处理特征之间的互相影响， 因为特征之间的互相影响，我们并不像简单贝叶斯那样并列的处理这些特征。 例如某个特征可能在某个条件下式好的， 但在另外条件下就是坏的或者没影响。 比如说找对象，你只在对方漂亮的时候才care他学历。 我会根据之前问过的问题的答案来选择下一步问什么样的问题， 如此， 我就能很好的处理特征之间的关联。&lt;/p&gt;

&lt;p&gt;我们把这样的思维步骤写成伪代码， 大概是这样的 :&lt;/p&gt;

&lt;p&gt;训练集D （x1，y1）….&lt;/p&gt;
&lt;p&gt;属性 A attribute  （a1，a2…..）&lt;/p&gt;

&lt;p&gt;函数treegenerate &lt;/p&gt;

&lt;p&gt;1,   生成结点node A（任选一个特征）&lt;/p&gt;
&lt;p&gt;2， 判断D在A中样本是否都属于类型C，是则A标记为C类叶结点， 结束&lt;/p&gt;
&lt;p&gt;3， 判断A为空或D在A样本取值同（x相同而非y），将node 标记为样本多数分类的叶结点（max numbers），结束&lt;/p&gt;

&lt;p&gt;终止条件不成立则: &lt;/p&gt;

&lt;p&gt;从A中选择最优划分属性a*,   &lt;/p&gt;

&lt;p&gt;循环:&lt;/p&gt;
&lt;p&gt;对A*上的每一个值a*做如下处理：&lt;/p&gt;
&lt;p&gt;If a*上的样本为空，则a*为叶节点 （该值下用于判断的样本不足，判定为A*中样本最多的类），&lt;/p&gt;

&lt;p&gt;如果支点上的样本集为D**  &lt;/p&gt;
&lt;p&gt;如果存在某个位置，使得D**为空，&lt;/p&gt;
&lt;p&gt;则A*为叶节点，&lt;/p&gt;
&lt;p&gt;否则，以a*为分支节点，回到第一句     &lt;/p&gt;

&lt;p&gt;接下来我们看一看更为复杂的情况，比如我们拿到的数据特征不是两个，而是一百个，那么问题来了，我们的决策树也要100层那么深吗？如果真的这么深，那么这个模型很容易过拟合的，任何一颗决策树的都应该有终止条件，例如树最深多少层，每个节点最少要有多少样本，最多有多少个终止节点等，这些和终止条件有关的超参数设置决定了模型会不会过拟合。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面我们从一棵树过度到一群数，也就是机器学习中常用的begging，将原来的训练数据集分成多份，每一份分别训练一个分类器，最后再让这些分类器进行投票表决。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7505197505197505&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiabOM7yFPZH3GbWjwBkM8ibW5ib8azNn5W2dwG7LJvpSN7muFI4OwNDwicA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;481&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而随机森林，就是使用begging技巧加持的决策树，是不是很简单？相比于决策树，随机森林的可解释性差一些，另外对于标签为连续的回归问题，随机森林所采取的求多个树的平均数的策略会导致结果的不稳定。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8840864440078585&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfCI3lrovIztO53FwOjLuGia82D0XAAQgA2PvEvYjkyp9rcmFoiciaHrelNBsN3LFPibCA5riaPawRs7cQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1018&quot; /&gt;&lt;/p&gt;

&lt;p&gt;随机森林是将训练数据随机的分成很多类，分别训练很多分类器，再将这些分类器聚合起来，而boosting则不讲训练数据分类，而是将弱分类器聚合起来，下图的上半部分可以看成描述了三个弱分类器，每一个都有分错的，而将他们集合起来，可以得出一个准确率比每一个弱分类器都高的分类模型。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7373188405797102&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaUn7iccltHA6kgCcWg5ia8EIFVQES6NkGpLqP4exSjnBCXW4yNicIVfuIw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;552&quot; /&gt;&lt;/p&gt;
&lt;p&gt;你需要做的是将第一个分类器分类分错的部分交给第二个分类器，再将第二个分类器分错的部分交给第三个分类器，如下图依次所示&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6840148698884758&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGia1VhsROFuFFDbWzJh0A9ed1Oq6L8s11uiaPaZEpovyGicbrblHL7tcSCQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;269&quot; /&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6309523809523809&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaCBGlVicia4SxBjHx93K22OL0bK7Qa4srXicibSUWc3U4x2XLES54ibg7bAg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;252&quot; /&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5919117647058824&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiahP1lhIAUqgicsNoIaCXjqtVjia2PV59pSnlcnoOFMK8xKNRbBWgUFMmQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;272&quot; /&gt;&lt;/p&gt;
&lt;p&gt;最终得到了我们看到的强分类器。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.562962962962963&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaDZWhol4cKibgwHuiaOMAf1BMNXSb7PtwxLzQLvy8NJakr1icVsic3ZyOnw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;270&quot; /&gt;&lt;/p&gt;
&lt;p&gt;总结来看，begging类似于蚁群的智慧，没有一只蚂蚁知道全部的信息，但利用蚂蚁的集合，可以实现集愚成智，而boosting则是三个臭皮匠，胜过诸葛亮。Boost方法包含的非线性变换比较多，表达能力强，而且不需要做复杂的特征工程和特征变换。但不同于随机森林，它是一个串行过程，不好并行化，而且计算复杂度高。&lt;/p&gt;

&lt;p&gt;XGBoost 是 Extreme Gradient Boosting （极端梯度上升）的缩写，是当下最常用的树模型了，是上图描述的Boosting  Tree的一种高效实现，在R，Python等常用的语言下都有对应的包，它把树模型复杂度作为正则项加到优化目标中，从而避免了过于复杂而容易过拟合的模型。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在Boost方法中，每一个被错误分类的样本的权值会增加，以强调最困难的情况，从而使得接下来的模型能集中注意力来处理这些错误的样本，然而这种方法把基于学习器的决策树视为一个黑盒子，没有利用树结构本身。而Regularized Greedy Forest正则化贪心森林(RGF)会在当前森林某一步的结构变化后，依次调整整个森林中所有决策树对应的“叶子”的权重，使损失函数最小化。例如下图我们从原来的森林中发下右下的节点可以分叉，我们做的不止是将分叉后的树加入森林，而且对森林中已有的树中的对应节点也进行类似的分叉操作。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7467994310099573&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaGlISibqLP9vKNz9p8EIUq7Yju4NF5Kic9dEEakiaItNMcvvcMFkjy2CUw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;703&quot; /&gt;&lt;/p&gt;
&lt;p&gt;类似boost，RGF中每个节点的权重也要不断优化，但不同的是，RGF不需要在梯度下降决策树设置所需的树尺寸（tree size）参数（例如，树的数量，最大深度）。总结一下RGF是另一种树集成技术，它类似梯度下降算法，可用于有效建模非线性关系。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面说说去年周志华教授提出深度森林deep forest，也叫做 gcForest，这也是一种基于决策树的集成方法，下图中每一层包括两个随机森林（蓝色）和两个complete random forests（黑色），所谓complete random forest，指的是其中的1000棵决策树的每个节点都随机的选择一个特征作为分裂特征，不断增长整棵树，直到剩余所有样本属于同一类，或样本数量少于10。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4683357879234168&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiavUKYLKOZxt194P0CuJXXxrYg7SNGTt1VJ4ia00yP5eib0pux3cmZCdEA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;679&quot; /&gt;&lt;/p&gt;
&lt;p&gt;至于每一层的输出，也不是传统决策树的一个标签，而是一个向量。图中的每一个森林对每个输入样本都有一个输出，对应建立该决策树时，落在该叶子节点中的样本集合中各个类别的样本所占的比例，如下图所示，将多颗树的结果求平均，得出这一层的输出。为了避免过拟合，每个森林中 class vector 的产生采用了 k 折交叉验证的方法，随机的将k分之一的训练样本丢出去，再对k次训练的结果求平均值。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5438931297709924&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaJqeIwwrhCAD5zFfZCaSTEheUE8KeTZQkc3xoMNWU44QCIibEYnzzLpg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;524&quot; /&gt;&lt;/p&gt;
&lt;p&gt;deep forest还采取了类似卷积神经网络的滑动窗口，如下图所示，原始样本的为400维，定义一个大小为100的滑动窗口，将滑动窗口从原特征上依次滑过，每次移动一步，每次窗口滑动获取的100个特征作为一个新的实例，等效于在400维特征上每相邻100维的特征摘出来作为一个特征实例，得到301个新的特征实例（400 - 300 + 1）。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.36616702355460384&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaX03Gwuy7AyAAIw5vDadng2MYAqXKjuD5ayLm9r76yKe3BicnIO7mPLQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;467&quot; /&gt;&lt;/p&gt;
&lt;p&gt;深度森林的源代码也在Github上有开源版，总结一下，深度森林具有比肩深度神经网络的潜力，例如可以层次化的进行特征提取及使用预训练模型进行迁移学习，相比于深度学习，其具有少得多的超参数，并且对参数设置不太敏感，且在小数据集上，例如手写数字识别中，表现的不比CNN差。深度森林的数据处理流如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3986220472440945&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaT4ibuGlL5PIabyummicHuhBX0SaYIld2gA69u89XYdzO6HyoZYZk8F2Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1016&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;总结下，树模型作为一个常见的白盒模型，不管数据集的大小，不管是连续的回归问题还是分类问题都适用。它不怎么需要进行数据预处理，例如补全缺失值，去除异常点。树模型可以针对特征按照重要性进行排序，从而构造新的特征或从中选出子集来压缩数据。树模型可以通过统计的方式去验证模型的准确值，判断训练的进展，相比机器学习的模型，需要调整的超参数也更少。但和神经网络一样，树模型也不够健壮，如同图像上只需要改变几个像素点就可以改变模型的结果，树模型中输入数据的微小变化也可能会显著改变模型的结果。树模型也有过拟合的危险，通过剪纸purning，即先让树长的深一些，再去除那些不带来信息增益的分叉，留下那些最初的信息增益为负，但整体的信息增益为正的节点，可以组织树模型的过拟合。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.4889267461669506&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaghgFicuGJBzgr1yZCIrshhXpnP6mGY0eG38fP57TEocSkiaxcbtA6NOw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;587&quot; /&gt;&lt;/p&gt;








</description>
<pubDate>Sun, 25 Feb 2018 05:49:07 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/ChsTXw4a6G</dc:identifier>
</item>
</channel>
</rss>