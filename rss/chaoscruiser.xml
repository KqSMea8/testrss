<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>聊聊机器学习中的那些树</title>
<link>http://www.jintiankansha.me/t/ChsTXw4a6G</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/ChsTXw4a6G</guid>
<description>&lt;p&gt;树模型是机器学习领域内，除了深度学习之外，使用的最为广泛，也是变种特别多的一种模型了，树模型的好处是其很容易理解，且相对不容易过拟合，训练时对资源的消耗也更少。最常用树模型包括决策树，随机森林及XGBoos。而在去年，南大的周志华教授提出了deep forest，一种借鉴深度学习的树模型，树模型还有其他的更为冷门的变种，例如正则化贪心森林和。这篇文章将始简单的介绍下上述的几种树模型的原理，树模型是最容易理解的，请您放心，本文只有一个公式，是关于信息熵的。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7043650793650794&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccogNPcQ5WnNa6AdkIyxtnysA1u4drKZGG7VaNsawbhbH4PeB4PCE4ejFgAevhD7eSfNOgXYo28GQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;504&quot; /&gt;&lt;br /&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9223529411764706&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccogNPcQ5WnNa6AdkIyxtnyMw9nodQCiaXia2rSrHex5ERv0kNCPhdEXBuCofank9rc7ibJTyE71ibribw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;425&quot; /&gt;&lt;/p&gt;
&lt;p&gt;树模型主要用来做分类。最简单的一种叫做决策树，决策树是一个非常接近人类思维的模型。 它形象的说就是一个调查问卷， 把一个最终的决策转化为个若干问题， 每一步得到一个答案， 按照答案的正否来决定下一个问题是什么，如此形成一个树结构， 最后形成一个分类器。 比如经常被举出的例子， 你要买电脑， 要根据很多特征挑选电脑，比如cpu，gpu，硬盘，内存等， 你一定会问你自己一系列问题， 我要买那款cpu，gpu， 硬盘， 内存等，最后做出决策。决策树要做的是把这个过程自动化，最后给我们我们希望的判定结果。&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;table width=&quot;553&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;C&lt;/span&gt;&lt;span&gt;pu&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;G&lt;/span&gt;&lt;span&gt;pu&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;内存&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;决策&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;高&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;中&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;低&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;买&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;高&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;高&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;中&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;买&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;高&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;中&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;低&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;不买&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在一棵决策树上，其中的节点可以分成根节点（蓝色） 决策节点（红色）和终止节点（绿色），而图中的方框里包含的即是一颗子树，这么看来，树模型是不是特别好理解？树模型的第二个好处是可以方便的探索数据中那些维度更加重要（对做出正确的预测贡献更大），比如上述的买电脑的例子，你会发现对于大多数人来说，CPU的型号最关键。树模型的第三个好处是不怎么需要做数据清洗和补全，还用买电脑的例子，假设你拿到的数据部分中没有告诉GPU的型号，你不必要丢掉这部分数据，进入到相应的子树里，随机的让这条数据进入一个终止节点就好了，这样，你便能够利用缺失的数据了。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5506756756756757&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGia5Hb4EM9I3ice9oPfmgZWacJDocvMicthH9CmR8JVXcnErdmGOKQaoATw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;592&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;谈起树模型，就要说起基尼系数，这个指数最常见的场景是描述贫富差距，但也可以用来指导树模型在那里分叉。假设一颗最简单做二分类问题的决策树，拿到的数据分为两种特征，一个是性别，一个是班级，预测学生们愿不愿打板球，下面的图是两种不同的树模型，用性别来分，10个女生中有2个愿意打球，而20个男生中有13个愿意打球，而用班级分，效果则没有那么好，具体怎么计算了，先从左到右依次计算每个终止节点的基尼系数，(0.2)*(0.2)+(0.8)*(0.8)=0.68  (0.65)*(0.65)+(0.35)*(0.35)=0.55 (0.43)*(0.43)+(0.57)*(0.57)=0.51  (0.56)*(0.56)+(0.44)*(0.44)=0.51，之后对每棵树的基尼系数进行加权平均 ：10/30)*0.68+(20/30)*0.55 = 0.59（按性别分），(14/30)*0.51+(16/30)*0.51 = 0.51（按班级分），因此在该例子中，性别是一个更好的特征。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.35207823960880197&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGia5pP3W6ENljz9Vic1nPvicj6OVaojibwAJl0icom3R1aV8PRz3lQYibqf8gQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;818&quot; /&gt;&lt;/p&gt;


&lt;p&gt;理解决策树的下一个重要的概念是信息增益，信息可以看成是减少了多少系统中的无序，而描述系统的无序程度，可以用信息熵，对于二分类问题，计算公式是  &lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.13761467889908258&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaMD3iadJ2PO53rDHgltclFOtOZazLSmyYdnIiawYlNS6ic4JSISGAP5GLQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;218&quot; /&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccogNPcQ5WnNa6AdkIyxtnyGEibYlXRFjBvOGicGltricVQU2vZutNicZmy1vljdVwu8QL4YrpenrvbWA/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;0.7298245614035088&quot; data-w=&quot;570&quot; /&gt;&lt;/p&gt;
&lt;p&gt;对于每一次树上的分叉，先算下父节点的熵，再计算下子节点的熵的加权平均，就可以计算出决策树中的一个决策节点带来了多少信息增益了。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.17941952506596306&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaavfmHpWrxeTic90ArwS5nhSFhfgPbETKsXAhdVORwwpibfhqSdRNKwzA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;379&quot; /&gt;&lt;/p&gt;
&lt;p&gt;信息熵公式告诉我们的是，我们每次对所有特征都扫描一遍，选择那个让我们的信息增长最大的特征。 依次在这个特征的每个可能取值下，我们在寻找第二个关键特征，列出第二个特征选的可能取值并寻找第三个特征依次类推。 再对每一分支的操作里， 如果我们发现在某个特征组合下的样本均为一类， 则停止分叉的过程。 整个操作过程形似寻找一颗不断分叉的树木， 故名决策树。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  决策树能够处理特征之间的互相影响， 因为特征之间的互相影响，我们并不像简单贝叶斯那样并列的处理这些特征。 例如某个特征可能在某个条件下式好的， 但在另外条件下就是坏的或者没影响。 比如说找对象，你只在对方漂亮的时候才care他学历。 我会根据之前问过的问题的答案来选择下一步问什么样的问题， 如此， 我就能很好的处理特征之间的关联。&lt;/p&gt;

&lt;p&gt;我们把这样的思维步骤写成伪代码， 大概是这样的 :&lt;/p&gt;

&lt;p&gt;训练集D （x1，y1）….&lt;/p&gt;
&lt;p&gt;属性 A attribute  （a1，a2…..）&lt;/p&gt;

&lt;p&gt;函数treegenerate &lt;/p&gt;

&lt;p&gt;1,   生成结点node A（任选一个特征）&lt;/p&gt;
&lt;p&gt;2， 判断D在A中样本是否都属于类型C，是则A标记为C类叶结点， 结束&lt;/p&gt;
&lt;p&gt;3， 判断A为空或D在A样本取值同（x相同而非y），将node 标记为样本多数分类的叶结点（max numbers），结束&lt;/p&gt;

&lt;p&gt;终止条件不成立则: &lt;/p&gt;

&lt;p&gt;从A中选择最优划分属性a*,   &lt;/p&gt;

&lt;p&gt;循环:&lt;/p&gt;
&lt;p&gt;对A*上的每一个值a*做如下处理：&lt;/p&gt;
&lt;p&gt;If a*上的样本为空，则a*为叶节点 （该值下用于判断的样本不足，判定为A*中样本最多的类），&lt;/p&gt;

&lt;p&gt;如果支点上的样本集为D**  &lt;/p&gt;
&lt;p&gt;如果存在某个位置，使得D**为空，&lt;/p&gt;
&lt;p&gt;则A*为叶节点，&lt;/p&gt;
&lt;p&gt;否则，以a*为分支节点，回到第一句     &lt;/p&gt;

&lt;p&gt;接下来我们看一看更为复杂的情况，比如我们拿到的数据特征不是两个，而是一百个，那么问题来了，我们的决策树也要100层那么深吗？如果真的这么深，那么这个模型很容易过拟合的，任何一颗决策树的都应该有终止条件，例如树最深多少层，每个节点最少要有多少样本，最多有多少个终止节点等，这些和终止条件有关的超参数设置决定了模型会不会过拟合。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面我们从一棵树过度到一群数，也就是机器学习中常用的begging，将原来的训练数据集分成多份，每一份分别训练一个分类器，最后再让这些分类器进行投票表决。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7505197505197505&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiabOM7yFPZH3GbWjwBkM8ibW5ib8azNn5W2dwG7LJvpSN7muFI4OwNDwicA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;481&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而随机森林，就是使用begging技巧加持的决策树，是不是很简单？相比于决策树，随机森林的可解释性差一些，另外对于标签为连续的回归问题，随机森林所采取的求多个树的平均数的策略会导致结果的不稳定。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8840864440078585&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfCI3lrovIztO53FwOjLuGia82D0XAAQgA2PvEvYjkyp9rcmFoiciaHrelNBsN3LFPibCA5riaPawRs7cQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1018&quot; /&gt;&lt;/p&gt;

&lt;p&gt;随机森林是将训练数据随机的分成很多类，分别训练很多分类器，再将这些分类器聚合起来，而boosting则不讲训练数据分类，而是将弱分类器聚合起来，下图的上半部分可以看成描述了三个弱分类器，每一个都有分错的，而将他们集合起来，可以得出一个准确率比每一个弱分类器都高的分类模型。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7373188405797102&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaUn7iccltHA6kgCcWg5ia8EIFVQES6NkGpLqP4exSjnBCXW4yNicIVfuIw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;552&quot; /&gt;&lt;/p&gt;
&lt;p&gt;你需要做的是将第一个分类器分类分错的部分交给第二个分类器，再将第二个分类器分错的部分交给第三个分类器，如下图依次所示&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6840148698884758&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGia1VhsROFuFFDbWzJh0A9ed1Oq6L8s11uiaPaZEpovyGicbrblHL7tcSCQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;269&quot; /&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6309523809523809&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaCBGlVicia4SxBjHx93K22OL0bK7Qa4srXicibSUWc3U4x2XLES54ibg7bAg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;252&quot; /&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5919117647058824&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiahP1lhIAUqgicsNoIaCXjqtVjia2PV59pSnlcnoOFMK8xKNRbBWgUFMmQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;272&quot; /&gt;&lt;/p&gt;
&lt;p&gt;最终得到了我们看到的强分类器。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.562962962962963&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaDZWhol4cKibgwHuiaOMAf1BMNXSb7PtwxLzQLvy8NJakr1icVsic3ZyOnw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;270&quot; /&gt;&lt;/p&gt;
&lt;p&gt;总结来看，begging类似于蚁群的智慧，没有一只蚂蚁知道全部的信息，但利用蚂蚁的集合，可以实现集愚成智，而boosting则是三个臭皮匠，胜过诸葛亮。Boost方法包含的非线性变换比较多，表达能力强，而且不需要做复杂的特征工程和特征变换。但不同于随机森林，它是一个串行过程，不好并行化，而且计算复杂度高。&lt;/p&gt;

&lt;p&gt;XGBoost 是 Extreme Gradient Boosting （极端梯度上升）的缩写，是当下最常用的树模型了，是上图描述的Boosting  Tree的一种高效实现，在R，Python等常用的语言下都有对应的包，它把树模型复杂度作为正则项加到优化目标中，从而避免了过于复杂而容易过拟合的模型。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在Boost方法中，每一个被错误分类的样本的权值会增加，以强调最困难的情况，从而使得接下来的模型能集中注意力来处理这些错误的样本，然而这种方法把基于学习器的决策树视为一个黑盒子，没有利用树结构本身。而Regularized Greedy Forest正则化贪心森林(RGF)会在当前森林某一步的结构变化后，依次调整整个森林中所有决策树对应的“叶子”的权重，使损失函数最小化。例如下图我们从原来的森林中发下右下的节点可以分叉，我们做的不止是将分叉后的树加入森林，而且对森林中已有的树中的对应节点也进行类似的分叉操作。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7467994310099573&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaGlISibqLP9vKNz9p8EIUq7Yju4NF5Kic9dEEakiaItNMcvvcMFkjy2CUw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;703&quot; /&gt;&lt;/p&gt;
&lt;p&gt;类似boost，RGF中每个节点的权重也要不断优化，但不同的是，RGF不需要在梯度下降决策树设置所需的树尺寸（tree size）参数（例如，树的数量，最大深度）。总结一下RGF是另一种树集成技术，它类似梯度下降算法，可用于有效建模非线性关系。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面说说去年周志华教授提出深度森林deep forest，也叫做 gcForest，这也是一种基于决策树的集成方法，下图中每一层包括两个随机森林（蓝色）和两个complete random forests（黑色），所谓complete random forest，指的是其中的1000棵决策树的每个节点都随机的选择一个特征作为分裂特征，不断增长整棵树，直到剩余所有样本属于同一类，或样本数量少于10。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4683357879234168&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiavUKYLKOZxt194P0CuJXXxrYg7SNGTt1VJ4ia00yP5eib0pux3cmZCdEA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;679&quot; /&gt;&lt;/p&gt;
&lt;p&gt;至于每一层的输出，也不是传统决策树的一个标签，而是一个向量。图中的每一个森林对每个输入样本都有一个输出，对应建立该决策树时，落在该叶子节点中的样本集合中各个类别的样本所占的比例，如下图所示，将多颗树的结果求平均，得出这一层的输出。为了避免过拟合，每个森林中 class vector 的产生采用了 k 折交叉验证的方法，随机的将k分之一的训练样本丢出去，再对k次训练的结果求平均值。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5438931297709924&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaJqeIwwrhCAD5zFfZCaSTEheUE8KeTZQkc3xoMNWU44QCIibEYnzzLpg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;524&quot; /&gt;&lt;/p&gt;
&lt;p&gt;deep forest还采取了类似卷积神经网络的滑动窗口，如下图所示，原始样本的为400维，定义一个大小为100的滑动窗口，将滑动窗口从原特征上依次滑过，每次移动一步，每次窗口滑动获取的100个特征作为一个新的实例，等效于在400维特征上每相邻100维的特征摘出来作为一个特征实例，得到301个新的特征实例（400 - 300 + 1）。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.36616702355460384&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaX03Gwuy7AyAAIw5vDadng2MYAqXKjuD5ayLm9r76yKe3BicnIO7mPLQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;467&quot; /&gt;&lt;/p&gt;
&lt;p&gt;深度森林的源代码也在Github上有开源版，总结一下，深度森林具有比肩深度神经网络的潜力，例如可以层次化的进行特征提取及使用预训练模型进行迁移学习，相比于深度学习，其具有少得多的超参数，并且对参数设置不太敏感，且在小数据集上，例如手写数字识别中，表现的不比CNN差。深度森林的数据处理流如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3986220472440945&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaT4ibuGlL5PIabyummicHuhBX0SaYIld2gA69u89XYdzO6HyoZYZk8F2Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1016&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;总结下，树模型作为一个常见的白盒模型，不管数据集的大小，不管是连续的回归问题还是分类问题都适用。它不怎么需要进行数据预处理，例如补全缺失值，去除异常点。树模型可以针对特征按照重要性进行排序，从而构造新的特征或从中选出子集来压缩数据。树模型可以通过统计的方式去验证模型的准确值，判断训练的进展，相比机器学习的模型，需要调整的超参数也更少。但和神经网络一样，树模型也不够健壮，如同图像上只需要改变几个像素点就可以改变模型的结果，树模型中输入数据的微小变化也可能会显著改变模型的结果。树模型也有过拟合的危险，通过剪纸purning，即先让树长的深一些，再去除那些不带来信息增益的分叉，留下那些最初的信息增益为负，但整体的信息增益为正的节点，可以组织树模型的过拟合。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.4889267461669506&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaghgFicuGJBzgr1yZCIrshhXpnP6mGY0eG38fP57TEocSkiaxcbtA6NOw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;587&quot; /&gt;&lt;/p&gt;








</description>
<pubDate>Sun, 25 Feb 2018 05:49:07 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/ChsTXw4a6G</dc:identifier>
</item>
<item>
<title>当AI遇到生物-深度学习在生物研究中的应用案例列表</title>
<link>http://www.jintiankansha.me/t/mWM1JNlf05</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/mWM1JNlf05</guid>
<description>&lt;p&gt;深度学习究竟将如何改变未来，一个具有光明前景的领域就是其在医疗以及生物学相关问题上的应用，在相关论文预印本网站bioRxiv上，可以找到很多相关的文章。由于这个领域的进步很快，&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382048&amp;amp;idx=1&amp;amp;sn=2c0e54a51f8d35741a7ddd210d806d5c&amp;amp;chksm=84f3cf61b3844677cba2b65d113dcd11b3e59121c61b20c9c826b54d949e3acbfe27f75cb589&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;深度学习助力基因科技&lt;/a&gt;这篇一年前写的文章，现在看来已有些过时。本篇文章列出了部分现有的应用深度学习技术处理医学和生物学问题的工具，从这个列表中，可以看出当前深度学习在该领域的挑战和局限，也可以全面的了解深度学习在计算生物学，医学影像及生物信息等学科所具有的广泛应用场景。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5845697329376854&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf8Lw8osJWEV0UXpYFWmuwmAekAc9HNVqH9bP4Dpesl3kPT9eSMRPEUbpAg8lolrIKuKOBbXXdtTQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;337&quot; /&gt;&lt;/p&gt;
&lt;p&gt;深度学习+生物的论文发表数量&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5880681818181818&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcf8Lw8osJWEV0UXpYFWmuwmacvyhAzLtXia1ILun6KpK1R4a2Rs0r1emQFK7URia7mKQu8Gcg9kIobg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;352&quot; /&gt;&lt;/p&gt;
&lt;p&gt;发表的论文中所用的模型的比例&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;1&amp;gt;药物研发类&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1）药物研发中需要预测不同结构的分子相互作用的情况，这可以大幅节省研发的时间和金钱成本。通过使用实验验证的数据，Ryan Adams将分子中的原子看成图中的点，将原子间的化学键看成是图的边，使用卷积神经网络预测全新的分子的性质，这项技术又被称为分子指纹，具体参考 https://github.com/HIPS/neural-fingerprint。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7833001988071571&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcejQNVxSmiaFURGv147aIL2sujUcnficl1wtSBUaJdZumtndFsIJibDrsNjNibYqBkGIvhwT4nhiclIGwQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;503&quot; /&gt;&lt;/p&gt;
&lt;p&gt;药物分子间相互作用的示例图&lt;/p&gt;

&lt;p&gt;2）ORGAN 使用强化学习和对抗神经网络来自动化的指导新药的研发过程，该模型能够按照指定的目标，去寻找符合要求的药物的分子结构。这种方法本来是一个通用的框架，适合各种类型的离散型数据，包括文本，乐谱，而这里针对药物研发的问题进行了针对性的优化，参考 https://github.com/gablg1/ORGAN&lt;/p&gt;

&lt;p&gt;3）使用强化学习来从头开始生成药物序列，上述的两个工具，还只是辅助药物的研发，而通过RNN和强化学习的结合，https://github.com/MarcusOlivecrona/REINVENT中介绍的工具，可以从一个分子开始，生成只在特定的受体被激活的分子序列，例如针对多巴胺2型受体，这个工具生成的序列经过实验验证，95%都满足需求。&lt;/p&gt;

&lt;p&gt;4）DeepChem，这是一个python库，应用了LSTM和卷积神经网络，作为一个可以从小样本中学习的计算化学工具DeepChem不止可以应用在药物的研发，还可以用在材料科学，量子化学的研究中。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2&amp;gt;基因组学&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1）DeepVariant，这是一个2016年由谷歌的Deep Mind团队推出的工具，通过将基因数据转化成图像，再通过图像识别的模型，找出基因中有差异的部分，如下图所示，该工具在升级之后，在多项标准的检测指标中表现的和传统方法相差不多。https://github.com/google/deepvariant&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/EfuXwYe0KWq4cYNhAGFPiaRngYMww0yGZYxeBFhNgPYa0pdXjJ8U1ia5tSTJicpuicFGxU1ovhbicVibfwUaHDGucvWA/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;1.0526315789473684&quot; data-w=&quot;380&quot; /&gt;&lt;/p&gt;
&lt;p&gt;DeepVariant的原理示意图&lt;/p&gt;

&lt;p&gt;2）ADAGE ，这是一个用降噪自编码器来分析基因表达数据的工具，所谓的基因表达量数据，就是针对每个基因，在不同的细胞中检测有多少RNA从其中转录，从而得出对应的基因产生了多少影响。通过对高纬度的基因表达量数据进行降维，ADAGE可以识别出不同样本间的相互关系，相比于传统的PCA或ICA的方法，ADAGE能够更准确在表达量都较低的情况下识别出具有生物学意义的基因。https://github.com/greenelab/adage，类似的工具（使用相近的模型，实现相似的目地）还包括https://www.biorxiv.org/content/early/2017/11/05/214122 ， http://biorxiv.org/content/early/2015/11/16/031906 ， https://github.com/uci-cbcl/D-GEX 等，这里就不一一列出了。&lt;/p&gt;

&lt;p&gt;3）DanQ，DNA序列中编码蛋白质的区域被称为基因区，然而这只占序列总长度的2%，其他的序列有些作用是调控基因的表达，例如让一些基因多翻译一些，让另一些少翻译一些，而更多的部分，则不明确有什么功能。通过深度学习中的RNN或CNN等模型，可以预测基因中那一段是有调控作用的。类似的工具还有Basset DeepSEA DeepBind DeepMotif PEDLA FIDDLE， 从工具的数量上可以看出，这个领域的研究是相对容易出成果，也是具有较大潜力的。&lt;/p&gt;

&lt;p&gt;4）DeepCpG，这是一个用来预测不同细胞的基因组上那些未知会被甲基化的工具，甲基化意味着通过&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381311&amp;amp;idx=1&amp;amp;sn=d639fb0c75cf3e35de08a7502e22371b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;表观遗传学&lt;/a&gt;（点击查看表观遗传学是什么）改变了基因的表达，而基因上会发生甲基化的位置，和其附近的序列有关，因此可以进行预测。类似的还有针对单细胞测序开发的工具，参考http://www.nature.com/articles/srep19598&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;     &lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3&amp;gt; 其他应用&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1）和病人相关的一个重要应用场景是隐私保护，如何保证患者的生理数据能够有效的匿名化，是一个很重要的问题。通过使用对抗神经网络&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383280&amp;amp;idx=1&amp;amp;sn=a6fd903f2c47339c52dcea9eedf65851&amp;amp;chksm=84f3cbb1b38442a7f4aac491852e06c34794154946a3656bc4ac4805b1ef1b41cb4469ae8419&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;GAN&lt;/a&gt;，SPRINT这个工具可以生成和真实的患者数据类似的数据，但这些生成的数据无法对应到具体的患者，这样增加了对患者隐私的保护，可以增加患者共享自己数据的意愿，具体参考https://github.com/greenelab/SPRINT_gan 。&lt;/p&gt;

&lt;p&gt;2）预测衰老标记，人的年龄不止是身份证上写的那个，更关键的是你的身体是否还像年轻人那样，Young AI是一个集合了21个深度学习模型的集成模型，只需要通过你体检得出的19项生理指标，这个模型就可以预测你的实际年龄，平均误差只有5.9年，这项工具将可以用于自我评估自己的衰老状况。http://www.aging.ai&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4292682926829268&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfics3XpolQlMnibT3cav2oLT9EqHzZNnxuGD0KsXicCotm2AtSRCicr4HOVd4olBo8cTTBqSibCibvRhzw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1230&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3）Deep Heart 通过可穿戴设备检测的心跳数据提前预测中风的发作，从而为用户赢得抢救所需的时间，准确度高达97%。原理是因为心脏及各种动脉，静脉，胃，食管都连在植物性神经系统上，而心率变异率的变化与这些器官的状态有关，Deep heart  就可以通过加速度计和心率变异率的检测，来判断某个人是否有高血压或呼吸异常。&lt;/p&gt;

&lt;p&gt;4）生物实验中，每一个批次的实验，甚至是不同的实验操作者，都会引起系统性的误差，这被称为Batch Effect 批次效应。https://academic.oup.com/bioinformatics/article-abstract/33/16/2539/3611270 中提出了使用深度学习而不是传统的统计学来消除单细胞测序中的批次差异的工具，如果一个实验完成了两次，那这两次之中的差异就是批次差异，如果能通过训练神经网络，使其可以重复出两次实验之间的差异，那么就可以通过去除上述的差异来去除批次效应。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;参考资料&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://github.com/hussius/deeplearning-biology&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383250&amp;amp;idx=1&amp;amp;sn=59c8b29f52dcc4db8ddf286840bca434&amp;amp;chksm=84f3cb93b3844285742e9034aef6f23a1a88dfb17ea85b81cd364032fed123200a7cc80f3b14&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;深度学习入门最少需要知道什么？&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382986&amp;amp;idx=1&amp;amp;sn=13707c68d198bfa947eb90606ac99f05&amp;amp;chksm=84f3ca8bb384439d5c3ceeab35dd97373b1b461d4745ba2ab782998a8a3ea8722acca491f434&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;深度学习入门书单&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;


</description>
<pubDate>Sat, 24 Feb 2018 05:13:53 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/mWM1JNlf05</dc:identifier>
</item>
<item>
<title>站在风口的猪焦虑-谈谈一流知识和知识付费</title>
<link>http://www.jintiankansha.me/t/Kr20vbrVz0</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/Kr20vbrVz0</guid>
<description>&lt;p&gt;都说知识付费是风口，而站在风口，猪都能飞起来，只是如今这些飞起来的猪都很焦虑。得到预期今年上市，而最近朋友圈很多人都在转汪丁丁老师的关于一流的知识是无价的这样一篇文章。对于这样一篇文章，其中的观点既有认同之处，也有觉得不够清晰的地方，忍不住写一篇小文，集中的表达一次对知识付费的看法与期许。&lt;/p&gt;

&lt;p&gt;从用途上来说，知识有三种类别，第一种是让人吃上饭或吃好饭的学问，第二类是让吃饱饭的人获得幸福的学问，第三种则是让那些真正受使命感驱动的人获得生命意义的学问。我的观点是第三种学问是任何知识付费都无法提供的，因此你可以称其为无价。原因是自我驱动的人体悟得出的东西来自于其私人的经历，不是他们表达观点，而是观点借助他们的经历得以第一次更清晰的展现。这些观点，若没有完整的了解其背景，就不具有可复制性。无论是追求星辰大海的马斯克，还是在知识的海洋里寻找统一的汪丁丁，他们脑中的知识都是难以转化成能够为大众接受的形式的，这也是那篇文章中提到主要观点。&lt;/p&gt;

&lt;p&gt;然而对于其他两种知识，则是知识付费服务所要深耕的。得到的三个关键词是赋能，父爱，焦虑，这是针对第一类知识。而针对如何获得幸福的知识，由于幸福的生活是多姿多彩的，需要培养个人的兴趣爱好，属于喜马拉雅和知乎所关注的。下面的图是我参加的知乎live自助餐，看看其中的分类，就知道更多的是和怎么过得开心有关的。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.22628510863804982&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcez33Vv0jgtrFRqtXhkCCIoIjoRwargr0azibxsiaS122aHQicCRDmM7KszDkwUlxxKbWE3DvOZLEayQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1887&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在我看来，知识还有另外一种更复杂分类方式，这也算是一种思维套路了，设定两个维度，将不同的知识分在不同的象限上。在横轴，可以按照知识的来源是集中（单学科）还是开放（跨学科+应用）来分，而纵轴可以分成知识表达是依据理性还是依据直觉，由于大多数人大多数时候都是系统一的直觉思维，因此按照直觉写出的东西更加通俗易懂。霍金的《时间简史》属于单一学科+直觉表述（虽然书中的观点是反直觉的），薛定谔的《生命是什么》属于跨学科+理性表述，而众多的科研论文属于单一学科+理性表述，至于最难的是跨学科+直觉表述，这方面做得好的是像《人类简史》这样的作品，但同时也是伪科学泛滥的地方。&lt;/p&gt;

&lt;p&gt;说完了知识的分类，下一个问题就是排座次了。我不喜欢鄙视链，每个人都有自己的地域与地狱，知识的边际成本是零，是对传播者来说，但对于接收者却各不相同。所谓的一手知识二手知识的分类，其实也没有意义。只听一本书的主要观点，就一定只能接收到二手知识了吗？若是你听完有了自己的思想，做了自己的调查，那你拿到手的知识就带着你自己的体温，这也就是所谓的赋能。而能够重复出那些曲高和寡的知识，也不意味你掌握了世界的真相。&lt;/p&gt;

&lt;p&gt;不管做什么，都要想清楚自己最想获得什么。要做针对吃饱饭的知识服务，要做的是加强实践，当下的知识问答就很适合，你学完了，答对了所有的题目，能在小论文中恰当的引用相关的知识，那就算获得了实用的技能，就可以升职加薪。而要做针对吃饭开心的知识服务，则要做到开放，让每个人都能分享自己的快乐，让所有想要倾听的耳朵都能方便的找到频道。这两类知识服务，都是能为社会创造价值与福祉，是值得去做的。而对于那些试图回答如何成为自己这个问题的知识服务，则只能将其归于只给鸡汤不给勺的不地道行为，虽然短期会带来可观的流量，但长期必然会让人厌烦。&lt;/p&gt;

&lt;p&gt;熊逸在他的得到专栏说了解一个观点，首先要看他来自那里，反对谁。而我觉得真正要看的是那些人反对他，那些人又在找这个观点做儿子（认为他其实来源于我的思想）。任何一种观点一个概念都是动态的，社会性的。了解其历史，也就了解了内部的变化逻辑和运作机理，而看清楚了这些观点的支持者和反对者，才能预测其未来会怎样变化。这也是不管针对吃得饱还是吃得开心的知识服务都要做到的，你可以将其称之为系统化，或者是提供些百度搜不到的东西，但本质上，你做的是带着读者将书读厚而不是读薄的过程。同样是写一本书的缩略版，可以持久的知识服务要做的是提供一个登山的拐棍，让读者能够在个人攀登的时候能够累的时候支撑一下，而不是提供一个全景式的VR，让他们能够葛优躺着就登顶成功。&lt;/p&gt;

&lt;p&gt;关于知识付费，最后我还想说说我的一些担忧之处。真正推进社会进步的是那些忘记了自我幸福的孤独探索者，他们的所作所为，不管到了那个时代，都不会被大多数人理解。而现代社会的自由主义传统，包括对个人财产权的绝对保护及契约精神，都帮那些追求星辰大海的人获得了更多的发展空间。但知识付费的兴起，会不会造就一个“美丽新世界”式的牢笼。当越来越多的人认可，只要有了那怕一点认知差距，就能够打开印钞机了，那会不会导致本来自我驱动的人，因为无法发出自己的声音，而被人忽视。用更直觉的说法，以前梵高的悲剧是直到他死了，人们才发现他的伟大，而未来梵高的悲剧，会不会是他活着的时候，自己的画就被各种按自己风格仿制的画所淹没，以至于没有几个人看到原作。&lt;/p&gt;

&lt;p&gt;要避免这个问题，我觉得知识付费还是需要补上其欠缺的第三块，也就是如何活出自己。关于这个问题的答案，我认为是在素材上提供人物传记，在表现形式上要带着主观感情，要像史记或者巨人三传那样，既有细节也有评点。要写的当然不止是大人物，小人物找到自我的故事也是值得讲述。但这些故事，其实比跨学科的通俗化写作还要难，需要写作者自己就是自我驱动者，只有他们才能写好身边同行者的故事。而且人若是找到了自己真正想要的，那就不会在乎当下是不是开心，就算学习技能也不需要有别人为你搭梯子了，所以知识付费注定不会去做这样费力又自掘坟墓的事，而这些个人的经验感受，如果可以算是知识，也可以因为有价无市而称为无价了。&lt;/p&gt;

&lt;p&gt;总结一下，关于知识付费，从知识的内容和用途上，我在这篇小文中分别进行了分类，有些能做，有些不能做，能做的每一类该怎么做，我也提出了自己的建议。但我真正关心的还是那些所谓无价的知识，这才是让一个社会真正富足的根基，那些对未知永无止境的探索热情，那些不达目的不罢休的坚韧不拔，这都是只有自己能给自己的，我们不应该责怪知识付费不能给你提供这些，反而要提供伪造品的无良鸡汤充满警惕。&lt;/p&gt;

&lt;p&gt;原创不易，随喜赞赏&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9397590361445783&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd4icdSNQnx98Zicw7nnP3icPycqI1uRMvRxCezhYm4xQKdbiamvHVmC19a0o7YS03eqTrIL9QJS4wS4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;249&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383313&amp;amp;idx=1&amp;amp;sn=a4279c3f2ec27302a50b42235301be71&amp;amp;chksm=84f3c850b3844146ca1042a90fe56414a72ab9f8b7a7215c10e2db42f336a8f38d77a6bfe582&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;读 当呼吸变为空气 没有活过的生活不值得审视&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=210687560&amp;amp;idx=1&amp;amp;sn=30100912ce5a7ee60a5090c86dc386a9&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;碎片式的知识是无用的吗？应该如何看待？&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

</description>
<pubDate>Mon, 12 Feb 2018 16:02:03 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/Kr20vbrVz0</dc:identifier>
</item>
</channel>
</rss>