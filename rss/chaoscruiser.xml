<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>聊聊机器学习中的那些树</title>
<link>http://www.jintiankansha.me/t/ChsTXw4a6G</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/ChsTXw4a6G</guid>
<description>&lt;p&gt;树模型是机器学习领域内，除了深度学习之外，使用的最为广泛，也是变种特别多的一种模型了，树模型的好处是其很容易理解，且相对不容易过拟合，训练时对资源的消耗也更少。最常用树模型包括决策树，随机森林及XGBoos。而在去年，南大的周志华教授提出了deep forest，一种借鉴深度学习的树模型，树模型还有其他的更为冷门的变种，例如正则化贪心森林和。这篇文章将始简单的介绍下上述的几种树模型的原理，树模型是最容易理解的，请您放心，本文只有一个公式，是关于信息熵的。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7043650793650794&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccogNPcQ5WnNa6AdkIyxtnysA1u4drKZGG7VaNsawbhbH4PeB4PCE4ejFgAevhD7eSfNOgXYo28GQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;504&quot; /&gt;&lt;br /&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9223529411764706&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccogNPcQ5WnNa6AdkIyxtnyMw9nodQCiaXia2rSrHex5ERv0kNCPhdEXBuCofank9rc7ibJTyE71ibribw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;425&quot; /&gt;&lt;/p&gt;
&lt;p&gt;树模型主要用来做分类。最简单的一种叫做决策树，决策树是一个非常接近人类思维的模型。 它形象的说就是一个调查问卷， 把一个最终的决策转化为个若干问题， 每一步得到一个答案， 按照答案的正否来决定下一个问题是什么，如此形成一个树结构， 最后形成一个分类器。 比如经常被举出的例子， 你要买电脑， 要根据很多特征挑选电脑，比如cpu，gpu，硬盘，内存等， 你一定会问你自己一系列问题， 我要买那款cpu，gpu， 硬盘， 内存等，最后做出决策。决策树要做的是把这个过程自动化，最后给我们我们希望的判定结果。&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;table width=&quot;553&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;C&lt;/span&gt;&lt;span&gt;pu&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;G&lt;/span&gt;&lt;span&gt;pu&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;内存&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;决策&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;高&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;中&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;低&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;买&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;高&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;高&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;中&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;买&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;高&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;中&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;低&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;138&quot; valign=&quot;top&quot;&gt;
&lt;p&gt;&lt;span&gt;不买&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在一棵决策树上，其中的节点可以分成根节点（蓝色） 决策节点（红色）和终止节点（绿色），而图中的方框里包含的即是一颗子树，这么看来，树模型是不是特别好理解？树模型的第二个好处是可以方便的探索数据中那些维度更加重要（对做出正确的预测贡献更大），比如上述的买电脑的例子，你会发现对于大多数人来说，CPU的型号最关键。树模型的第三个好处是不怎么需要做数据清洗和补全，还用买电脑的例子，假设你拿到的数据部分中没有告诉GPU的型号，你不必要丢掉这部分数据，进入到相应的子树里，随机的让这条数据进入一个终止节点就好了，这样，你便能够利用缺失的数据了。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5506756756756757&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGia5Hb4EM9I3ice9oPfmgZWacJDocvMicthH9CmR8JVXcnErdmGOKQaoATw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;592&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;谈起树模型，就要说起基尼系数，这个指数最常见的场景是描述贫富差距，但也可以用来指导树模型在那里分叉。假设一颗最简单做二分类问题的决策树，拿到的数据分为两种特征，一个是性别，一个是班级，预测学生们愿不愿打板球，下面的图是两种不同的树模型，用性别来分，10个女生中有2个愿意打球，而20个男生中有13个愿意打球，而用班级分，效果则没有那么好，具体怎么计算了，先从左到右依次计算每个终止节点的基尼系数，(0.2)*(0.2)+(0.8)*(0.8)=0.68  (0.65)*(0.65)+(0.35)*(0.35)=0.55 (0.43)*(0.43)+(0.57)*(0.57)=0.51  (0.56)*(0.56)+(0.44)*(0.44)=0.51，之后对每棵树的基尼系数进行加权平均 ：10/30)*0.68+(20/30)*0.55 = 0.59（按性别分），(14/30)*0.51+(16/30)*0.51 = 0.51（按班级分），因此在该例子中，性别是一个更好的特征。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.35207823960880197&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGia5pP3W6ENljz9Vic1nPvicj6OVaojibwAJl0icom3R1aV8PRz3lQYibqf8gQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;818&quot; /&gt;&lt;/p&gt;


&lt;p&gt;理解决策树的下一个重要的概念是信息增益，信息可以看成是减少了多少系统中的无序，而描述系统的无序程度，可以用信息熵，对于二分类问题，计算公式是  &lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.13761467889908258&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaMD3iadJ2PO53rDHgltclFOtOZazLSmyYdnIiawYlNS6ic4JSISGAP5GLQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;218&quot; /&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccogNPcQ5WnNa6AdkIyxtnyGEibYlXRFjBvOGicGltricVQU2vZutNicZmy1vljdVwu8QL4YrpenrvbWA/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;0.7298245614035088&quot; data-w=&quot;570&quot; /&gt;&lt;/p&gt;
&lt;p&gt;对于每一次树上的分叉，先算下父节点的熵，再计算下子节点的熵的加权平均，就可以计算出决策树中的一个决策节点带来了多少信息增益了。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.17941952506596306&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaavfmHpWrxeTic90ArwS5nhSFhfgPbETKsXAhdVORwwpibfhqSdRNKwzA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;379&quot; /&gt;&lt;/p&gt;
&lt;p&gt;信息熵公式告诉我们的是，我们每次对所有特征都扫描一遍，选择那个让我们的信息增长最大的特征。 依次在这个特征的每个可能取值下，我们在寻找第二个关键特征，列出第二个特征选的可能取值并寻找第三个特征依次类推。 再对每一分支的操作里， 如果我们发现在某个特征组合下的样本均为一类， 则停止分叉的过程。 整个操作过程形似寻找一颗不断分叉的树木， 故名决策树。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  决策树能够处理特征之间的互相影响， 因为特征之间的互相影响，我们并不像简单贝叶斯那样并列的处理这些特征。 例如某个特征可能在某个条件下式好的， 但在另外条件下就是坏的或者没影响。 比如说找对象，你只在对方漂亮的时候才care他学历。 我会根据之前问过的问题的答案来选择下一步问什么样的问题， 如此， 我就能很好的处理特征之间的关联。&lt;/p&gt;

&lt;p&gt;我们把这样的思维步骤写成伪代码， 大概是这样的 :&lt;/p&gt;

&lt;p&gt;训练集D （x1，y1）….&lt;/p&gt;
&lt;p&gt;属性 A attribute  （a1，a2…..）&lt;/p&gt;

&lt;p&gt;函数treegenerate &lt;/p&gt;

&lt;p&gt;1,   生成结点node A（任选一个特征）&lt;/p&gt;
&lt;p&gt;2， 判断D在A中样本是否都属于类型C，是则A标记为C类叶结点， 结束&lt;/p&gt;
&lt;p&gt;3， 判断A为空或D在A样本取值同（x相同而非y），将node 标记为样本多数分类的叶结点（max numbers），结束&lt;/p&gt;

&lt;p&gt;终止条件不成立则: &lt;/p&gt;

&lt;p&gt;从A中选择最优划分属性a*,   &lt;/p&gt;

&lt;p&gt;循环:&lt;/p&gt;
&lt;p&gt;对A*上的每一个值a*做如下处理：&lt;/p&gt;
&lt;p&gt;If a*上的样本为空，则a*为叶节点 （该值下用于判断的样本不足，判定为A*中样本最多的类），&lt;/p&gt;

&lt;p&gt;如果支点上的样本集为D**  &lt;/p&gt;
&lt;p&gt;如果存在某个位置，使得D**为空，&lt;/p&gt;
&lt;p&gt;则A*为叶节点，&lt;/p&gt;
&lt;p&gt;否则，以a*为分支节点，回到第一句     &lt;/p&gt;

&lt;p&gt;接下来我们看一看更为复杂的情况，比如我们拿到的数据特征不是两个，而是一百个，那么问题来了，我们的决策树也要100层那么深吗？如果真的这么深，那么这个模型很容易过拟合的，任何一颗决策树的都应该有终止条件，例如树最深多少层，每个节点最少要有多少样本，最多有多少个终止节点等，这些和终止条件有关的超参数设置决定了模型会不会过拟合。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面我们从一棵树过度到一群数，也就是机器学习中常用的begging，将原来的训练数据集分成多份，每一份分别训练一个分类器，最后再让这些分类器进行投票表决。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7505197505197505&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiabOM7yFPZH3GbWjwBkM8ibW5ib8azNn5W2dwG7LJvpSN7muFI4OwNDwicA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;481&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而随机森林，就是使用begging技巧加持的决策树，是不是很简单？相比于决策树，随机森林的可解释性差一些，另外对于标签为连续的回归问题，随机森林所采取的求多个树的平均数的策略会导致结果的不稳定。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8840864440078585&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfCI3lrovIztO53FwOjLuGia82D0XAAQgA2PvEvYjkyp9rcmFoiciaHrelNBsN3LFPibCA5riaPawRs7cQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1018&quot; /&gt;&lt;/p&gt;

&lt;p&gt;随机森林是将训练数据随机的分成很多类，分别训练很多分类器，再将这些分类器聚合起来，而boosting则不讲训练数据分类，而是将弱分类器聚合起来，下图的上半部分可以看成描述了三个弱分类器，每一个都有分错的，而将他们集合起来，可以得出一个准确率比每一个弱分类器都高的分类模型。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7373188405797102&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaUn7iccltHA6kgCcWg5ia8EIFVQES6NkGpLqP4exSjnBCXW4yNicIVfuIw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;552&quot; /&gt;&lt;/p&gt;
&lt;p&gt;你需要做的是将第一个分类器分类分错的部分交给第二个分类器，再将第二个分类器分错的部分交给第三个分类器，如下图依次所示&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6840148698884758&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGia1VhsROFuFFDbWzJh0A9ed1Oq6L8s11uiaPaZEpovyGicbrblHL7tcSCQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;269&quot; /&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6309523809523809&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaCBGlVicia4SxBjHx93K22OL0bK7Qa4srXicibSUWc3U4x2XLES54ibg7bAg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;252&quot; /&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5919117647058824&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiahP1lhIAUqgicsNoIaCXjqtVjia2PV59pSnlcnoOFMK8xKNRbBWgUFMmQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;272&quot; /&gt;&lt;/p&gt;
&lt;p&gt;最终得到了我们看到的强分类器。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.562962962962963&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaDZWhol4cKibgwHuiaOMAf1BMNXSb7PtwxLzQLvy8NJakr1icVsic3ZyOnw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;270&quot; /&gt;&lt;/p&gt;
&lt;p&gt;总结来看，begging类似于蚁群的智慧，没有一只蚂蚁知道全部的信息，但利用蚂蚁的集合，可以实现集愚成智，而boosting则是三个臭皮匠，胜过诸葛亮。Boost方法包含的非线性变换比较多，表达能力强，而且不需要做复杂的特征工程和特征变换。但不同于随机森林，它是一个串行过程，不好并行化，而且计算复杂度高。&lt;/p&gt;

&lt;p&gt;XGBoost 是 Extreme Gradient Boosting （极端梯度上升）的缩写，是当下最常用的树模型了，是上图描述的Boosting  Tree的一种高效实现，在R，Python等常用的语言下都有对应的包，它把树模型复杂度作为正则项加到优化目标中，从而避免了过于复杂而容易过拟合的模型。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在Boost方法中，每一个被错误分类的样本的权值会增加，以强调最困难的情况，从而使得接下来的模型能集中注意力来处理这些错误的样本，然而这种方法把基于学习器的决策树视为一个黑盒子，没有利用树结构本身。而Regularized Greedy Forest正则化贪心森林(RGF)会在当前森林某一步的结构变化后，依次调整整个森林中所有决策树对应的“叶子”的权重，使损失函数最小化。例如下图我们从原来的森林中发下右下的节点可以分叉，我们做的不止是将分叉后的树加入森林，而且对森林中已有的树中的对应节点也进行类似的分叉操作。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7467994310099573&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaGlISibqLP9vKNz9p8EIUq7Yju4NF5Kic9dEEakiaItNMcvvcMFkjy2CUw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;703&quot; /&gt;&lt;/p&gt;
&lt;p&gt;类似boost，RGF中每个节点的权重也要不断优化，但不同的是，RGF不需要在梯度下降决策树设置所需的树尺寸（tree size）参数（例如，树的数量，最大深度）。总结一下RGF是另一种树集成技术，它类似梯度下降算法，可用于有效建模非线性关系。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面说说去年周志华教授提出深度森林deep forest，也叫做 gcForest，这也是一种基于决策树的集成方法，下图中每一层包括两个随机森林（蓝色）和两个complete random forests（黑色），所谓complete random forest，指的是其中的1000棵决策树的每个节点都随机的选择一个特征作为分裂特征，不断增长整棵树，直到剩余所有样本属于同一类，或样本数量少于10。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4683357879234168&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiavUKYLKOZxt194P0CuJXXxrYg7SNGTt1VJ4ia00yP5eib0pux3cmZCdEA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;679&quot; /&gt;&lt;/p&gt;
&lt;p&gt;至于每一层的输出，也不是传统决策树的一个标签，而是一个向量。图中的每一个森林对每个输入样本都有一个输出，对应建立该决策树时，落在该叶子节点中的样本集合中各个类别的样本所占的比例，如下图所示，将多颗树的结果求平均，得出这一层的输出。为了避免过拟合，每个森林中 class vector 的产生采用了 k 折交叉验证的方法，随机的将k分之一的训练样本丢出去，再对k次训练的结果求平均值。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5438931297709924&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaJqeIwwrhCAD5zFfZCaSTEheUE8KeTZQkc3xoMNWU44QCIibEYnzzLpg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;524&quot; /&gt;&lt;/p&gt;
&lt;p&gt;deep forest还采取了类似卷积神经网络的滑动窗口，如下图所示，原始样本的为400维，定义一个大小为100的滑动窗口，将滑动窗口从原特征上依次滑过，每次移动一步，每次窗口滑动获取的100个特征作为一个新的实例，等效于在400维特征上每相邻100维的特征摘出来作为一个特征实例，得到301个新的特征实例（400 - 300 + 1）。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.36616702355460384&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaX03Gwuy7AyAAIw5vDadng2MYAqXKjuD5ayLm9r76yKe3BicnIO7mPLQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;467&quot; /&gt;&lt;/p&gt;
&lt;p&gt;深度森林的源代码也在Github上有开源版，总结一下，深度森林具有比肩深度神经网络的潜力，例如可以层次化的进行特征提取及使用预训练模型进行迁移学习，相比于深度学习，其具有少得多的超参数，并且对参数设置不太敏感，且在小数据集上，例如手写数字识别中，表现的不比CNN差。深度森林的数据处理流如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3986220472440945&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaT4ibuGlL5PIabyummicHuhBX0SaYIld2gA69u89XYdzO6HyoZYZk8F2Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1016&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12104283054003724&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQoHXW3TLcADEaAG6TUurgQicXObHN6Mjyy4e0LZfU4BfU1ia0QSvCPWpA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;537&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;总结下，树模型作为一个常见的白盒模型，不管数据集的大小，不管是连续的回归问题还是分类问题都适用。它不怎么需要进行数据预处理，例如补全缺失值，去除异常点。树模型可以针对特征按照重要性进行排序，从而构造新的特征或从中选出子集来压缩数据。树模型可以通过统计的方式去验证模型的准确值，判断训练的进展，相比机器学习的模型，需要调整的超参数也更少。但和神经网络一样，树模型也不够健壮，如同图像上只需要改变几个像素点就可以改变模型的结果，树模型中输入数据的微小变化也可能会显著改变模型的结果。树模型也有过拟合的危险，通过剪纸purning，即先让树长的深一些，再去除那些不带来信息增益的分叉，留下那些最初的信息增益为负，但整体的信息增益为正的节点，可以组织树模型的过拟合。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.4889267461669506&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfCI3lrovIztO53FwOjLuGiaghgFicuGJBzgr1yZCIrshhXpnP6mGY0eG38fP57TEocSkiaxcbtA6NOw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;587&quot; /&gt;&lt;/p&gt;








</description>
<pubDate>Sun, 25 Feb 2018 05:49:07 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/ChsTXw4a6G</dc:identifier>
</item>
<item>
<title>想当然与爱做梦-从《梦幻之境》这本书说起</title>
<link>http://www.jintiankansha.me/t/IiXDLPFP0u</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/IiXDLPFP0u</guid>
<description>&lt;p&gt;春节阶段读了一本书，令我印象极深。这本书目前还没有翻译版，英文名叫做《&lt;span&gt;Fantasyland: How America Went Haywire: A 500-Year History》，中文名翻译成《梦幻之境》，这是一本美国人写的，批判美国人脑残的书。类似的作品还有大前研一的《低智商社会》以及武志红的《巨婴国》，这两本同样也是我很喜欢的书。三本书说的是在不同文化下出现的类似的问题，将这三本书混杂着阅读，让我们由不得在深夜里思考，沉溺于白日梦，这究竟是不是人类的通病，我们当下的中国人又能否幸免了？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.5075528700906344&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcf8Lw8osJWEV0UXpYFWmuwmZXnDvnleL4jYe5v3St88mRD9wU9q5emvFS1CSw36zdEx1eG8Rtk7dw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;331&quot; /&gt;&lt;/p&gt;
&lt;p&gt;春节回家，听到亲戚们讲着自己的发财梦，有的是P2P，还有的是比特币，他们口中说着每年20%的稳定收益，却不考虑这些收益来自那里。如同次贷危机爆发前的美国人，都认为房价会只涨不跌。这正是《梦幻之境》要反映的问题，在哈利波特第一部中，有一面可以让人看到内心最深处的欲望的镜子。看过原著的人都知道，那面镜子不能提供我们真正的知识或者真相，很多本来很聪明的人在镜子里虚度光阴，甚至因此而发疯。人不能活在梦里，不要依赖梦想而放弃生活。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6054545454545455&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcf8Lw8osJWEV0UXpYFWmuwmvwd4uBztqicBOW2BPQ9DDDTNEQqbTwicicmZSwGEoQ6B0rHulBiaGPNhnw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;550&quot; /&gt;&lt;/p&gt;

&lt;p&gt;但当今的科技发展，却使得这样的魔镜变得越来越多。推荐系统记录着你在网上的一举一动，只为了告诉你需要买什么东西，需要看那些奶头乐的短讯；虚拟现实还有增强现实，让你看到了你本来应该披荆斩棘后才可以体验的风景；而游戏设计者则通晓上瘾的心理学，他们群策群力推出的异世界，总是要比真实的世界更具有吸引力；而你的照相机默认会让你你的图像变得更美，还会根据你的喜好去投其所好的迎合。手机仿佛成了红楼梦中的风月宝鉴，正着看是让人生厌的白骨，如同很多人无趣的工作时间，而反着看则是红粉，像级了休息时的自我放纵和沉溺幻想。&lt;/p&gt;

&lt;p&gt;之前曾读过一本书，和这个主题有关，（参考&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382826&amp;amp;idx=1&amp;amp;sn=4f32d7d040bc5d0413e54f5b3f567153&amp;amp;chksm=84f3ca6bb384437d8e695a071a91757eb23ab667e1bec344544f4e0f11c4c53ce2ecfa9d88cb&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;该怎么做白日梦 Rethink about positive thinking 读书笔记&lt;/a&gt;），其中写道“人脑实际上难以分清楚真实与幻想，所以当你幻想你拿到了冠军之后，你就会放松，就会喜悦，你下意识会认为已经成功了，所以没必要那么努力了。”要想搞清楚什么是真实的，不止需要应用理性的思辨，更需要一个人首先不逃避自由，一个人能够，并且应该让自己做到的，不是感到安全，而是能够接纳不安全的现实。&lt;/p&gt;

&lt;p&gt;我这么说，并不是觉得虚幻的东西毫无价值，我喜欢读虚构性的小说和电影，尤其是幻想文学作品，不管是《基地》系列还是《魔戒》系列，也能从这里得到有关生活本身的启示。然而沉迷幻想，是指无法从幻想中走出来，进入现实的生活。当今很多的网文爽文，是你不管有多少空闲时间，都能够填满的，我在地铁上经常看到很多人拿着手机，在上下班的路上看书，但扫几眼他们的看的书的标题，就知道他们在看着什么，第3××章，手指快速的刷着屏幕，没刷几次，又到了下一章。即使是红楼梦这样的巨著，也就百二十回，这些连快餐文学都算不上的作品，只能是让人沉迷的幻想。&lt;/p&gt;

&lt;p&gt;沉迷幻想本身是人畜无害的，但正如弗洛姆所说，&lt;em&gt;现代人误以为自己知道自己想要什么，而实际上他所想要的是别人期望他要的东西。&lt;/em&gt;每一个幻想背后都有着一双看不见的手在拉着绳索。不管是巨婴症还是低智商，其背后的根源都在与沉溺于幻境，一旦进入真实世界，就如同洞穴中人第一次见到阳光，短暂的不适应之后，不愿背负自由的重担，便只能以盲人摸象的姿势坐井观天。&lt;/p&gt;

&lt;p&gt;我曾经以为未富先老是中国未来面临的最大风险，然而一想到以后生物技术的进步会使老人不意味着衰弱，也可以为社会创造价值，就不那么担忧了。然而现在看来，中国面临的最大风险是年轻人的未富先避，避指的是闭上眼睛，逃避自由。美国人躲进幻想，但他们毕竟有美元霸权要全世界为他们买单。但随着这些年西风东渐，西方的价值观被曲解，例如不顾实际的追求梦想，提前享受物质成果等在中国社会成为主流思想。&lt;/p&gt;

&lt;p&gt;这让我想起最近看的一篇文章，“现在不是寒门难出贵子，而是穷家富养出太多败家子”。文中写道“我穷我有理、我弱我有理”的心态，让很多寒门子弟多了很多本被贴在富二代身上的标签，缺少责任心是表象，本质是不能够面对现实。再联想到AI带来的失业潮让有些人变得自暴自弃，等着AI会最终赡养人类，这同样是在逃避思考与行动的自由。&lt;/p&gt;

&lt;p&gt;回到《梦幻之境》这本书，这本书追根溯源，从历史的过往中，讲述为什么美国的主流文化会沉迷于阴谋论伪科学这样的幻境。其实每个民族历史上沉迷幻想的都是长态，人天生好逸恶劳，不愿接受复杂的解释。1919年，遗传学家摩尔根曾就遗传规律写道：“人类终将会了解自然的本质，而所谓的神秘莫测不过是场错觉罢了”，随着数据科学的发展，当代的我们正在把摩尔根的结论从了解自然扩展至了解人性。&lt;/p&gt;

&lt;p&gt;以复杂应对复杂，AI带给人类的那些可见的礼物，什么无人驾驶的汽车啊工业机器人啊，都不过是冰山露出的一小部分。AI给人类的影响最深的是新的知识产生方式，人类可以用全新的方式进行生物科学，社会科学的研究，使得人们终于可以第一次认清楚的定量化的搞清楚是什么样的物质基础决定了自己的爱与恨。而为了更好应用这些知识，我们第一要叫醒那些沉睡在幻想中的人，第二要告诉更多的人新的知识是如何借由AI得到的。而这正是我一直心心念想要做的，也是我为什么在业余时间坚持码字的初心。&lt;/p&gt;


&lt;p&gt;原创不易，随喜赞赏&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9397590361445783&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd4icdSNQnx98Zicw7nnP3icPycqI1uRMvRxCezhYm4xQKdbiamvHVmC19a0o7YS03eqTrIL9QJS4wS4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;249&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383358&amp;amp;idx=1&amp;amp;sn=c2191785a58ad5c4f3d8fe3e099bd954&amp;amp;chksm=84f3c87fb3844169d257f6f38717972c143e9849418baba7c404c56dee28679d97040ff7e428&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;Soonish 读书笔记-什么样的科技能改变世界&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383323&amp;amp;idx=1&amp;amp;sn=1b954794ff6ea866a19d420a517ccab3&amp;amp;chksm=84f3c85ab384414c1df772db5510441e30a3c4983923280528e6296b7fc82603d7e23ec8c12a&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;黑镜书单-列一列那些反思互联网和人工智能的30本警世之书&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;






</description>
<pubDate>Fri, 23 Feb 2018 01:24:33 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/IiXDLPFP0u</dc:identifier>
</item>
<item>
<title>当AI遇到狗年，说一说用深度学习识别不同品种的狗子</title>
<link>http://www.jintiankansha.me/t/vb1N26VP3J</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/vb1N26VP3J</guid>
<description>&lt;p&gt;新的一年即将到来了，这里先祝各位新年快乐，狗年财运旺旺的 。在新年的最后一篇推送中，就简单的介绍一下和狗相关的一项黑科技，狗练识别。&lt;/p&gt;

&lt;p&gt;狗是已知的，人类最早驯化的动物了，早在一万五千多年之前，还处在农业时代之前的原始人类就靠着几根骨头骗到了贪吃的灰狼，又等到了农业时代，人们发现狗真的是十项全能的好帮手，于是开始了对狗的定向演化，于是就有了许多看起来差异巨大的不同品种的狗，下图所展示的只是一些常见的品种，你认出那些你熟悉的品种了吗？&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.0866666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccc3aFpbgqOYVraGSE6uA3zic4g3eb9LXh4nibq5oaG2uEUlZL2dyPV2Al8WgL0X60KWIUbkq9aSPIA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;300&quot; /&gt;&lt;/p&gt;
&lt;p&gt;而根据不同品种的狗狗的照片，按照品种进行分类，在机器视觉中属于Fine-grained classification，也就是根据图像的细节进行分类，类似的问题还包括根据植物的照片判断是那种花等，你也许会说，这样的问题不应该很简单吗，用深度学习就行了，一层不行就再加一层。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7015625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccc3aFpbgqOYVraGSE6uA3zRoUVjnZqKkSMEFJ9eS1QDxsFibKMItGEB0t7RjhyYzsyKWmWXUvZOVQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;且慢，让我们先看看这个问题本身有那些本质的困难之处。首先是不同品种的狗都很相似，比如下图的三种狗，若你不是狗专家，你能够分清楚吗？&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.46255506607929514&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccc3aFpbgqOYVraGSE6uA3ztRNQLHmeZZWuJFKhrZ0kYAGWSicyuyLd5zdPXqRH4pMSVrUL9e1PkWg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;454&quot; /&gt;&lt;/p&gt;
&lt;p&gt;第二个问题是即使是同一种狗，也会差距很大，例如下图的三种狗，竟然是一家人，你说这叫AI头大不头大。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4409090909090909&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccc3aFpbgqOYVraGSE6uA3zkanZmTMGFTOT4iahg2Rqp9x5gTKrdGSNibdhFgq9yZA9Gucu5J34jyMg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;440&quot; /&gt;&lt;/p&gt;
&lt;p&gt;更要命的是不同的狗狗都有各自独特的pose，而且他们所在的图片的区域，图片背后的背景是草地还是森林都不同。下图展示了三只不同姿势的狗狗。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4967462039045553&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFTIodib4zpias3tZAFxlic0Kf43LVRtaJGClia8V8qFeHzB9QXjhXY7mMvw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;461&quot; /&gt;&lt;/p&gt;
&lt;p&gt;正是由于有这三个问题，要做好狗脸识别，并不是一件简单的事。所以说，&lt;strong&gt;使用深度学习去解决具体问题，需要先调研清楚这个问题的背景和障碍，才能够便于设计下一步具体优化方法。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;该怎么做了？第一种思路是数据增强，也就是用&lt;strong&gt;随机应对随机&lt;/strong&gt;。既然狗子的位置在照片中不固定，那就将原始的图片随机的裁剪一下，旋转一下，将图像的颜色做一些微调，总之就是想象一个熊孩子打开ps修改了每张狗子的照片，给你留下了一堆看起来和原始的训练数据差不多的照片作为新的训练集。下面给出了随机剪裁之后得到的狗的照片示例。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;114&quot; data-backw=&quot;307&quot; data-ratio=&quot;0.3713355048859935&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFHl5vTARl66Vj4eLsWtzbIiaeIibJ0SehhFxVUiagWFWdJQQTTYDg7cia9w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;307&quot; /&gt;&lt;/p&gt;
&lt;p&gt;而另一种方式，可以看成是&lt;strong&gt;用有序来应对随机&lt;/strong&gt;，也就是先通过识别出狗子所在的区域，再将这个区域拉伸成一样大小的图片，来去除背景的干扰。例如下图所示，展示了经过了背景去除和拉伸前后的狗子的照片，可以看到经过处理后的照片，只剩下了我们关心的狗子的信息。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;102&quot; data-backw=&quot;356&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmF4JuIFNAyOH7amAtVib2KIfWv27LiawicpLg7xMDc5AibxUcia7VcYVeS5dw/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;356&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;96.97491039426524&quot; data-ratio=&quot;0.2696629213483146&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFRCyHkfeePDYpfswZBQFiaWWUBudsjyR5TyibbUgqkf3ONtCnUYCRAmqA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;356&quot; /&gt;&lt;/p&gt;

&lt;p&gt;接下来的问题是狗子的照片不够多，斯坦福大学针对狗的品种识别，搞出了一个数据库，里面有一百多种狗，一共一万多张照片，但若是指望这些图片就能够训练出一个靠谱的深度神经网络，那效果多半不好。&lt;strong&gt;数据增强虽然能够改善预测的准确性，但其上限不高，毕竟原始的信息就那么多。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.0225225225225225&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFyXnqUSXNdQVNS815GEIBRkiaau5UEFEbYZ0ichqQRibh3LXDibkJg3NWXA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;222&quot; /&gt;&lt;/p&gt;
&lt;p&gt;若是你自己做不到，不妨站在巨人的肩膀上。迁移学习正是这样，深度学习的好处是模型不再是铁板一块，而是可以拆解成一层一层的，不用花一分钱，你可以拿大牛们训练好的网络，将其用做自己的用途。关于迁移学习，曾经写过一篇名叫&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383309&amp;amp;idx=1&amp;amp;sn=f0ee86dc43994673f2b818cb2c2efe4b&amp;amp;chksm=84f3c84cb384415a67b8469578df8268a490f12355d4213c126d767227a0da52a77297b47be4&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;白话迁移学习&lt;/a&gt;的小文，感兴趣的可以点击深入了解。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;而迁移学习迁移的是什么样的神经网络，自然是深度学习中最出名的卷积神经网络，关于这个话题，可以参考&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383212&amp;amp;idx=1&amp;amp;sn=e6dbbda2acc5984c8d06e24ec9c84d09&amp;amp;chksm=84f3cbedb38442fb58f0aea635821fcf4ba3edaacef4685716c7eadb6191197ebfa70a6bf14b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;你所不能不知道的CNN&lt;/a&gt;和&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381959&amp;amp;idx=1&amp;amp;sn=1b920dd476849d88b67a2ef1cf3ed8fc&amp;amp;chksm=84f3ce86b3844790627d2f15256aff0753be1f0b0623da64aaa7357d73e8ed14c415061acb27&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;用CNN来识别鸟or飞机的图像&lt;/a&gt;，这里就不再重复了。&lt;/p&gt;

&lt;p&gt;但迁移学习并不意味着什么都不需要来做，你需要利用成熟的网络提取出的高级特征，用他们来进行预测，但是要注意的是，你预测得出的结果并不是一个确定的狗的品种，而是这个图属于哪个品种的概率，而这又是怎么得出来的，靠的是一个名叫softMax的激活函数，这可是深度学习中最出名的两个激活函数了，soft是保证输出的结果是符合概率分布，也就是不会出现概率为120%的情况，而max是让错的更错，从而提高学习的效率。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7857142857142857&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFwMvjt0icic9QxPRjqIQZC2auUSQ2GjnGNCtBia2Zu2sbs2QHodQ0oInhw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;280&quot; /&gt;&lt;/p&gt;
&lt;p&gt;最后说一说这个例子的实现难度，类似Minst数据集的手写数字识别，宠物狗的品种分类，也是一个相对容易上手的例子，你不需要昂贵的显卡，就普通的个人电脑，就可以基于已有的图像识别网络，例如谷歌的Inception，来搭建一个属于你的狗脸识别程序。除了用来展示，这个例子还可以锻炼你诊断网络的能力，训练的太慢，不妨换换更快学习率，训练的结果起伏太大，还可以再换换更慢的学习率啦。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;开个玩笑，虽然学习率是深度学习中很重要的一个参数，但不是万能的，这个例子的好处是让你能够直观的看到你训练出神经网络有那些不足，又为什么会犯错，例如下图，如果是将图片变成黑白的，神经网络是不会犯错的，但一旦加上了色彩，你就会发现神经网络没有将黄色那只分成拉布拉多犬，这时你就能发现，是你的神经网络过拟合了，网络中的一个神经元学到的速记口诀，但凡黄色的都不是拉布拉多，这时不管你怎么做数据增强，对数据又拉伸又旋转，都无法教会这个神经元忘掉这个口诀，这也是我为什么上文说数据增强的效果是有上限的。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6631799163179917&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFulepXU1KeThKxTzg4wmaMqIE3QK4kQgyL7G5pa4Cp9MFRrWgL6bSmg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;478&quot; /&gt;&lt;/p&gt;
&lt;p&gt;而这时如果你能用dropout机制，时不时的将这个背小抄的神经元踢下线，那么你的神经网络就能够有机会学到拉布拉多真正的特征，这也就解释了为什么dropout机制是一种极为高效的防止模型过拟合的方法。&lt;/p&gt;

&lt;p&gt;卷积神经网络的另一个特点是能够打破所谓的黑箱，让你看看&lt;strong&gt;网络内部特征是如何一步步被抽象出来的&lt;/strong&gt;，下图依次分别展示了卷积神经网络的输入图像和第一层的输出结果，可以看出神经网络第一层的卷积核提取出的特征分别是什么，又对图像进行了怎样的抽象。（虽然小编表示我就算看出神经网络做了什么，也没法说出来，不过这反而说明了网络本身能做一些人无法明确言说的任务，而这正是深度神经网络最革命性的创新）&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9905660377358491&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFF991GMibUUibJloen3eW88NLok7kYKpAMkPPa37WPaf8iaG5pB93uhwHw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;424&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9891067538126361&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmF2GxlEzTSS98pb1ch2DzibA7k89GJqeTac5Yadnia63sIaNHTJibfAEk8Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;459&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在这篇小文的结尾，放上一章狗狗们的全家福照片，愿各位不管是什么汪，新的一年的事事兴旺，合家安康。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4498186215235792&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFibAHmXwfTOALb9OVSK0eBhkde9UvecicrSXBAIHMDZdVM0nD1jy9VV4A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;827&quot; /&gt;&lt;/p&gt;


&lt;p&gt;&lt;span&gt;参考资料&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1）Using Convolutional Neural Networks to Classify Dog Breeds  Hsu, David&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2）Dog Breed Identification  Whitney LaRow ，Brian Mittl， Vijay Singh&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3）Automatic Dog Breed Identification     Dylan Rhodes&lt;/span&gt;&lt;/p&gt;








</description>
<pubDate>Wed, 14 Feb 2018 12:31:46 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/vb1N26VP3J</dc:identifier>
</item>
</channel>
</rss>