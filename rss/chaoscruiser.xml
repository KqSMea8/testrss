<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>从Q-learning的小游戏看阿尔法元技术</title>
<link>http://www.jintiankansha.me/t/Pzd7krtfkL</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/Pzd7krtfkL</guid>
<description>&lt;p&gt;机器学习的三个框架监督学习 ， 无监督学习， 强化学习， 唯强化学习最难理解 。 如果说监督学习是感知和预测（Perception &amp;amp; Prediction)， 强化学习就是决策，它赋予机器以动机和计划的能力，同时你也可以它身上学习你改怎么决策！&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGytCcNt1I0LPKhPnqPGJjesvSmhFofwgHwz91R8sBVHBcPG8dMOT5yw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;853&quot; /&gt;关键时刻，知道前面有老虎还是会被吃，你得会跑！ 后者就是强化学习
&lt;p&gt;阿尔法元用到的框架是深度强化学习，在那里深度学习其实只是起到提取特征的作用，而背后的核心框架正是强化学习的一套基本功。&lt;/p&gt;

&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.8583333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGEkv9kPiaOXFyTRAXibln4Q066UNc5D8wOvyReEicdM7hfpiaiabZgRXy5ng/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;802&quot; /&gt;任何强化学习都包含几个基本部分，状态， 行为， 和奖励
&lt;p&gt;铁哥在这里通过一个简单的例子给大家讲一下这个通向强人工智能最有可能的路径。&lt;/p&gt;
&lt;p&gt;首先， 强化学习比监督学习难很多， 因为监督学习， 随时有一个老师在那里告诉你怎么是对，怎么是错，错了多少。 而强化学习， 只有游戏结束曲终人散的时候， 得到一个奖励， 或者惩罚， 然而， 你怎么都回想不清楚你究竟哪一步对， 哪一步错误， 可谓是急死老师傅。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGibwlNbS6H2cCs9HQNKn9O9E1vicptvQLb6aXKGWApvVfk4swpuEOceOw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;2000&quot; /&gt;没有老师了这回！
&lt;p&gt;而且， 你更加直接的面对不确定性， 这种残酷的感觉，就是你需要预测的是n多步以后的时候， 那不确定性累加起来， 可以哭倒长城了。&lt;/p&gt;
&lt;p&gt;不过， 这才是真实的人生， 走出的步子永远不知道上帝给你什么 ，而且， 不是不报，时候未到， 虽然你不知道什么时候到，只能哀叹一声， 苍天绕过谁啊。&lt;/p&gt;
&lt;p&gt;这个问题， 术语叫做sparse， 也就是说可以用于当下决策的有关最终奖励的信号太弱太弱了。 如何解决这个问题？ 有个词，叫“趋势” ， 你需要从历史的经验里， 得到一个对未来趋势的判断，然后赋予此刻每个可能的行动一个值， 因为总有某个行动， 比其它的更容易导致良好的趋势， 哪怕优势只是一点点， 我们这样一步步走出， 总会得到一个帕累托最优吧 。 然后呢 ，假设你有无穷的生命， 你通过每一轮游戏的经历得到的信息不停迭代这个趋势。&lt;/p&gt;
&lt;p&gt;这就是强化学习的最简单的方法 所谓Q 学习（Q- learning）方法。 它自然的包含了两个部分， 一个是通过经历不停的强化对某个状态行为下趋势的认知 ，另一个则是根据这个趋势，每步走出当下最优的选择（这不就是理性人决策吗！）。&lt;/p&gt;
&lt;p&gt;我们再学究一点， 探究一下这个趋势的数学含义， 其实数学家的角度， 所谓的Q,  就是在当下选择之下未来收益的期望， 瞧，一句话把趋势是什么， 问题里的不确定性都囊括了。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.17222222222222222&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGGqwI7nk462eOhlIsc1mvRMRfevfqtVkOqNRxq7NJjBRwrasm33dic2Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1041&quot; /&gt;&lt;p&gt;我们再来从人类认知角度看这个问题， 人脑学习如何决策的原理其实是类似的， 你决策的依据， 也是未来的趋势。&lt;/p&gt;
&lt;p&gt;一向自由洒脱的小明问了， 我哪有那么理性？ 我平时的生活从没有思考过什么趋势，更不要说什么未来收益的期望， 我就是根据我的性情决策的。&lt;/p&gt;
&lt;p&gt;只是小明没有意识到。 虽然人都生活在情绪而不是理性之中，但情绪亦是一种一种朴拙的生存理性，正是某种藏在你基因（祖先的经历）或者过往经历中的Q - value，使你你冥冥中觉得大势不妙而产生了恐惧，反之则是欣喜。  这里印证了当下的某种经历与你祖先的某一次窘境的一致性，比如你祖宗在水边看到的一条蛇， 引起了你对水杯中蛇影的恐惧。&lt;/p&gt;
&lt;p&gt;你看到美女的兴奋是因为它预示了一种未来子孙满堂的丰盛（基因所致）， 这来自于你赶地铁的焦虑是它预示了一系列的停薪考试挂科的衰微（经历所致）。 情绪，使你在一秒钟内能够决策。 那些缺少情绪而只有理性的人， 通常无法在日常生活中正常决策。 未来虽然是不确定的， 但是你的喜怒哀乐还是让你做出大致适合生存的选择。 甚至更神奇的，你有时候不必经历后果， 就可以通过你的情绪反应学习，比如某次开车太快差点被撞，这种恐惧和后怕可以让你学到开慢点，而不是等撞死了再说（TD learning）。&lt;/p&gt;
&lt;p&gt;这个Q - learning  方法 ， 就是赋予机器以“情绪” 智能， 来克服这个缺乏“ 监督信号”问题的。&lt;/p&gt;
&lt;p&gt;你要给一个行为所能导致的所有未来趋势赋值，你就要面对一个问题， 即使此刻你做了一个决定， 你的行动导致的下一个状态你又需要决策，每个决策又导致不同的结果， 这样始终都是处于不确定的状态。&lt;/p&gt;
&lt;p&gt;为了简化这个问题， Q - learning做了一个大刀阔斧的假设， 就是我现在做个决策， 后面的决定也都是按照某个最优的法则走的（选择当时的可选行动力最优的）， 这样我忽略那些基于此刻行动导致的下个状态下那些不太好的选择 ，而是只考虑那时候最优选择的回报，这也就给出了一个很自洽的解。 也就是说， 此处行动导致的趋势， 就是未来可能回到的最大值，瞧， 一句话就把这个不确定难题搞定了， 我们基于此把趋势这个模糊的东西量化。&lt;/p&gt;
&lt;p&gt;另外一个难点是什么呢？  下一步就得到的回报没有人会把它和未来十年的回报等同视之， 也就是说，你要以某种方式对不同时刻到来的奖励加权， 这有点像银行的贴现，未来那虚无缥缈的金银宝藏，还是好好乘个贴现因子，说不定乘出来还不如此刻的一个蛋糕。   这个原理被藏在一个叫bellaman equation的方程里。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.2833333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGogAod8mBhGKy18KMutjiaKT4MbAw6pmMgSNIux7764LDkz16p57xqicQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;926&quot; /&gt;一个用迭代方法写出的数学表达， 不能再优美。
&lt;p&gt;这两点放在一块，我们就可以完美的定义Q value，它就是在理性人（没步下做最优选择）假设下每一步的收益按贴现率加和来的未来收益总期望。这么绕口，其实我还没说下半句， 那就是在此刻某个行为下的期望，太累了，分开讲。 这整个定义，就是充满智慧的Q - value。&lt;/p&gt;
&lt;p&gt;然后Q - learning呢？ 刚说过了， 学习，是通过既往的经验，  Q - learning的过程就是根据行为做出后得到的真实境遇（可以是奖励或是惩罚，也可以是新的位置的Q值）来更新这个期望的过程， that‘s  all。  当Q 值被更新， 你的行为也就被更新了 ，因为我每一次的行为无非选择最大的Q值。&lt;/p&gt;
&lt;p&gt;这里， 我们做一个更简单的例子， 把强化学习变成一个简单走迷宫的游戏，  在最经典的Q-learning下看看效果。 不要小看它简单， 小小的迷宫可以玩出深度学习的众多花样。&lt;/p&gt;
&lt;p&gt;游戏的框架被称为“冰湖”  ： 北方之王（king of north）统治的国度里有一个巨大的冰湖，冰湖里隐藏着一些深不可测的冰洞， 冰湖的某个角落藏着关系帝国龙脉的宝藏， 一群流亡士兵要在黑夜里穿越冰湖找到保藏， 而且不要掉到洞里。 而且冰湖上时刻管着凌琳的风， 你跑路的风向以某个概率被风吹偏。。士兵要决策自己跑路的方向，最终更快更安全的拿到宝藏。&lt;/p&gt;
&lt;p&gt;这个游戏的版本一是你手里有一个GPS， 士兵知道坐标，但没有记忆， 士兵手里有一个巨大的表格， 记下每个位置下不同方向的。Q值，士兵们一个接个去寻找， 通过Q叠代， 让后面的士兵更快更安全的走到地方。&lt;/p&gt;
&lt;p&gt;此处我们得到一个典型的马尔科夫决策过程，每一步的状态（位置）都可以决定你的全部未来， 你唯一需要迭代的是那个往不同方向迈一步可能得到的后果。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.2569444444444444&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGQJbJWLOkfws4H5OvicnpF55dYIHkpdPlgicYLzib0y9BibfteY7icwJ5T1A/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1118&quot; /&gt;经过几轮迭代以后，我们的这个小小冰湖世界的所有位置都被填进了这个 神秘的Q-value
&lt;p&gt;那么我说好，这个黑色的冰湖， 绝地大多说地方一个信号都没有， 你都收不到远方的奖励感知信号， 哪来的学习呢？ 那么好了 ， 我们这个Q - leanring的表格， 就是关键。 因为你第一步迭代的一定是离你和目标最近的那个点，这个点率先得到更高的Q值，当这个点被更新了，下一次走到它附近的agent就很可能会迭代它周边的这个Q值， 如此以往，Q值会被逐步的迭代出来，如同一个从不幸到幸福的梯度场， agent无论何时开始出发，都可以很快的循着这个梯度场（找到相邻位置里最幸福的那个位置）达到目标， 并且越来越聪明的躲开冰洞，无论它离目标和危险有多远， 无论这些士兵本身对冰湖并无概念，也对地理毫无了解， 他们的表现却让你觉得他们对远方的局势了如执掌。&lt;/p&gt;
&lt;p&gt;这一点对于这样一个状态数量非常有限， 当下的行为仅需要考虑当下状态 ，而且奖励非常确定的时候能是可以求解的。 上述这个框架又被称为马尔科夫决策框架，“马尔科夫”无论出现在哪里，都是描述当下状态可以完全决定下一刻状态的某种时间“离散”的游戏（典型的如围棋，象棋）&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.7991266375545851&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHG9dicEQGaLuCP7jiavia4o4icyu6zicu9jgMOsFpWD82bZRiaxNAM7ibib90sUw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;687&quot; width=&quot;687&quot; /&gt;
&lt;p&gt;一旦上面说的任何一条被打破，游戏就没有那么简单了。 例如：&lt;/p&gt;

&lt;p&gt;1，  当我的冰湖变得非常巨大， 可能达到的状态达到无穷多， 这时候你要计算一个通往“幸福”的梯度场变得没那么容易。 所以， 我用我深度学习的惯有招数， 我用一个带有先验信息的神经网络来假设这个梯度场的结构再学习！  此处可以有deep Q learning。&lt;/p&gt;
&lt;p&gt;2，  既然状态多不好搞，能不能更直接一点， 不学习Q值， 而是直接学习行为？ 于是我们有了policy learning。比如著名的policy-gradient  ，  actor-critic这些deepmind的家常菜。&lt;/p&gt;
&lt;p&gt;3，  士兵手里不再具有GPS定位信息， 而是只知道相邻位置的状态， 这时候他只能够像盲人摸象一样四处摸。  这时候你有一个非全局的马尔科夫决策过程。 因为你每一步的信息已经不足以让你做出智慧的决定。  那怎么办？  当下信息不足，我就用历史信息来补充， 来个循环神经网络给我， 再加一个能够自己生成定位的神经网络！  此处请看deepmind最新文章Vector-based navigation using grid-like representations in artificial agents。&lt;/p&gt;
&lt;p&gt;由此我们从一个简单的自己对世界环境毫无知觉的agent，一步步迈向了自己心中具有世界模型， 可以通过自己的学习生成世界模型的agent， 认知诞生了， 知识诞生了，而一切都提高了我们强化学习创造的虚拟生命的学习效率和生存可能。&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5166666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGQjXYib1oQaoUze8Ke5OuOicwf0OAaCWKBZtIGPRsR6yia31WGoC3kD2lg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1485&quot; /&gt;&lt;p&gt;有了这些更新的方法， 我们就从最简单的游戏迈向了围棋， 迈向了星际争霸这些你真正爱玩的游戏， 最终迈向真实的生活。&lt;/p&gt;

更多阅读&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382963&amp;amp;idx=1&amp;amp;sn=53c1f03208ca7a41285566b9bf8aa83d&amp;amp;chksm=84f3caf2b38443e40428d245046814ac4ca8883c4403a88f68980b86e57b4c754759edf6eece&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;铁哥谈AI： 浅析阿尔法元之元&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381615&amp;amp;idx=1&amp;amp;sn=04c5cc6cda0f7bd7032c1beb7aeea64d&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;阿尔法狗是怎么用机器学习做决策的&lt;/a&gt;&lt;br /&gt;&lt;span&gt;作者铁哥的微信号：ironcruiser。再今年7月，铁哥会推出深度强化学习的在线课程，敬请期待。&lt;/span&gt;&lt;p&gt;下面是广告时间，总时长超过16个小时，全面的涵盖常见深度学习模型架构，优化方法。本文提到的正则化技术都有详细的逐行代码详解。超值的良心价。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;9.821333333333333&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGGB3P7tjOcxTibLZ3J6CzkuzaMRoB3nUxdiavuQlnR6Rl6YDMUIU8t33g/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;750&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

</description>
<pubDate>Sun, 13 May 2018 18:42:21 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/Pzd7krtfkL</dc:identifier>
</item>
<item>
<title>聊聊神经网络中的正则化</title>
<link>http://www.jintiankansha.me/t/g4ROEb9VpW</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/g4ROEb9VpW</guid>
<description>&lt;p&gt;如何减少泛化误差，是机器学习的核心问题。这篇文章首先将从六个角度去探讨什么是泛化能力，接着讲述有那些提高泛化能力的方法，这些正则化方法可以怎样进行分类，最后会通过讲述一篇论文，来说明目前的正则化方法在解释深度神经泛化能力方面的问题。本文假设读者对深度学习具有基本的了解，清楚卷积神经网络的前向传播和训练过程。如何提高泛化能力是一个面试中常见的问题，由于这个问题有太多的答案，如何有条理的组织自己的回答，本文第二部分可以供参考。&lt;/p&gt;

&lt;p&gt;泛化能力最直接的定义是&lt;strong&gt;训练数据和真实数据间的差异&lt;/strong&gt;，训练模型的目地是要模型在完全陌生的数据上进行测试的。因此在进行交叉验证的时候，要保证测试集和训练集有相同的数据分布。而当测试集和训练集本身的分布就不一致的时候，则可以使用将训练集和测试集混合的Adversarial Validation来应对。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6925675675675675&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfAxrvTibyDf3RrJI7leVcj6QIyCP0pIibCIsKxt60cM5LpV4mxRVXvMrZOEqTMxCyPQ4DobpCXrFYg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;592&quot; /&gt;&lt;/p&gt;

&lt;p&gt;泛化能力还可以看成模型的&lt;strong&gt;稀疏&lt;/strong&gt;性。正如奥斯姆的剪刀指出的，面对不同的解释时，最简单的解释是最好的解释。在机器学习中，具有泛化能力的模型中应该有很多参数是接近0的。而在深度学习中，则是待优化的矩阵应该对稀疏性有偏好性。&lt;/p&gt;

&lt;p&gt;泛化能力的第三种解释是&lt;strong&gt;生成模型中的高保真能力&lt;/strong&gt;。具有泛化能力的模型应在其每个抽象层次具有重构特征的能力。第四种解释是模型能够有&lt;strong&gt;效的忽视琐碎的特征&lt;/strong&gt;，或者说在无关的变化下都能找到相同的特征。比如CNN就能够忽视其关注特征所在的位置，而capsule网络则能够忽略特征是否旋转。去除掉越来越多的无关特征后，才能保证模型对真正在意的特征的准确生成能力。这和上述的第三点是相辅相成的。&lt;/p&gt;

&lt;p&gt;泛化能力还可以看成模型的&lt;strong&gt;信息压缩&lt;/strong&gt;能力。这里涉及到解释为什么深度学习有效的一种假说，信息瓶颈（information bottleneck），说的是一个模型对特征进行压缩（降维）的能力越强，其就越更大的可能性做出准确的分类。信息压缩能力可以概括上述的四种关于泛化能力的解释，稀疏的模型因其结构而完成了信息的压缩，生成能力强，泛化误差低的模型因信息压缩而可能，而忽略无关特征是信息压缩的副产品。&lt;/p&gt;

&lt;p&gt;理解泛化能力的最后一种角度是&lt;strong&gt;风险最小化&lt;/strong&gt;。这是从博弈论的角度来看，泛化能力强的模型能尽可能降低自己在真实环境中遇到意外的风险，因此会在内部产生对未知特征的预警机制，并提前做好应对预案。这是一种很抽象的也不那么精确的解释，但随着技术的进步，人们会找出在该解释下进行模型泛化能力的量化评价方法。&lt;/p&gt;

&lt;p&gt;当然，以上的6种对泛化能力的解释不是全部说的通的解释，未来会有更多理解泛化能力的角度。对同一个概念理解的越深，达到其的可能道路就越多，接下来让我们看看怎么做才能提高泛化能力。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;在机器学习中，正则化很容易理解，不管是&lt;strong&gt;L1&lt;/strong&gt;还是&lt;strong&gt;L2&lt;/strong&gt;，都是针对模型中参数过大的问题引入惩罚项。而在深度学习中，要优化的变成了一个个矩阵，参数变得多出了几个数量级，过拟合的可能性也相应的提高了。而要惩罚的是神经网络中每个神经元的权重大小，从而避免网络中的神经元走极端抄近路。&lt;/p&gt;

&lt;p&gt;最直接的正则化是在损失函数中加入惩罚项，比如L2正则化，又称&lt;strong&gt;权重衰减&lt;/strong&gt;（weight decay）关注的是权重平方和的平方根，是要网络中的权重接近0但不等于0，而在L1正则中，要关注的是权重的绝对值，权重可能被压缩成0。在深度学习中，L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。神经网络需要每一层的神经元尽可能的提取出有意义的特征，而这些特征不能是无源之水，因此L2正则用的多一些。&lt;/p&gt;

&lt;p&gt;而深度学习中最常用的正则化技术是&lt;strong&gt;dropout&lt;/strong&gt;，随机的丢掉一些神经元。类似的方法是&lt;strong&gt;drop link&lt;/strong&gt;，即随机的丢掉一些网络中的连接。丢掉的神经元既可以在隐藏层，也可以在输入层。dropout是一种将模型进行集成的算法，每一个不完整的网络，都可以看成是一个弱分类器。由于要引入随机性，dropout适合本身就相对复杂的网络，一个三个隐藏神经元的三层神经网络就不要让神经元随机的耍大牌了。&lt;/p&gt;

&lt;p&gt;另一个增加模型泛化能力的方法是&lt;strong&gt;数据增强&lt;/strong&gt;，比如将原始图像翻转平移拉伸，从而是模型的训练数据集增大。数据增强已经是深度学习的必需步骤了，其对于模型的泛化能力增加普遍有效，但是不必做的太过，将原始数据量通过数据增加增加到2倍可以，但增加十倍百倍就只是增加了训练所需的时间，不会继续增加模型的泛化能力了。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.2670807453416149&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfAxrvTibyDf3RrJI7leVcj6AF1vAYM088cBtpyXucowY6UdeUyWpgCH5yhCQwibrPBm60UR2ejon1A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;644&quot; /&gt;&lt;/p&gt;
&lt;p&gt;另一个增加泛化能力的方法是&lt;strong&gt;提前停止&lt;/strong&gt;（early stopping），就是让模型在训练的差不多的时候就停下来，比如继续训练带来提升不大或者连续几轮训练都不带来提升的时候，这样可以避免只是改进了训练集的指标但降低了测试集的指标。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6170886075949367&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfAxrvTibyDf3RrJI7leVcj6OrDFWbXSrrDjdSg7976by6m3g8wBR8n2raPebbagjzte242X6NC8uA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;316&quot; /&gt;&lt;/p&gt;
&lt;p&gt;最后一个改善模型泛化能力的方式是&lt;strong&gt;批量正则化&lt;/strong&gt;（BN），就是将卷积神经网络的每层之间加上将神经元的权重调成标准正态分布的正则化层，这样可以让每一层的训练都从相似的起点出发，而对权重进行拉伸，等价于对特征进行拉伸，在输入层等价于数据增强。注意正则化层是不需要训练。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;除了上述的四种方法，&lt;strong&gt;权重共享，随机梯度下降&lt;/strong&gt;及其改进方法，例如Adam，都可以看做是另一种正则化的方法。而我还脑洞过一种正则化的方法，在卷积网络中的pooling层，可以做average pooling，也可以做max pooling，能不能在池化层引入随机性，在每次训练时一定比例的池化层神经员做max pooling，另外的做average pooling。这个方法是集成模型的套路，将不同的池化策略看成弱分类器的区别。当然我还没有试验过这样的方法，但由于池化层的影响不大，所以对这个方法的效果不乐观。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6828478964401294&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfAxrvTibyDf3RrJI7leVcj6vrlQoSHa1InH8ibgxQWN7VAj13icP4NQamoTNKd1B3LIwS4ibS6R917fg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;309&quot; /&gt;&lt;/p&gt;
&lt;p&gt;权重共享的示意图，就是神经员去模仿周围神经元的权重。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/dcEP2tDMibcfAxrvTibyDf3RrJI7leVcj691GicPaiblokWHJqW0dMibWsO2f9KrF05JmNiaicvtRdHJnXsMkxEXuO9aQ/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;255&quot; /&gt;&lt;/p&gt;
&lt;p&gt;随机梯度下降的动态示意图，通过引入随机性避免局部最优，也可看成通过引入随机项来避免模型中的参数过大（回忆统计学中均值回归，说的是两诺贝尔奖得主的子女智商很大可能要比他们的父母低，因为他们的父母碰巧拿到了影响智商的随机项中的最高值，而他们的后代从概率上不应该还是这么幸运）&lt;/p&gt;

&lt;p&gt;正则化的方法，可以分为显式的和隐式的，前者的目的就是提高泛化能力，包括dropout，drop link，权重共享，数据增强，L1和L2惩罚项。结合泛化能力的六个解释，Drop Connect 和 Dropout 相似的地方在于它涉及在模型中引入稀疏性，不同之处在于drop connect引入的是权重的稀疏性而不是层的输出向量的稀疏性，权重共享和数据增强对应的忽视无关的细节，L1正则是增加模型的稀疏性，L2正则关注的是让模型在每一层生成的特征更加真实准确。&lt;/p&gt;

&lt;p&gt;而隐式的正则化，则是其出现的目的不是为了正则化，而正则化的效果是其副产品，包括early stopping，批量标准化，随机梯度下降。另一种分类的方式是训练时的正则化和模型构建时的正则化。前者只用在模型训练时，而后者在训练和模型实际运行时都会出现。前者包括随机梯度下降，early stopping dropout和drop link，这些技巧都不会用在训练好的模型中。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;最后让我们看一篇17年ICLR上的论文，&lt;span&gt;Understanding Deep Learning Requires Re-thinking Generalization，这篇文章的作者想看看深度学习为什么具有超过之前方法的泛化能力，为了定义清楚这个问题，他观察了机器视觉领域成熟的网络，&lt;span&gt;例如ImageNet和AlexNet，在&lt;/span&gt;不改变模型的超参数，优化器和网络结构和大小时，在部分/全部随机标签的CIFAR 10数据集，以及加入了高斯噪音的图片上的表现。如果在随机生成的分类标签上，模型表现的也很好，这对于模型的泛化能力意味着什么了？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5210577864838394&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfAxrvTibyDf3RrJI7leVcj6I4LAUoQibn8s1urHG3W6UMAPssD05TxaLk3ZiaibQXXwNwLILwh1eknbQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1021&quot; /&gt;&lt;/p&gt;
&lt;p&gt;先让我们看看文中给出的数据，A图指出，不管怎样在模型中怎样引入随机性，在图像中加入随机噪音，对像素进行随机洗牌，还是用随机生成的像素点组成的图片，哪怕图像的标签都是随机生成的，模型也能让训练集上的误差达到最小值。这很反直觉，在上述的情况下，人脑是学不到什么的，但深度学习却可以。这说明神经网络的有效容量是足够大的，甚至足够使用暴力记忆的方式记录整个数据集。但这并不是我们想要的。而在部分随机标签的情况下，模型用暴力记忆的方式记住了数据点，而对正确标注的数据进行了正常的特征提取。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.46168768186226966&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfAxrvTibyDf3RrJI7leVcj6oVZ7WQlfpff6hTfKstFycBg8caoOkombLDtRvA6JMzsLTxLTibSHtTA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1031&quot; /&gt;&lt;/p&gt;
&lt;p&gt;接下来B图讨论的是在不同比例的label是随机产生的情况下模型训练所需的时间，这里得出的结论是即便使用越来越多的随机标签，进行优化仍然是很容易的。实际上，与使用真实标签进行训练相比，随机标签的训练时间仅仅增长了一个小的常数因子。而且不管模型本身的结构有多么复杂，在随机标签的数据下训练起来时间都不会增长太多。这里多层感知机的训练时间增长要高于AlexNet，这点令我意外，在待优化的参数相差不多的前提下，可能的原因我猜测是CNN中待优化的参数相对均匀的原因。&lt;/p&gt;

&lt;p&gt;而C图展示了在是不同比例的随机标签下，不同的网络结构在测试集下的表现，这里选择的都是在训练集上错误为0的网络结构。可以看出深度学习网络即使在拟合随机数据时，仍能保持相对良好的泛化能力。这意味着标签随机化仅仅是一种数据转换，学习问题的其他性质仍保持不变。当全部的标签都是随机生成的时候，那么理论的泛化误差就是0.9（这里是十分类问题，随机猜有10%的机会是对的），但只要部分的标签不是随机生成的时候，那越复杂，容量越大的模型表现的泛化能力就越好。在40%的标签是随机生成的时候，如果网络完全没有暴力的对数据点的记忆，那么模型的最好表现应该是0.4×0.9+0.6×0.1即0.42，任何比这个好的表现都说明正则化的方法没有完全的阻止神经网络去死记硬背，但我们看到即使最好的模型，其训练时误差都能到0。但越先进的模型，在避免模型brute force式的记忆上做的越好，从而使测试集上误差更接近理论最优值。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6586901763224181&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfAxrvTibyDf3RrJI7leVcj6FTfyAo4zwOc53JYaHJibhO84wBQW3I5toTEQUShonKFicEXHZzRoue2A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;794&quot; /&gt;&lt;/p&gt;
&lt;p&gt;这里列出了不同的模型在不同的参数下的泛化能力，可以看出上文提到的drop link和权重衰减，标准正则化（BN）的效果，但没有列出我更关注数据，即在部分数据为随机标签时模型的泛化能力。&lt;/p&gt;

&lt;p&gt;这篇文章的好处是其实验模式是可以很容易去重复的，你可以在Minst数据集上去重复类似的实验，还可以看看在加入了不同的正则化策略后，网络在部分随机标签的数据上表现的怎么样。我还没有做实验，但预测相比于原始的2个隐藏层的CNN，加入drop out或drop link的模型泛化效果最好，这时可以让那些拟合到随机生成的数据的神经元消失。而加入L2正则项的模型泛化能力最差，原因是这时小的权重还是会引入错误，从而干扰特征的提取。不过重复一遍，以上的不过是个人的脑洞，我是通过在脑中模拟人的大脑在这种情况下怎么表现的更好来思考这个问题的，这样一种自我中心的视角，并不适合理解深度神经网络。但这就是人的具身认知的极限了。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;许多支撑神经网络有效性的依据都建立在这样一个猜想之上：“自然”数据往往存在于多维空间中一个非常窄的流形中。然而，随机数据并不具备这样的趋势。但很显然，这篇理论性的文章证伪了这个看法。我们并不理解是什么让神经网络具有好的泛化能力。而理解是什么让神经网络具有泛化能力，不止能让模型更具有可解释性，还能为构建更鲁棒的模型提供指导方向和设计原则。这么看来，现在的深度学习，更类似古代的炼金术而不是化学，还缺少一个统一的普世的理论架构。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;最后借着上面文章的图总结下这篇小文，通过从不同角度观察泛化能力，将深度学习中常用的分成了显式和隐式两类。下图可以看看early stopping的影响，在没有批量正则化的时候，early stopping并没有多少效果，等到满足条件（例如5次迭代时在训练集上准确度不明显变化）时，模型已经过拟合了，而再加入了批量正则化之后，early stopping可以发挥效果了，这说明正则化的方法要组合在一起用才会有效。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.8062360801781737&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfAxrvTibyDf3RrJI7leVcj6d51S9OwrobAYZYcpZRUib9xKYHKVFuFhIjNp2HkASWXPye4yyHuiccBw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;449&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而下面的这幅图展示了权重衰减，数据增强和dropout三种方法在Inception网络上的效果，除了要看到使用正则化带来的泛化能力提高之外，还要看到正则化的技术能解释的泛化能力只是一部分，还要很多未知的因素，对神经网络的泛化能力做出了贡献。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8290398126463701&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfAxrvTibyDf3RrJI7leVcj6VeWZkGXMgkFSNEoIddProwlwpZ7SwpKclbu1v1IS8f2ppIdaqBsICw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;427&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  这篇论文中只写了优化方法和网络结构对泛化能力的影响，而深度学习的另一个支柱优化函数的影响则没有提到。在分类任务上这点体现的不明显，但对于生成任务，改变的更多是优化的目标函数，比如各种GAN及其衍生模型，这时能很直觉的看出目标函数对泛化能力的影响。这与传统的观点是相悖的，传统的机器学习，能影响泛化能力的只是模型。但深度学习将数据和模型的界限变得模糊了，优化目标对泛化能力的影响这个问题该怎样变成一个可以量化的问题，类似这篇文章中做的，是一个值得思考的问题。可以在给&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383280&amp;amp;idx=1&amp;amp;sn=a6fd903f2c47339c52dcea9eedf65851&amp;amp;chksm=84f3cbb1b38442a7f4aac491852e06c34794154946a3656bc4ac4805b1ef1b41cb4469ae8419&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;GAN&lt;/a&gt;（点击查看相关介绍文章）的带标注输入数据中加入不同程度的随机性，但如何量化生成模型的泛化可靠性了？这值得深入的思考。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;更多阅读&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383444&amp;amp;idx=1&amp;amp;sn=94627b6e19ba9598afc551362f9e177a&amp;amp;chksm=84f3c8d5b38441c357851b92294f8b61bc24f08962f3dc399c7bc58287b52764f3c5e48961fb&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;什么让深度学习与众不同-《Artificial Intuition》读书笔记上&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383460&amp;amp;idx=1&amp;amp;sn=944fe726d386556f7308539397e97c60&amp;amp;chksm=84f3c8e5b38441f39eeac95a9393604c8c8b7ac50719cfccaac495c59952c37457bcc2a11bbb&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;《Artificial Intuition》读书笔记下 创造一种新的语言&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383350&amp;amp;idx=1&amp;amp;sn=f396de4fbaffd7dcf7af2af20b55bb2e&amp;amp;chksm=84f3c877b38441617cf9ecaea03d652f18ab2d159eccdc966107df071cca384e55618ce57671&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;胶囊网络结构Capsule初探&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;


&lt;p&gt;&lt;span&gt;原创不易，随喜赞赏&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9397590361445783&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd4icdSNQnx98Zicw7nnP3icPycqI1uRMvRxCezhYm4xQKdbiamvHVmC19a0o7YS03eqTrIL9QJS4wS4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;249&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;



&lt;p&gt;下面是广告时间，总时长超过16个小时，全面的涵盖常见深度学习模型架构，优化方法。本文提到的正则化技术都有详细的逐行代码详解。超值的良心价。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;9.821333333333333&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGGB3P7tjOcxTibLZ3J6CzkuzaMRoB3nUxdiavuQlnR6Rl6YDMUIU8t33g/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;750&quot; /&gt;&lt;/p&gt;






</description>
<pubDate>Sun, 13 May 2018 12:31:29 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/g4ROEb9VpW</dc:identifier>
</item>
</channel>
</rss>