<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>[原创]学习如何学习 之 meta learning</title>
<link>http://www.jintiankansha.me/t/Zu8xwvhsNT</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/Zu8xwvhsNT</guid>
<description>&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;img data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.665625&quot; data-w=&quot;640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcd9g9aFjBooVn5U4PP1EDF3ugVJrLlia2ELtxHbXUJs7SUPtRaxmkUBHhx3jLciaHXpx1ABVYwYBu9w/640?wx_fmt=jpeg&quot; /&gt;&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;span&gt;作者许铁，微信号：ironcruiser &lt;/span&gt;&lt;br /&gt;&lt;span&gt;法国&lt;/span&gt;&lt;strong&gt;巴黎高师&lt;/strong&gt;&lt;span&gt;物理硕士 ，&lt;/span&gt;&lt;strong&gt;以色列理工大学&lt;/strong&gt;&lt;span&gt;（以色列85%科技创业人才的摇篮, 计算机科学享誉全球）计算神经科学博士，巡洋舰科技有限公司创始人,   《机器学习与复杂系统》纸质书作者。曾在香港浸会大学非线性科学中心工作一年 ，万门童校长好战友。&lt;br /&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-ratio=&quot;0.10966542750929369&quot; data-w=&quot;538&quot; class=&quot;&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; /&gt;&lt;/pre&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;e9vou-0-0&quot;&gt;深度学习如火如荼的今天， 我们看到强大的视觉算法识别了复杂的疾病和所有人所观察不到的事物细节， 强大的阿法狗能够结束人类历史最精妙的发明围棋。一方面这些算法无时无刻不再鄙视着人类， 另一方面它们越来越透漏出它们的局限，比如惊人的数据消耗， 惊人的耗能，   “ 有限的范化能力”  ，  使得越来越多的人了解它们不过是一些头脑巨大的爬虫 而非真正的智能物种。    给你和人类学习一样的数据和一样的能量， 看你还能牛到哪去？  这是人们经常的质疑。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7mu6m-0-0&quot;&gt;这些问题事实也是学界的焦点，  前两年一片叫做“元学习” 的文章， 就试图从学习的本质解决问题。 那就是 - 学习 如何 学习， 就像那个cousera 的爆款课程一样 learn how to learn. 机器能够用暴力的算力和数据来梯度下降，但是它们真的懂学习吗？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8ko21-0-0&quot;&gt;我们知道， 所谓的机器学习还是深度学习， 无非给出一个问题， 你写出一个带着一堆参数（假设）的问题表达式和损失函数， 然后上一个损失函数， 然后梯度下降大法， 求出一个最优解。    &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3np5r-0-0&quot;&gt;然而如果我说这是学习的唯一方法， 你一定会觉得我学傻了。 难道你要学习人任何一个任务，都要在那里等着你大脑里的神经突触一点点的改造（梯度下降）？    简直有一种欲练神功， 必先自宫的既视感。    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;3np5r-0-0&quot;&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2f6vl-0-0&quot;&gt;我们的生活无时无刻不需要我们快速的学习和适应新的情况， 而神经突触的改变是一个慢过程， 这就决定了它必然不是学习的全部方法。  &lt;/span&gt;这个“快” 与 “ 慢” 的矛盾如何调和呢？  一个解决方法就是学习本身也是一个“快” 与“慢” 相加的过程。  &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;d03is-0-0&quot;&gt;事实上我们人类能够快速学习新情况， 是因为我们已经经过了大量的基础性学习， 我们后天的学习 ， 事实是因为我们的学习建立在更基础的模型之上。   这一点你的反应可能立即是迁移学习， 但是光看我们深度学习里那个迁移参数的深度学习是一个解决方法， 你可以把迁移学习看作今天这类方法的一个特例 ，而我们要看的是一种更广泛的方法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;95efr-0-0&quot;&gt;这里提到的双层学习， 是另一种更基本的框架， 又称元学习 ， “meta learning”  ，  它的概念是学习一大类任务的先验信息 ， 然后利用学习到的先验学习， 加速这一大类里的每个具体任务的学习。 其实这样的一个工作正是深度视觉能够成功的关键， 我们常说的CNN结构本身就是抓住了图像理解这个任务里平移不变等关键先验信息， 然后把它们手写到网络里面去的。  这里元认知唯一的区别在于我们不再靠手写，   而是让神经网络自己学到这个先验。  就好比给你一个特别巨大的多层感知机网络， 然后让它自己学出类似深度卷积的结构来。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;9ugf9-0-0&quot;&gt;今天讲的元学习的一个具体实例是指利用RNN网络加上深度强化学习的方法先学得一大类相关任务的先验信息，  然后利用这个先验信息，不用梯度下降学习， 纯凭网络动力学， 也可以适应每一个具体的任务得方法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;44afo-0-0&quot;&gt;文章声称， 当您 有一个RNN，加上比较强大的深度强化学习算法， 你就可能拥有这种能力。 具体怎么做呢？ 首先， 我们知道一个强化学习任务里， 你需要根据当下的状态做决策，来最大化最后的奖励， 这个东西我们通常称之为策略 ，数学上看就是根据每个状态， 采取一定决策的条件概率函数。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;d6ptk-0-0&quot;&gt;对于传统的强化学习算法， 我们通常需要动态优化等技巧来求解这个最优策略。 一个最典型的任务就是多臂赌博机。 这是一个由很多摇臂组成的赌博机，   每个臂都按照一定概率给你带来一个回报 ， 然后你要在有限的次数里最大化你的利益。  这个问题浓缩了强化学习的核心矛盾 -  探索与发现，假定你能够绝对准确的测量中奖概率， 那么你无非选择概率最高的那个臂就够了， 但是你要懂得绝对准确的测量概率需要无限长的时间， 而你的时间是有限的， 这时候， 你就需要在探索（测量）与发现之间做出权衡，摇臂赌博机涵盖了有限生命和未知世界的永恒矛盾， 因而也在生活或者广告电商应用里无处不见其踪影。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.6316666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkE0YyoA4lE9rAY3XWGz7luAtTezXibqjDvDVmbqicdv1wOLRV5ibTnAIZOg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;af696-0-0&quot;&gt;对于这个问题的求解有很多经典算法， 一大类经典算法称为托马斯采样，  这个方法的本质就是求解最大后验概率， 你用一个β分布作为先验，然后根据每一次实验（摇臂）的结构来更新这个概率， 比如一次结果为空， 就会按照贝叶斯后验概率公式轻微下降这个选项的概率而上升其它，最终求得结果。 托马斯采样是一种保守的，将探索和利用混合在一起的策略，  也能够取得相当不错的成绩。    &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;d12g2-0-0&quot;&gt;另外一个常用的方法称为upper confidence bound（UCB）,  可以翻译成最大信心策略吧。 这个策略与上面的托马斯采样相反，是一种激进乐观主义的策略，  它的信条是在那片黑暗的未知里隐藏着好事情。  具体的公式类似于当你在当下对某个摇臂积累了一定的经验， 比如N次尝试有m次奖励，此时的经验概率为m/N，  你知道这个测量由于数据不足有不准确性， 假定这个奖励符合高斯分布， 那么你就可以根据你的置信区间（自己定的）得到真实可能的概率的上界（乐观因此取上界）， 这样你就得到你的决策根据。 这个策略会偏向于探索未知， 因为你对某个摇臂尝试的次数越少，你的高斯分布就越宽（不确定）， 你的上确界距离你的&lt;/span&gt;实验概率就越远。  &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6c1jj-0-0&quot;&gt;好了， 我们来看看现在的任务， 我们生成一些的二摇臂（两个摇臂）的任务， 每个任务里的摇臂的得奖概率都是随机制定的0到1间的数。 有多少个这样的任务呢？ 两万个， 相反的， 我们对于这些任务不适用那些量身定制的传统算法，  而是使用一个LSTM加上Actor Critic 方法训练， 这是一种当下流行的， 同时进行策略估值和生成的方法， 我们把这个游戏里每一步骤的奖励和决策作为输入给网络 ， 然后让网络去最优化总奖励数。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c1snv-0-0&quot;&gt;这样训练完之后， 我们再来一组（300）个这样的任务， 没一个二摇臂任务由不同的概率组成， 不同的是，这组任务里我们固定网络参数， 不允许网络在梯度下降。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;beb0o-0-0&quot;&gt;如果按照传统方法的思维， 这时候的表现应该很差， 因为每个任务里概率都不一样， 需要重新通过刚刚讲过的方法学习得到， 你不允许网络学习， 它也就无所适从了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dgbl5-0-0&quot;&gt;然而这个时候我们发现，  LSTM仿佛自己得到了学习的真谛，它在任务的开始， 逐步探索积累经验， 然后通过学习得到的概率，很好的完成了任务， 就跟那些传统的学习算法结果一样好！    &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5633333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEjHKMicNK59QRF47CFCNhfibjcZVdgdyQl7vrTgFDejtNdQDPrtV10pZQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;8sua3-0-0&quot;&gt;不仅如此， 我们再进一步， 让这个游戏便的更复杂，我们刚刚假设摇臂是独立的， 真实世界的选项之间往往由相关性， 比如A的好往往暗示了B的坏， 宇宙能量守恒吗。  再摇臂之间的概率由相关性的时候又会怎样呢？      比如最简单的相关性 p1 +p2 = 1.  最后的结果就是相当的好！  再经过一段时间的训练后，网路无需学习， 就可以利用这种概率间的相关性规律， 来加速探索取得更好的成绩！  这也就是所谓的RNN可以自发学习掌握任务里的先验规律的意思。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;8sua3-0-0&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;9jpj4-0-0&quot;&gt;然后还有更有意思的 ， 我们可以设计一个更加奇妙古怪的相关性， 比如：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;b5sci-0-0&quot;&gt;我们又12个摇臂， 其中的某一个摇臂叫做信息摇臂，它可以指示其它的摇臂的奖励值，仿佛一个先知， 然而先知本身不给你奖励，只是告诉你哪一个摇臂的奖励值可能更高， 而且发现先知的过程要耗费时间的。  训练后的RNN网络仿佛理解了这个游戏的含义 - 通过寻找提示信息来学习。   在全新的任务里， RNN可以不再经过任何梯度下降， 主动的寻找信息摇臂， 并利用摇臂指示的信息寻找最高奖励。  这个问题里， RNN仿佛理解了探索与利用的矛盾， 并在全新的网络里， 利用自己的动力系统去实现这个过程。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dkk1t-0-0&quot;&gt;更上一层楼， 我们让这回连固定的概率都不要， 让每个摇臂的概率值随时间变化，唯一固定的是有的时候这个概率变化的速度快， 有的时候这个变化的概率小，  这个时候， RNN训练后的网络竟然仿佛能够在任何新来到的时候， 具有一种可调节学习速度的能力， 在那些概率经常变化的时间段让自己的学习速度也加快。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5633333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEEx9Y6LNrAAewzPhvaNfFt8wFjkhcicib51umtBLSSFcVOgpzaIxT7cfg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;除了K摇臂这个实验 ， 论文还尝试了经典的马尔科夫决策问题，  这个任务旨在通过一个简单的实验， 证实一个概念，   那就是刚刚的RNN 加上深度强化学习方法， 具备某种“构造世界模型” 的能力，  所谓构建世界模型，就是通过主动的了解和预测外界环境的变化来加强决策（对未知事物做出计划）。 而非光通过经验积累。  这也是区分人类高级学习和低级学习的关键所在，我们饱读诗书，都是在建立世界模型， 让我们对于毫无经历的事物也能够决策。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5633333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEIiaVibPU73DicUURCAE34s5pjvTd4j6DmVynLEeCXaCQbGmIEo8SItkNQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dr5de-0-0&quot;&gt;但是这个能力以往需要通过工程方法加入到模型， 也就是人类自己先了解了模型， 然后通过程序嵌入进去。  &lt;/span&gt;而在刚刚的框架下， 这种通过构建世界模型的学习能力似乎也能够被学到。  上面的例子里，有一个状态S1 ， 状态S1 可以通过行为a1 或者a2  得到新的状态S2 或S3，  在此之上根据S2或S3得到一个奖励0或1， 这个奖励也是随时间变化的随机变量。 当然任何一种行为都可以得到后面的两个状态只是概率不同， 就像经典的马尔科夫决策问题一样， 这可以被一个转移概率矩阵描述， 而这个矩阵对于行为者是未知的。 由于这个转移概率包含了世界在我的行为下会如何变化， 它就是我们所说的世界模型。 如果学习者主动掌握这个模型来决策， 就好比它学会了有模型学习。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;59tjr-0-0&quot;&gt;我们可以通过学习者的行为判断它是否试图建立这个模型， 如果行为者做出a1 ，并且最终达到S3得到奖励1，  那么它是应该更加鼓励行为a1还是行为a2 呢？  一朝中大彩， 十年买彩票， 不考虑任何模型的学习者所依靠的就是纯经验， 那么这个好经历无疑会让他更倾向于在S1下进行a1的决策，  而有模型的决策者就不然了，因为它知道这个结果是在一个小概率事件（从a2到S3是一个25%的小概率事件）  ，  同时通过“深思世界的本质” 他知道， 其实这真正说明的是S3是一个好状态， 更加容易达到S3的路径是a1而不是a2， 那么反过来我应该增大a2决策的权值而不是a1 ！   在这个事件下， 通过经验学习， 和模型结合经验学习就天壤之别了。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8igi7-0-0&quot;&gt;后面论文还举了一些比较复杂的任务，其中一个比较有趣的是导航。  这个导航的任务， 是让行为单元走迷宫，这个迷宫的某个角落藏着宝物， 然而隔一段事件宝物的位置就会被移动到另一个位置， 而更加残酷的是， 每次行为者发现宝物后就会被转移到另一个随机的地点。 这个任务里， 行为者的输入只有它眼前的迷宫景物和它本身的行动速度，以及是否得到了奖励， 这样的情景很像我们的3D射击游戏， 同样的RNN 框架下， 通过不停游戏的初始训练阶段，游戏者仿佛掌握了一种进行空间行走规划的能力。在测试阶段，无需重新训练调整， 行为者也可以去发现目标宝藏， 并且在一次发现之后，无论它被重新扔到哪个未知角落， 都已十分有效的方法返回回去。这里的世界模型， 事实上是这个环境的地图和宝藏的坐标， RNN在某种程度掌握了这个概念， 因此， 它会在全新的位置也能够做出类似于合理的行为， 这一点， 和人是类似的。    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5633333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEmPhjuKavicy2XQ8nGvrn7Cyj8pmDYrqibj4vPRZV0o8CfwSq6d6VHkBA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-ratio=&quot;0.10966542750929369&quot; data-w=&quot;538&quot; class=&quot;&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span data-offset-key=&quot;8h5th-0-0&quot;&gt;广告时间：&lt;/span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383610&amp;amp;idx=1&amp;amp;sn=eae53f91ea3bdb1d99464d3824175707&amp;amp;chksm=84f3c97bb384406dd3942d73be8d1dbe5a16815743990686d9054e1a3e9fa8fc42c8519ba270&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;铁哥开设的一个为期两日（12小时）的强化学习特训班&lt;/a&gt;&lt;span&gt;。&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;为什么是强化学习？  请看下图的技术泡沫爆裂图。  机器学习和深度学习在2017处于关注热度的顶峰， 大家看处在上升期的人工智能技术， 第一当属深度强化学习， 据这张非常表格非常粗糙的估计， 深度强化学习的技术成熟期在未来5-10年， 此时此刻， 正类似于深度学习在2010的状况。  &lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.67&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfqJgX6L51kcnJ07DHpdBzqmXKic2iaDgZOpUiacyYFWzBHst0BCZVTJINPzpfRibpMV6hWgUKSlj7ibBw/640?wx_fmt=jpeg&quot; /&gt;&lt;/p&gt;

&lt;br /&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-ratio=&quot;0.10966542750929369&quot; data-w=&quot;538&quot; class=&quot;&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; /&gt;&lt;img data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;5.896&quot; data-w=&quot;750&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJNQDiajMVL3arkx9IJnm10baZ1RjdLTN2KH6SKHZqnzyGO5K0G3dNOwg/640?wx_fmt=jpeg&quot; /&gt;
</description>
<pubDate>Sun, 23 Sep 2018 18:44:40 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/Zu8xwvhsNT</dc:identifier>
</item>
<item>
<title>[原创]自律与他律-从圆桌会议这个西方的发明开始</title>
<link>http://www.jintiankansha.me/t/VVSwNDUQks</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/VVSwNDUQks</guid>
<description>&lt;p&gt;最近做了一些组织协调的工作，有一些特创的感悟，希望能由小见大，说说自律和他律这个话题，同时比较一下东西方的思维方式上的差异。&lt;/p&gt;

&lt;p&gt;所谓组织协调，就是组织别人来开会，那在那里开会，就有俩种选择，一种是椭圆形的桌子，大家围成一圈做。另一种则是像教室那样，有讲台，底下是一排排的椅子。麦克卢汉说&lt;strong&gt;媒介就是内容&lt;/strong&gt;。会议室的选择，不应该只考虑能不能坐的下，不同的会议室，会给参会者不同的暗示，从而影响会议的预期效果。&lt;/p&gt;

&lt;p&gt;会议室能有选择，这也多亏了西方文化的渗透，毕竟圆桌会议是亚瑟王的发明，中国以前的会议室，都必须有主席台的。这从两千年前就能看出来，我们画中描述的孔子和他的门徒是这样的，一个人坐在前面，其他人围着他听。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfKbOxaIQrt0Z0Lufm8SapiczSO3bC9FFVuiaj9DSCeCQ0e8JWtibRRHAzo8QjibqAtPmVRmOtwwUMS7Q/0?wx_fmt=jpeg&quot; data-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfKbOxaIQrt0Z0Lufm8Sapic3CkAGlrUodAcJPhTJy7vHMARj4EUFx8d5ia6FQnibCcdq3p4v8aP7yXQ/0?wx_fmt=jpeg&quot; data-ratio=&quot;0.5476190476190477&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfKbOxaIQrt0Z0Lufm8SapiczSO3bC9FFVuiaj9DSCeCQ0e8JWtibRRHAzo8QjibqAtPmVRmOtwwUMS7Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;504&quot; /&gt;&lt;/p&gt;
&lt;p&gt;而西方的柏拉图学园，在其著名的画中是这样的，这里你看不出谁是中心人物，不是每个人都围着柏拉图的。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7457627118644068&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfKbOxaIQrt0Z0Lufm8Sapic5IRZQePmandNwSvKiaEtvZdqXRDsI2KggCKLvh85H9bO7ZIvvttX37A/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;295&quot; /&gt;&lt;/p&gt;

&lt;p&gt;圆桌会议和主席团式的会议，有什么区别了？从信息的不平等上讲，开会就是为了更高效的去交换信息，所以必然会存在着信息的不平等，但主席台式的会议，强调的是单向的信息交流，坐在下面的人想说话，要举手，要得到同意，而圆桌会议上每个人都可以畅所欲言。这个区别反映的就是交流规则背后的假设，首先是信息来源的问题，主席台的会议的目地首先是要让所有参加会议的人都记住某件事，其次才是收集参与者的反馈，而圆桌会议则是要从参会者那里收集到信息，虽然会议的主持者可能比参会人有更清晰的议题，但并没有一个预设的权威。&lt;/p&gt;

&lt;p&gt;这俩种不同的会议方式，延伸开既可以说到自由和平等的古希腊遗产，说到等级社会留给我们的奴性，但这样不叫以小见大，而是用已有的成见来按图索骥的找例子。其实主席台式的会议主持者，何尝不想博采众家之长，这样的会议模式，也有提问的时间，也会要坐在台下的人来发言，只是这样的会议模式，假设的是参加会议的人需要他律，不然会议就会低效，而圆桌会议讲究的是自律，会议上的人自己决定何时合适开口发言。&lt;/p&gt;

&lt;p&gt;之所以要他律，首先是由于会议的主持者有着&lt;strong&gt;不对称的信息优势&lt;/strong&gt;，比如会议中要宣布一件众人议论了很久的事，主持者不希望自己被打断，Ta觉得如果没有等级制，参加会议的人会在自己还在讲的时候就开始进行私下的信息沟通。就像经济学中禁止皮肉交易的讨论，一个对交易双方都能带来交易剩余的行为，因为其对第三者带来的负面影响，可能会被禁止。主席台的会议模式不是一无是处，这是一种特定环境下最高效的沟通模式。&lt;/p&gt;

&lt;p&gt;需要他律的第二个假设是&lt;strong&gt;静态社会&lt;/strong&gt;，会议的主持者不相信参会人俩俩之间的交流会带来新的价值。主席台式的会议，参会者有问题是问主持人，征集意见时也是针对主持人抛出的话题，参加会议的人无法自己提出议题，也无法为选择题多加一个选项。在动态社会里，只有零和博弈，任何俩俩之间的串联都会损害整体的利益，所以需要等级制施加的他律来摆脱囚徒困境。&lt;/p&gt;

&lt;p&gt;然而在一个动态的社会里，主席台的会议模式变得越来越不适合。首先是决策的来源，主席台的会议是要求决策者已做好了一个决策，只是借助会议让决策能高效的执行，而越是变化快的时代，做决策所需的信息就越需要来自于第一手的信息，需要头脑风暴带来的全新视角。其次是决策的执行，如果一个决策能足够细化，每个人只负责其中一小部分，那主席台的模式最适合，但若是扁平化的小团队，每个人都要做多面手，那就需要圆桌会议带来的和而不同。&lt;/p&gt;

&lt;p&gt;你也许会问，为什么要花时间来讨论会议室的选择这样琐碎的问题，我已经知道了圆桌会议更好了，但关键的是要知道其为什么好，从而能举一反三，在其他领域改变自己的思维模式。圆桌会议的目标和主席团式的会议，本质的区别在于其对真相的态度，前者是主动去寻找真相，后者是主动去遮蔽真相。其实没有什么是百分百的真相，任何理论都是对现实不完整的误读。然而圆桌会议是希望让参加会议者通过相互的妥协，从而让每个人都能达到更接近事实的理解，而主席台的会议的目地则是将会议主持者认为的真相强加给其他人。&lt;/p&gt;

&lt;p&gt;所以说改变会议的模式是手段，真正要改变的是会议预期要达到的目标，从达成既定目标改变为对真相的探寻，从而更高效的服务于任何会议的终极目标，即群策群力办成一件大事。既然是要寻找真相，首先就不要怕失败，要让参加会议的人每个人都回答这样一个问题，如果我们现在做的事在未来三年失败了，那是由于什么原因造成的，只有对失败可能的复盘，才能避免这些失败。其次是要鼓励跑偏，没有随机性就没有创造性的源泉，但跑偏的应该是内容，而不是逻辑，就如同搞笑诺贝尔奖并不是再鼓励伪科学，追求真相的会议，要鼓励的是那些看起来很好笑但却和议题有关且符合逻辑推演的观点。&lt;/p&gt;

&lt;p&gt;然而圆桌会议要成功，首先要求参会者能够做到自律，能够不那么自私自利。没有自我节制，会议上的自由会变成闲聊，经济上的自由会变成互相伤害和欺骗。在一个完全失去了自律的坏境下，每个人都只考虑自己那一点点小九九，要想合作就必须要等级制的监管。其次是要参会者是互补而不是互替的关系，互补意味着合作会带来额外的收益，所以寻找真相，从而找到最佳的合作方式，能够创造出全新的价值，而互替则意味着每个人都是一个螺丝钉，需要的是人与人之间的竞争来带来更高的效率。&lt;/p&gt;

&lt;p&gt;从会议延伸到广义的沟通，任何的沟通，其目的都是要在三个成本之间做权衡，从而找到三者加起来的最小值。第一个是自己做一件事的成本，第二是通过交易让别人做这件事的成本，第三是通过等级或权力让别人做这件事的成本。如果对于大多数人，第三个是消耗最低的选项，那官僚体系就会由于自身基座的千疮百孔而枯竭，例如东北的经济断崖，如果第二个选项变得很容易，那就会带来大量的搭便车者，他们从交易中拿走了蛋糕的大部分，从而加剧了社会的阶级分化，同时带来技能的降级，例如美国的制造业空心化。真正要关注的是这三者的加和最小，而真正决定这三者之和的，是第一个选项自己做的成本，如果自己做不了，那么不管是交易还是通过跑部来获得，都需要超额的成本，但如果自己能做，那就不会有过高的溢价。&lt;/p&gt;

&lt;p&gt;前几日看到一位知识服务大V说起他和他孩子的对话，他问孩子医生工资高是不是应当，孩子说应当，因为医生需要很多知识，而学知识很累很艰难。但考古学家也需要很多的知识，可为什么考古学家不是人们认为的高薪职业了。其实医生能拿高薪，那是由于医生能做其他人想做做不到的事情，如果未来的AI医生常驻手机里，那医生就不会是高薪职业了，虽然成为一个医生还是需要付出同样多的努力，而如果未来人们对文物古玩如痴如狂，那考古学家就会成为高薪职业，即使他们没有获得额外的知识。&lt;/p&gt;

&lt;p&gt;所以说一个人，一个团队，首要做的是补齐短板。一个人要是没有点医学常识，就会被忽悠的人财两空，但若是一个国家工业门类齐全，就不会由于外部的打压伤到根本。只有自己知道什么可行什么不可能，才能够在这三者中根据具体情景做出符合自己当下处境的选择。获取常识的管道顺畅，是除了批判性思维的普及外，最能够促进社会公平的途径，而从小训练出的自律和自我驱动（做好自己擅长的事而不是在热门的事上做的好），才能够为之后的圆桌会议打下基础，从而促进社会整体的发展。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383628&amp;amp;idx=1&amp;amp;sn=c4f36bbc6c78591917265cf34cd92a47&amp;amp;chksm=84f3c90db384401b390bb642961e86bd7a131b5fd1cfa5286617f4f8477be271512ee73e79f1&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;精英失灵与知识付费的悖论&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383574&amp;amp;idx=1&amp;amp;sn=385b4d01fb52473e8a7506d9952d9bf2&amp;amp;chksm=84f3c957b3844041bd80d0751953b67ff1ead4948d4673f4e3e6ae2cc3b6e94453e1be076812&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;物理才是最好的创业指南-读《宇宙从何而来》&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

</description>
<pubDate>Sun, 16 Sep 2018 23:31:03 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/VVSwNDUQks</dc:identifier>
</item>
</channel>
</rss>