<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>放养的深度学习-浅谈自编码器</title>
<link>http://www.jintiankansha.me/t/npYi6vxlsI</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/npYi6vxlsI</guid>
<description>&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5559322033898305&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGeRlt2tPet7t9EFQs0zLjkwUOmkyuKL0ib9CTekpRN36BKrUbLcicHGicw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;590&quot; /&gt;&lt;/p&gt;
&lt;p&gt;说起非监督学习，总逃不过Yann LeCnn的这张图，他将强化学习比做蛋糕上的樱桃，将监督学习比做蛋糕的奶油，而将非监督学习比做蛋糕底座。这个比喻不止说明了这三种学习范式的江湖地位。更形象的指出了各自的特点，强化学习得到最多的关注，监督学习好吃但是相对价格更贵（需要人工进行数据标记），而非监督学习虽然潜力巨大，但却相对来说最默默无闻。&lt;/p&gt;

&lt;p&gt;除了不需要人工数据标记这个优点，非监督学习的另一个优点是不会引入人的偏见。举一个具体的例子，如果要开发一个能够自动对手机里照片按照内容分类的工具，那么监督学习的做法就是将图像进行分类，看看图片中的主体是人物，动植物，建筑，自然风景或是文字，然后按照每张照片的分类将照片放到不同的相册中。而非监督的方式要做的是将每一张图片放到一个二维空间中，不是随便的放，而是要让内容相近的图片都凑到一起，最终出现几个图片簇，而在每个簇之间的距离要尽可能的大。&lt;/p&gt;

&lt;p&gt;如何判断图片的语义是否相近了，可以假设同一个时间段拍摄的，在同一个地点拍摄的照片具有相似的内容。一旦训练好了这样一个将图片自动聚类（降维）的模型，那么你朋友第二天发给你的昨天聚会照片，即使系统显示的时间不在同一天，也会被归档在正确的相册中。&lt;/p&gt;

&lt;p&gt;在深度学习出现之前，聚类已经有了很多种成熟的方法了，从最简单的K means到不需要设定聚类数的affinity propagation，再到层次化的聚类。然而，这些聚类方法对于非结构化的数据，例如图片，声音处理的不好，如果能够将非结构化数据的维度降低，那就可以使用传统的聚类方法了。然而，线性的聚类方法，例如PCA或MDS，在图像上的表现不佳。这里展示的是使用PCA对MINST数据进行降维的结果。不同的数字并没有被明显的区分开。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceCicV64SRTQnx3BRq4k511ZSJETWx8QNbWf8wGiae19HVib8FHxHNg3HiczzmquQ7zbmj5jjUg6HoQZg/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;0.8078125&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而使用非监督学习在深度学习中最典型的架构-自编码器，就可以做到对图像等非结构数据进行降维。下面展示的是用深度学习对手写数字进行的降维，不同的数字区分要比上一副图好的多。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceCicV64SRTQnx3BRq4k511ZMfRMrSCIUnicFBDJalm7f8VmE2lzQ7gzySGuwGib2ChZjvI684DmiaiaBA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.9076376554174067&quot; data-w=&quot;563&quot; /&gt;&lt;/p&gt;

&lt;p&gt;任何深度学习的架构的构建，都是将想要达到的目标翻译成损失函数的过程，数据降维的目标是让降维后的数据能够更好的保持原有数据的区分度，让原来能分开的数据现在也能分开。但原始的没有标签的数据点之间距离都没有明确的定义，又如何用一个公式来量化降维后数据点之间的区分度。&lt;/p&gt;

&lt;p&gt;如果在横向上无法解决问题，那可以试试在纵向上进行探索。先假设问题已经解决了，我们找到了一种完美的将MINST图像进行降维的方案，那么我们能拿这个方案做什么？假设每个MINST数据集中的图像有一个唯一的6位ID，越靠前的位置表示对该图片的越大类的分类。那么我们完美的降维方案给出的结果将可以100%的预测出图片的ID。接着假设我们还没有找到这个完美的降维方案，但已经差的不多了，那么我们根据降维后预测出的图像ID应该只在最后几位有差距。到了这一步，待优化的损失函数已经呼之欲出了，即降维后的特征生成的图像和原始的图像差距有多大。接下来要做的就是先训练一个神经网络来降维，再训练一个网络进行升维。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.952755905511811&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGbP13WPHIY6l6ibxRAWng4EMBRGQZ5PI6STxTNlL6CjJdTF36jTuNVxg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;254&quot; /&gt;&lt;/p&gt;
&lt;p&gt;从最基本的自编码器出发，将损失函数进行改变，让输入加入误差，而待重构的是不带误差的原始数据，就可以得到变分自编码器（VAE）。而在网络结构上，也可以使用卷积网络或循环神经网络。然而不同于深度网络，自编码器的结构使得网络可以逐层训练。例如一个堆叠的自编码器，先将原始的100维数据降低为50维，再降低为10维，最后降低为3维。那么就可以先训练一个浅层的100-&amp;gt;50-&amp;gt;100的自编码器，训练好后将网络固定，再使用上次训练得出的降维数据训练50-&amp;gt;10&amp;gt;50的自编码器，依次类推，最后将训练好的编码器和解码器按顺序堆叠起来。这样的训练方式可以避免梯度消失的问题。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8387096774193549&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGKcTX2C9MmhZL4F8sL3I8NvlPia8JFdS7KCOcvbSZBSWOZ5Hbzn7xChg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;279&quot; /&gt;&lt;/p&gt;
&lt;p&gt;不同深度的网络学习率随着深度增加显著降低&lt;/p&gt;

&lt;p&gt;自编码器进行数据降维的目地，是为了进行方便聚类。使用Kmeans等算法，可以对原来的Minst数据进行聚类。在有数据标签的时候，使用Normalized Mutual Information (NMI)，可以来评价聚类的效果，这个数字越大，表示被正确的聚在一起的数字越多。如此，就可以评价使用原始的数据聚类和使用自编码器降维后的数据聚类的效果。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7507836990595611&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHG0BTBHIbcOeWoEN4gXMSvebAcOChVJVcicuasOlmImKia8ibKZicykh4pRQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;638&quot; /&gt;&lt;/p&gt;
&lt;p&gt;以下是Keras相关代码&lt;/p&gt;
&lt;pre readability=&quot;4&quot;&gt;
&lt;span class=&quot;c1&quot;&gt;# this is our input placeholder&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;784&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,))&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# &quot;encoded&quot; is the encoded representation of the input&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'sigmoid'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# &quot;decoded&quot; is the lossy reconstruction of the input&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;784&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# this model maps an input to its reconstruction&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autoencoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt;&lt;span class=&quot;p&quot; readability=&quot;2&quot;&gt;)&lt;p&gt;#  this model maps an input to its encoded representation&lt;/p&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;span class=&quot;n&quot;&gt;autoencoder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'adam'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'mse'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;span class=&quot;n&quot;&gt;pred_auto_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_auto&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;span class=&quot;n&quot;&gt;km&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_auto_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;km&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred_auto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;br /&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;span class=&quot;n&quot;&gt;normalized_mutual_info_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;可以看到，使用自编码器+Kmeans 的方法，其MNI从0.50提升到了0.74，效果很明显。类似的，使用自编码器降维后的特征，也可以结合传统的机器学习算法，例如KNN，随机森林等进行分类任务，你可以训练多个层数不同自编码器，有的将原始数据（100维）降低为10维，有的降低为2维，再分别使用降维后的数据进行分类，这可以理解成集成弱分类器，也可以从神经网络的角度理解成残差网络，即在深度的网络中加入直接连接的短路层。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.546242774566474&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGcL3oPA5crOuGwNgd1S9eia4FsRyyMNlgMqdh9armExJB5oaM879iakuw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;346&quot; /&gt;&lt;/p&gt;
&lt;p&gt;残差网络的结构&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;p&gt;自编码器在监督学习中还有另一种应用。还是拿Minst数据分类举例，已知标签时，可以对每一个数字训练一个自编码器，然后一个待分类的数字来了，分别用在训练集中训练好的自编码器进行重构，根据那个自编码器的重构误差大小，使用softmax函数，将不同数字对应的自编码器的重构误差转化为该图片为那个数字的概率分布。&lt;/p&gt;

&lt;p&gt;这样的分类方式下，假设你拿一张写着字母A的图片，那么这十个用手写数字训练出来的自编码器，就会出现没有一个重构误差足够小的情况，这使得你的模型能够发现异常点，而传统的分类模型，你的图片总会被归到一类中。再假设现在你在Minst数据集之外，又拿到了一万张手写的8的照片，这时你不需要重新训练模型，只需要重新训练数字8对应的自编码器。&lt;/p&gt;

&lt;p&gt;假设你在训练中发现数字8和数字6总是容易区分不开，那你可以根据一个新的图片在数字6和8中对应的自编码器中的重构误差进行人为的调整，比对在经过softmax时放大其中的差异。这使的深度学习不在是一个黑盒，使得研究者可以看到预测过程中发生了什么。&lt;/p&gt;

&lt;p&gt;在模型训练的过程中，也可以针对原始图像和生成图像的距离进行类似Relu函数的截断。通过更改权重，让模型在重构误差小于一定cutoff的情况就不进行优化，而只关注那些重构误差还相对较大的样本。这里的思路就类似Xgboost中将预测结果不好的样本再放回重新训练的思路了。&lt;/p&gt;

&lt;p&gt;假设现在数字8的样本数已经是其他数字的100倍了，上述的使用自编码器的分类模型不会像传统的分类模型那样，受到样本数不均的负面影响。这对于医疗，金融这种正负样本量差距很大的应用场景，极其有用。同时，这样的方法进行的预测，更加稳健，不用担心图像修改一个像素，模型的预测结果就会改变。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;将自编码器用于图像生成，可以避免GAN中的模式塌缩问题（只生成黄色的猫）。分别用猫和狗的图像训练自编码器，将原始图像的降维表示进行微调，就可以用来生成新的猫和狗的照片。由于生成的图片都在原始图片降维后的空间附近， 自编码器生成的照片，不会出现GAN那样五条腿的猫这种明显不符合常识的图片。&lt;/p&gt;

&lt;p&gt;使用自编码器降维，还会有更好玩的应用。还是MINST数据集，将所有6的图片，分别转90，180，270度，这样一张图片就变成了4张，通过自编码器降维再进行Kmeans 聚类（K取4），是可以将图像按照转动的角度分成4类的。现在拿一张正常的9的图片交给自编码器去降维，这个图片会被分到旋转180度的那一簇6中，这说明模型可以学到图片的语义信息，这种在不同事物间进行关联的能力，是人类推理的基石。&lt;/p&gt;

&lt;p&gt;更哲学一些的论述，非监督学习做的是根据现有的数据去预测将来。MInst数据集的例子可以看成是将一个人写数字的过程拍了下来，假设一个人要花3秒写数字，自编码器做的可以看成根据第三秒后的笔迹预测前一秒之前的笔迹，最终能够达到在一个数字还没有写完的时候就预测出这个数字本身是什么。（空间的降维再升维对应时间上的历史匹配加预知）&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;总结一下，这篇小文介绍了自编码器的原理，训练方法及在分类和聚类和图像生成中多种的应用场景及优势。自编码器的变种很多，发展很快，代码相比于CNN，RNN来，实现起来也不难，是一个值得细致学习的探索深度学习框架，这篇小文要做的只是抛砖引玉。&lt;/p&gt;

&lt;p&gt;更多越多&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383201&amp;amp;idx=1&amp;amp;sn=d401d14056886ca4ffc747821ee3d4c1&amp;amp;chksm=84f3cbe0b38442f69e9bbb440e30aa6b3386601f7c597ded9200f73a46b0f24ddb775a684642&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;R 语言中的深度学习 Minst数据集下的聚类分析&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;原创不易，随喜赞赏&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9397590361445783&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd4icdSNQnx98Zicw7nnP3icPycqI1uRMvRxCezhYm4xQKdbiamvHVmC19a0o7YS03eqTrIL9QJS4wS4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;249&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;9.821333333333333&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfM7yZclSnvabLiaAb6IyTHGGB3P7tjOcxTibLZ3J6CzkuzaMRoB3nUxdiavuQlnR6Rl6YDMUIU8t33g/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;750&quot; /&gt;&lt;/p&gt;





</description>
<pubDate>Sat, 12 May 2018 11:32:55 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/npYi6vxlsI</dc:identifier>
</item>
<item>
<title>《Artificial Intuition》读书笔记下 创造一种新的语言</title>
<link>http://www.jintiankansha.me/t/TvmGxZK8sI</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/TvmGxZK8sI</guid>
<description>&lt;p&gt;在上一篇相关的推送&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383444&amp;amp;idx=1&amp;amp;sn=94627b6e19ba9598afc551362f9e177a&amp;amp;chksm=84f3c8d5b38441c357851b92294f8b61bc24f08962f3dc399c7bc58287b52764f3c5e48961fb&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;什么让深度学习与众不同-《Artificial Intuition》读书笔记上&lt;/a&gt;中，谈论了深度学习为什么说传统的机器学习有着显著的不同，为什么深度学习不是传统机器学习方法的小修小改，而是一个全新的物种，关键词是自组织，动态和负反馈（GAN中的猫鼠游戏，探索与利用的权衡），在下半部分，将要谈谈深度学习能够怎样帮助人们提高生产力，不止是更高的效率，更多的闲暇，还有全新的生产方式。&lt;/p&gt;

&lt;p&gt;迁移学习，尤其是不同领域间的迁移学习（例如将一个能把英语翻译成法语的网络迁移到英语翻译汉语上）做到极致，就会使模型具有模块化U盘化的特点，即插即用，还可以相互组合相互制约。而模型的目地是创造对真实世界有用的简化，从而使得人们能更好的认知外界。人类上一个符合这里描述的发明是文字，再上一个是语言。正是语言，使我们能够将知识隔代传递。而深度学习训练出的模型，其意义等同于创造了一种新的语言。&lt;/p&gt;

&lt;p&gt;文字是为了解决口语无法准确的做到隔代传播而被发明的。而深度学习要解决的是文字难以大范围的即时传播的问题。为了理解这个问题，你可以回想你看一本书时，每个字都认识却连起来看不懂的感觉。这种情况下，你至少知道你自己看不懂，更多的时候，你以为你看懂了，但实际上却只看到了作者文字的皮毛。认知心理学家Leonid Rozenblit将其称为illusion of explanatory depth，在这之后，一个个实验都证实了不管在什么话题的文章中，读者都无法抓住文章的全部知识点和细节。不用做什么实验，只需要想想从小到大做的阅读理解，不管是高考英语还是GRE的阅读，要拿满分那有那么容易。&lt;/p&gt;

&lt;p&gt;那这个微观层面的问题在人类这个群体的宏观上有什么影响了？文字使我们能够保留上一代人新创造的知识50%，剩下的由于来不及写下来丢失了，印刷机的普及使这个比例增加到了90%，而互联网的普及使得这个比例增加到了99.99%，可问题是在文字稀少的年代，人们有大把的时间阅读，理解的比例是99.99%，但如今认知过载的年代，理解的比例却只剩下了不到50%。如此一来，知识的积累效率反而比不上文字发明前的水平了。&lt;/p&gt;

&lt;p&gt;与illusion of explanatory depth对等的一个概念叫知识的诅咒。说的是具有专业知识的人，无法意识到自己说的习以为常的东西对于行外人来说，无异于天书一般。如果能有一个APP，能够根据讲解的对象，自动的给出提出预警，告诉讲解者这里使用了未经解释的术语，那里的逻辑有些跳跃，这同样有助于信息的传递。&lt;/p&gt;

&lt;p&gt;这就是深度学习能够帮助人类的第一个领域，去在屏幕（其他人）和我们的大脑之间构建一个便捷的接口（interface）。不止是优质信息的筛选，相关背景知识的提供，更关键的是保证阅读者真的读懂了这篇文章，讲解者提供了所需的全部信息。接下来让我们开开脑洞，具体的谈论些可能的应用场景。&lt;/p&gt;

&lt;p&gt;Wiki上有一个认知谬误的列表（https://en.wikipedia.org/wiki/List_of_cognitive_biases），我觉得很有用，可以帮助自己在思考的时候检查有没有犯错。但如果有一个算法，能够自动检查文章中的逻辑错误，就像word会在语法错误下划线那样，那就更好了。另一个应用场景是在阅读中预测那些部分你阅读起来会有困难，然后给出相关的信息。预测的依据可以是之前的阅读记录，其他类似读者的反馈，当前阅读时的眼动信息，甚至还可以通过脑机接口，拿到你阅读时是否感到困惑和好奇的信息。而第三种应用场景是信息汇总，比如金融数据的自动摘要生成，医疗案例的结构化提取，这两项都是已经相当成熟的应用。&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcczHDIiaWOR2dPFBtH8RttCZLu0icHb4T1zLe45Lb7ib7pYYZTkwsbibdV5ia5bTIeqOtP3u1AoPpnFGAg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;以上谈的是深度学习如何帮助人类提高生产力的第一种方式，缓解认知过载，避免认知偏差。相比于改进式的提升，接下来要说的会是革命性的。不止是因为人类的认知受制于大脑的内存容量和运行速度有限，也容易受到情感的干扰，还因为人类的认知是基于自身的。我们已经看到AI生成新知识的例子包括AI教会顶级的围棋和象棋选手新的下法，通过GAN来合成新药，用AI来做数学证明。但为什么会有这样的成功了，这里给出一种直觉的解释，即深度学习创造全新的词汇。&lt;/p&gt;

&lt;p&gt;正如对于百万和千万这样的大数字，人们缺少直觉性的认识。对于过于细节的概念，人类的认知也会是无能为力的。人能够总结的概念受制于人的直接体验。比如之前的教育学研究，将学习者分成视觉型和听觉型，但这个分类，显然是不够精细，但要再分的细致，却不是更多的研究就可以的，这面临人类具身认知的一个局限。但AI拿到了更多的数据，就可以进行更细的分类，不会存在一个上线。深度学习可以通过越来越深的网络创造出越来越细分的概念，再通过细分概念间的组合创造出全新的分类方式。就如同精准医疗需要AI将病人分成不同的类别，未来的教育，扶贫，都可以借力AI，针对每一个人给出个性化的建议来。通过将人群细分，带来的生产力的进步以及个人幸福感的提升，将远远超过当前已出现的深度学习应用能带来的效果。&lt;/p&gt;

&lt;p&gt;那在未来，有那种思维方式是人工智能无法取代的了？答案是反事实的思考。监督学习能够做关联，强化学习能够进行有目的的干预。但目前只有人能够问如果她不爱我，那她为什么会多看我一眼这样的问题。反事实的问题不止让人类可以计划未来，具有比AI更大的自主性，还可以让人类更方便的协作，具有比所有AI都强的多的社会智能和合作能力。&lt;/p&gt;

&lt;p&gt;上述思考中用隐藏着用到的，是评价智能的三个维度。分别是自主性，社会性和计算性高效性，人类个体是在三个维度上都表现的很高的，而当前的人工智能，大多只在计算高效这一个维度上接近了人类的水平。而在科幻小说和电影中，则大多描述了在自主性上和人类比肩的机器人，在像机器人总动员中，则出现了社会智能超群的超级AI。&lt;/p&gt;

&lt;p&gt;关于《artificial intuition》这本书，书中精华的内容还有很多，这本书光参考文献列表就有几十页厚，其中给出了很多值得细读的论文。这里只摘取了书中的部分精华内容，结合自己的理解，给出了粗浅的演绎。由于我买的是纸质书，因此没法提供书中的精细的插图。这里只列出书开篇的图，是深度学习的一个知识点的树状展开图。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.7777777777777777&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdLDeu8uJoW2lHgmIdbxHWtVJ4nJOficNhB5Sd3vnQdQ5Gv24ic6fGo8kkGkdHL2qspmqLZnwBuYuzA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1080&quot; /&gt;&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383444&amp;amp;idx=1&amp;amp;sn=94627b6e19ba9598afc551362f9e177a&amp;amp;chksm=84f3c8d5b38441c357851b92294f8b61bc24f08962f3dc399c7bc58287b52764f3c5e48961fb&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;什么让深度学习与众不同-《Artificial Intuition》读书笔记上&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383391&amp;amp;idx=1&amp;amp;sn=2b027d666dba2cb0d1afd5cdef66af9b&amp;amp;chksm=84f3c81eb38441080c475b41dbf1e9e5118cb25a638d2a908593f99128bb733162d0f6f7cb49&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;当AI遇到生物-深度学习在生物研究中的应用案例列表&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383449&amp;amp;idx=1&amp;amp;sn=9517b20745cabe1acb08b3e473341dba&amp;amp;chksm=84f3c8d8b38441ce38c6c19bee152a11d6c88076c68a977941e6829329cd4c82000fa7c72eb1&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;机器学习+深度学习 巡洋舰特训课铁哥总结长文&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;原创不易，随喜赞赏,&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9397590361445783&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd4icdSNQnx98Zicw7nnP3icPycqI1uRMvRxCezhYm4xQKdbiamvHVmC19a0o7YS03eqTrIL9QJS4wS4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;249&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

</description>
<pubDate>Wed, 09 May 2018 14:31:44 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/TvmGxZK8sI</dc:identifier>
</item>
</channel>
</rss>