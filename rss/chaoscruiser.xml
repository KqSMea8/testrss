<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>[原创]当RNN碰上强化学习-为空间建立通用模型</title>
<link>http://www.jintiankansha.me/t/sAxMjmtY0C</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/sAxMjmtY0C</guid>
<description>&lt;p&gt;你可能了解强化学习，你也可能了解RNN， 这两个在机器学习的世界里都不算简单的东西在一起能碰出什么火花，我给大家随便聊几句。  &lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfrfdKcbEB2Kic4M8MfGAJ0EAuiatSW2r64cCwnyl4uLZjsLM0ujNAkjhw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;4hh4f-0-0&quot;&gt;在讲RNN前我们先来侃侃强化学习, 强化学习越来越受重视，它为什么很重要，用一张图就够了。想像一下我们是人工智能体，我们有强化学习和监督学习可以选择，但干的事情是不一样的。面对一只老虎的时候，如果只有监督学习就会反映出老虎两个字，但如果有强化学习就可以决定逃跑还是战斗，哪一个重要是非常明显的，因为在老虎面前你知道这是老虎是没有意义的，需要决定是不是要逃跑，所以要靠强化学习来决定了你的行为。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bfaa2-0-0&quot;&gt;从算法上看，  强化学习是一个对未来的优化问题， 其算法的总纲可以归结到下面这张图里面，强化学习有两大门派：一种是动态规划（Dynamic Programming），另一种是策略梯度（Policy Optimization），还用老虎的例子说， 见到老虎估计一个打和不打在20年里的收益函数（比如打死老虎，县衙领赏， 抱得美人得10分vs被打死负10分） 然后推出现在的决定就是动态规划。 凭直觉和既往经验直觉决定跑还是打就是策略梯度法。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bfaa2-0-0&quot;&gt;动态规划需要通过迭代来学习比较繁琐但是更精确，直接学习法更简单但是容易陷入局部最优，为了优势互补，两种方法相结合就得到Actor-Critic法，一方面直接学习行为，另一方面通过评估函数来最终评价那个行为对整个未来的影响，在反作用于对行为的学习，现在主流的模型都是基于这种方式。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfkpsUHQEQAbBImrY3mrC9veN1qG1EW95GicYdicndG2Hr7KoolQX1OFaw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;bu154-0-0&quot;&gt;强化学习优化算法的成功是在Markov决策框架里开始的，只要问题符合这个框架（把过程分解为无数离散的时刻，然后此刻的信息足以决策后面的结果， 而无需历史信息，无论问题多复杂， 都可以分解为状态（state）- 行为（action）- 奖励（reward）- 策略（policy）四要素。状态是每个时刻你拥有的和决策相关的信息总和，行为是你的决定， 奖励是你的此刻收益。而策略就是从状态到行为的对应关系。就拿方格矩阵走迷宫（下图a）的例子说，你的状态就是你的位置，你的行为就是上下左右走，你的奖励是中间的加号。行走的过程每一步都只与上一步相关，此刻拥有绝对信息（位置），因此可以简化为下图b的马尔科夫决策图框架。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fo343-0-0&quot;&gt;这里面只有策略是优化的目标。策略最简单的表格方法就是一个纵轴是状态，横轴是行为的表格，每个格子里是某状态下做出某个行为的概率（下图c）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如果你用刚提到的动态规划法来做优化，你还需要另一个表格，叫做Q表格， 这个纵轴同样是状态，横轴同样是行为，但是每个对应的格子里的内容是value， 数学上说就是某个状态和行为下未来收益的期望（d）, 我们通过学习不停迭代这个表格（e）。 有了Q表格， 你可以通过对随机性偏好的假设很自然的得到刚刚说的策略表格。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7899159663865546&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7Kf0JXXcicKyT1ZG6E23CBtMyN7gSIYlIObR2t8xWKooicujPYjkNqCQ5mg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;238&quot; /&gt;&lt;/p&gt;

a


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfCoicfghk7hfW0Go2dorPMkH6HEmnuvWGYUvGibdBtJUdgEnHXKkRpBuQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

b

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.225130890052356&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfQggzMLEIj83umLs9kNiaH4qjBQMx4V29ECGbH3vvJ1Tib6pU3J8uYuRA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;573&quot; /&gt;&lt;/p&gt;

c


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.225130890052356&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfXj1V9qyvdqcF0Vzia9SfiaEplbqLXmhqGt7O2eRu3gfGtPMWT0JHjvug/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;573&quot; /&gt;&lt;/p&gt;

d

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7069444444444445&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfyJEYpPQ3EC2GzEyApRbIDF77FWTpGpflxiaI39LEibeWHtWYRFEXMDVA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

Q表格逐步迭代的过程， 先更新最靠近结尾的状态收益，在往起始状态推演， 最终得到每个状态行为下的未来收益。 图中的gamma是贴现率， 贴现率越低，状态间的收益变化越平滑

&lt;p&gt;&lt;span data-offset-key=&quot;9g54b-0-0&quot;&gt;然后深度强化学习从何而来?  现实生活中的状态太多，如围棋有3的361次方个状态，你的表格几乎可以布满银河系。所以你不能用表格法，这就自然的引入了机器学习。 因为机器学习，尤其是神经网络，本质就是函数逼近， 或者某种插值方法。 当表格大到填不完，我们就用一个函数来替代它，然后用神经网络来学习这个函数，这就自然的引入了深度强化学习。通过数据可以模拟函数，有了函数就可以把值函数的问题解决。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7584b-0-0&quot;&gt;DQN深度值函数算法最基本的深度强化学习算法，2015年发表的关于Alpha-Go的文章就是值函数法，策略梯度，蒙卡树，结合了CNN得到的算法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7Kf5srE5mpkXicBCPPe62EInLwW2df4NUe2LfCkHNLXXGUJGDFJTgmOvNw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfNeHtwFhQPmlW8CGHVarOdZLC28ibhicrsGAQCVGPpREY880th5CpT4Pg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;
&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-ratio=&quot;0.10966542750929369&quot; data-w=&quot;538&quot; class=&quot;&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; /&gt;&lt;/pre&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4peqn-0-0&quot;&gt;然而Alpha-Go再强大也不能解决所有问题，比如星际争霸，因为当下游戏画面当中的信息是不足以进行决策的，针对决策的信息并没有全部在一个画面当中呈现出来，所有就需要使用其它的方法。这也是我们的另一个主角RNN登场的机会。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;egipd-0-0&quot;&gt;我们用要一个最微小的例子来说明，还是那个走方格的问题，骷髅就是有危险的意思，我们希望走到有奖励的地方。 我只做一个小的改动将使得之前问题面目全非，之前的马尔科夫决策附加的条件就是当下的状态含有用来决策的所有信息，方格问题里， 这个信息就是位置坐标。而如果我没有位置这个信息， 取之以感知信息， 比如我只能感知我所在方格的周围两个方格有什么（下图中的骷髅或金币）。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;p5te-0-0&quot;&gt;注意如果我们处在下图灰色方格的区域（左右各一个），此时相邻的两个方格的情况是完全一致的（白色），也就是说我无法确定我是处于左边还是右边的灰色方格， 这导致无法决策正确的行为（左边和右边的正确决策是相反的！ 一个向左一个向右， 但是我无法确定是哪一个！）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfGqJpRTS8zD1tJUJv64Mww8RPn8BnrLA19hLx0hCoHRbhqA3xWYZpjg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;bedq2-0-0&quot;&gt;真实的生物天天生活在非完全信息环境下， 到处都是刚刚图中的灰色方格。 它们还得学到正确的行为， 那么它们是如何解决这个问题呢？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4eu1v-0-0&quot;&gt;主要有三种方法：一种就是策略梯度的方法，虽然所知状态和信息是不全面的，我们可以利用概率的方法来学习。当不知道该往左走还是该往右走时，随便走出一步，这样有百分之五十的概率得到最后要的奖励（一个驴子在沙漠里， 怎么都要选择一个方向走，不走一定渴死），利用直接学习的策略函数也就是Policy Gradient解决掉这个问题。这个方法大家可以看到效率是比较低的， 如果时间有限那就死了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ablgj-0-0&quot;&gt;另外一种方法就是引入记忆，你有了记忆， 事实上你就可以把一些不同时间点的信息凑成一个整体，你的信息当然比之前多。在上图走方格的例子里如果你知道你上一步是从左边边界来还是从右边边界来问题迎刃而解。 而最后一种就是加入精确的世界模型， 你的信息不全面， 但是我可以用我的大脑给世界建模， 这样一个不完全信息的世界在我脑补下就变成了完整的世界。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1kqh8-0-0&quot;&gt;引入记忆的方法，就很自然的引入了是RNN，它是类似模仿生物工作记忆的原理，信息放在记忆里面，将其作为决策的依据。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a8guj-0-0&quot;&gt;RNN的网络结构和前向神经网络差距并不大，只是多了一个时间转移矩阵，即当下隐藏的状态再到下一步隐藏状态的连接。这个连接实际上是传递当下的信息到下一刻的过程，这个传递过程中包含了记忆，我们可以把过去的信息往将来不停的迭代，于是神经网络细胞当中就含有过去很多时刻的记忆，其实就是生物的Working Memory。LSTM只是在RNN的基础上多了一记忆细胞层，可以把过去的信息和当下的信息隔离开来，过去的信息直接在Cell运行，当下的决策在Hidden State里面运行。加入RNN以后就把DQN变成了DRQN，然后就可以走一些非常复杂的环境。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7Kfk56k6bIcM1zT9IOewKLJcSlaQicOfuoXic0ddW5UfomCU2pHfO1bkgpg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

RNN， U是隐层连接




&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7Kficj2E96qrs1L6E5oZdIWTaJehwregGsOGTwSFojNIAtfcPSy2icTVPZQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

LSTM： 加入Cell


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7Kfhq3BYoVU1JL4Ss9LY1ApnLmJpAFsJ8TLI8lNWQze6bM7uOQiaZBkA8w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

DRQN框架 ， RNN网络是大脑，做出的决策影响世界， 世界又给这个大脑以反馈。

&lt;p&gt;&lt;span data-offset-key=&quot;bldb6-0-0&quot;&gt;下图是一个二维的迷宫，只能看到周围格的情况，就是刚刚描述那个问题的拓展版（相当于在沙漠里寻找水源， 你只能看到自己周围有什么，没有其他信息）。 需要我们做的是在很复杂的情况下搜索到目标在哪里，这就是一个导航的问题。左下角的红点就是起始位置，中心的红点是目标，最终学习得到的行为是： agent会直接走到墙上得到自己位置有关的信息， 然后从这里奔向目标，这就模拟了空间搜索的过程，RNN在这个过程里把不同时间点的信息拼接成一个整体。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.36944444444444446&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7Kf6ENuOB66bNK4fEicoK2p5jlXHffycrxUg2h6TouXtHEpLggNrHTOT9Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

二维游戏， 输入信息只有相邻格子的状态


&lt;p&gt;&lt;span data-offset-key=&quot;58pir-0-0&quot;&gt;RNN虽然有这种能力,  但是一旦空间复杂了它还是会蒙蔽， 因为空间是很复杂的，比如一个有很多屋子的宾馆， 这个时候就需要更强大的空间表示能力。这个东西还是可以通过学习诱导， 通常的做法是加入监督学习的成分，比如学习预测你在空间里的什么位置， 这个过程里， 模拟空间的能力会在RNN的动力结构里自动浮现出现。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c4n3b-0-0&quot;&gt;监督学习信号比较多而且目标明确，它可以预测走到哪里了，距离奖励还有多远，可以说给之前的强化学习添上了翅膀。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfkAwV0tlKQALyibDntKgGXZIx2ic3Epia7SNkBAuotaGwkpFRhoVvZCUiaw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

在2017年， 人们使用类似的方法让agent很好的打赢了doom   Lample, Guillaume, and Devendra Singh Chaplot. &quot;Playing FPS Games with Deep Reinforcement Learning.&quot; AAAI. 2017.


&lt;p&gt;&lt;span data-offset-key=&quot;bnbsa-0-0&quot;&gt;一旦加入监督学习，事实上我们就达到了刚刚说的第三种策略，引入世界模型， 只不过这个过程是一步步的而非一蹴而就。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9nka2-0-0&quot;&gt;一篇发表在《Nature》上的论文就把这个东西更加推进了一部。同样是监督学习，但是它在基础学习基础上诱导RNN（lstm）在原基础上形成新的结构，这个东西竟然惊人的和小鼠脑中的栅格细胞相近。这个栅格细胞实际是把空间当作一个很多六边形组成的蜂巢网络来表达，每个细胞对六边形网络的端点位置是敏感的，而不同的细胞对不同空间周期（网格边长）的网格敏感。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1n3eu-0-0&quot;&gt;这个方法的本质是建立空间的通用模型。 显然你的大脑不会给北京，上海，或者天津建立完全不同和分离的神经表达，必然的有一种空间语言来支撑所有的空间概念，而从一个地方到另一个地方，最底层的这个表达是不变的。这个东西可以看作模型之上之模型，这个东西正是这个栅格细胞。栅格细胞的每个细胞相当于一个不同空间周期的六边形网，通过组合这些六边形网，我们可以很容易的得到对空间相对位置的表达（很像傅里叶变化，每个栅格细胞是傅里叶变换的成分，被下游的位置细胞组合读取）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fjiht-0-0&quot;&gt;有了这个细胞的网络会有更强的在空间当中运行的能力，一个标志性的表现在于可以在复杂的空间当中抄近路。如果路径发生变化（比如一个门堵死了），就会找次优的目标，也就是说有一种动态规划的能力，即具有空间行走的智能。在RNN的基础上加入适当的监督学习，从而产生与生物细胞类似的结构，具有了空间表达能力。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.6062407132243685&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfDdAJGCsbTCDURlia6Q0PEwjbTbtg7kUKH48ByHCSxbtdGZkNNCIGwtg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;673&quot; /&gt;&lt;/p&gt;

事实上，我们不满足于上述的结果， 有没有一个方法， 让agent学习得到一个通用的空间表示？ 这样生物才能真正学到对一般空间特征的泛华能力。



&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4775641025641026&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7Kf6gVXmgZ6ypo63MbxVNXYU9TibpiaXPDAWd6fMAJ6uKRLbg1NsB43q6yw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;624&quot; /&gt;&lt;/p&gt;

生物对空间的表示， 是通过一个叫grid cell- place cell组合实现的， 这个组合的特点是上游的grid cell 提供一组特定空间频率的细胞， 然后通过一个线性组合（place cell)， 我们就可以得到对空间位置的描述。  这类似于一种对空间进行的傅里叶变换。

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfmRsudSrliapsM7RwqTKqqlFQF2vGgnAX4bgq2vAXpIlYfpDGWpdYvJA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

在使用了这种grid的空间表示后，最大的特点是增加了agent的灵活性， agent真正更够进行某种空间规划了

&lt;p&gt;&lt;span&gt;最终我们可以这样总结RNN在强化学习的潜力：  RNN，作为一个动力学系统， 本身表达了过去，现在和将来的联系， 这可以看作是部分的， 或者全部的世界模型。 而强化学习， 作为一个对未来收益的优化， 可以看作一个序列决策问题， 你对系统的过去现在和将来了解的越透彻，这个决策能力就越强， 因此RNN天生和强化学习有某种契合。 &lt;strong&gt;RNN的这个动力系统， 可以说部分的，或者全部的表达了世界模型，因此， 它非但是解决局部马尔科夫问题的利器，更在免模型和有模型的强化学习当中构建了一个桥梁。  &lt;/strong&gt;&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;相关阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383552&amp;amp;idx=1&amp;amp;sn=d10cf1e80137c23827a8a57f1605f3b5&amp;amp;chksm=84f3c941b38440577a067cd312d6683f193000781fe1da2310769b88b850e6eabf21a5a0626a&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;铁哥长文：神经导航简介 - 论deepmind最新文章&lt;/a&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383480&amp;amp;idx=1&amp;amp;sn=c4ca5c392e66df49f61bd0efb0997f9c&amp;amp;chksm=84f3c8f9b38441efe87e238ba063fe6e308e13a9d8460835429aa5093ab899136c9351a5ac52&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;从Q-learning的小游戏看阿尔法元技术&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;更深入了解，请阅读的文章如下：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Bakker B. Reinforcement learning with long short-term memory[C]//Advances in neural information processing systems. 2002&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;最早在强化学习里引入RNN的尝试，  主要是强调RNN可以解POMDP&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span data-offset-key=&quot;1788i-0-0&quot;&gt;Hausknecht, Matthew, and Peter Stone. &quot;Deep recurrent q-learning for partially observable mdps.&quot; &lt;/span&gt;&lt;span data-offset-key=&quot;1788i-0-1&quot;&gt;CoRR&lt;/span&gt;&lt;span data-offset-key=&quot;1788i-0-2&quot;&gt;, abs/1507.06527 7.1 (2015).&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这一篇接着2002的文章， 主要是承接了2015 deepmind 在DQN的突破，强调那些信息并不全面的Atari Game， 可以通过RNN（LSTM）得到性能突破&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span data-offset-key=&quot;2ksd8-0-0&quot;&gt;Mirowski, Piotr, et al. &quot;Learning to navigate in complex environments.&quot; &lt;/span&gt;&lt;span data-offset-key=&quot;2ksd8-0-1&quot;&gt;arXiv preprint arXiv:1611.03673&lt;/span&gt;&lt;span data-offset-key=&quot;2ksd8-0-2&quot;&gt; (2016)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;导航领域的牛文， 介绍了在RNN（LSTM）下的深度强化学习里如何进一步加入监督学习， 获得性能突破&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Wang J X, Kurth-Nelson Z, Tirumala D, et al. Learning to reinforcement learn[J]. arXiv preprint arXiv:1611.05763, 2016.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;小众的神文， wang xiao jing 大神介绍了一种基于RNN的强化元学习能力， 一种举一反三的能力。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span data-offset-key=&quot;3pvui-0-0&quot;&gt;Banino, Andrea, et al. &quot;Vector-based navigation using grid-like representations in artificial agents.&quot; &lt;/span&gt;&lt;span data-offset-key=&quot;3pvui-0-1&quot;&gt;Nature&lt;/span&gt;&lt;span data-offset-key=&quot;3pvui-0-2&quot;&gt; 557.7705 (2018): 429.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;最新的Nature文章，  介绍了通过监督学习引导RNN（LSTM）产生空间栅格细胞的能力&lt;/span&gt;&lt;/p&gt;

&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-ratio=&quot;0.10966542750929369&quot; data-w=&quot;538&quot; class=&quot;&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; /&gt;&lt;br /&gt;&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcd9g9aFjBooVn5U4PP1EDF3ugVJrLlia2ELtxHbXUJs7SUPtRaxmkUBHhx3jLciaHXpx1ABVYwYBu9w/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.665625&quot; data-w=&quot;640&quot; /&gt;&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;span&gt;作者许铁，微信：ironcruiser &lt;/span&gt;&lt;br /&gt;&lt;span&gt;法国&lt;/span&gt;&lt;strong&gt;巴黎高师&lt;/strong&gt;&lt;span&gt;物理硕士 ，&lt;/span&gt;&lt;strong&gt;以色列理工大学&lt;/strong&gt;&lt;span&gt;（以色列85%科技创业人才的摇篮, 计算机科学享誉全球）计算神经科学博士，巡洋舰科技有限公司创始人,   曾在香港浸会大学非线性科学中心工作一年 ，万门童校长好战友。&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;5.896&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJNQDiajMVL3arkx9IJnm10baZ1RjdLTN2KH6SKHZqnzyGO5K0G3dNOwg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;750&quot; /&gt;&lt;/p&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;br /&gt;&lt;/pre&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;span&gt;许铁的《机器学习和复杂系统》这本书用物理学视角来看机器学习，目前已经正式上架。同时，许铁已在万门大学已开设机器学习原理和深度学习原理两门在线课程，即将于9月开设强化学习的在线直播课程。感兴趣的可以点击下图的二维码咨询。&lt;/span&gt;&lt;br /&gt;&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdtdhcvoVrIAictVtkxkfXSiambAf4zOLtDxdYq09GEcgnRltUzicyJbr7EcCX7bYVV3GKuKXdlFdQqw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;4.717741935483871&quot; data-w=&quot;620&quot; /&gt;&lt;/p&gt;




</description>
<pubDate>Sun, 12 Aug 2018 19:28:35 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/sAxMjmtY0C</dc:identifier>
</item>
<item>
<title>选择选择-多少罪恶汝之名-说说拼多多的为什么能吸引回头客</title>
<link>http://www.jintiankansha.me/t/30XkpLhKty</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/30XkpLhKty</guid>
<description>&lt;p&gt;最近看了太多拼多多的帖子，忍不住想写点什么，为此第一次体验下这个产品，于是找到微信中的拼多多商城，却发现在首页上找不到搜索按钮，还需要在页面底部的按键才能跳转到搜索。我在搜索框中搜索华为P20，想看看这样一款高端手机在这个以山寨货出名的地方会是怎样，结果发现好多店家，销售量也都不错。更令我震惊的是竟然没有一个山寨手机出现在搜索结果中。难道拼多多变了？&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJXtlKepqfgH2wlJBaQTbaLndm68XiaJeg0AW5fibsNgZbktyCv0G0GTJA/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;1080&quot; data-cropy1=&quot;69.6774193548387&quot; data-cropy2=&quot;1811.6129032258063&quot; data-ratio=&quot;1.612962962962963&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJibI0kwzK4iccLI776v0uiaibN9E4Ficv0BtCfBS3xSQrzSXC9X4dOTuTEWA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1080&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我于是进入拼多多的手机频道，看到了熟悉的Vivi，VIVD等山寨品牌，也许是拼多多看我之前搜索过P20，给我展示的都是没有键盘的智能机。但在拼多多给出的价位上，明明是有正牌的手机的啊，不管是红米，魅蓝还是荣耀，都有五百元这个价位的产品啊，可为什么拼多多上，却找不到这些大厂出品的产品，却只有一堆山寨厂家了？&lt;/p&gt;

&lt;p&gt;假设我现在买手机的预算只有500，那我会去京东上搜索一下，然后发现不管选什么牌子，基本就一种2G+16G的配置，没有双摄像头，也没有好看的外壳，但是选一个大厂出品的，我却会对手机的质量有信心。然而若以我的心态来推演三亿拼多多用户的心态，那就是大错特错了。拼多多的用户要的是海量选项，以及由此带来的做出决策时又省了三五斗的成就感。&lt;/p&gt;

&lt;p&gt;假设拼多多决定整改，下架了所有手机频道里的山寨品牌，那一个已经习惯用拼多多的用户会发现在他能接受的预算里，在关键的参数上完全没有选择的空间。要是预算再低，恐怕就只剩下中心酷派这样的二线品牌可以选择。这时用户多半会不开心的。而创造一个不开心的用户，是和资本逐利的天性相悖的，所以不指望拼多多能够一夜间在舆论的压力下改性。至于我没有在拼多多的百元机推荐页见到一线品牌，多半也是因为销量不佳的原因。毕竟，在一片6+128的配置中，你老老实实的放出一个2+16G的，肯定是无法吸引到眼球的。&lt;/p&gt;

&lt;p&gt;但问题就出在这里，钱能够带来更多的选项，而穷人是最渴望决策的。CEO每天穿一样的衣服，是因为他们已经有足够多的事情等着他们做决定，但穷人每天能决定却很少。因此选择的机会对他们很有价值。但选择太多，是会降低认知能力的，而拼多多的用户，最好是不要太聪明的。&lt;/p&gt;

&lt;p&gt;拼多多的老用户，按照大树定理，多半都有过一些不愉快的经历。我若是在京东自营上买到假货，会从此再也不用京东，而转向亚马逊。数学建模中有一个很经典的传染病模型，可以用来对社交电商的扩展进行建模，模型显示，如果人群中有足够多的用户因为在拼多多上受骗后就再也不用这个平台，那么拼多多的扩展速度就不会这么快了。（这里就不展示我做蒙特卡罗模拟的过程了）&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;但为什么拼多多的用户却没有因为假货而离开这个平台了？你不能只用穷来解释。要知道拼多多做的不是一次性买卖，而是回头客生意。但为什么一个山寨货泛滥的地方也能吸引回头客了？一定是这里提供了再别处都无法提供的东西，而这就要从人性中去看。绝不仅仅是价格低，而是相比线下商店，拼多多提供了一些无法复制的独特的东西。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8105781057810578&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJLK3BG9cJreE95peRu6ycx5pqlGJSFee1mSEX6AbtUWwUTmjWcPfkrQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;813&quot; /&gt;&lt;/p&gt;
&lt;p&gt;从马斯洛的需求金字塔来看，拼多多的三亿用户，多半是挣扎在最下的一层的，但这不意味着他们的安全需求，社交需求和尊重需求不存在。相反，被压抑的需求，一旦得到满足，反而会带来吸毒式的不对称的快感。满足安全需求，就需要一个能够让自己选择的平台，而不是一个告诉你只有这一个选择的“家长”。&lt;/p&gt;

&lt;p&gt;想象一个底层的人，本来觉得自己没有什么人脉，结果用上了拼多多，发现竟然能找到那么多人帮自己砍价，他甚至能够和身边人比赛谁砍价的快，这就满足了Ta的社交需求和尊重需求。而这是我思考后对拼多多为什么能在山寨货横行的同时还吸引到回头客的解释。&lt;/p&gt;

&lt;p&gt;然而拼多多的盛行，却是对经济的可持续发展有害的。原因首先是促进山寨产品的盛行。假设拼多多上那些山寨手机厂商的销量全部都归了华为小米这样的大厂，那么华为在出货量上也许早就超过苹果了，而华为用于研发的投入也会相应增加。而山寨企业就算有了利润，也不会投入到正经研发中。没有研发也就没有原创性的技术进步，也没有产业升级的可能。&lt;/p&gt;

&lt;p&gt;更关键的是，拼多多培养了穷人心态，引用好友Kevin的句子，&lt;em&gt;“穷人马不停蹄地在琐碎的小事上节约金钱, 而富人则在是把有限精力投入在创造巨大价值和财富的事情上. ”。&lt;/em&gt;我看过讲旧上海的名媛是如何在建国后的一场场运动中保持尊严的帖子，也看过《京华烟云》中描述了木兰在穷下来之后是怎样优雅的生活的。贫穷并不意味着活得没有一丝尊严。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.3181019332161688&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJb8G44CoandnF6r7tVOBpDTdic9Cw5hSibs83gd3uoNL4wltCicrVlHBSg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;569&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我去年读过最令我感动的一本书是何江的《走出自己的天空》，作者来自一个乡下的穷人家，但作者的父母，舅舅还有爷爷却各个都活得有尊严，不会为了几块钱去挨个打扰亲友，也不会相信自己能占便宜用几百元买到几千元配置的手机。只有这样家庭培养出的孩子，才有可能走学术的途径跨越阶级壁垒。而用着拼多多的父母养出的孩子，只会想着走选秀，球星等娱乐业的路子来跨越阶级壁垒。前者能为社会创造出实际的价值，而后者的成果则不能转化为生产力。有阶级壁垒不可怕，成熟的时代下必然会出现马太效应。真正可怕的是打破阶级壁垒的路都无法为社会指明进步的方向。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.335907335907336&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJX845EMpn9fIClzSa12jibBdleTzTzaWH27ibEkJxIe8eZnUNy0UpBTkA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;259&quot; /&gt;&lt;/p&gt;

&lt;p&gt;接下来要讲一本重量级的书，也是我之前讲过的《稀缺》，这本书是一个哈佛经济学家和普林斯顿心理学家合著的书。书中的结论是当你拥有太少的东西，无论是时间，金钱，还是关怀时，你都会落入恶性循环，当你拥有太少时，你的视野也会狭窄，你会只关注短期利益，而忽略长期利益。书中用的词是“tunneling effect”，它损害的是你的认知能力，比如做出长远计划的能力。稀缺的另一种损害则是降低了你自我控制的能力，比如意志力，克制本能冲动的能力。&lt;/p&gt;

&lt;p&gt;在这本书中，有这样的实验。两位作者要求参与者想象维修他们的车需要300美元。然后两位作者让参与者回答一系列由电脑生成的问题，这些问题旨在衡量他们的逻辑思维、认知功能和解决问题的能力。所有的参与者，无论是富人还是穷人，都表现出了相似的智力水平。&lt;/p&gt;

&lt;p&gt;然而，当两位作者将维修费提高到3000美元并重复这项实验时，穷人的智力水平远远落后于富人，甚至有人的IQ降低了13点。一次实地考察，两位作者让印度的甘蔗农民在准备收割前（大多数人钱很少时）和刚刚收割后（大多数人这时钱很多时）做心理测试。 结果不出所料：收割后，农民们在测试中的表现要好得多。”&lt;/p&gt;

&lt;p&gt;这个实验也许可以解释拼多多从另一个角度部分解释为什么拼多多能吸引到回头客。贫穷让Ta们对低价商品的预期变得很低，让他们变得对低质量更加宽容，也让他们的记忆变得短暂。就算上月在拼多多上买到了假货，下月看到便宜货，还是会去拼单。&lt;/p&gt;

&lt;p&gt;在关于稀缺的书评中，我写到：那些穷怕了的五零后在过上富足生活后依旧非理性的捡旧东西。这个故事与想说的是贫穷心态的影响是长久的，难以轻易消除的。简单地分钱给穷人，穷人的“稀缺头脑模式”也会导致无法利用好这些福利以脱贫。一个合理的社会流动方式应当是，建立最基本的社会安全体系，同时保有社会竞争上升通道，资源入口向全社会开放，使得个人能保持正常思维，有尊严地奋斗。&lt;/p&gt;

&lt;p&gt;而拼多多的出现和盛行，却是建立在“心智稀缺”基础上的。可以说拼多多堵住了三亿用户的打破阶级壁垒的窄门，又在旁边开了一道宽门，说这里能通向山寨版的天堂。一个人若是习惯了拼多多，则只怕这辈子都难以摆脱穷人的心态，即使获得了扒着井边看一看的机会，也没办法跳出贫穷这口井。&lt;/p&gt;

&lt;p&gt;写道这里，我明白了为什么五环内的人们大都喜欢关注和拼多多有关的评论。不管这些评论的观点是什么，它们都是在暗中告诉自己，不要成为喜欢占小便宜的心智上的穷人，不要走那容易走的路，而要选择那人迹罕至的小路，也不要把自己看得太该死的重要。每个人在生命中都会面对要不要选择穷人的思维方式的时候，这时候你要记得刘玄德说过的，勿以善小而不为，勿以恶小而为之。对自己每一次的放低要求，都会让你离站着活下去远那么一点。&lt;/p&gt;

&lt;h3 class=&quot;t&quot;&gt;“自由,自由,多少罪恶假汝之名而行”。罗兰夫人在法国大革命的高潮中说出的话。放在当下这个大数据被滥用的时代，就是“选择选择，多少罪恶假汝之名而隐身”。平台给你很多的选择不见得是在帮助你，平台让你做决策变得容易，只需左右滑滑手指也不是在帮你，只有当你自己懂得限制选项，关注真正重要的事，才是自助者。而唯有自助者、天助之，也唯有自助者，人助之，这里的人不是熟人，而是陌生人。&lt;/h3&gt;
&lt;h3 class=&quot;t&quot;&gt;&lt;br /&gt;&lt;/h3&gt;
&lt;h3 class=&quot;t&quot;&gt;拼多多的消费习惯建立在且不断加强着东方传统中的不存在脱离群体价值的个体价值的观念。而西方的个体价值基于自我意识，基于灵魂的偏好（兴趣）。东方个体的满足难与家国脱钩，那种纯粹的觉得做数学题或音乐带来的满足大于威加海内或匡扶社稷的人几乎没有。基于个体意识，就有可能带来信息和物质双向流通的网络，从而带来非零和博弈的商业帝国。而只依靠血缘和强权，则仅仅有物质间低效的流通。这也许才是毛衣战深层次的原因。不要以为中国占据了压倒性的人口优势。只有摆脱了穷人心态的人多起来，人口优势才能从数字变成实际的生产力。&lt;/h3&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibceKaRtcsukX0ibwPsCTGAXYUIPPY1kl4YZ0wCibDkhvkDxcrmYdnHtiatDWNIPXFGNbCJELCynRr87dA/640?&quot; class=&quot;&quot; data-ratio=&quot;0.546875&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;愿每个人都能够摆脱心智上的稀缺！&lt;/p&gt;



&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381039&amp;amp;idx=1&amp;amp;sn=2f0f362dd09a4a7699d28b91156d339b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;心智上的贫穷--什么是时间稀缺，注意力稀缺，关注稀缺以及该如何面对&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381039&amp;amp;idx=2&amp;amp;sn=255bc318fdb62ef92109357393cf2690&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;时间穷人vs时间富人。&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
</description>
<pubDate>Sat, 04 Aug 2018 05:44:54 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/30XkpLhKty</dc:identifier>
</item>
<item>
<title>[原创]生命3.0-在亿年的尺度下审视生命的演进</title>
<link>http://www.jintiankansha.me/t/kPUk2qDTUi</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/kPUk2qDTUi</guid>
<description>&lt;p&gt;去年曾读过的&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382862&amp;amp;idx=1&amp;amp;sn=81ca9b6a200d3c3d6db8a9a7ef5ba3ee&amp;amp;chksm=84f3ca0fb3844319c8733d3c640f93cc16e52838d5e87a01d9be2e835617b3ccfcd43c5736f6&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;《Life 3.0》&lt;/a&gt;，最近出了中文版，在豆瓣上获得了8.6的高分，这并不稀奇。等到我拿到湛庐出版的寄的样书后，我发现整本书翻译的质量很高，全书语言流畅，而且对于极权主义等敏感话题也没有一丝一毫的删减。更难得的是，由于译者是原书作者的好友，因此在翻译的过程中，加入了一些新的内容，包括18年AI领域的新鲜事，因此可以看成原书的升级版。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.2361111111111112&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfhUgyozt82nF10H6fM7N2I5IGlDsZRUJdqAQt12lv0XVbolvR3V2qtkGfbvMlvgaPCqV3YIVgulw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1080&quot; /&gt;&lt;/p&gt;
&lt;p&gt;不过书还是要批判性的去读，因此下面是针都是这本书的一些不足。作者在书中将生命从分为动植物，人类，人工智能这三个阶段，判定的标准是能不能自主的决定自己的硬件和软件。按照这个标准，目前的AI无法自主的决定自己能够在什么样的机器上运行，只能改变模型的参数，这只能算自主的修改其软件，而且只是软件中顶层的一小部分。真正要做到生命3.0，需要的是脑机接口和人机融合，但书中却没有提到这个技术，这不应该简单的归结于作者知识上的欠缺，而应该考虑到是这本书的宏大语境使得作者并没有局限在具体的技术上，而是通篇从理论的可能性上在思考问题。&lt;/p&gt;

&lt;p&gt;这本书的开篇是一个短篇的科幻故事，讲的是强人工智能是如何一统天下的。当我拿到中文版，第二次阅读这个故事时，发现这个故事中的每一步都是读起来顺理成章的。但正如狗没有叫才是线索，这反而是有问题的。过去的农人说起皇帝，会说他们拿着金锄头在犁地，如今我们看待强AI，也会犯类似的错误。技术的进步会带来文化，经济，政治等多方面的改变，但作者却按照当下的逻辑，去构思强AI如何成为统治全球者。因此引言仅仅应对被看成是一个故事，用来引出这本书真正想讲的话题，如果强AI出现了，那该怎么办。而这正是这本书最多被谈起的部分，作者列出了强AI可能对各行业产生的冲击，穷举了强AI出现后的历史走向，如同科幻小说的开放结局。这些结局从好到坏，可以分成以下12种：&lt;/p&gt;

&lt;p&gt;1 AI做主导者的自由主义乌托邦&lt;/p&gt;
&lt;p&gt;2 AI成为人类的善意独裁者&lt;/p&gt;
&lt;p&gt;3 AI和人类平等共存的乌托邦&lt;/p&gt;
&lt;p&gt;4 AI作为人类幸福的守护者和指路灯&lt;/p&gt;
&lt;p&gt;5 AI主导着人类，也保护人类的安全&lt;/p&gt;
&lt;p&gt;6 AI奴隶着人类&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;7 AI征服了人类&lt;/p&gt;
&lt;p&gt;8 人类灭亡后，AI继承了人类的文化和记忆&lt;/p&gt;
&lt;p&gt;9 AI把人类当成宠物养在动物园里&lt;/p&gt;
&lt;p&gt;10 AI成为独裁者的工具&lt;/p&gt;
&lt;p&gt;11 AI消灭了人类对地球的影响，让人类回归田园时代&lt;/p&gt;
&lt;p&gt;12 AI 和人类一起走向共同灭亡&lt;/p&gt;

&lt;p&gt;但不同于羡慕皇帝用金锄头犁地的农人，当下的物理学告诉了我们智能的极限，不管是怎样的智能行为，都是要符合物理定律的。既然计算是由一个记忆状态向另一个记忆状态转变的过程，那计算就涉及到信息的存储和擦除，而这是需要消耗能量的，而且对一比特的信息，物理学能给出理论上拆除信息所需的最小能量值，正是基于这一点，物理学家能够在给定的能量限制下，给出智能机器所能达到的极限能力。而这一极限能力大的惊人，书中写道，“从数量级上来说，今天最好的超级计算机与1千克“终极计算机”之间的距离，比它和汽车转向灯之间的距离还要远，转向灯只能存储一个比特的信息，每秒只能在开和关之间转换一次。”&lt;/p&gt;

&lt;p&gt;在这本书的第六章里，作者在当下的理论前提下，做了一些看似是科幻小说，但实际是硬科学的推导。这一章很容易被误读，从而让读者忽略了这一章真正的精华。我觉得从这一章中，你可以看出，对于智能生物来说，越大越慢的诅咒永远都在，生命的结构不能太小，太小则布朗运动的影响会带来持久的耗散，而太大则会受制于信息传递的速率。打蚊子时你就会发现自己的反应要比蚊子慢，但蚊子却从来没有叮到你的眼睛，如果眼皮的运动还要像手那样经过大脑，那就不一定的。&lt;/p&gt;

&lt;p&gt;书中写道：对一个智能信息处理系统来说，身体变大是一件喜忧参半的事，会带来此消彼长的有趣均衡。一方面，变大意味着它可以拥有更多粒子，也就能带来更复杂的思想。而另一方面，如果它想要真正的全局思维，这反而会降低速度，因为信息需要花更长的时间才能传遍它身体的各个部分。这像级了大企业大帝国的反应迟缓，为此要做的是等级化的管理和放权，比如人类的本能就来的比有意识的反应要快，而这就是将身体基本机能的控制权下放到小脑的结果。这个和尺度相关的普遍规律告诉我们，即使是从一本完全都在谈论形而上的问题的书中，也能够学到做事的智慧。&lt;/p&gt;

&lt;p&gt;费米悖论，宇宙间的不同智能该如何合作，这也是这本书讨论的问题，因此这本书不应该只被看成是和AI有关的一本书，而应当看成是升级版的《生命是什么》，同样的是从第一性原理来解释生命现象，《生命3.0》谈论的是生命的目的，而不是生命的机制。而这就体现谈论目标和意识的第七和第八章。而这里最值得细说的是第七章，古代人们害怕神灵的惩罚，但这不同于当下人们害怕的强AI崛起，前者我们决定自己可以知道神灵的目的，但面对强AI，人类真正害怕的是强AI误解或者产生一些人类想不明白的目标。如何让一个给你聪明多的东西赋予目标，这需要理解目标的根源。&lt;/p&gt;

&lt;p&gt;而这就涉及到不在同学科的角度下，目标是如何生成的，从物理学的熵增和耗散结构，到生物学在有限理性的前提下将目标从耗散改为复制，再到心理学用感觉完成了对基因规定的规则的反叛。用书中的话说，就是目标导向行为的三个阶段：&lt;/p&gt;
&lt;p&gt;◦　第一阶段，所有物质似乎都在努力实现耗散的目标，即熵增；&lt;/p&gt;
&lt;p&gt;◦　第二阶段，其中一些物质拥有了生命，转而聚焦于子目标-复制自身；&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;◦　第三阶段，生物重新排列的物质越来越多，以实现自己的独特的目标。&lt;/p&gt;

&lt;p&gt;随着人类掌握的东西的增加，意味着即使不发生智能爆炸，很快，地球上大部分展现出目标导向性质的物质都会是设计出来的，而不是进化出来的。而所有设计出的东西的目标不仅变得越来越多样化，而且变得越来越复杂。对于强AI来说，人们害怕的到不是其是恶意的，而是害怕其由于过于高效的完成它的目标后造成的意外后果。很多的传奇故事中精灵帮助人们实现三个愿望时，第三个愿望都是“请收回前两个愿望，因为那不是我真正想要的东西。”但对于强AI来说，试错的成本则显得太高。&lt;/p&gt;

&lt;p&gt;接着作者列出了让AI明白人们正在想要的是什么，目前可行的技术方案，第一种是让机器从行为中推断目标，这个在&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383568&amp;amp;idx=1&amp;amp;sn=fb2a6857f18cf4de917111406ef9bd4f&amp;amp;chksm=84f3c951b3844047e23a00d5aca0f9acac4aa27580dabfa7096401a166b9ad3196b34de9d251&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;《The book of Why》&lt;/a&gt;中有过介绍，第二种是逆向增强学习”（Inverse Reinforcement Learning），即通过观察许多人在许多场景中的行为，包括真实场景、电影和书籍，最终构建起关于人类偏好的精确模型。但对于AI来说，这个窗口却不确定会有多长，在任何一个领域，要在AI越过了愚钝得无法理解你之后，又要在聪明的你无法理解之前，教会他目标是什么，还要如保证AI在未来的新坏境下能够保持这个目标。&lt;/p&gt;

&lt;p&gt;书中写道：一个4岁小姑娘憧憬着，当她长大了，变得更聪明了，她就要给自己建造一间巨大的姜饼屋，然后在里面坐上一整天，除了吃糖果和冰激淋以外什么都不干。和她一样，地球上的生命也可能会长大成熟，而不再执着于童年时期的兴趣。就好像一只制造了人类水平的通用人工智能的老鼠想要建一座奶酪城市，听起来十分荒谬。这说明了强AI在未来一定要改变其目标，但另一方面，即使是类似的幼儿园伦理，也好过完全没有限制的AI,每个设计机器的工程师都应该思考，机器在使用过程中，有哪些事情是可以做但不应该做的，然后考虑如何避免用户不管是出于恶意还是愚蠢，实施这样的行为。&lt;/p&gt;

&lt;p&gt;在关于目标的这一章的结尾，作者指出了当前的研究表明AI既不能回答如何让AI和人类有共同目标的能力，而伦理学又无法对生命体的终极目标取得共识。但作者还是给出了一些在大的尺度上值得优化的数量，在过去，对单一指标的过分关注曾经让人类犯下了很多的错误，因此对未来强AI要在宇宙尺度优化的值，就需要给予最大的讨论，而本文也将在这里结尾，最后是一个投票，看看读者一下那个目标应该是未来的强AI应当最追求的。&lt;/p&gt;



&lt;p&gt;总结一下，在这本书的后记中，作者写道自从他14岁知道核军备竞赛以来，就一直担心我们的技术力量比我们控制它的智慧增长得更快。而类似的担忧，我也曾在最近的几本读书笔记中有所体现，只是我关注的更多是现实中露出苗头的问题，是数据驱动而不是理论驱动的思考方式。前者的好处是能够给出至少部分的解决方案，而理论驱动则是从第一性原理出发，不会遗漏其他的可能性。最后列出几篇相关的读书笔记，以供参考。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383571&amp;amp;idx=1&amp;amp;sn=ef644a2553db450538e464b2f392da85&amp;amp;chksm=84f3c952b38440449eefdc210d28ff756cc8575cabe47b484203e328e22aca9e145386f7d0d4&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;读《数学杀伤性武器》-大数据的字里行间写着吃人&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383539&amp;amp;idx=1&amp;amp;sn=74d94ee995d08aa15ba2cb934d173e45&amp;amp;chksm=84f3c8b2b38441a4ee1b23499264ab51a790ded63105882117f6cfb56333e0638d6e60b84939&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;从前慢背后的道理-读《The efficiency paradox》&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383535&amp;amp;idx=1&amp;amp;sn=0334ba372055e17c64d5761272877f51&amp;amp;chksm=84f3c8aeb38441b8ac99bdeec48ec09497d7922a1771560a1633e6de5d44672070bb1762fd12&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;《十条立刻删除你的社交媒体账号的理由》这本书说了什么&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383531&amp;amp;idx=1&amp;amp;sn=4e4b7c6a901810b72427392dad370f69&amp;amp;chksm=84f3c8aab38441bc45707f038e04ecf99cbe83572a16400715cd919d1e006b6575747deb3cb1&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;AI会怎样改变战争的未来-读《Army of None》&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383455&amp;amp;idx=1&amp;amp;sn=13680fea0a8b47d974f8df07096d1492&amp;amp;chksm=84f3c8deb38441c8024432e52790c009b35ec5113e4d4597f6ea4c59d3c405ce627e80b69e53&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;WTF阅读笔记-一个老实人对未来的理性乐观&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;













</description>
<pubDate>Fri, 03 Aug 2018 16:50:21 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/kPUk2qDTUi</dc:identifier>
</item>
</channel>
</rss>