<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>深度学习常见面试题及回答</title>
<link>http://www.jintiankansha.me/t/ImKQUOj4DJ</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/ImKQUOj4DJ</guid>
<description>&lt;p&gt;1）什么是深度学习？&lt;/p&gt;
&lt;p&gt;回答：深度学习是机器学习的一个领域, 它专注于使用包含不止一个隐藏层的人工神经网络, 这部分由大脑中神经元的结构启发而生成。深度学习适用于计算机视觉、语音识别、自然语言处理等一系列领域。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcczHDIiaWOR2dPFBtH8RttCZLu0icHb4T1zLe45Lb7ib7pYYZTkwsbibdV5ia5bTIeqOtP3u1AoPpnFGAg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;490&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;2）什么是损失函数&lt;/p&gt;
&lt;p&gt;损失函数告诉我们神经网络的执行效果如何。我们在训练神经网络的目标是找到使最小化成本函数的神经元权重组合。&lt;/p&gt;

&lt;p&gt;举一个成本函数的例子, 考虑平均平方误差函数:&lt;/p&gt;


&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccQOduTGLky8BfGHuUzsqoNmCAIv7sAl7jcDtY1fsXyDDMUh2z8rb3VVoVMn7AslgIich8VFsU2TWg/0?wx_fmt=png&quot; class=&quot;&quot; data-ratio=&quot;0.2777777777777778&quot; data-w=&quot;270&quot; /&gt;&lt;/p&gt;
&lt;p&gt;该函数表示我们预期的结果Y^和实际的结果Y之间的差距，而这正是我们力图减少的。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcczHDIiaWOR2dPFBtH8RttCZLu0icHb4T1zLe45Lb7ib7pYYZTkwsbibdV5ia5bTIeqOtP3u1AoPpnFGAg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;490&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;3）什么是梯度下降&lt;/p&gt;

&lt;p&gt;梯度下降是一种深度学习的最常用优化算法, 通过迭代来修改参数值，以最小化成本函数。 在每个迭代中, 我们计算每个参数的成本函数的梯度, 并通过以下方法更新函数的参数。&lt;/p&gt;

&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccQOduTGLky8BfGHuUzsqoNPibNzWDPLC9YXIVlgZ3wibW2ibxO0nSXg3G2MGPqMmtlk5wE3hA2Htj3A/0?wx_fmt=png&quot; class=&quot;&quot; data-ratio=&quot;0.31797235023041476&quot; data-w=&quot;217&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccQOduTGLky8BfGHuUzsqoNRvQH8RicpD45RaicIySUksLXGvxngj1JnyNibqLnSOD6fsaYJw5551zVQ/0?wx_fmt=png&quot; class=&quot;&quot; data-ratio=&quot;0.9555555555555556&quot; data-w=&quot;45&quot; /&gt;是待优化的参数向量，&lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccQOduTGLky8BfGHuUzsqoN5UnAHKL68Qe6v6yp4Kb0BMHqtWfgKvBqPsODBsgyenPDRz85XuP04w/0?wx_fmt=png&quot; class=&quot;&quot; data-ratio=&quot;1.4166666666666667&quot; data-w=&quot;24&quot; /&gt;是学习率，&lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccQOduTGLky8BfGHuUzsqoNVYNdnDwZV0FR5SmgVgGASZaHK6qWLQs8esthDuHvAuhEo9TNKZ1vCg/0?wx_fmt=png&quot; class=&quot;&quot; data-ratio=&quot;0.723404255319149&quot; data-w=&quot;47&quot; /&gt;是损失函数。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcczHDIiaWOR2dPFBtH8RttCZLu0icHb4T1zLe45Lb7ib7pYYZTkwsbibdV5ia5bTIeqOtP3u1AoPpnFGAg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;490&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;4）什么是反向传播算法&lt;/p&gt;
&lt;p&gt;反向传播算法是一种用在多层神经网络中的训练算法, 它允许对梯度的变化进行有效的计算。&lt;/p&gt;

&lt;p&gt;反向算法可分为以下几个步骤:&lt;/p&gt;

&lt;p&gt;1) 通过网络向前传播待训练数据, 以产生输出。&lt;/p&gt;
&lt;p&gt;2) 使用目标值和输出值来计算由损失函数定义的误差的导数。&lt;/p&gt;
&lt;p&gt;3) 向神经网络中的前一层的输出上一步计算的误差的导数, 并依次传递到每一个更前的隐藏层。&lt;/p&gt;
&lt;p&gt;4) 根据上一步传递的误差的导数，依据链式法则，计算上一步的误差相对于当前神经元的权重的导数&lt;/p&gt;
&lt;p&gt;5) 更新权重。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcczHDIiaWOR2dPFBtH8RttCZLu0icHb4T1zLe45Lb7ib7pYYZTkwsbibdV5ia5bTIeqOtP3u1AoPpnFGAg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;490&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;5）解释以下三种梯度下降算法的变体: 批处理、随机下降和 Minibatch？&lt;/p&gt;


&lt;p&gt;随机梯度下降：仅使用单个训练示例来计算渐变和更新参数。&lt;/p&gt;
&lt;p&gt;批处理渐变下降：计算整个数据集的渐变, 并在每次迭代中只执行一个更新。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Minibatch 梯度下降：将数据集拆分为小批, 并对每个 mini-batch 执行参数更新。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcczHDIiaWOR2dPFBtH8RttCZLu0icHb4T1zLe45Lb7ib7pYYZTkwsbibdV5ia5bTIeqOtP3u1AoPpnFGAg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;490&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;6）什么是单个热编码（One Hot Encoding）？&lt;/p&gt;

&lt;p&gt;单个热编码用于对分类特征进行编码。我们为每个唯一的值创建一个单独的特性, 这样, 值就会彼此相同。&lt;/p&gt;
&lt;p&gt;例如, 让我们假设有一个称为颜色的功能, 它可以采用如下值: 红色、蓝色、绿色。&lt;/p&gt;
&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccQOduTGLky8BfGHuUzsqoNmbKQHcibPQDkiaRSCd2ypXzBDyeEVsOdLJ6dj4Eribiaje2CGKGleX70aQ/0?wx_fmt=png&quot; class=&quot;&quot; data-ratio=&quot;0.4980392156862745&quot; data-w=&quot;255&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcczHDIiaWOR2dPFBtH8RttCZLu0icHb4T1zLe45Lb7ib7pYYZTkwsbibdV5ia5bTIeqOtP3u1AoPpnFGAg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;490&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;7）激活函数的作用是什么？&lt;/p&gt;

&lt;p&gt;激活函数的目的是将非线性引入神经网络, 使其能够学习更复杂的函数。如果没有它, 神经网络将只能够学习线性函数, 它只能产生输入数据的线性组合。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcczHDIiaWOR2dPFBtH8RttCZLu0icHb4T1zLe45Lb7ib7pYYZTkwsbibdV5ia5bTIeqOtP3u1AoPpnFGAg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;490&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;8）提供一些激活函数的例子？&lt;/p&gt;

&lt;p&gt;Sigmoid函数&lt;/p&gt;

&lt;p&gt;Sigmoid函数也称为logistic函数, 它是连续的, 也容易计算导数。它将所有的实数压缩到范围 0到1之间。&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt; &lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccQOduTGLky8BfGHuUzsqoN9YkXsmoD1qNWmYIeicuB2rkic9AawXw9ENXxMzTlicOvdkuicJtfoBjNnQ/0?wx_fmt=png&quot; class=&quot;&quot; data-ratio=&quot;0.9087779690189329&quot; data-w=&quot;581&quot; /&gt;&lt;/p&gt;

&lt;p&gt;softmax&lt;/p&gt;

&lt;p&gt;Softmax 是一个泛化的Sigmoid函数，,当我们要处理多个类。所有输出值都在范围 (0, 1) ，其总和为 1, 因此可以将输出解释为概率。&lt;/p&gt;


&lt;p&gt;整流线性单元– ReLU&lt;/p&gt;

&lt;p&gt;ReLU 函数如果输入小于或等于 0,则输出0 否则输出输入的值, 我们可以把它们看成开关。 它可以免受梯度消失的问题, 它计算非常快。在卷积网络中的应用时比应用Sigmoid函数更有效。&lt;/p&gt;
&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccQOduTGLky8BfGHuUzsqoNaGwTKvEnPoFkWNEoias88Aib7Kzfe9tEqxJ5ZMbpUZicY6hr3ME0kLKRA/0?wx_fmt=png&quot; class=&quot;&quot; data-ratio=&quot;0.8791018998272885&quot; data-w=&quot;579&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcczHDIiaWOR2dPFBtH8RttCZLu0icHb4T1zLe45Lb7ib7pYYZTkwsbibdV5ia5bTIeqOtP3u1AoPpnFGAg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;490&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;9）什么是超参数, 提供一些例子？&lt;/p&gt;

&lt;p&gt;参数相对于模型参数不能从数据中学习, 它们是在训练阶段之前设置的。下面是常用的超参数。&lt;/p&gt;

&lt;p&gt;学习率（Learning Rate）&lt;/p&gt;

&lt;p&gt;它决定了我们要在优化过程中更新权重的速度, 如果学习率太小, 梯度下降可以缓慢地找到最小值, 如果它的太大梯度下降可能不会收敛。它被认为是最重要的 超参数。&lt;/p&gt;

&lt;p&gt;epoch&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一个 epoch。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;批处理大小 Batch Size&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;在不能将数据一次性通过神经网络的时候，就需要将数据集分成几批分开处理。批处理大小指&lt;/span&gt;一个训练批次中的样本总数。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcczHDIiaWOR2dPFBtH8RttCZLu0icHb4T1zLe45Lb7ib7pYYZTkwsbibdV5ia5bTIeqOtP3u1AoPpnFGAg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;490&quot; width=&quot;auto&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;10）什么是模型容量？&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;模型容量用来评价系统的解释是否具有足够的扩展性。较高的模型容量是可以存储在网络中的信息量越大。如果模型容量过大，就像一个记忆力极为精准的植物学家，当她看到一颗新的树的时候，由于这棵树的叶子和她以前看到的树的叶子数目不一样，因此判断这不是树；但如果模型容量太小，就像一个懒惰的植物学家，只要看到绿色的东西都把它叫做树。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcczHDIiaWOR2dPFBtH8RttCZLu0icHb4T1zLe45Lb7ib7pYYZTkwsbibdV5ia5bTIeqOtP3u1AoPpnFGAg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;490&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;11）什么是卷积神经网络？&lt;/p&gt;

&lt;p&gt;卷积神经网络, 也被称为 CNN, 是一种神经网络的架构,常用与图像识别，也可用在自然语言处理。其使用卷积在至少一个隐藏层。卷积层由一组过滤器 (内核) 组成。这个过滤器滑动切割整个输入图像, 计算点产品之间的权重，经过待训练的神经网络，即过滤器，将处理后的结果输出到下一层。 由于训练结果, 网络会学到能够检测特定功能的过滤器。例如识别三角形，直角。将多个卷积层堆叠起来，可以得到对整个图像的全局特征的提取。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccQOduTGLky8BfGHuUzsqoNtl7ibWvBdSicxiboVgAbf2VNwkc5sV82o1Wvoq788uHvU8n59IlHKqohw/0?wx_fmt=png&quot; class=&quot;&quot; data-ratio=&quot;0.6312292358803987&quot; data-w=&quot;602&quot; data-type=&quot;png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcczHDIiaWOR2dPFBtH8RttCZLu0icHb4T1zLe45Lb7ib7pYYZTkwsbibdV5ia5bTIeqOtP3u1AoPpnFGAg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;490&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;12）什么是自编码器 （autoencoder）？&lt;/p&gt;

&lt;p&gt;自编码器是一种人工神经网络的结构, 它能够在没有任何标签的情况下学习数据集 (编码) 的表示。他们通过重现输入的特征进行学习, 通常其神经网络的内部表示比输入向量具有更小的维数, 这样他们就可以学习有效的表示数据的方法。自编码器由两部分组成, 编码器试图将输入与内部表示相适应, 解码器将内部状态转换为输出。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccQOduTGLky8BfGHuUzsqoNdmHupMQJsGwqEnEn4icVsL4qBCN7hrhbZFmvYMW54SdGjC3tZBUICqw/0?wx_fmt=png&quot; class=&quot;&quot; data-ratio=&quot;0.7888040712468194&quot; data-w=&quot;393&quot; data-type=&quot;png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcczHDIiaWOR2dPFBtH8RttCZLu0icHb4T1zLe45Lb7ib7pYYZTkwsbibdV5ia5bTIeqOtP3u1AoPpnFGAg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;490&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;13）什么是dropout（辍学&lt;img data-src=&quot;https://res.wx.qq.com/mpres/htmledition/images/icon/common/emotion_panel/smiley/smiley_7.png&quot; data-ratio=&quot;1&quot; data-w=&quot;20&quot; /&gt;）？&lt;/p&gt;

&lt;p&gt;dropout是一种在神经网络中减少过拟合的正则化技术。在每个训练步骤中, 我们随机地关闭一些网络中的神经元, 这样我们为每个训练用例创建不同的模型, 所有这些模型都共享权重。这是对一组容量略低的模型求平均值的方法，类似随机森林。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccQOduTGLky8BfGHuUzsqoNrgG6yxZQNjo0D5SpE9zibZuhyrNb9I7ZFqyVYNJ9Fkxal1t7Ov4fuGQ/0?wx_fmt=jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.5342019543973942&quot; data-w=&quot;614&quot; data-type=&quot;jpeg&quot; /&gt;&lt;/p&gt;
&lt;p&gt;14）我们如何使用交叉熵来作为成本函数？&lt;/p&gt;

&lt;p&gt;交叉熵作为成本函数可用于分类,这是一个最自然的选择, 如果有一个Sigmoid或 softmax 的非线性在输出层。&lt;/p&gt;
&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccQOduTGLky8BfGHuUzsqoNFpibmlmYjSN79xFXy8HOnib0sZicoe3hMogZ7C9dD9ScTKs3qAQYKibphg/0?wx_fmt=png&quot; class=&quot;&quot; data-ratio=&quot;0.16584158415841585&quot; data-w=&quot;404&quot; /&gt;&lt;/p&gt;
&lt;p&gt;其中C为交叉熵，a是神经网络的输出权重，y是目标函数，n是训练样本的个数。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;欢迎关注&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382937&amp;amp;idx=1&amp;amp;sn=d4510592a837c44fe75393c2578698d8&amp;amp;chksm=84f3cad8b38443ce6adcd155a2c45ee7707b4a9d8e48c05728aa7e93862475832bf89617025f&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;巡洋舰的深度学习实战课程&lt;/a&gt;， 手把手带你进行深度学习实战， 课程涵盖机器学习，深度学习， 深度视觉， 深度自然语言处理， 以及极具特色的深度强化学习，看你能不能学完在你的领域跨学科的应用深度学习惊艳你的小伙伴，成为身边人眼中的大牛， 感兴趣的小伙伴可以点击阅读原文。 &lt;span data-offset-key=&quot;duudf-0-0&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt; &lt;/span&gt;&lt;/strong&gt; &lt;span data-offset-key=&quot;duudf-0-0&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;duudf-0-0&quot;&gt;&lt;strong&gt;下图是这门课程的思维导图。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span data-offset-key=&quot;duudf-0-0&quot;&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7935540069686411&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdrg83hduKplaOkZeV6icFIST2rojIm4SLJSQU8CgNia1AYmETxrSibzh5P6vPiaOffICZibFcNKfichRhw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1148&quot; width=&quot;auto&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;duudf-0-0&quot;&gt;原创不易，随喜赞赏&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;duudf-0-0&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9397590361445783&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd4icdSNQnx98Zicw7nnP3icPycqI1uRMvRxCezhYm4xQKdbiamvHVmC19a0o7YS03eqTrIL9QJS4wS4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;249&quot; width=&quot;auto&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;duudf-0-0&quot;&gt;更多阅读&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;duudf-0-0&quot;&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382986&amp;amp;idx=1&amp;amp;sn=13707c68d198bfa947eb90606ac99f05&amp;amp;chksm=84f3ca8bb384439d5c3ceeab35dd97373b1b461d4745ba2ab782998a8a3ea8722acca491f434&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;深度学习入门书单&lt;/a&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;duudf-0-0&quot;&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382963&amp;amp;idx=1&amp;amp;sn=53c1f03208ca7a41285566b9bf8aa83d&amp;amp;chksm=84f3caf2b38443e40428d245046814ac4ca8883c4403a88f68980b86e57b4c754759edf6eece&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;铁哥谈AI： 浅析阿尔法元之元&lt;/a&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Wed, 25 Oct 2017 21:01:22 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/ImKQUOj4DJ</dc:identifier>
</item>
<item>
<title>读《科技想要什么》，说AlphaGo Zero的大新闻</title>
<link>http://www.jintiankansha.me/t/Z1BaMSbLnO</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/Z1BaMSbLnO</guid>
<description>&lt;p&gt;一年前，和朋友闲聊，他说AlphaGo的下一代是BetaGo，结果人家从Master升级成了Zero，俩个神经网络也变成了一个。一年前，人们讨论AlphaGo是否理解了围棋，人们举例说，如果围棋的棋盘变成了20乘20，那柯洁闭着眼睛都能虐狗，由此来说明AlphaGo并不懂围棋。 但有了从零开始的zero，改变了棋盘的大小，给我3天的时间，照样能够虐人类。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;还有说狗狗不懂人类选手常用的围棋定式的。然而，从Zero的论文中看出来，恰好是那些学会定式慢的狗狗，棋下的好。不同版本的狗狗，最终都学到了围棋的定式，但若是过早的将选择定式的几率提高，那么会限制对未知模式的探索。从这个意义上来说，围棋的定式，何尝不是一种过拟合。然而只有等人类见识到了全新的风格，才能够意识到自己的不足。&lt;/p&gt;

&lt;p&gt;之前我们反驳狗狗不懂围棋的论据，都被AlphaGo Zero打破了，但人类最擅长的是改游戏规则。给定一个选手最多只能记住10万张棋谱，下十万盘棋，在这样的限制下，人类在当下的技术条件下，无论如何都是赢家。但这说明人类理解围棋，学习围棋比AI快吗？答案是否定的。加上这个限制，说明人类由于自身的限制，将围棋变成了一个有限游戏，人类围棋的规则其实是在只记住十万棋谱，下过不多于十万盘棋时下出最好的棋，而AI的出现，将围棋变成了一个无限的游戏。&lt;/p&gt;

&lt;p&gt;这里不是说围棋的可能性变成了无穷的，而是说计算机去掉了存储的限制，狗狗可以记住其下过的所有左右互搏的棋局，而蒙特卡罗树则将最后的输赢反馈到了最初的每一次落子上，解决了奖励时间的不确定性&lt;span&gt;，&lt;/span&gt;而CNN则负责从中推导出该怎么评估当前的局势，将你的落子和整个棋局的大背景结合起来。这样的框架，适合无限游戏的假设。而人类棋手一开始学习定式，则是人类在其自身的限制下最佳的学习方式。人不必盲目的崇拜计算机，说到底，狗狗和人玩的是不同的游戏，自然应该有不同的玩法。&lt;/p&gt;

&lt;p&gt;说完了狗狗，我们将视线拉远，猜猜狗狗下一步想做什么。人们对于未来的想象，常常是线性的。即使对于想象力丰富如著名的科幻作家凡尔纳，他设想的人类登月的方式也只是用大炮，然而真正改变世界的创新，最初都是不那么显著的，没有多少人会注意到。所以与其看狗狗下一步能做什么，不如看看狗狗做不了什么。与其看具体的行业，不如给出算命式的趋势预测。&lt;/p&gt;

&lt;p&gt;有了AlphaGo Zero的框架，所有的完美信息环境下的零和式，有限个选择的策略游戏，都已经成了AI的天下。再结合AI战胜人类的德州扑克选手，对于即使不是一个信息完全公开的游戏，只要可能出现的选项是已知而未知的只是出现的概率时，AI占据绝对优势，也是必然的了。留给人类的唯一机会，就是不要去玩零和游戏，不去玩有限的游戏了。&lt;/p&gt;

&lt;p&gt;有本书就叫《有限和无限的游戏》，书中写道，有限游戏以取胜为目的，而无限游戏以延续游戏为目的。有限博弈者在边界内游戏，无限博弈者以边界为游戏对象。有限游戏旨在以一位参与者的胜利终结比赛，每个有限游戏都是为了结束自身。矛盾恰恰就在于，所有有限游戏都是在对抗自身。在这本书中，作者提醒读者转化游戏观，认识到生活其实不应当被看成是有限的游戏。&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdXzh90N8e8w3xCAIpwsdUgG1YlshggBCmZO4lzsFk7U18nUx81SjnjsnSPU5byzFIWWML761cWUg/0?wx_fmt=jpeg&quot; class=&quot;&quot; data-ratio=&quot;1.4455445544554455&quot; data-w=&quot;303&quot; data-type=&quot;jpeg&quot; /&gt;&lt;/p&gt;
&lt;p&gt;凯文凯利在《科技想要什么》中对《有限和无限的游戏》十分称赞。书中写道：&lt;/p&gt;
&lt;p&gt;&lt;em&gt;进化、生命、思维和技术元素都是无限博弈。它们的博弈就是让博弈持续下去，让所有博弈者尽可能地长时间参与。为了达到这样的目的，它们像所有无限博弈一样戏弄游戏规则。进化之进化就是如此。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;生活中我们最爱的事物──包括生活本身──都属于无限博弈。当我们参与生活博弈或技术元素的博弈时，目标不是固定的，规则不明朗，并且一直在变动。我们如何进行下去？好的选择是增加选择。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;无限博弈的目标是保持游戏的进行：摸索游戏的所有玩法，增加各种博弈，召集所有可能的玩家，扩展游戏的意义，倾尽所有，无所保留，创建宇宙中不太可能发生的博弈。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;弄清楚了无限博弈的思路，就能够再一个大的框架下去看狗狗战胜人类的这件事，狗狗的出现，将围棋这种有限的游戏，转化成了一个无限的游戏。游戏的目地不再是为了获得世界第一，而是为了探索出围棋中的所有玩法。狗狗胜利的方式，是狗狗背后的Deep mind巧妙的改变了游戏规则，而不断改变游戏规则，正是无限游戏唯一的规则。&lt;em&gt;&lt;br /&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;说完了哲学性的话题，这篇小文就以AI取代职业这个话题来结束吧。很多人会问自己，你的工作中有百分之多少是可以被AI所替代的，然后据此评估AI是否会取代你的工作，但从有限和无限的游戏这个角度来看，你要问自己的问题的答案会是更主观的。你问自己在做的工作，究竟是该被当成有限的游戏，还是无限的游戏。&lt;/p&gt;

&lt;p&gt;一个餐厅的服务员的工作，是自动化程度很高的，但这并不意味着未来餐厅服务员这个职业会消失，米其林餐厅，国宴会，服务员的岗位总是在的。一个服务员若是将自己的工作时间看成是零和的游戏，那么她必然会担心自己的岗位丢掉，而她的担心会使得她忽略由于游戏边界的改变而带来的新机会。而如果她只是将餐厅的服务员工作看成是无限游戏中的一部分，那么在机器人取代一步步取代服务员的过程中，她会随之成长，找到新的坏境下新的角色。你的担忧反映的更多是你对自身角色的看法，而不是事实。&lt;/p&gt;

&lt;p&gt;原创不易，随喜赞赏&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9397590361445783&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd4icdSNQnx98Zicw7nnP3icPycqI1uRMvRxCezhYm4xQKdbiamvHVmC19a0o7YS03eqTrIL9QJS4wS4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;249&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;更多阅读&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382963&amp;amp;idx=1&amp;amp;sn=53c1f03208ca7a41285566b9bf8aa83d&amp;amp;chksm=84f3caf2b38443e40428d245046814ac4ca8883c4403a88f68980b86e57b4c754759edf6eece&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;铁哥谈AI： 浅析阿尔法元之元&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382959&amp;amp;idx=1&amp;amp;sn=4e3f03464db9bf33225d9293d69c5eb6&amp;amp;chksm=84f3caeeb38443f8aa9f050568b0368b512851c70c355d1690c07c3b1c92c82fe5d22f0b41ec&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;机器学习：增加更多就业岗位&lt;/a&gt;&lt;br /&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt; &lt;/p&gt;



</description>
<pubDate>Tue, 24 Oct 2017 05:34:13 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/Z1BaMSbLnO</dc:identifier>
</item>
<item>
<title>从纯种狗的悲哀，最终说起人的理性和感性</title>
<link>http://www.jintiankansha.me/t/hhY7uwSJid</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/hhY7uwSJid</guid>
<description>&lt;p&gt;对于哺乳动物来说，体型越大，相对来说，寿命也越长。小老鼠的活不了几年，大象却能活的和人类相当，所谓朝菌不知晦朔，蟪蛄不知春秋。这条幂律法则决定的规律，在今年的新书《scale》中有过详细的描述。然而生物体内的第一铁律，就是除了这条规律之外，其他的规律都有反例。宠物狗作为一种被人工选择干预严重的生物，不同品种的狗狗虽然基因上差距不大，但体型上的差异却有一个数量级那么大。从最大的大丹犬到最小的茶杯贵宾犬。自然界中再也找不到一个体型差距这么大的物质了。&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdXzh90N8e8w3xCAIpwsdUgs4R9wslHwcozIe3lCxSJtVNkflX1kxdIZQsL4aJD1AvD02kqpgjajA/0?wx_fmt=jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.6499162479061976&quot; data-w=&quot;597&quot; data-type=&quot;jpeg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于每一个养狗人来说，狗狗的寿命都是大家最关心的，狗狗的一岁约略等于人的7岁。而不同品种的狗狗，其寿命差距是很大的，体型越大，寿命越小。下面的图片来源于大样本量的长期调查，具有统计上的显著性，该结论已多次被不同的研究者重复出来，也符合养狗人士的常识。这里将狗狗按照体重分成了7 组，可以看到体重越轻的狗寿命越长的趋势是很明显的。这里的纵轴代表的是符合条件的这一组包含了几个品种，我们看到养的最多的还是20-40磅的小型狗。而将体重最小和最大的品种相比，其体重差了一个数量级，而寿命则差了接近一倍。&lt;/p&gt;

&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdXzh90N8e8w3xCAIpwsdUgianEdG351GL4WLrMPAEFuH2jj5VB1rwWcG6VPiaeWtMmBW8uaHRCrV7g/0?wx_fmt=png&quot; data-copyright=&quot;0&quot; class=&quot;&quot; data-ratio=&quot;0.7856&quot; data-w=&quot;625&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;来源：http://users.pullman.com/lostriver/weight_and_lifespan.htm&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;下面的这幅图展示了不同标准体重的纯种狗的平均寿命，图中黑色的点代表母狗，白色代表公狗，每个点上标记的为狗狗的品种，和人类一样，雌性的寿命长一些。科学家将这么多种狗狗的体重和寿命画在一起，是为了通过拟合找出图中的黑线和虚线，也就是说，不同狗狗的寿命和他们的体重呈线性关系。&lt;/p&gt;



&lt;p&gt;&lt;img data-type=&quot;gif&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdXzh90N8e8w3xCAIpwsdUgN5PfhW7SEsbkkWd4KGxBd6beoyKSnUfS3oOwh18BpQR9zXWWndBuCA/0?wx_fmt=gif&quot; data-copyright=&quot;0&quot; class=&quot;&quot; data-ratio=&quot;0.6954545454545454&quot; data-w=&quot;440&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;图片来源：参考文献1&lt;/span&gt;&lt;/p&gt;

&lt;p&gt; 看来不止人超重了不好，狗狗太胖了也不好啊。但这和我们最初看到的普遍规律是相反的，这是为什么了？而这正是我感兴趣的。搞清楚这个问题，意义重大。因为狗狗的寿命和人的寿命的调控机制是一样的。你也许听说过，控制卡路里的摄入能够增加寿命，但这对吃货来说实在太痛苦了。也许通过对不同寿命的狗狗的研究，我们能够找到延长人类寿命的更有效的方法。而且科学史也告诉我们，弄清楚一个反常现象背后的原因，会带来意想不到的新研究方向。&lt;/p&gt;

&lt;p&gt;细心的读者也许会问，上图说的都是纯种狗，那对于混血狗，又是怎样的了？遗憾的是，由于纯种狗在人工培育的时候，都会经历一段瓶颈时期，也就是其种群数量很少，近交系数较高，从而积累很多的有害突变，所以导致相同体重的纯种狗，都不如杂种狗的寿命长。这是为什么狗狗的寿命和体型之间的关系异常的第一种猜想，即人工选择带来的初始种群过小。&lt;/p&gt;

&lt;p&gt;另一组科学家猜想是人们在培育体重大的狗狗的时候，保留了携带疾病的基因，参考文献3 4中列出的例如扩张型心肌病，胃扭转和肠扭转阻塞，人为的选择，使得那些本该被自然选择压淘汰的基因保存了下来。&lt;/p&gt;

&lt;p&gt;但疾病的因素还不能解释这么明显的差异，参考文献5从不同品种的狗狗的免疫系统的角度，解释了这个问题，研究发现个头小的狗狗胸腺功能的减弱来的晚的多，从而使得个头小的狗狗在年老后也能保持较高的免疫力。&lt;/p&gt;

&lt;p&gt;研究（文献6）表明个头小的狗狗&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382243&amp;amp;idx=1&amp;amp;sn=a6e7e40ecfcfe59045f2fd58f4b3066f&amp;amp;chksm=84f3cfa2b38446b4f92c3c5b55160c0935ac79f57bb987cadff1626b2455a1f54b8042e45983&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;端粒&lt;/a&gt;（点击链接，查看端粒究竟是什么）相对较长，这也可以部分解释为什么个头小的狗狗寿命长。但这并没有指出为何个头小的狗狗端粒相对长一些。&lt;/p&gt;

&lt;p&gt;科学家接着研究的问题是基因上的那些差异和狗狗的寿命有关，根据参考文献1，结果找到的变异位点竟然都落在和体重差异有关的基因上，例如IGF1基因，这究竟是不是仅仅是一种巧合了，还是有更深一步的生物学机理了？IGF1基因编码的蛋白是调控生长发育的，根据文献7，在大狗的血清中IGF1的浓度要相对高一些，高的时间也要相对长一些，没办法，狗狗要长那么大需要更多的时间，但IGF1蛋白的另一个影响是一旦细胞开始凋亡，它就会加速细胞的凋亡，而细胞凋亡又和衰老和癌症密切相关。这就是说，&lt;strong&gt;大狗壮硕的体型背后，意味着他们更早的开始衰老的过程。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;根据参考文献2，不同品种的狗狗为什么寿命差距这么大，在科学界依旧是一个开放的问题。对于这个问题的答案，我的想法是这个问题等价于两个问题，一是大狗为什么相对短命，二是为何小狗长寿。对于小型狗来说，它们的寿命长，我猜想是来自于它们吃不饱，也就是人类为了维持他们的体型，定向的选择了吃的少长的小的那些，而限制卡路里则会增加寿命，这已被科学界反复验证了的。&lt;/p&gt;

&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdXzh90N8e8w3xCAIpwsdUgn85aFoHWap9LT9cicUqGuOag5JKib7Y6XPh8ytufLlvzHNL1NZjE4Dag/0?wx_fmt=png&quot; data-copyright=&quot;0&quot; class=&quot;&quot; data-ratio=&quot;0.6814814814814815&quot; data-w=&quot;1350&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图片来源 Scale : the universal laws of growth, innovation, sustainability, and the pace of life in organisms, cities, economies, and companies / Geoffrey West.&lt;/p&gt;

&lt;p&gt;而对于为什么大狗短命，我倾向与人工选择带来的非预期后果，即无意中保留的治病突变。文献3和4的研究只看了单基因病，但更多被无意中保留下来的是癌症的易感性等复杂疾病的突变。只是这些要研究清楚，难度更大。&lt;/p&gt;

&lt;p&gt;最后，我想开一开脑洞，说说狗狗的驯化史和他们的寿命和那些不养狗的人有什么关系。我看过一部很伤感的纪录片《纯种狗的悲哀》，其中记录了纯种狗的悲剧。回到本文的第一幅图，寿命最小（5-7岁）的狗狗品种中，有4种不是大型犬，正是这些反例，使得我认为观察到的体型和寿命的关系只是相关性而不是因果性，是其他更基础成因带来的副产品。&lt;/p&gt;

&lt;p&gt; 而这更基本的成因，从自私的基因的角度来看，不管是大丹，还是那个平均寿命5-6岁的可怜品种，他们的基因都是异常成功的。但整个物种的角度来看，这些纯种狗的基因由于单一性状的选择，而使得其可进化性基本消失，它们不能再做任何改变了，也没有任何未来变化的可能性了。只要这种品种存在，那么他们就只能这样了。&lt;strong&gt;然而进化，如果说其有目的，在于扩大整个系统的可进化性，或者说是选择的空间。&lt;/strong&gt;但对于纯种狗来说，其基因组合是固定的。缺少可进化性，就注定会导致有害的突变富集。&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;  再开一层脑洞，对于一个智慧生物来说，改造自然是必然的。这使得他迟早会走上以自己的偏好来决定身边生物的进化足迹的路途，被改变的生物，最终也会包括他自己。若不考虑人的感情和主观感受，就像人在培育纯种狗时，对狗狗缺少共情心，那么未来的人类，也会变得像狗狗这样。这么说看起来有些耸人听闻，但且听我演绎下这其中的逻辑。&lt;/p&gt;

&lt;p&gt;AI带来无用阶层，无用阶级不关心生产，只关注找到自我，但就像当下的以瘦为美，拼命节食所预示的，未来的无用阶级会以超过当下百倍的能力，为了单一的指标而改变自己，不论这个指标是智力，体力还是体型，反正未来即使有了问题，也有医学来保证。对单一指标的推崇，还反映在当前对某一营养元素的过度宣传上，例如Omega-3，花青素等。以及正在升温的全民健身，很多人真是为了健康还是一种盲目的对“公认西方肌肉美”的推崇？&lt;/p&gt;

&lt;p&gt;然而当人类开始了对单一形状的自我选择，那就会导致有坏突变的累积，就像我们在宠物狗中所看到的。而只有人类的主观感情以及人与人之间的共情心，才能打破这个魔咒。感情聚集了我们所有时间的全部体验，即包括能言说的，更包括了那些我们体会到却说不出的体验。感情的好坏，永远不会只关注避单一指标。纯种狗的培育者忽视了他们养育的狗狗的感情，才造成了当今纯种狗的悲哀。人类若是自我压抑，自我设限，那么正应了那句“后人哀之而不鉴之,亦使后人而复哀后人也”。&lt;/p&gt;

&lt;p&gt;为什么会有情感，会有意识，会有共情心，对于这样的问题，进化上一直没有一个好的解释。这篇小文也不打算回答这么巨大的问题。只是想因小及大，由对狗的人工选择反思当下和未来人类可能出现的对自己体型的选择，在表述担忧的同时提出解决的可能办法是人与人之间的共情心（理解、包容和接纳多样性）。共情让我们能推己及人，不以单一的评判标准去要求别人。而纯种狗的遭遇，对于这个过于推崇理性的年代，则是一种别样的提醒，没有共情心，只要理性，在长期上终究是不行的。&lt;/p&gt;

&lt;p&gt;扩展阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=402243130&amp;amp;idx=1&amp;amp;sn=cd7954ee7e0fc8dfe89634151a46365f&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;理性是激情的奴隶&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;原创不易，随喜赞赏&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9397590361445783&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd4icdSNQnx98Zicw7nnP3icPycqI1uRMvRxCezhYm4xQKdbiamvHVmC19a0o7YS03eqTrIL9QJS4wS4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;249&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;


&lt;p&gt;&lt;span&gt;参考文献1：&lt;span&gt;Jones, P. et al. Single-nucleotide-polymorphism-based&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;association mapping of dog stereotypes. Genetics&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;span&gt;179, 1033&lt;/span&gt;&lt;span&gt;–&lt;/span&gt;&lt;span&gt;1044 (2008).&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;参考文献2： Demographic history, selection and functional diversity of the canine genome. Nature Review &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;参考文献3 Ohta, T. Role of very slightly deleterious mutations in molecular evolution and polymorphism. Theor. Popul.Biol. 10, 254–275 (1976).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;参考文献4 Eyre-Walker, A., Woolfit, M. &amp;amp; Phelps, T. The distribution of fitness effects of new deleterious amino acid mutations in humans. Genetics 173, 891–900 (2006).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;参考文献5 Holder, A., Mella, S., Palmer, D. B., Aspinall, R. &amp;amp; Catchpole, B. An age-associated decline in thymic output differs in dog breeds according to their longevity. PLoS ONE 11, e0165968 (2016)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;参考文献6  Fick, L. J. et al. Telomere length correlates with life span of dog breeds. Cell Rep. 2, 1530–1536 (2012).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;参考文献7 Bonnett, B. N., Egenvall, A., Hedhammar, A. &amp;amp; Olson, P. Mortality in over 350,000 insured Swedish dogs from 1995-2000: I. Breed-, gender-, age- and cause-specific rates. Acta Vet. Scand. 46, 105–120 (2005)&lt;/span&gt;&lt;/p&gt;



</description>
<pubDate>Sun, 22 Oct 2017 07:55:19 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/hhY7uwSJid</dc:identifier>
</item>
</channel>
</rss>