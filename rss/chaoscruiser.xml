<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>你所不能不知道的CNN</title>
<link>http://www.jintiankansha.me/t/fJWHbpOdQ8</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/fJWHbpOdQ8</guid>
<description>&lt;p&gt;说起CNN，最初人们想到的都是某电视台，但等过几年，人们想起的多半是深度学习了。&lt;/p&gt;

&lt;p&gt;应该说， CNN是这两年深度学习风暴的罪魁祸首， 自2012年， 正是它让打入冷宫的神经网络重见天日并且建立起自己在人工智能王国的霸主地位。&lt;/p&gt;

&lt;p&gt;如过你认为深度学习是只能用来理解图像的，你就大错特错了， 因为它的用途太广了，上至文字，中有图像， 下至音频， 从手写数字识别到大名鼎鼎的GAN对抗学习， 都离不开它。&lt;/p&gt;

&lt;p&gt;不过要了解CNN，还是拿图像做例子比较恰当。一句话来说CNN图像处理的本质，就是信息抽取， 巨大的网络可以抽取一步步得到最关键的图像特征， 我们有时也叫自动的特征工程。&lt;/p&gt;

&lt;p&gt;CNN的建造灵感来自于人类对视觉信息的识别过程。 人脑对物体的识别的第一个问题是： 对应某一类对象的图像千千万， 比如一个苹果， 就有各种状态的成千上万状态， 我们识别物体的类别，事实上是给这成千上万不同的图片都打上同一个标签。&lt;/p&gt;
&lt;img class=&quot;content_image lazy&quot; data-ratio=&quot;0.6863468634686347&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceukHX94qAOhC6RWQ9zCzJI259ewy6Db9CG9xFdiaf6yOwWXVVwibWTmWa5PqZBk001mkUP0NiapFIOw/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;271&quot; width=&quot;271&quot; /&gt;CNN的灵感来自人大脑
&lt;p&gt;物理里管这种一个事物的结果与一些列的变化都没有关的特性，叫不变性， 比如如果你转动一个苹果任何一个角度它都是苹果， 这就是苹果有旋转不变性，但是数字6就不行， 如果你给它旋转特定角度它就变成9了， 它就没有旋转不变性。&lt;/p&gt;

&lt;p&gt;我们人通常可以无视这些变化认出事物来，也就是把和这种变化有关的信息忽略。如果我们对图像进行识别， 事实上我们的算法就要有人的这种本领， 首先让它学会什么东西与真实的物体信息是无关的。&lt;/p&gt;

&lt;p&gt;就拿数字识别举个例子吧， 一个数字是什么，虽然与旋转的角度有关系， 但与它在图片中的上下左右没关系， 我们管这种不变性叫平移不变性。&lt;/p&gt;

&lt;p&gt;解决这个问题，最粗暴的一个方法是制造很多的样本，比如把“1” 放在很多不同的位置，然后让机器在错误中学习。 然后穷尽所有的位置， 不过我相信没有人是这么完成对物体的识别的。&lt;/p&gt;

&lt;p&gt;那怎么办？CNN中的卷积正是这一问题的答案，因为卷积操作本身具有平移不变性（我知道听起来不明觉厉 ，请看下文）。&lt;/p&gt;

&lt;p&gt;卷积，顾名思义， “卷”有席卷的意思，“积“ 有乘积的意思。 卷积实质上是用一个叫kernel的矩阵，从图像的小块上一一贴过去，一次和图像块的每一个像素乘积得到一个output值， 扫过之后就得到了一个新的图像。我们用一个3*3的卷积卷过一个4*4的图像， 看看取得的效果。&lt;/p&gt;

&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.6847222222222222&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceukHX94qAOhC6RWQ9zCzJIU7jaZqUeskJShdjW1xADTKJvormu9ia4F2ZPPLcpe1YAaVic2935Om3Q/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;800&quot; /&gt;卷积的数学过程
&lt;p&gt;一个卷积核就像一个小小的探测器， 它的DNA是被刻录在卷积核的数字里的， 告诉我们它要干什么， 而卷积核扫过图片，只要它的DNA是不变的，那么它在图片上下左右的哪个位置看到的结果都相同， 这变是卷积本身具有平移不变性的原理。 由于这种不变性， 一个能够识别1的卷积在图片的哪个位置都可以识别1，一次训练成本，即可以对任何图片进行操作。&lt;/p&gt;

&lt;p&gt;图像处理领域，卷积早已有另一个名字 ， 叫做滤镜，滤波器， 我们把图像放进去，它就出来一个新图像，可以是图像的边缘，可以是锐化过的图像，也可以是模糊过的图像。&lt;/p&gt;

&lt;p&gt;如果大家玩过photoshop， 大家都会发现里面有一些滤镜，比如说锐化，模糊， 高反差识别这一类，都是用着类似的技术，这样的技术所作的事情是图像的每个小片用一个矩阵进行处理，得到一个画面的转换 。 我们有时候会说低通和高通滤镜 ，低通滤镜通常可以用来降噪， 而高通则可以得到图像的细微纹理。 你玩photoshop，玩的就是卷积，卷积核里面的数字定了， 它的功能也就定了。&lt;/p&gt;

&lt;p&gt;为什么这样做有效果了？因为图像的特征往往存在于相邻像素之间， kernel就是通过计算小区域内像素的关系来提取局部特征，可以理解为一个局部信息的传感器， 或物理里的算子。&lt;/p&gt;

&lt;p&gt;比如提到的边缘提取滤镜， 它所做的物理操作又称为拉普拉斯， 只有像素在由明亮到变暗的过程里它才得1， 其他均得0，因此它所提取的图像特征就是边缘。 事实上我们知道图像中的信息往往包含在其边缘，你给以一个人画素描， 一定能够完全识别这个人 。 我们通过寻找到信息的关键载体-边缘， 而把其他多余的信息过滤掉，得到了比第一层更好处理的图像， 大大减少了需要搜索图像的可能性。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.7507836990595611&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceukHX94qAOhC6RWQ9zCzJIKRaoRHBpH3zugrS3Fn9jIYaNRDrvHqsY0seaDXaxyrLEUqnSsmEJGw/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;&quot; width=&quot;638&quot; /&gt;卷积的边缘抽取过程
&lt;p&gt;常用于卷积的Kernel本质是两个： 第一， kernel具有局域性， 即只对图像中的局部区域敏感， 第二， 权重共享。 也就是说我们是用一个kernel来扫描整个图像， 其中过程kernel的值是不变的。这点就可以保证刚刚说的平移不变形。 比如说你识别一个物体， 显然你的识别不应该依赖物体的位置。 和位置无关， 及平移不变。&lt;/p&gt;

&lt;p&gt;那卷积如何帮你从不同的图形中识别数字1了？数字的尖锐的线条会让卷积的值很高（响起警报）。无论你1出现在图像中的哪一个位置， 我的局部扫描+统一权重算法都给你搞出来， 你用同一个识别1的卷积核来扫过图片，voila，任何一个位置我都给你找出来。&lt;/p&gt;

&lt;p&gt;那卷积和神经网络有什么关系了？答案是卷积扫过图像，每一个卷积核与图像块相乘的过程，都可以看作是一个独立的神经元用它的神经突触去探测图像的一个小局部，然后再做一个决策，就是我看到了什么或没看到什么。整个卷积过程， 不就对应一层神经网络吗？啊哈， 整个卷积过程相当于一层神经网络！&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.6708333333333333&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceukHX94qAOhC6RWQ9zCzJIICwDf8icCuSiaSsicdKTJXGtAlA0FzVYW7M3OXraicLGlNNIwp6fibjPZzQ/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;878&quot; /&gt;一个个小探测器一般的神经元
&lt;p&gt;刚刚说了卷积是一个能够对图片中任何位置的同一类信息进行抽取的工具， 那么我们还讲到我们除了抽取， 还要做的一个工作是，取出重要信息，扔掉不重要的，实现这一个的操作，叫做pooling&lt;/p&gt;

&lt;p&gt;但是大家注意，这个时候如果原图像是28*28， 那么从kernel里出来的图形依然是28*28， 而事实上， 事实是上， 大部分时候一个图像的局部特征的变化都不会是像素级。我们可以把局部特征不变形看做一个假设， 把这个假设作为一个数学公式加入到卷积层里帮我们过滤冗余信息， 这就是pooling所做的事情 -也就是扔掉你周边得和你长得差不多得那些像素。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5216666666666666&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceukHX94qAOhC6RWQ9zCzJIScj5wqr3aLUCNmVqYd19HuZQo1Kxcv0efULV2kcEz9EGduLibXh3lQQ/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; width=&quot;600&quot; /&gt;Max Pooling的数学过程

&lt;p&gt;Pooling的本质即降采样，以提升统计效率，用一个比较冠冕的话说是利用局部特征不变性降维 ，pooling的方法很多，常见的叫做max pooling，就是找到相邻几个像素里值最大的那个作为代表其它扔掉。&lt;/p&gt;

&lt;p&gt;这样经过从卷积到pooling的过程， 在识别1的任务里，我们可以验明在每个小区域里有没有存在边缘， 从而找到可能存在1的区域。 在pooling的终结点， 我们得到的是一个降低维度了的图像，这个图像的含义是告诉你在原有的图像的每个区域里是含有1还是不含有1， 又叫做特征图。&lt;/p&gt;
&lt;p&gt;好了，我们可以从一堆图片中识别出1了， 那么我们怎么搞定2呢？ 我们把2写成一个Z型， 你有没有思路我们如何做到这点？ 我们不能只识别竖着的线条，还需要识别横向的线条，记住，一个卷积层只搞定一个特征，如果你既要找竖线也要找横线， 我们需要两个不同的卷积层，并且把他们并联在一起，&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;1.0194444444444444&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceukHX94qAOhC6RWQ9zCzJINwr565frR5WvtFweXQLyV05un9UicfU8hZCv51TibnBYF8L5XE6BFgIw/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1414&quot; /&gt;手写数字识别
&lt;p&gt;然后呢？ 横线对应一张特征图， 竖线对应另一个张特征图， 如果要识别2， 你无非需要比较这两张特征图，看是否有哪个位置两个特征图同时发生了警报（既有横线又有竖线）。&lt;/p&gt;
&lt;p&gt;这个比较的过程，我们还是可以用一个卷积搞定（理由依然是平移不变性）！&lt;/p&gt;
&lt;p&gt;这个时候， 新的卷积层对之前并连的两个卷积的结果做了一个综合， 或者说形成了一个特征之特征， 即横向和竖线交叉的特征。&lt;/p&gt;

&lt;p&gt;这里把我们的理论可以更上一层路。 深度意味着什么？ 我们想一下， 要正确的识别一个图像，你不可能只看变，也不可能只看边角， 你要对图像的整体有认识才知道张三李四。 也就是说我们要从局部关联进化到全局关联， 真实的图像一定是有一个全局的，比如手我的脸， 只有我的眼镜，鼻子耳朵都被一起观察时候才称得上我的脸，一个只要局部，就什么都不是了。如何提取全局特征？&lt;/p&gt;

&lt;p&gt;从一个层次到另一个层次的递进， 通常是对上一层次做横向以及纵向的整合（图层间的组合或图层之内的组合或两者），我们的特征组合是基于之前已经通过pooling降低维度的图层，因此事实上每一个神经元决策的信息相对上一层都更多，我们用一个学术名词 – 感受野来表述一个神经元决策所涵盖的像素多少， 上一层次看到更多的输入神经元， 因此感受野看更多了 。 越靠近顶层的神经元， 所要做的事情就越接近全局关联。&lt;/p&gt;
&lt;img class=&quot;content_image lazy&quot; data-ratio=&quot;0.459214501510574&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceukHX94qAOhC6RWQ9zCzJITZicswJMbK60yq98JmMiab3YziaicmZAoib9PiaQxfAibX3xJrrpoqzaOcklg/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;331&quot; width=&quot;331&quot; /&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5631229235880398&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceukHX94qAOhC6RWQ9zCzJIvMPdtamMf7XC3G9EA0u14JtXWroPKsKHrQia4ibTJ9QELHPBfHEDRx7A/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;602&quot; width=&quot;602&quot; /&gt;越深，感受野越大， 表示越抽象
&lt;p&gt;这和物理学的一个基本方法--尺度变换有着异曲同工之妙（我们后面讲）， 也是提取全局信息的一个非常核心的办法，我管它叫级级递进法。 你一级一级的进行对画面进行降采样， 把图像里的四个小格子合成一个， 再把新的图像里四个小格子合成一个， 直到一个很大的图像被缩小成一个小样。每一层的卷积，都不是一个卷积，而是一组执行不同特征提取的卷积网络，比如我刚刚说的 不同方向的边缘沟成的一组卷积， 你可以想象后面有不同大小的角度组成的一组网络， 他体现了在一个空间尺度上我们所能够达到的特征工程。&lt;/p&gt;

&lt;p&gt;如此级级互联， 越靠上层感受野就越大。 整个CNN网络如同一封建等级社会，最上层的，就是君王，它是整个集团唯一具有全局视野的人，下一级别， 是各大领主，然后是领主上的风尘，骑士，知道农民（底层神经元）。&lt;/p&gt;

&lt;p&gt;我们把刚刚的全局换一个词叫抽象。深度卷积赋予了神经网络以抽象能力。 这样的一级级向上卷积做基变换的过程，有人说叫搞基（深度学习就是搞基），深一点想叫表征， 和人的思维做个比喻就是抽象。 抽象是我在很深的层次把不同的东西联系起来，CNN教会了我们事先抽象的一种物理方法。&lt;/p&gt;

&lt;p&gt;到目前为止， 我所描述的是都是一些人工的特征工程，即使网络在深，顶多说的上是深度网络，而与学习无关。我们说这样一个系统（mxnxpxz）， 我们要人工设计，几乎穷经皓首也可能做的都是错的。我们说， 这样的一个结构， 只能靠机器自己学，这就是深度学习的本质了， 我们通过几条basic假设（正则）和一个优化函数，让优化（进化）来寻找这样一个结构。 Basic假设无非图像的几个基本结构， 体现在几个不变形上，物理真是好伟大啊。&lt;/p&gt;

&lt;p&gt;深度学习的训练，就是计算机帮助人完成了机器学习最难的一步特征工程（特征工程本质就是基变换啊）。以前人类穷尽脑汁思考如何做图像识别， 是寻找人是如何识别图像的， 希望把人能用来识别物体的特征输入给计算机， 但是现在通过深度卷积，计算机自己完成了这个过程。&lt;/p&gt;

&lt;p&gt;卷积网络在2012 年的发展趋势， 大家可以关注几个方向：&lt;/p&gt;

&lt;p&gt;1， 更深的模型 ： 从AlexNet到VCG19 ，High way network 再到残差网络， 一个主要的发展趋势是更深的模型。 当你采用更深的模型，经常你会发现一些神奇的事情发生了。 当然网络的宽度（通道数量）也在增加。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.7507836990595611&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceukHX94qAOhC6RWQ9zCzJI4MgHzMgW8BpgRIUIwjaZEib5mXYpPxWQ3O4h9lIagdzXlFTDrnuJ6sQ/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;&quot; width=&quot;638&quot; /&gt;这只是最初级的CNN&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5626959247648903&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceukHX94qAOhC6RWQ9zCzJIfQsZBwppJXcjH5e8rWOViaefnsstpzajyFibZfshKjl6wZGibFrbl9Cicg/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;&quot; width=&quot;638&quot; /&gt;这也只是小菜一碟

&lt;p&gt;2， 更通畅的信息交换 : 深，带来的第一个问题是训练困难， 反向传播难以传递。 从残差网络， 到目前开始流行的Dense Network， 一个主要的发展趋势是不同层级间的信息的交换越来越通畅。 我们逐步在不同层之间加入信息的直连通道。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.6652777777777777&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceukHX94qAOhC6RWQ9zCzJI1wS4QUHOtwmDrfIsmNdeYURdybYTjibibicK33KIiaxasE0kEJ6TVrFMEg/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1504&quot; /&gt;Dense Network
&lt;p&gt;3， 与监督学习之外的学习方法的结合， 如迁移学习， 半监督学习， 对抗学习， 和强化学习。 后两者的有趣程度远超监督学习。&lt;/p&gt;

&lt;p&gt;4， 轻量化， CNN网络越来越深， 使得网络的文件动辄装不下， 这点使得CNN网络的轻量化部署成为重点， 我们希望在性能和能耗中取中。 一个很好的办法是对网络权重进行减枝，去掉不重要的权重， 另外一个是把每个权重的数据位数本身缩减，甚至是使用0和1表示， 虽然看上去我们丢失了很多信息， 但是由于巨大网络中的信息是统计表达的，我们到底损失多大还真不一定。&lt;/p&gt;

&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5319148936170213&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceukHX94qAOhC6RWQ9zCzJImjNnqhBaSKLHdmb6kBX4qSSvIZBSRlx6icey6W4xzuyhlTVZKBBNNDQ/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;658&quot; width=&quot;658&quot; /&gt;酷似生物过程的剪枝处理

&lt;p&gt;以上是CNN的小结， 不要以为图像处理与你无关，我刚刚说的其实一篇文章如果你把它转化为一个矩阵无非一个图像， 一段音频你给它转换成一个矩阵无非一个图像， 你看， 都可以和CNN挂钩。&lt;/p&gt;

&lt;p&gt;我想说，无论你是做什么的， 无论是苦逼的计算机工程师， 游戏设计师，还是外表高大上的金融分析师，甚至作为一个普通消费者， 你的生活以后都和CNN脱不开干系了 ， 预知更多情报还请关注：&lt;/p&gt;

&lt;p&gt;巡洋舰的深度学习实战课程， 手把手带你进行深度学习实战， 课程涵盖机器学习，深度学习， 深度视觉， 深度自然语言处理， 以及极具特色的深度强化学习，看你能不能学完在你的领域跨学科的应用深度学习惊艳你的小伙伴，成为身边人眼中的大牛。刚刚讲的方法都将在课程里详细展开。&lt;/p&gt;

&lt;p&gt;目前课程线下版本已经基本报名完毕（特殊申请可加一到两个名额）， 为了缓解众多异地学员的需求， 我们提出一个线上加线下的课程简版， 课程包括全部课程视频， notebook作业， 和一个课程模块的来京线下实践机会， 名额限5名，预报从速，&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有兴趣的可加 陈欣 微信 ： &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfCFEFrIh4CD3HnwF5a6d9gqk3NhIsY8ThV50SQ5ut1tiaRVs5lSnvrYRqWuJNgxJTXTZ9fu54micsw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;800&quot; /&gt;&lt;/p&gt;

</description>
<pubDate>Fri, 15 Dec 2017 16:55:06 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/fJWHbpOdQ8</dc:identifier>
</item>
<item>
<title>再谈江歌案——情绪正义，还是程序正义？</title>
<link>http://www.jintiankansha.me/t/Zzyt0kLNZc</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/Zzyt0kLNZc</guid>
<description>&lt;p&gt;&lt;span&gt;今天，江歌被害案在日本东京开始审理。这是一场万众瞩目的悲剧，我和你们一样，都在期待真相，期待正义女神的天平终于静止，利剑得以挥下。&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcccoM20SegGqedLD4G6kcv3q4acibACxvVVzYAP1yaU9MnTrBNvgOSYItzdRJWSZrJuDwv6smia656w/0?wx_fmt=jpeg&quot; data-copyright=&quot;0&quot; class=&quot;&quot; data-ratio=&quot;1.43875&quot; data-w=&quot;800&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;（古罗马正义女神Justitia，由justice一词转变而来）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;可是不要忘了，正义女神最大的特征是蒙眼，因为司法纯靠理性，不靠感官印象。耶鲁法学院教授Cover曾写过：“蒙眼不是失明，是自我约束”，接着另起一行：&lt;strong&gt;“程序是正义的蒙眼布”&lt;/strong&gt;。这句话已作为格言收入法学词典，每每被人引证。&lt;/p&gt;

&lt;p&gt;所以谈生死，写法律，必须抱有最大限度的客观与克制，才能执笔。我不是法官，没有生杀予夺的权力，也不是卫道士，无意口诛笔伐，随手钉耻辱柱。&lt;/p&gt;

&lt;p&gt;我们首先要达成一个共识：&lt;strong&gt;带有偏见和信息不全的报导，和动辄喊打喊杀的舆论，绝无助于正义的到来。&lt;/strong&gt;公正只取决于两个东西：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;真相，程序。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcczHDIiaWOR2dPFBtH8RttCZLu0icHb4T1zLe45Lb7ib7pYYZTkwsbibdV5ia5bTIeqOtP3u1AoPpnFGAg/0?wx_fmt=jpeg&quot; data-copyright=&quot;0&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;p&gt;关于前者，我不敢做任何叙述和推断——抛开当事人和目击者，不谈警察取证、司法鉴定和法院卷宗，没有任何人、机构和媒体，有资格在庭审结束以前，把自以为是结论，编给公众听。&lt;/p&gt;

&lt;p&gt;我不是针对谁，说的就是某些个媒体，请有一点法律常识，或者药店碧莲。请报导既有的事实，不要肆意煽动情绪，更不要妄下结论，盗图打码前请看下日期。&lt;/p&gt;

&lt;p&gt;一个月前，江歌案在朋友圈流传不下十个版本，一篇篇写的身临其境，好像作者们就在现场组团围观一样。麻烦你们，这辈子去过日本吗？去过现场吗？看过笔录吗？知道证人和律师叫啥吗？&lt;/p&gt;

&lt;p&gt;今天被告律师主张过失杀人和正当防卫，曾在你们栩栩如生的版本里出现过吗？&lt;/p&gt;

&lt;p&gt;舆论干预司法为何如此敏感？&lt;strong&gt;因为三人成虎、众口熏天，在信息不全、报导不实的情况下，极有可能以讹传讹，草菅人命。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcczHDIiaWOR2dPFBtH8RttCZLu0icHb4T1zLe45Lb7ib7pYYZTkwsbibdV5ia5bTIeqOtP3u1AoPpnFGAg/0?wx_fmt=jpeg&quot; data-copyright=&quot;0&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;p&gt;再说后者，程序正义，是刑事审判的保险锁，是法律具备公信力的保障。这四个字写起来简单，其含义和理论却极复杂，所以经常成为自媒体写作的雷区与盲区。&lt;/p&gt;

&lt;p&gt;Justice must not only be done, but must be seen to be done. 用中文说，就是事儿不仅要做的好，还要做得好。&lt;/p&gt;

&lt;p&gt;即：&lt;strong&gt;公正的结果（实体正义），与获得结果的方式（程序正义）是否公正，同等重要。&lt;/strong&gt;只有判决过程符合公正的要求，裁判结论才能得到人们的普遍认可。由此，法制的威信力才会高于法西斯束棒。&lt;/p&gt;

&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcccoM20SegGqedLD4G6kcv3ADK5NSlRbY4hXHZia2JqOxWNGQBGeWk40w0NYJ8CibmsUsqTWfAfiaCXg/0?wx_fmt=png&quot; data-copyright=&quot;0&quot; class=&quot;&quot; data-ratio=&quot;0.9982758620689656&quot; data-w=&quot;580&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;束棒代表权力和威信的意义一直延续到今天&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;拿江歌案来说，今天是开庭第一天，整个庭审和宣判将用时七日，一切才刚开始。陈世峰律师提出“刘鑫递刀，正当防卫，过失杀人”，只是单方面的陈述。&lt;strong&gt;单一言词证据，没有经过庭审质证，没有经过控辩双方交锋，没有得到主审法官的采信，是没有任何法律效力的。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;接下来，检方还要举证，证人还要出庭，证物、证词还要交叉验证、双方还要法庭辩论，在法槌落地之前，还有严格冗长的程序要走。此时我们能做的，是冷静和耐心等待，而非见风是雨、人云亦云。&lt;/p&gt;

&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcczHDIiaWOR2dPFBtH8RttCZLu0icHb4T1zLe45Lb7ib7pYYZTkwsbibdV5ia5bTIeqOtP3u1AoPpnFGAg/0?wx_fmt=jpeg&quot; data-copyright=&quot;0&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;p&gt;现在来说本案的核心：&lt;strong&gt;陈世峰是否构成正当防卫，能否逃脱死刑？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;实在抱歉，我是数学系的，只能从逻辑上分析。从今早庭审的供述来看，有两个核心，直接决定陈的行为是蓄意谋杀，还是激情过失杀人：&lt;/p&gt;

&lt;p&gt;（1）刀是谁的？&lt;/p&gt;
&lt;p&gt;（2）刀是谁先拿出来的？&lt;/p&gt;

&lt;p&gt;现在的证据证词十分矛盾：警方在陈的研究室发现同款刀壳，但“不能确定是否就是凶器的外壳”；陈世峰则称“刀是刘鑫从屋里递给江歌的”，在争夺过程中刺伤江歌致死。&lt;/p&gt;

&lt;p&gt;孰真孰假，还需等刘鑫出庭作证，综合警方调查结果和法医鉴定，才能逐渐让这块信息拼图得以完满，我们才有可能更接近真相。&lt;/p&gt;

&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcccoM20SegGqedLD4G6kcv3CG5zLo1hhm1QfkBf4ia9e5UqWSE8u2XibJNAb8Q5t3b46icFyicaY5xB3Q/0?wx_fmt=jpeg&quot; data-copyright=&quot;0&quot; class=&quot;&quot; data-ratio=&quot;1.4283333333333332&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;图/澎湃新闻&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;平心而论，江歌颈部11到12处刀伤，还能辩解为正当防卫，此为初刻拍案惊奇。别说人了，你能连续捅一只鸡5刀试试？一般人连第二刀都握不稳。我也奇怪，杀人犯为逃脱死刑的证词，网民们都能照单全收，可谓二刻拍案惊奇。&lt;/p&gt;

&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcczHDIiaWOR2dPFBtH8RttCZLu0icHb4T1zLe45Lb7ib7pYYZTkwsbibdV5ia5bTIeqOtP3u1AoPpnFGAg/0?wx_fmt=jpeg&quot; data-copyright=&quot;0&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;p&gt;之后的几天，我们都会持续关注本案的进展。写到这里，我只想说，无论如何，一个生命就这样逝去了，每天还有数不清的悲剧上演。有的我们听得到，更多的我们听不到，想到这里，悲从中来，不可断绝。&lt;/p&gt;

&lt;p&gt;听报导说江歌妈妈想要寻死，无奈这么多人的关注与希冀压身。我想这真是世间最悲痛的感觉：哀莫大于心死，寻死不能，却又生不如死。&lt;/p&gt;

&lt;p&gt;惟愿公道在人间，上穷碧落下黄泉。&lt;/p&gt;





&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfWnH5bhxesLmmviahTl8tWKugO5svyoeZw2RJdKe7n8VmibgPpdAoEibec4qD28qoQ4J7PwRW9CyTXA/0?wx_fmt=jpeg&quot; data-copyright=&quot;0&quot; class=&quot;&quot; data-ratio=&quot;0.5625&quot; data-w=&quot;1280&quot; /&gt;&lt;/p&gt;

</description>
<pubDate>Tue, 12 Dec 2017 02:55:38 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/Zzyt0kLNZc</dc:identifier>
</item>
<item>
<title>R 语言中的深度学习 Minst数据集下的聚类分析</title>
<link>http://www.jintiankansha.me/t/grFZrWsqEf</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/grFZrWsqEf</guid>
<description>&lt;p&gt;&lt;span&gt;本文为巡洋舰的深度学习实战课程 预科准备。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;说到深度学习，想到的都是python中的框架，例如tensorflow。然而R语言作为另一种数据科学家常用的工具，也不会缺席深度学习的盛宴的。今天为大家介绍一个来自R语言的包（package），名叫h2o，相比tensorflow，他的功能虽然不够强大，可能无法实现CNN，RNN这种特殊的结构，但却可以满足日常数据分析和建模的应用。&lt;/p&gt;

&lt;p&gt;这个包的安装简单，只需一行命令就可以搞定，不管是在notebook中，还是R的自带的运行坏境，只要输入install.package(&quot;h2o&quot;), 然后选择相应的镜像服务器，就可以安装完成了。h2o这个包功能强大，不止包含深度学习的模型，还包括工程界流行树模型，例如xgBoost，随机森林等，还包括自然语言处理中的word2vec，由于这个包是由由java实现底层代码的，其运算速度相对较快。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;而h2o这个包中和深度学习有关的函数是deeplearning这个函数，这个函数既可以用来训练常见的分类模型，用来做有监督学习；也可以用来做无监督学习，对数据进行聚类。而对于用深度学习的模型来进行聚类，则是这里要介绍的要点，也就是自编码器。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;无监督学习的目的是为了展示出那些没有带标签的数据之间的关系。一种常见的应用场景是数据降维，也就是将原来的高纬度数据投影到2维，从而使人们可以清楚的看到其间的关系。而用来评价数据降维的效果好坏，有一套常用的数据集，也就是分类中常用的MINST，手写数字的照片集，如下图。&lt;/p&gt;


&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceCicV64SRTQnx3BRq4k511ZtNQx32nw4EmRFvgoXnq4b9SEVKFCMXOMcELC0VWOV5pkg6d81BdcVA/0?wx_fmt=jpeg&quot; data-copyright=&quot;0&quot; class=&quot;&quot; data-ratio=&quot;0.7423423423423423&quot; data-w=&quot;555&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而分类的任务是给定一个数据，由算法模型来预测这个数字究竟是几，而聚类的任务，则是去看看能不能相同的数字放到一起，常用的聚类方法有PCA和tsne， 其中tsne是效果较好的一种方法。下图分别是用PCA 和tsne'做聚类得到的结果，不同的颜色代表不同的数字，我们看到各个类之间还是分得比较开的。而之所以tsne效果要好于pca，那是因为tsne能更好的处理非线性的变换，从而造成较少的信息丢失。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceCicV64SRTQnx3BRq4k511ZSJETWx8QNbWf8wGiae19HVib8FHxHNg3HiczzmquQ7zbmj5jjUg6HoQZg/0?wx_fmt=png&quot; data-copyright=&quot;0&quot; class=&quot;&quot; data-ratio=&quot;0.8079561042524005&quot; data-w=&quot;729&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceCicV64SRTQnx3BRq4k511ZvUWlHSMEyNhrNJGcxhSdyicl0UKOHIseECczRGS5LP5PuPpB9VWL6Rw/0?wx_fmt=jpeg&quot; data-copyright=&quot;0&quot; class=&quot;&quot; data-ratio=&quot;1.1&quot; data-w=&quot;1280&quot; /&gt;&lt;/p&gt;


&lt;p&gt;而深度学习，则天生适合处理非线性的情况，所以从理论上来说，使用深度学习，也可以做到较好的聚类。自编码器是一种神经网络的结构，其左右互博的思路，有些类似GAN。一个神经网络用来降低维度，另一个网络用来从降维的信息中恢复出尽可能多的信息，整个神经网络的目标是使得恢复出的数据尽可能的和原始的数据相类似。&lt;/p&gt;

&lt;p&gt;&lt;img data-backh=&quot;301&quot; data-backw=&quot;556&quot; data-w=&quot;820&quot; data-ratio=&quot;0.5414634146341464&quot; class=&quot;&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceCicV64SRTQnx3BRq4k511ZSsT4Ehrz0jSZU1G6LrK659ZGMdic3rFFIqXt1exz8ZWDHv0pAzYIfWA/0?wx_fmt=png&quot; data-copyright=&quot;0&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以上的神经网络中，输入的信息有4维，经过一层名叫编码器的神经网络的降维，变成了2维的，也就是中间那俩个隐藏层的输出，之后进过4个解码器中人工神经元的处理，由恢复成了4维，这就是一个最基本的自编码器。&lt;/p&gt;

&lt;p&gt;而将许多个单层的编码器和解码器按照顺序堆叠起来，就构成了更强大的深度自编码器，如下图所示。先是将5维变成4维3维再变成2维，之后再按顺序升维。而要获得降维后的表示，只要看看中间那俩个神经元的输出就好。&lt;/p&gt;

&lt;p&gt;好，我们来看看在R 语言的h2o包中如何实现深度自编码器。首先是导入包，&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;library(h2o) 这一句就行了，之后是导入训练数据&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;r plain&quot;&gt;mfile =&lt;/code&gt; &lt;code class=&quot;r string&quot;&gt;&quot;D:\R_Projects\MNIST\MNIST_DIGITStrain.csv&quot;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;r plain&quot;&gt;MDIG =&lt;/code&gt; &lt;code class=&quot;r functions&quot;&gt;h2o.importFile&lt;/code&gt;&lt;code class=&quot;r plain&quot;&gt;(path = mfile,sep=&lt;/code&gt;&lt;code class=&quot;r string&quot;&gt;&quot;,&quot;&lt;/code&gt;&lt;code class=&quot;r plain&quot;&gt;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;之后要做的就是去规定模型了，这里的函数有很多参数，每一个读者都可以在了解后观察其对模型效果的影响，这里大多数采取了默认值。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;r plain&quot;&gt;NN_model =&lt;/code&gt; &lt;code class=&quot;r functions&quot;&gt;h2o.deeplearning&lt;/code&gt;&lt;code class=&quot;r plain&quot;&gt;(&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;r spaces&quot;&gt;  &lt;/code&gt;&lt;code class=&quot;r plain&quot;&gt;x = 2:785,&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;r spaces&quot;&gt;  &lt;/code&gt;&lt;code class=&quot;r plain&quot;&gt;training_frame = MDIG,&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;r spaces&quot;&gt;  &lt;/code&gt;&lt;code class=&quot;r plain&quot;&gt;hidden =&lt;/code&gt; &lt;code class=&quot;r functions&quot;&gt;c&lt;/code&gt;&lt;code class=&quot;r plain&quot;&gt;(400, 200, 2, 200, 400 ),&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;r spaces&quot;&gt;  &lt;/code&gt;&lt;code class=&quot;r plain&quot;&gt;epochs = 600,&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;r spaces&quot;&gt;  &lt;/code&gt;&lt;code class=&quot;r plain&quot;&gt;activation =&lt;/code&gt; &lt;code class=&quot;r string&quot;&gt;&quot;Tanh&quot;&lt;/code&gt;&lt;code class=&quot;r plain&quot;&gt;,&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;r spaces&quot;&gt;  &lt;/code&gt;&lt;code class=&quot;r plain&quot;&gt;autoencoder =&lt;/code&gt;  &lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt; &lt;code class=&quot;r keyword&quot;&gt;TRUE&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;r plain&quot;&gt;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这里x 指出了使用数据中的第2列到第785 列，第一列是该行突袭对应的数字标签，这里不用，第二行是告诉模型使用的训练数据是之前导入的MINST，第三个参数指定了有多少可隐藏神经元，最初是400个，之后是200个，最后是2个，再进行升维，第四个参数是说模型最多训练400轮，第五个函数是每个神经元的激励函数是什么，这里是双曲正切Tanh函数，最后一个参数是指定这里是一个自编码器而不是分类器。&lt;/p&gt;

&lt;p&gt;接着我们来看看模型的效果，第一幅图是用自编码器画出的，第二副则是由h2o这个包中的线性聚类方法SVD画出的，明显看起来第一幅要比第二副好的的，当然自编码器要慢一些，需要350秒来完成训练，而SVD只需要6.5 秒。有兴趣的小伙伴可以自己试试不同的网络结构，看看会不会得出更好的效果。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;r plain&quot;&gt;train_supervised_features2 =&lt;/code&gt; &lt;code class=&quot;r functions&quot;&gt;h2o.deepfeatures&lt;/code&gt;&lt;code class=&quot;r plain&quot;&gt;(NN_model, MDIG, layer=3)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;r plain&quot;&gt;plotdata2 =&lt;/code&gt; &lt;code class=&quot;r functions&quot;&gt;as.data.frame&lt;/code&gt;&lt;code class=&quot;r plain&quot;&gt;(train_supervised_features2)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;r plain&quot;&gt;plotdata2$label =&lt;/code&gt; &lt;code class=&quot;r functions&quot;&gt;as.character&lt;/code&gt;&lt;code class=&quot;r plain&quot;&gt;(&lt;/code&gt;&lt;code class=&quot;r functions&quot;&gt;as.vector&lt;/code&gt;&lt;code class=&quot;r plain&quot;&gt;(MDIG[,1]))&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;r functions&quot;&gt;qplot&lt;/code&gt;&lt;code class=&quot;r plain&quot;&gt;(DF.L3.C1, DF.L3.C2, data = plotdata2, color = label, main =&lt;/code&gt; &lt;code class=&quot;r string&quot;&gt;&quot;Neural network: 400 - 200 - 2 - 200 - 4000 &quot;&lt;/code&gt;&lt;code class=&quot;r plain&quot;&gt;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceCicV64SRTQnx3BRq4k511ZMfRMrSCIUnicFBDJalm7f8VmE2lzQ7gzySGuwGib2ChZjvI684DmiaiaBA/0?wx_fmt=jpeg&quot; data-copyright=&quot;0&quot; class=&quot;&quot; data-ratio=&quot;0.9076376554174067&quot; data-w=&quot;563&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceCicV64SRTQnx3BRq4k511Z1iaLXNDyDakIGGlfuwVrkQw9hjxpHomnlu2vWXRnSSc9ASklzhgGfTA/0?wx_fmt=jpeg&quot; data-copyright=&quot;0&quot; class=&quot;&quot; data-ratio=&quot;0.6305506216696269&quot; data-w=&quot;563&quot; /&gt;&lt;/p&gt;

&lt;p&gt;总结一下，在R平台下，也可以进行深度学习，而且可以进行聚类和数据降维。自编码器作为一种常见的非监督学习框架，在未来也会有广泛的应用。&lt;/p&gt;

&lt;p&gt;&lt;span&gt;欢迎关注巡洋舰的深度学习实战课程&lt;/span&gt;&lt;span&gt;， 手把手带你进行深度学习实战， 课程涵盖机器学习，深度学习， 深度视觉， 深度自然语言处理， 以及极具特色的深度强化学习，看你能不能学完在你的领域跨学科的应用深度学习惊艳你的小伙伴，成为身边人眼中的大牛。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;原创不易，随喜赞赏&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9397590361445783&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd4icdSNQnx98Zicw7nnP3icPycqI1uRMvRxCezhYm4xQKdbiamvHVmC19a0o7YS03eqTrIL9QJS4wS4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;249&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

</description>
<pubDate>Thu, 30 Nov 2017 20:32:57 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/grFZrWsqEf</dc:identifier>
</item>
<item>
<title>聊聊我的R语言学习路径和感受</title>
<link>http://www.jintiankansha.me/t/kYXqJqwUMZ</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/kYXqJqwUMZ</guid>
<description>&lt;p&gt;&lt;span&gt;第一次接触R语言是我读研的时候，算到现在有5年多了。R语言可以算得上是我进入编程世界的启蒙语言，尽管在大学期间为了考试而被迫学习过计算机二级，但那真心是没有一丁点的兴趣可言。进入R的世界后，真的越来越喜欢，可以帮助我解决学术研究过程中的很多探索，最起码读研期间的所有小论文和毕业论文的案例分析都是通过R语言完成的。&lt;/span&gt;&lt;span&gt;&lt;strong&gt;工作后，数据分析、可视化和数据挖掘的落地更是通过R语言帮我实现的，她对我的学习和工作起到了很大的帮助&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2015&lt;/span&gt;&lt;span&gt;年2月份毕业开始了一份正式的工作，半年多后突然产生一个想法，就是把自己在工作中的所学所用通过公众微信号（lsxxx2011）记录下来，并给自己的公众号起名“每天进步一点点2015”。到今天已维护了近2年半的时间，积累了近160篇文章，也结识了很多对数据分析、挖掘感兴趣的朋友。曾经有好多朋友都向我问起过一个类似的问题：&lt;span&gt;&lt;strong&gt;“我是***，对数据分析很感兴趣，但目前是R语言的小白，也自学了一段时间，但总感觉使不上力，有没有比较好的学习方法？”&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;那&lt;/span&gt;&lt;span&gt;就抽空来讲一讲我学习R语言的过程，希望对处于迷茫或困惑的朋友起到一点帮助。接下来，我就从以下几个方面来讲讲我学习R语言的经历：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;基础篇&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;       &lt;/span&gt;&lt;span&gt;我接触R语言的第一本书是薛毅老师的&lt;span&gt;&lt;strong&gt;《统计建模与R软件》&lt;/strong&gt;&lt;/span&gt;，这本书个人感觉非常棒，至少有这&lt;span&gt;&lt;strong&gt;三方面的优点&lt;/strong&gt;&lt;/span&gt;：&lt;span&gt;&lt;strong&gt;首先，&lt;/strong&gt;&lt;/span&gt;该书将统计学里面的基础知识和理论都作了比较详细的剖析；&lt;span&gt;&lt;strong&gt;其次，&lt;/strong&gt;&lt;/span&gt;也详细介绍了R软件本身的基础知识，如语法、数据结构、数据读取、控制流等；&lt;span&gt;&lt;strong&gt;最后&lt;/strong&gt;&lt;/span&gt;但也是最重要的，运用R软件（很多情况都不是直接调包）将统计学模型的原理作了相应的代码实现，包括统计检验、线性回归、方差分析等。这本书我看了至少2遍，但&lt;span&gt;&lt;strong&gt;对于R语言的初学者并不建议将这本书当作学习的第一本书&lt;/strong&gt;&lt;/span&gt;。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;       &lt;/span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;《R语言实战》&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;，这是一本非常好的初学R语言的资料，而且该书在今年也更新到了中文版的第二版。之所以首推这本书作为初学教程，是因为这本书偏实战，而非理论，你可以通过这本书&lt;span&gt;&lt;strong&gt;提高R软件的使用技能&lt;/strong&gt;&lt;/span&gt;，该书包括R软件编程基础、可视化作图和统计建模等内容。这本书我至少也看了2遍，书中包含了各式各样的R软件包和函数的使用，要想记住这些内容还不得不多敲代码。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;       &lt;/span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;《R语言数据操作》&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;，是一本页数非常少的书籍，但都是精华。书中没有任何关于统计学相关的内容，都是在讨论&lt;span&gt;&lt;strong&gt;数据集的管理，如缺失值处理、数据读取、日期数据的处理、正则表达式、数据汇总&lt;/strong&gt;&lt;/span&gt;等。通过阅读和学习这本书，相信在数据预处理这块会对你有很大的提升，我曾经针对这本书的每一章内容作了整理，并分享到公众号中，对我本身而言，也是一个提升的过程。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;       &lt;/span&gt;&lt;span&gt;当然，网上也有很多其他初学者资料，如《153分钟学会R》、《R语言经典入门_2012》、《R语言初学者指南》、《RCookbook》等。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;可视化篇&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;       &lt;/span&gt;&lt;span&gt;在学习R软件的同时，你肯定会碰到关于使用R软件来完成数据的可视化操作，确实，R软件也是一款非常棒的可视化工具，包含了各种各样的可视化包，如graphics、lattice、plotrix、plotly、ggplot2等等。关于这一块内容的学习，你可以根据上面提到的书籍《统计建模与R软件》、《R语言实战》、《R Cookbook》等初识一下可视化，了解R软件基础包中是如何实现数据可视化（尽管每一个绘图函数都包含很多参数），这样会对你的绘图思维带来好处。如果要推荐几本数据可视化的书，我会强烈推荐&lt;span&gt;&lt;strong&gt;《ggplot2：数据分析与图形艺术》&lt;/strong&gt;&lt;/span&gt;和&lt;span&gt;&lt;strong&gt;《R数据可视化手册》&lt;/strong&gt;&lt;/span&gt;。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第一本书是ggplot2包的开发作者所编著的，在R中ggplot2也是非常火爆的可视化包，书中详细介绍了如何利用图层的思想绘制完美的统计图形，有系统的绘图组件，如geom_*函数、stat_*函数、sacle_*函数及theme类函数。这本书更多的是&lt;span&gt;&lt;strong&gt;从绘图思想和理论出发&lt;/strong&gt;&lt;/span&gt;，介绍ggplot2包的庞大功能（当然也有很多绘图案例）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第二本书则弥补了第一本书的轻松感，说实话，我读《R数据可视化手册》的次数要比《ggplot2：数据分析与图形艺术》多一些，因为该书&lt;span&gt;&lt;strong&gt;通过非常多的例子来对比基础包与ggplot2包在绘图方面的差异&lt;/strong&gt;&lt;/span&gt;，既让我巩固了基础包的绘图方法，也提升了我对ggplot2包的浓厚兴趣，目前工作中涉及到基本统计图形（如条形图、直方图、折线图、散点图等）的绘制，我都是优选ggplot2包来完成。当然，我也把ggplot2包中常见的绘图功能作了整理，并以系列的形式分享在了公众号中。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;谢益辉整理的《现代统计绘图》、肖凯的PPT《30分钟学会ggplot2》及英文版的《R语言之多变量数据可视化--Lattice》都是比较好的可视化学习材料。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;数据挖掘篇&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;       &lt;/span&gt;&lt;span&gt;上面介绍的这些书更多的都是从数据分析角度，如果你想提升自己，研究一些目前比较火的数据挖掘知识，R语言同样提供了出路。这里介绍几本我看过的相关书籍，它们是&lt;span&gt;&lt;strong&gt;《数据挖掘：R语言实战》&lt;/strong&gt;&lt;/span&gt;、&lt;span&gt;&lt;strong&gt;《机器学习与R语言》&lt;/strong&gt;&lt;/span&gt;、&lt;span&gt;&lt;strong&gt;《R语言数据分析与挖掘实战》&lt;/strong&gt;&lt;/span&gt;、&lt;span&gt;&lt;strong&gt;《数据挖掘概念与技术》&lt;/strong&gt;&lt;/span&gt;和&lt;span&gt;&lt;strong&gt;《统计学习方法》&lt;/strong&gt;&lt;/span&gt;。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;       &lt;/span&gt;&lt;span&gt;前面三本书都是基于&lt;span&gt;&lt;strong&gt;R语言工具的数据挖掘实战&lt;/strong&gt;&lt;/span&gt;，内容包含了常见的数据挖掘方法，如Knn、Logistic回归、决策树、朴素贝叶斯、神经网络、SVM、随机森林、Bagging、Adboosting、K均值聚类、密度聚类、EM聚类、关联规则等，每一种挖掘方法都配备了详细的数据案例，甚至也会解释挖掘函数中重要参数的含义和使用方法。当你读完这几本实战类的书，&lt;span&gt;&lt;strong&gt;你就会发现&lt;/strong&gt;&lt;/span&gt;通过R语言的调包来完成&lt;span&gt;&lt;strong&gt;数据挖掘是特别简单&lt;/strong&gt;&lt;/span&gt;的。此时，我相信你一定会对数据挖掘的理论感兴趣（最起码，你在实战数据挖掘的过程中你会反问自己为什么这个参数这样调整会更好？），因为你或多或少的感觉到你遇到了瓶颈，你想更往上走一步，但又力不从心。此时，你需要的是数据挖掘理论方面的材料来给充实自己，给自己补充能量。那这就是我接下来要跟你介绍的另两本理论书籍。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;《数据挖掘概念与技术》这本书虽说是理论方面的书籍，但&lt;span&gt;&lt;strong&gt;读起来还是蛮轻松的&lt;/strong&gt;&lt;/span&gt;。书籍中的挖掘部分，首先介绍挖掘方法的概念和理论知识，然后通过某些数据集来完成手工计算的过程，对于读者来说，具有代入感，学习起来也会比较有劲。这本书相对于《数据挖掘导论》来说会稍微难一点，如果你对自己没有信心，可以先看看《数据挖掘导论》这本书。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;《统计学习方法》是一本&lt;span&gt;&lt;strong&gt;完全偏理论的书籍&lt;/strong&gt;&lt;/span&gt;，包含了很多算法的推理过程，如knn算法、贝叶斯算法、决策树算法、支持向量机算法等，这些推理&lt;span&gt;&lt;strong&gt;对读者的数学知识&lt;/strong&gt;&lt;/span&gt;要求比较高，如线性代数、微积分、概率论等。如果你能够静下心来对整本书的推理进行一遍梳理（哪怕是抄一遍），我相信你一定会受益匪浅，对数据挖掘的理解会更加深刻。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;其他学习资源&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;       &lt;/span&gt;&lt;span&gt;除了上面所提及的书本（几乎每本书我都看了至少两遍），网络上还有更多的习资源，如视频网站、论坛、博客、社区等。接下来我跟大家介绍几个我常去的免费资源：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;统计之都&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：&lt;/span&gt;&lt;span&gt;https://cosx.org/&lt;/span&gt;&lt;span&gt;，里面有非常丰富的高质量文章，包含数据挖掘、可视化、统计学、分析报告等；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;统计之都论坛&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：&lt;/span&gt;  &lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt; &lt;span&gt;https://d.cosx.org/&lt;/span&gt;&lt;span&gt;，如果你有学习上的疑问，你可以在论坛上提问，热心的网友也会给出他们的答案；同样，你也可以看别人提出的问题，尝试回答或查看别人的回复，进一步提高自己的R语言技能；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;R&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;语言官网&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：&lt;/span&gt;&lt;span&gt;https://www.r-project.org/&lt;/span&gt;&lt;span&gt;，官网中有一些不错的学习手册，同时也会定期更新R语言包，截止到写稿目前已有11,899个第三方包，使用R真的就是站在巨人的肩膀上；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;经管之家&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：&lt;/span&gt;&lt;span&gt;http://bbs.pinggu.org/&lt;/span&gt;&lt;span&gt;，网站中有专门R语言论坛，其性质跟统计之都论坛相似，可以互动，相互学习；当然该网站还有其他学习资源，如SAS、计量经济论坛、Python论坛等；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;中国统计网&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：&lt;/span&gt;&lt;span&gt;http://www.itongji.cn/cms/article/index&lt;/span&gt;&lt;span&gt;，网站中有非常多的好文，包含数据运营、数据分析、数据挖掘、干货分享等栏目；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;Github&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;官网&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：https://github.com/，在这个网站上你可以查到很多别人做过的项目案例，含有R语言代码，通过一步步学习，能够提高R语言编程技能 ；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;Kaggle&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;官网&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;：&lt;/span&gt;&lt;span&gt;https://www.kaggle.com/&lt;/span&gt;&lt;span&gt;，这是一个提供数据挖掘比赛的网站，你既可以查看别人提交的作页，也可以通过报名比赛来提高自己的实战能力；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;       &lt;/span&gt;&lt;span&gt;尽管有如此多的优秀学习资料，但不要贪杯，&lt;span&gt;&lt;strong&gt;在学习过程中一定要各个击破&lt;/strong&gt;&lt;/span&gt;（一本一本的用心看，用心记，用心敲代码），系统学习。如果你想学好R语言这个工具，千万不要着急，基础很重要，否则基础刚学一半就去看高级的或看别人的比赛代码，我相信你还会被打回原形。而且&lt;span&gt;&lt;strong&gt;学习还是一个坚持的过程&lt;/strong&gt;&lt;/span&gt;，坚持记得学习；坚持写写心得；坚持将每一个知识点串起来尝试替代你最拿手的工具（如Excel，SPSS等）。加油！“&lt;strong&gt;革命尚未成功，同志仍需努力！&lt;/strong&gt;”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;OK&lt;/span&gt;&lt;span&gt;，到此就分享的差不多了，我在学习R语言的过程中，看书是&lt;span&gt;&lt;strong&gt;一方面&lt;/strong&gt;&lt;/span&gt;，&lt;span&gt;&lt;strong&gt;另一方面&lt;/strong&gt;&lt;/span&gt;要通过书籍中的知识点去想想我会在哪些场景下去使用，怎样使用；同时，通过查看大量的文章（论坛，博客）来学习别人所分享内容的思想、步骤和结论。作为数据分析或挖掘工作者，&lt;span&gt;&lt;strong&gt;技能是一方面&lt;/strong&gt;&lt;/span&gt;，另一方面是关于如何&lt;span&gt;&lt;strong&gt;培养好自己的分析思维&lt;/strong&gt;&lt;/span&gt;，毕竟技术这个东西是很容易替代的，而思维才是属于自己的，才是自己有别于其他人的地方。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;本次分享中，所提及的&lt;span&gt;&lt;strong&gt;所有书籍我都已经整理好&lt;/strong&gt;&lt;/span&gt;，链接: https://pan.baidu.com/s/1dEQOAtz 密码: 2q97。欢迎更多的朋友能够与小编一起学习，互相监督，取长补短，最终达到自己理想的目标状态。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;欢迎关注巡洋舰的深度学习实战课程&lt;/span&gt;&lt;span&gt;， 手把手带你进行深度学习实战， 课程涵盖机器学习，深度学习， 深度视觉， 深度自然语言处理， 以及极具特色的深度强化学习，看你能不能学完在你的领域跨学科的应用深度学习惊艳你的小伙伴，成为身边人眼中的大牛。&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 30 Nov 2017 20:32:56 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/kYXqJqwUMZ</dc:identifier>
</item>
</channel>
</rss>