<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>当AI遇到狗年，说一说用深度学习识别不同品种的狗子</title>
<link>http://www.jintiankansha.me/t/vb1N26VP3J</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/vb1N26VP3J</guid>
<description>&lt;p&gt;新的一年即将到来了，这里先祝各位新年快乐，狗年财运旺旺的 。在新年的最后一篇推送中，就简单的介绍一下和狗相关的一项黑科技，狗练识别。&lt;/p&gt;

&lt;p&gt;狗是已知的，人类最早驯化的动物了，早在一万五千多年之前，还处在农业时代之前的原始人类就靠着几根骨头骗到了贪吃的灰狼，又等到了农业时代，人们发现狗真的是十项全能的好帮手，于是开始了对狗的定向演化，于是就有了许多看起来差异巨大的不同品种的狗，下图所展示的只是一些常见的品种，你认出那些你熟悉的品种了吗？&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.0866666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccc3aFpbgqOYVraGSE6uA3zic4g3eb9LXh4nibq5oaG2uEUlZL2dyPV2Al8WgL0X60KWIUbkq9aSPIA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;300&quot; /&gt;&lt;/p&gt;
&lt;p&gt;而根据不同品种的狗狗的照片，按照品种进行分类，在机器视觉中属于Fine-grained classification，也就是根据图像的细节进行分类，类似的问题还包括根据植物的照片判断是那种花等，你也许会说，这样的问题不应该很简单吗，用深度学习就行了，一层不行就再加一层。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7015625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccc3aFpbgqOYVraGSE6uA3zRoUVjnZqKkSMEFJ9eS1QDxsFibKMItGEB0t7RjhyYzsyKWmWXUvZOVQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;且慢，让我们先看看这个问题本身有那些本质的困难之处。首先是不同品种的狗都很相似，比如下图的三种狗，若你不是狗专家，你能够分清楚吗？&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.46255506607929514&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccc3aFpbgqOYVraGSE6uA3ztRNQLHmeZZWuJFKhrZ0kYAGWSicyuyLd5zdPXqRH4pMSVrUL9e1PkWg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;454&quot; /&gt;&lt;/p&gt;
&lt;p&gt;第二个问题是即使是同一种狗，也会差距很大，例如下图的三种狗，竟然是一家人，你说这叫AI头大不头大。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4409090909090909&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccc3aFpbgqOYVraGSE6uA3zkanZmTMGFTOT4iahg2Rqp9x5gTKrdGSNibdhFgq9yZA9Gucu5J34jyMg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;440&quot; /&gt;&lt;/p&gt;
&lt;p&gt;更要命的是不同的狗狗都有各自独特的pose，而且他们所在的图片的区域，图片背后的背景是草地还是森林都不同。下图展示了三只不同姿势的狗狗。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4967462039045553&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFTIodib4zpias3tZAFxlic0Kf43LVRtaJGClia8V8qFeHzB9QXjhXY7mMvw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;461&quot; /&gt;&lt;/p&gt;
&lt;p&gt;正是由于有这三个问题，要做好狗脸识别，并不是一件简单的事。所以说，&lt;strong&gt;使用深度学习去解决具体问题，需要先调研清楚这个问题的背景和障碍，才能够便于设计下一步具体优化方法。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;该怎么做了？第一种思路是数据增强，也就是用&lt;strong&gt;随机应对随机&lt;/strong&gt;。既然狗子的位置在照片中不固定，那就将原始的图片随机的裁剪一下，旋转一下，将图像的颜色做一些微调，总之就是想象一个熊孩子打开ps修改了每张狗子的照片，给你留下了一堆看起来和原始的训练数据差不多的照片作为新的训练集。下面给出了随机剪裁之后得到的狗的照片示例。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;114&quot; data-backw=&quot;307&quot; data-ratio=&quot;0.3713355048859935&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFHl5vTARl66Vj4eLsWtzbIiaeIibJ0SehhFxVUiagWFWdJQQTTYDg7cia9w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;307&quot; /&gt;&lt;/p&gt;
&lt;p&gt;而另一种方式，可以看成是&lt;strong&gt;用有序来应对随机&lt;/strong&gt;，也就是先通过识别出狗子所在的区域，再将这个区域拉伸成一样大小的图片，来去除背景的干扰。例如下图所示，展示了经过了背景去除和拉伸前后的狗子的照片，可以看到经过处理后的照片，只剩下了我们关心的狗子的信息。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;102&quot; data-backw=&quot;356&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmF4JuIFNAyOH7amAtVib2KIfWv27LiawicpLg7xMDc5AibxUcia7VcYVeS5dw/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;356&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;96.97491039426524&quot; data-ratio=&quot;0.2696629213483146&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFRCyHkfeePDYpfswZBQFiaWWUBudsjyR5TyibbUgqkf3ONtCnUYCRAmqA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;356&quot; /&gt;&lt;/p&gt;

&lt;p&gt;接下来的问题是狗子的照片不够多，斯坦福大学针对狗的品种识别，搞出了一个数据库，里面有一百多种狗，一共一万多张照片，但若是指望这些图片就能够训练出一个靠谱的深度神经网络，那效果多半不好。&lt;strong&gt;数据增强虽然能够改善预测的准确性，但其上限不高，毕竟原始的信息就那么多。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.0225225225225225&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFyXnqUSXNdQVNS815GEIBRkiaau5UEFEbYZ0ichqQRibh3LXDibkJg3NWXA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;222&quot; /&gt;&lt;/p&gt;
&lt;p&gt;若是你自己做不到，不妨站在巨人的肩膀上。迁移学习正是这样，深度学习的好处是模型不再是铁板一块，而是可以拆解成一层一层的，不用花一分钱，你可以拿大牛们训练好的网络，将其用做自己的用途。关于迁移学习，曾经写过一篇名叫&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383309&amp;amp;idx=1&amp;amp;sn=f0ee86dc43994673f2b818cb2c2efe4b&amp;amp;chksm=84f3c84cb384415a67b8469578df8268a490f12355d4213c126d767227a0da52a77297b47be4&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;白话迁移学习&lt;/a&gt;的小文，感兴趣的可以点击深入了解。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;而迁移学习迁移的是什么样的神经网络，自然是深度学习中最出名的卷积神经网络，关于这个话题，可以参考&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383212&amp;amp;idx=1&amp;amp;sn=e6dbbda2acc5984c8d06e24ec9c84d09&amp;amp;chksm=84f3cbedb38442fb58f0aea635821fcf4ba3edaacef4685716c7eadb6191197ebfa70a6bf14b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;你所不能不知道的CNN&lt;/a&gt;和&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381959&amp;amp;idx=1&amp;amp;sn=1b920dd476849d88b67a2ef1cf3ed8fc&amp;amp;chksm=84f3ce86b3844790627d2f15256aff0753be1f0b0623da64aaa7357d73e8ed14c415061acb27&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;用CNN来识别鸟or飞机的图像&lt;/a&gt;，这里就不再重复了。&lt;/p&gt;

&lt;p&gt;但迁移学习并不意味着什么都不需要来做，你需要利用成熟的网络提取出的高级特征，用他们来进行预测，但是要注意的是，你预测得出的结果并不是一个确定的狗的品种，而是这个图属于哪个品种的概率，而这又是怎么得出来的，靠的是一个名叫softMax的激活函数，这可是深度学习中最出名的两个激活函数了，soft是保证输出的结果是符合概率分布，也就是不会出现概率为120%的情况，而max是让错的更错，从而提高学习的效率。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7857142857142857&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFwMvjt0icic9QxPRjqIQZC2auUSQ2GjnGNCtBia2Zu2sbs2QHodQ0oInhw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;280&quot; /&gt;&lt;/p&gt;
&lt;p&gt;最后说一说这个例子的实现难度，类似Minst数据集的手写数字识别，宠物狗的品种分类，也是一个相对容易上手的例子，你不需要昂贵的显卡，就普通的个人电脑，就可以基于已有的图像识别网络，例如谷歌的Inception，来搭建一个属于你的狗脸识别程序。除了用来展示，这个例子还可以锻炼你诊断网络的能力，训练的太慢，不妨换换更快学习率，训练的结果起伏太大，还可以再换换更慢的学习率啦。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;开个玩笑，虽然学习率是深度学习中很重要的一个参数，但不是万能的，这个例子的好处是让你能够直观的看到你训练出神经网络有那些不足，又为什么会犯错，例如下图，如果是将图片变成黑白的，神经网络是不会犯错的，但一旦加上了色彩，你就会发现神经网络没有将黄色那只分成拉布拉多犬，这时你就能发现，是你的神经网络过拟合了，网络中的一个神经元学到的速记口诀，但凡黄色的都不是拉布拉多，这时不管你怎么做数据增强，对数据又拉伸又旋转，都无法教会这个神经元忘掉这个口诀，这也是我为什么上文说数据增强的效果是有上限的。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6631799163179917&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFulepXU1KeThKxTzg4wmaMqIE3QK4kQgyL7G5pa4Cp9MFRrWgL6bSmg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;478&quot; /&gt;&lt;/p&gt;
&lt;p&gt;而这时如果你能用dropout机制，时不时的将这个背小抄的神经元踢下线，那么你的神经网络就能够有机会学到拉布拉多真正的特征，这也就解释了为什么dropout机制是一种极为高效的防止模型过拟合的方法。&lt;/p&gt;

&lt;p&gt;卷积神经网络的另一个特点是能够打破所谓的黑箱，让你看看&lt;strong&gt;网络内部特征是如何一步步被抽象出来的&lt;/strong&gt;，下图依次分别展示了卷积神经网络的输入图像和第一层的输出结果，可以看出神经网络第一层的卷积核提取出的特征分别是什么，又对图像进行了怎样的抽象。（虽然小编表示我就算看出神经网络做了什么，也没法说出来，不过这反而说明了网络本身能做一些人无法明确言说的任务，而这正是深度神经网络最革命性的创新）&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9905660377358491&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFF991GMibUUibJloen3eW88NLok7kYKpAMkPPa37WPaf8iaG5pB93uhwHw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;424&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9891067538126361&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmF2GxlEzTSS98pb1ch2DzibA7k89GJqeTac5Yadnia63sIaNHTJibfAEk8Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;459&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在这篇小文的结尾，放上一章狗狗们的全家福照片，愿各位不管是什么汪，新的一年的事事兴旺，合家安康。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4498186215235792&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFibAHmXwfTOALb9OVSK0eBhkde9UvecicrSXBAIHMDZdVM0nD1jy9VV4A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;827&quot; /&gt;&lt;/p&gt;


&lt;p&gt;&lt;span&gt;参考资料&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1）Using Convolutional Neural Networks to Classify Dog Breeds  Hsu, David&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2）Dog Breed Identification  Whitney LaRow ，Brian Mittl， Vijay Singh&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3）Automatic Dog Breed Identification     Dylan Rhodes&lt;/span&gt;&lt;/p&gt;








</description>
<pubDate>Wed, 14 Feb 2018 12:31:46 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/vb1N26VP3J</dc:identifier>
</item>
<item>
<title>[原创]Soonish 读书笔记-什么样的科技能改变世界</title>
<link>http://www.jintiankansha.me/t/jQjk4HYH1s</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/jQjk4HYH1s</guid>
<description>&lt;p&gt;今天说一本17年11月出的英文新书，书名叫Soonish，副标题是十种会让所有事情变好或变坏的新兴技术。这本的特点是简明易懂，书的作者是一对夫妻，一位是漫画家，另一位是生物学教授。作者为了让读者不要被技术细节吓到，书中加上了很多幽默的桥段（虽然对于来自不同文化的读者，理解这些幽默并不容易），同时书中还有很多漫画。&lt;/p&gt;

&lt;p&gt;在轻松的外壳下，是书中对技术实现的可行性，目前遇到的阻碍，技术的潜力以及可能带来的伦理问题的精准描摹。这使得这本书的阅读体验是一种享受，作为一本非虚构的作品，作者处理的技巧使得我愿意以读小说一般的激情一口气读完，可见背后作者花了多少功夫。在这本书的附录中，还简单列出了那些没有入选十大改变世界的技术名单，以及为什么没有入选的原因，这份对读者的诚实也值得我学习。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.333764553686934&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceHWGW8WGUXmXu1GiazQiajslyHZl11BaQicxZjNVFpwm4mznuxMQaXWMVwm0ia9ibJm8ia83jFsA753FZQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1546&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在列出书中技术列表之前，我想先让读者猜一猜书中的技术会有那些，我想大部分读者会说深度学习，区块链，物联网这些当今热门的玩意，然而这三项都没有入选。这不是作者的疏忽，而是作者自有一番道理，而这正是这篇读书笔记想指出的。&lt;/p&gt;

&lt;p&gt;整本书看问题是从大到小来看的，首先关注宇宙，例如低廉的进入太空的方式（太空电梯，复用火箭等），再介绍由此带来的小行星矿产开发和太空太阳能发电。之后关注和我们差不多大的东西，这一部分内容很丰富包括可控核聚变，可编程微型机器人，机器建筑工人，增强现实，合成生物学，常温超导和量子计算。最后关注的是我们自身，因为毕竟人才是最根本的生产力，这部分的内容包括精准医学，3D器官打印，脑机接口。&lt;/p&gt;

&lt;p&gt;这本书我今年元旦就读完，等到今天再写读书笔记，这是因为我想借着这本书说说我对未来技术的看法。有一种很流行的观点说，科技的发展经历了突飞猛进的20世纪，低垂的果实都被摘完了。读者可以试想自己穿越到了20年前，那时你的生活水平不会有任何下降，幸福水平可能还会提高。98年家庭里有的电视，冰箱，空调，电脑，现在也还有，只是那时没有移动支付，没有高铁。对于美国人，这种感觉更明显，父辈过着怎样的生活，孩子也过着类似的生活，这对于19世纪的人来说，是不可以想象的。那时蒸汽机革命跟随者电力革命，那时的科幻小说会写道人们在2001年就在火星殖民。&lt;/p&gt;

&lt;p&gt;究竟是什么原因，导致科技进步，至少是关于如何将蛋糕做大方面的科技进步减速了。我在读完了Soonish这本书后，试图总结书中列出的技术的共性，尤其是为什么这种技术发展遇到阻碍的共同点。但看来看去，只看出一个“穷”字。表面是投资不足，深层次则是需求不足。&lt;/p&gt;

&lt;p&gt;西方社会在二战以后，尤其是冷战结束之后，进入了大前研一描述的“低欲望社会”，年轻人对周围的一切丧失兴趣，得过且过，过着佛系的宅男宅女式的生活，这才是阻碍科技发展的根本原因。经济学中对这一现象，有另一种描述方式，内卷化。这本来是形容中国清朝中期人口增加导致边际工资较低，从而导致资本边际收益低到无法支持工业革命。而当前发生的则是人口不增加，但由于人的欲望降低，导致资本的收益率无法支持下一次工业革命的。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.0061823802163834&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceHWGW8WGUXmXu1GiazQiajsliaH8ZlSWO6eTPCGyrn2JGdzoXibNwQnjIHVHwqD9S8Ik8K19nal00olw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;647&quot; /&gt;&lt;/p&gt;
&lt;p&gt;瓦特在一步步改进蒸汽机的过程中，失败过很多次，但是他始终能找到投资者，愿意花大价钱为他的创新尝试买单。而如今可控核聚变，一旦成功，所带来的影响比蒸汽机要大千倍，但新闻中却没有看到各国向这项技术给予充足的投入，反而看到相关实验室因为拨款不足而关闭。这是西方民主制度和福利社会必然造成的短视行为，未来只会更严重。&lt;/p&gt;

&lt;p&gt;那个人的企业，有没有可能引领下一次科技革新了？在做大事上最成功的SpaceX，其实也有着政府的背景。而即使是在近期能够看到可见经济收益的技术，例如增强现实，精准医学，3D器官打印及太空旅行等，也需要极高的投入，除非企业能够预期到未来有显著增加的中产阶级消费者，而且始终有一批愿意花大价钱吃螃蟹的先行者，也不愿意在这些领域投入太多。而如今福利国家的低欲望社会，使得消费者没有积蓄，不愿意冒险尝试新东西，这就限制了个人企业的成长。&lt;/p&gt;

&lt;p&gt;而这正是我为什么对未来十年的科技进展不够乐观的原因。和《snooish》这本书作者开篇的乐观预测不同，我对未来的预测要平淡一些，书中描述的科技进步终究会到来，只是时间会晚一些。而且更关键的是，我预期这些技术多半会发生以华夏文明为核心的东方，只有这里，才会有足够的需求去拉动投资，去发展革命性技术。而中国的崛起需要的时间，要比我们预测的更长。中国需要完成文化上的整合和重建，先确定了精神上意志上的领先，才能带动技术上的领先。中国需要一套脱离于超脱于现有民族主义说法的全新意识形态，一个新的想象视野。这是我看了施展的枢纽以及《Soonish》这本书后，结合经济去探讨科技问题的一次尝试。如有不当之处，还请读者指正包涵。&lt;/p&gt;

&lt;p&gt;最后说说我为什么觉得像互联网，人工智能，区块链这些技术没有列到书中，不仅没有减少这本书的价值，反而让我对《soonish》这本书高看了的原因。首先系统性的阅读一本书而不是碎片化的阅读，就是为了摆脱自己的回音室，找回曾经读报纸从自己不关心的版面学到新知的快乐。因此书中没有介绍人们熟悉的技术，这是一件好事，是值得写书人学习的。其次，这本书中列出的技术，关注的都是怎样将蛋糕做大，而更容易见到效果的技术，则是如何降低交易成本，从而更好分蛋糕的技术。互联网就是这样的典型，而AI取代的司机，放射科医生，律师等，其实也没有明显的增加社会的总福利，属于为他人提供服务的。诚然，更好的服务能够轻松的获取我们的注意力，但真正改变生活本质的东西，却不是累加式的更好的更个性化的体验，而是全新的生产方式。&lt;/p&gt;

&lt;p&gt;而中国要引领未来，一是要坚持走出去，二是要避免过度的福利导致人缺少奋斗的动力，三是要充分的保护私有财产。不过这涉及到国事了，作为一篇科普书的读书笔记，就不方便多说了。&lt;/p&gt;

&lt;p&gt;原创不易，随喜赞赏&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9397590361445783&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd4icdSNQnx98Zicw7nnP3icPycqI1uRMvRxCezhYm4xQKdbiamvHVmC19a0o7YS03eqTrIL9QJS4wS4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;249&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383313&amp;amp;idx=1&amp;amp;sn=a4279c3f2ec27302a50b42235301be71&amp;amp;chksm=84f3c850b3844146ca1042a90fe56414a72ab9f8b7a7215c10e2db42f336a8f38d77a6bfe582&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;读 当呼吸变为空气 没有活过的生活不值得审视&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382886&amp;amp;idx=1&amp;amp;sn=87a0623a78bc64771c247c2eba3b0708&amp;amp;chksm=84f3ca27b3844331e9789d76df01b3e479b69a0590c82be1b8e4d9d7c69915750b6e97bf564e&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;读《经济的限度》看汪丁丁眼中的中国社会面临的本质问题&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;




</description>
<pubDate>Thu, 08 Feb 2018 16:02:05 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/jQjk4HYH1s</dc:identifier>
</item>
<item>
<title>贝叶斯大脑</title>
<link>http://www.jintiankansha.me/t/eNQhobGY97</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/eNQhobGY97</guid>
<description>&lt;p&gt;本文来自巡洋舰读者投稿，作者为复旦物理博士，现从事机器学习算法研究。 首发于作者的的个人公众号“Kane的世界线”，欢迎各位关注。这里发布的这篇文章，和原文有些不同，进行了进一步的修改，以便于读者理解。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;如果要从1到100里面猜一个和16最像的数，你会猜什么？&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;可能你会觉得无从下手，因为相像有无数可能性，可以是15或者17，因为数值相近；可以是96或者4，因为是16的倍数或者都是偶数；还可以是2,4,16,32，因为都是2的幂次。那接着告诉你，除了16之外，还有8,2,64也在同一组，那么你觉得下一个可能的数是什么？我想很多人会由此推断出要找的数是2的幂次；而如果说23,19,20和16是同一组呢，那么可能会推断是想找数值相近的数。&lt;/p&gt;

&lt;p&gt;咋一看，这很显然。但细想，却很玄妙。在很多情况下，只给一个或少数几个例子，而且仅仅是正面例子，我们便可以从中学习、推断和做分类，这是一项神奇的能力，至少目前的机器学习算法还没有人类做得好。我们的大脑是怎么做到这一点的呢，这还要从Bayes，哦，不，Sheldon说起。&lt;/p&gt;
&lt;h3&gt;从Sheldon到Bayes定理&lt;/h3&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5193621867881549&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/AF0iapL8bN1PZ4qH1iatN3fwuLmtPoMoibt9yWfab9aBepbcvictwiaiayw9C1V40T6WCws2VUQUFMN9LUxxo2NSJJaA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;878&quot; /&gt;&lt;/p&gt;
&lt;p&gt;很多人都喜欢看《生活大爆炸》，欣赏里面Sheldon的绝顶聪明，上面的图片就出自《生活大爆炸》第四季第二集。里面的Sheldon非常担心，害怕自己活不到技术“奇点”的出现，也就无法通过意识上传获得永生。他根据家族成员的寿命和疾病史等，预期自己还有六十年可以活。他是怎么做到的呢？用的就是黑板上的贝叶斯定理，也是今天要讲的主题。&lt;/p&gt;

&lt;p&gt;贝叶斯是18世纪英国的一位统计学家，他的生平事迹这里就不赘述，只需要知道他发现了这一定理的一种特别情况，后人因此用他名字给这一定理命名。这一定理看起来是如此的显然和稀松平常，以致于初次遇见可能会忽视它。而细究之下，又会发现，它的内涵是如此丰富，不仅仅改变了我们对概率论的看法，并且很多情况下，我们的思维和决策本身也是基于其基础之上的，就像前面所讲的例子。&lt;/p&gt;

&lt;p&gt;在概率论中，设两个事件发生的概率分别是P(A)和P(B)，那么他们同时发生的概率P(A,B)可以用两种方式计算，既可以表述为事件A发生的概率P(A)乘以事件A发生时事件B也发生的概率（条件概率）P(B|A)，也可以表述为事件B发生的概率P(B)乘以事件B发生时事件A也发生的概率P(A|B)，公式表达如下：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.12859884836852206&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/AF0iapL8bN1PZ4qH1iatN3fwuLmtPoMoibtT1S3Qb5F7xNiaPxkcicRQV8x40YhyibnhDO5neMrTOBr0ic3ukQ38uEs5A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;521&quot; /&gt;&lt;/p&gt;
&lt;p&gt;这就是贝叶斯定理的全部。很简单而且显然，对不对。只不过为了更好的理解其中的含义，我们把上述公式变换到它的标准形式：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.1895551257253385&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/AF0iapL8bN1PZ4qH1iatN3fwuLmtPoMoibt1QbEI04Qiaib1AaAe2t3HVdzgT392YMUa8Ur8Ej5ATFXGHEibLN0ibj65g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;517&quot; /&gt;&lt;/p&gt;
&lt;p&gt;通常情况下，B表示某一论断，例如“太阳每天从东方升起”，P(B)表示最初我们对这一论断的信念，称为先验概率prior。A表示对这一论断我们收集的证据，例如，今天太阳从东方升起。P(A|B)表示假如论断成立，出现这一证据的可能性，称为似然概率likelihood。那么我们便可以根据上述公式对信念进行更新，从先验概率P(B)变到后验概率posterior P(B|A)。&lt;/p&gt;

&lt;p&gt;这里很重要的一点是，和我们平常所使用的概率方式不同，这里，一开始我们并没有假定“太阳每天从东方升起”一定正确，而是万事看证据，根据证据来修正我们对一件事物的看法。这一范式的改变发展出了概率论的贝叶斯学派，和传统的频率学派对概率论的解释形成对立，争论至今。&lt;/p&gt;

&lt;p&gt;由此说开去，我们发现，不管是科学理论的建立还是发明创造，很多时候都是一条漫长曲折的寻找证据，并从证据中逐步抽象，建立起理论的道路。但在理论建立完备后，常常讲解的方式却是另外一种，高屋建瓴式的、抽象的、预设的前提假设出发，一步步小心求证，最后得到结论，这一方式发挥到极致的学科便是数学。后一种方法我们称之为演绎推理deduction，而前一种更多的是归纳推理induction。对归纳推理炉火纯青的应用，正是人类学习的一个很大优势。&lt;/p&gt;

&lt;h3&gt;&lt;strong&gt;认知的贝叶斯模型&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;回到开头提到的猜数字的游戏，有了贝叶斯定理的武装，我们便能更好的理解在这一任务中，大脑究竟发生了什么。这一例子出自Tenenbaum的博士论文，并被Murphy在《机器学习》[1]一书中采用，为了便于解释，我们截取Murphy书中的两张图：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.0553191489361702&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/AF0iapL8bN1PZ4qH1iatN3fwuLmtPoMoibt8EmbWYAiaOjvicaPWE86IHXA5QoBwGhAnrH1G5gMbHfwjXt2tkvf9ibvA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;705&quot; /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;上图给出了知道数字16之后，我们所做的各种模型假设，以及相应的先验概率、似然概率和后验概率分布。纵坐标为各个模型假设，横坐标标记为&quot;prior&quot;的左侧图是每个模型对应的先验概率P(h_i)，它表示我们对每种假设的信念大小，例如把数字分成奇数偶数比较常见，于是我们把相应模型的先验概率设得比较大，而对于“都是2的幂次但排除32”这样的规则，我们会觉得很不“自然”，相应的会给予很小的概率。对模型的偏好来自于我们的先验知识，在两个一样解释力的模型中，我们会偏好更简单的模型，这就是经典的Occam剃刀原则。&lt;/p&gt;

&lt;p&gt;同样的，我们还需要知道对每种既定假设，出现数字16的概率大小P(O|h_i)，这表示在上图横坐标为&quot;lik&quot;的中部。具体计算逻辑为，设假设h_i允许出现的结果有|h_i|种，那么每种结果出现的可能性便是：&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.45982142857142855&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/AF0iapL8bN1PZ4qH1iatN3fwuLmtPoMoibtXXxOHOWwDK8lFzxUF1n3MicIpGVQrHbIVnDO3IRibm9PpXRoaicS0sCKg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;224&quot; /&gt;&lt;/p&gt;
&lt;p&gt;所以我们可以看到，因为满足“都是4的幂次”假设的结果只有4,16,64三种，所以16对应的似然概率为1/3。&lt;/p&gt;

&lt;p&gt;根据贝叶斯定理，最终对每种假设的信念便是两者的乘积，既要考虑到先验假设，也要考虑到似然概率，相乘的结果显示在图中横坐标为“post&quot;的右侧。对于”都是偶数“这样的假设，尽管先验概率比较大，但因为1到100间的偶数太多，出现16的概率仅1/50，如果恰恰出现了，我们会觉得是“惊人的巧合”，而不太会相信它是真的。这对应着贝叶斯版的Occam剃刀，在机器学习中，它化身为正则化项以防止模型过拟合。&lt;/p&gt;

&lt;p&gt;这样，我们就有了知道数字16后各模型的后验概率P(h_i|O),从中我们就可以选择概率最大的一个作为最大似然估计，图中，我们可以看到选出的模型是“都是4的幂次”。如果有更多的证据，模型便会快速收敛至真实情况。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/AF0iapL8bN1PZ4qH1iatN3fwuLmtPoMoibtCwIgn2nqcGXOWqnztmbDG66CdW2oKlSACzCN2K8HNRX1Lvf4zv7FAg/640?wx_fmt=png&quot; data-type=&quot;png&quot; class=&quot;&quot; data-ratio=&quot;0.6851851851851852&quot; data-w=&quot;1080&quot; /&gt;&lt;/p&gt;
&lt;p&gt;那么我们又是如何猜测下一个数字x的呢？我们已经有了每个模型的后验概率，下一个数字是x的概率就可以表示为每个模型的后验概率和相应模型出现x的概率的乘积的求和，俗称贝叶斯模型平均。表示为：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.24937027707808565&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/AF0iapL8bN1PZ4qH1iatN3fwuLmtPoMoibtnEyASleib8xAkkasB0Tdz8HHS5X7ic33MXcbKibzR9arQ7gYS9TJaXPbg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;397&quot; /&gt;&lt;/p&gt;
&lt;p&gt;这一计算过程漂亮的反映在上图中。中间那一大块图纵轴表示各种可能假设，而每条横线表示1到100的数值区间，那么每条线上的点便表示各模型假设允许出现的数值。可以看到假设“all&quot;的横线布满了点，因为从1到100，它每个数都可以取到。我们再看图的右边那条曲线，它表示的便是给定数值16后各模型的后验概率分布，可以看到，假设”都是4的幂次“的后验概率最大。将两者结合并叠加起来，就会得到图中上部所示的x的概率分布。可以看到，数字16,64,4的概率最大，与我们料想的非常一致。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们再看大脑推断中用到贝叶斯的两个例子。第一个例子同样来自Tenenbaum[2]的论文，说的不仅仅是我们如何学习单个概念，还说明了我们是如何将概念对应到事物的不同范畴的。所谓范畴，就是对事物的分类，并且这种分类通常是有不同层次的。例如你的写字桌，它既属于写字桌这一类，也属于桌子这一类，还属于家具这一类，在范畴论中，它分别可以对应着下位范畴、基本范畴和上位范畴。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7096330275229358&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceJvQ7HtlOv1xOicdicJDrQS21t67oyBNBXMgXdicclSx7lSoGNsn36YWGLaJ29nl8QQmwxJX1FItJMw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;2180&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在Tenenbaum论文的例子中，当指着一张标记为fep的斑点狗图片，来猜测fep的含义时，我们既可以认为fep表示上位范畴的动物，表示基本范畴的狗，也可以是表示下位范畴的斑点狗。而我们会倾向于推断fep的意思是狗。这是由基本范畴偏差（prior)造成的，因为我们日常处理事物大多都在基本范畴，这也是为什么基本范畴的中英文单词大多非常简单且长度很短。但当给了三张斑点狗的图片，而且每张都标记为fep的时候，我们却更可能推断fep意思是斑点狗而不是所有的狗。因为直观上来讲，如果fep表示的是所有的狗，但随机抽取的三个样本都是斑点狗，那将是“惊人的巧合”。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第二个例子来自刘未鹏的《暗时间》，里面提到了一个自然语言的二义性例子。&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;the girl saw the boy with a telescope.&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;对于上面这句话，我们既可以理解为那个女孩拿着望远镜看那个男孩，也可以理解为那个女孩看到那个拿着望远镜的男孩。那么为什么通常情况下，我们会想当然的理解为第一个意思而消除歧义？从语法结构上讲，两种结构都是成立的，在这里体现为先验概率P(h)大致一样，但是P(O|h)却很不一样。如果是第二种情况，那么为何偏偏那个男孩拿的是一个望远镜，而不是一本书或一只苹果呢？有很多不同的可能性，恰巧是望远镜的可能性是非常小的。但是如果用第一种语义理解就不一样了，女孩通过某种东西看男孩，那么，拿的是望远镜就很显然。&lt;/p&gt;

&lt;p&gt;在很多情况下，贝叶斯原理很好用，我们大脑也用它做很多事。但另一方面，它也是认知偏差的孵化池。&lt;/p&gt;

&lt;h3&gt;&lt;strong&gt;认知偏差&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;在《机器人叛乱》一书中，斯坦诺维奇讲到了认知心理学文献中的琳达问题：&lt;/p&gt;
&lt;blockquote readability=&quot;23&quot;&gt;
&lt;p&gt;琳达今年31岁，单身、率真、非常聪明。她的专业是哲学。作为一个学生，她格外关心歧视和社会公正问题，也曾参加过反核示威游行。请根据可能性对下面的陈述进行评价，1代表可能性最高，8代表可能性最低。&lt;/p&gt;
&lt;p&gt;a. 琳达是一名小学老师。&lt;/p&gt;
&lt;p&gt;b. 琳达在书店工作，上瑜伽课。&lt;/p&gt;
&lt;p&gt;c. 琳达积极参加女权运动。&lt;/p&gt;
&lt;p&gt;d. 琳达是一名精神病学的社工。&lt;/p&gt;
&lt;p&gt;e. 琳达是妇女选民联盟的一员。&lt;/p&gt;
&lt;p&gt;f. 琳达是一名银行出纳。&lt;/p&gt;
&lt;p&gt;g. 琳达是一名保险销售员。&lt;/p&gt;
&lt;p&gt;h. 琳达是一名银行出纳，积极参加女权运动。&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;因为选项h是选项c和f的组合，所以从概率来看，肯定比两者来得小，但是研究表明，有85%的参与者出现了“组合偏差”，他们认为选项h比f的可能性更高。&lt;/p&gt;
&lt;p&gt;这可以看成是混淆了似然概率与后验概率的区别。本来需要计算后验概率P(h|O)，却计算了似然函数P(O|h)，或者说本来需要用induction的地方却错误的使用了deduction。&lt;/p&gt;

&lt;p&gt;因为按照似然函数的思路，相比于“琳达是一名银行出纳”的论断，“琳达是一名银行出纳，并且积极参加女权运动”的论断，更可能得到琳达关心歧视和社会公正问题等具体描述。而没有注意到，对于后验概率，还需要关注先验概率prior，而f选项的prior明显比h大得多。&lt;/p&gt;

&lt;p&gt;类似的认知谬误比比皆是，我们可以再看赌徒谬误的例子，里面混淆了前提假设和后验概率。&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.1671875&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcfyNibceeGvVKo0qCGicGJ9lQ9J5zzOIxmTUAE4OYfwx25QZ2J2KLUic1lWYkMwelC8Ld30RF0FsibAPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;赌徒谬误[3]说的是：&lt;/p&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;抛一枚公平的硬币，连续出现越多次正面朝上，下次抛出正面的机率就越小，抛出反面的机率就越大。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;把这个谬误和热手谬误[4]及选择性记忆相结合，就不难理解为何赌徒永远赢不了。理性的分析容易看到，每次抛硬币都是相互独立事件，前面的结果不会对之后的结果产生影响。而我们又有了前提假设：硬币是无偏的。所以不管哪次抛掷硬币，出现正反的可能性都是1/2。&lt;/p&gt;

&lt;p&gt;更精确的，我们可以用数学语言描述。假设硬币出现正面朝上的概率为h,已抛掷4次，每次都是正面朝上，这一事实表述为O. 硬币无偏，满足P(h=0.5)=1,则下一次出现正面朝上的概率为P(u,O|h=0.5)=0.5，出现反面朝上的概率也是P(d,O|h=0.5)=0.5.&lt;/p&gt;

&lt;p&gt;但是，赌徒错误的使用了硬币无偏的结论，没有把它看成是前提假设，而看成是证据之后的推断，也就是后验概率。因为之前四次的正面朝上已经让硬币正面朝上的概率偏向于E(h)&amp;gt;0.5,为了维持硬币无偏的信念，那么我们期望的是下次的抛掷能使E(h)偏回来一点。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceJvQ7HtlOv1xOicdicJDrQS2dyvxXNJQYPfxibfzduOHc8sU1QIVYTC5Ahdqj7fZUEz8CFNlIDWAh1A/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;800&quot; /&gt;&lt;/p&gt;
&lt;p&gt;具体的，我们假设h的先验分布是均匀的（当然这里只是为了方便，用其他的分布不影响结论），那么抛掷四次正面朝上，使我们对h的概率预期变为：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.18579234972677597&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/AF0iapL8bN1PZ4qH1iatN3fwuLmtPoMoibtOHynyPeJPsCjFh8aWbGsYjIGpeGNicMU0kWibCbvKXVhySoE2zk4zASg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;366&quot; /&gt;&lt;/p&gt;
&lt;p&gt;可以得到期望E(h|O)=5/6。和设想的一样，经过四次正面朝上后，我们的证据偏向于硬币是h&amp;gt;0.5的。然后我们计算，下一次抛掷结果分别为正面朝上u和反面朝上d，h后验概率的期望。具体的：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.18857901726427623&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/AF0iapL8bN1PZ4qH1iatN3fwuLmtPoMoibtYf55SllvS3WatZPdHnhLzvhr131ib1h4iaTAAWy80O57biabwicLwPdggQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;753&quot; /&gt;&lt;/p&gt;
&lt;p&gt;由此，计算可得E(h|u,O)=6/7,而E(h|d,O)=5/7.可以看到，确实下一次抛掷如果反面朝上便可以增强我们对硬币无偏的信念。不仅如此，我们还可以发现E(h|O)介于两者之间。&lt;/p&gt;

&lt;h3&gt;&lt;strong&gt;总结&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;我歌月徘徊，我舞影零乱。我们的贝叶斯大脑根据已有知识对外界进行响应。这一方面让我们可以在稀疏的、少量的、只有正例的情境下快速学习、构建各种概念。但同时，也得警惕这种启发式的学习可能导致的各种认知谬误。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381623&amp;amp;idx=1&amp;amp;sn=079563ccdd30186de04e4826b5b37d18&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;玩转贝叶斯分析&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382073&amp;amp;idx=1&amp;amp;sn=50ba7a0867dbce73e967215ca9ef2d1d&amp;amp;chksm=84f3cf78b384466e971c57f3bf1db19a55280afe68a68c80b217f2501438a3b1c4029de7f73d&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;贝叶斯分析解码谁是卧底的游戏&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;参考文献：&lt;/p&gt;
&lt;p&gt;&lt;span&gt;[1]: Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective (1 edition). Cambridge, MA: The MIT Press.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;[2]: Xu, F., &amp;amp; Tenenbaum, J. B. (2007). Word learning as Bayesian inference. Psychological Review, 114(2), 245.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;[3]: [维基百科：赌徒谬误](https://zh.wikipedia.org/wiki/%E8%B3%AD%E5%BE%92%E8%AC%AC%E8%AA%A4)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;[4]: 热手谬误认为某事多次发生则未来发生的机率会较大，见维基百科。&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Tue, 06 Feb 2018 21:34:07 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/eNQhobGY97</dc:identifier>
</item>
</channel>
</rss>