<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>[原创]一本带你读懂无形资产的书-《CAPITALISM WITHOUT CAPITAL》</title>
<link>http://www.jintiankansha.me/t/hWkWlX9LKL</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/hWkWlX9LKL</guid>
<description>&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.5458937198067633&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccCdxJ5bV9zIWh0DZWSSqvmqN3X7jWicXmTVS9FREWyBfgxiaakODFZxj3FPOETyvMNV6yafqbnmRmA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1035&quot; /&gt;&lt;/p&gt;
&lt;p&gt;今天说一本比尔盖茨推荐过的书，17年11月出版的《CAPITALISM WITHOUT CAPITAL》，书的作者是伦敦帝国理工学院的商学院教授和创新慈善团体Nesta的主管。这本书围绕着无形资产这个时常被提到的热点话题讲起，先谈无形资产包括什么，再谈无形资产具有那些特征，之后用无形资产来解释当前的政经形势，最后针对企业，金融机构和政策制定者，分别给出无形资产带来的机遇和挑战。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5633333333333334&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccCdxJ5bV9zIWh0DZWSSqvmuYlKKN209LJ3EHTh1XDpg3Vfpq773bWqh1uOQkOD38uZpwZPlMmA6Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1200&quot; /&gt;&lt;/p&gt;
&lt;p&gt;图示： 在科学论文中“无形资产”这个词出现的次数&lt;/p&gt;

&lt;p&gt;无形资产常常和知识经济被联系起来，但无形资产不止包括专利，软件，算法，AI模型，科研成果，物流网络这些可以量化的东西，还包括品牌，商誉，甚至还包括组织的规章，组织间的信任和领导力。随着无形资产这个概念的普及，这些从前在会计账本中被记录为一次性消耗品的投入，被逐渐看做是长期的投资，被记入GDP。也正是由于无形资产开始一步步被量化记录，使得投入无形资产变得越来越日常化，从而使得在西方国家，无形资产的投入逐步超过了有形资产，而这个超越的时间正好是08年的全球金融危机。（下图所示）&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5266666666666666&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccCdxJ5bV9zIWh0DZWSSqvmM7HibnoPwpAetNLP4YuC0sT0peqxjV6s0BUQ3sL4tATxy1QxIaTHDmw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1200&quot; /&gt;&lt;/p&gt;
&lt;p&gt;无形资产不同于传统的实体资产，带有更多的不确定性。从时间的层面来看，在投入之前，不确定会有什么样的产出，即预期就会有很多投入会成为沉没成本（&lt;strong&gt;sunk&lt;/strong&gt;），比如投入新品研发后才发现研发所需的投入其实很多，比如投资让员工提升专业技能，但员工却在学会了之后跳槽；即使在研发成功后，投资到无形资产带来的收益却会溢出（&lt;strong&gt;spill over&lt;/strong&gt;），比如发现自己的发明被其他人山寨了。在空间上，无形资产的投入不会像有形资产那样有固定的损耗，因此一旦成功，就可以低成本的扩展到极大的规模，但在投入之前，并不能确认自己的成果是否能够成功的扩张（&lt;strong&gt;scalable&lt;/strong&gt;）；而要让自己的无形资产最大的发挥作用，就需要和其他的无形资产相配合，无形资产之间的相互作用，其结果不是简单的1+1&amp;gt;2，而可能是1+1+1&amp;gt;10，问题是你需要找到这个1，而这则是书中提出的无形资产的第四个性质（&lt;strong&gt;synergies&lt;/strong&gt;）。&lt;/p&gt;

&lt;p&gt;伴随着无形资产的增加，当代的西方发达国家进入了所谓的“长期性的经济增长停滞”，其典型的症状是生产效率增长减速，投资减少，曾经的中层实际收入保持不变甚至较低或，但大企业的利润却很好，从而导致贫富差距变大。虽然同时出现只能佐证相关性，不代表因果关系，但无形资产的四个性质却可以部分解释当前的经济状况。投资减少归结于部分沉没的无形资产投资没有被记录，无形资产的可扩展性使得领先者的马太效应更加显著，从而带来了贫富差距，而金融危机带来的闲置资金减少和企业家的心态保守阻止了无形资产形成合力的可能，从而没有带来应有的生产效率提升。&lt;/p&gt;

&lt;p&gt;无形资产要求其持有者具有开放的思维方式，从而和西方的左派很合拍，而这会使的在经济上本来就没有分到蛋糕的右派感到价值观上的冲突，不管是英国的脱欧还是川普的当选，支持者都是在知识经济中落后的一方，他们不止出于经济上的原因，更因为自己的尊严问题，选择了逆全球化的道路。通过无形资产这个视角，能够解释当前西方社会为什么分裂，也能解释为什么中国的阶级固化更值得担忧。这一点我在梳理完整本书的重要观点后之后会细说。&lt;/p&gt;

&lt;p&gt;无形资产更加计量，有巨大的不确定性，这对金融机构提出了调整。传统的债卷需要抵押品，但投入到无形资产在投入是沉没成本，从而不会有抵押品。而VC则无法应对像可控核聚变这样大规模的投资，也不倾向于做基础科学的研究而这就需要政府更多的去关注和投入。&lt;/p&gt;

&lt;p&gt;无形资产的兴起对政府提出了新的要求，首先是即要保护知识产权，又要避免过度保护从而阻止了潜在的无形资产间的化学反应，其次是为信息的交流提供好基础设施，比如使得大城市的规划即宜居又可扩展，从而使得更多的人能够参与到面对面的思想碰撞中。&lt;/p&gt;

&lt;p&gt;而对于个人来说，无形资产已经较多的企业也更加重视权威，比如星巴克的服务员，Uber的司机就没有多少自由；而那些还在积累无形资产的企业，则最需要领导力。一个能将不同领域整合起来的人，也适合促进无形资产之间的聚合，但未来的领导者所需要的不止是跨界，还需要有促进团队成员间信任的能力，从而能使成员间和而不同。&lt;/p&gt;

&lt;p&gt;以上就是这本书中的核心观点。接下来我想放飞自我，说说我读这本书的看法。首先是无形资产的溢出效应，溢出的不一定都是正面的。&lt;strong&gt;无形资产包括信任，但不包括人脉，关系这些寻租的入口；无形资产包括促进创新的规章，但不包括牺牲环境招商的政策；无形资产包括人力资本的投入，但却不包括通过水军抹黑竞争者。&lt;/strong&gt;总结来看，无形资产之所以是投资，首先是因为参与者玩的是非零和博弈，不管实际怎样，投入着的初心是要将蛋糕做大，而不是将蛋糕多给自己分一份。&lt;/p&gt;

&lt;p&gt;溢出效应让人想起经济学中的外部效应，但由于缺少计量，外部效应究竟是好是坏常常被忽略了。因此类似于将无形资本记录在GDP中，生产带来的外部性也应该被定量的记录。该怎么记录了，比如一个卖旧书的平台，就帮助社会整体传播了知识，从而带来了正向的外部性。假设这个平台不存在，那国家为了保证同等的人能以便宜的价格看到旧书，就需要通过税收去补贴。一个企业带来的外部性，可以看成是假设这个企业不存在，政府为了达到同样的效果，需要花的钱。在agent based model的框架下，上述的等价分析虽然不准确，但却可以做为对外部性计量的一个开始。&lt;/p&gt;

&lt;p&gt;未来的经济发展如何，不止取决于消费和产出，无形资产的增长使得当下积累的外部性是正是负变得举足轻重。无形资产计量和投入间的良性循环告诉我们，只有开始定量的统计一个效应，人们才能重视其影响，从而投入更多来改进该指标。如果人们不能统计外部性，那外部性带来的影响就是不可控的。而无形资产的增加，使得外部性成为了经济发展的核心问题。全社会需要一起想办法，使得每一元的生产产出的外部性被记录下来。在有了大数据这一基础资源的情况下，新的统计方法和准则需要及时跟上。&lt;/p&gt;

&lt;p&gt;接下来说说无形资产对应的文化属性对中国意味着什么。拿中美贸易战来说，这背后不止是利益之争，更是理念之争。去阻止一个在正常环境下肯定会崛起的大国，这是需要一些理想主义的。从表面来看，西方的保守派看似是反对全球化，而一带一路则是推动全球化，但实际上保守派要保住的是全球化所在意的价值观，比如思想的自由，知识产权的保护和市场的开放。最好的结局是中国坚持全球化的同时通过改革学到了适应无形资产的价值观。但在当代的复杂社会下，成功的改革都是自下而上的。&lt;/p&gt;

&lt;p&gt;中国的经济奇迹得益于自己积累的和西方溢出的无形资产的聚合，但传统文化因循守旧缺少开放又不思进取小富即安的心态，这使得那些没有从增长的蛋糕中分到多少的人群无法继续积累无形资产。这使得中国的阶级固化是心理上的，而仅仅是经济上的。即使加大了农村教育的投入，也难以让课堂变得比快手上的视频更具吸引力；即使推广成人教育，鼓励职场人士再充电，被高房价高租金压得消费降级的人也更愿意选择安稳的工作和从抖音中得到的放松。Coursera这样的MOOC网站在中国是能正常访问的，但相比于在西方，又多少国人花钱在上面拿证了？&lt;/p&gt;

&lt;p&gt;总结一下，《CAPITALISM WITHOUT CAPITAL》这本书讲述的话题是和每个人都息息相关的，比如无形资产的兴起使得一线城市的房价飞涨，因为只有这里的无形资产足够多，从而能形成合力提高生产率。书中提到无形资产的四个性质，用四个S概括，但可以通过不确定性将其串起来，也可以用开放的心态来将其相互联系起来。面对可能的沉没成本，溢出效应，拥有开放心态的人能够面向未来，用增量来思考问题，从而使得无形资产能够应用到更大的范围，能够和其他人的无形资产协作。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383506&amp;amp;idx=1&amp;amp;sn=5e13f2adaf0891bb9707195f56c7188f&amp;amp;chksm=84f3c893b384418586f9a44ca9359a7f895c6fdabfb4f1ed19e6545ea0edb6324c6a03ed1195&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;《Factfulness》一本让那些知识问答羞愧的必读书&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;





</description>
<pubDate>Sun, 26 Aug 2018 18:05:24 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/hWkWlX9LKL</dc:identifier>
</item>
<item>
<title>[原创]当RNN碰上强化学习-为空间建立通用模型</title>
<link>http://www.jintiankansha.me/t/sAxMjmtY0C</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/sAxMjmtY0C</guid>
<description>&lt;p&gt;你可能了解强化学习，你也可能了解RNN， 这两个在机器学习的世界里都不算简单的东西在一起能碰出什么火花，我给大家随便聊几句。  &lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfrfdKcbEB2Kic4M8MfGAJ0EAuiatSW2r64cCwnyl4uLZjsLM0ujNAkjhw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;4hh4f-0-0&quot;&gt;在讲RNN前我们先来侃侃强化学习, 强化学习越来越受重视，它为什么很重要，用一张图就够了。想像一下我们是人工智能体，我们有强化学习和监督学习可以选择，但干的事情是不一样的。面对一只老虎的时候，如果只有监督学习就会反映出老虎两个字，但如果有强化学习就可以决定逃跑还是战斗，哪一个重要是非常明显的，因为在老虎面前你知道这是老虎是没有意义的，需要决定是不是要逃跑，所以要靠强化学习来决定了你的行为。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bfaa2-0-0&quot;&gt;从算法上看，  强化学习是一个对未来的优化问题， 其算法的总纲可以归结到下面这张图里面，强化学习有两大门派：一种是动态规划（Dynamic Programming），另一种是策略梯度（Policy Optimization），还用老虎的例子说， 见到老虎估计一个打和不打在20年里的收益函数（比如打死老虎，县衙领赏， 抱得美人得10分vs被打死负10分） 然后推出现在的决定就是动态规划。 凭直觉和既往经验直觉决定跑还是打就是策略梯度法。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;bfaa2-0-0&quot;&gt;动态规划需要通过迭代来学习比较繁琐但是更精确，直接学习法更简单但是容易陷入局部最优，为了优势互补，两种方法相结合就得到Actor-Critic法，一方面直接学习行为，另一方面通过评估函数来最终评价那个行为对整个未来的影响，在反作用于对行为的学习，现在主流的模型都是基于这种方式。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfkpsUHQEQAbBImrY3mrC9veN1qG1EW95GicYdicndG2Hr7KoolQX1OFaw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;bu154-0-0&quot;&gt;强化学习优化算法的成功是在Markov决策框架里开始的，只要问题符合这个框架（把过程分解为无数离散的时刻，然后此刻的信息足以决策后面的结果， 而无需历史信息，无论问题多复杂， 都可以分解为状态（state）- 行为（action）- 奖励（reward）- 策略（policy）四要素。状态是每个时刻你拥有的和决策相关的信息总和，行为是你的决定， 奖励是你的此刻收益。而策略就是从状态到行为的对应关系。就拿方格矩阵走迷宫（下图a）的例子说，你的状态就是你的位置，你的行为就是上下左右走，你的奖励是中间的加号。行走的过程每一步都只与上一步相关，此刻拥有绝对信息（位置），因此可以简化为下图b的马尔科夫决策图框架。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fo343-0-0&quot;&gt;这里面只有策略是优化的目标。策略最简单的表格方法就是一个纵轴是状态，横轴是行为的表格，每个格子里是某状态下做出某个行为的概率（下图c）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如果你用刚提到的动态规划法来做优化，你还需要另一个表格，叫做Q表格， 这个纵轴同样是状态，横轴同样是行为，但是每个对应的格子里的内容是value， 数学上说就是某个状态和行为下未来收益的期望（d）, 我们通过学习不停迭代这个表格（e）。 有了Q表格， 你可以通过对随机性偏好的假设很自然的得到刚刚说的策略表格。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7899159663865546&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7Kf0JXXcicKyT1ZG6E23CBtMyN7gSIYlIObR2t8xWKooicujPYjkNqCQ5mg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;238&quot; /&gt;&lt;/p&gt;

a


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfCoicfghk7hfW0Go2dorPMkH6HEmnuvWGYUvGibdBtJUdgEnHXKkRpBuQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

b

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.225130890052356&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfQggzMLEIj83umLs9kNiaH4qjBQMx4V29ECGbH3vvJ1Tib6pU3J8uYuRA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;573&quot; /&gt;&lt;/p&gt;

c


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.225130890052356&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfXj1V9qyvdqcF0Vzia9SfiaEplbqLXmhqGt7O2eRu3gfGtPMWT0JHjvug/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;573&quot; /&gt;&lt;/p&gt;

d

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.7069444444444445&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfyJEYpPQ3EC2GzEyApRbIDF77FWTpGpflxiaI39LEibeWHtWYRFEXMDVA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

Q表格逐步迭代的过程， 先更新最靠近结尾的状态收益，在往起始状态推演， 最终得到每个状态行为下的未来收益。 图中的gamma是贴现率， 贴现率越低，状态间的收益变化越平滑

&lt;p&gt;&lt;span data-offset-key=&quot;9g54b-0-0&quot;&gt;然后深度强化学习从何而来?  现实生活中的状态太多，如围棋有3的361次方个状态，你的表格几乎可以布满银河系。所以你不能用表格法，这就自然的引入了机器学习。 因为机器学习，尤其是神经网络，本质就是函数逼近， 或者某种插值方法。 当表格大到填不完，我们就用一个函数来替代它，然后用神经网络来学习这个函数，这就自然的引入了深度强化学习。通过数据可以模拟函数，有了函数就可以把值函数的问题解决。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7584b-0-0&quot;&gt;DQN深度值函数算法最基本的深度强化学习算法，2015年发表的关于Alpha-Go的文章就是值函数法，策略梯度，蒙卡树，结合了CNN得到的算法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7Kf5srE5mpkXicBCPPe62EInLwW2df4NUe2LfCkHNLXXGUJGDFJTgmOvNw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfNeHtwFhQPmlW8CGHVarOdZLC28ibhicrsGAQCVGPpREY880th5CpT4Pg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;
&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-ratio=&quot;0.10966542750929369&quot; data-w=&quot;538&quot; class=&quot;&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; /&gt;&lt;/pre&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4peqn-0-0&quot;&gt;然而Alpha-Go再强大也不能解决所有问题，比如星际争霸，因为当下游戏画面当中的信息是不足以进行决策的，针对决策的信息并没有全部在一个画面当中呈现出来，所有就需要使用其它的方法。这也是我们的另一个主角RNN登场的机会。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;egipd-0-0&quot;&gt;我们用要一个最微小的例子来说明，还是那个走方格的问题，骷髅就是有危险的意思，我们希望走到有奖励的地方。 我只做一个小的改动将使得之前问题面目全非，之前的马尔科夫决策附加的条件就是当下的状态含有用来决策的所有信息，方格问题里， 这个信息就是位置坐标。而如果我没有位置这个信息， 取之以感知信息， 比如我只能感知我所在方格的周围两个方格有什么（下图中的骷髅或金币）。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;p5te-0-0&quot;&gt;注意如果我们处在下图灰色方格的区域（左右各一个），此时相邻的两个方格的情况是完全一致的（白色），也就是说我无法确定我是处于左边还是右边的灰色方格， 这导致无法决策正确的行为（左边和右边的正确决策是相反的！ 一个向左一个向右， 但是我无法确定是哪一个！）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfGqJpRTS8zD1tJUJv64Mww8RPn8BnrLA19hLx0hCoHRbhqA3xWYZpjg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;bedq2-0-0&quot;&gt;真实的生物天天生活在非完全信息环境下， 到处都是刚刚图中的灰色方格。 它们还得学到正确的行为， 那么它们是如何解决这个问题呢？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;4eu1v-0-0&quot;&gt;主要有三种方法：一种就是策略梯度的方法，虽然所知状态和信息是不全面的，我们可以利用概率的方法来学习。当不知道该往左走还是该往右走时，随便走出一步，这样有百分之五十的概率得到最后要的奖励（一个驴子在沙漠里， 怎么都要选择一个方向走，不走一定渴死），利用直接学习的策略函数也就是Policy Gradient解决掉这个问题。这个方法大家可以看到效率是比较低的， 如果时间有限那就死了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;ablgj-0-0&quot;&gt;另外一种方法就是引入记忆，你有了记忆， 事实上你就可以把一些不同时间点的信息凑成一个整体，你的信息当然比之前多。在上图走方格的例子里如果你知道你上一步是从左边边界来还是从右边边界来问题迎刃而解。 而最后一种就是加入精确的世界模型， 你的信息不全面， 但是我可以用我的大脑给世界建模， 这样一个不完全信息的世界在我脑补下就变成了完整的世界。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1kqh8-0-0&quot;&gt;引入记忆的方法，就很自然的引入了是RNN，它是类似模仿生物工作记忆的原理，信息放在记忆里面，将其作为决策的依据。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;a8guj-0-0&quot;&gt;RNN的网络结构和前向神经网络差距并不大，只是多了一个时间转移矩阵，即当下隐藏的状态再到下一步隐藏状态的连接。这个连接实际上是传递当下的信息到下一刻的过程，这个传递过程中包含了记忆，我们可以把过去的信息往将来不停的迭代，于是神经网络细胞当中就含有过去很多时刻的记忆，其实就是生物的Working Memory。LSTM只是在RNN的基础上多了一记忆细胞层，可以把过去的信息和当下的信息隔离开来，过去的信息直接在Cell运行，当下的决策在Hidden State里面运行。加入RNN以后就把DQN变成了DRQN，然后就可以走一些非常复杂的环境。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7Kfk56k6bIcM1zT9IOewKLJcSlaQicOfuoXic0ddW5UfomCU2pHfO1bkgpg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

RNN， U是隐层连接




&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7Kficj2E96qrs1L6E5oZdIWTaJehwregGsOGTwSFojNIAtfcPSy2icTVPZQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

LSTM： 加入Cell


&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7Kfhq3BYoVU1JL4Ss9LY1ApnLmJpAFsJ8TLI8lNWQze6bM7uOQiaZBkA8w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

DRQN框架 ， RNN网络是大脑，做出的决策影响世界， 世界又给这个大脑以反馈。

&lt;p&gt;&lt;span data-offset-key=&quot;bldb6-0-0&quot;&gt;下图是一个二维的迷宫，只能看到周围格的情况，就是刚刚描述那个问题的拓展版（相当于在沙漠里寻找水源， 你只能看到自己周围有什么，没有其他信息）。 需要我们做的是在很复杂的情况下搜索到目标在哪里，这就是一个导航的问题。左下角的红点就是起始位置，中心的红点是目标，最终学习得到的行为是： agent会直接走到墙上得到自己位置有关的信息， 然后从这里奔向目标，这就模拟了空间搜索的过程，RNN在这个过程里把不同时间点的信息拼接成一个整体。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.36944444444444446&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7Kf6ENuOB66bNK4fEicoK2p5jlXHffycrxUg2h6TouXtHEpLggNrHTOT9Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; /&gt;&lt;/p&gt;

二维游戏， 输入信息只有相邻格子的状态


&lt;p&gt;&lt;span data-offset-key=&quot;58pir-0-0&quot;&gt;RNN虽然有这种能力,  但是一旦空间复杂了它还是会蒙蔽， 因为空间是很复杂的，比如一个有很多屋子的宾馆， 这个时候就需要更强大的空间表示能力。这个东西还是可以通过学习诱导， 通常的做法是加入监督学习的成分，比如学习预测你在空间里的什么位置， 这个过程里， 模拟空间的能力会在RNN的动力结构里自动浮现出现。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c4n3b-0-0&quot;&gt;监督学习信号比较多而且目标明确，它可以预测走到哪里了，距离奖励还有多远，可以说给之前的强化学习添上了翅膀。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfkAwV0tlKQALyibDntKgGXZIx2ic3Epia7SNkBAuotaGwkpFRhoVvZCUiaw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

在2017年， 人们使用类似的方法让agent很好的打赢了doom   Lample, Guillaume, and Devendra Singh Chaplot. &quot;Playing FPS Games with Deep Reinforcement Learning.&quot; AAAI. 2017.


&lt;p&gt;&lt;span data-offset-key=&quot;bnbsa-0-0&quot;&gt;一旦加入监督学习，事实上我们就达到了刚刚说的第三种策略，引入世界模型， 只不过这个过程是一步步的而非一蹴而就。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;9nka2-0-0&quot;&gt;一篇发表在《Nature》上的论文就把这个东西更加推进了一部。同样是监督学习，但是它在基础学习基础上诱导RNN（lstm）在原基础上形成新的结构，这个东西竟然惊人的和小鼠脑中的栅格细胞相近。这个栅格细胞实际是把空间当作一个很多六边形组成的蜂巢网络来表达，每个细胞对六边形网络的端点位置是敏感的，而不同的细胞对不同空间周期（网格边长）的网格敏感。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;1n3eu-0-0&quot;&gt;这个方法的本质是建立空间的通用模型。 显然你的大脑不会给北京，上海，或者天津建立完全不同和分离的神经表达，必然的有一种空间语言来支撑所有的空间概念，而从一个地方到另一个地方，最底层的这个表达是不变的。这个东西可以看作模型之上之模型，这个东西正是这个栅格细胞。栅格细胞的每个细胞相当于一个不同空间周期的六边形网，通过组合这些六边形网，我们可以很容易的得到对空间相对位置的表达（很像傅里叶变化，每个栅格细胞是傅里叶变换的成分，被下游的位置细胞组合读取）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;fjiht-0-0&quot;&gt;有了这个细胞的网络会有更强的在空间当中运行的能力，一个标志性的表现在于可以在复杂的空间当中抄近路。如果路径发生变化（比如一个门堵死了），就会找次优的目标，也就是说有一种动态规划的能力，即具有空间行走的智能。在RNN的基础上加入适当的监督学习，从而产生与生物细胞类似的结构，具有了空间表达能力。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.6062407132243685&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfDdAJGCsbTCDURlia6Q0PEwjbTbtg7kUKH48ByHCSxbtdGZkNNCIGwtg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;673&quot; /&gt;&lt;/p&gt;

事实上，我们不满足于上述的结果， 有没有一个方法， 让agent学习得到一个通用的空间表示？ 这样生物才能真正学到对一般空间特征的泛华能力。



&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.4775641025641026&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7Kf6gVXmgZ6ypo63MbxVNXYU9TibpiaXPDAWd6fMAJ6uKRLbg1NsB43q6yw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;624&quot; /&gt;&lt;/p&gt;

生物对空间的表示， 是通过一个叫grid cell- place cell组合实现的， 这个组合的特点是上游的grid cell 提供一组特定空间频率的细胞， 然后通过一个线性组合（place cell)， 我们就可以得到对空间位置的描述。  这类似于一种对空间进行的傅里叶变换。

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcfh8M2ib5ZtShLeSLsVicG7KfmRsudSrliapsM7RwqTKqqlFQF2vGgnAX4bgq2vAXpIlYfpDGWpdYvJA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

在使用了这种grid的空间表示后，最大的特点是增加了agent的灵活性， agent真正更够进行某种空间规划了

&lt;p&gt;&lt;span&gt;最终我们可以这样总结RNN在强化学习的潜力：  RNN，作为一个动力学系统， 本身表达了过去，现在和将来的联系， 这可以看作是部分的， 或者全部的世界模型。 而强化学习， 作为一个对未来收益的优化， 可以看作一个序列决策问题， 你对系统的过去现在和将来了解的越透彻，这个决策能力就越强， 因此RNN天生和强化学习有某种契合。 &lt;strong&gt;RNN的这个动力系统， 可以说部分的，或者全部的表达了世界模型，因此， 它非但是解决局部马尔科夫问题的利器，更在免模型和有模型的强化学习当中构建了一个桥梁。  &lt;/strong&gt;&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;相关阅读&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383552&amp;amp;idx=1&amp;amp;sn=d10cf1e80137c23827a8a57f1605f3b5&amp;amp;chksm=84f3c941b38440577a067cd312d6683f193000781fe1da2310769b88b850e6eabf21a5a0626a&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;铁哥长文：神经导航简介 - 论deepmind最新文章&lt;/a&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383480&amp;amp;idx=1&amp;amp;sn=c4ca5c392e66df49f61bd0efb0997f9c&amp;amp;chksm=84f3c8f9b38441efe87e238ba063fe6e308e13a9d8460835429aa5093ab899136c9351a5ac52&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;从Q-learning的小游戏看阿尔法元技术&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;更深入了解，请阅读的文章如下：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Bakker B. Reinforcement learning with long short-term memory[C]//Advances in neural information processing systems. 2002&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;最早在强化学习里引入RNN的尝试，  主要是强调RNN可以解POMDP&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span data-offset-key=&quot;1788i-0-0&quot;&gt;Hausknecht, Matthew, and Peter Stone. &quot;Deep recurrent q-learning for partially observable mdps.&quot; &lt;/span&gt;&lt;span data-offset-key=&quot;1788i-0-1&quot;&gt;CoRR&lt;/span&gt;&lt;span data-offset-key=&quot;1788i-0-2&quot;&gt;, abs/1507.06527 7.1 (2015).&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这一篇接着2002的文章， 主要是承接了2015 deepmind 在DQN的突破，强调那些信息并不全面的Atari Game， 可以通过RNN（LSTM）得到性能突破&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span data-offset-key=&quot;2ksd8-0-0&quot;&gt;Mirowski, Piotr, et al. &quot;Learning to navigate in complex environments.&quot; &lt;/span&gt;&lt;span data-offset-key=&quot;2ksd8-0-1&quot;&gt;arXiv preprint arXiv:1611.03673&lt;/span&gt;&lt;span data-offset-key=&quot;2ksd8-0-2&quot;&gt; (2016)&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;导航领域的牛文， 介绍了在RNN（LSTM）下的深度强化学习里如何进一步加入监督学习， 获得性能突破&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Wang J X, Kurth-Nelson Z, Tirumala D, et al. Learning to reinforcement learn[J]. arXiv preprint arXiv:1611.05763, 2016.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;小众的神文， wang xiao jing 大神介绍了一种基于RNN的强化元学习能力， 一种举一反三的能力。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span data-offset-key=&quot;3pvui-0-0&quot;&gt;Banino, Andrea, et al. &quot;Vector-based navigation using grid-like representations in artificial agents.&quot; &lt;/span&gt;&lt;span data-offset-key=&quot;3pvui-0-1&quot;&gt;Nature&lt;/span&gt;&lt;span data-offset-key=&quot;3pvui-0-2&quot;&gt; 557.7705 (2018): 429.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;最新的Nature文章，  介绍了通过监督学习引导RNN（LSTM）产生空间栅格细胞的能力&lt;/span&gt;&lt;/p&gt;

&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-ratio=&quot;0.10966542750929369&quot; data-w=&quot;538&quot; class=&quot;&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; /&gt;&lt;br /&gt;&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcd9g9aFjBooVn5U4PP1EDF3ugVJrLlia2ELtxHbXUJs7SUPtRaxmkUBHhx3jLciaHXpx1ABVYwYBu9w/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.665625&quot; data-w=&quot;640&quot; /&gt;&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;span&gt;作者许铁，微信：ironcruiser &lt;/span&gt;&lt;br /&gt;&lt;span&gt;法国&lt;/span&gt;&lt;strong&gt;巴黎高师&lt;/strong&gt;&lt;span&gt;物理硕士 ，&lt;/span&gt;&lt;strong&gt;以色列理工大学&lt;/strong&gt;&lt;span&gt;（以色列85%科技创业人才的摇篮, 计算机科学享誉全球）计算神经科学博士，巡洋舰科技有限公司创始人,   曾在香港浸会大学非线性科学中心工作一年 ，万门童校长好战友。&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;5.896&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJNQDiajMVL3arkx9IJnm10baZ1RjdLTN2KH6SKHZqnzyGO5K0G3dNOwg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;750&quot; /&gt;&lt;/p&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;br /&gt;&lt;/pre&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;span&gt;许铁的《机器学习和复杂系统》这本书用物理学视角来看机器学习，目前已经正式上架。同时，许铁已在万门大学已开设机器学习原理和深度学习原理两门在线课程，即将于9月开设强化学习的在线直播课程。感兴趣的可以点击下图的二维码咨询。&lt;/span&gt;&lt;br /&gt;&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdtdhcvoVrIAictVtkxkfXSiambAf4zOLtDxdYq09GEcgnRltUzicyJbr7EcCX7bYVV3GKuKXdlFdQqw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;4.717741935483871&quot; data-w=&quot;620&quot; /&gt;&lt;/p&gt;







</description>
<pubDate>Sun, 12 Aug 2018 19:28:35 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/sAxMjmtY0C</dc:identifier>
</item>
</channel>
</rss>