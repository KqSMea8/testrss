<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>怎么样用深度学习取悦你的女朋友（有代码）</title>
<link>http://www.jintiankansha.me/t/PfqJe3yAlh</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/PfqJe3yAlh</guid>
<description>&lt;p&gt;&lt;span&gt;本文为巡洋舰的深度学习实战课程 预科准备。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;深度学习目前为止最有用的东西是图像处理，我们可以用它在极早期判断癌症， 也可以用它在茫茫人海里寻找犯人，但是要我说你能写一个小程序取悦女朋友， 你就不一定能信， 这一招叫艺术风格变换，就是你点击一下，就可以把你女朋友的大头照换成一个毕加索的后现代艺术作品（当然是取代还是找打要看你的艺术品位）。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.23965141612200436&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbYic4o3ws1KeJDFhvTm58UIPuMhxlGQ2UiciaAyZWHk9R0Hh8dkh5Fh9jyg/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;459&quot; width=&quot;459&quot; /&gt;入行需谨慎
&lt;p&gt;艺术风格迁移是一个古老而现代的主题 ， 多少艺术家为了描摹他人作品而竞折腰。 在出现了IT之后， 它也成为adobe之类的公司竞相追求的宠儿，却始终进展缓慢。&lt;/p&gt;
&lt;p&gt;而深度学习， 却可以轻轻点击自动完成这个任务， 铁哥在此给大家拆拆招 ， 看如何玩转神经风格迁移。&lt;/p&gt;
&lt;p&gt;我们说，神经风格迁移就是把一张图片的内容和另一个图片的风格合成的一个方法，比如说你给出一个猫的图片和一个梵高的自画像，就可以生成一只梵高的猫。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.43333333333333335&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbYr42IhDQYJ9YHANqFzwPcFkX4vB66wcpZKVNgYD2JCYBHCP5uWD1CNw/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; width=&quot;600&quot; /&gt;&lt;p&gt;在深度学习之前，机器视觉的工程师就尝试用各类神奇的滤镜提取图像的纹理信息，抽取取来的纹理图在经过某个变换放回到原图片里，就得到了一个新的风格的图片。　&lt;/p&gt;
&lt;p&gt;深度学习所作的事情，是把这个东西给自动化。我们利用卷积网络的深层结构提取的信息，来替代之前的各种滤镜。　&lt;/p&gt;
&lt;p&gt;首先，卷积网络不仅能够做猫狗识别这一类分类任务，在其中间层里，其实包含了丰富的有用信息，而这些信息，正是我们做风格迁移的基础。如果你可视化CNN的各层级结构，你会发现里面的每一层神经元的激活态都对应了一种特定的信息，越是底层的，就越接近画面的纹理信息，如同物品的材质。 越是上层的，就越接近实际内容（能说出来是个什么东西的那些信息），如同物品的种类。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.8819444444444444&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbYtHhWia3sSnXwsXtRXPP3Sgq9ibDZRIF0guMlvE30JnFmJwxjTVxBBibYw/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1164&quot; /&gt;研究人员提出的一套可视化CNN的方法，把深层的内容通过反卷积映射回图象，好比你关心什么，就给你投影出来(Visualizing CNN 2014 )。&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.41944444444444445&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbYuMdgmuZUu2Z4uSK6gGbIib0Sic19HEibKIVjYXDb6mkHAEVicKlj1RthSg/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;2068&quot; /&gt;底层神经元关心画面的材质&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.3472222222222222&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbY1KVNsUIqsIzTRcbzoqaySD0h1Eia9co1BVjGTgrCuSbVYBJqZg3icMAg/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;2178&quot; /&gt;深层神经元关心物品的种类
&lt;p&gt;那么好了，风格迁移不就是这么简单吗，把一张图片的底层信息和另一张图片的上层信息合成一起不久可以了吗？ 用适当的数学方法，我们可以在卷积网络的中间层里左手提取图象内容有关的信息，右手提取图象风格有关的信息。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.6888888888888889&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbYQx9EibUZNRW5l5nEdDzhYfScqPmKRUCDEkvt7BV7T5c91IficozfQTFg/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1764&quot; /&gt;用中间层的信息恢复的内容，可以看到不同层里里都可以找到风格和内容有关的信息， 但是层次越深， 具体的信息就越少， 而“实体” 的概念轮廓犹在

&lt;p&gt;看起来是的，我们可以通过一个已经训练好的CNN， 把一张风格图片和内容图片的信息都抽取出来， 然后拼在一起！&lt;/p&gt;
&lt;p&gt;为什么这里要用一个已经训练好的CNN呢？ 一个用分类任务训练好的CNN，通常已经具有了对世界大多数图像提取信息的能力， 因为图像传递信息的底层机制是想通的。 我们把这个网络连接的权衡直接共享过来， 图片一导进来， 网络就可以生成直接可用的特征！ 这正是迁移学习的原理。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.7507836990595611&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbYs7AoRBGTAIVlBicbHGFwuNQ4OIr1WY8Kg0HWLR8ROxlp6S0iabDJMnuQ/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;638&quot; width=&quot;638&quot; /&gt;这里我们导入一个已经训练好的VGG19网络，一种非常流行的CNN图像分类框架
&lt;p&gt;所有深度学习和机器学习，都是预先设定好一个损失函数，然后在进行梯度回传，这里也不例外，我们可以通过设定合理的损失函数，来解决问题。这个损失函数，正是一种能够测量生成图片与风格图片，内容图片距离的函数。　&lt;/p&gt;
&lt;p&gt;来，兄弟们，看我们如何设定这样一个函数。既然我们的深度卷积网络可以做到测量与内容有关的特征，　那么我们只需要在这个层次上找一下特征向量的距离就好了。　&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.6597222222222222&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbY4yZ09u9kBCIGxP4T5lWcaqkWVsRAjO4btfsIw7cwGA0tBCMY7XEiazQ/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1514&quot; /&gt;图像无非是高维空间的一个点，通过神经网络变换再经过特定降维方法处理后我们可以给它转化成二维曲面上的一个点， 我们会发现，在这个世界里， 狗在狗的国度 ， 猫在猫的国度。 而我们只需要度量不同图像的空间距离，就测量了内容的相似度。&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.26136363636363635&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbY2paPnepwZeWvZm9Yoz78JxwR5YsrbLFCEzgbNUNcS8bszXbic95NJyA/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;528&quot; width=&quot;528&quot; /&gt;哇， 这不就是定义距离的公式吗！
&lt;p&gt;然后呢，如何搞定风格，风格通常是一个艺术家眼中主关的有点虚无缥缈的概念，也就是我们通常说的感觉， 比如梵高或者莫奈的画，你没有经过艺术熏陶也可以得来。&lt;/p&gt;
&lt;p&gt;而在深度学习的角度下， 这种感觉却发现与不同神经元活动的相关性有关！ 也就是说，风格是深度网络神经元活动的某种统计特性！ 悄悄的，我们把艺术和数学对接上了。 统计果然是上帝的语言啊有木有！&lt;/p&gt;
&lt;p&gt;这里我们借助一个叫gram矩阵的数学工具，它通过测量同一层卷积输出不同通道之间的相关性（计算不同通道间的相关性，组成一个矩阵）给出一个对风格的度量。然后，我们在测量一下风格之间的距离不就行了吗？&lt;/p&gt;

&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.48333333333333334&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbY5sepmib4hJQbzL66Fnq348wTSibMmU5Z1flGNZR1yEcW18WoYEPBqpFA/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1570&quot; /&gt;把CNN某一层对应不同特征的神经元像摊煎饼一样摊开， 然后计算之间的相关性&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.325&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbYTyKWmy40p9TRlDgJGicrScDAtmWE3GficSxQw01aG6iaFC4x9033wE2kw/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1790&quot; /&gt;得到一个矩阵，矩阵的每个元素对应不同特征间的相关性

&lt;img class=&quot;content_image lazy&quot; data-ratio=&quot;0.3803680981595092&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbY6ItuRzYmgHJZWPJR1kCFraIpFEnBEOXgQVSxCQ4QKkicF083zdkia6ug/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;326&quot; width=&quot;326&quot; /&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.49295774647887325&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbYKrhVYKh7reuEI53qNHlrPxqHqV8bea2RxrX9Wj0msxcjXhpWoNzQZg/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;568&quot; width=&quot;568&quot; /&gt;这个损失函数就是gram 矩阵之间的距离！
&lt;p&gt;注意，衡量风格之间的距离， 我们是把不同网络层级间的gram矩阵的距离都计算一下加在一起，这样可以把不同层次度量的东西综合起来 。&lt;/p&gt;
&lt;p&gt;好了，到这一步， 大功告成， 把两个损失函数叠加在一起就好了。&lt;/p&gt;

&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.18055555555555555&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbY6NHs2k9libLbZmsD6fDURhyg7xwYTnMpz6YiaOdjIr1TkG2ccLCicjt1w/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1072&quot; /&gt;&lt;p&gt;目标函数的设计学问可大了，改变a和b的比例就能造成很多区别，大家注意风格图片的比例越高，图像就越纹理化。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.8736111111111111&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbY6H4zp6eorl2Soq5sL7ibMbGo6WIW2t6ltj6un6rroAHK7cicicpxInybw/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1422&quot; /&gt;风格的权重变大的时候， 那图像就变成了意大利瓷砖！


&lt;p&gt;然后我们可以做什么呢？ 梯度下降！但注意，这里我们优化的目标不是网络权重而是图像本身，这样我们就大功告成了！&lt;/p&gt;

&lt;p&gt;当然这里说的只是风格迁移的一种， 这种方法的优点是通俗易懂， 而缺点是速度很慢。 还有一个方法，是借用生成网络，直接给搞出来， 这个方法更快速， 更加适合工业封装。 我来给大家展示一下这个方法的实质。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.2611111111111111&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbYOZfamhMQlq3vu12hkUlXvcb3nqWQFZQmiaVCjicSicd7OLvJaEWX4IISA/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1852&quot; /&gt;像不像GAN的结构！
&lt;p&gt;哈哈，这样我们就可以完成一幅艺术作品交给家里领导了，但是不要忘记哦， 这件事给我们的启示绝不止这一个呀。 它给我们启示的是，我们深层神经编码的机制里，深度学习的踪影， 你对风格的认知，其实是和内容的认知一样， 是可以量化的，而不像某些艺术家所言， 完全主观，与数学无关。 不仅可以量化，而且这个信息是可以独立被提取的， 这种信息不是存在于某个神经元之上， 而是分布式的存在于多级神经网络的不同尺度之间， 通过每一层神经元的统计规律表达。&lt;/p&gt;
&lt;p&gt;虽然我们尚不知道这些猜想是否正确， 他们我们人类深奥的视觉处理机制提供了一种聪明的理解方法。&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;&gt;&lt;/ins&gt;  附： 代码, 看看用pytorch做出来是多么简洁：&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.20416666666666666&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbYkW3KqZibWm1Qnj2bLXnO94rj0clSaTHCbfElOBPu2YaAW30ZlNHJ2zg/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1618&quot; /&gt;计算内容损失函数&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.8236111111111111&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbYTTcNv6BZMQvH4Z3trGs1sXuNnGVxOUKZWgcN49vn8VraKK5NQL7pTQ/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1614&quot; /&gt;计算风格损失函数&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.7583333333333333&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbYOGnSUIGv8TG89XLcNbmCknZgTZ6gks7LnyvMm9iahTno5U1VTmpW6lw/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1616&quot; /&gt;设定模型主体！&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.4152777777777778&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibce26jm4VbZHNZy8LN13ejbYno7cPsax1X1Cbsw1oEiaibQVd8B3jnTpdYSrVBwicgVHb408AaXpKiaU1g/0?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1620&quot; /&gt;训练过程！
&lt;p&gt;&lt;span&gt;如果你对上面的脑洞感兴趣，欢迎关注巡洋舰的深度学习实战课程， 手把手带你进行深度学习实战， 课程涵盖机器学习，深度学习， 深度视觉， 深度自然语言处理， 以及极具特色的深度强化学习，看你能不能学完在你的领域跨学科的应用深度学习惊艳你的小伙伴，成为身边人眼中的大牛。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;许铁关于风格迁移的讲解视频查看，请点击链:http://pan.baidu.com/s/1c6Siaa  密码:2g61。&lt;/p&gt;


</description>
<pubDate>Tue, 28 Nov 2017 19:41:31 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/PfqJe3yAlh</dc:identifier>
</item>
<item>
<title>小学生都看得懂之 白话数据降维</title>
<link>http://www.jintiankansha.me/t/tfYe8PS62H</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/tfYe8PS62H</guid>
<description>&lt;p&gt;数据降维是数据分析中最常用到的一种技术了，这篇小文将试图用大白话讲一讲数据降维到底是什么，有什么用，常用的方法分别是什么？希望写的让小学生也能听懂，下面先为各位奉上这篇小文的思维导图。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdELMqvjP6J8kM6RDh7vCSlGHsrvSxVJgAQM6CwLX3q7uj4icwWAzv0fZQmNT0mv6dUaL5jmzRq5lg/0?wx_fmt=png&quot; class=&quot;&quot; data-ratio=&quot;0.40923076923076923&quot; data-w=&quot;975&quot; /&gt;&lt;/p&gt;
&lt;p&gt;这篇文章贯穿始终的一个故事场景是如何在一个小镇上将横纵的街道编号，例如科技三路和凤城五路的接口这样一个用俩个数字标注的位置信息，转化更一个只用一个数字标识的位置描述。在上述的例子中，原来的位置信息有俩个维度，我们就用笛卡尔坐标系中的X轴和Y轴来表示吧，而数据降维的目地就是要让数据的维度降低到一维。而在上述的小镇上，如果有一条铁路通过了小镇，而小镇上所有重要的建筑都在铁路边，那么就可以根据距离铁路的起点多远来定义每一个点的位置。当然，这样的定位不如用俩个维度来的准，有的地点离铁路远，但是远多少，在新的表示中就没有得到展示了。这说明数据降维不是无损的，会造成信息的部分丢失。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdELMqvjP6J8kM6RDh7vCSlDyqI9ibbzhUUlGZoCY02o6RGibj9I4dHlGsBuicQSNjb6sicpFhqxogVjg/0?wx_fmt=png&quot; class=&quot;&quot; data-ratio=&quot;0.24566473988439305&quot; data-w=&quot;692&quot; /&gt;&lt;/p&gt;

&lt;p&gt;那数据降维在什么时候应该应用了？回到小镇的例子，数据降维的第一个用途是数据压缩，如果你只能在一张小便签向一位你新认识的朋友写下你家的地址，便签上写不下是××路××号，那你可写铁路第五。而数据降维还可以去做数据可视化或特征提取，比如你要在小镇上开一家点，你先看看那里人群更加密集，你可以通过数据降维，做出那些地方周围的点多，从而人流更密。数据降维的第三个用途是异常值检测和聚类，例如你通过数据降维，发现小镇上的大部分人家都住在两个火车站附近，但是有一俩个家却不在这里，这样你就发现了小镇里那些特立独行的人，接着你发现俩个火车站附件的人家，一家都姓张，一家都姓李，这样，你就将小镇的人家通过数据降维，分成了俩类。&lt;/p&gt;

&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdELMqvjP6J8kM6RDh7vCSlB1aqBDEF7Fic0r5cV16EaxNPoXr114ukwKsePabVriaX2W5meAGQtXVQ/0?wx_fmt=png&quot; class=&quot;&quot; data-ratio=&quot;0.47074468085106386&quot; data-w=&quot;376&quot; data-backw=&quot;376&quot; data-backh=&quot;177&quot; /&gt;&lt;/p&gt;

&lt;p&gt;接下来说说各种做数据降维的方法，最熟悉的方法是主成分分析法，即PCA，在上面的例子中，主成分就是我们找到的铁路线，我们将小镇上所有的人家按照铁路开来的顺序，依次排序，从而得到了只用一个数字表示的距离。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;然而PCA有一个问题，就是他画出的这条线，一定要是一条直线，如果这个小镇上的人家不是根据铁路线，而是根据一条弯弯区区的河流，在河两岸安家的，那么PCA就无法找出这条弯曲的线了。而我们遇到的大部分小镇，都是逐水而居的，正如我们通常遇到的需要降维数据，而这就需要所谓的非线性数据降维方法了。&lt;/p&gt;

&lt;p&gt;第一种非线性的降维方法叫做核PCA，即Kernel PCA。理解核PCA，可以想象我们在标识一家人的住址的时候，先通过一个核函数的方式估算出这家人住的地方海拔有多高，由于水是向低流的，通过先将数据的维度提升，再降低的方式，从而在非线性的约束下实现数据降维。&lt;/p&gt;

&lt;p&gt;而第二种非线性的降维方式叫做tsne，tsne是一种很强大，也很费计算时间的非线性降维方法。tsne的逻辑可以这样理解，你要将小镇上每户人家的住址降低成用一个数字表达的数，由于在之前小镇上人们之所以张家和李家住的近，是因为他们自己是亲家，而刘家和王家住的远，是因为他们之间打过架。在之前的住址中，包含了这样拓扑信息，而在降维之后，你在地图上画出一条弯弯曲曲的线，不管这条线你是怎么画的，你都希望在这条线上张家和李家还是很近，刘家和王家还是很远。这就是tsne要保留的高维数据中的拓扑结构。可以想象，小镇里要是住户越多，这条线也越难画，tsne是一种迭代的算法，也就是说人民最开始画的线不一定是最好的，一次次向着优化目标的修改，直到达到相对较好的点。&lt;/p&gt;

&lt;p&gt;而这里要介绍的最后一种降维方法，来自于机器学习的门派，叫做自编码器。还是拿小镇上的例子来说，现在你假设要对镇上每户人家的住址用一个数字来编码，但你不知道该怎么办。于是你叫来你的双胞胎儿子，叫弟弟想办法用一个数字来标记每户人家，而叫哥哥在不知情的情况下，根据他弟弟给出的数字去猜测每户人家在那里，最初哥哥猜出的地方和这户人家本来的地方差距很远的，而等到几个月之后，兄弟俩有了默契，他们猜到的地方就差不多了。这时你将弟弟称为编码器，将哥哥称为解码器。而你叫来兄弟俩，问清楚他们各自是怎么做的。通过这种方法，你完成了任务。为了保证兄弟俩没有相互串通，你还故意在告诉弟弟每户人的住址时，故意说错一点，也就是给数据增加了一些误差，通过这样的方式，你可以确保兄弟俩是学到了数据中真实的规律，而不只是鹦鹉学舌。&lt;/p&gt;

&lt;p&gt;总结一下，数据降维是拿到高纬度数据后，不可不做的一件事，其既可以用来探索数据的结构，比如做聚类，又可以用来去找出数据中质量不好的离群点，数据降维的方式很多，最常用的是PCA及其衍生改进方法，以及相比PCA慢的多但也强大的多的tsne，另外还有来自神经网络的自编码器。这些方法可以结合起来使用，比如将一个10维的数据先用PCA降成5维，再用tsne降低成2维，这样兼顾的计算耗时和准确度。&lt;/p&gt;
&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdELMqvjP6J8kM6RDh7vCSl5ApQPrl6wTwJCAwmZiaOEibBpuwibc2V7xFm2VgpKdLvhk0w7Iu9UicZGg/0?wx_fmt=jpeg&quot; class=&quot;&quot; data-backw=&quot;239&quot; data-backh=&quot;136&quot; data-croporisrc=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdELMqvjP6J8kM6RDh7vCSlxdZg5QsD5SbWSMyveUovrXJdyJa83UFiaoy7zKQ4tEr5quMvojMrckw/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;239&quot; data-cropy1=&quot;38.687050359712224&quot; data-cropy2=&quot;135.8345323741007&quot; data-ratio=&quot;0.4100418410041841&quot; data-w=&quot;239&quot; /&gt;&lt;/p&gt;

&lt;p&gt;扩展阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383161&amp;amp;idx=1&amp;amp;sn=b27c2a0686d57b13daadcfd16cb35dac&amp;amp;chksm=84f3cb38b384422ecfca55da7b54978a8f3742605549566920507d5e032e16da1be50c1f5f97&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;你需要的深度学习数学基础： 从入门到进阶&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;


&lt;p&gt;以下的内容不适合小学生哦，一个是R语言中对应的包，而是一些测试题，测试你对这篇小文的理解。&lt;/p&gt;

&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdELMqvjP6J8kM6RDh7vCSlicMrpaVDKNMrYPHn8vxKubz4pO5ZhsS9k0cxxCiaIWkjgF6wLzEHfaog/0?wx_fmt=png&quot; class=&quot;&quot; data-ratio=&quot;0.5093240093240093&quot; data-w=&quot;858&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面是想深入了解的读者可以回答的一些问题，前四提单选，后两题多选。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdELMqvjP6J8kM6RDh7vCSllqCnCcA6uAjibdRho0DQQJibxdvicBib4fT3tUm3iahWichdUNOzxGyHj0iaw/0?wx_fmt=png&quot; class=&quot;&quot; data-ratio=&quot;1.0521042084168337&quot; data-w=&quot;499&quot; data-backw=&quot;499&quot; data-backh=&quot;525&quot; /&gt;&lt;/p&gt;

&lt;p&gt;原创不易，随喜赞赏&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9397590361445783&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd4icdSNQnx98Zicw7nnP3icPycqI1uRMvRxCezhYm4xQKdbiamvHVmC19a0o7YS03eqTrIL9QJS4wS4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;249&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;欢迎关注&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382937&amp;amp;idx=1&amp;amp;sn=d4510592a837c44fe75393c2578698d8&amp;amp;chksm=84f3cad8b38443ce6adcd155a2c45ee7707b4a9d8e48c05728aa7e93862475832bf89617025f&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;巡洋舰的深度学习实战课程&lt;/a&gt;， 手把手带你进行深度学习实战， 课程涵盖机器学习，深度学习， 深度视觉， 深度自然语言处理， 以及极具特色的深度强化学习，看你能不能学完在你的领域跨学科的应用深度学习惊艳你的小伙伴，成为身边人眼中的大牛， 感兴趣的小伙伴可以点击阅读原文。&lt;/strong&gt;&lt;/p&gt;


</description>
<pubDate>Mon, 27 Nov 2017 19:06:34 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/tfYe8PS62H</dc:identifier>
</item>
<item>
<title>多余的话 借深度网络说说最近发生的几件事</title>
<link>http://www.jintiankansha.me/t/i6XlKO9l9h</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/i6XlKO9l9h</guid>
<description>&lt;p&gt;关于刘鑫的事情，我曾经写过一篇明知道不讨喜会找骂的小文-&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383146&amp;amp;idx=1&amp;amp;sn=052ca08472d6e71c1343266ef6dacfbc&amp;amp;chksm=84f3cb2bb384423d04dc390a568db078d4d5abd8ecd283cd30ac8bbd50179daa00a4542d2e82&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;Do not pity the dead. Pity the living&lt;/a&gt;，写的是关于这件事有关的一些我觉得有关系的格言。将这些格言组织成一篇有论据，更重要的是有观点，也注定会更讨喜的文， 对我来说一点不难。但我从来都不是为了讨读者的喜欢而写作的，有没有人来读，我根本不care，也别给我说什么媒体和个人写作的区别，都不过只是一个说人话的地方。我在乎的只是我在写作的时候是否有所提高，永远都是写给一俩个人的。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;记得曾经的孙志刚还是小悦悦的事件，当年我一个要好的朋友因为这件事很生气，很难受，哭着对我说这世界怎么这样，我不知怎么去安慰。就陪着一起读书，一起查资料，想象我们要为这件事做一期访谈类的节目，类似铿锵三人行吧，我们查了很多的书，比如路西法效应等社会心理学的研究，还有小说和影视作品中的话，以及哲学家关于正义的讨论，后来我们终于不那么生气了。&lt;/p&gt;

&lt;p&gt;如今又是一件令我们哭不出声音的事情出现。我觉得关于这件事，是该写点什么的，我喜欢写写新奇的角度，那就按照我熟悉的写起吧。最近看到一个微课，名字是阿尔法元100：0完爆阿尔法狗的给人类的三个启示，说的是人类可以从深度神经网络的架构中能学到些什么。那就照猫画虎，说说从深度学习的角度来看，三颜色这件事该怎么去看。我这里会少谈时事，多讲技术。&lt;/p&gt;

&lt;p&gt;所谓学龄前教育，就如同神经网络的参数初始化。任何一个有过实战经验的人，都知道参数初始化的重要性。不止会影响模型收敛的速率，也就是需要花更多的时间，才能够找到一个相对好的解，还会影响模型的泛化能力。举一个极端的例子，如果你对模型初始的参数都设置为一样的值，例如0，那么神经网络就变成了一个确定性的模型，也就是无法在之后的训练数据中，无法学到任何需要用到概率的判断。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;那么该怎么对神经网络的参数进行初始化了，常用的有俩种方法，一种是随机给每个权重一个符合均匀分布的数字，比如拿一个公平的骰子，从1到6的数字随机之中选一个。第二种方法是给这些权重随机选择一个符合正态分布，平均为0的数。我没有尝试过，如果给参数初始化时，随机给他们分配一个符合尾巴很肥的幂律分布的随机数会是怎样。可以推算下，对于那些被随机分配到初始权重接近负无穷的点，也许这些点的权重很难通过训练回到他们应有的样子了，这不是稀疏编码，让训练好的神经元随机的失联，而是从一开始就将这些信息丢掉。就假设我们是训练识别猫狗图像的神经网络吧，这意味着有些时候，我们的神经网络会怎么都认不出图中的动物的耳朵在那里，也许缺少了耳朵形状这一个特征，对网络整体的影响并不大。但正如在雪崩时，没有一片雪花是无辜的。但凡涌现的系统，每一个输入的信号，都可能成为那影响飓风的蝴蝶之翼。&lt;/p&gt;

&lt;p&gt;接着来说说深度学习的深。为什么深度学习的神经网络需要那么多层了？这个问题的答案，我最初的回答是因为局部感知，也就是先只看整体图景的一个局部，正因为你限制了自己的任务，从而使得你能够更准确的完成你的任务，正如经济学中讲的分工带来效率的突飞猛进，如果每一个辨别能力不那么强的神经元，可以通过较少的训练就能够很好的完成需要整合局部信息的任务，那么通过层次化的管理，例如深度学习中的KPI--交叉熵，那么就可以聚沙成塔，完成复杂的信息整合。&lt;/p&gt;

&lt;p&gt;但是后来的我回答这个问题，却会说为什么深度学习需要这么多层，是因为每层的神经元都需要做到权值共享。所谓weight sharing，就是让神经元们有一套普世的价值观，这不止对加快网络的收敛，提高训练的速度很重要，更可以增加模型的泛化能力。若没有了权值共享，那么你以为你身边的人和你有一样的价值观，等到不知什么时候却发现那么对那些你以为天经地义的事情，你们都有着不同的观点，这时你就会发现，网络的深度变得没有多少意义，因为你无法根据自己身处的环境，判定你处于网络中的那一层。&lt;/p&gt;

&lt;p&gt;所以我觉得权重共享是比dropout更应该向所有人普及的一个概念，dropout说的是在面对不确定的未来时，通过小的可控的失败来避免大错误，类似反脆弱的概念，而权重共享却关系到我们每一个人该怎么去交流，关于权重共享，我曾写过一篇文 &lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651382656&amp;amp;idx=1&amp;amp;sn=cf461c00d2af8b1cf1afeb2c5622aa47&amp;amp;chksm=84f3cdc1b38444d75ba2f93162acab88141e0887f9a04e6cef80cfd4591e7d56d963c2ebf131&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;从深度学习中的weight sharing说说共同价值观&lt;/a&gt;，可以参考。&lt;/p&gt;

&lt;p&gt;接着说一说深度学习中最大的魔鬼，也就是梯度消失和梯度爆炸。先说梯度消失，今天公众号李松蔚发了一篇文&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA4NTI3NTkyNQ==&amp;amp;mid=2654002999&amp;amp;idx=1&amp;amp;sn=1695e3987dc94252a429790c900aa7af&amp;amp;chksm=841e1b4db369925b83a2d0a997fad15b8ff9c2d9152fc754d8b465dc883b638c895cb5f7a9bb&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;不只是被虐的孩子，整个社会都应激障碍了&lt;/a&gt;，我觉得应激障碍这个词，很形象的说了梯度爆炸是什么。由错误驱动的学习是一层一层进行的，但学习的过程中使用的信号是上一层错误的偏导数，偏导数是只关注变化的，在一层层的信号传递链，变化被放大，也许只是一件小事，但就如同玩电话传消息的孩子，会将原本的信息扭曲，从而使得即使网络本来有很多层，但深度越深，得到的反馈越少。&lt;/p&gt;

&lt;p&gt;只关注变化，这很想《怪诞行为学》中描述的锚定效应，更概括的来说，是人的情绪，就是这样一个短时的偏导运算符，只关注相比别人，你得到了多少。当上海廉价的幼儿园和北京的高价幼儿园依次出事，社会作为一个整体由不得会反应过度。情绪的链式传递，导致了更为关键的信息被忽视，比如长期以来对基础教育尤其是幼儿教育的投入不和法律缺失。不要以为找到了幕后的黑手，就算是深度的思考了。面对每一个都可能会切身面对的社会问题，每一个人都需要拿出创业者的激情来。既然现在幼师的准入门槛还没有明确的规定，以剔除那些本身不喜欢小孩的人，那么能不能通过社交网络中留下的痕迹，通过APP中的行为测试，去识别你孩子的幼儿园老师是不是喜欢孩子。&lt;/p&gt;

&lt;p&gt;接着说说梯度消失对应的梯度消失。这里可以看六神磊磊今天发的文 &lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA4NDEzNTMyMA==&amp;amp;mid=2650316540&amp;amp;idx=2&amp;amp;sn=5e0bb1af48ae7bd1aeafb294f3c587f5&amp;amp;chksm=87e7e20bb0906b1dee12267f6e7c2f876a82c7bd5df2b6f6eba7a48061f9a28c2b0b1593b5dc&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;没有中间层的结果，就是直面下层的火力&lt;/a&gt;。我们常说米国这二十年来中产阶级消失了，对应到深度学习中，这个问题就是梯度消失。消失的中层，使得哪怕预先优化好的网络结构变成了一张白纸，制度不管用了。&lt;/p&gt;

&lt;p&gt;吵闹但无序的底层无法和自以为国师的精英上层对话。你说你拆除违章建筑，是合法合规，就理应如此。而他们则说着要生存，说着自己的孩子已经受了多少不平等的对待。然而若是送快递的小哥和收快递的白领之间能够说几句贴心话，那么这些白领也许能够写出带着感情的文章，写成内参，让更上层的决策者知道自己的每一个决策究竟对于活生生的人意味着什么，那信息的传递就不算脱节。然而若是收快递的白领也人人自危，觉得自己不过是长的肥一些的韭菜，那么我们就说梯度消失了，模型中离输出层越远，能学到的东西越少。&lt;/p&gt;

&lt;p&gt;任何社会运都行在一个变化无穷的环境中，整个社会可以看成是一个需要不断优化的神经网络。而面对复杂的环境，本来深层的网络是能够相对更好的应对的。然而，正如做科普的童鞋常常觉得为什么科普这么难，不靠谱的养生神帖那么多。本质的原因不是科普文写的不好，而是由于要学习的网络太深了，而在向中间层的传递过程中，出现了梯度消失。&lt;/p&gt;

&lt;p&gt;而解决梯度消失或者梯度下降的一个常用方法，就是批量正则化（batch normalization），也就是在每一层的时候，都对要传递的信号进行一下平移和拉伸，使得他们呈现为平均值为0，符合高斯分布的一组变量。通过批量正则化，网络的每一层都会拿到分布的相似一组信号，这样做的好处是让网络学的更快，同时缓解了梯度消失/爆炸的问题，如果你得到的初始信号不包含上一层的偏见，那么你学习中也没有偏见可以放大或者忽视。&lt;/p&gt;

&lt;p&gt;而这需要在神经网络的每一层之间，加上一个专门正则化的处理层。而这正是媒体在一个成熟的社会里应做的事。好的媒体不是1984，那对应的是梯度消失，或者是美丽新世界，那对应的是梯度爆炸。好的媒体用每一层能够听懂的话，去克制的说出，更多的时候是一遍遍的复述普遍共享的价值观和事实。就如同精准饮食，针对每一层的文化背景，去说他们能听懂的话，去讲他们愿意听的故事。&lt;/p&gt;

&lt;p&gt;人人都能发声的时代，如果你只是表示情绪，那么你也许贡献的更多是噪音，若你只是想获得关注，那么你传递的更多是别人想要的而不是别人真的需要的，写任何文，我总觉得要做的既是不得不写，又清楚知道自己写的不过是篇多余的话，才算是为自己而写。古之学者为己 今之学者为人，不可不戒。&lt;/p&gt;

&lt;p&gt;原创不易，随喜赞赏&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9397590361445783&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd4icdSNQnx98Zicw7nnP3icPycqI1uRMvRxCezhYm4xQKdbiamvHVmC19a0o7YS03eqTrIL9QJS4wS4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;249&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383146&amp;amp;idx=1&amp;amp;sn=052ca08472d6e71c1343266ef6dacfbc&amp;amp;chksm=84f3cb2bb384423d04dc390a568db078d4d5abd8ecd283cd30ac8bbd50179daa00a4542d2e82&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;Do not pity the dead. Pity the living&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383124&amp;amp;idx=1&amp;amp;sn=80b21ed5c144027b12388abf85c7dccb&amp;amp;chksm=84f3cb15b38442034b620347e8530958e53546d377e990c5a4c8bd111a3238be44926496b57b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;人的价值在于提问-读《Human are underrated》&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

</description>
<pubDate>Sat, 25 Nov 2017 12:05:41 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/i6XlKO9l9h</dc:identifier>
</item>
</channel>
</rss>