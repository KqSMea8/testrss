<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>[原创]强化学习最小手册</title>
<link>http://www.jintiankansha.me/t/w9o1udg7Ui</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/w9o1udg7Ui</guid>
<description>&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;img data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.665625&quot; data-w=&quot;640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcd9g9aFjBooVn5U4PP1EDF3ugVJrLlia2ELtxHbXUJs7SUPtRaxmkUBHhx3jLciaHXpx1ABVYwYBu9w/640?wx_fmt=jpeg&quot; /&gt;&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;span&gt;作者许铁，微信号：ironcruiser &lt;/span&gt;&lt;br /&gt;&lt;span&gt;法国&lt;/span&gt;&lt;strong&gt;巴黎高师&lt;/strong&gt;&lt;span&gt;物理硕士 ，&lt;/span&gt;&lt;strong&gt;以色列理工大学&lt;/strong&gt;&lt;span&gt;（以色列85%科技创业人才的摇篮, 计算机科学享誉全球）计算神经科学博士，巡洋舰科技有限公司创始人,   《机器学习与复杂系统》纸质书作者。曾在香港浸会大学非线性科学中心工作一年 ，万门童校长好战友。&lt;br /&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-ratio=&quot;0.10966542750929369&quot; data-w=&quot;538&quot; class=&quot;&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; /&gt;&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;br /&gt;&lt;/pre&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkE1ckKibaar2bTdxLtaicqVBEXSGvAb10W7tbXkPowUKteMDHfIvdMxicjg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; width=&quot;640&quot; /&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;强化学习越来越受重视，它为什么很重要，用一张图就够了。想像一下我们是人工智能体，我们有强化学习和监督学习可以选择，但干的事情是不一样的。面对一只老虎的时候，如果只有监督学习就会反映出老虎两个字，但如果有强化学习就可以决定逃跑还是战斗，哪一个重要是非常明显的，因为在老虎面前你知道这是老虎是没有意义的，需要决定是不是要逃跑，所以要靠强化学习来决定你的行为。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;强化学习有哪些实打实的应用呢？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;只要在问题里包含了动态的决策与控制， 都可以用到强化学习&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;1， 制造业&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;强化学习之于制药业有一种天然的契合 ， 把强化学习翻个牌子换个叫法， 也可以叫做控制论， 学习控制机器手的精确动作， 比如让它自动的做比目前所能及的更复杂的事情， 强化学习在制造业的应用潜力是显然的 。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.6608946608946609&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEPPbIfYFaGoX3ylAox1oc8k1icMPuViaxTr6sX7ZQMRYpr5Qqa4g73G9g/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;693&quot; width=&quot;693&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;2， &lt;span&gt;无人驾驶&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这就不用多说了， 开车本质是个控制问题, 　自动驾驶不仅需要模拟人类行为，　还需要对前所未遇的情况进行决策，　这需要强化学习。　&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkE1c5ic9cQAN8gRKeh7QRFZR4CUy6adse9x5YozN999AKlXz1yh4ocjwQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;744&quot; /&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;3, &lt;span&gt;智能交通&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;智能交通， 显然这里包含了非常多的决策与控制问题， 就拿目前的共享汽车行业 ，滴滴和uber的派单系统时时都是一个动态的决策， 如何把正确的司机和乘客连接在一起， 如何让车辆调动到需求量最大的地方， 这些都要时时的考虑各种因素调整决策。 我们说这里面既包含了效率的问题， 也包含了乘客的安全。 比如这一次滴滴的事故如果修正强化学习的效用函数， 是有可能避免的。 当然除了派单和调动问题， 在每个十字路口交通灯的控制等， 整个城市里的立体交通网络的协调， 本质都是强化学习问题， 所以强化学习在智能交通大有可为 。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5840840840840841&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkERRP9rENk9ibS9x8S2H6ibKh94pau9I4nHc9WJJ8NDjvCFiaSB63lt7ArQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;666&quot; width=&quot;666&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;4，&lt;span&gt;金融&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  金融的核心， 交易， 是一个动态控制问题， 即使你不能完全预测明天股市的涨跌， 你依然需要直到我今天要不要下单，下多少单， 这，就是一个强化学习的决策， 它可以影响明天的股市， 也会在非常长远的时间里让我收益或亏损。机器交易，本质是个强化学习问题。 当然，金融里能够应用强化学习的绝不仅仅这一个。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.425&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEFQMZFOlDE7tADqGaEaUksjWWzpWl6hgPJlIXxzx00Pkebl2tIGaqrw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1140&quot; /&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;5, 智能客服&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;智能客服本质是个强化学习问题， 如果你把它处理成监督学习问题， 那个对话机器人只能照猫画虎， 不能够真正从顾客的好恶的角度出发来发言， 而如果用强化学习， 那么机器人学习的就是如何正确的决策， 每句话都是为了最终讨得顾客欢心。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEWJhXH4PMibyjIlVPBQ6K6FNdjofKHSEI99qNBiaVSbzWUngZDibhAc9LQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1047&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;6， 电商&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;电商的本事是如何吸引人买更多的东西， 因此我们买了一个东西后它总会在下面给我们推荐其它的东西。 然后我们看到了一个新的东西， 又会点开下一个连接， 这样一步步的就买了一大堆东西， 这样在每一步给你展示不同东西吸引你上钩的过程， 也可以看作是电商系统的动态决策过程， 是一个强化学习问题。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5671875&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEliaul1IicIAzicEuMFicibeic5mx2EXMd6zo2ZmKHg1xbibSlwP7ZMscB5Uow/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; width=&quot;640&quot; /&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;7， 艺术创作&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;艺术创作领域看起来与强化学习无关， 事实上它可以很灵活的把人类的好恶加在强化学习的过程里，通过强化学习， 机器作曲可以自发的得到取悦于人的风格，也就是范式。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.4777777777777778&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEictGRHAWicDXwRnVkUNtyXdz7ibEGiaegnF4ysFNhyAzvEmnFCz02OPtJg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;986&quot; /&gt;


&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;强化学习的基本要素&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;如果说监督学习的基本框架是函数拟合， 我们常用的语言是特征， 标签， 那么强化学习， 就有另外一套语言， 这套语言的元素包括&lt;span&gt;状态， 行为， 观测， 奖励&lt;/span&gt;。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;每一个元素都有很多可以说的地方， 首先看&lt;span&gt;状态&lt;/span&gt;s， 状态是什么呢？ 它指的是智能体（agent）所在的环境里所有和游戏有关的信息， 它起到的角色类似于监督学习里的特征。既然如此，状态的数学表示你可以认为和特征是类似的， 也是类似于一种函数向量的形式， 我们说状态空间， 正如监督学习的特征空间， 是一个维度很高的几何空间。 状态特征有连续和离散之分 ，会影响学习算法的基本性质。 我们可以思考走迷宫和平衡摆的例子来思考状态的可能表示有哪些 。在走迷宫的例子里， agent所在的位置作为一种状态， 而在平衡摆的问题里， 这个状态就是角度。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;再看&lt;span&gt;观测&lt;/span&gt;， 观测是学习体可以接收到的状态有关的信息， 有的时候行为体可以收到环境的全部信息， 也就是整个状态， 但是大部分时候则只能收到非常局部的信息。观察可以看作状态的一个函数O（s）， 这个O（s），正是对应真正的系统输入， 会决定下一步的行为A（action）， 为了描述简单，我们不必区分状态和观测。&lt;br /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;再来看行为，所谓行为，是指智能体的&lt;span&gt;决策&lt;/span&gt;，某种情况下我们可以认为它就是监督学习要求的那个y， 或者预测， 但一个决策与预测不同的是，我们并不能马上取得一个信号告诉我们这个决策对不对， 只有在游戏的最后 ，我们才能从整个游戏的收益反观当时的决策好坏。 另一点是， 它可以间接的影响状态， 环境等因素， 因此， 比起深度学习里的预测更具有“反身性”， 也就是说这个y会影响下一步的x。 这种输入到输出的闭环形式也是强化学习和监督学习的主要区别之一。　&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;决策的数学形式类似于监督学习的分类问题， 每个行为类似监督学习里的类， 这也是容易理解的， 虽然状态可以取无限的数值 ，但我们的行为决策事实上往往只可以取有限的几个值， 比如在走迷宫的例子里它就是上下左右， 东南西北。 因此， 行为的表示往往是有限的离散的数值， 通过类似字典的东西赋值。 决策是一个随机变量， 因为在某个状态下的决策， 很多时候包含随机性， 这个随机性使得这个决策函数变成了条件概率的形式P（a|s）， 通常我们给它换个名字叫做pi。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;最后看&lt;span&gt;奖励&lt;/span&gt;， 奖励是什么呢？ 奖励就是在某个行为之下环境给我们返还的一个反馈信号， 这个东西正是智能体在游戏里所追求的 ， 奖励和状态有关， 也和决策有关， 比如多臂赌博机问提， 你的奖励直接就是你的行为（选择摇臂）的一个结果。奖励具有随机性， 同样的条件性， 有的时候我们可以得到奖励， 有时候没有， 因此， 它也是一个随机变量， 理解这一点非常重要， 因此才可以理解很多的后面的算法。 奖励可以是正向的，也可以是负向的（惩罚）。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.8597222222222223&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEjFBzNFcGulOr8XicbMINSzX2wtfdicyia9EQTbLasqdBicriaUg2uTtmNtA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;802&quot; /&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.6458333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEm8UL7DM70dJIia0kD2puTJOD9RicQH3U5JyKlNYW19087iaE6XUFib1OoA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;955&quot; /&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这样我们就有state（observation）- action - reward 这样的一个组合。 或者说环境给你一个state， 然后智能体得到一个action ， 这个action改变环境， 并且环境返回智能体一个reward，如此循环， 当然在真实的游戏下我们并没有这样机械的一步步的过程， 而是一个连续的整体， 这种机械的方法是为了让问题可以轻松的被一个程序解决。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这样的思路和图灵最早提出的图灵机智能模型具有异曲同工之妙， 而图灵机被认为是智能产生的基本模型，因此你也可以理解为什么强化学习和强人工智能有关。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;从状态到行为action的函数，也就是刚刚提到的那个条件概率， 通常称之为&lt;span&gt;策略&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  ， 犹如通常意义上说的战略， 也就是一个行为的指导方案。 当游戏结束的时候， 我们把所有环境给我们的奖励加在一起算分， 越好的策略得到的分数越高， 这就是强化学习的本质。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;马尔科夫决策与动态规划&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;那么如何得到一个好的策略呢？ 这就是强化学习的中心问题， 大家以看就知道这本质上还是一个优化问题。 那么整个后面的篇章都围绕这个展开。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;如何得到好的策略？如果游戏没有很多步，而是一步就可以拿到奖励，　那么我们只需要写一个函数作为总奖励，　这个函数里自然包含策略函数，然后直接对它取最大即可。　当然，　如果奖励的概率函数未知，　那么就会引发一个探索与收益的问题。此处不再详述。　&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们还是展开简化大法， 我们用一个马尔科夫决策的东西， 把这个东西大大的简化。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;什么是马尔科夫决策？ 你观察到我上面的描述没有，这里面包含的东西大部分是离散的，问题里的大部分元素， 都可以描绘成条件概率的形式。 如果我们假定从state 到 action的那个条件概率P（a|s）， 从这一刻state到下一刻state的那个状态是P（s|s，a） 从环境里得到奖励的那个条件概率是P（r|s，a）。 在这个语言系统下，马尔科夫决策就是假定P（s|a,s）与P（r|a,s） 都具有这一步的状态和决策有关，如果是这样， 我们的决策函数P（a|s）也仅仅需要考虑当下的状态， 这使得所有问题的数学形式大大简化。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5046296296296297&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEZGGlSgCYDsFVhibib6xtS8UIOzxaYJnssDGgepHfwvZo5As8ia9eoQ1qg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;432&quot; width=&quot;432&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;环境决定的那两个条件概率P（s|a，s）和P（r|a，s）往往不是已知的， 或者说极为的难以求解， 比如在围棋在某个棋局之下你的某一步走子所能引起的对方的变化会十分复杂你根本无法求解。而一旦它是已知的， 整个game将变得十分简单。 由于它们往往不知道到的， 如此才需要引入整个后面的强化学习体系。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;有了马尔科夫决策这个框架，我们就可以看看如何推得我的最优策略。 强化学习的优化问题， 本质围绕三大主线， 一个是你要考虑的是未来的收益， 而整个未来的收益是不确定的。 另一个是你要考虑探索和收益， 假定你在游戏里走是选择开一扇门有奖励你就没完没了的开一扇门， 这样就陷入了局部收益最大的陷阱， 而忽略了全局更大的收益， 但是我们的游戏时间又有限， 不可能一直把时间花在探索， 这样就有一个探索和收益的平衡问题。第三是奖励是稀疏， 在游戏里往往是在很少的时刻会收到环境给的奖励， 你确要根据这个信号去学一个非常连续的动作控制流。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们先来看如果已知前面环境决定的两个条件概率如何求解策略。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;首先为了解决当下收益和未来收益的问题， 我们需要一个新的数学函数把当下和未来的收益统一起来，这就引出了另一个中心角色 - value 价值。 这个概念想办法把我们把当下和未来的收益统一在一起， 这个函数的定义方法是把当下的奖励和未来的奖励加在一起， 由于奖励本身就是随机变量且未来是不确定的， 我们要是把奖励都加在一起， 依然得到的是一个随机变量， 你要衡量一个随机变量的大小， 只能对它取期望。 令一点， 你是否觉得当下的奖励和十年后的奖励应该值一样的钱呢？ 你是否觉得如果把所有的奖励都加在一起， 如果游戏很长， 这个数字会趋于无限呢？ 如何解决这个问题？ 我们做一个贴现率， 让未来的收益乘上一个和时间有关的乘子， 这样最后把求和变成等比数列求和， 这个值就收敛了。 。 我们就得到了这个统一当下和未来收益的函数- value 。 它的定义就是现在和奖励和未来的奖励乘以时间的贴现加在一起的期望。 因此得到value的定义&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.39267015706806285&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkE2SCV9yyGicM8S8K5gQk6ov37o2S1FDHklNpsQbg2icKRQq7jmycPe3cg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;191&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这个期望依然是依赖于一定条件的，它依赖于当下的状态值， 或者说状态初值， 因此， 它具有条件期望的形式。 另一个关于值函数要知道的要点是再整个过程里唯一由智能体确定的是它的策略pi（a|s）， 其它都是环境给出的，不同的策略下， 值函数显然是不同的， 因此这个v（s）也是依赖于策略的， 通过我们pi写在v的右下角就是提醒大家注意这一点。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;好了， 现在我们的强化学习的优化问题就换了一种说法，我们要优化我们的策略， 最大化我们的这个value函数， 当然是所有状态下的value函数（是一个向量形式V（s））。 假定我们知道环境的动力学： P（s|a,s）和P（r|a,s）, 那么我们可以通过直接展开的方法求解这个value， 然后直接在这个基础上做优化。 加入我们的游戏只有一步：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.21019108280254778&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEr5naDWibGfZMofiba5czyAElVf3VCHCrRib5WAhqvdrP1JXUcGPvLFjDA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;314&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;大家可以像对于一个动辄几百个步骤的游戏， 这样展开下去要多繁琐，我们有没有其它方法对付这个问题呢？ 这就引出了一个求解强化学习优化的最基本方程 - 贝曼方程，这个方程的内容说的是可以把值函数按照定义展开为当下的奖励， 和下一步的值函数乘上贴现因子的值的和。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.08012326656394453&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEubc5ticMJAxaAj6knC82GU4lKKC0kVf32OPTrRKUI89rFdT18e00dSQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;649&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;如果我们所针对的不是某一个特定的值函数， 而是刚讲到的那个针对所有初始状态的值函数向量， 且我们假定所有的状态只有有限个（在马氏问题里一定成立的）那么这个关系式的左边和右边就同时包含了这个值。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.2781065088757396&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEicHrcGBpZGqecWPvEWDcDrUF6uKMr4G04NqVdw5mZBgKnONSDHnPoTg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;169&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这样我们就得到了一个左右两边都包含v向量的方程， 由于两边的关于环境的两个条件概率都已知， 我们可以通过线性代数的方法求解。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;如果我们得到了v的值， 再来看看如果去改变我们的策略使得这个值最大。 我们还是把最优化方程展开为当下的奖励加上贴先后的下一步的值函数的形式:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.09422492401215805&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEiauoaLK9IxU5bibzORe009Qp0PIRnZHiaohczYeQFO7FSF76HlYkxkLdA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;658&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;为什么要做这样得分解？ 这里包含了贝尔曼优化的关键思想， 这个公式既包含当下的v值又包含下一刻的v值， 这意味着， 我们我们可以通过把整个优化过程， 展开为一个个的这样的环节， 如果每一步都是最优的， 那么全局就是最优的（通过迭代法传导）。 或者反过来想， 如果我要达到了最优，那么从任状态出发， 我都应该是最优的。 这就包含了所谓动态规划的思想。 也就是你要优化全体， 就要优化每个部分。 对应的方法， 就是一种迭代的方法， 我先来计算每个状态的v值， 然后直接在算式里改变pi的值来最大化v函数（求导！）， 当改变了pi值，新策略的v值也必然要重新估算，有了新的v的值我们又可以使之最大的pi值， 由此形成一个螺线结构， 直到收敛到最优解，在这个点上， 我们就达到了一种稳定， 此时无论如何改变pi都不能使v达到最大。&lt;br /&gt;&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEF22FsuqjgCmr47OIe97BM6NAYh5vIOB86wqouR29LGdQoibt3Gvs0fg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;991&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;蒙特卡洛抽样 与TD方法&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;但是大多数条件下， 刚刚说的那个假定是不成立的， 也就是环境的动力学未知， 我们没有那两个有关环境的条件概率， 这个时候我们根本没法按照刚刚的思路求解。 对于未知的环境，我们的解析解行不通， 我们唯一能够借助的手段就是抽样， 通过抽样的方法， 取代掉算式里未知的条件概率。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们来看这个情况下对v的估值和对策略的迭代是怎么样的。 先来看v， 这个时候的v与其看出成对未来收益的期望， 不如看包含了以往所有历史的信息的对未来期望的估计， 因为你的抽样只能是估算，严格说你需要针对每个状态抽取无限个抽样样本才能完全准确， 但是显然没人等的了， 那么我们能否针对有限的被抽样样本来更新v函数呢， 如何更新呢？ 思路有两种， 一个是按照定义来搞， 既然v值是从某个状态出发的总收益的期望， 那么我们就可以从某个状态出来把整个游戏走完来测量这个收益， 从被抽样的状态出来， 走完整个游戏， 最后算出某总奖励作为v值得抽样。由于数据是一个个增加的， 我们用流平均来替代期望，这里也是一样得。 这样的思路我们称为蒙特卡洛， 蒙特卡洛几乎就是随机抽样的代名词。&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.28160919540229884&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEyazt9LZPmhq2gIl2SgDWZvtnfQvMYarESk5cVoWfSl6jdcoEk9wlbA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;174&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;alpha 越大， 表明我们之前的样本越多。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;然而这个方法有着一个致命的弊病 ，那就是你得等到游戏的结束才能更新一次v值，速度太慢了，而且想想有些时候你只能进行一次或几次游戏， 比如人生的游戏你只有一次， 你不能死了再回来， 所以这个方法就不那么给力了。 怎么办呢？ 我们还是有办法，这个办法就是不等到游戏结束就更新， 还是利用刚刚说的v函数的迭代展开式， 我们可以在每次得到一个奖励的时候更新这个值， 你想想无论是在游戏里还是人生里其实你每次都到一次胜利或者失败， 你都会对全局的输赢多一点信息，如果你能利用好每次到达的这一点信息来更新你的v值， 就达到了我们不结束游戏就更新的目标。 这个方法所做的就是每次多一个奖励出来时候严格按照v的迭代式定义来更新v函数，这样多步之后v也会趋于正确的值。 这就是大名鼎鼎的TD方法。好比当你在开车的时候， 你险些撞车， 游戏没有终止， 但是足以让你使用这个惊险来更新值函数。 另外一个例子是你打公交车去上班， 每过一个站你看一个时间， 看和你的预计是否有差别， 这每一站的时间， 足以让你不停的调整对最终实现时间的预期。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.13101604278074866&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEewr8GaLLz6umDw2J7DWhMxyZicwvwhBkSdIPK6SaibhNuEhiaiaQnQicpnw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;374&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;从另一个角度看它的本质， 这就是之前讲的动态规划的一个抽样版本，我们还是按照贝拉曼方程展开递归公式， 不过这个时候我们去掉了那些不好求的概率， 而是每次出来一个证据， 我就根据这个证据， 来调整我的v。 本质上， TD方法就是根据新增加得方法一点点得调整预期，最终使得我得估计准确得一个方法， 这点让我们想到贝叶斯方法。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;零一点关于TD的重要认识是， 它预设了一个状态中间的关联， 因为在每个TD的步骤里， 我们都假定了一个从st到st+1的马尔科夫结构，即t+1时候只和t相关， 所以TD方法在越接近马尔科夫决策结构的时候工作的效率就越高， 反之， 则引入一个偏差。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;如何优化策略&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们谈了通过抽样的方法更新值函数， 下一步就看看如何更新策略。 之前的动态规划思路是把全局分解为部分优化， 这里也一样， 每次更新v函数后，都意味着我们可以针对被更新的v值调整一次策略， 但是一个非常棘手的问题是哪几个环境的条件概率不知道，还有， 我们的v值是根据一两个新的数据估算的不准确， 这有两个直接的后果：&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;1， 我们无法简单的取最大值，&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;2， 即使我们能够取最大值， 我们也无法确定我们做的是正确的，因为我们不知道真实的v， 根据错误的局限的v做的策略极有可能是局部最优。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;对于解决1， 我们玩一个trick， 重新定义一个量&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEU3mtYq8tkZ5jetb8MLK8atniaBd1iazeczgKycxvvoLr1as6XACia2czQ/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;523&quot; data-cropy1=&quot;7&quot; data-cropy2=&quot;79&quot; data-ratio=&quot;0.13957934990439771&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEL0vIicBrZeTablLUomyln0K3YkwboLVNsyibrPj0rhPHYdenmQPgQ0kQ/640?wx_fmt=jpeg&quot; data-type=&quot;png&quot; data-w=&quot;523&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这个量把环境的那两个条件概率完美的包含进去， 如此，我们把刚刚说的TD和蒙特卡洛估计的对象就改成Q函数， 我们立马会看到刚刚得贝拉曼优化方程变成&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.13702623906705538&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEJDzc7iaOftqrgEhzTMgrKZwL8CX5hb7N6A4A7tA0epAfYc3sOGpqSNg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;343&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt; 这时候你用眼睛都可以看出这个东西的解吧， 我们的策略pi只要更新为取最大Q值得函数就可以了， 因为这正是这一关系所能取到得所有pi里最大的那个。 这样我们就解决了第一个问题在环境未知下求最大值&lt;br /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;Q 我们称为action value ， 是在控制问题里比v更好用的量， 因为它甩掉了那一堆概率关系， 直接了当的告诉你， 你要做好决策， 你就直接搞定那个action value最大的决策就行， 不需要一丝犹豫， 由于它直接忽略其它所有不是最大的选项， 因此给人一种十分贪婪的感觉， 因此它又叫贪婪算法。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;然后看第二我呢提， 如果没有第二个问题， 那么我们可以更新了策略之后， 进一步估算新的策略下我们的Q值（同v值更新方法一样），然后再根据贝拉曼方程告诉我们的， 不停重复这个过程，直到整个过程停下来， 我们就达到了最优解。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;同学们看看上面的逻辑是否天衣无缝呢 ? 如果没有第二点， 是对的， 但是由于那个刚刚说的那个问题，你对一个不准确的估计做贪婪运算， 极容易陷入局部极小， 我们机油可能仅仅是由于我们非常局限的经验而直接认定了一个选项，放弃了更有机会的选项， 比如你某次和某男生约会很高兴就马上放弃了其它所有男生。 由于视野很局限， 我们就容易落入局部最优解。 那么如何避免这个情况? 我们要加入这样一个因子， 它描述我的不确定性， 虽然我倾向于选择那个Q最大的动作来做， 我还是保持一个随机因子 epsilon。 这个值多大我自己定， 比如我如果选择epsilon 为0.5， 那么也就是有50%的概率我直接选最大的那个Q, 另外50% 随机选一个。 这个随机选项让我们有机会瞧瞧那些其它的选项， 也称为探索。 我们刚讲的探索与收益的矛盾， 正是这个直接选最大值， 与继续探索世界更新我的认知直接的矛盾。 如果我们拥有全部的世界的动力学知识，对世界了如执掌， 我们可以直接最大化， 由于那些是未知的， 而游戏世间是有限的， 就引出了这个探索与收益的矛盾， 也就是epsilon与greedy的矛盾， 如果epsilon设置的过小， 不足以解决局部最优的问题， 如果设计的过大， 则会使得策略的迭代过于缓慢。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;既然我们为了效率放弃了计算准确的Q， 我们也要把我们对世界估计的这种不准确以在决策中加入随机性的方式放回去。 这就是epsilon的本质， 它是伴随着对值函数的采样估计产生的。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这个被改进的算法被称为epsilon-greedy， 也就是结合了贪婪和探索的算法， 通常为了缓解探索和收益的矛盾， 我们会按照一定的进程表变化epsilon，开始比较多探索， 后期比较多贪婪， 这个也可想而知， 前期对世界的知识很少， 需要多探索来尝试所有的可能 ，后面反过来，游戏将要结束， 需要利用已经取得的知识最大化的得到成果，这样的一个过程我们通常称之为退火，就好像逐步降低温度一样。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;用这样的方法最终得到的最优解一定是一个确定性的解，当退火到0的时候， 我们得到的那个最优是不包含随机性的， 它的涵义就是搞定那个最大的action value 这类方法又称作值函数法， 就是说我最终决策的依据只有action value， 它是确定性的。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这里面有一个没有讲到的非常重要的细节， 刚刚讲到用TD方法来更新我的v值和Q值， 这个v和Q无疑都是针对我现阶段策略的， 一开始时候我说的策略是一个贪婪策略， 后面加入了epsilon， 我们说在智能体训练后的真实使用阶段我们是不含有这个epsilon的，那么这个评估时候针对的策略是要根据我真实使用中的那个贪婪策略， 还是根据我在训练中为了避免局部最优使用的这个epsilon-greedy呢？ 答案是这两个都可以. 如果死磕定义， 我当然应该使用epsilon-greedy，因为这才是我在训练中真实采用的策略。 但是你不要忘了， 我真正care的永远是那真正使用的策略， 也就是贪婪的策略， 这个epsilon只是在训练中为了纠错加入的。聪明的同学一定想到了我在评估的那个真正用的策略和我所用的含有探索的训练策略没必要一样， 这个思路就产生了off-line learning 的思想， 翻译出来叫做离线训练， 它说的就是我在训练中使用的策略和我真正学的那个策略可以不一样， 好比在评估阶段我处于离线阶段， 是通过一个与真实训练有区别的进程完成的。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;相反的， 如果两者是一致的， 我们称为online-learning。 这样我们的epsilon-greedy算法就衍生出了两个流派， 一个是online-leanring版本的sarsa（state-action-reward-....）， 另一个是offline版本的Q-learning。&lt;/p&gt;

&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.42916666666666664&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkE4M8o1agFAq4wc1h1YnJqIskF2sK9MiacrNpyIMbpKDsdD4YwBC17L5g/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;930&quot; /&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.4097222222222222&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkE9JBCibHR6124nTY4H1uy4dYib8ic4yf9lVl3W0s7PVYfm3DLM74akmWlQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1068&quot; /&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;BoostStrap 方法&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;刚刚讲到的TD更新方法， 是最常用的方法， 但基本款的TD方法还是略显缓慢， 有一种迅速加速这个方法的手段， 叫做TD-lambda。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们且一步步的展开这个问题： 1， TD方法每次得到一个环境给的奖励信号， 然后更新上一状态的Q值 2， 那么我们可不可以等两部，　得到两个奖励信号再更新一次， 这样是不是在更新的时候做到更准确． 3， 可不可以等到n步奖励的结果再更新，这样对每次更新更确定？ 4， 如果一直到游戏结束才更新我们是不是得到了蒙特卡洛方法？ 如此的确很减少了偏差，　但是同时降低了效率　5， 因此n步更新方法是1步TD方法和蒙特卡洛方法的中间状态&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;6， n该取多少，似乎比较小的ｎ和比较大的ｎ都各自有各自的道理 7 ，不如同时一起做了，　从一步更新到ｎ步更新， 此处引入TD lambda， 我们可以从1步到n步同时更新，然后用一个方法把它们混合起来　8， 能不能倒过来看， 在每一步更新的时候， 我们不仅更新这一步的Q值， 而是把状态轨迹上的所有之前的Q值都更新？ 这个方法可以证明就是第7步的TD lambda， 它的效率非常高， 因为它可以通过最后一步得到的奖励， 更新所有的中间状态Q值。 你可以看出来， 这个TD lambda， 更新Q值的效率还是很高的。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;把学习的思想引入Q-learning&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;有了上述这样的方法， 对于任意的问题，我们都可以通过先设定状态- 行为 -奖励， 然后给每个状态，行为的组合设定一个Q （a，s）， 再不停的通过TD方法和蒙特卡洛来优化控制流， 然而， 这样的方法确实有着天然的弊端， 一个最典型的问题是， 它本质是在做一张状态-动作组合的表格， 对应每个状态-动作的组合， 我要计算一个收益期望Q，如果状态和动作的组合是可数的， 那么这个问题是可以操作的， 因为这个时候我们要做的无非是一个个的迭代。 然而如果状态非常多， 这个问题就不再那么好做了 ，因为你总会在某个时刻， 到达一个先前从未见过的全新的状态， 这时候该怎么做呢？ 终于又到我们回忆监督学习的时候了， 我们说了监督学习的终极目标是通过训练数据最终在没见过的数据集上做预测， 我们通常称之为泛华能力。 你看这不正是我们需要的东西吗？ 我们说我们有无穷多的状态动作组合 ，当遇到一组新的组合， 我们不需要慌张， 而是由模型进行预测。 我们的模型是由那些已经由了数据的组合训练出来了， 通过模型所把握的规律， 来在未知的数据上做预测。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这一个思路进来，那么所有监督学习里的模型就都称为了强化学习里的模型， 从线性回归到神经网络， 从树模型到随机森林。 如果所使用的模型是深度学习模型， 我们就不由自主的开启了深度强化学习。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们先前的状态就如同监督学习里的特征， 而Q（s，a）就是我们学习预测的对象。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;那么随机梯度下降这类家常便饭的优化方法也很自然的引入进来。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;重新回顾一下， 这一段落得根基其实是泛化二字， 也就是说我们的模型其实能够捕捉某种状态之间的关联。 比如你客户以想象在导航走迷宫的任务里， 从相似的起点出发， 你追随目标的路径其实是相似的， 当然这是一种非常表象的泛华能力， 还有一些更深层的联系， 需要抽象出一些特征才能干掉， 这就是我们深度学习要登场的理由了。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;引入策略梯度算法&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;有了策略梯度算法是不是就足够了呢？ 这个Q学习的算法， 虽然体系滴水不漏，但是依然有一个比较严重的弊端， 那就是最终学到的策略只能确定性的， 根据定义 ，这个最优化的策略是每一次选取Q（s,a)里最大的那一个。 可是很多时候， 我们的最优化策略本身就需要包含随机性， 而且这个随机性的大小需要精确的描述， 而不是像epsilon-greedy那样通过随机抽样粗糙的给出。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们用要一个最微小的例子来说明，还是那个走方格的问题，骷髅就是有危险的意思，我们希望走到有奖励的地方。 我只做一个小的改动将使得之前问题面目全非，之前的马尔科夫决策附加的条件就是当下的状态含有用来决策的所有信息，方格问题里， 这个信息就是位置坐标。而如果我没有位置这个信息， 取之以感知信息， 比如我只能感知我所在方格的周围两个方格有什么（下图中的骷髅或金币）。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;注意如果我们处在下图灰色方格的区域（左右各一个），此时相邻的两个方格的情况是完全一致的（白色），也就是说我无法确定我是处于左边还是右边的灰色方格， 这导致无法决策正确的行为（左边和右边的正确决策是相反的！ 一个向左一个向右， 但是我无法确定是哪一个！）。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;如果此时引入一个随机性的策略， 这个问题影刃而解，我无非子啊左右两个灰色的格子里制定左右各50%的策略， 这时候总是最终客户以达到宝藏，就是时间可能稍微长一点。 这样的随机性的策略， 引入策略函数就可以可以很容易的学出来。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.2875&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEuNMe0pFV5WrRuKGzIZZqPc5jMOXDdL9xLU1icAlfT5YPytJRUPGeeibg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1208&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;另一个必须使用策略函数的理由还是当agent可以执行的动作很多的时候， 比如机械臂的运动， 它的控制事实上是在一系列连续变化的角度里选择， 这几乎就是无穷多的动作。 这个时候， 那个比较古老的epsilon-greedy就要退出了， 我们可以模仿Q学习里的思路，把整个策略概率函数， 用一个神经网络精确的表达出来， 这样即使可能的动作很多， 你也可以通过设逆境网络的泛化能力达到（也有其它模型， 最常见的就是神经网络）&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这个策略概率函数，能够精确的表达即使是无穷个动作， 每个动作被执行的概率。 这样通过模型的泛华能力， 我们很容易对一个未知的状态动作组合被执行的概率做出预测。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;有了策略概率函数， 我们如何优化这个函数呢？ 注意，之前的值函数方法，事实上我们也有策略优化这一步， 只不过那里直接通过更新v函数， 然后取最大Q绕过了这个步骤。我现在要硬碰硬的干掉它怎么搞呢？ 当然是把公式全部展开然后求导 ， 然后梯度下降。 我们拿一个最简单的问题开刀， 就是一个一步的游戏（摇臂赌博机），而且奖励的值是确定的， 这个游戏的奖励的总值是&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.358974358974359&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEdibHjUKfV7PgvicUdAmYicNvjN0eOAibR9RDSbGmoT38GH4QlNk900vjmA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;195&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;，然后我们对E进行求导， &lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.25311203319502074&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEH7UAH69cbrHpZMM7f1xo0sEm7BzaNDiaQHBjKTiaVXwF08WoMtia5cvicQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;241&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;由于此时我的pi已经是含有w的函数， 因此求导变成这个德行， 对这个公式里的 &lt;img class=&quot;&quot; data-ratio=&quot;0.6833333333333333&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEHmdqRRb4hY5J63lhOYeTZ4l2I9EF3SQDjeet4MJL1TaU6InHqCSMNA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;60&quot; helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; /&gt; 进行求导是有技巧的， 我们对于类似问题最常用的问题就是把它变成&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEicEpwBHrTlYU0dgGWrich1WpLK7RLw9iawoyw54XjHRb5IuiaOcHS5eDEQ/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;513&quot; data-cropy1=&quot;2&quot; data-cropy2=&quot;39&quot; data-ratio=&quot;0.07212475633528265&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkELacddL5WUgSgXbHXT5JcMNsPf8ibmExD7cyUEC3PQ0rhFNtgaLeNwZQ/640?wx_fmt=jpeg&quot; data-type=&quot;png&quot; data-w=&quot;513&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt; , 为什么要做这样一个变化呢? 原因在于， 这样可以保持我的期望的形式不变， 因为我会多出一个pi来， 这时候， 我事实上得到的是 &lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkExfsg9zibc9o8DwiaO7EzoVEPth0picDoZJfKfIBSe97bG1XDDj7YxparA/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;317&quot; data-cropy1=&quot;4&quot; data-cropy2=&quot;36&quot; data-ratio=&quot;0.10094637223974763&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkE9h6PjpzpGgVbgicjUJxQdtImruuxwSPVU5def3yKpunBduQjlne1dmQ/640?wx_fmt=jpeg&quot; data-type=&quot;png&quot; data-w=&quot;317&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  ， 这里面的&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEnYUlmLZ16LcriaOHcqmCdK4ja47RXNMiaDyqyr6g2OdnWkDwqElmdZ1g/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;97&quot; data-cropy1=&quot;7&quot; data-cropy2=&quot;41&quot; data-ratio=&quot;0.35051546391752575&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkER0b2ojUibNAFAdV6ogqmAzOxVKCA8rFejYHnr3Pa3Bn8icou0t34c5Pg/640?wx_fmt=jpeg&quot; data-type=&quot;png&quot; data-w=&quot;97&quot; helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; /&gt; （称之为score function，类比机器学习最大似然函数的梯度）是一个相对容易计算的量， 原因它是在不改变原先条件概率函数下的期望形式！ 我们只需要按照之前对Q函数取样的思维， 直接在游戏的进程中对它进行抽样即可， 然后进行随机梯度上升。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;刚刚讲述了摇臂赌博机这类只有一步的问题。 如果是多步游戏呢？ 这时候我们要改变我们的目标函数， 用q替代刚刚的r, 由于策略梯度完美保持所有其它的概率表达式， 因此我们得到的表达式依然保持期望形式不变 &lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.09705882352941177&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEh3QMCKBRxstenLAhNic1FD6TUcPGr2xyxXNn6dpiaKjMr7Ef8PbUtOhw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;340&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;。&lt;br /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;有了上面的基础， 你知不知道如何取干掉这个Q呢？ 有几种方法？ 蒙特卡洛和TD方法，以及TD方法的各种booststrap形式。 采取蒙特卡洛方法在游戏结束时候计算策略梯度的方法通常称作reinforce， 是最基本的方法。 如果采用td方法呢？ 这时候我们就来到了当下的今日之星 - actor-critic。&lt;br /&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;Actor - Critic 算法&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;Actor - Critic 方法在整个强化学习优化算法的位置里极为特殊的位置， 因为它综合了上述两个方法的优势， 我们在游戏里同时把Q和策略pi用神经网络表达， 在得到新的证据的时候两个网络都会反传一个梯度，策略梯度用来寻找更多的奖励，用于策略网络直接和行为相关， 因此这个神经网络被称为actor 而Q的梯度用来更加准确的对期望收益估值， 由于这种评估的本质， 这个Q网络又被称为批评者critic 。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们可以通过一段代码理解这个框架．　&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pytorch/exam&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;ples/blob/master/reinforcement_learning/actor_critic.py&lt;/span&gt;&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.37777777777777777&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEHwW2qE1322CTJhG27xYaRTVludW89OdkSrQFDJ8ZuibjnZ37StGfX0Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1341&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;有模型学习&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;刚刚讲的方法，通常称为免模型学习，所谓免模型，　就是当环境的动力学（也就是那两个条件概率，想象下棋的例子）不知道的时候，　我们通过直接抽样的方法来更新Ｑ（ａ，ｓ）和ｐｉ来进行控制住．我们知道当环境的动力学规律完全已知，　我们根本不需要抽样，　而是可以效率非常高的使用动态规划求解．在此处，我们可可不可以学习创造一个世界模型，　来提高我们抽样学习的效率呢？　当然可以，　你不是由监督学习吗？ 我们可以用类似监督学习的思路来把这个环境的动力学学出来啊！&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们在这里就进入了有模型学习的范畴．　一旦ａｇｅｎｔ开始学习掌握世界模型，并用它影响决策，　这个时候我们就是说ａｇｅｎｔ获得了一个全新的能力planning, 　这个能力的获取使得agent 使用环境奖励的效率大大提升．　&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;何谓planning, agent 试图在自己的心理展开世界模型，估计当它做出如何如何行为，会得到如何的奖励，　虽然agent 并没有真正经历那些行为，　就好像经历过了一样，　这样agent 就如同获得了非常多的虚拟数据，　可以更准确的对未知的状态进行估值，　在数据极为稀缺高维诅咒极为明显的强化学习问题里，　这个效果是巨大的．&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;那么这个有模型学习的范式就变成了我们先利用真实的数据来学习环境的动力学矩阵 P（st+1|st, a）和P（r|st，a），然后用这两个矩阵来进行simulation， 得到很多的模拟数据计算Q（s，a）的过程。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.7444444444444445&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEI3hzBVsNGLo5bKKhJJQCwQGQzlYq8z4X98EQnCcZ2PA5O8g0QZC0yA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;850&quot; /&gt;&lt;br /&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-ratio=&quot;0.10966542750929369&quot; data-w=&quot;538&quot; class=&quot;&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; /&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span data-offset-key=&quot;8h5th-0-0&quot;&gt;广告时间：&lt;/span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383610&amp;amp;idx=1&amp;amp;sn=eae53f91ea3bdb1d99464d3824175707&amp;amp;chksm=84f3c97bb384406dd3942d73be8d1dbe5a16815743990686d9054e1a3e9fa8fc42c8519ba270&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;铁哥开设的一个为期两日（12小时）的强化学习特训班&lt;/span&gt;&lt;/a&gt;&lt;span&gt;。&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;为什么是强化学习？  请看下图的技术泡沫爆裂图。  机器学习和深度学习在2017处于关注热度的顶峰， 大家看处在上升期的人工智能技术， 第一当属深度强化学习， 据这张非常表格非常粗糙的估计， 深度强化学习的技术成熟期在未来5-10年， 此时此刻， 正类似于深度学习在2010的状况。  &lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.67&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfqJgX6L51kcnJ07DHpdBzqmXKic2iaDgZOpUiacyYFWzBHst0BCZVTJINPzpfRibpMV6hWgUKSlj7ibBw/640?wx_fmt=jpeg&quot; /&gt;&lt;/p&gt;
&lt;br /&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-ratio=&quot;0.10966542750929369&quot; data-w=&quot;538&quot; class=&quot;&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; /&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJNQDiajMVL3arkx9IJnm10baZ1RjdLTN2KH6SKHZqnzyGO5K0G3dNOwg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;5.896&quot; data-w=&quot;750&quot; /&gt;&lt;br /&gt;</description>
<pubDate>Sat, 22 Sep 2018 17:40:14 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/w9o1udg7Ui</dc:identifier>
</item>
<item>
<title>[原创]读《无穷的开始》重新了解进步与启蒙</title>
<link>http://www.jintiankansha.me/t/LJJpiME5RW</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/LJJpiME5RW</guid>
<description>&lt;p&gt;今天说的这本书，是11年出版的《无穷的开始-世界进步的本源》，这本书长期位于美国亚马逊科普类图书畅销榜，在14年出了中文版，但却一直不是一本大热的书。书的作者是量子物理学戴维多伊奇，他是霍金的师弟，但不同的是，霍金的书由于相对的阅读门槛低，所以有更高的知名度，而多伊奇的书读起来则十分烧脑。在这本书中他试图下定义、作解释的不仅是科学领域，而是整个人类文明，甚至包括未来尚未出现的文明。本书有相当大的部分都颠覆了我们的观念和知识。不要急着反驳，先思考一下。在保持开放的创造性的同时，仍保持批评精神，这正是作者教给我们的。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.5202702702702702&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccq0zempmlp8VotKImghN7aicYvzLic0Ddb3H1Z6qKwFM17ibPiaBaHia4WicJoYiblmLB0Z7cfItKeP1ThA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;592&quot; /&gt;&lt;/p&gt;
&lt;p&gt;读《无穷的开始》，需要你在很多学科都有所涉猎，需要你反复的去检查作者段落之间的逻辑关联，而不只是记住书中的结论。这是一本值得反复读的书，也是一本在不同年龄读来会有不同感悟的书。而立之年读，会让你对未来充满了乐观，不惑之年读这本书，会让你审视自己做错了什么，而知天命之人读这本书，则会让你去思考历史大势的偶然与浩浩汤汤。&lt;/p&gt;

&lt;p&gt;这本书有四个关键词，乐观，解释，通用性，动态。&lt;strong&gt;未来的乐观是和好解释带来的知识互为因果，通用性带来微观层面上的无穷，动态带来组织层面上的无穷&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;探索关于事情为何发生的回答，就是在寻找关于世界的解释。小孩子总问缠着父母问，为什么会有太阳的东升西落，为什么会有四季的更替，为什么会有花开花落。面对这样的问题，父母若是选择了简单的回答，那就会扼杀孩子的好奇心，因为孩子问的多了，就知道自己的下一个问题会得到什么样的答案，或是你上学了就知道了，或是一个自己也能编造的童话故事。&lt;/p&gt;

&lt;p&gt;而若是父母选择了困难的回答，那要做的就不止是简单的几句话了，要回答四季的生成，就先要让孩子想象一个我们生活在一个巨大的大球上，然后再找一个发热比较大的老式灯泡或者浴室的浴霸，将一个倾斜的苹果在灯下照很长的时间，再让孩子感受不同部分的温度，最后还要让孩子去设想我们生活的大球有一个倾斜，所以有的地方热，有的地方冷。&lt;/p&gt;

&lt;p&gt;但这正反映了好的解释的俩点本质属性，&lt;strong&gt;一是&lt;/strong&gt;&lt;strong&gt;这个解释不容易被验证，二是解释中的每个细节都必不可少&lt;/strong&gt;。相比于好解释，坏解释也有俩个特点，比如星座神话就不是个好解释：它能轻易改变（根据神话使用者的需要），它的适用范围也非常狭窄（北半球的星座神话不适宜南半球）。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;《十万个为什么》这本书，曾经是一套国民级的科普书，但鲜为人知的是，这本书最初是作为核战危机下的战略储备物资而编写的。编者想象的场景是在遭受了核弹的打击后，该如何重建我们的文明。但为什么不直接告诉劫后余生的人该做什么，而是要告诉他们一些孩子们会问的傻傻问题的答案了？&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;然而细想就会明白，未来会出现的具体问题是什么，你无法提前预知，即使现在的人们列出重建文明的种种操作指南，也无法让未来的人们能够按图索骥。但若是教会他们为什么事情会发生，这些解释就可以指导他们自己想出解决方案，&lt;strong&gt;世界上任何一个灭亡了的文明面对的问题，都是缺少知识造成的&lt;/strong&gt;，穿越小说的主角带不走现代社会的技术实体，但只凭着现代社会教给Ta的知识和理念，就能够改变历史。但这知识具体值得是什么了？让我们看看《无穷的开始》这本书对这个问题有怎样的回答？&lt;/p&gt;

&lt;p&gt;古人有的是实验和观察，然而实验和观测的功能是在现有理论中作出选择，并不是作为新理论的来源。我们通过解释性理论对经验进行解释，但真正的解释并不是显而易见的。然而人都是好逸恶劳的，直到启蒙运动提出的易谬主义，将凡人皆会犯错这一信条变成了共识，从而建立了一种批评的传统。书中写道“由此产生的一些观念有着极大的延伸范围：它们能够解释的东西，比它们被创建出来用于解释的东西更多。&lt;strong&gt;解释的延伸是一种内在属性&lt;/strong&gt;，不是经验主义和归纳主义所说的那种由我们提出的假设。”&lt;/p&gt;

&lt;p&gt;理解什么好解释，能够让我们认清楚经验和观察在科学中的作用，波普尔认为：&lt;strong&gt;如果没有预先存在的知识，就不可能进行科学观察，这些知识涉及要去看什么、寻找什么、怎样去看、怎样解释看到的东西。&lt;/strong&gt;理论必须先行。理论要靠猜想得来，而不是推演得来。我们对任何东西都不是直接观察的。所有的观察都是对世界的感知都经过了意识和潜意识的加工，所以一件事情，如果你只盯着它看，结果除了它本身之外你什么也看不到&lt;span&gt;。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;同样地，每当我们犯错，它都是对某种事物的解释里的错误。这就是为什么表象会具有欺骗性。经验对于科学研究的确是必不可少的，但它的作用却同经验主义者所说的大相径庭。它不是推演出理论的源泉，其&lt;strong&gt;主要作用是用于挑选已经提出的猜想&lt;/strong&gt;，这就是“从经验中学习”的意义所在。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;能够创造和运用解释性知识，使人获得了一种改变自然的能力，这种能力不像其他所有的适应性那样从根本上受狭隘因素限制，而仅受普遍规律限制。正是这点构成了作者为什么对人类未来的前景保持乐观，这种乐观不同于《奇点降临》中对人的存在本身的悲观。作者认为：&lt;strong&gt;人是宇宙中最重要的实体。人并非由环境“支持”，而是通过创造知识来自我支持。&lt;/strong&gt;一旦人有了合适的知识（实质上就是启蒙运动的知识），就可以触发无限的进步。&lt;/p&gt;

&lt;p&gt;解释世界和控制世界之间的关系看上去越来越密切，这绝不是偶然的，而是世界深层结构的一部分。在给定的时间、给定的资源或其他条件下，物质的转变要么是不可能发生，因为被自然法则所禁止；或者在有正确知识的情况下，就一定会发生。&lt;strong&gt;认为世界不可解释的主张，等同于诉诸超自然。&lt;/strong&gt;而所有可能的物理现象的解释，都与怎样运用知识来实现这些现象有关。&lt;/p&gt;

&lt;p&gt;作者的乐观还体现在书中下面的句子中：不管是特定的人类生存条件还是通常的解释性知识，都不会达到完美，甚至不能接近完美。我们将永远处在无穷的开始。人类总是行动走在解释的前面，在《走出自己的天空》这本书中，来自农村的何江在哈佛毕业演讲中讲到，一只毒蜘蛛咬杀了他的右手，母亲用火疗的方式对他进行了治疗。那时他母亲不知道这背后的解释，高温使蜘蛛毒液中的蛋白质变形失活。了解了这个解释，可以创造出痛苦更少的治疗方法。如今的医学还有很多类似火疗这样的有效却痛苦的治疗，比如化疗放疗等，如果人类彻底了解了癌症的机理，那就能以最不痛苦的方式去治疗癌症。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;人类永远有值得去探索的新边疆，这是终极层面的乐观。&lt;/strong&gt;宇宙不是用来把我们比下去的，它是我们的家，我们的资源。无穷的无知是知识存在无穷潜力的必要条件。拒绝接受我们“即将到达终点”的观点，是避免教条主义、停滞和专制的必要条件。科学理论预测未来的能力，取决于其解释的延伸，但没有什么解释的延伸范围达到能预测它自身的后继者将包含什么内容。我们自身的未来也将由我们尚未拥有的知识来塑造。不管是特定的人类生存条件还是通常的解释性知识，都不会达到完美，甚至不能接近完美。我们将永远处在无穷的开始。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;能应对未来无穷问题的只有不断进化的知识库，书中区分了用进废退的拉马克式的进化和先随机在选择的达尔文式的进化，用作者的话来说，拉马克主义的根本错误与归纳法具有相同的逻辑。两者都假定，新知识（在两种情形下分别是适应性知识和科学理论）已经存在于经验中，或可以机械地从经验中得出，新的适应性可以通过实际上只是现有知识展示的变化来解释。但真相永远是，&lt;strong&gt;知识必须先假设再检验。&lt;/strong&gt;这正如达尔文理论所说的：首先，随机突变发生（它们并不考虑要解决什么问题），然后自然选择把那些不太擅长重现在子孙后代身上的基因变种剔除掉，从而偏爱在群体中散布得最广的基因。&lt;/p&gt;

&lt;p&gt;这俩种进化的学说，不止是单纯的学术观点之争，更会影响我们的处事方式。为什么强化学习只在游戏中取得了超越人类水平的成功，这是因为除了游戏，其他的领域算法的进化都是拉马克模式的，算法从人类的行为中获得关于未来预期的知识，而唯有在游戏中，算法可以通过随机的试错（自我对弈），产生全新的策略（以至于比人类更有创造力），从而达到达尔文式的进化。&lt;/p&gt;

&lt;p&gt;而在日常的处事中，也要进化自己的知识库，但选择那种进化模式，就会影响到我们的长久的幸福。比如在婚姻中，拉马克式的进化类似于对爱人百依百顺，对Ta的痛点爽点痒点G点一清二楚。你以为这样能让爱情永恒但这样的进化方式，会让你自贬身价，成为一个低质量的性爱机器人，科幻电影中的那些会自我学习的机器人，其实都是拉马克式的进化，因为其背后隐藏着神创论或上天注定的假设。而达尔文式的进化观指导的爱情应该是把进步放到首位的，俩人都先想着如何提升自己的姿势水平，相互之间的你追我赶带来了数不清的意外，而少数有价值的意料之外成为了值得珍惜的回忆。&lt;/p&gt;

&lt;p&gt;认为人工智能可以通过积累聊天机器人的技巧来实现，这种观点对应拉马克主义。由于我们还不了解创造行如何运作。近似的，人工进化也可能没有实现，尽管看上去好像实现了。问题在于，我们不了解DNA复制系统的通用性的性质。图灵发明了图灵测试，希望绕开这个哲学问题。换句话说，他希望在解释这项功能之前就实现这项功能。不幸的是，类似这样的情形极为罕见。而这也是为什么说强化学习自身是无法带来强人工智能的，不是算法逻辑的缺失，而是由于在对人类影响深远的领域，人类不会允许一个未经验证的验证的算法去试错，只要这一条限制不改变，强化学习的算法就永远不是无穷的开始，不会成为人类的主宰。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;好解释的通用性第一体现在达尔文式的进化上，其次体现在其抽象性上。我们所知的好解释，大多会涉及一些我们看不见摸不着的东西，比如原子在电子显微镜发现前是无法直接的观测的，那该如何解释花粉的随机游走了，使用了原子的假设，和使用了童话中花仙子的解释有什么区别了？为什么使用原子的解释，不管原子有没有直接被观测到，都是比花仙子的解释要好了？其中主要的原因是使用原子的解释是从涌现的角度，以较低层次的实体，来解释较高层次的问题。&lt;/p&gt;

&lt;p&gt;书中写道：所有的高层次现象都是准自备的，接近于完备。在更高的、准自备的层次上变得可解释，称为突现。&lt;strong&gt;而对同一现象有着不同层次上的多个解释，其中并无矛盾。&lt;/strong&gt;认为微观物理学的解释比突现的解释更加基本，是武断而且错误的。关于抽象概念的知识，与我们所有的知识一样，它来自猜想、批评和对好解释的追求。这一点就区分出了基于原子的解释和基于花仙子的解释，花仙子的解释会假设其是最基本的，是不可批判的。而基于原子的解释则会说原子可能是一个实心小球，可以有内部结构，但其细节不影响其涌现带来的特征。&lt;/p&gt;

&lt;p&gt;在现实中，各种解释不会组成以最低层次为最基本解释的等级体系。相反，任何层次的突现解释都可能是基本的。抽象的实体是真实的，并且在产生物理现象的过程中发挥作用。因果关系本身就是这样一个抽象概念。&lt;strong&gt;如果我们在相关领域里的最好解释涉及某个实体，我们就必须认为该实体确实存在。&lt;/strong&gt;而且，就像引力的力量那样，如果我们的最好解释否认它存在，那就不应该再认为它存在。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;而好解释通用性的第三个体现，在于其具有1+1&amp;gt;2的特性，书中写道：所有知识增长都是通过渐进的改进实现的，但在许多领域里有一个节点，&lt;strong&gt;知识或技术系统一项渐进的改进在此时会导致延伸范围突然扩大，使系统成为相关领域的一个通用系统。&lt;/strong&gt;这方面的例子，可以去看看那些睡美人发明，很多很棒的主义很早就被人发现了，但必须等到配套的基础设施都具备了，才能具有很大的适用范围。具有向通用性转化的潜力，也是好解释的另一个特征。&lt;/p&gt;

&lt;p&gt;在这本书的第八章，作者用康托集合论无穷旅店的例子来解释数学和物理学的关系，这可以看成是对通用性的一种限制。人们都说数学是最通用的，能够跨越学科，解释不同领域的现象。但作者指出所有的知识都由物理过程产生，其适用范围和局限性受自然规律制约。我们的数学知识的可靠性，永远从属于我们关于物理现实的知识的可靠性。一个数学命题是否可以证明、是否可判定，都取决于物理规律，这决定了哪些抽象实体和关系可用物理对象来模拟。&lt;strong&gt;数学的目的是去理解，它的总体方法是提出假设、根据它们身为好解释的程度来进行批评。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;好解释的通用性还体现在其是美的，美不是个人的主观判断，而是客观事实。花在人眼中总是美丽的，而它们的设计是为一个显然与人类无关的目的进化而来。花儿之所以美丽，是如同好解释一样，要和伪造者做持久斗争。其结果是一个每一部分都比不可少但却有序可循的结构，从而在预先的共享知识不足以提供难以伪造的信号时，解决了创造此类信号的问题。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;在书的十三章，作者展现了他真正想在这本书中达到的目标，即&lt;strong&gt;在广大读者心中重塑启蒙运动中对进步的渴望。&lt;/strong&gt;这章的标题是《一位物理学家的怀哲学史以及对坏哲学的若干意见》，作者先是反对了量子力学对其学科基础的哲学解释的漠视，认为不能仅仅认为一个理论能够预测未来，就忽略其背后的对世界的解释是什么的问题，接着作者指出了什么是坏哲学，坏哲学指不仅本身错误还主动阻止其他知识增长的哲学。经验主义的后裔、实证主义、逻辑实证主义、工具主义、维特根斯坦、语言哲学、“后现代主义”都属于坏哲学。&lt;/p&gt;

&lt;p&gt;例如19世纪发展起来的实证主义学说试图把一切不是“从观察中推演而来”的东西从科学理论中剔除。而后演化出的逻辑实证主义则认为不能通过观察验证的陈述不仅毫无价值，而且毫无意义。这一学说不仅威胁要扫除解释性的科学知识，而且要扫除整个哲学领域。特别是逻辑实证主义本身就是一种哲学理论，它也不能被观察验证，因此它声称自己（以及所有其他哲学）是无意义的。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;而后现代主义是一种拒绝理性批评或改善的叙事，正是因为它拒绝所有的批评、认为它们只是叙事。创建一个成功的后现代主义理论，实际上纯粹就是一个如何满足后现代主义者群体标准的问题，其标准已经变得非常复杂、排他、以权威为基础。这样的东西绝不适用于理性思考：创造一个好解释很困难，其原因并不在于谁决定了什么，而在于存在一种客观现实，它不符合任何人的预先期望，包括权威的期望。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;足够多的人如果相信了知识的井已经干涸了，那人类知识的进步就真的会停摆，低垂的果实会被摘光，经济会陷入滞涨，而如果能摆脱坏哲学的影响，那进步的脚步就是不可阻挡的。这也是这本书的现实意义所在。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  最后来看看这本书的第四个关键词，动态。动态的思维意味着不拘泥与已有的选项，把选择和决策过程想象成按照固定公式在现有选项中进行选择，是一种错误的做法。这忽略了决策的最重要元素，即创造新的选项，以及去掉错误的选项。评价动态的决策，要能区分决策的能力与决策带来的效果，&lt;strong&gt;良好的决策力不一定能带来最大的收益，但却可以最高效的去除坏选项，同时创造出全新的选择。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;动态的视角意味着，执行决策的方式，应该和执行决策的效率分开执行。一种执行方式是将决策的执行看成是理性迷因的传播，其依赖于接收者的批评能力实现自身的复制。另一种执行方式依赖于使接收人失去批评能力实现自身的复制。即便是高层具有了动态的决策力，但如果决策的执行依赖于后一种的愚民模式，那长久下来，就会使一个动态社会转化为一个静态社会。&lt;/p&gt;

&lt;p&gt;静态社会和动态社会的区别，在于变化发生的速度，静态社会发生变化的时间尺度很大，超过了其成员能注意到的范围。静态社会倾向于用暴力来解决问题, 确实倾向于为社会的'利益'而牺牲个人福祉。但一旦灾变发生，静态社会就会因为无法适应而消亡。&lt;strong&gt;静态社会最怕失败，是因为它们无法迅速创造知识的特性必定会把一些问题变成灾难&lt;/strong&gt;，而动态社会则具有反脆弱的特征，能从失败中迅速改变。我们生活的时代，是历史上存活时间最长，影响范围最广的动态社会。动态社会的发生不依赖于初生地的禀赋，因此戴蒙德在《枪炮病菌与钢铁》中对不同社会的不同历史所作的“终极解释”是错的：到历史开始被记录下来的时候，它早已成为思想观念的历史，远胜于作为其他任何事物的历史。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;但吊诡的是，驱动动态社会运转的人的创造力，却不会确保能带来动态社会。在本书的第16章，作者注意到，在智人进化上完成后的数千年，创造力的使用并不明显，那是什么导致了高创造力的个体被自然选择所偏爱了。答案是为了把他人脑中的迷因保持原样复制下去而进化出来的。&lt;strong&gt;迷因传播的核心是理解, 而不是重复。&lt;/strong&gt;学习归纳是一个创造力的过程，会用了才算学会了迷因。&lt;/p&gt;

&lt;p&gt;比如学生可能通过一堂课掌握了一个 迷因, 但无法重复教师的每一句话。这背后的原因是, 首先是由于迷因具有模糊性, 需要&quot;隐性知识&quot;来理解, 这包括对社会体制, 人性, 对错, 时空, 意图, 因果, 自由, 必要性的假设。我们并不是从书本和条文上得到这些隐性知识, 而是通过事先拥有相关的“隐性理论”来处理。&lt;strong&gt;人类不是单纯模仿, 而是试图，解释， 创造解释根植于我们的基因之中。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在原始的部落中，最聪明的头脑面临着和现代的科学家类似的问题，两者都必须发现一个隐藏解释。对前者而言，该解释是其他人头脑中的一个思想观念；对后者而言，是一种规律或一条自然法则。两者谁都不能直接触及这个解释，但都能获取可用来检验解释的证据，对前者是谜米持有者被观察到的行为，对后者是与法则一致的物理现象。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;既然创造力本身不会带来动态社会，那什么是这第一驱动力了？答案是启蒙运动带来的乐观和进步的观念。书中写道：启蒙主义告诉大众，自然规律不可能对进步施加任何约束，否认这一点相当于诉诸超自然。换句话说，&lt;strong&gt;进步是可维持的、无限期的。&lt;/strong&gt;但是，这要通过采取特定思维方式和行为的人来实现，而你可以选择成为自己想要成为的样子，不必受制于传统和权威。&lt;/p&gt;

&lt;p&gt;这本书中对民主制度的多数选举这一和启蒙运动同时发生的现象也有精彩的论述。多数投票制度的逻辑是，政治家和政党如果不能说服相当一部分人口投票支持他们，就没有机会获得权力。这刺激所有的政党都去寻找更好的解释，或者至少是去说服更多的人接受他们现有的解释，因为一旦失败，他们在下次大选中就会被扔到一边，毫无权力。&lt;/p&gt;

&lt;p&gt;在启蒙主义之前，人们对历史的解释是机械性再阐释，通常不仅缺少解释能力，在道德上也是错误的，因为它们实质上否认了参与者的人类属性，只把他们和他们的思想当作自然环境的副作用。启蒙主义强调，&lt;strong&gt;只有知识能把自然环境转变成资源，只有人类能创造解释性知识，从而创造出称为“历史”的人类独有行为。&lt;/strong&gt;自文明开始以来以及更早的时候，进步的主要机遇和主要障碍都仅仅由思想观念组成，它们是整个历史的决定性因素。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;总结一下这本书，书中虽然涉及了诸多学科，却形散神不散，整本书围绕人类是如何开启通向无穷，如何维持动态的乐观社会这个话题展开论述。全书时刻都在彰显人的价值，&lt;strong&gt;人作为通用解释者的意义，不需要虚构出一个“他者”或“彼岸世界”来定义，世界最终是否有意义，取决于人——与我们相似的人怎样选择去思考和行动。&lt;/strong&gt;而只有一种思维方式有能力取得进步或者长久生存，那就是通过创造力和批评寻求好解释的方式。&lt;/p&gt;

&lt;p&gt;在悲观主义者看来, 人是浪费者, 他们取得珍贵的资源, 疯狂地将它们转换成没有用的彩色图像; 在乐观主义的观点看来, 人类是问题的解决者, 能创造出不可维持的解决方法, 并带来新的问题。既然我们要面对的东西，无论如何都是无穷。&lt;strong&gt;我们能选择的只有：是无穷的无知还是无穷的知识，是错误还是正确，是死亡还是生存。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;且不说读这本书带来的智力上的训练，单就现实意义上，这本书就值得每一个人在对未来的不确定性充满悲观的时代阅读。人们现在担心贸易战带来的萧条，担心AI带来的技术奇点和失业潮，担心全球变暖带来的极端气候。但要做的是掌握在变化后的坏境下过好日子的知识，而不是抗拒变化。书中写道，预防可预见灾难的战略最终必将失败，而且对不可预见的灾难连讨论一下都做不到。&lt;strong&gt;为了防备不可预见的灾难，我们需要在科学技术方面取得迅速进步，还需要尽可能多的财富。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;然而这不等于简单的开源节流，外加管好钱袋子和脑瓜子，避免其进水，而在于重新找回启蒙运动时的乐观和进步，要追求的是从好解释涌现出的范式革命，而不是猴子搬苞谷式的提升技能。&lt;strong&gt;启蒙运动文明发展到今天的程度，并不是靠对财富、选票或其他期初存在争议的东西进行聪明的分配而取得的，而是靠无中生有地创造。&lt;/strong&gt;三体第二部中在描述“第二次启蒙运动”时写道，人性的解放必然带来科学和技术的进步。这话没错。但书中关于推动启蒙的第一推动力却不符合历史发展的规律，在一个看不到未来的历史舞台上，没有了乐观主义的支撑，得过且过人类发展不出“给岁月以文明，而不是给文明以岁月”这样的符合动态社会的观念。&lt;/p&gt;

&lt;p&gt;动态社会是坦承随时都会有不可预料的灾难会逆转进步的趋势，小行星会撞地球，超新星会爆发产生超额的辐射，这些都会毁掉人类的文明，但这不妨碍社会进步的轨迹。如果小行星撞地球都不会停止进步的脚步，那对每个普通人来说，也不应被所谓的不确定性吓倒，吃饭时吃饭,睡觉时睡觉，一心一用,，做好自己眼下的事，就是过冬最好的储备。&lt;strong&gt;给时光以生命，让自己活得有声有色，给岁月以文明，让自己参与到一个更大的远景的构建中。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccXGqI1vC7ZRtFxawSEUMEJHibUvh5daPgAU8S4JqxE7ZoAibtibiaK6hunMMsCVp515qujnknm8nfduw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.12857142857142856&quot; data-w=&quot;490&quot; /&gt;&lt;/p&gt;
&lt;p&gt;ps， 书中关于苏格拉底的梦以及多重宇宙的俩章我没有提及，原因是前者是对全书前俩个关键词的总结，很精彩，即使放到如同柏拉图的对话录中，也是最前三的篇章。而后者则由于我知识的有限，不做评论。&lt;/p&gt;

&lt;p&gt;pps.附上别人家的该书思维导图，能将书中内容提炼到这个层次，虽然不够全面，也很难得了。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7111111111111111&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibceuiarZ9TWPrg4en2154vibp3HJCu4qecmIqibK4NPc62icezBX4YA1sDPw1LRict6u7OfZ21B1HBjPZhw/640?wx_fmt=other&quot; data-type=&quot;other&quot; data-w=&quot;1080&quot; /&gt;&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383604&amp;amp;idx=1&amp;amp;sn=f84e20d5e821c3da952162d194518838&amp;amp;chksm=84f3c975b3844063c568d563542ce61d7608901d92394691899682dc7acc21752223d17d2163&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;恐慌与困惑-读《今日简史》看当下的四重困惑&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383596&amp;amp;idx=1&amp;amp;sn=edc6831710e59830dfde184a1676b77b&amp;amp;chksm=84f3c96db384407bddf5b8403566855e96e5e629c4a12910e33f62848f8376ce794478b9caa2&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;读《生命的法则》，邂逅生态学的四条智慧&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;










</description>
<pubDate>Sun, 09 Sep 2018 12:49:48 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/LJJpiME5RW</dc:identifier>
</item>
</channel>
</rss>