<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>当AI遇到狗年，说一说用深度学习识别不同品种的狗子</title>
<link>http://www.jintiankansha.me/t/vb1N26VP3J</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/vb1N26VP3J</guid>
<description>&lt;p&gt;新的一年即将到来了，这里先祝各位新年快乐，狗年财运旺旺的 。在新年的最后一篇推送中，就简单的介绍一下和狗相关的一项黑科技，狗练识别。&lt;/p&gt;

&lt;p&gt;狗是已知的，人类最早驯化的动物了，早在一万五千多年之前，还处在农业时代之前的原始人类就靠着几根骨头骗到了贪吃的灰狼，又等到了农业时代，人们发现狗真的是十项全能的好帮手，于是开始了对狗的定向演化，于是就有了许多看起来差异巨大的不同品种的狗，下图所展示的只是一些常见的品种，你认出那些你熟悉的品种了吗？&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.0866666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccc3aFpbgqOYVraGSE6uA3zic4g3eb9LXh4nibq5oaG2uEUlZL2dyPV2Al8WgL0X60KWIUbkq9aSPIA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;300&quot; /&gt;&lt;/p&gt;
&lt;p&gt;而根据不同品种的狗狗的照片，按照品种进行分类，在机器视觉中属于Fine-grained classification，也就是根据图像的细节进行分类，类似的问题还包括根据植物的照片判断是那种花等，你也许会说，这样的问题不应该很简单吗，用深度学习就行了，一层不行就再加一层。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7015625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccc3aFpbgqOYVraGSE6uA3zRoUVjnZqKkSMEFJ9eS1QDxsFibKMItGEB0t7RjhyYzsyKWmWXUvZOVQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;且慢，让我们先看看这个问题本身有那些本质的困难之处。首先是不同品种的狗都很相似，比如下图的三种狗，若你不是狗专家，你能够分清楚吗？&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.46255506607929514&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccc3aFpbgqOYVraGSE6uA3ztRNQLHmeZZWuJFKhrZ0kYAGWSicyuyLd5zdPXqRH4pMSVrUL9e1PkWg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;454&quot; /&gt;&lt;/p&gt;
&lt;p&gt;第二个问题是即使是同一种狗，也会差距很大，例如下图的三种狗，竟然是一家人，你说这叫AI头大不头大。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4409090909090909&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibccc3aFpbgqOYVraGSE6uA3zkanZmTMGFTOT4iahg2Rqp9x5gTKrdGSNibdhFgq9yZA9Gucu5J34jyMg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;440&quot; /&gt;&lt;/p&gt;
&lt;p&gt;更要命的是不同的狗狗都有各自独特的pose，而且他们所在的图片的区域，图片背后的背景是草地还是森林都不同。下图展示了三只不同姿势的狗狗。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4967462039045553&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFTIodib4zpias3tZAFxlic0Kf43LVRtaJGClia8V8qFeHzB9QXjhXY7mMvw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;461&quot; /&gt;&lt;/p&gt;
&lt;p&gt;正是由于有这三个问题，要做好狗脸识别，并不是一件简单的事。所以说，&lt;strong&gt;使用深度学习去解决具体问题，需要先调研清楚这个问题的背景和障碍，才能够便于设计下一步具体优化方法。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;该怎么做了？第一种思路是数据增强，也就是用&lt;strong&gt;随机应对随机&lt;/strong&gt;。既然狗子的位置在照片中不固定，那就将原始的图片随机的裁剪一下，旋转一下，将图像的颜色做一些微调，总之就是想象一个熊孩子打开ps修改了每张狗子的照片，给你留下了一堆看起来和原始的训练数据差不多的照片作为新的训练集。下面给出了随机剪裁之后得到的狗的照片示例。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;114&quot; data-backw=&quot;307&quot; data-ratio=&quot;0.3713355048859935&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFHl5vTARl66Vj4eLsWtzbIiaeIibJ0SehhFxVUiagWFWdJQQTTYDg7cia9w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;307&quot; /&gt;&lt;/p&gt;
&lt;p&gt;而另一种方式，可以看成是&lt;strong&gt;用有序来应对随机&lt;/strong&gt;，也就是先通过识别出狗子所在的区域，再将这个区域拉伸成一样大小的图片，来去除背景的干扰。例如下图所示，展示了经过了背景去除和拉伸前后的狗子的照片，可以看到经过处理后的照片，只剩下了我们关心的狗子的信息。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;102&quot; data-backw=&quot;356&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmF4JuIFNAyOH7amAtVib2KIfWv27LiawicpLg7xMDc5AibxUcia7VcYVeS5dw/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;356&quot; data-cropy1=&quot;0&quot; data-cropy2=&quot;96.97491039426524&quot; data-ratio=&quot;0.2696629213483146&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFRCyHkfeePDYpfswZBQFiaWWUBudsjyR5TyibbUgqkf3ONtCnUYCRAmqA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;356&quot; /&gt;&lt;/p&gt;

&lt;p&gt;接下来的问题是狗子的照片不够多，斯坦福大学针对狗的品种识别，搞出了一个数据库，里面有一百多种狗，一共一万多张照片，但若是指望这些图片就能够训练出一个靠谱的深度神经网络，那效果多半不好。&lt;strong&gt;数据增强虽然能够改善预测的准确性，但其上限不高，毕竟原始的信息就那么多。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.0225225225225225&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFyXnqUSXNdQVNS815GEIBRkiaau5UEFEbYZ0ichqQRibh3LXDibkJg3NWXA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;222&quot; /&gt;&lt;/p&gt;
&lt;p&gt;若是你自己做不到，不妨站在巨人的肩膀上。迁移学习正是这样，深度学习的好处是模型不再是铁板一块，而是可以拆解成一层一层的，不用花一分钱，你可以拿大牛们训练好的网络，将其用做自己的用途。关于迁移学习，曾经写过一篇名叫&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383309&amp;amp;idx=1&amp;amp;sn=f0ee86dc43994673f2b818cb2c2efe4b&amp;amp;chksm=84f3c84cb384415a67b8469578df8268a490f12355d4213c126d767227a0da52a77297b47be4&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;白话迁移学习&lt;/a&gt;的小文，感兴趣的可以点击深入了解。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;而迁移学习迁移的是什么样的神经网络，自然是深度学习中最出名的卷积神经网络，关于这个话题，可以参考&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383212&amp;amp;idx=1&amp;amp;sn=e6dbbda2acc5984c8d06e24ec9c84d09&amp;amp;chksm=84f3cbedb38442fb58f0aea635821fcf4ba3edaacef4685716c7eadb6191197ebfa70a6bf14b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;你所不能不知道的CNN&lt;/a&gt;和&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651381959&amp;amp;idx=1&amp;amp;sn=1b920dd476849d88b67a2ef1cf3ed8fc&amp;amp;chksm=84f3ce86b3844790627d2f15256aff0753be1f0b0623da64aaa7357d73e8ed14c415061acb27&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;用CNN来识别鸟or飞机的图像&lt;/a&gt;，这里就不再重复了。&lt;/p&gt;

&lt;p&gt;但迁移学习并不意味着什么都不需要来做，你需要利用成熟的网络提取出的高级特征，用他们来进行预测，但是要注意的是，你预测得出的结果并不是一个确定的狗的品种，而是这个图属于哪个品种的概率，而这又是怎么得出来的，靠的是一个名叫softMax的激活函数，这可是深度学习中最出名的两个激活函数了，soft是保证输出的结果是符合概率分布，也就是不会出现概率为120%的情况，而max是让错的更错，从而提高学习的效率。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7857142857142857&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFwMvjt0icic9QxPRjqIQZC2auUSQ2GjnGNCtBia2Zu2sbs2QHodQ0oInhw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;280&quot; /&gt;&lt;/p&gt;
&lt;p&gt;最后说一说这个例子的实现难度，类似Minst数据集的手写数字识别，宠物狗的品种分类，也是一个相对容易上手的例子，你不需要昂贵的显卡，就普通的个人电脑，就可以基于已有的图像识别网络，例如谷歌的Inception，来搭建一个属于你的狗脸识别程序。除了用来展示，这个例子还可以锻炼你诊断网络的能力，训练的太慢，不妨换换更快学习率，训练的结果起伏太大，还可以再换换更慢的学习率啦。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;开个玩笑，虽然学习率是深度学习中很重要的一个参数，但不是万能的，这个例子的好处是让你能够直观的看到你训练出神经网络有那些不足，又为什么会犯错，例如下图，如果是将图片变成黑白的，神经网络是不会犯错的，但一旦加上了色彩，你就会发现神经网络没有将黄色那只分成拉布拉多犬，这时你就能发现，是你的神经网络过拟合了，网络中的一个神经元学到的速记口诀，但凡黄色的都不是拉布拉多，这时不管你怎么做数据增强，对数据又拉伸又旋转，都无法教会这个神经元忘掉这个口诀，这也是我为什么上文说数据增强的效果是有上限的。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6631799163179917&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFulepXU1KeThKxTzg4wmaMqIE3QK4kQgyL7G5pa4Cp9MFRrWgL6bSmg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;478&quot; /&gt;&lt;/p&gt;
&lt;p&gt;而这时如果你能用dropout机制，时不时的将这个背小抄的神经元踢下线，那么你的神经网络就能够有机会学到拉布拉多真正的特征，这也就解释了为什么dropout机制是一种极为高效的防止模型过拟合的方法。&lt;/p&gt;

&lt;p&gt;卷积神经网络的另一个特点是能够打破所谓的黑箱，让你看看&lt;strong&gt;网络内部特征是如何一步步被抽象出来的&lt;/strong&gt;，下图依次分别展示了卷积神经网络的输入图像和第一层的输出结果，可以看出神经网络第一层的卷积核提取出的特征分别是什么，又对图像进行了怎样的抽象。（虽然小编表示我就算看出神经网络做了什么，也没法说出来，不过这反而说明了网络本身能做一些人无法明确言说的任务，而这正是深度神经网络最革命性的创新）&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9905660377358491&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFF991GMibUUibJloen3eW88NLok7kYKpAMkPPa37WPaf8iaG5pB93uhwHw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;424&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9891067538126361&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmF2GxlEzTSS98pb1ch2DzibA7k89GJqeTac5Yadnia63sIaNHTJibfAEk8Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;459&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在这篇小文的结尾，放上一章狗狗们的全家福照片，愿各位不管是什么汪，新的一年的事事兴旺，合家安康。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4498186215235792&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibceYWb48b16ic2ABZZ3UzWOmFibAHmXwfTOALb9OVSK0eBhkde9UvecicrSXBAIHMDZdVM0nD1jy9VV4A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;827&quot; /&gt;&lt;/p&gt;


&lt;p&gt;&lt;span&gt;参考资料&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1）Using Convolutional Neural Networks to Classify Dog Breeds  Hsu, David&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2）Dog Breed Identification  Whitney LaRow ，Brian Mittl， Vijay Singh&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3）Automatic Dog Breed Identification     Dylan Rhodes&lt;/span&gt;&lt;/p&gt;








</description>
<pubDate>Wed, 14 Feb 2018 12:31:46 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/vb1N26VP3J</dc:identifier>
</item>
<item>
<title>站在风口的猪焦虑-谈谈一流知识和知识付费</title>
<link>http://www.jintiankansha.me/t/Kr20vbrVz0</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/Kr20vbrVz0</guid>
<description>&lt;p&gt;都说知识付费是风口，而站在风口，猪都能飞起来，只是如今这些飞起来的猪都很焦虑。得到预期今年上市，而最近朋友圈很多人都在转汪丁丁老师的关于一流的知识是无价的这样一篇文章。对于这样一篇文章，其中的观点既有认同之处，也有觉得不够清晰的地方，忍不住写一篇小文，集中的表达一次对知识付费的看法与期许。&lt;/p&gt;

&lt;p&gt;从用途上来说，知识有三种类别，第一种是让人吃上饭或吃好饭的学问，第二类是让吃饱饭的人获得幸福的学问，第三种则是让那些真正受使命感驱动的人获得生命意义的学问。我的观点是第三种学问是任何知识付费都无法提供的，因此你可以称其为无价。原因是自我驱动的人体悟得出的东西来自于其私人的经历，不是他们表达观点，而是观点借助他们的经历得以第一次更清晰的展现。这些观点，若没有完整的了解其背景，就不具有可复制性。无论是追求星辰大海的马斯克，还是在知识的海洋里寻找统一的汪丁丁，他们脑中的知识都是难以转化成能够为大众接受的形式的，这也是那篇文章中提到主要观点。&lt;/p&gt;

&lt;p&gt;然而对于其他两种知识，则是知识付费服务所要深耕的。得到的三个关键词是赋能，父爱，焦虑，这是针对第一类知识。而针对如何获得幸福的知识，由于幸福的生活是多姿多彩的，需要培养个人的兴趣爱好，属于喜马拉雅和知乎所关注的。下面的图是我参加的知乎live自助餐，看看其中的分类，就知道更多的是和怎么过得开心有关的。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.22628510863804982&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcez33Vv0jgtrFRqtXhkCCIoIjoRwargr0azibxsiaS122aHQicCRDmM7KszDkwUlxxKbWE3DvOZLEayQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1887&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在我看来，知识还有另外一种更复杂分类方式，这也算是一种思维套路了，设定两个维度，将不同的知识分在不同的象限上。在横轴，可以按照知识的来源是集中（单学科）还是开放（跨学科+应用）来分，而纵轴可以分成知识表达是依据理性还是依据直觉，由于大多数人大多数时候都是系统一的直觉思维，因此按照直觉写出的东西更加通俗易懂。霍金的《时间简史》属于单一学科+直觉表述（虽然书中的观点是反直觉的），薛定谔的《生命是什么》属于跨学科+理性表述，而众多的科研论文属于单一学科+理性表述，至于最难的是跨学科+直觉表述，这方面做得好的是像《人类简史》这样的作品，但同时也是伪科学泛滥的地方。&lt;/p&gt;

&lt;p&gt;说完了知识的分类，下一个问题就是排座次了。我不喜欢鄙视链，每个人都有自己的地域与地狱，知识的边际成本是零，是对传播者来说，但对于接收者却各不相同。所谓的一手知识二手知识的分类，其实也没有意义。只听一本书的主要观点，就一定只能接收到二手知识了吗？若是你听完有了自己的思想，做了自己的调查，那你拿到手的知识就带着你自己的体温，这也就是所谓的赋能。而能够重复出那些曲高和寡的知识，也不意味你掌握了世界的真相。&lt;/p&gt;

&lt;p&gt;不管做什么，都要想清楚自己最想获得什么。要做针对吃饱饭的知识服务，要做的是加强实践，当下的知识问答就很适合，你学完了，答对了所有的题目，能在小论文中恰当的引用相关的知识，那就算获得了实用的技能，就可以升职加薪。而要做针对吃饭开心的知识服务，则要做到开放，让每个人都能分享自己的快乐，让所有想要倾听的耳朵都能方便的找到频道。这两类知识服务，都是能为社会创造价值与福祉，是值得去做的。而对于那些试图回答如何成为自己这个问题的知识服务，则只能将其归于只给鸡汤不给勺的不地道行为，虽然短期会带来可观的流量，但长期必然会让人厌烦。&lt;/p&gt;

&lt;p&gt;熊逸在他的得到专栏说了解一个观点，首先要看他来自那里，反对谁。而我觉得真正要看的是那些人反对他，那些人又在找这个观点做儿子（认为他其实来源于我的思想）。任何一种观点一个概念都是动态的，社会性的。了解其历史，也就了解了内部的变化逻辑和运作机理，而看清楚了这些观点的支持者和反对者，才能预测其未来会怎样变化。这也是不管针对吃得饱还是吃得开心的知识服务都要做到的，你可以将其称之为系统化，或者是提供些百度搜不到的东西，但本质上，你做的是带着读者将书读厚而不是读薄的过程。同样是写一本书的缩略版，可以持久的知识服务要做的是提供一个登山的拐棍，让读者能够在个人攀登的时候能够累的时候支撑一下，而不是提供一个全景式的VR，让他们能够葛优躺着就登顶成功。&lt;/p&gt;

&lt;p&gt;关于知识付费，最后我还想说说我的一些担忧之处。真正推进社会进步的是那些忘记了自我幸福的孤独探索者，他们的所作所为，不管到了那个时代，都不会被大多数人理解。而现代社会的自由主义传统，包括对个人财产权的绝对保护及契约精神，都帮那些追求星辰大海的人获得了更多的发展空间。但知识付费的兴起，会不会造就一个“美丽新世界”式的牢笼。当越来越多的人认可，只要有了那怕一点认知差距，就能够打开印钞机了，那会不会导致本来自我驱动的人，因为无法发出自己的声音，而被人忽视。用更直觉的说法，以前梵高的悲剧是直到他死了，人们才发现他的伟大，而未来梵高的悲剧，会不会是他活着的时候，自己的画就被各种按自己风格仿制的画所淹没，以至于没有几个人看到原作。&lt;/p&gt;

&lt;p&gt;要避免这个问题，我觉得知识付费还是需要补上其欠缺的第三块，也就是如何活出自己。关于这个问题的答案，我认为是在素材上提供人物传记，在表现形式上要带着主观感情，要像史记或者巨人三传那样，既有细节也有评点。要写的当然不止是大人物，小人物找到自我的故事也是值得讲述。但这些故事，其实比跨学科的通俗化写作还要难，需要写作者自己就是自我驱动者，只有他们才能写好身边同行者的故事。而且人若是找到了自己真正想要的，那就不会在乎当下是不是开心，就算学习技能也不需要有别人为你搭梯子了，所以知识付费注定不会去做这样费力又自掘坟墓的事，而这些个人的经验感受，如果可以算是知识，也可以因为有价无市而称为无价了。&lt;/p&gt;

&lt;p&gt;总结一下，关于知识付费，从知识的内容和用途上，我在这篇小文中分别进行了分类，有些能做，有些不能做，能做的每一类该怎么做，我也提出了自己的建议。但我真正关心的还是那些所谓无价的知识，这才是让一个社会真正富足的根基，那些对未知永无止境的探索热情，那些不达目的不罢休的坚韧不拔，这都是只有自己能给自己的，我们不应该责怪知识付费不能给你提供这些，反而要提供伪造品的无良鸡汤充满警惕。&lt;/p&gt;

&lt;p&gt;原创不易，随喜赞赏&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9397590361445783&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcd4icdSNQnx98Zicw7nnP3icPycqI1uRMvRxCezhYm4xQKdbiamvHVmC19a0o7YS03eqTrIL9QJS4wS4w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;249&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383313&amp;amp;idx=1&amp;amp;sn=a4279c3f2ec27302a50b42235301be71&amp;amp;chksm=84f3c850b3844146ca1042a90fe56414a72ab9f8b7a7215c10e2db42f336a8f38d77a6bfe582&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;读 当呼吸变为空气 没有活过的生活不值得审视&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=210687560&amp;amp;idx=1&amp;amp;sn=30100912ce5a7ee60a5090c86dc386a9&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;碎片式的知识是无用的吗？应该如何看待？&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

</description>
<pubDate>Mon, 12 Feb 2018 16:02:03 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/Kr20vbrVz0</dc:identifier>
</item>
<item>
<title>胶囊网络结构Capsule初探</title>
<link>http://www.jintiankansha.me/t/DFL1G8ddhQ</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/DFL1G8ddhQ</guid>
<description>&lt;p&gt;作为神经网络的大牛Geoffrey Hinton在17年十月提出了一种新的网络结构，他称之为Capsule，这个词的中文意思是胶囊。在本周，Capsule的代码在github上开源，瞬间成为爆款。 Capsule式的网络结构，和卷积神经网络一样，最初是用来处理视觉问题的，但是也可以应用到其他领域。这篇小文不涉及Capsule神经网络的数学细节，而是关注Capsule网络是如何克服CNN存在的问题的。阅读之前，对于卷积神经网络不熟悉的可以先看看&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383212&amp;amp;idx=1&amp;amp;sn=e6dbbda2acc5984c8d06e24ec9c84d09&amp;amp;chksm=84f3cbedb38442fb58f0aea635821fcf4ba3edaacef4685716c7eadb6191197ebfa70a6bf14b&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;你所不能不知道的CNN&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.3333333333333333&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdRef6B9WjltyUVqWvhQX2cVakZ7GRGnXvdSYNcLe0nYcfIwkZJKEXuCPIuGv0RTYWDFf6vWuwDIw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;225&quot; /&gt;&lt;/p&gt;
&lt;p&gt;谷歌公司的Geoffrey Hinton大神&lt;/p&gt;

&lt;p&gt;用Hinton大神的话来说，计算机图形学做的是渲染，而计算视觉想做的就是渲染的逆过程。渲染是将三维的图像投影到二维，在数学上意味着给原图乘以一个固定的矩阵。而计算机视觉做的，则是Inverse Graphics，也就是从二维的图像推测出本身的三维结构。为了比较胶囊网络和CNN之间的区别，让我们先看看CNN想做什么，有什么不足。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1) CNN那里做的不够好&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;有一些事情，对小孩子来说很容易，但若是卷积神经网络，则很难，例如认清楚下面两章图上的猫是同一只。然而若是神经网络无法做到这一点，机器视觉的模型，不管处理怎样的任务，都注定会不够稳键（robust）。解决图像迁移后的一致识别，是Capsule神经网络出现的第一个目的。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.46530612244897956&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcesX0Jso3CcjQl3jZyyVQ2RmWaSadwAKZ6rUhjnbDFCIiazxkyzoVLbzHEm7oqyC8EPrubRkp45Izg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;735&quot; /&gt;&lt;/p&gt;

&lt;p&gt;不止是左右的平移，CNN对于旋转，加上边框也会难以觉察出其一致性，CNN会认为下图的三个R是两个不同的字母，而这是由网络结构所带来的，这也造成了CNN所需的训练集要很大，而数据增强技术虽然有用，但提升有限。&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.47793190416141235&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcesX0Jso3CcjQl3jZyyVQ2Ry60uqy6EUl4QsIbf9W9Xhttb9YibT6kGD1MJYbe6pCt96WlHoUotiaDA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;793&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9135338345864662&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcesX0Jso3CcjQl3jZyyVQ2RT5uCI0gYIibENKuQQUIKQgdd8yNvCQs1QibibJoCReIAygTQQicIHBQdhQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;532&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一张脸上不应该只是由两只眼睛一张嘴巴一对鼻子构成的，组成整体的部分之间的相互关系对于识别图形，也很重要。下面的这张图会被CNN看成是一张脸，这是因为CNN识别脸的时候，是识别脸的几个组成部分，下图中的确有两个眼睛，有鼻子，有嘴，虽然其位置不对，这对于CNN来说就够了，CNN是不会注意子结构之间关系的。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8577777777777778&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdRef6B9WjltyUVqWvhQX2cH8rtUb5xpYGmmic09jCMQbGaLPFcZ1ppZ073Z2GgT7d447vj1icVNI1Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;225&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;160&quot; data-backw=&quot;558&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdRef6B9WjltyUVqWvhQX2cK4vlUic0z2MJ5ZWc2vBDwia5ficTKbX1gzpQPuVa21NTKelhgPsQ2zWqw/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;590&quot; data-cropy1=&quot;5.286738351254481&quot; data-cropy2=&quot;174.46236559139786&quot; data-ratio=&quot;0.2864406779661017&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcdRef6B9WjltyUVqWvhQX2c6A6Z5P1Urzr7uVamKwqeL2tq0icpB48zgN2cFRicdSkkMf1IomvKUIXQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;590&quot; /&gt;&lt;/p&gt;



&lt;p&gt;同样的道理，CNN的网络难以区分左图和右图，他们是由相同的形状（椭圆形）组成的，其信息都储存在子结构之间的关系中。卷积核之间的独立即使的训练简单，又丢失了全局的关联信息。&lt;/p&gt;
&lt;p&gt;              &lt;img class=&quot;&quot; data-ratio=&quot;0.9884393063583815&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdRef6B9WjltyUVqWvhQX2cBO7cFcRicet6sru6oTRkPlwYHfpXkn54YQeYBHkoCs3Q1eWngnVFONg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;173&quot; /&gt;      &lt;img class=&quot;&quot; data-ratio=&quot;0.7896995708154506&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdRef6B9WjltyUVqWvhQX2copuVfdwH3NlenyoYOQfSEj5seGibQAEtKn8XGwLrs2JDgSCQWMVwMfg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;233&quot; /&gt;&lt;/p&gt;
&lt;p&gt;人类在识别图像的时候，是遵照树形的方式，由上而下展开式的，而CNN则是通过一层层的过滤，将信息一步步由下而上的进行抽象。这是Hinton认为的人与CNN神经网络的最大区别。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.46210526315789474&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcesX0Jso3CcjQl3jZyyVQ2RMTVDGSnxcduFItCqrU0PDvAXgnQxUFGKKMRqibogsglohWA7BfZsACQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;950&quot; /&gt;&lt;/p&gt;

&lt;p&gt;总结下这一小节，卷积神经网络的问题是其神经员之间都是平等的，没有内部的组织结构，这导致无法保证在不同位置，不同角度下对同一个物品做出相同的识别，也无法提取不同卷积核得出的子结构之间的相互关系。CNN中采用的分块和共享权重，其目标都是希望能够使神经网络学到的特征提取之术能够在图形出现微小变化时能够应对，而不是针对图形的变化，对应神经网络进行相应的改变，而这正是capsule神经网络所要做的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2）脆弱与反脆弱&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;用英文来说，为了提升模型的可扩展性，CNN追求的是Invariance，而胶囊网络追求的是Equivariance。熟悉塔勒布的童鞋会想起他的命运，脆弱的反义词不是稳健，而是反脆弱。这正是胶囊神经网络强大的地方，追求的是在不同视角下对同一个物品的相同感知（标签），而通过对各个视角下都有标记的数据死记硬背做到对相似物品图像的识别。&lt;/p&gt;

&lt;p&gt;回到上文提到的字母R的例子，下图的两个字母左边的那个不是R，但人类大脑在做出这一判断的时候，不需要一个老师指出这一事实，人会在自己的大脑中对左边的图像进行翻转，使得半圆处于图形的上方，之后再识别出图形，如果能够让神经网络也能做类似的事，那就不需要太多的标注数据了。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4941329856584094&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdRef6B9WjltyUVqWvhQX2c4lkW5qicZ1fCV0gjDYynFJ8Ypicwdq7CMwsaWkdP07YRCyWSIzeecbqg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;767&quot; /&gt;&lt;/p&gt;

&lt;p&gt;正是由于胶囊网络可以对图形进行翻转，这使得其不需要那么多的标注数据，下图对比了CNN和胶囊神经网络在识别人脸上的区别。CNN中需要有一层去应对不同翻转角度的脸，而胶囊网络则可以用一个胶囊去完成这个任务&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.2700106723585913&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdRef6B9WjltyUVqWvhQX2c6HGafvh4Pibuvfw7rXUSINcIpDfPPQhhE63ANnZDBiaRNtUHBAkp6TnQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;937&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.24180327868852458&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdRef6B9WjltyUVqWvhQX2cNXWPvUhQuyib17UMM473hUL4DWScDcommbp5ibGwM1gTv7cIuicPXKw8Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;732&quot; /&gt;&lt;/p&gt;

&lt;p&gt;具体看看胶囊神经网络想做什么，假设我们训练一个识别数字2的网络，我们首先做的是在每一层训练一些胶囊（即一组相对独立的神经元），每一个胶囊只负责图形中的一个子结构，例如图中的三个胶囊，分别识别横线，竖线和锐角，每个胶囊给出图形中每个位置出现自己负责识别的特征的概率（intensity），之后再通过一个编码器，综合每一个胶囊给出的在各个位置的强度信息，最终得到经过重构后的图像。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7443820224719101&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdRef6B9WjltyUVqWvhQX2c1hJamiaArMdZFxib7rERlIyUL7AQlVK5x4ya5q9QLeT1ve59SlUaAaGg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;712&quot; /&gt;&lt;/p&gt;

&lt;p&gt;例如我们关注的是识别数字7，那么下图中的第一行是传统的神经元，会给出对应图形是不是7的概率，但这个神经网络却无法识别第三行的数字。一个包含两个神经元的胶囊可以一个识别图形的翻转情况，一个识别具体的形状。不管图形是第二行那样竖直站立的，还是像第三行那样歪倒的，胶囊网络都可以正确的识别。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.614853195164076&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdRef6B9WjltyUVqWvhQX2cr3icgBmyqib6qnVkaAkA18K5aLy9uDVTFjEsYslHR2RicBS0sGTUHM6SA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;579&quot; /&gt;&lt;/p&gt;

&lt;p&gt;由于每个胶囊给出的是输出是一组向量，不是如同传统的人工神经元是一个单独的数值（权重），为了解决这组向量向那一个更高层的神经元传输的问题，就需要Dynamic Routing的机制，而这是胶囊神经网络的一大创新点，也是理解中的难点。Dynamic routing使得胶囊神经网络可以识别图形中的多个图形，这一点也是CNN所不能的。由于讲解Dynamic routing需要很多数学细节，这里就不再细说。可以理解成一种实时的投票，决定不同胶囊网络之间提取的特征哪一个更显著，更有益于上一层网络的重构。&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;3）冗余对抗复杂&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;下面的图展示了胶囊神经网络在MINST数据集下的表现，这里可以看出胶囊神经网络的另一个好处，即可解释性强，你可以看出每个胶囊想做什么。不过即使你知道了每个胶囊识别的是那些子结构，你也很难想象出是怎样通过这些结构的组合识别出数字的。我猜那是你理解的数字识别的规则，根本不需要这么多的子结构，但这只不过是你对当前情况的解释，真实的大脑，如同胶囊神经网络，所做的都是利用冗余来对抗复杂，让模型模仿人类大脑用冗余带动生产的能力， 来对抗复杂的问题。引申来说，说深度学习缺少解释性，部分的原因是我们对解释性的理解太浅了，人们天生不习惯复杂的解释，但要应对复杂，就必须有冗余。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8304552590266876&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdRef6B9WjltyUVqWvhQX2cBMt0lUd0SQm3981NhGLMhcCIz0uyzrxN5rRoNDGrmQMEyOPECqn50Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;637&quot; /&gt;&lt;/p&gt;
&lt;p&gt;此图来自Hinton关于Capsule的论文，图中的第一列是网络的输入数据，第二列是网络根据原图重构的图形，网络的目标是让这两幅图的差距尽可能小，第三列给出了这两幅图之间的差别，之后的列是网络中每一个胶囊所识别的子结构，可以看到，这些子结构之间很多是相似的，也就是网络有冗余。&lt;/p&gt;

&lt;p&gt;冗余带来的好处，从下图可以看出，这里展示了如果对于每一个胶囊给出的intensity值加上或减去两个标准差后，网络重构出来的图像会是怎样，可以看出，图形基本还是能识别成原始的2的，这说明网络具有鲁棒性，不会像CNN一样，仅仅因为一两个像素点的改变，就将图形识别错误。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.43537414965986393&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdRef6B9WjltyUVqWvhQX2cOG9eKJc2bkNLsT9BtbcXoR1lFDxWibvbLxylsIGibroDCta517sDzhOg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;441&quot; /&gt;&lt;/p&gt;
&lt;p&gt;类似CNN的深层网络，胶囊网络也可以有层次结构，如下图所示，红色的是底层的胶囊结构，绿色的是高层的胶囊结构，绿色的胶囊无法知道图形中具体的形状是什么在哪里，这使得绿色的胶囊结构通过训练所学到的只是图形中子结构之间的相互关系，这就实现了CNN所不能的，也就是将图像在其头脑中进行翻转伸张平移等视角变换，但同时保持其特征的一致性。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7510373443983402&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdRef6B9WjltyUVqWvhQX2cPib3JFuWBAcgOxp32O9NibqSDfHjGub4n0bnY72ApEv1uAoAcJwzyibKA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;723&quot; /&gt;&lt;/p&gt;
&lt;p&gt;下面展示了胶囊神经网络在3D图像中的表现，这里的目标是看看胶囊神经网络能否正确的的从下图中左边的一列，将图形做选择，得出右图的图形。我们看到，胶囊网络翻转生成的图像效果不错。这说明神经网络学到了每个图形中子结构之间应该怎样连接。而在第二幅图中，展示了说学到的连接也可以应用在没有出现在训练集的测试数据上，哪怕训练数据是汽车，测试数据是飞机，这展示了胶囊网络优良的可扩展性。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.798219584569733&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdRef6B9WjltyUVqWvhQX2cibRjSbibSUhwcMMuvjakTqhFV3tiaA63bOAA90r58rHhkdAftHJibvO7Jw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;674&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7635829662261381&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdRef6B9WjltyUVqWvhQX2cKrqPJSs3ommkvXAsm9zicYLOexffTkwap3sUKibAk3ZgwnricsWsnBOUA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;681&quot; /&gt;&lt;/p&gt;

&lt;p&gt;任何结构的神经网络需要提高其模型的可扩展性，都需要做正则化。CNN的标准做法是随机的扔掉一些神经元，即dropout，而胶囊神经网络则是通过重构的方式，在识别某一个图形的标签时，将网络中与这个图形无关的部分都关闭，而使剩下的网络试图尽可能准确的重构这个图形，从而提高模型的泛化能力，这里借鉴了自编码器的思路，结合胶囊网络子结构之间的独立性，这使得模型的泛化能力很强。而这也可以看成是通过冗余来应对复杂，明明识别6和9的网络可以很类似，但因为有了分工，弄混的概率也少了。&lt;/p&gt;

&lt;p&gt;4）小结&lt;/p&gt;
&lt;p&gt;胶囊网络可以算是一种革命性的网络架构，神经元的输出从一个标量变成了一组向量，这如同让网络流动起来了。每一个识别子结构的胶囊，使原始图形中隐藏的信息在bits数变小的情况下实现了高度的保真，而这个保真，又是从复杂结构里直接进化得到的。通过重构原图，模型做到了在视角变换后，通过网络结构的改变，给出相同的认知（重构），这使的胶囊网络具有了反脆弱的性质。另外需要指出的是，CNN和胶囊神经网络并不是互斥的，网络的底层也可以是卷积式的，毕竟胶囊网络善于的是在已抽象信息中用更少的比特做到高保真的重述。&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.7255216693418941&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcdRef6B9WjltyUVqWvhQX2cAJ3EysYYBwhTaGqIO9n7q4VOg3KwH847JNbaXd9AOjz5PfUOuP69Jg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;623&quot; /&gt;&lt;/p&gt;
&lt;p&gt;这里给出了用来识别MInst数字集的CapsNet网络结构图，底层是卷积层，而用来最终做预测的也是熟悉的Sigmoid全连接层，改变是只是中间那些层，由卷积网络变成了胶囊网络。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383309&amp;amp;idx=1&amp;amp;sn=f0ee86dc43994673f2b818cb2c2efe4b&amp;amp;chksm=84f3c84cb384415a67b8469578df8268a490f12355d4213c126d767227a0da52a77297b47be4&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;白话迁移学习&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383280&amp;amp;idx=1&amp;amp;sn=a6fd903f2c47339c52dcea9eedf65851&amp;amp;chksm=84f3cbb1b38442a7f4aac491852e06c34794154946a3656bc4ac4805b1ef1b41cb4469ae8419&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;对抗神经网络初探&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;参考资料&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;http://www.cs.toronto.edu/~fritz/absps/transauto6.pdf&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;http://cseweb.ucsd.edu/~gary/cs200/s12/Hinton.pdf&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://www.arxiv-vanity.com/papers/1712.03480/&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://jhui.github.io/2017/11/03/Dynamic-Routing-Between-Capsules/&lt;/span&gt;&lt;/p&gt;


</description>
<pubDate>Sat, 03 Feb 2018 06:25:33 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/DFL1G8ddhQ</dc:identifier>
</item>
</channel>
</rss>