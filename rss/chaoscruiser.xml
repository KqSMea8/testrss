<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fchaoscruiser-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/chaoscruiser-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fchaoscruiser-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>混沌巡洋舰</title>
<link>http://www.jintiankansha.me/column/ultiWL8Axg</link>
<description>混沌巡洋舰 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>[原创]为什么感觉是不可阻挡的力量-读《 The Strange Order of Thing》</title>
<link>http://www.jintiankansha.me/t/Hb2QinSu45</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/Hb2QinSu45</guid>
<description>&lt;p&gt;今天说一本18年2月的新书，作者是神经科学家达马西奥，，而他的新书《The Strange Order of Things》则延续了前作《笛卡尔的错误》，从这本书的副标题Life, Feeling, and the Making of Cultures可以看出书中的几个关键词，感觉（feeling）是如何指导生命的进化，并在文化的发展中起到的枢纽作用的。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.5197568389057752&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibceH3XibibtYaFTyhHDyqyq1RcWAiaghQYVUp0UmZ7x0ONQM4KtyLESIeMFnvscWQAnB6a2VY3mZWMehA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;329&quot; /&gt;&lt;/p&gt;
&lt;p&gt;要概述这本书的内容，可以将其看成是对你是谁，从哪来，到哪去，这三个根本问题的回答。相比与《笛卡尔的错误》，这本书的历史感更远，人文关怀更深，前一本书是批判西方传统的理性中心，而这一本书则是讲具身认知（Embodied cognition）的故事。&lt;/p&gt;

&lt;p&gt;记得在读《粉红牢房效应》这本书时，好奇为什么外界的坏境能够对人的思考产生如此深远的影响，比如名字是否读起来顺口，是否蕴含了更多积极的/合适的意义会极大影响一个人的职业路径，或者将监狱的墙壁改为粉红色，会减少暴力犯错发生的概率，所有这些看似不合理的现象，放在具身认知的视角下，就变得理所应当了。&lt;/p&gt;

&lt;p&gt;传统的认知科学，把认知简化为计算，类似于计算机的符号加工过程，都是一种对信息的处理、操纵和加工。然而具身认知的视角却说你的身体不是支持大脑的电池，认知是依赖和发端于身体的，身体的构造、神经的结构、感官和运动系统的活动方式决定了我们怎样认识世界。&lt;/p&gt;

&lt;p&gt;但身体是怎么影响认知的，这就涉及到本书最重要的关键词-homeostasis（动态平衡或内环境稳态），而这也是作者对前文给出的第一个问题（我是谁）的答案。所有活着的生物，在其多姿多彩的外表和迥然不同的行为之下，其根本目的都是去追求动态平衡，从而确保“生物体在一个有利于适应生存环境和有利于物种繁荣的范围内进行自我调节”&lt;span&gt;。&lt;/span&gt;就像体检时你的每一个指标都有一个上限下限，一旦超标就意味着你的身体有了问题。&lt;/p&gt;

&lt;p&gt;然而体检检测的指标只是身体要维持的动态平衡的冰山一角，比如体检不会检测血液的ph值，不是因为这个指标不重要，而是一旦这个指标出了问题，可能出问题的人会挨不到出体检报告的时间。所有的生命为了自身的延续，都需要将其内在的环境稳定在适合生存的范围内，越高级的生物，需要维持的平衡越多，这就是动态平衡所要达到的目标。从维持动态平衡出发，接下来要问的是生物进化出了那些机制，来维持越来越复杂的动态平衡（从哪来）。答案是从一开始，在最早的原始生命形式中，“情绪和感觉的世界”是驱使生物不断演化的力量，最终推动人类形成丰富的意识和创造灿烂的文化。&lt;/p&gt;

&lt;p&gt;第一个重要的发明是神经系统，最初的神经系统，是感知周围的坏境，比如眼睛，之后是体察自身内部，比如肠道的神经元。之后是感觉（feeling），动物和人一样，会有喜怒哀乐，对于不同的生物，情感带来相同的进化优势，高兴意味着你的身体正处于动态平衡的状态，哀伤或恐惧意味着你的身体正在处于或者即将面对动态平衡的失衡。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;接下来重要的发明是意识，有些生物能够认识到自己的存在（subjectivity），这意味着他们能够将不同时间，不同地点的经验整合，从而使得自己当下的感觉不仅仅能够综合反映当下身体是否处在平衡状态，还能够影响自己身体是否将处在动态平衡状态。比如安慰剂效应，你拿小白鼠做实验，安慰剂就没用。或者反安慰剂效应，你总觉得自己老了，那么自己的身体也会显得衰老。&lt;/p&gt;

&lt;p&gt;黑猩猩海豚都有自我意识，使人类不同于动物的是文化的诞生，然而文化中的每一项发明，也和感觉以及其要达成的目的-动态平衡有关。能不能对动态平衡的维持有帮助，是该文化创新是否有用的最终裁判。比如音乐和绘画使得人的情感能够被高效的改变，对于不会艺术欣赏的人，好听的歌，好看的画也会让Ta心情舒畅；比如医学能够直接的帮助那些没有处理好自身动态平衡的人，比如通过宗教和政治制度来疏导天性中的暴力和欺骗，从而促成合作，以更高效的维持更多人的动态平衡，而科学通过提供对看似无常的变化的解释，让人们安心，从而维持了动态平衡。&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;接下来要说这本书关心的第三个问题，去哪儿？这也是我为什么要介绍这本书的原因，其中会有较多个人的议论和延伸。英国卫报的书评说尼采读了这本书肯定会点头称赞，但只从非理性哲学的角度来解读这本书，会忽视这本书在科学上的严谨，作者不是在鼓励更多的酒神精神，而是说“感觉作为人类文化繁荣的推动器、监督者和谈判者，并没有得到它们应得的荣誉”。&lt;/p&gt;

&lt;p&gt;这一点在人工智能盛行的当下，尤为明显。人被当成了一个算法的执行者，这使得人误以为自己会被算法取代，甚至推出未来会有无用阶级来。然而人的感觉是来源于身体的，没有身体的算法在人的感觉领域永远是无能为力的，就如同瞎子摸象一样，因为人的感觉来源是多维度的，互动的（反身性），而算法却是可预测的，不灵活的，所以只有人能够模拟出他人的情感会怎样变化。&lt;/p&gt;

&lt;p&gt;然而另一个值得担心的趋势则是对情感真正作用的缺乏理解，其体现在对所谓的“至情至性”的歌颂。如果一个人想到什么就说什么，那这个人的人品就比那些说话周全过脑子的人更值得称赞。但从动态平衡的角度来看，文化的目的是通过驯服人性中野蛮的部分来更好的维持自身。实际上大多数的真性情都是被特权惯坏了，没有约束，自然可以为所欲为，但这并不等于说话做事不过脑子代表了好的道德品格。理解了感觉是为了促进动态平衡，那就要通过修身，让自己以及自己散发的气场带着中正平和的气息，起伏不要那么大，这才是在进化意义上的善良。&lt;/p&gt;

&lt;p&gt;我们在大多数情况下以为能够消除身体感受，让理性占上风，仿佛我们是纯粹的精神存在，只不过不情愿地被束缚在百来斤的肉体上。但实际上这样的幻想却给世界带来的问题。比如你以为自己有深度有内涵会独立思考，但实际上你却会被猫咪（小姐姐）的短视频吸引，被网站上的特效多点击一下，被极端的观点影响，这不是由于你的感觉机制出了问题，生物学的机制关注的是个体上如何高效的维持动态平衡，但当下的问题却是社会层面的动态平衡做的不好。而要解决社会层面的问题，就需要社会整体的观念改变。&lt;/p&gt;

&lt;p&gt;总结一下，这本书要论证的是人类以及所有生命的真正活力和来源，其实是感觉。正如他所说的，“体弱的病人，被遗弃的爱人，受伤的战士，和追求爱情的行吟诗人，都能感觉到周围的事物。”真理是简单而深刻的；缺少了感觉，理性不知何处落脚，然而同时也要注意，不要将任性当成了感觉，感觉是为了维持自身的动态平衡，而善良则是帮着身边的人维持动态平衡。&lt;/p&gt;

&lt;p&gt;更多阅读&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383614&amp;amp;idx=1&amp;amp;sn=5297deb6ecea3d9d83c755513be53adb&amp;amp;chksm=84f3c97fb38440695d8f2839ced47b7d44d4c2cf110266471ab3005e05c81ee59b2924b351e4&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;节律的跨学科研究-读《同步：秩序如何从混沌中涌现》&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383596&amp;amp;idx=1&amp;amp;sn=edc6831710e59830dfde184a1676b77b&amp;amp;chksm=84f3c96db384407bddf5b8403566855e96e5e629c4a12910e33f62848f8376ce794478b9caa2&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;读《生命的法则》，邂逅生态学的四条智慧&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;







</description>
<pubDate>Sun, 23 Sep 2018 18:44:41 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/Hb2QinSu45</dc:identifier>
</item>
<item>
<title>[原创]学习如何学习 之 meta learning</title>
<link>http://www.jintiankansha.me/t/Zu8xwvhsNT</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/Zu8xwvhsNT</guid>
<description>&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;img data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.665625&quot; data-w=&quot;640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcd9g9aFjBooVn5U4PP1EDF3ugVJrLlia2ELtxHbXUJs7SUPtRaxmkUBHhx3jLciaHXpx1ABVYwYBu9w/640?wx_fmt=jpeg&quot; /&gt;&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;span&gt;作者许铁，微信号：ironcruiser &lt;/span&gt;&lt;br /&gt;&lt;span&gt;法国&lt;/span&gt;&lt;strong&gt;巴黎高师&lt;/strong&gt;&lt;span&gt;物理硕士 ，&lt;/span&gt;&lt;strong&gt;以色列理工大学&lt;/strong&gt;&lt;span&gt;（以色列85%科技创业人才的摇篮, 计算机科学享誉全球）计算神经科学博士，巡洋舰科技有限公司创始人,   《机器学习与复杂系统》纸质书作者。曾在香港浸会大学非线性科学中心工作一年 ，万门童校长好战友。&lt;br /&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-ratio=&quot;0.10966542750929369&quot; data-w=&quot;538&quot; class=&quot;&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; /&gt;&lt;/pre&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;e9vou-0-0&quot;&gt;深度学习如火如荼的今天， 我们看到强大的视觉算法识别了复杂的疾病和所有人所观察不到的事物细节， 强大的阿法狗能够结束人类历史最精妙的发明围棋。一方面这些算法无时无刻不再鄙视着人类， 另一方面它们越来越透漏出它们的局限，比如惊人的数据消耗， 惊人的耗能，   “ 有限的范化能力”  ，  使得越来越多的人了解它们不过是一些头脑巨大的爬虫 而非真正的智能物种。    给你和人类学习一样的数据和一样的能量， 看你还能牛到哪去？  这是人们经常的质疑。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;7mu6m-0-0&quot;&gt;这些问题事实也是学界的焦点，  前两年一片叫做“元学习” 的文章， 就试图从学习的本质解决问题。 那就是 - 学习 如何 学习， 就像那个cousera 的爆款课程一样 learn how to learn. 机器能够用暴力的算力和数据来梯度下降，但是它们真的懂学习吗？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8ko21-0-0&quot;&gt;我们知道， 所谓的机器学习还是深度学习， 无非给出一个问题， 你写出一个带着一堆参数（假设）的问题表达式和损失函数， 然后上一个损失函数， 然后梯度下降大法， 求出一个最优解。    &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;3np5r-0-0&quot;&gt;然而如果我说这是学习的唯一方法， 你一定会觉得我学傻了。 难道你要学习人任何一个任务，都要在那里等着你大脑里的神经突触一点点的改造（梯度下降）？    简直有一种欲练神功， 必先自宫的既视感。    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;3np5r-0-0&quot;&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;2f6vl-0-0&quot;&gt;我们的生活无时无刻不需要我们快速的学习和适应新的情况， 而神经突触的改变是一个慢过程， 这就决定了它必然不是学习的全部方法。  &lt;/span&gt;这个“快” 与 “ 慢” 的矛盾如何调和呢？  一个解决方法就是学习本身也是一个“快” 与“慢” 相加的过程。  &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;d03is-0-0&quot;&gt;事实上我们人类能够快速学习新情况， 是因为我们已经经过了大量的基础性学习， 我们后天的学习 ， 事实是因为我们的学习建立在更基础的模型之上。   这一点你的反应可能立即是迁移学习， 但是光看我们深度学习里那个迁移参数的深度学习是一个解决方法， 你可以把迁移学习看作今天这类方法的一个特例 ，而我们要看的是一种更广泛的方法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;95efr-0-0&quot;&gt;这里提到的双层学习， 是另一种更基本的框架， 又称元学习 ， “meta learning”  ，  它的概念是学习一大类任务的先验信息 ， 然后利用学习到的先验学习， 加速这一大类里的每个具体任务的学习。 其实这样的一个工作正是深度视觉能够成功的关键， 我们常说的CNN结构本身就是抓住了图像理解这个任务里平移不变等关键先验信息， 然后把它们手写到网络里面去的。  这里元认知唯一的区别在于我们不再靠手写，   而是让神经网络自己学到这个先验。  就好比给你一个特别巨大的多层感知机网络， 然后让它自己学出类似深度卷积的结构来。  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;9ugf9-0-0&quot;&gt;今天讲的元学习的一个具体实例是指利用RNN网络加上深度强化学习的方法先学得一大类相关任务的先验信息，  然后利用这个先验信息，不用梯度下降学习， 纯凭网络动力学， 也可以适应每一个具体的任务得方法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;44afo-0-0&quot;&gt;文章声称， 当您 有一个RNN，加上比较强大的深度强化学习算法， 你就可能拥有这种能力。 具体怎么做呢？ 首先， 我们知道一个强化学习任务里， 你需要根据当下的状态做决策，来最大化最后的奖励， 这个东西我们通常称之为策略 ，数学上看就是根据每个状态， 采取一定决策的条件概率函数。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;d6ptk-0-0&quot;&gt;对于传统的强化学习算法， 我们通常需要动态优化等技巧来求解这个最优策略。 一个最典型的任务就是多臂赌博机。 这是一个由很多摇臂组成的赌博机，   每个臂都按照一定概率给你带来一个回报 ， 然后你要在有限的次数里最大化你的利益。  这个问题浓缩了强化学习的核心矛盾 -  探索与发现，假定你能够绝对准确的测量中奖概率， 那么你无非选择概率最高的那个臂就够了， 但是你要懂得绝对准确的测量概率需要无限长的时间， 而你的时间是有限的， 这时候， 你就需要在探索（测量）与发现之间做出权衡，摇臂赌博机涵盖了有限生命和未知世界的永恒矛盾， 因而也在生活或者广告电商应用里无处不见其踪影。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.6316666666666667&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkE0YyoA4lE9rAY3XWGz7luAtTezXibqjDvDVmbqicdv1wOLRV5ibTnAIZOg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;af696-0-0&quot;&gt;对于这个问题的求解有很多经典算法， 一大类经典算法称为托马斯采样，  这个方法的本质就是求解最大后验概率， 你用一个β分布作为先验，然后根据每一次实验（摇臂）的结构来更新这个概率， 比如一次结果为空， 就会按照贝叶斯后验概率公式轻微下降这个选项的概率而上升其它，最终求得结果。 托马斯采样是一种保守的，将探索和利用混合在一起的策略，  也能够取得相当不错的成绩。    &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;d12g2-0-0&quot;&gt;另外一个常用的方法称为upper confidence bound（UCB）,  可以翻译成最大信心策略吧。 这个策略与上面的托马斯采样相反，是一种激进乐观主义的策略，  它的信条是在那片黑暗的未知里隐藏着好事情。  具体的公式类似于当你在当下对某个摇臂积累了一定的经验， 比如N次尝试有m次奖励，此时的经验概率为m/N，  你知道这个测量由于数据不足有不准确性， 假定这个奖励符合高斯分布， 那么你就可以根据你的置信区间（自己定的）得到真实可能的概率的上界（乐观因此取上界）， 这样你就得到你的决策根据。 这个策略会偏向于探索未知， 因为你对某个摇臂尝试的次数越少，你的高斯分布就越宽（不确定）， 你的上确界距离你的&lt;/span&gt;实验概率就越远。  &lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;6c1jj-0-0&quot;&gt;好了， 我们来看看现在的任务， 我们生成一些的二摇臂（两个摇臂）的任务， 每个任务里的摇臂的得奖概率都是随机制定的0到1间的数。 有多少个这样的任务呢？ 两万个， 相反的， 我们对于这些任务不适用那些量身定制的传统算法，  而是使用一个LSTM加上Actor Critic 方法训练， 这是一种当下流行的， 同时进行策略估值和生成的方法， 我们把这个游戏里每一步骤的奖励和决策作为输入给网络 ， 然后让网络去最优化总奖励数。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;c1snv-0-0&quot;&gt;这样训练完之后， 我们再来一组（300）个这样的任务， 没一个二摇臂任务由不同的概率组成， 不同的是，这组任务里我们固定网络参数， 不允许网络在梯度下降。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;beb0o-0-0&quot;&gt;如果按照传统方法的思维， 这时候的表现应该很差， 因为每个任务里概率都不一样， 需要重新通过刚刚讲过的方法学习得到， 你不允许网络学习， 它也就无所适从了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dgbl5-0-0&quot;&gt;然而这个时候我们发现，  LSTM仿佛自己得到了学习的真谛，它在任务的开始， 逐步探索积累经验， 然后通过学习得到的概率，很好的完成了任务， 就跟那些传统的学习算法结果一样好！    &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5633333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEjHKMicNK59QRF47CFCNhfibjcZVdgdyQl7vrTgFDejtNdQDPrtV10pZQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;8sua3-0-0&quot;&gt;不仅如此， 我们再进一步， 让这个游戏便的更复杂，我们刚刚假设摇臂是独立的， 真实世界的选项之间往往由相关性， 比如A的好往往暗示了B的坏， 宇宙能量守恒吗。  再摇臂之间的概率由相关性的时候又会怎样呢？      比如最简单的相关性 p1 +p2 = 1.  最后的结果就是相当的好！  再经过一段时间的训练后，网路无需学习， 就可以利用这种概率间的相关性规律， 来加速探索取得更好的成绩！  这也就是所谓的RNN可以自发学习掌握任务里的先验规律的意思。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;8sua3-0-0&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;9jpj4-0-0&quot;&gt;然后还有更有意思的 ， 我们可以设计一个更加奇妙古怪的相关性， 比如：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;b5sci-0-0&quot;&gt;我们又12个摇臂， 其中的某一个摇臂叫做信息摇臂，它可以指示其它的摇臂的奖励值，仿佛一个先知， 然而先知本身不给你奖励，只是告诉你哪一个摇臂的奖励值可能更高， 而且发现先知的过程要耗费时间的。  训练后的RNN网络仿佛理解了这个游戏的含义 - 通过寻找提示信息来学习。   在全新的任务里， RNN可以不再经过任何梯度下降， 主动的寻找信息摇臂， 并利用摇臂指示的信息寻找最高奖励。  这个问题里， RNN仿佛理解了探索与利用的矛盾， 并在全新的网络里， 利用自己的动力系统去实现这个过程。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;dkk1t-0-0&quot;&gt;更上一层楼， 我们让这回连固定的概率都不要， 让每个摇臂的概率值随时间变化，唯一固定的是有的时候这个概率变化的速度快， 有的时候这个变化的概率小，  这个时候， RNN训练后的网络竟然仿佛能够在任何新来到的时候， 具有一种可调节学习速度的能力， 在那些概率经常变化的时间段让自己的学习速度也加快。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5633333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEEx9Y6LNrAAewzPhvaNfFt8wFjkhcicib51umtBLSSFcVOgpzaIxT7cfg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;除了K摇臂这个实验 ， 论文还尝试了经典的马尔科夫决策问题，  这个任务旨在通过一个简单的实验， 证实一个概念，   那就是刚刚的RNN 加上深度强化学习方法， 具备某种“构造世界模型” 的能力，  所谓构建世界模型，就是通过主动的了解和预测外界环境的变化来加强决策（对未知事物做出计划）。 而非光通过经验积累。  这也是区分人类高级学习和低级学习的关键所在，我们饱读诗书，都是在建立世界模型， 让我们对于毫无经历的事物也能够决策。&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5633333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEIiaVibPU73DicUURCAE34s5pjvTd4j6DmVynLEeCXaCQbGmIEo8SItkNQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span data-offset-key=&quot;dr5de-0-0&quot;&gt;但是这个能力以往需要通过工程方法加入到模型， 也就是人类自己先了解了模型， 然后通过程序嵌入进去。  &lt;/span&gt;而在刚刚的框架下， 这种通过构建世界模型的学习能力似乎也能够被学到。  上面的例子里，有一个状态S1 ， 状态S1 可以通过行为a1 或者a2  得到新的状态S2 或S3，  在此之上根据S2或S3得到一个奖励0或1， 这个奖励也是随时间变化的随机变量。 当然任何一种行为都可以得到后面的两个状态只是概率不同， 就像经典的马尔科夫决策问题一样， 这可以被一个转移概率矩阵描述， 而这个矩阵对于行为者是未知的。 由于这个转移概率包含了世界在我的行为下会如何变化， 它就是我们所说的世界模型。 如果学习者主动掌握这个模型来决策， 就好比它学会了有模型学习。&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;59tjr-0-0&quot;&gt;我们可以通过学习者的行为判断它是否试图建立这个模型， 如果行为者做出a1 ，并且最终达到S3得到奖励1，  那么它是应该更加鼓励行为a1还是行为a2 呢？  一朝中大彩， 十年买彩票， 不考虑任何模型的学习者所依靠的就是纯经验， 那么这个好经历无疑会让他更倾向于在S1下进行a1的决策，  而有模型的决策者就不然了，因为它知道这个结果是在一个小概率事件（从a2到S3是一个25%的小概率事件）  ，  同时通过“深思世界的本质” 他知道， 其实这真正说明的是S3是一个好状态， 更加容易达到S3的路径是a1而不是a2， 那么反过来我应该增大a2决策的权值而不是a1 ！   在这个事件下， 通过经验学习， 和模型结合经验学习就天壤之别了。  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span data-offset-key=&quot;8igi7-0-0&quot;&gt;后面论文还举了一些比较复杂的任务，其中一个比较有趣的是导航。  这个导航的任务， 是让行为单元走迷宫，这个迷宫的某个角落藏着宝物， 然而隔一段事件宝物的位置就会被移动到另一个位置， 而更加残酷的是， 每次行为者发现宝物后就会被转移到另一个随机的地点。 这个任务里， 行为者的输入只有它眼前的迷宫景物和它本身的行动速度，以及是否得到了奖励， 这样的情景很像我们的3D射击游戏， 同样的RNN 框架下， 通过不停游戏的初始训练阶段，游戏者仿佛掌握了一种进行空间行走规划的能力。在测试阶段，无需重新训练调整， 行为者也可以去发现目标宝藏， 并且在一次发现之后，无论它被重新扔到哪个未知角落， 都已十分有效的方法返回回去。这里的世界模型， 事实上是这个环境的地图和宝藏的坐标， RNN在某种程度掌握了这个概念， 因此， 它会在全新的位置也能够做出类似于合理的行为， 这一点， 和人是类似的。    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;Image FocusPlugin--unfocused Image--isBlock&quot; data-ratio=&quot;0.5633333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEmPhjuKavicy2XQ8nGvrn7Cyj8pmDYrqibj4vPRZV0o8CfwSq6d6VHkBA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; /&gt;&lt;/p&gt;
&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-ratio=&quot;0.10966542750929369&quot; data-w=&quot;538&quot; class=&quot;&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span data-offset-key=&quot;8h5th-0-0&quot;&gt;广告时间：&lt;/span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383610&amp;amp;idx=1&amp;amp;sn=eae53f91ea3bdb1d99464d3824175707&amp;amp;chksm=84f3c97bb384406dd3942d73be8d1dbe5a16815743990686d9054e1a3e9fa8fc42c8519ba270&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;铁哥开设的一个为期两日（12小时）的强化学习特训班&lt;/a&gt;&lt;span&gt;。&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;为什么是强化学习？  请看下图的技术泡沫爆裂图。  机器学习和深度学习在2017处于关注热度的顶峰， 大家看处在上升期的人工智能技术， 第一当属深度强化学习， 据这张非常表格非常粗糙的估计， 深度强化学习的技术成熟期在未来5-10年， 此时此刻， 正类似于深度学习在2010的状况。  &lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.67&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfqJgX6L51kcnJ07DHpdBzqmXKic2iaDgZOpUiacyYFWzBHst0BCZVTJINPzpfRibpMV6hWgUKSlj7ibBw/640?wx_fmt=jpeg&quot; /&gt;&lt;/p&gt;

&lt;br /&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-ratio=&quot;0.10966542750929369&quot; data-w=&quot;538&quot; class=&quot;&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; /&gt;&lt;img data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;5.896&quot; data-w=&quot;750&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJNQDiajMVL3arkx9IJnm10baZ1RjdLTN2KH6SKHZqnzyGO5K0G3dNOwg/640?wx_fmt=jpeg&quot; /&gt;
</description>
<pubDate>Sun, 23 Sep 2018 18:44:40 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/Zu8xwvhsNT</dc:identifier>
</item>
<item>
<title>[原创]强化学习最小手册</title>
<link>http://www.jintiankansha.me/t/w9o1udg7Ui</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/w9o1udg7Ui</guid>
<description>&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;img data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;0.665625&quot; data-w=&quot;640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibcd9g9aFjBooVn5U4PP1EDF3ugVJrLlia2ELtxHbXUJs7SUPtRaxmkUBHhx3jLciaHXpx1ABVYwYBu9w/640?wx_fmt=jpeg&quot; /&gt;&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;span&gt;作者许铁，微信号：ironcruiser &lt;/span&gt;&lt;br /&gt;&lt;span&gt;法国&lt;/span&gt;&lt;strong&gt;巴黎高师&lt;/strong&gt;&lt;span&gt;物理硕士 ，&lt;/span&gt;&lt;strong&gt;以色列理工大学&lt;/strong&gt;&lt;span&gt;（以色列85%科技创业人才的摇篮, 计算机科学享誉全球）计算神经科学博士，巡洋舰科技有限公司创始人,   《机器学习与复杂系统》纸质书作者。曾在香港浸会大学非线性科学中心工作一年 ，万门童校长好战友。&lt;br /&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-ratio=&quot;0.10966542750929369&quot; data-w=&quot;538&quot; class=&quot;&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; /&gt;&lt;/pre&gt;
&lt;pre contenteditable-directive=&quot;&quot; mm-paste=&quot;&quot; ng-blur=&quot;editAreaBlur($event)&quot; ng-model=&quot;editAreaCtn&quot; ng-click=&quot;editAreaClick($event)&quot; ng-keyup=&quot;editAreaKeyup($event)&quot; ng-keydown=&quot;editAreaKeydown($event)&quot;&gt;
&lt;br /&gt;&lt;/pre&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkE1ckKibaar2bTdxLtaicqVBEXSGvAb10W7tbXkPowUKteMDHfIvdMxicjg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; width=&quot;640&quot; /&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;强化学习越来越受重视，它为什么很重要，用一张图就够了。想像一下我们是人工智能体，我们有强化学习和监督学习可以选择，但干的事情是不一样的。面对一只老虎的时候，如果只有监督学习就会反映出老虎两个字，但如果有强化学习就可以决定逃跑还是战斗，哪一个重要是非常明显的，因为在老虎面前你知道这是老虎是没有意义的，需要决定是不是要逃跑，所以要靠强化学习来决定你的行为。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;强化学习有哪些实打实的应用呢？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;只要在问题里包含了动态的决策与控制， 都可以用到强化学习&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;1， 制造业&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;强化学习之于制药业有一种天然的契合 ， 把强化学习翻个牌子换个叫法， 也可以叫做控制论， 学习控制机器手的精确动作， 比如让它自动的做比目前所能及的更复杂的事情， 强化学习在制造业的应用潜力是显然的 。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.6608946608946609&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEPPbIfYFaGoX3ylAox1oc8k1icMPuViaxTr6sX7ZQMRYpr5Qqa4g73G9g/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;693&quot; width=&quot;693&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;2， &lt;span&gt;无人驾驶&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这就不用多说了， 开车本质是个控制问题, 　自动驾驶不仅需要模拟人类行为，　还需要对前所未遇的情况进行决策，　这需要强化学习。　&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkE1c5ic9cQAN8gRKeh7QRFZR4CUy6adse9x5YozN999AKlXz1yh4ocjwQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;744&quot; /&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;3, &lt;span&gt;智能交通&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;智能交通， 显然这里包含了非常多的决策与控制问题， 就拿目前的共享汽车行业 ，滴滴和uber的派单系统时时都是一个动态的决策， 如何把正确的司机和乘客连接在一起， 如何让车辆调动到需求量最大的地方， 这些都要时时的考虑各种因素调整决策。 我们说这里面既包含了效率的问题， 也包含了乘客的安全。 比如这一次滴滴的事故如果修正强化学习的效用函数， 是有可能避免的。 当然除了派单和调动问题， 在每个十字路口交通灯的控制等， 整个城市里的立体交通网络的协调， 本质都是强化学习问题， 所以强化学习在智能交通大有可为 。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5840840840840841&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkERRP9rENk9ibS9x8S2H6ibKh94pau9I4nHc9WJJ8NDjvCFiaSB63lt7ArQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;666&quot; width=&quot;666&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;4，&lt;span&gt;金融&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  金融的核心， 交易， 是一个动态控制问题， 即使你不能完全预测明天股市的涨跌， 你依然需要直到我今天要不要下单，下多少单， 这，就是一个强化学习的决策， 它可以影响明天的股市， 也会在非常长远的时间里让我收益或亏损。机器交易，本质是个强化学习问题。 当然，金融里能够应用强化学习的绝不仅仅这一个。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.425&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEFQMZFOlDE7tADqGaEaUksjWWzpWl6hgPJlIXxzx00Pkebl2tIGaqrw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1140&quot; /&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;5, 智能客服&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;智能客服本质是个强化学习问题， 如果你把它处理成监督学习问题， 那个对话机器人只能照猫画虎， 不能够真正从顾客的好恶的角度出发来发言， 而如果用强化学习， 那么机器人学习的就是如何正确的决策， 每句话都是为了最终讨得顾客欢心。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEWJhXH4PMibyjIlVPBQ6K6FNdjofKHSEI99qNBiaVSbzWUngZDibhAc9LQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1047&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;6， 电商&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;电商的本事是如何吸引人买更多的东西， 因此我们买了一个东西后它总会在下面给我们推荐其它的东西。 然后我们看到了一个新的东西， 又会点开下一个连接， 这样一步步的就买了一大堆东西， 这样在每一步给你展示不同东西吸引你上钩的过程， 也可以看作是电商系统的动态决策过程， 是一个强化学习问题。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5671875&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEliaul1IicIAzicEuMFicibeic5mx2EXMd6zo2ZmKHg1xbibSlwP7ZMscB5Uow/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; width=&quot;640&quot; /&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;7， 艺术创作&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;艺术创作领域看起来与强化学习无关， 事实上它可以很灵活的把人类的好恶加在强化学习的过程里，通过强化学习， 机器作曲可以自发的得到取悦于人的风格，也就是范式。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.4777777777777778&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEictGRHAWicDXwRnVkUNtyXdz7ibEGiaegnF4ysFNhyAzvEmnFCz02OPtJg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;986&quot; /&gt;


&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;强化学习的基本要素&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;如果说监督学习的基本框架是函数拟合， 我们常用的语言是特征， 标签， 那么强化学习， 就有另外一套语言， 这套语言的元素包括&lt;span&gt;状态， 行为， 观测， 奖励&lt;/span&gt;。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;每一个元素都有很多可以说的地方， 首先看&lt;span&gt;状态&lt;/span&gt;s， 状态是什么呢？ 它指的是智能体（agent）所在的环境里所有和游戏有关的信息， 它起到的角色类似于监督学习里的特征。既然如此，状态的数学表示你可以认为和特征是类似的， 也是类似于一种函数向量的形式， 我们说状态空间， 正如监督学习的特征空间， 是一个维度很高的几何空间。 状态特征有连续和离散之分 ，会影响学习算法的基本性质。 我们可以思考走迷宫和平衡摆的例子来思考状态的可能表示有哪些 。在走迷宫的例子里， agent所在的位置作为一种状态， 而在平衡摆的问题里， 这个状态就是角度。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;再看&lt;span&gt;观测&lt;/span&gt;， 观测是学习体可以接收到的状态有关的信息， 有的时候行为体可以收到环境的全部信息， 也就是整个状态， 但是大部分时候则只能收到非常局部的信息。观察可以看作状态的一个函数O（s）， 这个O（s），正是对应真正的系统输入， 会决定下一步的行为A（action）， 为了描述简单，我们不必区分状态和观测。&lt;br /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;再来看行为，所谓行为，是指智能体的&lt;span&gt;决策&lt;/span&gt;，某种情况下我们可以认为它就是监督学习要求的那个y， 或者预测， 但一个决策与预测不同的是，我们并不能马上取得一个信号告诉我们这个决策对不对， 只有在游戏的最后 ，我们才能从整个游戏的收益反观当时的决策好坏。 另一点是， 它可以间接的影响状态， 环境等因素， 因此， 比起深度学习里的预测更具有“反身性”， 也就是说这个y会影响下一步的x。 这种输入到输出的闭环形式也是强化学习和监督学习的主要区别之一。　&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;决策的数学形式类似于监督学习的分类问题， 每个行为类似监督学习里的类， 这也是容易理解的， 虽然状态可以取无限的数值 ，但我们的行为决策事实上往往只可以取有限的几个值， 比如在走迷宫的例子里它就是上下左右， 东南西北。 因此， 行为的表示往往是有限的离散的数值， 通过类似字典的东西赋值。 决策是一个随机变量， 因为在某个状态下的决策， 很多时候包含随机性， 这个随机性使得这个决策函数变成了条件概率的形式P（a|s）， 通常我们给它换个名字叫做pi。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;最后看&lt;span&gt;奖励&lt;/span&gt;， 奖励是什么呢？ 奖励就是在某个行为之下环境给我们返还的一个反馈信号， 这个东西正是智能体在游戏里所追求的 ， 奖励和状态有关， 也和决策有关， 比如多臂赌博机问提， 你的奖励直接就是你的行为（选择摇臂）的一个结果。奖励具有随机性， 同样的条件性， 有的时候我们可以得到奖励， 有时候没有， 因此， 它也是一个随机变量， 理解这一点非常重要， 因此才可以理解很多的后面的算法。 奖励可以是正向的，也可以是负向的（惩罚）。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.8597222222222223&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEjFBzNFcGulOr8XicbMINSzX2wtfdicyia9EQTbLasqdBicriaUg2uTtmNtA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;802&quot; /&gt;&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.6458333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEm8UL7DM70dJIia0kD2puTJOD9RicQH3U5JyKlNYW19087iaE6XUFib1OoA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;955&quot; /&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这样我们就有state（observation）- action - reward 这样的一个组合。 或者说环境给你一个state， 然后智能体得到一个action ， 这个action改变环境， 并且环境返回智能体一个reward，如此循环， 当然在真实的游戏下我们并没有这样机械的一步步的过程， 而是一个连续的整体， 这种机械的方法是为了让问题可以轻松的被一个程序解决。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这样的思路和图灵最早提出的图灵机智能模型具有异曲同工之妙， 而图灵机被认为是智能产生的基本模型，因此你也可以理解为什么强化学习和强人工智能有关。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;从状态到行为action的函数，也就是刚刚提到的那个条件概率， 通常称之为&lt;span&gt;策略&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  ， 犹如通常意义上说的战略， 也就是一个行为的指导方案。 当游戏结束的时候， 我们把所有环境给我们的奖励加在一起算分， 越好的策略得到的分数越高， 这就是强化学习的本质。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;马尔科夫决策与动态规划&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;那么如何得到一个好的策略呢？ 这就是强化学习的中心问题， 大家以看就知道这本质上还是一个优化问题。 那么整个后面的篇章都围绕这个展开。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;如何得到好的策略？如果游戏没有很多步，而是一步就可以拿到奖励，　那么我们只需要写一个函数作为总奖励，　这个函数里自然包含策略函数，然后直接对它取最大即可。　当然，　如果奖励的概率函数未知，　那么就会引发一个探索与收益的问题。此处不再详述。　&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们还是展开简化大法， 我们用一个马尔科夫决策的东西， 把这个东西大大的简化。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;什么是马尔科夫决策？ 你观察到我上面的描述没有，这里面包含的东西大部分是离散的，问题里的大部分元素， 都可以描绘成条件概率的形式。 如果我们假定从state 到 action的那个条件概率P（a|s）， 从这一刻state到下一刻state的那个状态是P（s|s，a） 从环境里得到奖励的那个条件概率是P（r|s，a）。 在这个语言系统下，马尔科夫决策就是假定P（s|a,s）与P（r|a,s） 都具有这一步的状态和决策有关，如果是这样， 我们的决策函数P（a|s）也仅仅需要考虑当下的状态， 这使得所有问题的数学形式大大简化。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.5046296296296297&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEZGGlSgCYDsFVhibib6xtS8UIOzxaYJnssDGgepHfwvZo5As8ia9eoQ1qg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;432&quot; width=&quot;432&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;环境决定的那两个条件概率P（s|a，s）和P（r|a，s）往往不是已知的， 或者说极为的难以求解， 比如在围棋在某个棋局之下你的某一步走子所能引起的对方的变化会十分复杂你根本无法求解。而一旦它是已知的， 整个game将变得十分简单。 由于它们往往不知道到的， 如此才需要引入整个后面的强化学习体系。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;有了马尔科夫决策这个框架，我们就可以看看如何推得我的最优策略。 强化学习的优化问题， 本质围绕三大主线， 一个是你要考虑的是未来的收益， 而整个未来的收益是不确定的。 另一个是你要考虑探索和收益， 假定你在游戏里走是选择开一扇门有奖励你就没完没了的开一扇门， 这样就陷入了局部收益最大的陷阱， 而忽略了全局更大的收益， 但是我们的游戏时间又有限， 不可能一直把时间花在探索， 这样就有一个探索和收益的平衡问题。第三是奖励是稀疏， 在游戏里往往是在很少的时刻会收到环境给的奖励， 你确要根据这个信号去学一个非常连续的动作控制流。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们先来看如果已知前面环境决定的两个条件概率如何求解策略。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;首先为了解决当下收益和未来收益的问题， 我们需要一个新的数学函数把当下和未来的收益统一起来，这就引出了另一个中心角色 - value 价值。 这个概念想办法把我们把当下和未来的收益统一在一起， 这个函数的定义方法是把当下的奖励和未来的奖励加在一起， 由于奖励本身就是随机变量且未来是不确定的， 我们要是把奖励都加在一起， 依然得到的是一个随机变量， 你要衡量一个随机变量的大小， 只能对它取期望。 令一点， 你是否觉得当下的奖励和十年后的奖励应该值一样的钱呢？ 你是否觉得如果把所有的奖励都加在一起， 如果游戏很长， 这个数字会趋于无限呢？ 如何解决这个问题？ 我们做一个贴现率， 让未来的收益乘上一个和时间有关的乘子， 这样最后把求和变成等比数列求和， 这个值就收敛了。 。 我们就得到了这个统一当下和未来收益的函数- value 。 它的定义就是现在和奖励和未来的奖励乘以时间的贴现加在一起的期望。 因此得到value的定义&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.39267015706806285&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkE2SCV9yyGicM8S8K5gQk6ov37o2S1FDHklNpsQbg2icKRQq7jmycPe3cg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;191&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这个期望依然是依赖于一定条件的，它依赖于当下的状态值， 或者说状态初值， 因此， 它具有条件期望的形式。 另一个关于值函数要知道的要点是再整个过程里唯一由智能体确定的是它的策略pi（a|s）， 其它都是环境给出的，不同的策略下， 值函数显然是不同的， 因此这个v（s）也是依赖于策略的， 通过我们pi写在v的右下角就是提醒大家注意这一点。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;好了， 现在我们的强化学习的优化问题就换了一种说法，我们要优化我们的策略， 最大化我们的这个value函数， 当然是所有状态下的value函数（是一个向量形式V（s））。 假定我们知道环境的动力学： P（s|a,s）和P（r|a,s）, 那么我们可以通过直接展开的方法求解这个value， 然后直接在这个基础上做优化。 加入我们的游戏只有一步：&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.21019108280254778&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEr5naDWibGfZMofiba5czyAElVf3VCHCrRib5WAhqvdrP1JXUcGPvLFjDA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;314&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;大家可以像对于一个动辄几百个步骤的游戏， 这样展开下去要多繁琐，我们有没有其它方法对付这个问题呢？ 这就引出了一个求解强化学习优化的最基本方程 - 贝曼方程，这个方程的内容说的是可以把值函数按照定义展开为当下的奖励， 和下一步的值函数乘上贴现因子的值的和。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.08012326656394453&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEubc5ticMJAxaAj6knC82GU4lKKC0kVf32OPTrRKUI89rFdT18e00dSQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;649&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;如果我们所针对的不是某一个特定的值函数， 而是刚讲到的那个针对所有初始状态的值函数向量， 且我们假定所有的状态只有有限个（在马氏问题里一定成立的）那么这个关系式的左边和右边就同时包含了这个值。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.2781065088757396&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEicHrcGBpZGqecWPvEWDcDrUF6uKMr4G04NqVdw5mZBgKnONSDHnPoTg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;169&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这样我们就得到了一个左右两边都包含v向量的方程， 由于两边的关于环境的两个条件概率都已知， 我们可以通过线性代数的方法求解。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;如果我们得到了v的值， 再来看看如果去改变我们的策略使得这个值最大。 我们还是把最优化方程展开为当下的奖励加上贴先后的下一步的值函数的形式:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.09422492401215805&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEiauoaLK9IxU5bibzORe009Qp0PIRnZHiaohczYeQFO7FSF76HlYkxkLdA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;658&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;为什么要做这样得分解？ 这里包含了贝尔曼优化的关键思想， 这个公式既包含当下的v值又包含下一刻的v值， 这意味着， 我们我们可以通过把整个优化过程， 展开为一个个的这样的环节， 如果每一步都是最优的， 那么全局就是最优的（通过迭代法传导）。 或者反过来想， 如果我要达到了最优，那么从任状态出发， 我都应该是最优的。 这就包含了所谓动态规划的思想。 也就是你要优化全体， 就要优化每个部分。 对应的方法， 就是一种迭代的方法， 我先来计算每个状态的v值， 然后直接在算式里改变pi的值来最大化v函数（求导！）， 当改变了pi值，新策略的v值也必然要重新估算，有了新的v的值我们又可以使之最大的pi值， 由此形成一个螺线结构， 直到收敛到最优解，在这个点上， 我们就达到了一种稳定， 此时无论如何改变pi都不能使v达到最大。&lt;br /&gt;&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEF22FsuqjgCmr47OIe97BM6NAYh5vIOB86wqouR29LGdQoibt3Gvs0fg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;991&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;蒙特卡洛抽样 与TD方法&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;但是大多数条件下， 刚刚说的那个假定是不成立的， 也就是环境的动力学未知， 我们没有那两个有关环境的条件概率， 这个时候我们根本没法按照刚刚的思路求解。 对于未知的环境，我们的解析解行不通， 我们唯一能够借助的手段就是抽样， 通过抽样的方法， 取代掉算式里未知的条件概率。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们来看这个情况下对v的估值和对策略的迭代是怎么样的。 先来看v， 这个时候的v与其看出成对未来收益的期望， 不如看包含了以往所有历史的信息的对未来期望的估计， 因为你的抽样只能是估算，严格说你需要针对每个状态抽取无限个抽样样本才能完全准确， 但是显然没人等的了， 那么我们能否针对有限的被抽样样本来更新v函数呢， 如何更新呢？ 思路有两种， 一个是按照定义来搞， 既然v值是从某个状态出发的总收益的期望， 那么我们就可以从某个状态出来把整个游戏走完来测量这个收益， 从被抽样的状态出来， 走完整个游戏， 最后算出某总奖励作为v值得抽样。由于数据是一个个增加的， 我们用流平均来替代期望，这里也是一样得。 这样的思路我们称为蒙特卡洛， 蒙特卡洛几乎就是随机抽样的代名词。&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.28160919540229884&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEyazt9LZPmhq2gIl2SgDWZvtnfQvMYarESk5cVoWfSl6jdcoEk9wlbA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;174&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;alpha 越大， 表明我们之前的样本越多。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;然而这个方法有着一个致命的弊病 ，那就是你得等到游戏的结束才能更新一次v值，速度太慢了，而且想想有些时候你只能进行一次或几次游戏， 比如人生的游戏你只有一次， 你不能死了再回来， 所以这个方法就不那么给力了。 怎么办呢？ 我们还是有办法，这个办法就是不等到游戏结束就更新， 还是利用刚刚说的v函数的迭代展开式， 我们可以在每次得到一个奖励的时候更新这个值， 你想想无论是在游戏里还是人生里其实你每次都到一次胜利或者失败， 你都会对全局的输赢多一点信息，如果你能利用好每次到达的这一点信息来更新你的v值， 就达到了我们不结束游戏就更新的目标。 这个方法所做的就是每次多一个奖励出来时候严格按照v的迭代式定义来更新v函数，这样多步之后v也会趋于正确的值。 这就是大名鼎鼎的TD方法。好比当你在开车的时候， 你险些撞车， 游戏没有终止， 但是足以让你使用这个惊险来更新值函数。 另外一个例子是你打公交车去上班， 每过一个站你看一个时间， 看和你的预计是否有差别， 这每一站的时间， 足以让你不停的调整对最终实现时间的预期。&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.13101604278074866&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEewr8GaLLz6umDw2J7DWhMxyZicwvwhBkSdIPK6SaibhNuEhiaiaQnQicpnw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;374&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;从另一个角度看它的本质， 这就是之前讲的动态规划的一个抽样版本，我们还是按照贝拉曼方程展开递归公式， 不过这个时候我们去掉了那些不好求的概率， 而是每次出来一个证据， 我就根据这个证据， 来调整我的v。 本质上， TD方法就是根据新增加得方法一点点得调整预期，最终使得我得估计准确得一个方法， 这点让我们想到贝叶斯方法。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;零一点关于TD的重要认识是， 它预设了一个状态中间的关联， 因为在每个TD的步骤里， 我们都假定了一个从st到st+1的马尔科夫结构，即t+1时候只和t相关， 所以TD方法在越接近马尔科夫决策结构的时候工作的效率就越高， 反之， 则引入一个偏差。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;如何优化策略&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们谈了通过抽样的方法更新值函数， 下一步就看看如何更新策略。 之前的动态规划思路是把全局分解为部分优化， 这里也一样， 每次更新v函数后，都意味着我们可以针对被更新的v值调整一次策略， 但是一个非常棘手的问题是哪几个环境的条件概率不知道，还有， 我们的v值是根据一两个新的数据估算的不准确， 这有两个直接的后果：&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;1， 我们无法简单的取最大值，&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;2， 即使我们能够取最大值， 我们也无法确定我们做的是正确的，因为我们不知道真实的v， 根据错误的局限的v做的策略极有可能是局部最优。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;对于解决1， 我们玩一个trick， 重新定义一个量&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEU3mtYq8tkZ5jetb8MLK8atniaBd1iazeczgKycxvvoLr1as6XACia2czQ/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;523&quot; data-cropy1=&quot;7&quot; data-cropy2=&quot;79&quot; data-ratio=&quot;0.13957934990439771&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEL0vIicBrZeTablLUomyln0K3YkwboLVNsyibrPj0rhPHYdenmQPgQ0kQ/640?wx_fmt=jpeg&quot; data-type=&quot;png&quot; data-w=&quot;523&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这个量把环境的那两个条件概率完美的包含进去， 如此，我们把刚刚说的TD和蒙特卡洛估计的对象就改成Q函数， 我们立马会看到刚刚得贝拉曼优化方程变成&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.13702623906705538&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEJDzc7iaOftqrgEhzTMgrKZwL8CX5hb7N6A4A7tA0epAfYc3sOGpqSNg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;343&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt; 这时候你用眼睛都可以看出这个东西的解吧， 我们的策略pi只要更新为取最大Q值得函数就可以了， 因为这正是这一关系所能取到得所有pi里最大的那个。 这样我们就解决了第一个问题在环境未知下求最大值&lt;br /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;Q 我们称为action value ， 是在控制问题里比v更好用的量， 因为它甩掉了那一堆概率关系， 直接了当的告诉你， 你要做好决策， 你就直接搞定那个action value最大的决策就行， 不需要一丝犹豫， 由于它直接忽略其它所有不是最大的选项， 因此给人一种十分贪婪的感觉， 因此它又叫贪婪算法。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;然后看第二我呢提， 如果没有第二个问题， 那么我们可以更新了策略之后， 进一步估算新的策略下我们的Q值（同v值更新方法一样），然后再根据贝拉曼方程告诉我们的， 不停重复这个过程，直到整个过程停下来， 我们就达到了最优解。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;同学们看看上面的逻辑是否天衣无缝呢 ? 如果没有第二点， 是对的， 但是由于那个刚刚说的那个问题，你对一个不准确的估计做贪婪运算， 极容易陷入局部极小， 我们机油可能仅仅是由于我们非常局限的经验而直接认定了一个选项，放弃了更有机会的选项， 比如你某次和某男生约会很高兴就马上放弃了其它所有男生。 由于视野很局限， 我们就容易落入局部最优解。 那么如何避免这个情况? 我们要加入这样一个因子， 它描述我的不确定性， 虽然我倾向于选择那个Q最大的动作来做， 我还是保持一个随机因子 epsilon。 这个值多大我自己定， 比如我如果选择epsilon 为0.5， 那么也就是有50%的概率我直接选最大的那个Q, 另外50% 随机选一个。 这个随机选项让我们有机会瞧瞧那些其它的选项， 也称为探索。 我们刚讲的探索与收益的矛盾， 正是这个直接选最大值， 与继续探索世界更新我的认知直接的矛盾。 如果我们拥有全部的世界的动力学知识，对世界了如执掌， 我们可以直接最大化， 由于那些是未知的， 而游戏世间是有限的， 就引出了这个探索与收益的矛盾， 也就是epsilon与greedy的矛盾， 如果epsilon设置的过小， 不足以解决局部最优的问题， 如果设计的过大， 则会使得策略的迭代过于缓慢。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;既然我们为了效率放弃了计算准确的Q， 我们也要把我们对世界估计的这种不准确以在决策中加入随机性的方式放回去。 这就是epsilon的本质， 它是伴随着对值函数的采样估计产生的。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这个被改进的算法被称为epsilon-greedy， 也就是结合了贪婪和探索的算法， 通常为了缓解探索和收益的矛盾， 我们会按照一定的进程表变化epsilon，开始比较多探索， 后期比较多贪婪， 这个也可想而知， 前期对世界的知识很少， 需要多探索来尝试所有的可能 ，后面反过来，游戏将要结束， 需要利用已经取得的知识最大化的得到成果，这样的一个过程我们通常称之为退火，就好像逐步降低温度一样。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;用这样的方法最终得到的最优解一定是一个确定性的解，当退火到0的时候， 我们得到的那个最优是不包含随机性的， 它的涵义就是搞定那个最大的action value 这类方法又称作值函数法， 就是说我最终决策的依据只有action value， 它是确定性的。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这里面有一个没有讲到的非常重要的细节， 刚刚讲到用TD方法来更新我的v值和Q值， 这个v和Q无疑都是针对我现阶段策略的， 一开始时候我说的策略是一个贪婪策略， 后面加入了epsilon， 我们说在智能体训练后的真实使用阶段我们是不含有这个epsilon的，那么这个评估时候针对的策略是要根据我真实使用中的那个贪婪策略， 还是根据我在训练中为了避免局部最优使用的这个epsilon-greedy呢？ 答案是这两个都可以. 如果死磕定义， 我当然应该使用epsilon-greedy，因为这才是我在训练中真实采用的策略。 但是你不要忘了， 我真正care的永远是那真正使用的策略， 也就是贪婪的策略， 这个epsilon只是在训练中为了纠错加入的。聪明的同学一定想到了我在评估的那个真正用的策略和我所用的含有探索的训练策略没必要一样， 这个思路就产生了off-line learning 的思想， 翻译出来叫做离线训练， 它说的就是我在训练中使用的策略和我真正学的那个策略可以不一样， 好比在评估阶段我处于离线阶段， 是通过一个与真实训练有区别的进程完成的。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;相反的， 如果两者是一致的， 我们称为online-learning。 这样我们的epsilon-greedy算法就衍生出了两个流派， 一个是online-leanring版本的sarsa（state-action-reward-....）， 另一个是offline版本的Q-learning。&lt;/p&gt;

&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.42916666666666664&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkE4M8o1agFAq4wc1h1YnJqIskF2sK9MiacrNpyIMbpKDsdD4YwBC17L5g/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;930&quot; /&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.4097222222222222&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkE9JBCibHR6124nTY4H1uy4dYib8ic4yf9lVl3W0s7PVYfm3DLM74akmWlQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1068&quot; /&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;BoostStrap 方法&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;刚刚讲到的TD更新方法， 是最常用的方法， 但基本款的TD方法还是略显缓慢， 有一种迅速加速这个方法的手段， 叫做TD-lambda。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们且一步步的展开这个问题： 1， TD方法每次得到一个环境给的奖励信号， 然后更新上一状态的Q值 2， 那么我们可不可以等两部，　得到两个奖励信号再更新一次， 这样是不是在更新的时候做到更准确． 3， 可不可以等到n步奖励的结果再更新，这样对每次更新更确定？ 4， 如果一直到游戏结束才更新我们是不是得到了蒙特卡洛方法？ 如此的确很减少了偏差，　但是同时降低了效率　5， 因此n步更新方法是1步TD方法和蒙特卡洛方法的中间状态&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;6， n该取多少，似乎比较小的ｎ和比较大的ｎ都各自有各自的道理 7 ，不如同时一起做了，　从一步更新到ｎ步更新， 此处引入TD lambda， 我们可以从1步到n步同时更新，然后用一个方法把它们混合起来　8， 能不能倒过来看， 在每一步更新的时候， 我们不仅更新这一步的Q值， 而是把状态轨迹上的所有之前的Q值都更新？ 这个方法可以证明就是第7步的TD lambda， 它的效率非常高， 因为它可以通过最后一步得到的奖励， 更新所有的中间状态Q值。 你可以看出来， 这个TD lambda， 更新Q值的效率还是很高的。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;把学习的思想引入Q-learning&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;有了上述这样的方法， 对于任意的问题，我们都可以通过先设定状态- 行为 -奖励， 然后给每个状态，行为的组合设定一个Q （a，s）， 再不停的通过TD方法和蒙特卡洛来优化控制流， 然而， 这样的方法确实有着天然的弊端， 一个最典型的问题是， 它本质是在做一张状态-动作组合的表格， 对应每个状态-动作的组合， 我要计算一个收益期望Q，如果状态和动作的组合是可数的， 那么这个问题是可以操作的， 因为这个时候我们要做的无非是一个个的迭代。 然而如果状态非常多， 这个问题就不再那么好做了 ，因为你总会在某个时刻， 到达一个先前从未见过的全新的状态， 这时候该怎么做呢？ 终于又到我们回忆监督学习的时候了， 我们说了监督学习的终极目标是通过训练数据最终在没见过的数据集上做预测， 我们通常称之为泛华能力。 你看这不正是我们需要的东西吗？ 我们说我们有无穷多的状态动作组合 ，当遇到一组新的组合， 我们不需要慌张， 而是由模型进行预测。 我们的模型是由那些已经由了数据的组合训练出来了， 通过模型所把握的规律， 来在未知的数据上做预测。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这一个思路进来，那么所有监督学习里的模型就都称为了强化学习里的模型， 从线性回归到神经网络， 从树模型到随机森林。 如果所使用的模型是深度学习模型， 我们就不由自主的开启了深度强化学习。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们先前的状态就如同监督学习里的特征， 而Q（s，a）就是我们学习预测的对象。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;那么随机梯度下降这类家常便饭的优化方法也很自然的引入进来。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;重新回顾一下， 这一段落得根基其实是泛化二字， 也就是说我们的模型其实能够捕捉某种状态之间的关联。 比如你客户以想象在导航走迷宫的任务里， 从相似的起点出发， 你追随目标的路径其实是相似的， 当然这是一种非常表象的泛华能力， 还有一些更深层的联系， 需要抽象出一些特征才能干掉， 这就是我们深度学习要登场的理由了。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;引入策略梯度算法&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;有了策略梯度算法是不是就足够了呢？ 这个Q学习的算法， 虽然体系滴水不漏，但是依然有一个比较严重的弊端， 那就是最终学到的策略只能确定性的， 根据定义 ，这个最优化的策略是每一次选取Q（s,a)里最大的那一个。 可是很多时候， 我们的最优化策略本身就需要包含随机性， 而且这个随机性的大小需要精确的描述， 而不是像epsilon-greedy那样通过随机抽样粗糙的给出。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们用要一个最微小的例子来说明，还是那个走方格的问题，骷髅就是有危险的意思，我们希望走到有奖励的地方。 我只做一个小的改动将使得之前问题面目全非，之前的马尔科夫决策附加的条件就是当下的状态含有用来决策的所有信息，方格问题里， 这个信息就是位置坐标。而如果我没有位置这个信息， 取之以感知信息， 比如我只能感知我所在方格的周围两个方格有什么（下图中的骷髅或金币）。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;注意如果我们处在下图灰色方格的区域（左右各一个），此时相邻的两个方格的情况是完全一致的（白色），也就是说我无法确定我是处于左边还是右边的灰色方格， 这导致无法决策正确的行为（左边和右边的正确决策是相反的！ 一个向左一个向右， 但是我无法确定是哪一个！）。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;如果此时引入一个随机性的策略， 这个问题影刃而解，我无非子啊左右两个灰色的格子里制定左右各50%的策略， 这时候总是最终客户以达到宝藏，就是时间可能稍微长一点。 这样的随机性的策略， 引入策略函数就可以可以很容易的学出来。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.2875&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEuNMe0pFV5WrRuKGzIZZqPc5jMOXDdL9xLU1icAlfT5YPytJRUPGeeibg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1208&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;另一个必须使用策略函数的理由还是当agent可以执行的动作很多的时候， 比如机械臂的运动， 它的控制事实上是在一系列连续变化的角度里选择， 这几乎就是无穷多的动作。 这个时候， 那个比较古老的epsilon-greedy就要退出了， 我们可以模仿Q学习里的思路，把整个策略概率函数， 用一个神经网络精确的表达出来， 这样即使可能的动作很多， 你也可以通过设逆境网络的泛化能力达到（也有其它模型， 最常见的就是神经网络）&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;这个策略概率函数，能够精确的表达即使是无穷个动作， 每个动作被执行的概率。 这样通过模型的泛华能力， 我们很容易对一个未知的状态动作组合被执行的概率做出预测。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;有了策略概率函数， 我们如何优化这个函数呢？ 注意，之前的值函数方法，事实上我们也有策略优化这一步， 只不过那里直接通过更新v函数， 然后取最大Q绕过了这个步骤。我现在要硬碰硬的干掉它怎么搞呢？ 当然是把公式全部展开然后求导 ， 然后梯度下降。 我们拿一个最简单的问题开刀， 就是一个一步的游戏（摇臂赌博机），而且奖励的值是确定的， 这个游戏的奖励的总值是&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.358974358974359&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEdibHjUKfV7PgvicUdAmYicNvjN0eOAibR9RDSbGmoT38GH4QlNk900vjmA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;195&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;，然后我们对E进行求导， &lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.25311203319502074&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEH7UAH69cbrHpZMM7f1xo0sEm7BzaNDiaQHBjKTiaVXwF08WoMtia5cvicQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;241&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;由于此时我的pi已经是含有w的函数， 因此求导变成这个德行， 对这个公式里的 &lt;img class=&quot;&quot; data-ratio=&quot;0.6833333333333333&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEHmdqRRb4hY5J63lhOYeTZ4l2I9EF3SQDjeet4MJL1TaU6InHqCSMNA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;60&quot; helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; /&gt; 进行求导是有技巧的， 我们对于类似问题最常用的问题就是把它变成&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEicEpwBHrTlYU0dgGWrich1WpLK7RLw9iawoyw54XjHRb5IuiaOcHS5eDEQ/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;513&quot; data-cropy1=&quot;2&quot; data-cropy2=&quot;39&quot; data-ratio=&quot;0.07212475633528265&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkELacddL5WUgSgXbHXT5JcMNsPf8ibmExD7cyUEC3PQ0rhFNtgaLeNwZQ/640?wx_fmt=jpeg&quot; data-type=&quot;png&quot; data-w=&quot;513&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt; , 为什么要做这样一个变化呢? 原因在于， 这样可以保持我的期望的形式不变， 因为我会多出一个pi来， 这时候， 我事实上得到的是 &lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkExfsg9zibc9o8DwiaO7EzoVEPth0picDoZJfKfIBSe97bG1XDDj7YxparA/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;317&quot; data-cropy1=&quot;4&quot; data-cropy2=&quot;36&quot; data-ratio=&quot;0.10094637223974763&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkE9h6PjpzpGgVbgicjUJxQdtImruuxwSPVU5def3yKpunBduQjlne1dmQ/640?wx_fmt=jpeg&quot; data-type=&quot;png&quot; data-w=&quot;317&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  ， 这里面的&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEnYUlmLZ16LcriaOHcqmCdK4ja47RXNMiaDyqyr6g2OdnWkDwqElmdZ1g/0?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;97&quot; data-cropy1=&quot;7&quot; data-cropy2=&quot;41&quot; data-ratio=&quot;0.35051546391752575&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkER0b2ojUibNAFAdV6ogqmAzOxVKCA8rFejYHnr3Pa3Bn8icou0t34c5Pg/640?wx_fmt=jpeg&quot; data-type=&quot;png&quot; data-w=&quot;97&quot; helvetica=&quot;&quot; neue=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; /&gt; （称之为score function，类比机器学习最大似然函数的梯度）是一个相对容易计算的量， 原因它是在不改变原先条件概率函数下的期望形式！ 我们只需要按照之前对Q函数取样的思维， 直接在游戏的进程中对它进行抽样即可， 然后进行随机梯度上升。&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;刚刚讲述了摇臂赌博机这类只有一步的问题。 如果是多步游戏呢？ 这时候我们要改变我们的目标函数， 用q替代刚刚的r, 由于策略梯度完美保持所有其它的概率表达式， 因此我们得到的表达式依然保持期望形式不变 &lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.09705882352941177&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEh3QMCKBRxstenLAhNic1FD6TUcPGr2xyxXNn6dpiaKjMr7Ef8PbUtOhw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;340&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;。&lt;br /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;有了上面的基础， 你知不知道如何取干掉这个Q呢？ 有几种方法？ 蒙特卡洛和TD方法，以及TD方法的各种booststrap形式。 采取蒙特卡洛方法在游戏结束时候计算策略梯度的方法通常称作reinforce， 是最基本的方法。 如果采用td方法呢？ 这时候我们就来到了当下的今日之星 - actor-critic。&lt;br /&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;Actor - Critic 算法&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;Actor - Critic 方法在整个强化学习优化算法的位置里极为特殊的位置， 因为它综合了上述两个方法的优势， 我们在游戏里同时把Q和策略pi用神经网络表达， 在得到新的证据的时候两个网络都会反传一个梯度，策略梯度用来寻找更多的奖励，用于策略网络直接和行为相关， 因此这个神经网络被称为actor 而Q的梯度用来更加准确的对期望收益估值， 由于这种评估的本质， 这个Q网络又被称为批评者critic 。&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们可以通过一段代码理解这个框架．　&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span class=&quot;invisible&quot;&gt;https://&lt;/span&gt;&lt;span class=&quot;visible&quot;&gt;github.com/pytorch/exam&lt;/span&gt;&lt;span class=&quot;invisible&quot;&gt;ples/blob/master/reinforcement_learning/actor_critic.py&lt;/span&gt;&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.37777777777777777&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEHwW2qE1322CTJhG27xYaRTVludW89OdkSrQFDJ8ZuibjnZ37StGfX0Q/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;1341&quot; /&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;&lt;span&gt;有模型学习&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;刚刚讲的方法，通常称为免模型学习，所谓免模型，　就是当环境的动力学（也就是那两个条件概率，想象下棋的例子）不知道的时候，　我们通过直接抽样的方法来更新Ｑ（ａ，ｓ）和ｐｉ来进行控制住．我们知道当环境的动力学规律完全已知，　我们根本不需要抽样，　而是可以效率非常高的使用动态规划求解．在此处，我们可可不可以学习创造一个世界模型，　来提高我们抽样学习的效率呢？　当然可以，　你不是由监督学习吗？ 我们可以用类似监督学习的思路来把这个环境的动力学学出来啊！&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;我们在这里就进入了有模型学习的范畴．　一旦ａｇｅｎｔ开始学习掌握世界模型，并用它影响决策，　这个时候我们就是说ａｇｅｎｔ获得了一个全新的能力planning, 　这个能力的获取使得agent 使用环境奖励的效率大大提升．　&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;何谓planning, agent 试图在自己的心理展开世界模型，估计当它做出如何如何行为，会得到如何的奖励，　虽然agent 并没有真正经历那些行为，　就好像经历过了一样，　这样agent 就如同获得了非常多的虚拟数据，　可以更准确的对未知的状态进行估值，　在数据极为稀缺高维诅咒极为明显的强化学习问题里，　这个效果是巨大的．&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; yahei=&quot;&quot; han=&quot;&quot; sans=&quot;&quot; cjk=&quot;&quot; micro=&quot;&quot; hei=&quot;&quot; sans-serif=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot;&gt;那么这个有模型学习的范式就变成了我们先利用真实的数据来学习环境的动力学矩阵 P（st+1|st, a）和P（r|st，a），然后用这两个矩阵来进行simulation， 得到很多的模拟数据计算Q（s，a）的过程。&lt;/p&gt;
&lt;img class=&quot;origin_image zh-lightbox-thumb lazy&quot; data-ratio=&quot;0.7444444444444445&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcenH9WLvLsrBZukiafEJwhkEI3hzBVsNGLo5bKKhJJQCwQGQzlYq8z4X98EQnCcZ2PA5O8g0QZC0yA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;720&quot; width=&quot;850&quot; /&gt;&lt;br /&gt;&lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-ratio=&quot;0.10966542750929369&quot; data-w=&quot;538&quot; class=&quot;&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; /&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span data-offset-key=&quot;8h5th-0-0&quot;&gt;广告时间：&lt;/span&gt;&lt;a href=&quot;http://mp.weixin.qq.com/s?__biz=MzA3MzQwNzI3OA==&amp;amp;mid=2651383610&amp;amp;idx=1&amp;amp;sn=eae53f91ea3bdb1d99464d3824175707&amp;amp;chksm=84f3c97bb384406dd3942d73be8d1dbe5a16815743990686d9054e1a3e9fa8fc42c8519ba270&amp;amp;scene=21#wechat_redirect&quot; target=&quot;_blank&quot;&gt;&lt;span&gt;铁哥开设的一个为期两日（12小时）的强化学习特训班&lt;/span&gt;&lt;/a&gt;&lt;span&gt;。&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot; sc=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; rgb=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;为什么是强化学习？  请看下图的技术泡沫爆裂图。  机器学习和深度学习在2017处于关注热度的顶峰， 大家看处在上升期的人工智能技术， 第一当属深度强化学习， 据这张非常表格非常粗糙的估计， 深度强化学习的技术成熟期在未来5-10年， 此时此刻， 正类似于深度学习在2010的状况。  &lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.67&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibcfqJgX6L51kcnJ07DHpdBzqmXKic2iaDgZOpUiacyYFWzBHst0BCZVTJINPzpfRibpMV6hWgUKSlj7ibBw/640?wx_fmt=jpeg&quot; /&gt;&lt;/p&gt;
&lt;br /&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-ratio=&quot;0.10966542750929369&quot; data-w=&quot;538&quot; class=&quot;&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz/dcEP2tDMibccdYT8ViaXic1q1ibC3U0Ub0WhaaX0dxl5oRO3YicRx7fSozVkP7Z5UfiaQdwyaxxEM5AZaMAGHjY4yS4Q/640?wx_fmt=jpeg&quot; /&gt;&lt;img data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/dcEP2tDMibccvEGHcvx6vn7ibqucwWjTLJNQDiajMVL3arkx9IJnm10baZ1RjdLTN2KH6SKHZqnzyGO5K0G3dNOwg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; class=&quot;&quot; data-ratio=&quot;5.896&quot; data-w=&quot;750&quot; /&gt;&lt;br /&gt;</description>
<pubDate>Sat, 22 Sep 2018 17:40:14 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/w9o1udg7Ui</dc:identifier>
</item>
</channel>
</rss>