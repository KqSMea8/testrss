<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed.cnblogs.com%2Fblog%2Fsitehome%2Frss&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://feed.cnblogs.com/blog/sitehome/rss" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed.cnblogs.com%252Fblog%252Fsitehome%252Frss%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed.cnblogs.com%252Fblog%252Fsitehome%252Frss%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>博客园_首页</title>
<link></link>
<description>代码改变世界</description>
<item>
<title>算法学习（一） - 谁将新樽辞旧月，今月曾经照古人</title>
<link>http://www.cnblogs.com/jiangwz/p/7765694.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/jiangwz/p/7765694.html</guid>
<description>&lt;h3&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;不论学习有多忙，也要抽空读点书。&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h3&gt;

&lt;h2&gt;&lt;span&gt;什么是算法？&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;有一个很著名的公式  “程序=数据结构+算法”。&lt;/p&gt;
&lt;p&gt;曾经跟朋友吃饭的时候我问他什么是算法，他说算法嘛，就是一套方法，需要的时候拿过来，套用就可以，我吐槽他，他说的是小学数学题的算法，不是编程的算法。&lt;/p&gt;
&lt;p&gt;算法，从字面意义上解释，就是用于计算的方法，通过该这种方法可以达到预期的计算结果。目前，被广泛认可的算法专业定义是：算法是模型分析的一组可行的，确定的，有穷的规则。通俗的说，算法也可以理解为一个解题步骤，有一些基本运算和规定的顺序构成。但是从计算机程序设计的角度看，算法由一系列求解问题的指令构成，能根据规范的输入，在有限的时间内获得有效的输出结果。算法代表了用系统的方法来描述解决问题的一种策略机制。&lt;/p&gt;
&lt;p&gt;完成同一件事的不同的算法完成的时间和占用的资源可能并不相同，这就牵扯到效率的问题。算法的基本任务是针对一个具体的问题，找到一个高效的处理方法，从而完成任务。而这就是我们的责任了。&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;算法的五个特征：&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;一个典型的算法一般都可以抽象出5个特征：&lt;/p&gt;
&lt;p&gt;有穷性：算法的指令或者步骤的执行次数和时间都是有限的。&lt;/p&gt;
&lt;p&gt;确切性：算法的指令或步骤都有明确的定义。&lt;/p&gt;
&lt;p&gt;输入：有相应的输入条件来刻画运算对象的初始情况。&lt;/p&gt;
&lt;p&gt;输出：一个算应有明确的结果输出。&lt;/p&gt;
&lt;p&gt;可行性：算法的执行步骤必须是可行的。&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;算法的分类：&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;根据应用分：&lt;/h3&gt;
&lt;p&gt;按照算法的应用领域，可以分为基本算法，数据结构相关算法，几何算法，图论算法，规划算法，数值分析算法，加密解密算法，排序算法，查找算法，并行算法，数值算法……&lt;/p&gt;
&lt;h3&gt;根据确定性分：&lt;/h3&gt;
&lt;p&gt;确定性算法：有限时间内完成，得到结果唯一。&lt;/p&gt;
&lt;p&gt;非确定性算法：有限时间内完成，得到结果不唯一，存在多值性。&lt;/p&gt;
&lt;h3&gt;根据算法的思路分：&lt;/h3&gt;
&lt;p&gt;递推算法，递归算法，穷举算法，贪婪算法，分治算法，动态规划算法，迭代算法等。&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;算法和公式的关系&lt;/span&gt;&lt;/h2&gt;
&lt;h4&gt;&lt;span&gt;算法&amp;gt;=公式&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;如果没有接触到编程，的确很容易将算法理解为数学公式。公式的确具备算法的特征，但是算法并不等于公式，公式是一种高度精简的算法，算法的形式可以比公式更复杂，解决的问题更加广泛。&lt;/p&gt;
&lt;p&gt;算法和程序的关系 程序也是算法的一种表现形式，也是一种工具&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;算法和数据结构的关系 &lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;数据结构是数据的组织形式，可以用来表现特定的对象数据。&lt;/p&gt;
&lt;p&gt;因为不同的数据结构所采用的处理方法不同，计算的复杂程度也不同，因此算法往往依赖于某种某种数据结构。数据结构是算法实现的基础。&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;算法的表示：&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;自然语言表示：&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;就是用我们的口头语言来表示算法，这样很多算法难以描述，不利于发展交流。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;&lt;span&gt;流程图表示：&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;一般有三种流程结构：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;顺序结构，分支结构，循环结构&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/1176316/201711/1176316-20171105135415529-884365767.png&quot; alt=&quot;&quot; width=&quot;442&quot; height=&quot;196&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/1176316/201711/1176316-20171105135442295-1842716550.png&quot; alt=&quot;&quot; width=&quot;456&quot; height=&quot;361&quot;/&gt;&lt;/p&gt;
&lt;h3&gt;N-S图表示：&lt;/h3&gt;
&lt;p&gt;NS图也叫作盒图或者CHAPIN图，是用于取代传统流程图的一种描述方式。 以 SP方法为基础，NS图仅含有下图4.61 的5种基本成分，它们分别表示SP方法的几种标准控制结构。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/1176316/201711/1176316-20171105135903279-632809424.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;h3&gt;伪代码表示：&lt;/h3&gt;
&lt;p&gt;伪代码并不是程序代码，伪代码介于自然语言和编程用语言之间，是将算法描述成类似编程语言的一种形式。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/1176316/201711/1176316-20171105140106607-1481925368.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;算法的性能评价&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;算法的效率作为判断算法优劣的标准。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;一个算法的优劣往往通过算法复杂度来衡量，算法复杂度包括&lt;strong&gt;时间复杂度&lt;/strong&gt;和&lt;strong&gt;空间复杂度&lt;/strong&gt;两个方面。其作用：时间复杂度是指执行算法所需要的计算工作量；而空间复杂度是指执行这个算法所需要的内存空间。（算法的复杂性体现在运行该算法时的计算机所需资源的多少上，计算机资源最重要的是时间和空间（即寄存器）资源，因此复杂度分为时间和空间复杂度）。&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;时间复杂度&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;即通常所说的算法执行所需要耗费的时间，时间越短，算法越好。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;计算方法&lt;/p&gt;
&lt;p&gt;      1.一般情况下，算法中基本操作重复执行的次数是问题规模n的某个函数，用T(n)表示，若有某个辅助函数f(n),使得当n趋近于无穷大时，T(n)/f(n)的极限值为不等于零的常数，则称f(n)是T(n)的同数量级函数。记作T(n)=O(f(n))，称O(f(n)) 为算法的渐进时间复杂度，简称时间复杂度。&lt;/p&gt;
&lt;p&gt;      分析：随着模块n的增大，算法执行的时间的增长率和 f(n) 的增长率成正比，所以 f(n) 越小，算法的时间复杂度越低，算法的效率越高。&lt;/p&gt;
&lt;p&gt;      2. 在计算时间复杂度的时候，先找出算法的基本操作，然后根据相应的各语句确定它的执行次数，再找出 T(n) 的同数量级（它的同数量级有以下：1，log2n，n，n log2n ，n的平方，n的三次方，2的n次方，n!），找出后，f(n) = 该数量级，若 T(n)/f(n) 求极限可得到一常数c，则时间复杂度T(n) = O(f(n))。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;34&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;for&lt;/span&gt;(i=1; i&amp;lt;=n; ++&lt;span&gt;i) { 
&lt;/span&gt;&lt;span&gt; 2&lt;/span&gt; 
&lt;span&gt; 3&lt;/span&gt;     &lt;span&gt;for&lt;/span&gt;(j=1; j&amp;lt;=n; ++&lt;span&gt;j) { 
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt; 
&lt;span&gt; 5&lt;/span&gt;         c[i][j] = 0;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;该步骤属于基本操作执行次数：n的平方次 &lt;/span&gt;
&lt;span&gt; 6&lt;/span&gt; 
&lt;span&gt; 7&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt;(k=1; k&amp;lt;=n; ++&lt;span&gt;k) 
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt; 
&lt;span&gt; 9&lt;/span&gt;             c[i][j] += a[i][k] * b[k][j];&lt;span&gt;//&lt;/span&gt;&lt;span&gt;该步骤属于基本操作执行次数：n的三次方次 &lt;/span&gt;
&lt;span&gt;10&lt;/span&gt; 
&lt;span&gt;11&lt;/span&gt; &lt;span&gt;    } 
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt; 
&lt;span&gt;13&lt;/span&gt; }    
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;      则有 T(n) = n 的平方+n的三次方，根据上面括号里的同数量级，我们可以确定 n的三次方 为T（n）的同数量级&lt;/p&gt;
&lt;p&gt;      则有 f(n) = n的三次方，然后根据 T(n)/f(n) 求极限可得到常数c&lt;/p&gt;
&lt;p&gt;      则该算法的时间复杂度：T(n) = O(n^3) 注：n^3即是n的3次方。&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;空间复杂度&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;空间复杂度可以分为两个方面：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.程序保存所需要的存储空间，也就是程序的大小。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.程序在执行过程中所需要消耗的存储空间资源，如程序在执行过程中的中间变量等。&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;简单算法实例：&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;随机生成一个20个整数数据的数组，然后输入要查找的数，然后用顺序查找法：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;伪代码：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;变量X&amp;lt;-输入待查找的数据&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;变量arr&amp;lt;-随机生成数据数组&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;for 1 to 20&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　if arr[i] ==x&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　break;找到数据&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　else&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;输出该数据的位置&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;程序结束&lt;/span&gt;&lt;/p&gt;

&lt;div class=&quot;cnblogs_code&quot; readability=&quot;37&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; java.util.Random;
&lt;/span&gt;&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; java.util.Scanner;
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; 
&lt;span&gt; 4&lt;/span&gt; &lt;span&gt;public&lt;/span&gt; &lt;span&gt;class&lt;/span&gt;&lt;span&gt; P1_1 {
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt;     &lt;span&gt;static&lt;/span&gt; &lt;span&gt;int&lt;/span&gt; N=20&lt;span&gt;;
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;     &lt;span&gt;public&lt;/span&gt; &lt;span&gt;static&lt;/span&gt; &lt;span&gt;void&lt;/span&gt;&lt;span&gt; main(String[] args) {
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt;         &lt;span&gt;int&lt;/span&gt;[] arr=&lt;span&gt;new&lt;/span&gt; &lt;span&gt;int&lt;/span&gt;&lt;span&gt;[N];
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt;         &lt;span&gt;int&lt;/span&gt;&lt;span&gt; x,n,i;
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt;         &lt;span&gt;int&lt;/span&gt; f=-1&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt; 
&lt;span&gt;11&lt;/span&gt;         Random r=&lt;span&gt;new&lt;/span&gt; Random();                     &lt;span&gt;//&lt;/span&gt;&lt;span&gt;随机种子&lt;/span&gt;
&lt;span&gt;12&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt;(i=0;i&amp;lt;N;i++&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt; &lt;span&gt;        {
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt;             arr[i]=r.nextInt(100);                    &lt;span&gt;//&lt;/span&gt;&lt;span&gt;产生数组&lt;/span&gt;
&lt;span&gt;15&lt;/span&gt; &lt;span&gt;        }
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt;         
&lt;span&gt;17&lt;/span&gt;         System.out.print(&quot;随机生成的数据序列:\n&quot;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt;(i=0;i&amp;lt;N;i++&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt; &lt;span&gt;        {
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;             System.out.print(arr[i]+&quot; &quot;);                    &lt;span&gt;//&lt;/span&gt;&lt;span&gt;输出序列&lt;/span&gt;
&lt;span&gt;21&lt;/span&gt; &lt;span&gt;        }
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt;         System.out.print(&quot;\n\n&quot;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;23&lt;/span&gt;             
&lt;span&gt;24&lt;/span&gt;         System.out.print(&quot;输入要查找的整数:&quot;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;25&lt;/span&gt;         Scanner input=&lt;span&gt;new&lt;/span&gt;&lt;span&gt; Scanner(System.in);
&lt;/span&gt;&lt;span&gt;26&lt;/span&gt;         x=input.nextInt();                            &lt;span&gt;//&lt;/span&gt;&lt;span&gt;输入要查找的数&lt;/span&gt;
&lt;span&gt;27&lt;/span&gt; 
&lt;span&gt;28&lt;/span&gt;         &lt;span&gt;for&lt;/span&gt;(i=0;i&amp;lt;N;i++)                            &lt;span&gt;//&lt;/span&gt;&lt;span&gt;顺序查找&lt;/span&gt;
&lt;span&gt;29&lt;/span&gt; &lt;span&gt;        {
&lt;/span&gt;&lt;span&gt;30&lt;/span&gt;             &lt;span&gt;if&lt;/span&gt;(x==arr[i])                            &lt;span&gt;//&lt;/span&gt;&lt;span&gt;找到数据&lt;/span&gt;
&lt;span&gt;31&lt;/span&gt; &lt;span&gt;            {
&lt;/span&gt;&lt;span&gt;32&lt;/span&gt;                 f=&lt;span&gt;i;
&lt;/span&gt;&lt;span&gt;33&lt;/span&gt;                 &lt;span&gt;break&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;34&lt;/span&gt; &lt;span&gt;            }
&lt;/span&gt;&lt;span&gt;35&lt;/span&gt; &lt;span&gt;        }
&lt;/span&gt;&lt;span&gt;36&lt;/span&gt; 
&lt;span&gt;37&lt;/span&gt; 
&lt;span&gt;38&lt;/span&gt;         &lt;span&gt;if&lt;/span&gt;(f&amp;lt;0)                                    &lt;span&gt;//&lt;/span&gt;&lt;span&gt;输出查找结果&lt;/span&gt;
&lt;span&gt;39&lt;/span&gt; &lt;span&gt;        {
&lt;/span&gt;&lt;span&gt;40&lt;/span&gt;             System.out.println(&quot;没找到数据:&quot;+&lt;span&gt;x);
&lt;/span&gt;&lt;span&gt;41&lt;/span&gt; &lt;span&gt;        }
&lt;/span&gt;&lt;span&gt;42&lt;/span&gt;         &lt;span&gt;else&lt;/span&gt;
&lt;span&gt;43&lt;/span&gt; &lt;span&gt;        {
&lt;/span&gt;&lt;span&gt;44&lt;/span&gt;             System.out.print(&quot;数据:&quot;+x+&quot; 位于数组的第 &quot;+(f+1)+&quot; 个元素处.\n&quot;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;45&lt;/span&gt; &lt;span&gt;        }
&lt;/span&gt;&lt;span&gt;46&lt;/span&gt;         
&lt;span&gt;47&lt;/span&gt; &lt;span&gt;    }
&lt;/span&gt;&lt;span&gt;48&lt;/span&gt; 
&lt;span&gt;49&lt;/span&gt; }
&lt;/pre&gt;&lt;/div&gt;

</description>
<pubDate>Sun, 05 Nov 2017 22:18:00 +0000</pubDate>
<dc:creator>谁将新樽辞旧月，今月曾经照古人</dc:creator>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/jiangwz/p/7765694.html</dc:identifier>
</item>
<item>
<title>javascript 之变量对象-09 - 风吹De麦浪</title>
<link>http://www.cnblogs.com/CandyManPing/p/7791309.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/CandyManPing/p/7791309.html</guid>
<description>&lt;p&gt;在上篇中说到，当执行流执行一个函数时，会创建对应的执行上下文(execution context)。每个执行上下文，都有三个重要属性：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;变量对象(Variable object，VO)&lt;/li&gt;
&lt;li&gt;作用域链(Scope chain)&lt;/li&gt;
&lt;li&gt;this&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;每个执行环境（执行上下文）都有一个对应的变量对象（variable object），环境中（执行上下文中）定义的所有变量、函数都保存在变量对象中。&lt;/p&gt;

&lt;p&gt;创建执行环境分为两个阶段：&lt;/p&gt;
&lt;ol readability=&quot;-1&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;创建/进入阶段（当函数被调用，但是开始执行函数内部代码之前）&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;激活/代码执行阶段（赋值、函数引用、以及其他代码）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;h2&gt;创建阶段&lt;/h2&gt;
&lt;p&gt;VO包括：&lt;/p&gt;
&lt;p&gt;1、根据函数的参数，创建并初始化arguments object&lt;/p&gt;
&lt;p&gt;2、扫描该执行上下文中的函数声明（不包括函数表达式）&lt;/p&gt;
&lt;p&gt;      a)  找到所有的function 声明，将函数名当做属性创建，值为函数定义&lt;/p&gt;
&lt;p&gt;      b)  在扫描过程中如果存在重名的函数声明，那么后面的会覆盖前面的声明，函数声明与变量声明有冲突时，会忽略（以函数声明为主）&lt;/p&gt;
&lt;p&gt;3、 扫描该执行上下文中的var变量声明&lt;/p&gt;
&lt;p&gt;      a)  找到所有的变量声明，将变量名当做属性创建，值初始为undefined&lt;/p&gt;
&lt;p&gt;      b)  在扫描过程中如果存在重名的变量声明以及重名的函数声明，会忽略；&lt;/p&gt;
&lt;p&gt;如下代码：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
&lt;span&gt;1&lt;/span&gt;     &lt;span&gt;function&lt;/span&gt;&lt;span&gt; foo(name) {
&lt;/span&gt;&lt;span&gt;2&lt;/span&gt;         &lt;span&gt;var&lt;/span&gt; age = 20&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;3&lt;/span&gt;         &lt;span&gt;function&lt;/span&gt;&lt;span&gt; run() {}
&lt;/span&gt;&lt;span&gt;4&lt;/span&gt;         &lt;span&gt;var&lt;/span&gt; say = &lt;span&gt;function&lt;/span&gt;&lt;span&gt;() {};
&lt;/span&gt;&lt;span&gt;5&lt;/span&gt;         age = 22&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;6&lt;/span&gt; &lt;span&gt;    }
&lt;/span&gt;&lt;span&gt;7&lt;/span&gt;     foo('Joel');
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;进入函数执行上下文时变量对象（vo）如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;38&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; foo.EC.VO =&lt;span&gt; {
&lt;/span&gt;&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;    arguments: {
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt;         0: 'Joel'&lt;span&gt;,
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt;         length: 1
&lt;span&gt; 5&lt;/span&gt; &lt;span&gt;    },
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;     name: 'Joel',&lt;span&gt;//参数&lt;/span&gt;
&lt;span&gt; 7&lt;/span&gt; &lt;span&gt;    age: undefined,
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt;     run: reference to &lt;span&gt;function&lt;/span&gt;&lt;span&gt; run(){},
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt; &lt;span&gt;    say: undefined    
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt; }
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;执行阶段&lt;/h2&gt;
&lt;p&gt;在代码执行阶段，会顺序执行代码，根据代码，修改VO的值  如：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;38&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; foo.EC.VO =&lt;span&gt; {
&lt;/span&gt;&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;      arguments: {
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt;           0: 'Joel'&lt;span&gt;,
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt;           length: 1
&lt;span&gt; 5&lt;/span&gt; &lt;span&gt;      },
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;       name: 'Joel',&lt;span&gt;//&lt;/span&gt;&lt;span&gt;形参&lt;/span&gt;
&lt;span&gt; 7&lt;/span&gt; &lt;span&gt;      age: 20,
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt;       run: reference to &lt;span&gt;function&lt;/span&gt;&lt;span&gt; run(){},
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt; &lt;span&gt;      say: reference to FunctionExpression say
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt;  }
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;变量提升的本质就是函数在创建执行环境时变量对象初始化下了参数、函数声明、变量；&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; &amp;lt;script&amp;gt;
&lt;span&gt; 2&lt;/span&gt;   &lt;span&gt;function&lt;/span&gt;&lt;span&gt; run(){
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; &lt;span&gt;      console.log(name);
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt; &lt;span&gt;      console.log(say);
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt;       &lt;span&gt;var&lt;/span&gt; name='Joel'&lt;span&gt;;
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;       &lt;span&gt;function&lt;/span&gt;&lt;span&gt; say(){
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt;           console.log('say'&lt;span&gt;);
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt; &lt;span&gt;      }
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt; &lt;span&gt;  }
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt; &lt;span&gt;    run();
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt; &amp;lt;/script&amp;gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/825196/201711/825196-20171106011935118-948125301.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;如上代码可以理解为这样：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt;   &lt;span&gt;function&lt;/span&gt;&lt;span&gt; run(){
&lt;/span&gt;&lt;span&gt; 2&lt;/span&gt;     &lt;span&gt;function&lt;/span&gt;&lt;span&gt; say(){  //Hoisting
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt;           console.log('say'&lt;span&gt;); 
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt; &lt;span&gt;      }
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt;      &lt;span&gt;var&lt;/span&gt; name=&lt;span&gt;undefined; //Hoisting
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt; 
&lt;span&gt; 7&lt;/span&gt; &lt;span&gt;      console.log(name);
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt; &lt;span&gt;      console.log(say);
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt;       &lt;span&gt;var&lt;/span&gt; name='Joel'&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt;       
&lt;span&gt;11&lt;/span&gt; &lt;span&gt;  }
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt;     run();
&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;第一题&lt;/h2&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; &amp;lt;script&amp;gt;
&lt;span&gt; 2&lt;/span&gt;     &lt;span&gt;function&lt;/span&gt;&lt;span&gt; run() {
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; &lt;span&gt;        console.log(a);
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt;         a = 1&lt;span&gt;;
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt; &lt;span&gt;    }
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt;     run(); &lt;span&gt;//&lt;/span&gt;&lt;span&gt; ?&lt;/span&gt;
&lt;span&gt; 7&lt;/span&gt;     &lt;span&gt;function&lt;/span&gt;&lt;span&gt; say() {
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt;         a = 1&lt;span&gt;;
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt; &lt;span&gt;        console.log(a);
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt; &lt;span&gt;    }
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt;     say(); &lt;span&gt;//&lt;/span&gt;&lt;span&gt; ?&lt;/span&gt;
&lt;span&gt;12&lt;/span&gt; &amp;lt;/script&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;第一段会报错：&lt;code&gt;Uncaught ReferenceError: a is not defined&lt;/code&gt;。第二段会打印：&lt;code&gt;1&lt;/code&gt;。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
&lt;span&gt;1&lt;/span&gt;     &lt;span&gt;function&lt;/span&gt;&lt;span&gt; foo(name) {
&lt;/span&gt;&lt;span&gt;2&lt;/span&gt; 
&lt;span&gt;3&lt;/span&gt; &lt;span&gt;        console.log(run);// 输出run 函数定义
&lt;/span&gt;&lt;span&gt;4&lt;/span&gt; &lt;span&gt;        console.log(say); //undefined
&lt;/span&gt;&lt;span&gt;5&lt;/span&gt;         &lt;span&gt;function&lt;/span&gt;&lt;span&gt; run() {}
&lt;/span&gt;&lt;span&gt;6&lt;/span&gt;         &lt;span&gt;var&lt;/span&gt; say = &lt;span&gt;function&lt;/span&gt;&lt;span&gt; () {};
&lt;/span&gt;&lt;span&gt;7&lt;/span&gt; &lt;span&gt;    }
&lt;/span&gt;&lt;span&gt;8&lt;/span&gt;     foo('Joel');
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;函数表达式会当做一个var 变量来处理&lt;/p&gt;
&lt;h2&gt;第二题&lt;/h2&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
&lt;span&gt;1&lt;/span&gt; &lt;span&gt;    console.log(run)
&lt;/span&gt;&lt;span&gt;2&lt;/span&gt;     &lt;span&gt;function&lt;/span&gt;&lt;span&gt; run() {
&lt;/span&gt;&lt;span&gt;3&lt;/span&gt; &lt;span&gt;        console.log(a);
&lt;/span&gt;&lt;span&gt;4&lt;/span&gt;         a = 1&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;5&lt;/span&gt; &lt;span&gt;    }
&lt;/span&gt;&lt;span&gt;6&lt;/span&gt;   &lt;span&gt;var&lt;/span&gt; run=1;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;输出： ƒ run() {&lt;br/&gt;            console.log(a);&lt;br/&gt;             a = 1;&lt;br/&gt;         }&lt;/p&gt;
&lt;p&gt;当声明的变量与函数重名时，声明的变量会忽略；&lt;/p&gt;
&lt;p&gt;如果在第6行代码后面添加 console.log(run)，那么run 值会被重置为1，因为在上下文对象创建阶段发现已经存在run的函数声明，var 变量会被忽略，当代码在真正执行到6行时run的值被改变了；&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
&lt;span&gt;1&lt;/span&gt; &lt;span&gt;    console.log(run)
&lt;/span&gt;&lt;span&gt;2&lt;/span&gt;     &lt;span&gt;function&lt;/span&gt;&lt;span&gt; run() {
&lt;/span&gt;&lt;span&gt;3&lt;/span&gt; &lt;span&gt;        console.log(a);
&lt;/span&gt;&lt;span&gt;4&lt;/span&gt;         a = 1&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;5&lt;/span&gt; &lt;span&gt;    }
&lt;/span&gt;&lt;span&gt;6&lt;/span&gt;   &lt;span&gt;var&lt;/span&gt; run=1&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;7&lt;/span&gt;     console.log(run)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/825196/201711/825196-20171106015700227-131971938.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;上篇中说到执行环境分为全局执行环境，函数执行环境，在本文开篇说到变量对象：&lt;/p&gt;
&lt;p&gt;每个执行环境（执行上下文）都有一个对应的变量对象（variable object），环境中（执行上下文中）定义的所有变量、函数都保存在这个对象中，那么全局执行环境是不是可以理解为也存在一个变量对象。&lt;/p&gt;
&lt;p&gt;我们先了解一个概念，什么叫叫全局对象。在 &lt;a href=&quot;http://www.w3school.com.cn/jsref/jsref_obj_global.asp&quot;&gt;W3School&lt;/a&gt; 中也有介绍：&lt;/p&gt;
&lt;p&gt;全局对象是预定义的对象，作为 JavaScript 的全局函数和全局属性的占位符。通过使用全局对象，可以访问所有其他所有预定义的对象、函数和属性。&lt;/p&gt;
&lt;p&gt;在顶层 JavaScript 代码中，可以用关键字 this 引用全局对象。但通常不必用这种方式引用全局对象，因为全局对象是作用域链的头，这意味着所有非限定性的变量和函数名都会作为该对象的属性来查询。&lt;/p&gt;
&lt;p&gt;例如，当JavaScript 代码引用 parseInt() 函数时，它引用的是全局对象的 parseInt 属性。全局对象是作用域链的头，还意味着在顶层 JavaScript 代码中声明的所有变量都将成为全局对象的属性。&lt;/p&gt;
&lt;p&gt;1.可以通过 this 引用，在客户端 JavaScript 中，全局对象就是 Window 对象。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot;&gt;
&lt;pre&gt;
console.log(&lt;span&gt;this&lt;/span&gt;);
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;2.全局对象是由 Object 构造函数实例化的一个对象。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
console.log(&lt;span&gt;this&lt;/span&gt; &lt;span&gt;instanceof&lt;/span&gt; Object);
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;3.预定义了一大堆函数和属性。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
&lt;span&gt;//&lt;/span&gt;&lt;span&gt; 都生效&lt;/span&gt;
&lt;span&gt;console.log(Math.random());
console.log(&lt;/span&gt;&lt;span&gt;this&lt;/span&gt;.Math.random());
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;4.作为全局变量的宿主。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
&lt;span&gt;var&lt;/span&gt; a = 1&lt;span&gt;;
console.log(&lt;/span&gt;&lt;span&gt;this&lt;/span&gt;.a);
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;5.客户端 JavaScript 中，全局对象有 window 属性指向自身。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
&lt;span&gt;var&lt;/span&gt; a = 1&lt;span&gt;;
console.log(window.a);

&lt;/span&gt;&lt;span&gt;this&lt;/span&gt;.window.b = 2&lt;span&gt;;
console.log(&lt;/span&gt;&lt;span&gt;this&lt;/span&gt;.b);
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;写了这么多介绍全局对象，其实就是想说：全局上下文中的变量对象就是全局对象！&lt;/p&gt;

&lt;p&gt;它们其实都是同一个对象，只是处于执行上下文的不同生命周期。未进入执行阶段之前，变量对象(VO)中的属性都不能访问，但是进入执行阶段之后，执行环境被压入执行环境栈变量对象(VO)转变为了活动对象(AO)，里面的属性都能被访问了，然后开始进行执行阶段的操作。&lt;/p&gt;

&lt;ol&gt;&lt;li&gt;全局上下文的变量对象是全局对象&lt;/li&gt;
&lt;li&gt;函数上下文的变量对象初始化，最开始只包括 arguments 对象&lt;/li&gt;
&lt;li&gt;在创建执行上下文时会给变量对象添加形参、函数声明、变量声明等初始的属性值&lt;/li&gt;
&lt;li&gt;在代码执行阶段，会再次修改变量对象的属性值&lt;/li&gt;
&lt;li&gt;创建执行环境----》初始化变量对象（AO）（参数、函数声明、var 变量）---》执行环境被推入栈----》执行代码-----》VO 激活为AO ------》改变AO 的值&lt;/li&gt;
&lt;/ol&gt;
</description>
<pubDate>Sun, 05 Nov 2017 19:11:00 +0000</pubDate>
<dc:creator>风吹De麦浪</dc:creator>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/CandyManPing/p/7791309.html</dc:identifier>
</item>
<item>
<title>Branch Prediction - TaigaComplex</title>
<link>http://www.cnblogs.com/TaigaCon/p/7791303.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/TaigaCon/p/7791303.html</guid>
<description>&lt;p&gt;现代微处理器的pipeline中包含许多阶段，粗略地可以分成fetch、decode、execution、retirement，细分开来可以分成十多甚至二十多个阶段。在处理器处理指令时，可以像流水线一样同时处理位于不同阶段的指令。&lt;/p&gt;
&lt;p&gt;下图，假设一个pipeline分为四个阶段，每个阶段耗费一个时钟周期。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://images2017.cnblogs.com/blog/421096/201711/421096-20171106023938649-105373503.png&quot;&gt;&lt;img title=&quot;image&quot; src=&quot;http://images2017.cnblogs.com/blog/421096/201711/421096-20171106023939602-537760257.png&quot; alt=&quot;image&quot; width=&quot;471&quot; height=&quot;340&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;4条指令按照先后顺序进入pipeline，每间隔一个时钟周期，指令就能从pipeline的上一个阶段转移到下一个阶段，在第四个时钟周期时，4条指令全部进入pipeline内，各个阶段都含有一条指令。按照这种策略，最佳的情况就是指令源源不断地进入pipeline，pipeline中就会一直都在同时处理四条指令，那么指令的处理效率就是原来的4倍。&lt;/p&gt;



&lt;p&gt;由于指令之间存在依赖关系，因此需要采用各种辅助机制来保证指令的流畅执行。我们之前就已经讨论过指令间的源/目标操作数依赖，pipeline中是用in-flight机制来加速指令处理，这里讨论另外一种依赖，就是分支（branch）对比较结果（flag）的依赖。指令中经常会出现跳转指令，特别是条件跳转指令，在得到条件的结果前，我们是不知道接下来会走哪个分支的，因此按照一般的逻辑，应该需要先等待比较结果执行完毕，再根据结构去取相应的分支进入pipeline内处理。不过这会导致指令的执行效率下降，因为在等待比较指令执行完成的过程中，后续的指令无法进入pipeline，也就是执行时间几乎延迟了一整个个pipeline的时钟周期。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://images2017.cnblogs.com/blog/421096/201711/421096-20171106023940040-736862254.png&quot;&gt;&lt;img title=&quot;image&quot; src=&quot;http://images2017.cnblogs.com/blog/421096/201711/421096-20171106023940649-194882564.png&quot; alt=&quot;image&quot; width=&quot;583&quot; height=&quot;340&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在现代微处理器中，由于pipeline的细分，长度（阶段）达到十多甚至二十多，因此如果不采取相应措施则会导致出现10~20个时钟周期的延迟。&lt;/p&gt;



&lt;p&gt;为了克服上述问题，pipeline中引入了Branch Prediction机制。Branch Prediction就是通过预测，把接下来最有可能执行的分支获取进入pipeline，就像不存在对比较结果的依赖那样直接执行，这么一来就保持了指令的流畅执行，这也被称为Speculative Execution。不过这种通过预测获取进入pipeline的分支终究只是预测分支，实际上不一定是执行这一分支，因此这部分指令的执行结果不应该从pipeline中输出，即不应该执行retirement这一步骤。在得到比较结果后，就能知道预测的分支是否为实际应该执行的分支，如果是，pipeline中的预测分支指令就能继续执行下去，否则就需要把预测分支的指令排空，重新获取正确分支的指令进入pipeline继续执行。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://images2017.cnblogs.com/blog/421096/201711/421096-20171106023941134-945433081.png&quot;&gt;&lt;img title=&quot;image&quot; src=&quot;http://images2017.cnblogs.com/blog/421096/201711/421096-20171106023941556-214880979.png&quot; alt=&quot;image&quot; width=&quot;583&quot; height=&quot;372&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;采用Branch Prediction机制后，条件跳转的延迟将取决于预测的成功率。成功率越高，则能保证指令流畅执行，提升指令处理效率；成功率低，则会导致Branch Misprediction经常发生，这需要把预测分支排空，重新获取指令执行，因此会降低指令处理效率。&lt;/p&gt;



&lt;p&gt;我们把进行分支预测的硬件称为Branch Predictor，也称之为Branch Prediction Unit(BPU)。如前文所述，BPU的主要作用是预测接下来执行的指令分支，也就是说BPU作用于pipeline的前端(front-end)。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://images2017.cnblogs.com/blog/421096/201711/421096-20171106023942009-2087552198.png&quot;&gt;&lt;img title=&quot;image&quot; src=&quot;http://images2017.cnblogs.com/blog/421096/201711/421096-20171106023942477-732559854.png&quot; alt=&quot;image&quot; width=&quot;550&quot; height=&quot;454&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;如上图为一个NetBurst(Pentium 4)微处理器的粗略pipeline。如果所预测的分支还没进入pipeline内，则需要从cache中读取，BPU会在Fetch阶段去控制读取所预测的指令分支。所预测的分支也有可能已经在pipeline内，如以前执行过该分支，而该分支的指令在被解码成μops后会存储在Trace Cache，BPU可以通过控制Trace Cache向EU发送预测分支的μops。在确定了实际上所走的分支之后，retirement会向BPU进行反馈，更新BPU中的信息并用于下一次分支预测。&lt;/p&gt;



&lt;p&gt;Branch Prediction早在1950s末就引入了到了IBM Stretch处理器中，经过几十年的发展，Branch Prediction进化出了多种实现方式。现代的处理器中往往包含其中的多个实现，以应用在不同的指令环境。。&lt;/p&gt;

&lt;h4&gt;Static Branch Prediction&lt;/h4&gt;
&lt;p&gt;Static Branch Prediction是最简单的分支预测，因为它不依赖于历史的分支选择。Static Branch Prediction可以细分为三类：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Early Static Branch Prediction，总是预测接下来的指令不走跳转分支，即执行位于跳转指令前方相邻（比当前指令晚执行）的指令。&lt;/li&gt;
&lt;li&gt;Advanced Static Branch Prediction，如果所跳转的目标地址位于跳转指令的前方（比当前指令晚执行），则不跳转；如果所跳转的目标地址位图跳转指令的后方（比当前指令早执行）则跳转。这种方法可以很有效地应用在循环的跳转中。&lt;/li&gt;
&lt;li&gt;Hints Static Branch Prediction，可以在指令中插入提示，用于指示是否进行跳转。x86架构中只有Pentium 4用过这种预测方式。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;http://images2017.cnblogs.com/blog/421096/201711/421096-20171106023942962-867288308.png&quot;&gt;&lt;img title=&quot;image&quot; src=&quot;http://images2017.cnblogs.com/blog/421096/201711/421096-20171106023943446-433328795.png&quot; alt=&quot;image&quot; width=&quot;714&quot; height=&quot;410&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;目前的Intel处理器会在缺少历史分支信息的时候采用Advanced Static Branch Prediction来进行分支预测，也就是说如果某分支在第一次执行时会采用该预测方式。因此我们在进行编码是需要进行注意，以便优化代码的执行效率。&lt;/p&gt;
&lt;pre class=&quot;brush: js; auto-links: true; collapse: false; first-line: 1; gutter: true; html-script: false; light: false; ruler: false; smart-tabs: true; tab-size: 4; toolbar: true;&quot;&gt;
//Forward condition branches not taken (fall through)
IF&amp;lt;condition&amp;gt; {....
↓
}

//Backward conditional branches are taken
LOOP {...
↑ −− }&amp;lt;condition&amp;gt;

//Unconditional branches taken
JMP
------→
&lt;/pre&gt;
&lt;p&gt;碰到IF条件语句时会预测走不命中分支，碰到循环（while在循环首部除外）时默认进入循环，碰到无条件跳转则必然走跳转分支了。&lt;/p&gt;

&lt;h4&gt;One-level Branch Prediction/Saturating Counter&lt;/h4&gt;
&lt;p&gt;Saturating Counter可以当作一个状态机，这类型的Branch Prediction就是记录该分支的状态，并根据这个状态来预测走哪一条分支。&lt;/p&gt;
&lt;p&gt;1-bit saturating counter记录的就是分支上一次的走向，并预测这次的分支会走同一方向。&lt;/p&gt;
&lt;p&gt;2-bit saturating counter有如下状态转换：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://images2017.cnblogs.com/blog/421096/201711/421096-20171106023943993-2019339012.png&quot;&gt;&lt;img title=&quot;image&quot; src=&quot;http://images2017.cnblogs.com/blog/421096/201711/421096-20171106023944493-2049249193.png&quot; alt=&quot;image&quot; width=&quot;488&quot; height=&quot;164&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;即分支有四种状态：strongly not taken、weakly not taken、weakly taken、strongly taken。其中not taken的状态会预测走非跳转分支，而taken状态会预测走跳转分支，并且会根据实际分支为跳转（T）或者非跳转（NT）进行状态的调整。&lt;/p&gt;

&lt;h4&gt;Two-level adaptive predictor with local history tables&lt;/h4&gt;
&lt;p&gt;上述的One-level Branch Prediction有个缺陷，以2-bit为例，假设目前某个分钟的状态为strongly taken，然后该分支的实际走向为0011-0011-0011（0表示NT，1表示T），但是预测的走向为1100-1100-1100，也就是说该2-bit预测的准确率将为0%。为了改善这个问题，引入了Two-level adaptive predictor with local history tables。&lt;/p&gt;
&lt;p&gt;Two-level adaptive predictor如其名字所述，分为两级：Branch history以及Pattern history table。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://images2017.cnblogs.com/blog/421096/201711/421096-20171106023944852-1357231284.png&quot;&gt;&lt;img title=&quot;image&quot; src=&quot;http://images2017.cnblogs.com/blog/421096/201711/421096-20171106023945337-178128150.png&quot; alt=&quot;image&quot; width=&quot;359&quot; height=&quot;245&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Branch history的长度为n bit，用于记录某个branch上n次的分支走向。&lt;/p&gt;
&lt;p&gt;Pattern history table共有2&lt;sup&gt;n&lt;/sup&gt;个项，每个项记录一个Suturating Counter的状态。&lt;/p&gt;
&lt;p&gt;某个分支在进行分支预测时，会根据该分支上n次历史分支走向来选择对应的Pattern history table entry，然后依据其中的状态来进行预测。在得出实际的分支走向后，也会按照该路线去修改对应table entry中的状态，然后更新Branch history。&lt;/p&gt;
&lt;p&gt;回到上面的例子，如果Branch history的n=2，那么Pattern history table会有四项：00、01、10、11。而0011-0011-0011这种分支选择方式有规律：00后为1、01后为1、11后为0、10后为0。因此在经过三个周期的分支选择后，Pattern history table存储的状态就能完美预测该分支的下一次走向，也就是说这种n=2的Two-level adaptive predictor就能完美解决上面提出的问题。&lt;/p&gt;
&lt;p&gt;不过如果实际的分支走向为0001-0001-0001，那么n=2就显得不够了，因为00后可能为0或者1。此时就需要n=3，有000后为1、001后为0、010后为0、100后为0，此时table中的011、101、110、111项为空闲项。&lt;/p&gt;
&lt;p&gt;实际上我们可以总结出以下规律：如果某分支的实际走向有固定的周期规律，周期内部有p项，并且该p项内的任意连续n项没有重复（并且满足n+1&amp;lt;p&amp;lt;=2&lt;sup&gt;n&lt;/sup&gt;），则n bit的Two-level adaptive predictor就能完美得预测这类型的分支。&lt;/p&gt;

&lt;h4&gt;Two-level adaptive predictor with global history table&lt;/h4&gt;
&lt;p&gt;上一小节描述的是local history table，即branch history中存储的是单个branch的历史走向，而global history table中存储的是位于当前branch后方（比当前指令早执行）的n个branches的走向。在local history table实现中，需要为每一个branch维护独立的Branch history以及Pattern history table，这导致需要大容量的Branch Target Buffer（BTB）来存储这些数据，不过实际上BTB的容量是有限的。而在global history table中，仅保留一个Branch history以及Pattern history table，能很大程度地节约BTB空间。&lt;/p&gt;
&lt;p&gt;不过这种实现的缺点也很明显，由于只有一个Branch history，也就是说每个分支都是以这个Branch history为基础来选择Pattern history table，不过不同的分支也有可能出现相同的Branch history值，这就很难保证一个独立的分支对应一个独立Pattern history table entry，也就是说需要较长的Branch history（较大的n，很多现代的微处理器为n=16），以降低不同分支间由于定位到了相同entry带来的交叉影响。而且也不是每次执行到某个分支时它的Branch history都一样，Branch history也会改变，这就使得无法定位到所需的entry。另外，由于采用的是不久前执行过的历史分支来预测当前分支，也就是认为相邻分支间具有相关性，不过实际上也不一定如此。所以说global history table预测的准确程度是不如local history table的。&lt;/p&gt;

&lt;h4&gt;Agree Predictor&lt;/h4&gt;
&lt;p&gt;上面在讨论global history table时说到不同分支可能会由于有同一Branch history而定位到同一Pattern history entry，导致不同分支间交叉影响，Agree Predictor为这种情形提供了解决方法。&lt;/p&gt;
&lt;p&gt;Agree Predictor采用了global以及local混合的方法。global仍然是采用较长的Branch history以及2-bit Saturating Counter，预测的是某一分支是否与其上次走向相同；local则只用1bit为每个Branch存储其上一次的分支走向。global的输出与local的输出进行异或则能得到分支预测。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://images2017.cnblogs.com/blog/421096/201711/421096-20171106023945712-1864962545.png&quot;&gt;&lt;img title=&quot;image&quot; src=&quot;http://images2017.cnblogs.com/blog/421096/201711/421096-20171106023946165-1741541117.png&quot; alt=&quot;image&quot; width=&quot;573&quot; height=&quot;338&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;上图中加入了Branch Address用于定位分支，Branch Address也能用于与Branch history经过某种方式的混合（Indexing function）使得定位的Pattern history table entry更准确。&lt;/p&gt;

&lt;h4&gt;Loop Predictor&lt;/h4&gt;
&lt;p&gt;一个周期为n的循环在进行分支选择的时候，会走n-1次跳转/不跳转以及1次不跳转/跳转，假设n=6，则会形成如111110-111110的形式。因此循环的分支预测算是一种比较容易预测的分支，不过如果循环判断的次数n非常大，并且体内部有多个分支，那么在Branch history长度有限的情况下，单靠前面所述的global/Agree的预测方式会很难达到较好的预测效果，所以需要一个独立的Loop Predictor来对循环分支进行预测。&lt;/p&gt;
&lt;p&gt;Loop Predictor在第一次对循环分支进行预测时能记录下该分支的循环周期为n，那么在下一次碰到该循环分支的时候就会预测走n-1次taken/not taken然后走一次not taken/taken。BTB中则需要记录某个Branch的跳转目标地址、Branch是否为循环、循环的周期n以及循环在退出的时候是taken还是not taken。&lt;/p&gt;

&lt;h4&gt;Indirect Branch Predictor&lt;/h4&gt;
&lt;p&gt;我们前面所讨论的分支都是以二叉分支为基础展开讨论，不过分支不总是二叉的。switch以及多态的虚函数在编译时可能（编译器相关）会被编译成 &lt;span&gt;jmp eax/call eax&lt;/span&gt; 这类需要通过计算才能得到目标地址的跳转，这种分支就不是二叉的，而是会有多个候选的目标地址，这就是所谓的Indirect Branch。而前面所述的Branch history是1个bit代表一个分支，这种predictor在碰Indirect Branch的时候只能固定地指定一个跳转地址，因此是不够合适的。&lt;/p&gt;
&lt;p&gt;在Indirect Branch Predictor中，Branch history用多个bit代表一个分支，如此一来则能很好地适配Indirect Branch。&lt;/p&gt;

&lt;h4&gt;Prediction of function returns&lt;/h4&gt;
&lt;p&gt;在函数返回时，会用到 &lt;span&gt;ret/leave&lt;/span&gt; 等指令进行跳转，这些指令是需要从栈中读取跳回地址后再进行跳转的，因此也算是一种比较另类Indirect Branch。不过由于这种返回指令总是与 &lt;span&gt;call/enter&lt;/span&gt; 成对出现，因此一种较好的处理方法就是在每次进入函数的时候都去读取其返回地址入栈（这里的栈不是程序的栈，而是Predictor维护的，专门用于返回跳转），在碰到返回指令时从栈内取出目标地址直接用于Branch Prediction。这就是所谓的return stack buffer机制。&lt;/p&gt;
&lt;p&gt;由于这种预测机制用到了stack，也就是说需要 &lt;span&gt;call/enter&lt;/span&gt; 跟 &lt;span&gt;ret/leave&lt;/span&gt; 成对出现，因此为了保证指令的执行效率，尽量不要用 &lt;span&gt;jmp&lt;/span&gt; 来代替函数的跳入跳出指令。&lt;/p&gt;

&lt;h4&gt;Hybrid Predictor&lt;/h4&gt;
&lt;p&gt;Hybrid Predictor就是采用多种predictor混合预测，然后从中选择出对当前branch来说较优的Predictor，以其输出结果进行分支预测。&lt;/p&gt;

&lt;h4&gt;Neural branch predictor&lt;/h4&gt;
&lt;p&gt;采用机器学习来进行分支预测，好处是相比其他predictor预测更为准确，不过相应地需要消耗更多的时间，延迟较大，不过这是早期的说法了。目前AMD Ryzen最新的处理器就是基于神经网络来进行分支预测。&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Reference：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.agner.org/optimize/microarchitecture.pdf&quot;&gt;Agner Fog : The microarchitecture of Intel, AMD and VIA CPUs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-optimization-manual.html&quot;&gt;Intel® 64 and IA-32 Architectures Optimization Reference Manual&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-software-developer-manual-325462.html&quot;&gt;Intel 64 and IA-32 Architectures Software Developer's Manual&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Branch_predictor&quot;&gt;Wiki Branch predictor&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Sun, 05 Nov 2017 18:40:00 +0000</pubDate>
<dc:creator>TaigaComplex</dc:creator>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/TaigaCon/p/7791303.html</dc:identifier>
</item>
<item>
<title>学习笔记TF059:自然语言处理、智能聊天机器人 - 利炳根</title>
<link>http://www.cnblogs.com/libinggen/p/7790162.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/libinggen/p/7790162.html</guid>
<description>
&lt;div id=&quot;post_detail&quot;&gt;
&lt;div class=&quot;block&quot;&gt;

&lt;div class=&quot;post&quot;&gt;
&lt;div class=&quot;postcontent&quot; readability=&quot;73&quot;&gt;
&lt;div id=&quot;cnblogs_post_body&quot; readability=&quot;313.5&quot;&gt;
&lt;p&gt;自然语言处理，语音处理、文本处理。语音识别(speech recognition)，让计算机能够“听懂”人类语音，语音的文字信息“提取”。&lt;/p&gt;
&lt;p&gt;日本富国生命保险公司花170万美元安装人工智能系统，客户语言转换文本，分析词正面或负面。智能客服是人工能智能公司研究重点。循环神经网络(recurrent neural network,RNN)模型。&lt;/p&gt;
&lt;p&gt;模型选择。每一个矩形是一个向量，箭头表示函数。最下面一行输入向量，最上面一行输出向量，中间一行RNN状态。一对一，没用RNN，如Vanilla模型，固定大小输入到固定大小输出(图像分类)。一对多，序列输出，图片描述，输入一张图片输出一段文字序列，CNN、RNN结合，图像、语言结合。多对一，序列输入，情感分析，输入一段文字，分类积极、消极情感，如淘宝商品评论分类，用LSTM。多对多，异步序列输入、序列输出，机器翻译，如RNN读取英文语句，以法语形式输出。多对多，同步序列输入、序列输出，视频分类，视频每帧打标记。中间RNN状态部分固定，可多次使用，不需对序列长度预先约束。Andrej Karpathy《The Unreasonable Effectiveness of Recurrent Neural Networks》。http://karpathy.github.io/2015/05/21/rnn-effectiveness/ 。自然语言处理，语音合成(文字生成语音)、语单识别、声纹识别(声纹鉴权)、文本处理(分词、情感分析、文本挖掘)。&lt;/p&gt;
&lt;p&gt;英文数字语音识别。https://github.com/pannous/tensorflow-speech-recognition/blob/master/speech2text-tflearn.py 。20行Python代码创建超简单语音识别器。LSTM循环神经网络，TFLearn训练英文数字口语数据集。spoken numbers pcm数据集 http://pannous.net/spoken_numbers.tar 。多人阅读0~9数字英文音频，分男女声，一段音频(wav文件)只有一个数字对应英文声音。标识方法{数字}_人名_xxx。&lt;/p&gt;
&lt;p&gt;定义输入数据，预处理数据。语音处理成矩阵形式。梅尔频率倒谱系数(Mel frequency cepstral coefficents, MFCC)特征向量。语音分帧、取对数、逆矩阵，生成MFCC代表语音特征。&lt;/p&gt;
&lt;p&gt;定义网络模型。LSTM模型。&lt;/p&gt;
&lt;p&gt;训练模型，并存储模型。&lt;/p&gt;
&lt;p&gt;预测模型。任意输入一个语音文件，预测。&lt;/p&gt;
&lt;p&gt;语音识别，可用在智能输入法、会议快速录入、语音控制系统、智能家居领域。&lt;/p&gt;

&lt;div class=&quot;cnblogs_code&quot; readability=&quot;69&quot;&gt;
&lt;pre&gt;
&lt;span&gt;#&lt;/span&gt;&lt;span&gt;!/usr/bin/env python&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt;!/usr/local/bin/python&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt; -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span&gt;from&lt;/span&gt; &lt;span&gt;__future__&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; division, print_function, absolute_import
&lt;/span&gt;&lt;span&gt;import&lt;/span&gt;&lt;span&gt; tflearn
&lt;/span&gt;&lt;span&gt;import&lt;/span&gt;&lt;span&gt; speech_data
learning_rate &lt;/span&gt;= 0.0001&lt;span&gt;
training_iters &lt;/span&gt;= 300000  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; steps 迭代次数&lt;/span&gt;
batch_size = 64&lt;span&gt;
width &lt;/span&gt;= 20  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; mfcc features MFCC特征&lt;/span&gt;
height = 80  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; (max) length of utterance 最大发音长度&lt;/span&gt;
classes = 10  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; digits 数字类别&lt;/span&gt;
batch = word_batch = speech_data.mfcc_batch_generator(batch_size) &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 生成每一批MFCC语音&lt;/span&gt;
X, Y =&lt;span&gt; next(batch)
&lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; train, test, _ = ,X&lt;/span&gt;
trainX, trainY =&lt;span&gt; X, Y
testX, testY &lt;/span&gt;= X, Y &lt;span&gt;#&lt;/span&gt;&lt;span&gt;overfit for now&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt; Data preprocessing&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt; Sequence padding&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt; trainX = pad_sequences(trainX, maxlen=100, value=0.)&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt; testX = pad_sequences(testX, maxlen=100, value=0.)&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt; # Converting labels to binary vectors&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt; trainY = to_categorical(trainY, nb_classes=2)&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt; testY = to_categorical(testY, nb_classes=2)&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt; Network building&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt; LSTM模型&lt;/span&gt;
net =&lt;span&gt; tflearn.input_data([None, width, height])
&lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; net = tflearn.embedding(net, input_dim=10000, output_dim=128)&lt;/span&gt;
net = tflearn.lstm(net, 128, dropout=0.8&lt;span&gt;)
net &lt;/span&gt;= tflearn.fully_connected(net, classes, activation=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;softmax&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
net &lt;/span&gt;= tflearn.regression(net, optimizer=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;adam&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, learning_rate=learning_rate, loss=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;categorical_crossentropy&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Training&lt;/span&gt;
model = tflearn.DNN(net, tensorboard_verbose=&lt;span&gt;0)
model.load(&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;tflearn.lstm.model&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;while&lt;/span&gt; 1: &lt;span&gt;#&lt;/span&gt;&lt;span&gt;training_iters&lt;/span&gt;
  model.fit(trainX, trainY, n_epoch=100, validation_set=(testX, testY), show_metric=&lt;span&gt;True,
          batch_size&lt;/span&gt;=&lt;span&gt;batch_size)
  _y&lt;/span&gt;=&lt;span&gt;model.predict(X)
model.save(&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;tflearn.lstm.model&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;)
&lt;/span&gt;&lt;span&gt;print&lt;/span&gt;&lt;span&gt; (_y)
&lt;/span&gt;&lt;span&gt;print&lt;/span&gt; (y)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;智能聊天机器人。未来方向“自然语言人机交互”。苹果Siri、微软Cortana和小冰、Google Now、百度度秘、亚马逊蓝牙音箱Amazon Echo内置语音助手Alexa、Facebook 语音助手M。通过和用户“语音机器人”对话，引导用户到对应服务。今后智能硬件、智能家居嵌入式应用。&lt;/p&gt;
&lt;p&gt;智能聊天机器人3代技术。第一代特征工程，大量逻辑判断。第二代检索库，给定问题、聊天，从检索库找到与已有答案最匹配答案。第三代深度学习，seq2seq+Attention模型，大量训练，根据输入生成输出。&lt;/p&gt;
&lt;p&gt;seq2seq+Attention模型原理、构建方法。翻译模型，把一个序列翻译成另一个序列。两个RNNLM，一个作编码器，一个解码器，组成RNN编码器-解码器。文本处理领域，常用编码器-解码器(encoder-decoder)框架。输入-&amp;gt;编码器-&amp;gt;语义编码C-&amp;gt;解码器-&amp;gt;输出。适合处理上下文(context)生成一个目标(target)通用处理模型。一个句子对&amp;lt;X,Y&amp;gt;，输入给定句子X，通过编码器-解码器框架生成目标句子Y。X、Y可以不同语言，机器翻译。X、Y是对话问句答句，聊天机器人。X、Y可以是图片和对应描述，看图说话。&lt;br/&gt;X由x1､x2等单词序列组成，Y由y1､y2等单词序列组成。编码器编码输入X，生成中间语义编码C，解码器解码中间语义编码C，每个i时刻结合已生成y1､y2……yi-1历史信息生成Yi。生成句子每个词采用中间语义编码相同 C。短句子贴切，长句子不合语义。&lt;br/&gt;实际实现聊天系统，编码器和解码器采用RNN模型、LSTM模型。句子长度超过30，LSTM模型效果急剧下降，引入Attention模型，长句子提升系统效果。Attention机制，人在做一件事情，专注做这件事，忽略周围其他事。源句子中对生成句子重要关键词权重提高，产生更准确应答。增加Attention模型编码器-解码器模型框架：输入-&amp;gt;编码器-&amp;gt;语义编码C1､C2､C3-&amp;gt;解码器-&amp;gt;输出Y1、Y2、Y3。中间语义编码Ci不断变化，产生更准确Yi。&lt;/p&gt;
&lt;p&gt;最佳实践。https://github.com/suriyadeepan/easy_seq2seq ，依赖TensorFlow 0.12.1环境。康奈尔大学 Corpus数据集(Cornell Movie Dialogs Corpus) http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html 。600 部电影对白。&lt;/p&gt;
&lt;p&gt;处理聊天数据。&lt;/p&gt;
&lt;p&gt;先把数据集整理成“问”、“答”文件，生成.enc(问句)、.dec(答句)文件。test.dec #测试集答句，test.enc #测试集问句，train.dec #训练集答句，train.enc #训练集问句。&lt;br/&gt;创建词汇表，问句、答句转换成对应id形式。词汇表文件2万个词汇。vocab20000.dec #答句词汇表，vocab20000.enc #问句词汇表。_GO、_EOS、_UNK、_PAD seq2seq模型特殊标记，填充标记对话。_GO标记对话开始。_EOS标记对话结束。_UNK标记未出现词汇表字符，替换稀有词汇。_PAD填充序列，保证批次序列长度相同。转换成ids文件，test.enc.ids20000､train.dec.ids20000､train.enc.ids20000。问句、答句转换ids文件，每行是一个问句或答句，每行每个id代表问句或答句对应位置词。&lt;/p&gt;
&lt;p&gt;采用编码器-解码器框架训练。&lt;/p&gt;
&lt;p&gt;定义训练参数。seq2seq.ini。&lt;/p&gt;

&lt;div class=&quot;cnblogs_code&quot; readability=&quot;45&quot;&gt;
&lt;pre&gt;
&lt;span&gt;[strings]
&lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Mode : train, test, serve 模式&lt;/span&gt;
mode =&lt;span&gt; train
train_enc &lt;/span&gt;= data/&lt;span&gt;train.enc
train_dec &lt;/span&gt;= data/&lt;span&gt;train.dec
test_enc &lt;/span&gt;= data/&lt;span&gt;test.enc
test_dec &lt;/span&gt;= data/&lt;span&gt;test.dec
&lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; folder where checkpoints, vocabulary, temporary data will be stored&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt; 模型文件和词汇表存储路径&lt;/span&gt;
working_directory = working_dir/&lt;span&gt;
[ints]
&lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; vocabulary size&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt; 词汇表大小&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt;     20,000 is a reasonable size&lt;/span&gt;
enc_vocab_size = 20000&lt;span&gt;
dec_vocab_size &lt;/span&gt;= 20000
&lt;span&gt;#&lt;/span&gt;&lt;span&gt; number of LSTM layers : 1/2/3&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt; LSTM层数&lt;/span&gt;
num_layers = 3
&lt;span&gt;#&lt;/span&gt;&lt;span&gt; typical options : 128, 256, 512, 1024 每层大小，可取值&lt;/span&gt;
layer_size = 256
&lt;span&gt;#&lt;/span&gt;&lt;span&gt; dataset size limit; typically none : no limit&lt;/span&gt;
max_train_data_size =&lt;span&gt; 0
batch_size &lt;/span&gt;= 64
&lt;span&gt;#&lt;/span&gt;&lt;span&gt; steps per checkpoint&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt; 每多少次迭代存储一次模型&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt;     Note : At a checkpoint, models parameters are saved, model is evaluated&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt;            and results are printed&lt;/span&gt;
steps_per_checkpoint = 300&lt;span&gt;
[floats]
learning_rate &lt;/span&gt;= 0.5 &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 学习速率&lt;/span&gt;
learning_rate_decay_factor = 0.99 &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 学习速率下降系数&lt;/span&gt;
max_gradient_norm = 5.0
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;定义网络模型 seq2seq。seq2seq_model.py。TensorFlow 0.12。定义seq2seq+Attention模型类，3个函数。《Grammar as a Foreign Language》 http://arxiv.org/abs/1412.7499 。初始化模型函数(__init__)、训练模型函数(step)、获取下一批次训练数据函数(get_batch)。&lt;/p&gt;

&lt;div class=&quot;cnblogs_code&quot; readability=&quot;175&quot;&gt;
&lt;pre&gt;
&lt;span&gt;from&lt;/span&gt; &lt;span&gt;__future__&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; absolute_import
&lt;/span&gt;&lt;span&gt;from&lt;/span&gt; &lt;span&gt;__future__&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; division
&lt;/span&gt;&lt;span&gt;from&lt;/span&gt; &lt;span&gt;__future__&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; print_function
&lt;/span&gt;&lt;span&gt;import&lt;/span&gt;&lt;span&gt; random
&lt;/span&gt;&lt;span&gt;import&lt;/span&gt;&lt;span&gt; numpy as np
&lt;/span&gt;&lt;span&gt;from&lt;/span&gt; six.moves &lt;span&gt;import&lt;/span&gt; xrange  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; pylint: disable=redefined-builtin&lt;/span&gt;
&lt;span&gt;import&lt;/span&gt;&lt;span&gt; tensorflow as tf
&lt;/span&gt;&lt;span&gt;from&lt;/span&gt; tensorflow.models.rnn.translate &lt;span&gt;import&lt;/span&gt;&lt;span&gt; data_utils
&lt;/span&gt;&lt;span&gt;class&lt;/span&gt;&lt;span&gt; Seq2SeqModel(object):
  &lt;/span&gt;&lt;span&gt;def&lt;/span&gt; &lt;span&gt;__init__&lt;/span&gt;&lt;span&gt;(self, source_vocab_size, target_vocab_size, buckets, size,
               num_layers, max_gradient_norm, batch_size, learning_rate,
               learning_rate_decay_factor, use_lstm&lt;/span&gt;=&lt;span&gt;False,
               num_samples&lt;/span&gt;=512, forward_only=&lt;span&gt;False):
    &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt; 构建模型
    Args: 参数
      source_vocab_size: size of the source vocabulary. 问句词汇表大小
      target_vocab_size: size of the target vocabulary.答句词汇表大小
      buckets: a list of pairs (I, O), where I specifies maximum input length
        that will be processed in that bucket, and O specifies maximum output
        length. Training instances that have inputs longer than I or outputs
        longer than O will be pushed to the next bucket and padded accordingly.
        We assume that the list is sorted, e.g., [(2, 4), (8, 16)].
        其中I指定最大输入长度，O指定最大输出长度
      size: number of units in each layer of the model.每层神经元数量
      num_layers: number of layers in the model.模型层数
      max_gradient_norm: gradients will be clipped to maximally this norm.梯度被削减到最大规范
      batch_size: the size of the batches used during training;
        the model construction is independent of batch_size, so it can be
        changed after initialization if this is convenient, e.g., for decoding.批次大小。训练、预测批次大小，可不同
      learning_rate: learning rate to start with.学习速率
      learning_rate_decay_factor: decay learning rate by this much when needed.调整学习速率
      use_lstm: if true, we use LSTM cells instead of GRU cells.使用LSTM 单元代替GRU单元
      num_samples: number of samples for sampled softmax.使用softmax样本数
      forward_only: if set, we do not construct the backward pass in the model.是否仅构建前向传播
    &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;
    self.source_vocab_size &lt;/span&gt;=&lt;span&gt; source_vocab_size
    self.target_vocab_size &lt;/span&gt;=&lt;span&gt; target_vocab_size
    self.buckets &lt;/span&gt;=&lt;span&gt; buckets
    self.batch_size &lt;/span&gt;=&lt;span&gt; batch_size
    self.learning_rate &lt;/span&gt;= tf.Variable(float(learning_rate), trainable=&lt;span&gt;False)
    self.learning_rate_decay_op &lt;/span&gt;=&lt;span&gt; self.learning_rate.assign(
        self.learning_rate &lt;/span&gt;*&lt;span&gt; learning_rate_decay_factor)
    self.global_step &lt;/span&gt;= tf.Variable(0, trainable=&lt;span&gt;False)
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; If we use sampled softmax, we need an output projection.&lt;/span&gt;
    output_projection =&lt;span&gt; None
    softmax_loss_function &lt;/span&gt;=&lt;span&gt; None
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Sampled softmax only makes sense if we sample less than vocabulary size.&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 如果样本量比词汇表量小，用抽样softmax&lt;/span&gt;
    &lt;span&gt;if&lt;/span&gt; num_samples &amp;gt; 0 &lt;span&gt;and&lt;/span&gt; num_samples &amp;lt;&lt;span&gt; self.target_vocab_size:
      w &lt;/span&gt;= tf.get_variable(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;proj_w&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;, [size, self.target_vocab_size])
      w_t &lt;/span&gt;=&lt;span&gt; tf.transpose(w)
      b &lt;/span&gt;= tf.get_variable(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;proj_b&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;, [self.target_vocab_size])
      output_projection &lt;/span&gt;=&lt;span&gt; (w, b)
      &lt;/span&gt;&lt;span&gt;def&lt;/span&gt;&lt;span&gt; sampled_loss(inputs, labels):
        labels &lt;/span&gt;= tf.reshape(labels, [-1, 1&lt;span&gt;])
        &lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt; tf.nn.sampled_softmax_loss(w_t, b, inputs, labels, num_samples,
                self.target_vocab_size)
      softmax_loss_function &lt;/span&gt;=&lt;span&gt; sampled_loss
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Create the internal multi-layer cell for our RNN.&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 构建RNN&lt;/span&gt;
    single_cell =&lt;span&gt; tf.nn.rnn_cell.GRUCell(size)
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt;&lt;span&gt; use_lstm:
      single_cell &lt;/span&gt;=&lt;span&gt; tf.nn.rnn_cell.BasicLSTMCell(size)
    cell &lt;/span&gt;=&lt;span&gt; single_cell
    cell &lt;/span&gt;= tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.5&lt;span&gt;)
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; num_layers &amp;gt; 1&lt;span&gt;:
      cell &lt;/span&gt;= tf.nn.rnn_cell.MultiRNNCell([single_cell] *&lt;span&gt; num_layers)
    
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; The seq2seq function: we use embedding for the input and attention.&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Attention模型&lt;/span&gt;
    &lt;span&gt;def&lt;/span&gt;&lt;span&gt; seq2seq_f(encoder_inputs, decoder_inputs, do_decode):
      &lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt; tf.nn.seq2seq.embedding_attention_seq2seq(
          encoder_inputs, decoder_inputs, cell,
          num_encoder_symbols&lt;/span&gt;=&lt;span&gt;source_vocab_size,
          num_decoder_symbols&lt;/span&gt;=&lt;span&gt;target_vocab_size,
          embedding_size&lt;/span&gt;=&lt;span&gt;size,
          output_projection&lt;/span&gt;=&lt;span&gt;output_projection,
          feed_previous&lt;/span&gt;=&lt;span&gt;do_decode)
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Feeds for inputs.&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 给模型填充数据&lt;/span&gt;
    self.encoder_inputs =&lt;span&gt; []
    self.decoder_inputs &lt;/span&gt;=&lt;span&gt; []
    self.target_weights &lt;/span&gt;=&lt;span&gt; []
    &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt; xrange(buckets[-1][0]):  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Last bucket is the biggest one.&lt;/span&gt;
      self.encoder_inputs.append(tf.placeholder(tf.int32, shape=&lt;span&gt;[None],
                                                name&lt;/span&gt;=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;encoder{0}&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;.format(i)))
    &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt; xrange(buckets[-1][1] + 1&lt;span&gt;):
      self.decoder_inputs.append(tf.placeholder(tf.int32, shape&lt;/span&gt;=&lt;span&gt;[None],
                                                name&lt;/span&gt;=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;decoder{0}&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;.format(i)))
      self.target_weights.append(tf.placeholder(tf.float32, shape&lt;/span&gt;=&lt;span&gt;[None],
                                                name&lt;/span&gt;=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;weight{0}&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;.format(i)))
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Our targets are decoder inputs shifted by one.&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; targets值是解码器偏移1位&lt;/span&gt;
    targets = [self.decoder_inputs[i + 1&lt;span&gt;]
               &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt; xrange(len(self.decoder_inputs) - 1&lt;span&gt;)]
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Training outputs and losses.&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 训练模型输出&lt;/span&gt;
    &lt;span&gt;if&lt;/span&gt;&lt;span&gt; forward_only:
      self.outputs, self.losses &lt;/span&gt;=&lt;span&gt; tf.nn.seq2seq.model_with_buckets(
          self.encoder_inputs, self.decoder_inputs, targets,
          self.target_weights, buckets, &lt;/span&gt;&lt;span&gt;lambda&lt;/span&gt;&lt;span&gt; x, y: seq2seq_f(x, y, True),
          softmax_loss_function&lt;/span&gt;=&lt;span&gt;softmax_loss_function)
      &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; If we use output projection, we need to project outputs for decoding.&lt;/span&gt;
      &lt;span&gt;if&lt;/span&gt; output_projection &lt;span&gt;is&lt;/span&gt; &lt;span&gt;not&lt;/span&gt;&lt;span&gt; None:
        &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; b &lt;span&gt;in&lt;/span&gt;&lt;span&gt; xrange(len(buckets)):
          self.outputs[b] &lt;/span&gt;=&lt;span&gt; [
              tf.matmul(output, output_projection[0]) &lt;/span&gt;+ output_projection[1&lt;span&gt;]
              &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; output &lt;span&gt;in&lt;/span&gt;&lt;span&gt; self.outputs[b]
          ]
    &lt;/span&gt;&lt;span&gt;else&lt;/span&gt;&lt;span&gt;:
      self.outputs, self.losses &lt;/span&gt;=&lt;span&gt; tf.nn.seq2seq.model_with_buckets(
          self.encoder_inputs, self.decoder_inputs, targets,
          self.target_weights, buckets,
          &lt;/span&gt;&lt;span&gt;lambda&lt;/span&gt;&lt;span&gt; x, y: seq2seq_f(x, y, False),
          softmax_loss_function&lt;/span&gt;=&lt;span&gt;softmax_loss_function)
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Gradients and SGD update operation for training the model.&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 训练模型，更新梯度&lt;/span&gt;
    params =&lt;span&gt; tf.trainable_variables()
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; &lt;span&gt;not&lt;/span&gt;&lt;span&gt; forward_only:
      self.gradient_norms &lt;/span&gt;=&lt;span&gt; []
      self.updates &lt;/span&gt;=&lt;span&gt; []
      opt &lt;/span&gt;=&lt;span&gt; tf.train.AdamOptimizer()
      &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; b &lt;span&gt;in&lt;/span&gt;&lt;span&gt; xrange(len(buckets)):
        gradients &lt;/span&gt;=&lt;span&gt; tf.gradients(self.losses[b], params)
        clipped_gradients, norm &lt;/span&gt;=&lt;span&gt; tf.clip_by_global_norm(gradients,
                                                         max_gradient_norm)
        self.gradient_norms.append(norm)
        self.updates.append(opt.apply_gradients(
            zip(clipped_gradients, params), global_step&lt;/span&gt;=&lt;span&gt;self.global_step))
    self.saver &lt;/span&gt;=&lt;span&gt; tf.train.Saver(tf.global_variables())
  &lt;/span&gt;&lt;span&gt;def&lt;/span&gt;&lt;span&gt; step(self, session, encoder_inputs, decoder_inputs, target_weights,
           bucket_id, forward_only):
    &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;Run a step of the model feeding the given inputs.
    定义运行模型的每一步
    Args:
      session: tensorflow session to use.
      encoder_inputs: list of numpy int vectors to feed as encoder inputs.问句向量序列
      decoder_inputs: list of numpy int vectors to feed as decoder inputs.答句向量序列
      target_weights: list of numpy float vectors to feed as target weights.
      bucket_id: which bucket of the model to use.输入bucket_id
      forward_only: whether to do the backward step or only forward.是否只做前向传播
    Returns:
      A triple consisting of gradient norm (or None if we did not do backward),
      average perplexity, and the outputs.
    Raises:
      ValueError: if length of encoder_inputs, decoder_inputs, or
        target_weights disagrees with bucket size for the specified bucket_id.
    &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Check if the sizes match.&lt;/span&gt;
    encoder_size, decoder_size =&lt;span&gt; self.buckets[bucket_id]
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; len(encoder_inputs) !=&lt;span&gt; encoder_size:
      &lt;/span&gt;&lt;span&gt;raise&lt;/span&gt; ValueError(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Encoder length must be equal to the one in bucket,&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;
                       &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt; %d != %d.&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; %&lt;span&gt; (len(encoder_inputs), encoder_size))
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; len(decoder_inputs) !=&lt;span&gt; decoder_size:
      &lt;/span&gt;&lt;span&gt;raise&lt;/span&gt; ValueError(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Decoder length must be equal to the one in bucket,&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;
                       &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt; %d != %d.&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; %&lt;span&gt; (len(decoder_inputs), decoder_size))
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; len(target_weights) !=&lt;span&gt; decoder_size:
      &lt;/span&gt;&lt;span&gt;raise&lt;/span&gt; ValueError(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Weights length must be equal to the one in bucket,&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;
                       &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt; %d != %d.&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; %&lt;span&gt; (len(target_weights), decoder_size))
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Input feed: encoder inputs, decoder inputs, target_weights, as provided.&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 输入填充&lt;/span&gt;
    input_feed =&lt;span&gt; {}
    &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; l &lt;span&gt;in&lt;/span&gt;&lt;span&gt; xrange(encoder_size):
      input_feed[self.encoder_inputs[l].name] &lt;/span&gt;=&lt;span&gt; encoder_inputs[l]
    &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; l &lt;span&gt;in&lt;/span&gt;&lt;span&gt; xrange(decoder_size):
      input_feed[self.decoder_inputs[l].name] &lt;/span&gt;=&lt;span&gt; decoder_inputs[l]
      input_feed[self.target_weights[l].name] &lt;/span&gt;=&lt;span&gt; target_weights[l]
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Since our targets are decoder inputs shifted by one, we need one more.&lt;/span&gt;
    last_target =&lt;span&gt; self.decoder_inputs[decoder_size].name
    input_feed[last_target] &lt;/span&gt;= np.zeros([self.batch_size], dtype=&lt;span&gt;np.int32)
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Output feed: depends on whether we do a backward step or not.&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 输出填充：与是否有后向传播有关&lt;/span&gt;
    &lt;span&gt;if&lt;/span&gt; &lt;span&gt;not&lt;/span&gt;&lt;span&gt; forward_only:
      output_feed &lt;/span&gt;= [self.updates[bucket_id],  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Update Op that does SGD.&lt;/span&gt;
                     self.gradient_norms[bucket_id],  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Gradient norm.&lt;/span&gt;
                     self.losses[bucket_id]]  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Loss for this batch.&lt;/span&gt;
    &lt;span&gt;else&lt;/span&gt;&lt;span&gt;:
      output_feed &lt;/span&gt;= [self.losses[bucket_id]]  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Loss for this batch.&lt;/span&gt;
      &lt;span&gt;for&lt;/span&gt; l &lt;span&gt;in&lt;/span&gt; xrange(decoder_size):  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Output logits.&lt;/span&gt;
&lt;span&gt;        output_feed.append(self.outputs[bucket_id][l])
    outputs &lt;/span&gt;=&lt;span&gt; session.run(output_feed, input_feed)
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; &lt;span&gt;not&lt;/span&gt;&lt;span&gt; forward_only:
      &lt;/span&gt;&lt;span&gt;return&lt;/span&gt; outputs[1], outputs[2], None  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Gradient norm, loss, no outputs.有后向传播输出，梯度、损失值、None&lt;/span&gt;
    &lt;span&gt;else&lt;/span&gt;&lt;span&gt;:
      &lt;/span&gt;&lt;span&gt;return&lt;/span&gt; None, outputs[0], outputs[1:]  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; No gradient norm, loss, outputs.仅有前向传播输出，None，损失值，None&lt;/span&gt;
  &lt;span&gt;def&lt;/span&gt;&lt;span&gt; get_batch(self, data, bucket_id):
    &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;
    从指定桶获取一个批次随机数据，在训练每步(step)使用
    Args:参数
      data: a tuple of size len(self.buckets) in which each element contains
        lists of pairs of input and output data that we use to create a batch.长度为(self.buckets)元组，每个元素包含创建批次输入、输出数据对列表
      bucket_id: integer, which bucket to get the batch for.整数，从哪个bucket获取批次
    Returns:返回
      The triple (encoder_inputs, decoder_inputs, target_weights) for
      the constructed batch that has the proper format to call step(...) later.一个包含三项元组(encoder_inputs, decoder_inputs, target_weights)
    &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;
    encoder_size, decoder_size &lt;/span&gt;=&lt;span&gt; self.buckets[bucket_id]
    encoder_inputs, decoder_inputs &lt;/span&gt;=&lt;span&gt; [], []
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Get a random batch of encoder and decoder inputs from data,&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; pad them if needed, reverse encoder inputs and add GO to decoder.&lt;/span&gt;
    &lt;span&gt;for&lt;/span&gt; _ &lt;span&gt;in&lt;/span&gt;&lt;span&gt; xrange(self.batch_size):
      encoder_input, decoder_input &lt;/span&gt;=&lt;span&gt; random.choice(data[bucket_id])
      &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Encoder inputs are padded and then reversed.&lt;/span&gt;
      encoder_pad = [data_utils.PAD_ID] * (encoder_size -&lt;span&gt; len(encoder_input))
      encoder_inputs.append(list(reversed(encoder_input &lt;/span&gt;+&lt;span&gt; encoder_pad)))
      &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Decoder inputs get an extra &quot;GO&quot; symbol, and are padded then.&lt;/span&gt;
      decoder_pad_size = decoder_size - len(decoder_input) - 1&lt;span&gt;
      decoder_inputs.append([data_utils.GO_ID] &lt;/span&gt;+ decoder_input +&lt;span&gt;
                            [data_utils.PAD_ID] &lt;/span&gt;*&lt;span&gt; decoder_pad_size)
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Now we create batch-major vectors from the data selected above.&lt;/span&gt;
    batch_encoder_inputs, batch_decoder_inputs, batch_weights =&lt;span&gt; [], [], []
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Batch encoder inputs are just re-indexed encoder_inputs.&lt;/span&gt;
    &lt;span&gt;for&lt;/span&gt; length_idx &lt;span&gt;in&lt;/span&gt;&lt;span&gt; xrange(encoder_size):
      batch_encoder_inputs.append(
          np.array([encoder_inputs[batch_idx][length_idx]
                    &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; batch_idx &lt;span&gt;in&lt;/span&gt; xrange(self.batch_size)], dtype=&lt;span&gt;np.int32))
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Batch decoder inputs are re-indexed decoder_inputs, we create weights.&lt;/span&gt;
    &lt;span&gt;for&lt;/span&gt; length_idx &lt;span&gt;in&lt;/span&gt;&lt;span&gt; xrange(decoder_size):
      batch_decoder_inputs.append(
          np.array([decoder_inputs[batch_idx][length_idx]
                    &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; batch_idx &lt;span&gt;in&lt;/span&gt; xrange(self.batch_size)], dtype=&lt;span&gt;np.int32))
      &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Create target_weights to be 0 for targets that are padding.&lt;/span&gt;
      batch_weight = np.ones(self.batch_size, dtype=&lt;span&gt;np.float32)
      &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; batch_idx &lt;span&gt;in&lt;/span&gt;&lt;span&gt; xrange(self.batch_size):
        &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; We set weight to 0 if the corresponding target is a PAD symbol.&lt;/span&gt;
        &lt;span&gt;#&lt;/span&gt;&lt;span&gt; The corresponding target is decoder_input shifted by 1 forward.&lt;/span&gt;
        &lt;span&gt;if&lt;/span&gt; length_idx &amp;lt; decoder_size - 1&lt;span&gt;:
          target &lt;/span&gt;= decoder_inputs[batch_idx][length_idx + 1&lt;span&gt;]
        &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; length_idx == decoder_size - 1 &lt;span&gt;or&lt;/span&gt; target ==&lt;span&gt; data_utils.PAD_ID:
          batch_weight[batch_idx] &lt;/span&gt;= 0.0&lt;span&gt;
      batch_weights.append(batch_weight)
    &lt;/span&gt;&lt;span&gt;return&lt;/span&gt; batch_encoder_inputs, batch_decoder_inputs, batch_weights
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;训练模型。修改seq2seq.ini文件mode值“train”，execute.py训练。&lt;/p&gt;
&lt;p&gt;验证模型。修改seq2seq.ini文件mode值“test”，execute.py测试。&lt;/p&gt;

&lt;div class=&quot;cnblogs_code&quot; readability=&quot;226&quot;&gt;
&lt;pre&gt;
&lt;span&gt;from&lt;/span&gt; &lt;span&gt;__future__&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; absolute_import
&lt;/span&gt;&lt;span&gt;from&lt;/span&gt; &lt;span&gt;__future__&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; division
&lt;/span&gt;&lt;span&gt;from&lt;/span&gt; &lt;span&gt;__future__&lt;/span&gt; &lt;span&gt;import&lt;/span&gt;&lt;span&gt; print_function
&lt;/span&gt;&lt;span&gt;import&lt;/span&gt;&lt;span&gt; math
&lt;/span&gt;&lt;span&gt;import&lt;/span&gt;&lt;span&gt; os
&lt;/span&gt;&lt;span&gt;import&lt;/span&gt;&lt;span&gt; random
&lt;/span&gt;&lt;span&gt;import&lt;/span&gt;&lt;span&gt; sys
&lt;/span&gt;&lt;span&gt;import&lt;/span&gt;&lt;span&gt; time
&lt;/span&gt;&lt;span&gt;import&lt;/span&gt;&lt;span&gt; numpy as np
&lt;/span&gt;&lt;span&gt;from&lt;/span&gt; six.moves &lt;span&gt;import&lt;/span&gt; xrange  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; pylint: disable=redefined-builtin&lt;/span&gt;
&lt;span&gt;import&lt;/span&gt;&lt;span&gt; tensorflow as tf
&lt;/span&gt;&lt;span&gt;import&lt;/span&gt;&lt;span&gt; data_utils
&lt;/span&gt;&lt;span&gt;import&lt;/span&gt;&lt;span&gt; seq2seq_model
&lt;/span&gt;&lt;span&gt;try&lt;/span&gt;&lt;span&gt;:
    &lt;/span&gt;&lt;span&gt;from&lt;/span&gt; ConfigParser &lt;span&gt;import&lt;/span&gt;&lt;span&gt; SafeConfigParser
&lt;/span&gt;&lt;span&gt;except&lt;/span&gt;&lt;span&gt;:
    &lt;/span&gt;&lt;span&gt;from&lt;/span&gt; configparser &lt;span&gt;import&lt;/span&gt; SafeConfigParser &lt;span&gt;#&lt;/span&gt;&lt;span&gt; In Python 3, ConfigParser has been renamed to configparser for PEP 8 compliance.&lt;/span&gt;
gConfig =&lt;span&gt; {}
&lt;/span&gt;&lt;span&gt;def&lt;/span&gt; get_config(config_file=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;seq2seq.ini&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;):
    parser &lt;/span&gt;=&lt;span&gt; SafeConfigParser()
    parser.read(config_file)
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; get the ints, floats and strings&lt;/span&gt;
    _conf_ints = [ (key, int(value)) &lt;span&gt;for&lt;/span&gt; key,value &lt;span&gt;in&lt;/span&gt; parser.items(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;ints&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;) ]
    _conf_floats &lt;/span&gt;= [ (key, float(value)) &lt;span&gt;for&lt;/span&gt; key,value &lt;span&gt;in&lt;/span&gt; parser.items(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;floats&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;) ]
    _conf_strings &lt;/span&gt;= [ (key, str(value)) &lt;span&gt;for&lt;/span&gt; key,value &lt;span&gt;in&lt;/span&gt; parser.items(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;strings&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;) ]
    &lt;/span&gt;&lt;span&gt;return&lt;/span&gt; dict(_conf_ints + _conf_floats +&lt;span&gt; _conf_strings)
&lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; We use a number of buckets and pad to the closest one for efficiency.&lt;/span&gt;&lt;span&gt;
#&lt;/span&gt;&lt;span&gt; See seq2seq_model.Seq2SeqModel for details of how they work.&lt;/span&gt;
_buckets = [(5, 10), (10, 15), (20, 25), (40, 50&lt;span&gt;)]
&lt;/span&gt;&lt;span&gt;def&lt;/span&gt; read_data(source_path, target_path, max_size=&lt;span&gt;None):
  &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;Read data from source and target files and put into buckets.
  Args:
    source_path: path to the files with token-ids for the source language.
    target_path: path to the file with token-ids for the target language;
      it must be aligned with the source file: n-th line contains the desired
      output for n-th line from the source_path.
    max_size: maximum number of lines to read, all other will be ignored;
      if 0 or None, data files will be read completely (no limit).
  Returns:
    data_set: a list of length len(_buckets); data_set[n] contains a list of
      (source, target) pairs read from the provided data files that fit
      into the n-th bucket, i.e., such that len(source) &amp;lt; _buckets[n][0] and
      len(target) &amp;lt; _buckets[n][1]; source and target are lists of token-ids.
  &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;
  data_set &lt;/span&gt;= [[] &lt;span&gt;for&lt;/span&gt; _ &lt;span&gt;in&lt;/span&gt;&lt;span&gt; _buckets]
  with tf.gfile.GFile(source_path, mode&lt;/span&gt;=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;r&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;) as source_file:
    with tf.gfile.GFile(target_path, mode&lt;/span&gt;=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;r&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;) as target_file:
      source, target &lt;/span&gt;=&lt;span&gt; source_file.readline(), target_file.readline()
      counter &lt;/span&gt;=&lt;span&gt; 0
      &lt;/span&gt;&lt;span&gt;while&lt;/span&gt; source &lt;span&gt;and&lt;/span&gt; target &lt;span&gt;and&lt;/span&gt; (&lt;span&gt;not&lt;/span&gt; max_size &lt;span&gt;or&lt;/span&gt; counter &amp;lt;&lt;span&gt; max_size):
        counter &lt;/span&gt;+= 1
        &lt;span&gt;if&lt;/span&gt; counter % 100000 ==&lt;span&gt; 0:
          &lt;/span&gt;&lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;  reading data line %d&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; %&lt;span&gt; counter)
          sys.stdout.flush()
        source_ids &lt;/span&gt;= [int(x) &lt;span&gt;for&lt;/span&gt; x &lt;span&gt;in&lt;/span&gt;&lt;span&gt; source.split()]
        target_ids &lt;/span&gt;= [int(x) &lt;span&gt;for&lt;/span&gt; x &lt;span&gt;in&lt;/span&gt;&lt;span&gt; target.split()]
        target_ids.append(data_utils.EOS_ID)
        &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; bucket_id, (source_size, target_size) &lt;span&gt;in&lt;/span&gt;&lt;span&gt; enumerate(_buckets):
          &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; len(source_ids) &amp;lt; source_size &lt;span&gt;and&lt;/span&gt; len(target_ids) &amp;lt;&lt;span&gt; target_size:
            data_set[bucket_id].append([source_ids, target_ids])
            &lt;/span&gt;&lt;span&gt;break&lt;/span&gt;&lt;span&gt;
        source, target &lt;/span&gt;=&lt;span&gt; source_file.readline(), target_file.readline()
  &lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt; data_set
&lt;/span&gt;&lt;span&gt;def&lt;/span&gt;&lt;span&gt; create_model(session, forward_only):
  &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;Create model and initialize or load parameters&lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;
  model &lt;/span&gt;= seq2seq_model.Seq2SeqModel( gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;enc_vocab_size&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;dec_vocab_size&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], _buckets, gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;layer_size&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;num_layers&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;max_gradient_norm&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;batch_size&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;learning_rate_decay_factor&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], forward_only=&lt;span&gt;forward_only)
  &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;pretrained_model&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; &lt;span&gt;in&lt;/span&gt;&lt;span&gt; gConfig:
      model.saver.restore(session,gConfig[&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;pretrained_model&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;])
      &lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt; model
  ckpt &lt;/span&gt;= tf.train.get_checkpoint_state(gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;working_directory&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;])
  &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; ckpt &lt;span&gt;and&lt;/span&gt;&lt;span&gt; ckpt.model_checkpoint_path:
    &lt;/span&gt;&lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Reading model parameters from %s&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; %&lt;span&gt; ckpt.model_checkpoint_path)
    model.saver.restore(session, ckpt.model_checkpoint_path)
  &lt;/span&gt;&lt;span&gt;else&lt;/span&gt;&lt;span&gt;:
    &lt;/span&gt;&lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Created model with fresh parameters.&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;)
    session.run(tf.global_variables_initializer())
  &lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt; model
&lt;/span&gt;&lt;span&gt;def&lt;/span&gt;&lt;span&gt; train():
  &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; prepare dataset&lt;/span&gt;
  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 准备数据集&lt;/span&gt;
  &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Preparing data in %s&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; % gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;working_directory&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;])
  enc_train, dec_train, enc_dev, dec_dev, _, _ &lt;/span&gt;= data_utils.prepare_custom_data(gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;working_directory&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;],gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;train_enc&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;],gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;train_dec&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;],gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;test_enc&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;],gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;test_dec&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;],gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;enc_vocab_size&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;],gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;dec_vocab_size&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;])
  &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; setup config to use BFC allocator&lt;/span&gt;
  config =&lt;span&gt; tf.ConfigProto()
  config.gpu_options.allocator_type &lt;/span&gt;= &lt;span&gt;'&lt;/span&gt;&lt;span&gt;BFC&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;
  with tf.Session(config&lt;/span&gt;=&lt;span&gt;config) as sess:
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Create model.&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 构建模型&lt;/span&gt;
    &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Creating %d layers of %d units.&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; % (gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;num_layers&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;layer_size&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]))
    model &lt;/span&gt;=&lt;span&gt; create_model(sess, False)
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Read data into buckets and compute their sizes.&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 把数据读入桶(bucket)中，计算桶大小&lt;/span&gt;
    &lt;span&gt;print&lt;/span&gt; (&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Reading development and training data (limit: %d).&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;
           % gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;max_train_data_size&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;])
    dev_set &lt;/span&gt;=&lt;span&gt; read_data(enc_dev, dec_dev)
    train_set &lt;/span&gt;= read_data(enc_train, dec_train, gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;max_train_data_size&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;])
    train_bucket_sizes &lt;/span&gt;= [len(train_set[b]) &lt;span&gt;for&lt;/span&gt; b &lt;span&gt;in&lt;/span&gt;&lt;span&gt; xrange(len(_buckets))]
    train_total_size &lt;/span&gt;=&lt;span&gt; float(sum(train_bucket_sizes))
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; A bucket scale is a list of increasing numbers from 0 to 1 that we'll use&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; to select a bucket. Length of [scale[i], scale[i+1]] is proportional to&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; the size if i-th training bucket, as used later.&lt;/span&gt;
    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) /&lt;span&gt; train_total_size
                           &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt;&lt;span&gt; xrange(len(train_bucket_sizes))]
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; This is the training loop.&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 开始训练循环&lt;/span&gt;
    step_time, loss = 0.0, 0.0&lt;span&gt;
    current_step &lt;/span&gt;=&lt;span&gt; 0
    previous_losses &lt;/span&gt;=&lt;span&gt; []
    &lt;/span&gt;&lt;span&gt;while&lt;/span&gt;&lt;span&gt; True:
      &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Choose a bucket according to data distribution. We pick a random number&lt;/span&gt;
      &lt;span&gt;#&lt;/span&gt;&lt;span&gt; in [0, 1] and use the corresponding interval in train_buckets_scale.&lt;/span&gt;
      &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 随机生成一个0-1数，在生成bucket_id中使用&lt;/span&gt;
      random_number_01 =&lt;span&gt; np.random.random_sample()
      bucket_id &lt;/span&gt;= min([i &lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt;&lt;span&gt; xrange(len(train_buckets_scale))
                       &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; train_buckets_scale[i] &amp;gt;&lt;span&gt; random_number_01])
      &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Get a batch and make a step.&lt;/span&gt;
      &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 获取一个批次数据，进行一步训练&lt;/span&gt;
      start_time =&lt;span&gt; time.time()
      encoder_inputs, decoder_inputs, target_weights &lt;/span&gt;=&lt;span&gt; model.get_batch(
          train_set, bucket_id)
      _, step_loss, _ &lt;/span&gt;=&lt;span&gt; model.step(sess, encoder_inputs, decoder_inputs,
                                   target_weights, bucket_id, False)
      step_time &lt;/span&gt;+= (time.time() - start_time) / gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;steps_per_checkpoint&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
      loss &lt;/span&gt;+= step_loss / gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;steps_per_checkpoint&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]
      current_step &lt;/span&gt;+= 1
      &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Once in a while, we save checkpoint, print statistics, and run evals.&lt;/span&gt;
      &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 保存检查点文件，打印统计数据&lt;/span&gt;
      &lt;span&gt;if&lt;/span&gt; current_step % gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;steps_per_checkpoint&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] ==&lt;span&gt; 0:
        &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Print statistics for the previous epoch.&lt;/span&gt;
        perplexity = math.exp(loss) &lt;span&gt;if&lt;/span&gt; loss &amp;lt; 300 &lt;span&gt;else&lt;/span&gt; float(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;inf&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
        &lt;/span&gt;&lt;span&gt;print&lt;/span&gt; (&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;global step %d learning rate %.4f step-time %.2f perplexity &lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;
               &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;%.2f&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; %&lt;span&gt; (model.global_step.eval(), model.learning_rate.eval(),
                         step_time, perplexity))
        &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Decrease learning rate if no improvement was seen over last 3 times.&lt;/span&gt;
        &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 如果损失值在最近3次内没有再降低，减小学习率&lt;/span&gt;
        &lt;span&gt;if&lt;/span&gt; len(previous_losses) &amp;gt; 2 &lt;span&gt;and&lt;/span&gt; loss &amp;gt; max(previous_losses[-3&lt;span&gt;:]):
          sess.run(model.learning_rate_decay_op)
        previous_losses.append(loss)
        &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Save checkpoint and zero timer and loss.&lt;/span&gt;
        &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 保存检查点文件，计数器、损失值归零&lt;/span&gt;
        checkpoint_path = os.path.join(gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;working_directory&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;], &lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;seq2seq.ckpt&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;)
        model.saver.save(sess, checkpoint_path, global_step&lt;/span&gt;=&lt;span&gt;model.global_step)
        step_time, loss &lt;/span&gt;= 0.0, 0.0
        &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Run evals on development set and print their perplexity.&lt;/span&gt;
        &lt;span&gt;for&lt;/span&gt; bucket_id &lt;span&gt;in&lt;/span&gt;&lt;span&gt; xrange(len(_buckets)):
          &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; len(dev_set[bucket_id]) ==&lt;span&gt; 0:
            &lt;/span&gt;&lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;  eval: empty bucket %d&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; %&lt;span&gt; (bucket_id))
            &lt;/span&gt;&lt;span&gt;continue&lt;/span&gt;&lt;span&gt;
          encoder_inputs, decoder_inputs, target_weights &lt;/span&gt;=&lt;span&gt; model.get_batch(
              dev_set, bucket_id)
          _, eval_loss, _ &lt;/span&gt;=&lt;span&gt; model.step(sess, encoder_inputs, decoder_inputs,
                                       target_weights, bucket_id, True)
          eval_ppx &lt;/span&gt;= math.exp(eval_loss) &lt;span&gt;if&lt;/span&gt; eval_loss &amp;lt; 300 &lt;span&gt;else&lt;/span&gt; float(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;inf&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
          &lt;/span&gt;&lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;  eval: bucket %d perplexity %.2f&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; %&lt;span&gt; (bucket_id, eval_ppx))
        sys.stdout.flush()
&lt;/span&gt;&lt;span&gt;def&lt;/span&gt;&lt;span&gt; decode():
  with tf.Session() as sess:
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Create model and load parameters.&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 建立模型，定义超参数batch_size&lt;/span&gt;
    model =&lt;span&gt; create_model(sess, True)
    model.batch_size &lt;/span&gt;= 1  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; We decode one sentence at a time.一次只解码一个句子&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Load vocabularies.&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 加载词汇表文件&lt;/span&gt;
    enc_vocab_path = os.path.join(gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;working_directory&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;],&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;vocab%d.enc&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; % gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;enc_vocab_size&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;])
    dec_vocab_path &lt;/span&gt;= os.path.join(gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;working_directory&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;],&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;vocab%d.dec&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; % gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;dec_vocab_size&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;])
    enc_vocab, _ &lt;/span&gt;=&lt;span&gt; data_utils.initialize_vocabulary(enc_vocab_path)
    _, rev_dec_vocab &lt;/span&gt;=&lt;span&gt; data_utils.initialize_vocabulary(dec_vocab_path)
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Decode from standard input.&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 对标准输入句子解码&lt;/span&gt;
    sys.stdout.write(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;&amp;gt; &lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;)
    sys.stdout.flush()
    sentence &lt;/span&gt;=&lt;span&gt; sys.stdin.readline()
    &lt;/span&gt;&lt;span&gt;while&lt;/span&gt;&lt;span&gt; sentence:
      &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Get token-ids for the input sentence.&lt;/span&gt;
      &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 得到输入句子的token-ids&lt;/span&gt;
      token_ids =&lt;span&gt; data_utils.sentence_to_token_ids(tf.compat.as_bytes(sentence), enc_vocab)
      &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Which bucket does it belong to?&lt;/span&gt;
      &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 计算token_ids属于哪个桶(bucket)&lt;/span&gt;
      bucket_id = min([b &lt;span&gt;for&lt;/span&gt; b &lt;span&gt;in&lt;/span&gt;&lt;span&gt; xrange(len(_buckets))
                       &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; _buckets[b][0] &amp;gt;&lt;span&gt; len(token_ids)])
      &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Get a 1-element batch to feed the sentence to the model.&lt;/span&gt;
      &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 句子送入模型&lt;/span&gt;
      encoder_inputs, decoder_inputs, target_weights =&lt;span&gt; model.get_batch(
          {bucket_id: [(token_ids, [])]}, bucket_id)
      &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Get output logits for the sentence.&lt;/span&gt;
      _, _, output_logits =&lt;span&gt; model.step(sess, encoder_inputs, decoder_inputs,
                                       target_weights, bucket_id, True)
      &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; This is a greedy decoder - outputs are just argmaxes of output_logits.&lt;/span&gt;
      &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 贪心解码器，输出output_logits argmaxes&lt;/span&gt;
      outputs = [int(np.argmax(logit, axis=1)) &lt;span&gt;for&lt;/span&gt; logit &lt;span&gt;in&lt;/span&gt;&lt;span&gt; output_logits]
      &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; If there is an EOS symbol in outputs, cut them at that point.&lt;/span&gt;
      &lt;span&gt;if&lt;/span&gt; data_utils.EOS_ID &lt;span&gt;in&lt;/span&gt;&lt;span&gt; outputs:
        outputs &lt;/span&gt;=&lt;span&gt; outputs[:outputs.index(data_utils.EOS_ID)]
      &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Print out French sentence corresponding to outputs.&lt;/span&gt;
      &lt;span&gt;#&lt;/span&gt;&lt;span&gt; 打印与输出句子对应法语句子&lt;/span&gt;
      &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt; &lt;span&gt;&quot;&lt;/span&gt;.join([tf.compat.as_str(rev_dec_vocab[output]) &lt;span&gt;for&lt;/span&gt; output &lt;span&gt;in&lt;/span&gt;&lt;span&gt; outputs]))
      &lt;/span&gt;&lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;&amp;gt; &lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;, end=&lt;span&gt;&quot;&quot;&lt;/span&gt;&lt;span&gt;)
      sys.stdout.flush()
      sentence &lt;/span&gt;=&lt;span&gt; sys.stdin.readline()
&lt;/span&gt;&lt;span&gt;def&lt;/span&gt;&lt;span&gt; self_test():
  &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;Test the translation model.&lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;
  with tf.Session() as sess:
    &lt;/span&gt;&lt;span&gt;print&lt;/span&gt;(&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;Self-test for neural translation model.&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;)
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Create model with vocabularies of 10, 2 small buckets, 2 layers of 32.&lt;/span&gt;
    model = seq2seq_model.Seq2SeqModel(10, 10, [(3, 3), (6, 6)], 32, 2&lt;span&gt;,
                                       &lt;/span&gt;5.0, 32, 0.3, 0.99, num_samples=8&lt;span&gt;)
    sess.run(tf.initialize_all_variables())
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Fake data set for both the (3, 3) and (6, 6) bucket.&lt;/span&gt;
    data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6&lt;span&gt;])],
                [([&lt;/span&gt;1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6&lt;span&gt;])])
    &lt;/span&gt;&lt;span&gt;for&lt;/span&gt; _ &lt;span&gt;in&lt;/span&gt; xrange(5):  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Train the fake model for 5 steps.&lt;/span&gt;
      bucket_id = random.choice([0, 1&lt;span&gt;])
      encoder_inputs, decoder_inputs, target_weights &lt;/span&gt;=&lt;span&gt; model.get_batch(
          data_set, bucket_id)
      model.step(sess, encoder_inputs, decoder_inputs, target_weights,
                 bucket_id, False)
&lt;/span&gt;&lt;span&gt;def&lt;/span&gt; init_session(sess, conf=&lt;span&gt;'&lt;/span&gt;&lt;span&gt;seq2seq.ini&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;):
    &lt;/span&gt;&lt;span&gt;global&lt;/span&gt;&lt;span&gt; gConfig
    gConfig &lt;/span&gt;=&lt;span&gt; get_config(conf)
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Create model and load parameters.&lt;/span&gt;
    model =&lt;span&gt; create_model(sess, True)
    model.batch_size &lt;/span&gt;= 1  &lt;span&gt;#&lt;/span&gt;&lt;span&gt; We decode one sentence at a time.&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; Load vocabularies.&lt;/span&gt;
    enc_vocab_path = os.path.join(gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;working_directory&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;],&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;vocab%d.enc&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; % gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;enc_vocab_size&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;])
    dec_vocab_path &lt;/span&gt;= os.path.join(gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;working_directory&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;],&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;vocab%d.dec&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; % gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;dec_vocab_size&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;])
    enc_vocab, _ &lt;/span&gt;=&lt;span&gt; data_utils.initialize_vocabulary(enc_vocab_path)
    _, rev_dec_vocab &lt;/span&gt;=&lt;span&gt; data_utils.initialize_vocabulary(dec_vocab_path)
    &lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt; sess, model, enc_vocab, rev_dec_vocab
&lt;/span&gt;&lt;span&gt;def&lt;/span&gt;&lt;span&gt; decode_line(sess, model, enc_vocab, rev_dec_vocab, sentence):
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Get token-ids for the input sentence.&lt;/span&gt;
    token_ids =&lt;span&gt; data_utils.sentence_to_token_ids(tf.compat.as_bytes(sentence), enc_vocab)
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Which bucket does it belong to?&lt;/span&gt;
    bucket_id = min([b &lt;span&gt;for&lt;/span&gt; b &lt;span&gt;in&lt;/span&gt; xrange(len(_buckets)) &lt;span&gt;if&lt;/span&gt; _buckets[b][0] &amp;gt;&lt;span&gt; len(token_ids)])
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Get a 1-element batch to feed the sentence to the model.&lt;/span&gt;
    encoder_inputs, decoder_inputs, target_weights =&lt;span&gt; model.get_batch({bucket_id: [(token_ids, [])]}, bucket_id)
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Get output logits for the sentence.&lt;/span&gt;
    _, _, output_logits =&lt;span&gt; model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; This is a greedy decoder - outputs are just argmaxes of output_logits.&lt;/span&gt;
    outputs = [int(np.argmax(logit, axis=1)) &lt;span&gt;for&lt;/span&gt; logit &lt;span&gt;in&lt;/span&gt;&lt;span&gt; output_logits]
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; If there is an EOS symbol in outputs, cut them at that point.&lt;/span&gt;
    &lt;span&gt;if&lt;/span&gt; data_utils.EOS_ID &lt;span&gt;in&lt;/span&gt;&lt;span&gt; outputs:
        outputs &lt;/span&gt;=&lt;span&gt; outputs[:outputs.index(data_utils.EOS_ID)]
    &lt;/span&gt;&lt;span&gt;return&lt;/span&gt; &lt;span&gt;&quot;&lt;/span&gt; &lt;span&gt;&quot;&lt;/span&gt;.join([tf.compat.as_str(rev_dec_vocab[output]) &lt;span&gt;for&lt;/span&gt; output &lt;span&gt;in&lt;/span&gt;&lt;span&gt; outputs])
&lt;/span&gt;&lt;span&gt;if&lt;/span&gt; &lt;span&gt;__name__&lt;/span&gt; == &lt;span&gt;'&lt;/span&gt;&lt;span&gt;__main__&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; len(sys.argv) - 1&lt;span&gt;:
        gConfig &lt;/span&gt;= get_config(sys.argv[1&lt;span&gt;])
    &lt;/span&gt;&lt;span&gt;else&lt;/span&gt;&lt;span&gt;:
        &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; get configuration from seq2seq.ini&lt;/span&gt;
        gConfig =&lt;span&gt; get_config()
    &lt;/span&gt;&lt;span&gt;print&lt;/span&gt;(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;\n&amp;gt;&amp;gt; Mode : %s\n&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; %(gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;mode&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;]))
    &lt;/span&gt;&lt;span&gt;if&lt;/span&gt; gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;mode&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] == &lt;span&gt;'&lt;/span&gt;&lt;span&gt;train&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
        &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; start training&lt;/span&gt;
&lt;span&gt;        train()
    &lt;/span&gt;&lt;span&gt;elif&lt;/span&gt; gConfig[&lt;span&gt;'&lt;/span&gt;&lt;span&gt;mode&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;] == &lt;span&gt;'&lt;/span&gt;&lt;span&gt;test&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;:
        &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; interactive decode&lt;/span&gt;
&lt;span&gt;        decode()
    &lt;/span&gt;&lt;span&gt;else&lt;/span&gt;&lt;span&gt;:
        &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; wrong way to execute &quot;serve&quot;&lt;/span&gt;
        &lt;span&gt;#&lt;/span&gt;&lt;span&gt;   Use : &amp;gt;&amp;gt; python ui/app.py&lt;/span&gt;
        &lt;span&gt;#&lt;/span&gt;&lt;span&gt;           uses seq2seq_serve.ini as conf file&lt;/span&gt;
        &lt;span&gt;print&lt;/span&gt;(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;Serve Usage : &amp;gt;&amp;gt; python ui/app.py&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;)
        &lt;/span&gt;&lt;span&gt;print&lt;/span&gt;(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;# uses seq2seq_serve.ini as conf file&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em id=&quot;__mceDel&quot;&gt;基于文字智能机器人，结合语音识别，产生直接对话机器人。系统架构：&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em id=&quot;__mceDel&quot;&gt;人-&amp;gt;语音识别(ASR)-&amp;gt;自然语言理解(NLU)-&amp;gt;对话管理-&amp;gt;自然语言生成(NLG)-&amp;gt;语音合成(TTS)-&amp;gt;人。《中国人工智能学会通讯》2016年第6卷第1期。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;图灵机器人公司，提高对话和语义准确度，提升中文语境智能程度。竹间智能科技，研究记忆、自学习情感机器人，机器人真正理解多模式多渠道信息，高度拟人化回应，最理想自然语言交流模式交流。腾讯公司，社交对话数据。微信，最庞大自然语言交流语料库，利用庞大真实数据，结合小程序成为所有服务入口。&lt;/p&gt;
&lt;p&gt;参考资料：&lt;br/&gt;《TensorFlow技术解析与实战》&lt;/p&gt;
&lt;p&gt;欢迎推荐上海机器学习工作机会，我的微信：qingxingfengzi&lt;/p&gt;
&lt;p&gt;人工智能工作机会分割线-----------------------------------------&lt;/p&gt;
&lt;p&gt;杭州阿里 新零售淘宝基础架构平台：移动AI高级专家&lt;/p&gt;
&lt;/div&gt;



&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;



&lt;/div&gt;
</description>
<pubDate>Sun, 05 Nov 2017 16:24:00 +0000</pubDate>
<dc:creator>利炳根</dc:creator>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/libinggen/p/7790162.html</dc:identifier>
</item>
<item>
<title>从Matlab文件中读取mxArray类型变量-部分代码分析 - qifengQIAO</title>
<link>http://www.cnblogs.com/a1778735775/p/7790081.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/a1778735775/p/7790081.html</guid>
<description>&lt;p&gt;这是我做的笔记，看到这个代码时觉得处理有点妙，做笔记记录之。&lt;/p&gt;
&lt;p&gt;部分源代码：&lt;/p&gt;
&lt;p&gt;。。。。&lt;/p&gt;
&lt;p&gt;int main(int argc,char** argv)&lt;/p&gt;
&lt;p&gt;{&lt;/p&gt;
&lt;p&gt;　　char name[_FILE_NAME_LEN];&lt;/p&gt;
&lt;p&gt;　　int num=0;&lt;/p&gt;
&lt;p&gt;　　int nFlag1,nFlag2;&lt;/p&gt;
&lt;p&gt;　　printf(&quot;请输入要读取的文件名称（*.mat):&quot;);&lt;/p&gt;
&lt;p&gt;　　scanf(&quot;%s&quot;,name);&lt;/p&gt;
&lt;p&gt;　　while ((name[num++] != '\0') &amp;amp;&amp;amp; (num&amp;lt;=_FILE_NAME_LEN-1)) &lt;span&gt;;&lt;/span&gt;   //注意&lt;span&gt;后面是分号&lt;/span&gt;，当时我一直以为跟下面一句是连起来组成的一个循环，其实不是。&lt;/p&gt;
&lt;p&gt;　　num=num-1;  //下面的代码是文件名的处理。&lt;/p&gt;
&lt;p&gt;　　if (num&amp;gt;_FILE_NEME_LEN-5)  //为什么是减5 呢，后面代码可以知，这是为了为加上后缀留下空间。&lt;/p&gt;
&lt;p&gt;　　　　{&lt;/p&gt;
&lt;p&gt;　　　　　　printf(&quot;输入的文件名太长！\n&quot;);&lt;/p&gt;
&lt;p&gt;　　　　　　return 0;&lt;/p&gt;
&lt;p&gt;　　　　}&lt;/p&gt;
&lt;p&gt;　　nFlag1=strcmp(&lt;span&gt;name+num-4&lt;/span&gt;,&quot;.MAT&quot;);  //该如何理解呢？根据后面的判断，可以知前面的要大于后面的，假设成立，然后逆推。&lt;/p&gt;
&lt;p&gt;　　　　　　　　　　　　　　　　　　 　//name是数组名，是指向name[0]的指针，所以是数组的表示知识，&lt;/p&gt;
&lt;p&gt;　　　　　　　　　　　　　　　　　 　　//所以这个应该是处理用户输入的文件名中是否加入后缀的判断处理。&lt;/p&gt;
&lt;p&gt;　　nFlag2=strcmp(name+num-4,&quot;.mat&quot;);&lt;/p&gt;
&lt;p&gt;　　if(nFlag1&amp;amp;&amp;amp; nFlag2)&lt;/p&gt;
&lt;p&gt;　　{&lt;/p&gt;
&lt;p&gt;　　　　name[num]='.';&lt;/p&gt;
&lt;p&gt;　　　　name[name+1]='M';&lt;/p&gt;
&lt;p&gt;　　　　name[name+2]='A';&lt;/p&gt;
&lt;p&gt;　　　　name[name+3]='T';&lt;/p&gt;
&lt;p&gt;　　　　name[name+4]='\0';&lt;/p&gt;
&lt;p&gt;　　}&lt;/p&gt;
&lt;p&gt;　　analyze_matfile(name);  //自定义的目标函数&lt;/p&gt;
&lt;p&gt;　　getchar();&lt;/p&gt;
&lt;p&gt;　　retuen 0;&lt;/p&gt;
&lt;p&gt;}&lt;/p&gt;
</description>
<pubDate>Sun, 05 Nov 2017 16:15:00 +0000</pubDate>
<dc:creator>qifengQIAO</dc:creator>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/a1778735775/p/7790081.html</dc:identifier>
</item>
<item>
<title>SQL——按照季度，固定时间段，分组统计数据 - 才丶</title>
<link>http://www.cnblogs.com/Caucasian/p/7790024.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/Caucasian/p/7790024.html</guid>
<description>&lt;p&gt;　　最近在工作中接到了一个需求，要求统计&lt;strong&gt;当月以10天为一个周期，每个周期的数据汇总信息。&lt;/strong&gt;&lt;strong&gt;假设有一张表如下：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;表table_test中&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ID           AMOUNT         CREATE_DATE&lt;/p&gt;
&lt;p&gt;1            50                     2017-01-01&lt;/p&gt;
&lt;p&gt;2            50                     2017-01-09&lt;/p&gt;
&lt;p&gt;3            50                     2017-01-11&lt;/p&gt;
&lt;p&gt;4            50                     2017-01-19&lt;/p&gt;
&lt;p&gt;5            50                     2017-01-21&lt;/p&gt;
&lt;p&gt;6            50                     2017-01-22&lt;/p&gt;
&lt;p&gt;7            50                     2017-01-24&lt;/p&gt;
&lt;p&gt;　　相当于以CREATE_DATE&lt;strong&gt;为组条件，1 - 10，11 -20，21 - 月末，每一个时间段的AMOUNT字段的和。&lt;/strong&gt;面对这个需求，首先想到的解决方案为&lt;strong&gt;以日期作为参数。使用代码传参重复调用&lt;/strong&gt;，发现该方法效率极低，后来做出相应优化，使用 &lt;strong&gt;UNION ALL &lt;/strong&gt;去调用 sql如下&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;35&quot;&gt;
&lt;pre&gt;
&lt;span&gt;1&lt;/span&gt; &lt;span&gt;SELECT&lt;/span&gt; &lt;span&gt;SUM&lt;/span&gt;(AMOUT) &lt;span&gt;AS&lt;/span&gt;&lt;span&gt; totalAmount FORM TABLE_TEST 
&lt;/span&gt;&lt;span&gt;2&lt;/span&gt; &lt;span&gt;WHERE&lt;/span&gt; CREATE_DATE &lt;span&gt;BETWEEN&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;2017-01-01&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; &lt;span&gt;AND&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;2017-01-10&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;
&lt;span&gt;3&lt;/span&gt; &lt;span&gt;UNION&lt;/span&gt; &lt;span&gt;ALL&lt;/span&gt;
&lt;span&gt;4&lt;/span&gt; &lt;span&gt;SELECT&lt;/span&gt; &lt;span&gt;SUM&lt;/span&gt;(AMOUT) &lt;span&gt;AS&lt;/span&gt;&lt;span&gt; totalAmount FORM TABLE_TEST 
&lt;/span&gt;&lt;span&gt;5&lt;/span&gt; &lt;span&gt;WHERE&lt;/span&gt; CREATE_DATE &lt;span&gt;BETWEEN&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;2017-01-11&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; &lt;span&gt;AND&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;2017-01-20&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;
&lt;span&gt;6&lt;/span&gt; &lt;span&gt;UNION&lt;/span&gt; &lt;span&gt;ALL&lt;/span&gt;
&lt;span&gt;7&lt;/span&gt; &lt;span&gt;SELECT&lt;/span&gt; &lt;span&gt;SUM&lt;/span&gt;(AMOUT) &lt;span&gt;AS&lt;/span&gt;&lt;span&gt; totalAmount FORM TABLE_TEST 
&lt;/span&gt;&lt;span&gt;8&lt;/span&gt; &lt;span&gt;WHERE&lt;/span&gt; CREATE_DATE &lt;span&gt;BETWEEN&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;2017-01-21&lt;/span&gt;&lt;span&gt;'&lt;/span&gt; &lt;span&gt;AND&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;2017-01-30&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;　　后来数据量大了后发现，效率有些跟不上。就想到了代替的优化SQL如下&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;36&quot;&gt;
&lt;pre&gt;
&lt;span&gt;1&lt;/span&gt; &lt;span&gt;SELECT&lt;/span&gt;
&lt;span&gt;2&lt;/span&gt;     &lt;span&gt;sum&lt;/span&gt;&lt;span&gt;(amount) totalAmount,
&lt;/span&gt;&lt;span&gt;3&lt;/span&gt;     &lt;span&gt;floor&lt;/span&gt;((&lt;span&gt;day&lt;/span&gt;(CREATE_DATE) &lt;span&gt;-&lt;/span&gt; &lt;span&gt;day&lt;/span&gt;(NOW()))&lt;span&gt;/&lt;/span&gt;&lt;span&gt;10&lt;/span&gt;) &lt;span&gt;AS&lt;/span&gt;&lt;span&gt; TIMESAPN
&lt;/span&gt;&lt;span&gt;4&lt;/span&gt; &lt;span&gt;FROM&lt;/span&gt;
&lt;span&gt;5&lt;/span&gt; &lt;span&gt;    table_test
&lt;/span&gt;&lt;span&gt;6&lt;/span&gt; &lt;span&gt;where&lt;/span&gt; &lt;span&gt;MONTH&lt;/span&gt;(create_date) &lt;span&gt;=&lt;/span&gt; &lt;span&gt;MONTH&lt;/span&gt;&lt;span&gt;(now())
&lt;/span&gt;&lt;span&gt;7&lt;/span&gt; &lt;span&gt;GROUP&lt;/span&gt; &lt;span&gt;BY&lt;/span&gt;
&lt;span&gt;8&lt;/span&gt;     TIMESAPN --该sql为mysql语句,如果为其他数据库可替换为其他数据库函数
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;　　关键点在于：&lt;/p&gt;
&lt;pre&gt;
floor((day(CREATE_DATE) - day(NOW()))/10) AS&lt;span&gt; TIMESAPN&lt;br/&gt;当 TIMESAPN 为 0 时则在第一个周期，为1则在第二个周期，以此类推，最后在以该字段为分组条件。完美解决效率问题。&lt;strong&gt;可以适当的想一下如果该需求改成按照季度统计。只需要截取日期的月份/4则可以完美解决。也就是说这个sql&lt;br/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;基本可以解决按照一段时间去统计的大部分需求。（该sql为博主自己想出来的，适用于本需求。如果有什么更好的解决方法可以留言。互相学习）&lt;/strong&gt;
&lt;/pre&gt;</description>
<pubDate>Sun, 05 Nov 2017 15:56:00 +0000</pubDate>
<dc:creator>才丶</dc:creator>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/Caucasian/p/7790024.html</dc:identifier>
</item>
<item>
<title>【每天半小时学框架】——React.js的模板语法与组件概念 - 卡尔西法calcifer</title>
<link>http://www.cnblogs.com/wq1994/p/7790018.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/wq1994/p/7790018.html</guid>
<description>&lt;p&gt;       【&lt;span&gt;重点提前说：组件化与虚拟DOM是React.js的核心理念！】&lt;/span&gt;&lt;/p&gt;&lt;p&gt;       先抛出一个论题：在React.js中，JSX语法提倡将 HTML 和 CSS 全都写入到JavaScript 中是代码书写规范中的&quot;资本主义复辟”吗？&lt;span class=&quot;RichText CopyrightRichText-richText&quot;&gt;react值得推荐的地方就是组件和virtualdom，前者解决多团队协作复杂前端的问题，后者使dom操作到视图刷新变得现实。&lt;/span&gt;对于React.js大家褒贬不一，脑残粉极力捧吹，而黑粉则是一昧的踩低。既然这样，那我们就自己学习使用，来下个定论吧~&lt;/p&gt;&lt;p&gt;&lt;span&gt;一、什么是React.js&lt;/span&gt;&lt;br/&gt;                React 是一个用于构建[用户界面]的 JAVASCRIPT 库，t主要用于构建UI，很多人认为 React 是 MVC 中的 V（视图）。&lt;br/&gt;                React 起源于 Facebook 的内部项目，用来架设 Instagram 的网站，并于 2013 年 5 月开源。&lt;br/&gt;&lt;span&gt; 二、React有哪些 特点？&lt;/span&gt;&lt;br/&gt;              ● 1.声明式设计 −React采用声明范式，可以轻松描述应用。&lt;br/&gt;              ● 2.高效 −React通过对DOM的模拟，最大限度地减少与DOM的交互。--虚拟DOM结构&lt;br/&gt;              ● 3.灵活 −React可以与已知的库或框架很好地配合。&lt;br/&gt;              ● 4.JSX − JSX 是 JavaScript 语法的扩展。React 开发不一定使用 JSX ，但我们建议使用它。&lt;br/&gt;              ● 5.组件 − 通过 React 构建组件，使得代码更加容易得到复用，能够很好的应用在大项目的开发中。&lt;br/&gt;              ● 6.单向响应的数据流 − React 实现了单向响应的数据流，从而减少了重复代码，这也是它为什么比传统数据绑定更简单&lt;br/&gt;&lt;span&gt;三、简单的解释几个概念：&lt;/span&gt;&lt;br/&gt;              ① &lt;span&gt;虚拟DOM（Virtual DOM）机制&lt;/span&gt;：对于每一个组件，React会在内存中构建一个相对应的DOM树，&lt;br/&gt;                          基于React开发时所有的DOM构造都是通过虚拟DOM进行，每当组件的状态发生变化时，React都会重新构建整个DOM数据。&lt;br/&gt;                          然后将当前的整个DOM树和上一次的DOM树进行对比，得出DOM结构变化的部分(Patchs)，然后将这些Patchs再更新到&lt;br/&gt;                          真实DOM中。     &lt;br/&gt;                     优点：避免了当页面数据发生变化时，DOM也被全部更新一遍并进行重新渲染。&lt;br/&gt;                                 整个过程都是在内存中进行，减少了不必要的性能开销，因此是非常高效的。   &lt;br/&gt;              ② &lt;span&gt;JSX语法：&lt;/span&gt;&lt;br/&gt;                      定义：JSX=JavaScript XML，是一种在React组件内部构建标签的类XML语法。&lt;br/&gt;                             React在不使用JSX的情况下一样可以工作，但是使用JSX可以提高组件的可读性，增强JS语义，结构清晰，抽象程度高，代码模块化。&lt;br/&gt;                      特点：&lt;br/&gt;                      1、元素名首字母大写&lt;br/&gt;                      2、符合嵌套规则&lt;br/&gt;                      3、可以写入求值表达式&lt;br/&gt;                      4、驼峰式命名&lt;br/&gt;                      5、不能使用javascript原生函数的一些关键词，如for和class。需要替换成htmlFor和className&lt;br/&gt;                      6、HTML语言直接写在 JavaScript 语言之中，不加任何引号，它允许 HTML 与 JavaScript 的混写.。&lt;br/&gt;                      优点：&lt;br/&gt;                     1、JSX 执行更快。&lt;br/&gt;                     2、它是类型安全的，在编译过程中就能发现错误。&lt;br/&gt;                     3、使用 JSX 编写模板更加简单快速。&lt;br/&gt;              ③ JSX 的基本语法规则：&lt;br/&gt;                                                      遇到 HTML 标签（以 &amp;lt; 开头），就用 HTML 规则解析；&lt;br/&gt;                                                  遇到代码块（以 { 开头），就用 JavaScript 规则解析；&lt;br/&gt;            &lt;br/&gt;&lt;span&gt;四、 React.js/Angular.js/Vue.js的对比&lt;/span&gt;&lt;br/&gt;           &lt;span&gt; 1、数据流(数据绑定)&lt;/span&gt;&lt;br/&gt;                ① Angular 使用双向绑定即：界面的操作能实时反映到数据，数据的变更能实时展现到界面。&lt;br/&gt;                ② Vue 也支持双向绑定，默认为单向绑定，数据从父组件单向传给子组件。&lt;br/&gt;                ③  React推崇的是函数式编程和单向数据流&lt;br/&gt;                         即给定原始界面（或数据），施加一个变化，就能推导出另外一个状态（界面或者数据的更新）。&lt;br/&gt;            &lt;span&gt;2、视图渲染&lt;/span&gt;&lt;br/&gt;                ① AngularJS的工作原理是:HTML模板将会被浏览器解析到DOM中, DOM结构成为AngularJS编译器的输入&lt;br/&gt;                                                          (NG框架是在DOM加载完成之后, 才开始起作用的)&lt;br/&gt;                ② React 的渲染建立在 Virtual DOM 上，一种在内存中描述 DOM 树状态的数据结构。&lt;br/&gt;                                    当状态发生变化时，React 重新渲染 Virtual DOM，比较计算之后给真实 DOM 打补丁。&lt;br/&gt;                ③ Vue.js 不使用 Virtual DOM 而是使用真实 DOM 作为模板，数据绑定到真实节点。&lt;br/&gt;                &lt;br/&gt;            &lt;span&gt;3、模块化与组件化&lt;/span&gt;&lt;br/&gt;                 ① Angular 依赖注入来解决模块之间的依赖问题&lt;br/&gt;                 ② Vue.js  指令只封装 DOM 操作，而组件代表一个自给自足的独立单元 —— 有自己的视图和数据逻辑操作。&lt;br/&gt;                 ③ React构建于组件之上，重要属性props,state。一个组件就是通过这两个属性的值在 render 方法里面生成这个组件对应的 HTML 结构。&lt;br/&gt;             &lt;span&gt;4、语法与代码风格&lt;/span&gt;&lt;br/&gt;                  ①Angular 2 依然保留以 HTML 为中心。Angular 2 将 “JS” 嵌入 HTML。&lt;br/&gt;                  ② Vue.js  Vue 也是以 HTML 为中心，将 “JS” 嵌入 HTML，但是进阶之后推荐的是使用 webpack + vue-loader 的单文件组件格式。&lt;br/&gt;                  ③ React 推荐的做法是 JSX + inline style，也就是把 HTML 和 CSS 全都整进 JavaScript&lt;/p&gt;&lt;p&gt;在文件的body区域，我们只需要写一个简单的DIV ，给它一个ID即可。&lt;/p&gt;&lt;div readability=&quot;88.5&quot;&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/1209629/201711/1209629-20171105231349404-121963295.png&quot; alt=&quot;&quot;/&gt;&lt;p&gt;之后创建script标签，我们的核心JSX语法就写在标签内部：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/1209629/201711/1209629-20171105231509201-1901211738.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;这里需要注意的地方是：&amp;lt;script&amp;gt; 标签的 type 属性为 text/babel 。凡是使用 JSX 的地方，都要加上 type=&quot;text/babel&quot; 。&lt;/p&gt;
&lt;p&gt;&lt;span&gt;tips&lt;/span&gt;:    在react 0.14前，浏览器端实现对jsx的编译依赖jsxtransformer.js   在react 0.14后，这个依赖的库改为browser.js。 页面script标签的type也由text/jsx改为text/babel。&lt;/p&gt;
&lt;p&gt;接下来我们写一个简单的小案例，利用React.js打印输出语句，代码如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;36&quot;&gt;
&lt;pre&gt;
 &amp;lt;script type=&quot;text/babel&quot;&amp;gt;
        &lt;span&gt;/*&lt;/span&gt;&lt;span&gt;一、基本结构：
         ReactDOM.render() 是react中的最基本的方法，作用是将模板转为HTML语言，并将其插入到DOM节点中。
        &lt;/span&gt;&lt;span&gt;*/&lt;/span&gt;&lt;span&gt;
          ReactDOM.render(&lt;/span&gt;&lt;span&gt;//&lt;/span&gt;&lt;span&gt;render:渲染的意思&lt;/span&gt;
              &amp;lt;h1&amp;gt;你好，React!&amp;lt;/h1&amp;gt;,
             &lt;span&gt;//&lt;/span&gt;&lt;span&gt;必须使用JavaScript原生的getElementByID方法，不能使用jQuery来选取DOM节点；&lt;/span&gt;
              document.getElementById(&quot;example1&quot;&lt;span&gt;)
              )&lt;br/&gt;&amp;lt;/script&amp;gt;&lt;br/&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;使用上面语句就可以打印输出语句“你好，React!”，这就实现了简单的案例。&lt;/p&gt;
      &lt;img src=&quot;http://images2017.cnblogs.com/blog/1209629/201711/1209629-20171105232102513-1977626866.png&quot; alt=&quot;&quot;/&gt;&lt;p&gt;接下来我们介绍一下，React.js中的&lt;span&gt;CSS样式书写方式&lt;/span&gt;，与原生分离式写法有很大不同：&lt;/p&gt;
&lt;p&gt;① React 推荐使用内联样式！(使用小驼峰法则)&lt;br/&gt;② React 会在指定元素数字后自动添加 px 。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;39&quot;&gt;
&lt;pre&gt;
 &amp;lt;script type=&quot;text/babel&quot;&amp;gt;
             &lt;span&gt;var&lt;/span&gt; myStyle ={ &lt;span&gt;//&lt;/span&gt;&lt;span&gt;传入一个样式数组&lt;/span&gt;
                fontSize: 100&lt;span&gt;,
                color: &lt;/span&gt;'red'&lt;span&gt;
            };
             &lt;/span&gt;&lt;span&gt;var&lt;/span&gt; testStyle =&lt;span&gt;{ 
                width:&lt;/span&gt;800&lt;span&gt;,
                heigth:&lt;/span&gt;500&lt;span&gt;,
                backgroundColor:&lt;/span&gt;&quot;yellow&quot;&lt;span&gt;
            };
        ReactDOM.render(
            &lt;/span&gt;&amp;lt;div style = {testStyle}&amp;gt;
             &amp;lt;h1 style = {myStyle}&amp;gt;这段文字使用了内联样式&amp;lt;/h1&amp;gt;
             &amp;lt;/div&amp;gt;,
             document.getElementById('example'&lt;span&gt;)
        )
  &lt;/span&gt;&amp;lt;/script&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;显示效果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/1209629/201711/1209629-20171105232354248-242285045.png&quot; alt=&quot;&quot; width=&quot;371&quot; height=&quot;92&quot;/&gt;&lt;/p&gt;
&lt;p&gt;接下来我们在模板插入 JavaScript 变量，以数组为例，看一下会是什么效果?&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;38&quot;&gt;
&lt;pre&gt;
 &amp;lt;script type=&quot;text/babel&quot;&amp;gt;
       &lt;span&gt;var&lt;/span&gt; arr =&lt;span&gt; [
          &lt;/span&gt;&amp;lt;h1 key=&quot;1&quot;&amp;gt;这是数组元素1&amp;lt;/h1&amp;gt;,
          &amp;lt;h2 key=&quot;2&quot;&amp;gt;这是数组元素2&amp;lt;/h2&amp;gt;,
          &amp;lt;h3 key=&quot;3&quot;&amp;gt;这是数组元素3&amp;lt;/h3&amp;gt;,
&lt;span&gt;          ];
       ReactDOM.render(
                &lt;/span&gt;&amp;lt;div&amp;gt;{arr}&amp;lt;/div&amp;gt;,
              document.getElementById(&quot;example&quot;&lt;span&gt;)
          )
                
  &lt;/span&gt;&amp;lt;/script&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;代码的运行结果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/1209629/201711/1209629-20171105232728263-1753775634.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;以上我们可以看到JSX 允许直接在模板插入 JavaScript 变量。如果这个变量是一个数组，则会展开这个数组的所有成员。&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;span&gt;三、React.js中的组件&lt;/span&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;span&gt; 一、什么是组件？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;组件化思想在&lt;span&gt;React.js&lt;/span&gt;中很重要，eact 允许将代码封装成组件，然后像插入普通 HTML标签一样，在网页中插入这个组件。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;二、组件的创建以及输出&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
&lt;span&gt;&lt;span&gt; 创建：&lt;/span&gt;&lt;/span&gt;&lt;br/&gt;&lt;span&gt;var&lt;/span&gt; HelloMessage =&lt;span&gt; React.createClass({
     &lt;/span&gt;                     render:&lt;span&gt;function&lt;/span&gt;&lt;span&gt; (){
     &lt;/span&gt;                            &lt;span&gt;return&lt;/span&gt;
     &lt;span&gt;                                            }
     &lt;/span&gt;&lt;span&gt;                                })
 &lt;/span&gt; 输出：使用伪标签 实例化组件类并输出信息   &amp;lt;HelloMessage /&amp;gt; 
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;span&gt;下面我们创建一个组件来实现一个简单的功能：&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;36&quot;&gt;
&lt;pre readability=&quot;6&quot;&gt;
 &amp;lt;script type=&quot;text/babel&quot;&amp;gt;
         &lt;span&gt;var&lt;/span&gt; HelloMessage =&lt;span&gt; React.createClass({
            render: &lt;/span&gt;&lt;span&gt;function&lt;/span&gt;&lt;span&gt;() {
            &lt;/span&gt;&lt;span&gt;return&lt;/span&gt; &amp;lt;h1&amp;gt;这是一个自定义组件---{&lt;span&gt;this&lt;/span&gt;.props.name}---{&lt;span&gt;this&lt;/span&gt;.props.age}&amp;lt;/h1&amp;gt;
&lt;span&gt;//&lt;/span&gt;&lt;span&gt;                        &amp;lt;p&amp;gt;这是个测试标签&amp;lt;/p&amp;gt;;&lt;/span&gt;
&lt;span readability=&quot;6&quot;&gt;          }
        });&lt;br/&gt;&lt;span&gt;            //输出组件&lt;/span&gt;
         ReactDOM.render(&lt;p&gt;&amp;lt;HelloMessage name = &quot;这是组件的name属性！&quot;   age = &quot;这是组件的age属性！&quot;/&amp;gt;,&lt;br/&gt;&lt;span&gt; //在调用组件的时候，如果想要多次调用组件，也需要给组件设置一个根标签&lt;/span&gt;&lt;br/&gt;&amp;lt;div&amp;gt;&lt;br/&gt;&amp;lt;HelloMessage name=&quot;这是组件的name属性！&quot; /&amp;gt;&lt;br/&gt;&amp;lt;HelloMessage age=&quot;这是组件的age属性！&quot; /&amp;gt;&lt;br/&gt;&amp;lt;/div&amp;gt;，&lt;br/&gt;document.getElementById('example')&lt;br/&gt;);  
  &lt;/p&gt;&lt;/span&gt;&amp;lt;/script&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上面代码运行结果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/1209629/201711/1209629-20171105233911779-101770235.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;在上述代码中，如果我们将 &lt;span&gt;&amp;lt;p&amp;gt;这是个测试标签&amp;lt;/p&amp;gt; &lt;span&gt;放开控制台会报错如下&lt;/span&gt;：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/1209629/201711/1209629-20171105234121185-1890118961.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;原因是因为组件类只能包含一个顶层标签(根标签)，可以嵌套标签例如我们可以将上述组件类写成如下格式：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
        &lt;span&gt;var&lt;/span&gt; HelloMessage =&lt;span&gt; React.createClass({
           render: &lt;/span&gt;&lt;span&gt;function&lt;/span&gt;&lt;span&gt;() {
            &lt;/span&gt;&lt;span&gt;return&lt;/span&gt; &amp;lt;h1&amp;gt;&lt;span&gt;
                        这是一个自定义组件&lt;/span&gt;---{&lt;span&gt;this&lt;/span&gt;.props.name}---{&lt;span&gt;this&lt;/span&gt;&lt;span&gt;.props.age}
                        &lt;/span&gt;&amp;lt;p&amp;gt;这是个测试标签&amp;lt;/p&amp;gt;
                        &amp;lt;/h1&amp;gt;
&lt;span&gt;         }
        }); &lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;运行时就不会再报错，结果如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/1209629/201711/1209629-20171105234612904-1764489230.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;在声明与调用组件的时候我们需要注意以下几点：&lt;/p&gt;
&lt;p&gt;&lt;span&gt;注意：&lt;/span&gt;&lt;br/&gt;     ① 自定义的 React组件类名以大写字母开头，使用大驼峰法则命名！&lt;br/&gt;     ② 所有的组件都必须拥有自己的render！&lt;br/&gt;   ③ 组件类只能包含一个顶层标签(根标签)，可以嵌套标签！&lt;br/&gt;     ④ 组件可以任意加入属性，其属性属可以在组件类的 this.props.对象上获取。(组件的所有子节点)&lt;br/&gt;        Tips: 在添加属性时，&lt;br/&gt;               class 属性需要写成 className ，&lt;br/&gt;                for 属性需要写成 htmlFor ，&lt;br/&gt;               这是因为 class 和 for 是 JavaScript 的保留字&lt;/p&gt;
&lt;p&gt;&lt;span&gt;复合组件&lt;/span&gt;：创建多个组件来合成一个组件，即把父组件的不同功能点进行分离&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;36&quot;&gt;
&lt;pre&gt;
 &amp;lt;script type=&quot;text/babel&quot;&amp;gt;
          &lt;span&gt;var&lt;/span&gt; BaseModule =&lt;span&gt; React.createClass({
                render:&lt;/span&gt;&lt;span&gt;function&lt;/span&gt;&lt;span&gt;(){
                    &lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt;(
                        &lt;/span&gt;&amp;lt;div&amp;gt;
                           &amp;lt;Module title={&lt;span&gt;this&lt;/span&gt;.props.title} /&amp;gt;
                          {&lt;span&gt;/*&lt;/span&gt;&lt;span&gt; &amp;lt;h2&amp;gt;作品:{this.props.title}&amp;lt;/h2&amp;gt;&lt;/span&gt;&lt;span&gt;*/&lt;/span&gt;&lt;span&gt;}
                           &lt;/span&gt;&amp;lt;p&amp;gt;作者:{&lt;span&gt;this&lt;/span&gt;.props.author}&amp;lt;/p&amp;gt;
                        &amp;lt;/div&amp;gt;
&lt;span&gt;                    )
                }
             });
             &lt;/span&gt;&lt;span&gt;var&lt;/span&gt; Module =&lt;span&gt; React.createClass({
                render:&lt;/span&gt;&lt;span&gt;function&lt;/span&gt;&lt;span&gt;(){
                    &lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt;(
                        &lt;/span&gt;&amp;lt;h2&amp;gt;{&lt;span&gt;this&lt;/span&gt;.props.title}&amp;lt;/h2&amp;gt;
&lt;span&gt;                    )
                }
            });
            ReactDOM.render(
                &lt;/span&gt;&amp;lt;BaseModule title=&quot;围城&quot; author=&quot;钱钟书&quot; /&amp;gt;,
                document.getElementById('container'&lt;span&gt;)
            );
   &lt;/span&gt;&amp;lt;/script&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上述复合组件运行结果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/1209629/201711/1209629-20171105235003357-329369665.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;好了。第一篇React的介绍就先这么多了，通过这些实例我们也大致了解了React所谓混写的概念以及其便捷性，和Vue等框架孰优孰劣现在还不能太早下定论，在以后的几篇文章里我们将会更深一步的了解下它们的异同再做比较。&lt;/p&gt;
&lt;/div&gt;</description>
<pubDate>Sun, 05 Nov 2017 15:55:00 +0000</pubDate>
<dc:creator>卡尔西法calcifer</dc:creator>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/wq1994/p/7790018.html</dc:identifier>
</item>
<item>
<title>html-webpack-plugin的使用 - 鲁小胖</title>
<link>http://www.cnblogs.com/lpggo/p/7790000.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/lpggo/p/7790000.html</guid>
<description>&lt;p&gt;　　　　使用前第一步：npm install 安装html-webpack-plugin --save--dev || --save  (tips:--save--dev跟--save最大的区别就是--dev安装是为了仅仅在develop开发环境所依赖的，线上运行并不需要安装也能运行的依赖)，献上webpack中文文档地址：=》http://www.jqhtml.com/7626.html&lt;/p&gt;
&lt;p&gt;next=&amp;gt;安装完之后需要在webpack配置文件webpack.base.config中引用该依赖,exp:var htmlWebpackPlugin=required ('html-webpack-plugin'),引入之后可以在配置中的plugins属性中new一个htmlWebpackPlugin({})后使用命令npm run  webpack会发现生成了一个html模板，证明插件使用成功了。&lt;/p&gt;
&lt;p&gt;　　　　接下来便是webpack配置中的plugins选项new 出来的htmlWebpackPlugin的参数介绍了：&lt;/p&gt;
&lt;p&gt;　　　　　　1）：可以通过template属性指定webpack打包后的输出模板文件：=》template:index.html这样子便会把入口的Index.html文件单做模板来打包输出。&lt;/p&gt;
&lt;p&gt;　　　　　　2）：可以通过filename属性指定输出文件名：=》filename:'index-[hash].html'指定文件名index+哈希值。(tips:chunkHash跟hash的最大区别是hash代表compliation的哈希值，而compliation对象会　　　　　　在任何一个要被打包的文件被改动后重新生成，也就是说单独修改一个文件会影响整个文件的compliation对象的重构，这样子打包后原来的未经改动的文件缓存也就会被新生成 的打包文件所覆　　　　　　　盖，而chunkHash则会是根据具体模块文件的内容计算所得的hash值，任何一个文件的更改只会影响到它本身的哈希值，并不会对其它文件哈希值造成影响，两者是独立的。本内容参见　　　　　　　　　　http://www.cnblogs.com/ihardcoder/p/5623411.html)。&lt;/p&gt;
&lt;p&gt;　　　　　　3）：可以通过inject属性指定打包输出的模板内容嵌入的位置：=》inject:'body'这样子便会把输出 的内容嵌入到body标签内&lt;/p&gt;
&lt;p&gt;　　　　　　4）：可以通过title属性可自定义标题名，并在模板中通过&amp;lt;%= htmlWebpackPlugin.options.title%&amp;gt;来引用到title的值并在打包后的文件中输出。&lt;/p&gt;
&lt;p&gt;　　　　　　5）：data属性同上。&lt;/p&gt;
&lt;p&gt;　　　　　　6）：可以在模板文件中使用&amp;lt;% for (var key in htmlWebpackPlugin) {%&amp;gt;&amp;lt;%=key%&amp;gt;&amp;lt;%}%&amp;gt;在打包文件中输出failes跟options两个对象，同时也可以以同样方式分别输出两个对象的所有值。&lt;/p&gt;
&lt;p&gt;　　　　　　7）：可以在模板文件中使用&amp;lt;script src=&quot;&amp;lt;%=htmlWebpackPlugin.chunks.main.entry%&amp;gt;&quot;&amp;gt;&amp;lt;/script&amp;gt;在打包文件中引用入口entry中的main.js文件&lt;/p&gt;
&lt;p&gt;　　　　　　8）：可以通过minify属性对输出后的文件进行压缩：=》removeComments:true//删除备注------其它minify属性参考webpack文档&lt;/p&gt;
&lt;p&gt;　　　　　　9）：多页面进行打包时需要在entry中指定多个html页面目录并创建多个对应的htmlWebpackPlugin,并可自定义其它属性名称来达到通过输出模板文件对该属性进行引用的目的&lt;/p&gt;
&lt;p&gt;　　　　　　10）：多页面打包时可以通过chunks属性指定需要引入的entry中的js文件路径&lt;/p&gt;
&lt;p&gt;　　　　　　11）：多页面打包时可以通过excludeChunks属性排除该属性指定的entry中的js文件路径&lt;/p&gt;
&lt;p&gt;　　　　　　12）：webpack的compilation对象中的assets[]方法接收去除publicPath(线上地址)后的文件路径，并通过source()方法将公用文件main直接写入到打包输出后的html文件中，这样可以大大提高　　　　　　　　　　wepack打包效率=》&amp;lt;%=compilation.assets[htmlWebpackPlugin.files.chunks.main.entry.substr(htmlWebpackPlugin.files.publicPath.length)].source%&amp;gt;，其它外链的js文件可以在模板文件中循　　　　　　　　环除了公用文件main.js的所有files对象中的chunk对象的每一个js文件名对应的entry目录，并通过script标签引入文件：&lt;/p&gt;
&lt;p&gt;　　　　　　　　　　&amp;lt;%for(var k in htmlWebpackPlugin.files.chunks){%&amp;gt;&lt;/p&gt;
&lt;p&gt;　　　　　　　　　　　　&amp;lt;%if(k!='main'){%&amp;gt;&lt;/p&gt;
&lt;p&gt;　　　　　　　　　　　　　　&amp;lt;script src=&quot;&amp;lt;%=htmlWebpackPlugin.files.chunks[k].entry%&amp;gt;&quot; type=&quot;text/javascript&quot;&amp;gt;&amp;lt;/script&amp;gt;&lt;/p&gt;
&lt;p&gt;　　　　　　　　　　　　&amp;lt;%}%&amp;gt;&lt;/p&gt;
&lt;p&gt;　　　　　　　　　　&amp;lt;%}%&amp;gt;&lt;/p&gt;
&lt;p&gt;　　　　　　　　最后run 一次webpack就可以看到嵌入的main的内容以及外链的其它js文件&lt;/p&gt;
&lt;p&gt;　　　　　　　　以上就是htmlWebpackPlugin使用过程&lt;/p&gt;










</description>
<pubDate>Sun, 05 Nov 2017 15:43:00 +0000</pubDate>
<dc:creator>鲁小胖</dc:creator>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/lpggo/p/7790000.html</dc:identifier>
</item>
<item>
<title>哈哈，原来这叫做“松鼠症”……并谈谈我建议的学习方法 - 自由飞</title>
<link>http://www.cnblogs.com/freeflying/p/7725385.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/freeflying/p/7725385.html</guid>
<description>&lt;p&gt;&lt;span&gt;好几天前在QQ群里有同学要我给一个技术栈，我没给，告诉他那玩意儿没用。第二天他就给了我一个链接，园子里的&lt;a id=&quot;cb_post_title_url&quot; href=&quot;http://www.cnblogs.com/1996V/p/7700087.html&quot;&gt;.Net Web开发技术栈&lt;/a&gt;，我看到的时候已有102个赞。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;“啪啪啪”打脸，&lt;span&gt;/(ㄒoㄒ)/~~&lt;/span&gt;，突然觉得心好累……&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;然而，我始终觉得哪里不对劲？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这其实不是个例。你在知乎搜索“如何学习xxx”的问题，看看高票答案，好多好多的都是一串一串的书单，或者一版一版的网址，以前我其实有点奇怪为什么？为什么这样的回答能得到这么多的赞呢？当然，答主的劳动值得肯定，他付出了大量的时间和精力来收集整理；然而，注意这个然而，同样有很多“答主付出了大量时间和精力”的答案无人问津啊？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;一定有什么原因的！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;哦，对了，除了那么多的赞以外，更多的是“收藏”。收藏的含义应该就是收藏起来我以后慢慢看——好了，我突然灵光一闪：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;收藏起来的那些东西，后来你都真的看了么？&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt; &lt;span&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/49387/201710/49387-20171026163547976-1881733008.gif&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;所以啊！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我想起了我的一些事。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;那时候学英语，练听力，他们说《老友记》不错，谁谁谁哪个学渣把它怎么怎么翻来覆去看了n遍，然后就华丽丽的变身学霸，从此看原版不用字幕，一口纯正的曼哈顿口音把老外唬得一愣一愣的……&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;于是我就像打了鸡血一样，到网上疯了一样的找《老友记》，一定不能要带字幕的，但又一定要有字幕脚本，从第一季第一集到第七季最后一集，一集都不能落下！为此还专门买了一个刻录机……等等，关刻录机什么事？哦，我担心硬盘会坏掉呀！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;事实上，你肯定猜到了，硬盘没坏，是我的脑子坏掉了：我连第一季都没看完。还看了多少遍！还担心什么把硬盘看坏掉？！天啊！为什么我会这么天真？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/49387/201710/49387-20171027002309383-1138600592.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;还有买书。唉哟，那些什么经典之类的大部头，当时买的时候那个激动啊！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;网上买的，寄到公司，唉哟我的妈，同事眼睛都大了，“老叶！牛逼啊……”然后下班我“哈呲哈呲”的把他们扛回家，租的房子没电梯，九楼——痛并快乐着啊。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/49387/201711/49387-20171105173740513-403974564.jpg&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;快十年过去了，现在那些书还像新的一样。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;但如果不是QQ群里 &lt;span&gt;@ARピ&lt;/span&gt; 同学贴出来一段话，我也不会写这篇博客。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://images2017.cnblogs.com/blog/49387/201711/49387-20171105220115904-1545942018.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;感觉一下子就说到了心坎上一样，而且居然已经很流行了？还有一个很贴切的名字：“&lt;span&gt;松鼠症&lt;/span&gt;”，耶！感觉有底气多了。搜索了一阵，这种行为除了“浪费”——浪费时间浪费精力浪费金钱以外，最大的问题是：&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;让我们产生了“我已经学会了”，“我很强”“我好厉害”的错觉，而不会去真正的学习和练习。&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p title=&quot;【TED】不要公开你的个人目标&quot;&gt;&lt;span&gt;这让我想起以前我看到过的&lt;a href=&quot;http://www.bilibili.com/video/av294900/&quot; target=&quot;_blank&quot;&gt;【TED】不要公开你的个人目标&lt;/a&gt;，有时间的同学可以看一下，其实道理都是一样的。&lt;/span&gt;&lt;/p&gt;

&lt;p title=&quot;【TED】不要公开你的个人目标&quot;&gt; &lt;/p&gt;
&lt;p title=&quot;【TED】不要公开你的个人目标&quot;&gt;&lt;span&gt;所以，一开始我的直觉是对的，至少是有一定道理的。给你一个什么“&lt;span&gt;技术栈&lt;/span&gt;”，对你一点用都没有，甚至可能害了你！而且这个“技术栈”越宏大越壮阔，害你越深：目标太宏伟，计划太宏大。你就会把太多的热情都消耗在了计划上面，而&lt;span&gt;开始行动的时候，&lt;/span&gt;咳咳，你懂的……&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p title=&quot;【TED】不要公开你的个人目标&quot;&gt; &lt;/p&gt;

&lt;p&gt;那这种情况肿么破？&lt;/p&gt;
&lt;p&gt;就编程而言，我觉得有两点：&lt;/p&gt;
&lt;p&gt;1、忘掉你那壮（hao）志（gao）凌（wu）云（远）的理想；&lt;/p&gt;
&lt;p&gt;2、脚踏实地的先把代码撸起来（参考：&lt;a href=&quot;https://www.zhihu.com/question/19578287/answer/243019750&quot; target=&quot;_blank&quot;&gt;如何学习编程？&lt;/a&gt;）&lt;/p&gt;

&lt;p&gt;我再补充一点，为什么“把代码撸起来”就可以破掉烦人的“松鼠症”。因为：&lt;/p&gt;
&lt;p&gt;1、这是你走出“舒适区”的第一步。我在&lt;a href=&quot;http://17bang.ren/Article/8&quot; target=&quot;_blank&quot;&gt;“零基础”全栈课程&lt;/a&gt;里一直呼吁大家把代码撸起来，然而，即使最简单的作业也没有几个同学去完成。为什么？因为完成作业需要动手，哪怕是最简单的作业，都得弄个页面，跑一跑，调试一下，哪有开着电脑左手瓜子右手可乐，看（听）飞哥吹牛逼舒服呢？我完全可以想象，并且理解这种心理。然而，这样不行啊！要是这样都能学会编程，我，我，我……我直接去买块豆腐撞死球算了！&lt;/p&gt;
&lt;p&gt;2、它会破灭你很多很多的幻想。只要你开始撸代码，你就会发现：唉呀什么高性能可扩展各种牛逼都是浮云，能把“hello，world”整出来就万事大吉了！非.NET系统的，连个开发环境都搭建不出来，搞个毛线……用你们年轻人的话说，叫“分分钟教你做人”，是不是？&lt;/p&gt;
&lt;p&gt;3、你会得到真正的、有效的、及时的反馈，激励你一步一步继续走下去。但一旦你克服眼前的这些困难，你就能马上获得反馈，成功的喜悦会和打游戏过关升级是一样一样的。但注意，是从新手村开始，一级一级的练，而不是攒着劲，&lt;span&gt;一口吃个大胖子&lt;/span&gt;，一下子干死大BOSS。“三年不鸣一鸣惊人”，这种事，长大了之后我就不再幻想了。&lt;/p&gt;

&lt;p&gt;刚好，&lt;span&gt;知乎上看到一个11K+赞的回答：&lt;a href=&quot;https://www.zhihu.com/question/19550362/answer/18044986&quot; target=&quot;_blank&quot;&gt;学习一个新领域的知识的最佳方法和最快时间各是什么？&lt;/a&gt;，答案超级长（知乎特色），我简单点总结就是：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1、10000个小时成为专家，是“练”（&lt;span class=&quot;RichText CopyrightRichText-richText&quot;&gt;practise&lt;/span&gt;）出来的，不是学（learn）出来的；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2、&lt;span class=&quot;RichText CopyrightRichText-richText&quot;&gt;学习时间和学习效果有边际效用递减的关系&lt;/span&gt;。所以学会了点，马上就开练，才有最佳效果。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我做个更形象点的解释：知识需要转化，才能变成我们的能力。读书听课，&lt;span&gt;就像吃肉一样，首先我们吃不了太多，其次如果只吃不锻炼，一次吃多了撑，长期吃多了肥，既不利于营养吸收，也不利于营养转化成肌肉。正确的方式，应该是边吃边练，相辅相成，最后才能练出一身的腱子肉。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;现在互联网时代，信息大爆炸，各种学习的资料不要太多太多，而且不乏精品，就像各种美味的实物一样，变着法儿的诱惑你品尝。IT培训我暂时没看到那种上课像讲相声的，但各种英语培训，呵呵，不管是大人小孩的，都是“寓教于乐”，一堂课嘻嘻哈哈就过去了，但效果呢？有个毛线的效果！怎么可能有效果？人家的课讲得再好，是人家的本事（就比如说飞哥我，嘿嘿，不要脸的笑.jpg），又不能掌对掌地“把我毕生的功力传授于你&lt;span&gt;&lt;span&gt;”&lt;/span&gt;&lt;/span&gt;，你除了乐呵一会儿，能又啥长进？&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;技术是自己练出来的啊，同学。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;++++++++++++++++++++++++&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;好了，刚才不小心把草稿发布了，好尴尬，赶快结尾吧。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这篇博客，主要针对初学者，谁让我现在做“零基础”的培训呢（有兴趣的同学这里：&lt;a href=&quot;http://17bang.ren/Article/8&quot; target=&quot;_blank&quot;&gt;一次尝试：真正的项目实战培训&lt;/a&gt;）？着重强调“练习”的重要性，并不是说“读书”什么用都没有。可能有同学看到了我的草稿，里面也讲了如何读书，为了不冲淡主题，本文就没有涉及了，下次专门开个博客再讲。&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;谢谢大家！欢迎各种评论、点赞，以及拍砖。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Sun, 05 Nov 2017 15:34:00 +0000</pubDate>
<dc:creator>自由飞</dc:creator>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/freeflying/p/7725385.html</dc:identifier>
</item>
<item>
<title>详细解读-this-关键字在全局、函数、对象、jQuery等中的基础用法！ - ../..随风</title>
<link>http://www.cnblogs.com/xyq1107/p/7789915.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/xyq1107/p/7789915.html</guid>
<description>
&lt;h2&gt;一、前言&lt;/h2&gt;
&lt;p&gt;1、 Javascript是一门基于对象的动态语言，也就是说，所有东西都是对象，一个很典型的例子就是函数也被视为普通的对象。Javascript可以通过一定的设计模式来实现面向对象的编程，其中this “指针”就是实现面向对象的一个很重要的特性。但是this也是Javascript中一个非常容易理解错，进而用错的特性。&lt;/p&gt;
&lt;p&gt;2、this是Javascript语言的一个关键字。 它代表函数运行时，自动生成的一个内部对象。&lt;/p&gt;
&lt;h2&gt;二、进入正题&lt;/h2&gt;
&lt;h3&gt;1、全局代码中的——this&lt;/h3&gt;
&lt;p&gt;　　1、 浏览器宿主的全局环境中，&lt;code&gt;this&lt;/code&gt;指的是&lt;code&gt;window&lt;/code&gt;对象。在全局代码中，this始终是全局对象本身，这样就有可能间接的引用到它了。&lt;/p&gt;
&lt;p&gt;　　示例:&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
&amp;lt;script type=&quot;text/javascript&quot;&amp;gt;
        console.log(this); //window
        console.log(this == window);//true
&amp;lt;/script&amp;gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;　　2、 浏览器中在全局环境下，使用&lt;code&gt;var&lt;/code&gt;声明变量将会把值赋给&lt;code&gt;this&lt;/code&gt;或&lt;code&gt;window&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;　　示例：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
&amp;lt;script type=&quot;text/javascript&quot;&amp;gt;
        var  name = &quot;Jack&quot; ;
        console.log(this.name);//Jack
        console.log(window.name);//Jack
&amp;lt;/script&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;　　3、任何情况下，即使创建变量时没有使用&lt;code&gt;var&lt;/code&gt;，也是在操作全局&lt;code&gt;this&lt;/code&gt;。而在函数里面创建变量时，结果为undefined。&lt;/p&gt;
&lt;p&gt;　　示例：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;34&quot;&gt;
&lt;pre&gt;
&amp;lt;script type=&quot;text/javascript&quot;&amp;gt;
    name = &quot;Jack&quot;;
    function testThis() {
         age = 20;
         console.log(this.age); // 20
    }
    console.log(this.age); // undefined 
    console.log(this.name);// Jack
    testThis();
    console.log(this.name);// Jack
&amp;lt;/script&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;二、函数代码中的——this&lt;/h3&gt;
&lt;p&gt;1、先来看一个最简单的例子&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;34&quot;&gt;
&lt;pre&gt;
&amp;lt;script type=&quot;text/javascript&quot;&amp;gt;
    var name = &quot;Jack&quot;;
    function sayHi(){
      console.log(&quot;你好，我的名字叫&quot; + name);// &quot;你好，我的名字叫Jack&quot;
      console.log(&quot;你好，我的名字叫&quot; + this.name);// &quot;你好，我的名字叫Jack&quot;
    }
    sayHi();
&amp;lt;/script&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;首先，我们定义了一个全局字符串对象name和函数对象sayHi。运行都会打印出，“你好，我的名字叫Jack”。全局变量name将值赋给this。&lt;/p&gt;
&lt;p&gt;2、我们把上面的代码改一改：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;34&quot;&gt;
&lt;pre&gt;
&amp;lt;script type=&quot;text/javascript&quot;&amp;gt;
    name = &quot;Jack&quot;;
    function testThis() {
      this.name = &quot;Alice&quot;;
    }
    console.log(&quot;你好，我的名字叫&quot;+this.name); //  &quot;你好，我的名字叫Jack&quot;
    testThis();
    console.log(&quot;你好，我的名字叫&quot;+this.name); //  &quot;你好，我的名字叫Alice&quot;
&amp;lt;/script&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;运行结果为Jack中的this为函数调用之前的全局变量name=Jack将值赋给this，此时textThis函数未将Alice赋值给this；运行结果为Alice为函数调用之后testThis函数将Alice值赋值给了全局变量name，此时name = &quot;Alice&quot;   this.name = &quot;Alice&quot;。&lt;/p&gt;
&lt;p&gt;3、函数也是普通的对象，可以将其当作一个普通变量使用。我们再把上面的代码改一改：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;35&quot;&gt;
&lt;pre&gt;
&amp;lt;script type=&quot;text/javascript&quot;&amp;gt;
    var name = &quot;Jack&quot;;
    function sayHi(){
       console.log(&quot;你好，我的名字叫&quot; + this.name);// undefined
    }
    var person = {};
    person.sayHello = sayHi;
    person.sayHello();
&amp;lt;/script&amp;gt;

//而定义  person = {name：Alice}  ，则：
&amp;lt;script type=&quot;text/javascript&quot;&amp;gt;
    var name = &quot;Jack&quot;;
    function sayHi(){
       console.log(&quot;你好，我的名字叫&quot; + this.name);// &quot;你好，我的名字叫Alice&quot;
    }
    var person = {name:Alice};
    person.sayHello = sayHi;
    person.sayHello();
&amp;lt;/script&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;第一，我们定义了一个全局函数对象sayHi并执行了这个函数，函数内部使用了this关键字，那么执行this这行代码的对象是sayHi（一切皆对象的体现），sayHi是被定义在全局作用域中。其实在Javascript中所谓的全局对象，无非是定义在window这个根对象下的一个属性而已。因此，sayHi的所有者是window对象。也就是说，在全局作用域下，你可以通过直接使用name去引用这个对象，你也可以通过window.name去引用同一个对象。因而&lt;strong&gt;this.name就可以翻译为window.name&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;第二，我们定义了一个person的对象，并定义了它的sayHello属性，使其指向sayHi全局对象。那么这个时候，当我们运行person.sayHello的时候，this所在的代码所属对象就是sayHello了（其实准确来说，sayHi和sayHello是只不过类似两个指针，指向的对象实际上是同一个），而sayHello对象的所有者就是person了。第一次，person里面没有name属性，因此弹出的对话框就是this.name引用的就是undefined对象（&lt;strong&gt;Javascript中所有只声明而没有定义的变量全都指向undefined对象&lt;/strong&gt;）；而第二次我们在定义person的时候加了name属性了，那么this.name指向的自然就是我们定义的字符串了。&lt;/p&gt;
&lt;p&gt;参考上面解释，我们将上面示例改造成面向对象式的代码：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;35&quot;&gt;
&lt;pre&gt;
&amp;lt;script type=&quot;text/javascript&quot;&amp;gt;
    var name = &quot;Jack&quot;;
    function sayHi(){
       console.log(&quot;你好，我的名字叫&quot; + this.name);//两次函数调用均成功，打印两次：&quot;你好，我的名字叫Marry&quot;     &quot;你好，我的名字叫Alice&quot;
    }
    function Person(name){
        this.name = name;
    }
    Person.prototype.sayHello = sayHi;
    var marry = new Person(&quot;Marry&quot;);   
    marry.sayHello();
    var alice = new Person(&quot;Alice&quot;);
    alice.sayHello();
&amp;lt;/script&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在上面这段代码中，我们定义了一个Person的“类”（实际上还是一个对象），然后在这个类的原型（&lt;strong&gt;类原型相当于C++中的静态成员变量的概念&lt;/strong&gt;）中定义了sayHello属性，使其指向全局的sayHi对象。运行代码我们可以看到，marry和kevin都成功打印出来。&lt;/p&gt;
&lt;p&gt;4、当用调用函数时使用了&lt;code&gt;new&lt;/code&gt;关键字，此刻&lt;code&gt;this&lt;/code&gt;指代一个新的上下文，不再指向全局&lt;code&gt;this。通常我将这个新的上下文称作实例。&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;34&quot;&gt;
&lt;pre&gt;
&amp;lt;script type=&quot;text/javascript&quot;&amp;gt;
    name = &quot;Jack&quot;;
    function testThis() {
      this.name = &quot;Alice&quot;;
    }
    console.log(this.name); // &quot;Jack&quot;
    new testThis();
    console.log(this.name); // &quot;Jack&quot;
    console.log(new testThis().name); // &quot;Alice&quot;
&amp;lt;/script&amp;gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;三、对象代码中的——this&lt;/h3&gt;
&lt;p&gt;1、可以在对象的任何方法中使用&lt;code&gt;this&lt;/code&gt;来访问该对象的属性。这与用&lt;code&gt;new&lt;/code&gt;得到的实例是不一样的。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
var obj = {
   name: &quot;Jack&quot;,
    func: function () {
        console.log(this.name); //   &quot;Jack&quot;
    }
};

obj.func();
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;2、可以将函数绑定到对象，就好像这个对象是一个实例一样。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
var obj = {
    name: &quot;Jack&quot;
};

function logName() {
    console.log(this.name); //logs &quot;Jack&quot;
}

logName.apply(obj);
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;3、也可以不通过&lt;code&gt;this&lt;/code&gt;，直接访问对象的属性。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;34&quot;&gt;
&lt;pre&gt;
var obj = {
   name: &quot;Jack&quot;,
    deeper: {
        logName: function () {
            console.log(obj.name);//    &quot;Jack&quot;
        }
    }
};

obj.deeper.logName(); 
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;四、jQuery代码中的——this&lt;/h3&gt;
&lt;p&gt;1、jQuery库中大多地方的&lt;code&gt;this&lt;/code&gt;也是指代的DOM元素。页面上的事件回调和一些便利的静态方法比如&lt;code&gt;$.each&lt;/code&gt; 都是这样的。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;36&quot;&gt;
&lt;pre&gt;
&amp;lt;div class=&quot;foo bar1&quot;&amp;gt;&amp;lt;/div&amp;gt;
&amp;lt;div class=&quot;foo bar2&quot;&amp;gt;&amp;lt;/div&amp;gt;
&amp;lt;script src=&quot;../05-JQuery/JS/jquery-1.10.2.min.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
&amp;lt;script type=&quot;text/javascript&quot;&amp;gt;
$(function(){

     　　$(&quot;.foo&quot;).each(function () {
            console.log(this); //  &amp;lt;div class=&quot;foo bar1&quot;&amp;gt;&amp;lt;/div&amp;gt;   &amp;lt;div class=&quot;foo bar2&quot;&amp;gt;&amp;lt;/div&amp;gt;
        });
        $(&quot;.foo&quot;).on(&quot;click&quot;, function () {
            console.log(this); //  &amp;lt;div class=&quot;foo bar1&quot;&amp;gt;   &amp;lt;div class=&quot;foo bar2&quot;&amp;gt;&amp;lt;/div&amp;gt;
        });
          
   })

&amp;lt;/script&amp;gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt; 五、DOM事件中的——this&lt;/h3&gt;
&lt;p&gt;在DOM事件的处理函数中，&lt;code&gt;this&lt;/code&gt;指代的是被绑定该事件的DOM元素。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;35&quot;&gt;
&lt;pre&gt;
function&lt;span&gt; Listener() {
    document.getElementById(&quot;foo&quot;).addEventListener(&quot;click&quot;&lt;span&gt;,
       this&lt;span&gt;.handleClick);
}
Listener.prototype.handleClick = function&lt;span&gt; (event) {
    console.log(this); //logs &quot;&amp;lt;div id=&quot;foo&quot;&amp;gt;&amp;lt;/div&amp;gt;&quot;
&lt;span&gt;}

var listener = new&lt;span&gt; Listener();
document.getElementById(&quot;foo&quot;).click();&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;六、使用&lt;code&gt;with&lt;/code&gt;时的——&lt;code&gt;this&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;使用&lt;code&gt;with&lt;/code&gt;可以将&lt;code&gt;this&lt;/code&gt;人为添加到当前执行环境中而不需要显示地引用&lt;code&gt;this&lt;/code&gt;。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;34&quot;&gt;
&lt;pre&gt;
function&lt;span&gt; Thing () {
}
Thing.prototype.foo = &quot;bar&quot;&lt;span&gt;;
Thing.prototype.logFoo = function&lt;span&gt; () {
    with (this&lt;span&gt;) {
        console.log(foo);
        foo = &quot;foo&quot;&lt;span&gt;;
    }
}

var thing = new&lt;span&gt; Thing();
thing.logFoo(); // logs &quot;bar&quot;
console.log(thing.foo); // logs &quot;foo&quot;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;结束：&lt;/h2&gt;
&lt;p&gt;　　上面，就是博主自己在实践的过程中，总结的一点经验，希望对大家能否有所帮助，欢迎大家能够提出高贵意见，博主定当补充改正。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;














</description>
<pubDate>Sun, 05 Nov 2017 15:24:00 +0000</pubDate>
<dc:creator>../..随风</dc:creator>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/xyq1107/p/7789915.html</dc:identifier>
</item>
</channel>
</rss>