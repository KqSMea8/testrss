<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed.cnblogs.com%2Fblog%2Fsitehome%2Frss&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://feed.cnblogs.com/blog/sitehome/rss" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed.cnblogs.com%252Fblog%252Fsitehome%252Frss%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed.cnblogs.com%252Fblog%252Fsitehome%252Frss%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>博客园_首页</title>
<link></link>
<description>代码改变世界</description>
<item>
<title>浅析JDK中ServiceLoader的源码 - throwable</title>
<link>http://www.cnblogs.com/throwable/p/9788819.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/throwable/p/9788819.html</guid>
<description>&lt;p&gt;紧接着上一篇《通过源码浅析JDK中的资源加载》，ServiceLoader是SPI(Service Provider Interface)中的服务类加载的核心类，也就是，这篇文章先介绍ServiceLoader的使用方式，再分析它的源码。&lt;/p&gt;

&lt;p&gt;这里先列举一个经典的例子，MySQL的Java驱动就是通过ServiceLoader加载的，先引入&lt;code&gt;mysql-connector-java&lt;/code&gt;的依赖：&lt;/p&gt;
&lt;pre class=&quot;xml&quot;&gt;
&lt;code&gt;&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;mysql&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;mysql-connector-java&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;5.1.47&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;查看这个依赖的源码包下的META-INF目录，可见：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://pazkqls86.bkt.clouddn.com/r-s-l-1.png&quot; alt=&quot;r-s-l-1&quot;/&gt;&lt;/p&gt;
&lt;p&gt;我们接着查看java.lang.DriverManager，静态代码块里面有：&lt;/p&gt;
&lt;pre class=&quot;java&quot;&gt;
&lt;code&gt;static {
    loadInitialDrivers();
    println(&quot;JDBC DriverManager initialized&quot;);
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;其中，可以查看&lt;code&gt;loadInitialDrivers()&lt;/code&gt;有如下的代码片段：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://pazkqls86.bkt.clouddn.com/r-s-l-2.png&quot; alt=&quot;r-s-l-2&quot;/&gt;&lt;/p&gt;
&lt;p&gt;java.lang.DriverManager是启动类加载器加载的基础类，但是它可以加载&lt;code&gt;rt.jar&lt;/code&gt;包之外的类，上篇文章提到，这里打破了双亲委派模型，原因是：ServiceLoader中使用了线程上下文类加载器去加载类。这里JDBC加载的过程就是典型的SPI的使用，总结规律如下：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;1、需要定义一个接口。&lt;/li&gt;
&lt;li&gt;2、接口提供商需要实现第1步中的接口。&lt;/li&gt;
&lt;li&gt;3、接口提供商在META-INF/services目录下建立一个文本文件，文件名是第1步中定义的接口的全限定类名，文本内容是接口的实现类的全限定类名，每个不同的实现占独立的一行。&lt;/li&gt;
&lt;li&gt;4、使用ServiceLoader加载接口类，获取接口的实现的实例迭代器。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;举个简单的实例，先定义一个接口和两个实现：&lt;/p&gt;
&lt;pre class=&quot;java&quot;&gt;
&lt;code&gt;public interface Say {

  void say();
}

public class SayBye implements Say {

    @Override
    public void say() {
        System.out.println(&quot;Bye!&quot;);
    }
}

public class SayHello implements Say {

    @Override
    public void say() {
        System.out.println(&quot;Hello!&quot;);
    }
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;接着在项目的META-INF/services中添加文件如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://pazkqls86.bkt.clouddn.com/r-s-l-3.png&quot; alt=&quot;r-s-l-3&quot;/&gt;&lt;/p&gt;
&lt;p&gt;最后通过main函数验证：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://pazkqls86.bkt.clouddn.com/r-s-l-4.png&quot; alt=&quot;r-s-l-4&quot;/&gt;&lt;/p&gt;
&lt;p&gt;基于SPI或者说ServiceLoader加载接口实现这种方式也可以广泛使用在相对基础的组件中，因为这是一个成熟的规范。&lt;/p&gt;

&lt;p&gt;上面通过一个经典例子和一个实例介绍了ServiceLoader的使用方式，接着我们深入分析ServiceLoader的源码。我们先看ServiceLoader的类签名和属性定义：&lt;/p&gt;
&lt;pre class=&quot;java&quot;&gt;
&lt;code&gt;public final class ServiceLoader&amp;lt;S&amp;gt; implements Iterable&amp;lt;S&amp;gt;{
    //需要加载的资源的路径的目录，固定是ClassPath下的META-INF/services/
    private static final String PREFIX = &quot;META-INF/services/&quot;;
    // ServiceLoader需要正在需要加载的类或者接口
    // The class or interface representing the service being loaded
    private final Class&amp;lt;S&amp;gt; service;
    // ServiceLoader进行类加载的时候使用的类加载器引用
    // The class loader used to locate, load, and instantiate providers
    private final ClassLoader loader;
    // 权限控制上下文
    // The access control context taken when the ServiceLoader is created
    private final AccessControlContext acc;
    //基于实例的顺序缓存类的实现实例，其中Key为实现类的全限定类名
    // Cached providers, in instantiation order
    private LinkedHashMap&amp;lt;String,S&amp;gt; providers = new LinkedHashMap&amp;lt;&amp;gt;();
    // 当前的&quot;懒查找&quot;迭代器，这个是ServiceLoader的核心
    // The current lazy-lookup iterator
    private LazyIterator lookupIterator;

    //暂时忽略其他代码...
}    &lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;ServiceLoader实现了Iterable接口，这一点提示了等下我们在分析它源码的时候，需要重点分析&lt;code&gt;iterator()&lt;/code&gt;方法的实现。ServiceLoader依赖于类加载器实例进行类加载，它的核心属性LazyIterator是就是用来实现&lt;code&gt;iterator()&lt;/code&gt;方法的，下文再重点分析。接着，我们分析ServiceLoader的构造函数：&lt;/p&gt;
&lt;pre class=&quot;java&quot;&gt;
&lt;code&gt;public void reload() {
    //清空缓存
    providers.clear();
    //构造LazyIterator实例
    lookupIterator = new LazyIterator(service, loader);
}

private ServiceLoader(Class&amp;lt;S&amp;gt; svc, ClassLoader cl) {
    service = Objects.requireNonNull(svc, &quot;Service interface cannot be null&quot;);
    loader = (cl == null) ? ClassLoader.getSystemClassLoader() : cl;
    acc = (System.getSecurityManager() != null) ? AccessController.getContext() : null;
    reload();
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;ServiceLoader只有一个私有的构造函数，也就是它不能通过构造函数实例化，但是要实例化ServiceLoader必须依赖于它的静态方法调用私有构造去完成实例化操作，而实例化过程主要做了几步：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;1、判断传入的接口或者类的Class实例不能为null，否则会抛出异常。&lt;/li&gt;
&lt;li&gt;2、如果传入的ClassLoader实例为null，则使用应用类加载器(Application ClassLoader)。&lt;/li&gt;
&lt;li&gt;3、实例化访问控制上下文。&lt;/li&gt;
&lt;li&gt;4、调用实例方法&lt;code&gt;reload()&lt;/code&gt;，清空目标加载类的实现类实例的缓存并且构造LazyIterator实例。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;注意一点是实例方法&lt;code&gt;reload()&lt;/code&gt;的修饰符是public，也就是可以主动调用去清空目标加载类的实现类实例的缓存和重新构造LazyIterator实例。接着看ServiceLoader提供的静态方法：&lt;/p&gt;
&lt;pre class=&quot;java&quot;&gt;
&lt;code&gt;public static &amp;lt;S&amp;gt; ServiceLoader&amp;lt;S&amp;gt; load(Class&amp;lt;S&amp;gt; service, ClassLoader loader){
    return new ServiceLoader&amp;lt;&amp;gt;(service, loader);
}

public static &amp;lt;S&amp;gt; ServiceLoader&amp;lt;S&amp;gt; load(Class&amp;lt;S&amp;gt; service) {
    ClassLoader cl = Thread.currentThread().getContextClassLoader();
    return ServiceLoader.load(service, cl);
}

public static &amp;lt;S&amp;gt; ServiceLoader&amp;lt;S&amp;gt; loadInstalled(Class&amp;lt;S&amp;gt; service) {
    ClassLoader cl = ClassLoader.getSystemClassLoader();
    ClassLoader prev = null;
    while (cl != null) {
        prev = cl;
        cl = cl.getParent();
    }
    return ServiceLoader.load(service, prev);
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;上面的三个公共静态方法都是用于构造ServiceLoader实例，其中&lt;code&gt;load(Class&amp;lt;S&amp;gt; service, ClassLoader loader)&lt;/code&gt;就是典型的静态工厂方法，直接调用ServiceLoader的私有构造器进行实例化，除了需要指定加载类的目标类型，还需要传入类加载器的实例。&lt;code&gt;load(Class&amp;lt;S&amp;gt; service)&lt;/code&gt;实际上也是委托到&lt;code&gt;load(Class&amp;lt;S&amp;gt; service, ClassLoader loader)&lt;/code&gt;，不过它使用的类加载器指定为线程上下文类加载器，一般情况下，线程上下文类加载器获取到的就是应用类加载器(系统类加载器)。&lt;code&gt;loadInstalled(Class&amp;lt;S&amp;gt; service)&lt;/code&gt;方法又看出了&quot;双亲委派模型&quot;的影子，它指定类加载器为最顶层的启动类加载器，最后也是委托到&lt;code&gt;load(Class&amp;lt;S&amp;gt; service, ClassLoader loader)&lt;/code&gt;。接着我们需要重点分析&lt;code&gt;ServiceLoader#iterator()&lt;/code&gt;：&lt;/p&gt;
&lt;pre class=&quot;java&quot;&gt;
&lt;code&gt;public Iterator&amp;lt;S&amp;gt; iterator() {

    //Iterator的匿名实现
    return new Iterator&amp;lt;S&amp;gt;() {
        
    //目标类实现类实例缓存的Map的Entry的迭代器实例
    Iterator&amp;lt;Map.Entry&amp;lt;String,S&amp;gt;&amp;gt; knownProviders = providers.entrySet().iterator();
        
        //先从缓存中判断是否有下一个实例，否则通过懒加载迭代器LazyIterator去判断是否存在下一个实例
        public boolean hasNext() {
            if (knownProviders.hasNext())
                return true;
            return lookupIterator.hasNext();
        }

        //如果缓存中判断是否有下一个实例，如果有则从缓存中的值直接返回
        //否则通过懒加载迭代器LazyIterator获取下一个实例
        public S next() {
            if (knownProviders.hasNext())
                return knownProviders.next().getValue();
            return lookupIterator.next();
        }

        //不支持移除操作，直接抛异常
        public void remove() {
            throw new UnsupportedOperationException();
        }
    };
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;iterator()&lt;/code&gt;内部仅仅是Iterator接口的匿名实现，&lt;code&gt;hasNext()&lt;/code&gt;和&lt;code&gt;next()&lt;/code&gt;方法都是优先判断缓存中是否已经存在实现类的实例，如果存在则直接从缓存中返回，否则调用懒加载迭代器LazyIterator的实例去获取，而LazyIterator本身也是一个Iterator接口的实现，它是ServiceLoader的一个私有内部类，源码如下：&lt;/p&gt;
&lt;pre class=&quot;java&quot;&gt;
&lt;code&gt;private class LazyIteratorimplements Iterator&amp;lt;S&amp;gt;{

        Class&amp;lt;S&amp;gt; service;
        ClassLoader loader;
        //加载的资源的URL集合
        Enumeration&amp;lt;URL&amp;gt; configs = null;
        //所有需要加载的实现类的全限定类名的集合
        Iterator&amp;lt;String&amp;gt; pending = null;
        //下一个需要加载的实现类的全限定类名
        String nextName = null;

        private LazyIterator(Class&amp;lt;S&amp;gt; service, ClassLoader loader) {
            this.service = service;
            this.loader = loader;
        }

        private boolean hasNextService() {
            //如果下一个需要加载的实现类的全限定类名不为null，则说明资源中存在内容
            if (nextName != null) {
                return true;
            }
            //如果加载的资源的URL集合为null则尝试进行加载
            if (configs == null) {
                try {
                    //资源的名称，META-INF/services + '需要加载的类的全限定类名'
                    //这样得到的刚好是需要加载的文件的资源名称
                    String fullName = PREFIX + service.getName();
                    //这里其实ClassLoader实例应该不会为null
                    if (loader == null)
                        configs = ClassLoader.getSystemResources(fullName);
                    else
                        //从ClassPath加载资源
                        configs = loader.getResources(fullName);
                } catch (IOException x) {
                    fail(service, &quot;Error locating configuration files&quot;, x);
                }
            }
            //从资源中解析出需要加载的所有实现类的全限定类名
            while ((pending == null) || !pending.hasNext()) {
                if (!configs.hasMoreElements()) {
                    return false;
                }
                pending = parse(service, configs.nextElement());
            }
            //获取下一个需要加载的实现类的全限定类名
            nextName = pending.next();
            return true;
        }

        private S nextService() {
            if (!hasNextService())
                throw new NoSuchElementException();
            String cn = nextName;
            nextName = null;
            Class&amp;lt;?&amp;gt; c = null;
            try {
                //反射构造Class&amp;lt;S&amp;gt;实例
                c = Class.forName(cn, false, loader);
            } catch (ClassNotFoundException x) {
                fail(service,
                     &quot;Provider &quot; + cn + &quot; not found&quot;);
            }
            //这里会做一次类型判断，也就是实现类必须是当前加载的类或者接口的派生类，否则抛出异常终止
            if (!service.isAssignableFrom(c)) {
                fail(service,
                     &quot;Provider &quot; + cn  + &quot; not a subtype&quot;);
            }
            try {
                //通过Class#newInstance()进行实例化，并且强制转化为对应的类型的实例
                S p = service.cast(c.newInstance());
                //添加缓存，Key为实现类的全限定类名，Value为实现类的实例
                providers.put(cn, p);
                return p;
            } catch (Throwable x) {
                fail(service,
                     &quot;Provider &quot; + cn + &quot; could not be instantiated&quot;,
                     x);
            }
            throw new Error();          // This cannot happen
        }

        public boolean hasNext() {
            if (acc == null) {
                return hasNextService();
            } else {
                PrivilegedAction&amp;lt;Boolean&amp;gt; action = new PrivilegedAction&amp;lt;Boolean&amp;gt;() {
                    public Boolean run() { return hasNextService(); }
                };
                return AccessController.doPrivileged(action, acc);
            }
        }

        public S next() {
            if (acc == null) {
                return nextService();
            } else {
                PrivilegedAction&amp;lt;S&amp;gt; action = new PrivilegedAction&amp;lt;S&amp;gt;() {
                    public S run() { return nextService(); }
                };
                return AccessController.doPrivileged(action, acc);
            }
        }

        public void remove() {
            throw new UnsupportedOperationException();
        }

    }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;LazyIterator&lt;/code&gt;也是Iterator接口的实现，它的Lazy特性表明它总是在ServiceLoader的Iterator接口匿名实现&lt;code&gt;iterator()&lt;/code&gt;执行&lt;code&gt;hasNext()&lt;/code&gt;判断是否有下一个实现或者&lt;code&gt;next()&lt;/code&gt;获取下一个实现类的实例的时候才会&quot;懒判断&quot;或者&quot;懒加载&quot;下一个实现类的实例。最后是加载资源文件后对资源文件的解析过程的源码：&lt;/p&gt;
&lt;pre class=&quot;java&quot;&gt;
&lt;code&gt;private Iterator&amp;lt;String&amp;gt; parse(Class&amp;lt;?&amp;gt; service, URL u) throws ServiceConfigurationError{
        InputStream in = null;
        BufferedReader r = null;
        //存放文件中所有的实现类的全类名，每一行是一个元素
        ArrayList&amp;lt;String&amp;gt; names = new ArrayList&amp;lt;&amp;gt;();
        try {
            in = u.openStream();
            r = new BufferedReader(new InputStreamReader(in, &quot;utf-8&quot;));
            int lc = 1;
            while ((lc = parseLine(service, u, r, lc, names)) &amp;gt;= 0);
        } catch (IOException x) {
            fail(service, &quot;Error reading configuration file&quot;, x);
        } finally {
            try {
                if (r != null) r.close();
                if (in != null) in.close();
            } catch (IOException y) {
                fail(service, &quot;Error closing configuration file&quot;, y);
            }
        }
        //返回的是ArrayList的迭代器实例
        return names.iterator();
}

//解析资源文件中每一行的内容
private int parseLine(Class&amp;lt;?&amp;gt; service, URL u, BufferedReader r, int lc,
                      List&amp;lt;String&amp;gt; names)throws IOException, ServiceConfigurationError{
        // 下一行没有内容，返回-1，便于上层可以跳出循环                 
        String ln = r.readLine();
        if (ln == null) {
            return -1;
        }
        //如果存在'#'字符，截取第一个'#'字符串之前的内容，'#'字符之后的属于注释内容
        int ci = ln.indexOf('#');
        if (ci &amp;gt;= 0) ln = ln.substring(0, ci);
        ln = ln.trim();
        int n = ln.length();
        if (n != 0) {
            //不能存在空格字符' '和特殊字符'\t'
            if ((ln.indexOf(' ') &amp;gt;= 0) || (ln.indexOf('\t') &amp;gt;= 0))
                fail(service, u, lc, &quot;Illegal configuration-file syntax&quot;);
            int cp = ln.codePointAt(0);
            //判断第一个char是否一个合法的Java起始标识符
            if (!Character.isJavaIdentifierStart(cp))
                fail(service, u, lc, &quot;Illegal provider-class name: &quot; + ln);
            //判断所有其他字符串是否属于合法的Java标识符
            for (int i = Character.charCount(cp); i &amp;lt; n; i += Character.charCount(cp)) {
                cp = ln.codePointAt(i);
                if (!Character.isJavaIdentifierPart(cp) &amp;amp;&amp;amp; (cp != '.'))
                    fail(service, u, lc, &quot;Illegal provider-class name: &quot; + ln);
            }
            //如果缓存中不存在加载出来的全类名或者已经加载的列表中不存在加载出来的全类名则添加进去加载的全类名列表中
            if (!providers.containsKey(ln) &amp;amp;&amp;amp; !names.contains(ln))
                names.add(ln);
        }
        return lc + 1;
    }&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;整个资源文件的解析过程并不复杂，主要包括文件内容的字符合法性判断和缓存避免重复加载的判断。&lt;/p&gt;

&lt;p&gt;SPI被广泛使用在第三方插件式类库的加载，最常见的如JDBC、JNDI、JCE(Java加密模块扩展)等类库。理解ServiceLoader的工作原理有助于编写扩展性良好的可插拔的类库。&lt;/p&gt;
&lt;p&gt;(本文完 c-1-d e-20181014)&lt;/p&gt;
</description>
<pubDate>Sun, 14 Oct 2018 15:42:00 +0000</pubDate>
<dc:creator>throwable</dc:creator>
<og:description>前提 紧接着上一篇《通过源码浅析JDK中的资源加载》，ServiceLoader是SPI(Service Provider Interface)中的服务类加载的核心类，也就是，这篇文章先介绍Servi</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/throwable/p/9788819.html</dc:identifier>
</item>
<item>
<title>内网渗透 关于GPO - st404</title>
<link>http://www.cnblogs.com/st404/p/9788681.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/st404/p/9788681.html</guid>
<description>&lt;p&gt;　　网上有很多讲内网渗透的文章，但看来看去还是一老外的博客给力，博客地址：www.harmj0y.net/blog，看完就明白这里面的很多思路都非常好。&lt;/p&gt;
&lt;p&gt;　　做内网时，有时会碰到目标的机器开防火墙，所有端口基本都访问不到，这种就比较难办了。就自己知道的方法只能靠GPO了（&lt;a href=&quot;https://technet.microsoft.com/en-us/windowsserver/bb310732.aspx&quot;&gt;Group Policy Objects&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;　　GPO先简单知道这三点，详细解释可查查资料：&lt;/p&gt;
&lt;p&gt;　　域管理员可根据GPO批量给机器装软件，关闭防火墙等；&lt;/p&gt;
&lt;p&gt;　　机器默认1个半小时到两个小时之间自动刷新一次GPO；&lt;/p&gt;
&lt;p&gt;　　每台域机器都有一特定的GUID。　　&lt;/p&gt;
&lt;p&gt;　　但这会儿应该想到内网中这样批量搞动静会很大，我们能不能专门针对特定的机器运用GPO，因为GPO对应特定的一OUs(organizational units),通过OUs可以简单的定位到GUID，所以这是可行的。起初看到www.harmj0y.net/blog/redteaming/abusing-gpo-permissions 这篇文章心情有点小激动。&lt;/p&gt;
&lt;p&gt;　　因为这篇文章说可根据&lt;strong&gt;New-GPOImmediateTask&lt;/strong&gt;（powerview中有）命令生成schtask .xml文件，然后将它放入&amp;lt;GPO_PATH&amp;gt;\Machine\Preferences\ScheduledTasks路径下，这样等待1个半小时到两个小时后会自动刷新执行命令任务。&lt;/p&gt;
&lt;p&gt;　　经过自己搭环境，反复实验后确定这篇博客是有问题的，&lt;span&gt;&lt;strong&gt;首先schtask .xml文件必须通过在域控制器下的组策略控制器手动产生才能起作用，其次GPO控制器下只能是批量的，不能单个搞&lt;/strong&gt;&lt;/span&gt;。不知道作者博客中的这张图是最后怎么上线的。关于GPO对单个域机器的问题或许还有其他办法，只是自己没发现而已。作者的&lt;strong&gt;Get-NetGPO&lt;/strong&gt;这条命令挺好的，具体可详看作者的博客。&lt;/p&gt;
&lt;p&gt;　　&lt;img src=&quot;https://img2018.cnblogs.com/blog/1447391/201810/1447391-20181014230636701-2144612569.png&quot; alt=&quot;&quot; width=&quot;768&quot; height=&quot;426&quot;/&gt;&lt;/p&gt;
</description>
<pubDate>Sun, 14 Oct 2018 15:41:00 +0000</pubDate>
<dc:creator>st404</dc:creator>
<og:description>网上有很多讲内网渗透的文章，但看来看去还是一老外的博客给力，博客地址：www.harmj0y.net/blog，看完就明白这里面的很多思路都非常好。 做内网时，有时会碰到目标的机器开防火墙，所有端口基</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/st404/p/9788681.html</dc:identifier>
</item>
<item>
<title>傻瓜式的go modules的讲解和代码 - lgp20151222</title>
<link>http://www.cnblogs.com/ydymz/p/9788804.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/ydymz/p/9788804.html</guid>
<description>&lt;p&gt;国内关于gomod的文章，哪怕是使用了百度 -csdn，依然全是理论，虽然golang的使用者大多是大神但是也有像我这样的的弱鸡是不是？&lt;/p&gt;
&lt;p&gt;所以，我就写个傻瓜式教程了。&lt;/p&gt;
&lt;p&gt;github地址：&lt;a href=&quot;https://github.com/247292980/go_moudules_demo&quot; target=&quot;_blank&quot;&gt;https://github.com/247292980/go_moudules_demo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;代码很少很简单。。。。&lt;/p&gt;


&lt;p&gt;1.新建文件夹 go_moudiules_demo&lt;/p&gt;
&lt;p&gt;2.go mod之，生成gomod.go文件&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
go mod init go_moudiules_demo&lt;br/&gt;语法&lt;br/&gt;go mod init [module]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1095725/201810/1095725-20181014224841101-505005851.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;3.创建main.go，默认包名是gomod，需要改成main&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1095725/201810/1095725-20181014222428547-304067604.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;4.创建正真的存放代码的文件夹 demo和文件gomod.go，注意不能与main放在同一文件夹下，因为会造成包名冲突&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1095725/201810/1095725-20181014224959803-421992495.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt; 5.根据规则引入代码，这里有个坑，因为goland做的不太好，实际上golang的所有工具都做的不太好，导致代码报红，但是实际上go build/run还是能跑通的&lt;/p&gt;
&lt;p&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/1095725/201810/1095725-20181014225405079-306593380.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;当然goland也可以配置，就是不知道怎么去红名。。。　　&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1095725/201810/1095725-20181014225246288-1930892805.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;


&lt;p&gt;gomod最容易让人进了误区就是，把自己之前的代码都gomod一次，那么后面使用的时候直接根据gomod的package找之前的代码，简直美滋滋。&lt;/p&gt;
&lt;p&gt;毕竟是go moudules但是，实际上只是go moudule，他只管一个项目里的多个包。&lt;/p&gt;
&lt;p&gt;为什么造成这个误区呢？因为国内说的都是包管理，我还真以为是针对包的操作，然后第一次尝试失败后，翻了下官网&lt;/p&gt;
&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;36&quot;&gt;
&lt;pre class=&quot;brush:java;gutter:true;&quot;&gt;
A module is a collection of related Go packages. &lt;br/&gt;Modules are the unit of source code interchange and versioning.&lt;br/&gt;The go command has direct support for working with modules, including recording and resolving dependencies on other modules.&lt;br/&gt;Modules replace the old GOPATH-based approach to specifying which source files are used in a given build.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt; a collection of related Go packages.&lt;/strong&gt; &lt;code class=&quot;hljs&quot;&gt;相关Go包的集合，这玩意的理解真的是难，什么相关，相关的是什么？这时候根据官网的usage代码反向理解下&lt;/code&gt;go mod init [module]&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;hljs&quot;&gt;，显然是 &lt;code class=&quot;hljs&quot;&gt;module的相关Go包的集合，而module是一个单数啊。。。&lt;code class=&quot;hljs&quot;&gt;&lt;code class=&quot;hljs&quot;&gt;module&lt;/code&gt;&lt;/code&gt;和go mudules。。。我该如何理解啊。。。模板我倒是知道。。。总感觉这个怪不到谷歌头上，而且这玩意大家试个两下，就能找到正确理解也不算什么事。而且我要是把自己的代码都丢到github上同样不会报错，只是我是想着不丢到github上面的使用所以进了歪路。而且看后面的语法解析 &lt;span&gt;go mod download 看起来就像是能实现我说的效果的，就是国内没什么材料，我只好一个一个翻英文，，，&lt;/span&gt;&lt;/code&gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;而第二句&lt;strong&gt;Modules are the unit of source code interchange and versioning. &lt;strong&gt;Modules&lt;/strong&gt;&lt;/strong&gt; 是源码的版本控制和交换的单位，也就说明go mod之间是独立的，，，不能互调，除非在gopath里面。感觉大神看到这句两下都不用试了。。。&lt;/p&gt;

&lt;p&gt;主要是一个人的博客 http://blog.51cto.com/qiangmzsx/2164520?source=dra&lt;/p&gt;
&lt;p&gt;我把其中的关键抽出来，去掉他的代码，有兴趣的可以去原文看看&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;35&quot;&gt;
&lt;pre&gt;
&lt;span&gt;    go mod init:初始化modules
    go mod download:下载modules到本地cache
    go mod edit:编辑go.mod文件，选项有&lt;/span&gt;-json、-require和-&lt;span&gt;exclude，可以使用帮助go help mod edit
    go mod graph:以文本模式打印模块需求图
    go mod tidy:删除错误或者不使用的modules
    go mod vendor:生成vendor目录
    go mod verify:验证依赖是否正确
    go mod why：查找依赖

    go test    执行一下，自动导包

    go list &lt;/span&gt;-&lt;span&gt;m  主模块的打印路径
    go list &lt;/span&gt;-m -f=&lt;span&gt;{{.Dir}}  print主模块的根目录
    go list &lt;/span&gt;-m all  查看当前的依赖和版本信息
&lt;/pre&gt;&lt;/div&gt;

</description>
<pubDate>Sun, 14 Oct 2018 15:36:00 +0000</pubDate>
<dc:creator>lgp20151222</dc:creator>
<og:description>一 国内关于gomod的文章，哪怕是使用了百度 -csdn，依然全是理论，虽然golang的使用者大多是大神但是也有像我这样的的弱鸡是不是？ 所以，我就写个傻瓜式教程了。 github地址：https</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/ydymz/p/9788804.html</dc:identifier>
</item>
<item>
<title>时钟组件 - 青风无痕</title>
<link>http://www.cnblogs.com/fenghen/p/9788760.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/fenghen/p/9788760.html</guid>
<description>&lt;p&gt;16年8月，我还是一个实习生，一个不知道 js 基础的重要性的小白。有段时间，老大没有什么任务给我，无聊之际，想到用 canvas 画一个时钟。那会我还会经常去逛慕课网，于是将自己写的代码发到慕课网。这个是地址：&lt;a href=&quot;https://www.imooc.com/article/9695&quot; target=&quot;_blank&quot;&gt;https://www.imooc.com/article/9695&lt;/a&gt; ，之后，慕课网上就相继出现各种各样有关 canvas 时钟的教程或者文章（说这个只是想说明 canvas 时钟是我自己的想法，不是从别人那来的）。&lt;/p&gt;
&lt;p&gt;后来，兴趣来了，在原有的基础上添加一些比较实用的功能（我认为比较之前的，哈哈），将它整理成一个 jquery 组件。这个是组件的 github 地址 ：&lt;a href=&quot;https://github.com/lonlyape/clock&quot; target=&quot;_blank&quot;&gt;https://github.com/lonlyape/clock&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本来呢，这个组件的事就到这子，因为我的项目中也没咋用这组件。再加上，我后来的项目一般都是用 vue 写的，所以，这个组件就这样被搁那了。&lt;/p&gt;
&lt;p&gt;……&lt;/p&gt;
&lt;p&gt;去年7月（也就是17年7月），我们的 ui 给我的一张设计稿（一个搜索列表，每一条记录前有一个时钟 icon）让我想起了这个组件。这个不是可以用我的那个组件来实现吗，而且后台接口配合一下，还可以用这个展示每条搜索是在那个时刻发生的。于是我就把我这个组件给引进来了（当时那个项目原本就引进了 jquery，所以引入这个组件也没啥影响）。那时我就在想，既然这个组件有用，我现在的项目又都在用 vue 写，那我为何不把它写成 vue 组件的形式呢，于是，我又在空闲时间写起了这个组件，有时想法来了，图片背景？这个功能可以加，抽空加上了。罗马数字？这个可以加，抽空加上了。时区？这个有趣，加上……&lt;/p&gt;
&lt;p&gt;……&lt;/p&gt;
&lt;p&gt;断断续续，来到了今年9月（即18年9月）。这个组件稍微有点样子了，我何不把它放到 npm 上呢。要发布到 npm 上，我这个项目还要做一些规范的处理，比如符合 vue 组件的打包，比如使用文档。比如……等等。完成这些，时间来到了今年的10月。&lt;/p&gt;
&lt;p&gt;终于&lt;/p&gt;
&lt;p&gt;npm login&lt;/p&gt;
&lt;p&gt;npm publish&lt;/p&gt;
&lt;p&gt;我的这个组件发布到 npm 上了。&lt;/p&gt;
&lt;p&gt;这个是 vue 组件的 github 地址：&lt;a href=&quot;https://github.com/lonlyape/vue-clock&quot; target=&quot;_blank&quot;&gt;https://github.com/lonlyape/vue-clock&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在这里写我开发这个组件的经历，希望对你有点启发，也希望我写的这个组件能对你有所有帮助。&lt;/p&gt;
</description>
<pubDate>Sun, 14 Oct 2018 15:27:00 +0000</pubDate>
<dc:creator>青风无痕</dc:creator>
<og:description>16年8月，我还是一个实习生，一个不知道 js 基础的重要性的小白。有段时间，老大没有什么任务给我，无聊之际，想到用 canvas 画一个时钟。那会我还会经常去逛慕课网，</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/fenghen/p/9788760.html</dc:identifier>
</item>
<item>
<title>加密算法 - failymao</title>
<link>http://www.cnblogs.com/failymao/p/9788753.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/failymao/p/9788753.html</guid>
<description>&lt;p class=&quot;toc&quot;&gt;目录&lt;/p&gt;
&lt;h2 id=&quot;进位存储单位&quot;&gt;进位&amp;amp;存储单位&lt;/h2&gt;
&lt;p&gt;一种计数的方法&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;用有限的数字符号来表示无限的数值，例如阿拉伯数字的10进制(0-9)&lt;/li&gt;
&lt;li&gt;可使用的计数符号的数目决定了进位制，简称&lt;code&gt;进制&lt;/code&gt;
&lt;ul&gt;&lt;li&gt;2进制（0，1），计算机机器语言唯一能明白的&lt;/li&gt;
&lt;li&gt;16进制（0-9，A,B,C,D,E,F）,每一个16进制的字符代表4个人二进值组合的数字&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;进制间的关系
&lt;ul&gt;&lt;li&gt;10进制： 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16&lt;/li&gt;
&lt;li&gt;2进制： 0000 0001 0010 0011 0100 0101 0110 0111 1000 1001 1010 1011 1100 1101 1110 1111 10000&lt;/li&gt;
&lt;li&gt;16进制：0 1 2 3 4 5 6 7 8 9 A B C E F&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;计算机为什么不使用10进制而用2进制？&lt;br/&gt;为了稳定性！&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;计量术语&quot;&gt;计量术语&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;位(bit)比特-- 最小的数据单位&lt;/li&gt;
&lt;li&gt;字节(Byte)--8个bit组成，存储空间最小的单位&lt;/li&gt;
&lt;li&gt;k-kilo 表示千。1024 (2^10次方)&lt;br/&gt;...&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;加密&quot;&gt;加密&lt;/h2&gt;
&lt;h3 id=&quot;对称加密&quot;&gt;对称加密&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;用相同密钥对原文进行加密和解密&lt;/li&gt;
&lt;li&gt;加密过程： 密钥 + 原文 =&amp;gt; 密文&lt;/li&gt;
&lt;li&gt;解密过程： 密文 - 密钥 =&amp;gt; 原文&lt;/li&gt;
&lt;li&gt;缺点： 无法确保密钥被安全传递&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;非对称加密&quot;&gt;非对称加密&lt;/h3&gt;
&lt;p&gt;公钥&amp;amp;私钥&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;公钥用于加密，私钥用于解密&lt;/li&gt;
&lt;li&gt;公钥由私钥生成，私钥可以推导出公钥&lt;/li&gt;
&lt;li&gt;从公钥无法推导出私钥&lt;/li&gt;
&lt;li&gt;优点：解决了私钥传输中的安全性问题&lt;/li&gt;
&lt;li&gt;Q: 解决了信息传递的问题，如何验证是&quot;确实是发送方发送的， 信件没有被篡改&quot;？
&lt;ul&gt;&lt;li&gt;通过数字签名&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;哈希--hash&quot;&gt;哈希- Hash&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;将一段数据（任意长度）经过一道计算，转换为一段定长的数据&lt;/li&gt;
&lt;li&gt;不可逆性 - 几乎无法通过hash的结果推导出原文，即无法通过x的Hash值，而推导出x
&lt;ul&gt;&lt;li&gt;无法通过人的指纹瑞到处他是谁&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;无碰撞性， 几乎没有可能找到一个y, 使得y的HASH值等于的hash值
&lt;ul&gt;&lt;li&gt;几乎没有两个人的指纹是一样的&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;使用场景：
&lt;ul&gt;&lt;li&gt;校验文件的完整性&lt;/li&gt;
&lt;li&gt;服务器中使用hash存储用户名的密码&lt;/li&gt;
&lt;li&gt;数字签名&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;数字签名---digital-signature&quot;&gt;数字签名 - Digital Signature&lt;/h3&gt;
&lt;p&gt;目的 ： 为了证明发送人是想要接收信息的人&lt;/p&gt;
&lt;p&gt;流程：&lt;/p&gt;
</description>
<pubDate>Sun, 14 Oct 2018 15:26:00 +0000</pubDate>
<dc:creator>failymao</dc:creator>
<og:description>非对称加密</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/failymao/p/9788753.html</dc:identifier>
</item>
<item>
<title>Electron 创建一个空白的界面 - 唐宋元明清2188</title>
<link>http://www.cnblogs.com/kybs0/p/9788661.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/kybs0/p/9788661.html</guid>
<description>&lt;h2&gt;添加应用&lt;/h2&gt;
&lt;p&gt;首先添加一个Lorikeet版本的Electron应用。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;38&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; &lt;span&gt;'&lt;/span&gt;&lt;span&gt;use strict&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;
&lt;span&gt; 2&lt;/span&gt; &lt;span&gt;const&lt;/span&gt; electron = require(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;electron&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; &lt;span&gt;const&lt;/span&gt; app =&lt;span&gt; electron.app;
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt; &lt;span&gt;const&lt;/span&gt; BrowserWindow =&lt;span&gt; electron.BrowserWindow;
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt; 
&lt;span&gt; 6&lt;/span&gt; let mainWindow = &lt;span&gt;null&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt; 
&lt;span&gt; 8&lt;/span&gt; app.on(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;window-all-closed&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, () =&amp;gt;&lt;span&gt; {
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; (process.platform !== &lt;span&gt;'&lt;/span&gt;&lt;span&gt;darwin&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;&lt;span&gt;) {
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt; &lt;span&gt;        app.quit();
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt; &lt;span&gt;    }
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt; &lt;span&gt;});
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt; 
&lt;span&gt;14&lt;/span&gt; app.on(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;ready&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, () =&amp;gt;&lt;span&gt; {
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt;     mainWindow = &lt;span&gt;new&lt;/span&gt;&lt;span&gt; BrowserWindow();
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt;     mainWindow.loadURL(`file:&lt;span&gt;//&lt;/span&gt;&lt;span&gt;${app.getAppPath()}/index.html`);&lt;/span&gt;
&lt;span&gt;17&lt;/span&gt;     mainWindow.on(&lt;span&gt;'&lt;/span&gt;&lt;span&gt;closed&lt;/span&gt;&lt;span&gt;'&lt;/span&gt;, () =&amp;gt;&lt;span&gt; {
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt;         mainWindow = &lt;span&gt;null&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt; &lt;span&gt;    });
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt; });
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt; &lt;img src=&quot;https://img2018.cnblogs.com/blog/685541/201810/685541-20181014230646974-1714786445.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;菜单设置&lt;/h2&gt;
&lt;h3&gt; 1. 永久隐藏菜单&lt;/h3&gt;
&lt;div readability=&quot;7&quot;&gt;
&lt;p&gt;const electronMenu=electron.Menu;&lt;/p&gt;
&lt;div readability=&quot;7&quot;&gt;
&lt;div readability=&quot;9&quot;&gt;
&lt;p&gt;electronMenu.setApplicationMenu(null);&lt;/p&gt;
&lt;p&gt;另：也可以直接设置mainWindow.setMenu(null);&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('3acc9229-a535-450c-84aa-9982855b9091')&quot; readability=&quot;34&quot;&gt;&lt;img id=&quot;code_img_closed_3acc9229-a535-450c-84aa-9982855b9091&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_3acc9229-a535-450c-84aa-9982855b9091&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('3acc9229-a535-450c-84aa-9982855b9091',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_3acc9229-a535-450c-84aa-9982855b9091&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;63&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; 'use strict'
&lt;span&gt; 2&lt;/span&gt; const electron = require('electron'&lt;span&gt;);
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; const app =&lt;span&gt; electron.app;
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt; const BrowserWindow =&lt;span&gt; electron.BrowserWindow;
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt; const electronMenu=&lt;span&gt;electron.Menu;
&lt;/span&gt;&lt;span&gt; 6&lt;/span&gt; 
&lt;span&gt; 7&lt;/span&gt; let mainWindow = &lt;span&gt;null&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt; 8&lt;/span&gt; 
&lt;span&gt; 9&lt;/span&gt; app.on('window-all-closed', () =&amp;gt;&lt;span&gt; {
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; (process.platform !== 'darwin'&lt;span&gt;) {
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt; &lt;span&gt;        app.quit();
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt; &lt;span&gt;    }
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt; &lt;span&gt;});
&lt;/span&gt;&lt;span&gt;14&lt;/span&gt; 
&lt;span&gt;15&lt;/span&gt; app.on('ready', () =&amp;gt;&lt;span&gt; {
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt;     electronMenu.setApplicationMenu(&lt;span&gt;null&lt;/span&gt;&lt;span&gt;);
&lt;/span&gt;&lt;span&gt;17&lt;/span&gt;     mainWindow = &lt;span&gt;new&lt;/span&gt;&lt;span&gt; BrowserWindow();
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt;     mainWindow.loadURL(`file:&lt;span&gt;//&lt;/span&gt;&lt;span&gt;${app.getAppPath()}/index.html`);&lt;/span&gt;
&lt;span&gt;19&lt;/span&gt;     mainWindow.on('closed', () =&amp;gt;&lt;span&gt; {
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;         mainWindow = &lt;span&gt;null&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;21&lt;/span&gt; &lt;span&gt;    });
&lt;/span&gt;&lt;span&gt;22&lt;/span&gt; });
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;h3&gt;2. 自动隐藏菜单&lt;/h3&gt;
&lt;div readability=&quot;9.4052558782849&quot;&gt;
&lt;p&gt;设置BrowserWindow的属性，autoHideMenuBar： true。&lt;/p&gt;
&lt;p&gt;设置autoHideMenuBar隐藏后，按Alt，可显示出菜单选项。&lt;/p&gt;
&lt;div readability=&quot;6.4828178694158&quot;&gt;
&lt;div class=&quot;cnblogs_code&quot; onclick=&quot;cnblogs_code_show('9d828270-fb59-47e1-a390-97cba4fb83b7')&quot; readability=&quot;34&quot;&gt;&lt;img id=&quot;code_img_closed_9d828270-fb59-47e1-a390-97cba4fb83b7&quot; class=&quot;code_img_closed&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif&quot; alt=&quot;&quot;/&gt;&lt;img id=&quot;code_img_opened_9d828270-fb59-47e1-a390-97cba4fb83b7&quot; class=&quot;code_img_opened&quot; onclick=&quot;cnblogs_code_hide('9d828270-fb59-47e1-a390-97cba4fb83b7',event)&quot; src=&quot;https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif&quot; alt=&quot;&quot;/&gt;&lt;div id=&quot;cnblogs_code_open_9d828270-fb59-47e1-a390-97cba4fb83b7&quot; class=&quot;cnblogs_code_hide&quot; readability=&quot;63&quot;&gt;
&lt;pre&gt;
&lt;span&gt; 1&lt;/span&gt; 'use strict'
&lt;span&gt; 2&lt;/span&gt; const electron = require('electron'&lt;span&gt;);
&lt;/span&gt;&lt;span&gt; 3&lt;/span&gt; const app =&lt;span&gt; electron.app;
&lt;/span&gt;&lt;span&gt; 4&lt;/span&gt; const BrowserWindow =&lt;span&gt; electron.BrowserWindow;
&lt;/span&gt;&lt;span&gt; 5&lt;/span&gt; 
&lt;span&gt; 6&lt;/span&gt; let mainWindow = &lt;span&gt;null&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt; 7&lt;/span&gt; 
&lt;span&gt; 8&lt;/span&gt; app.on('window-all-closed', () =&amp;gt;&lt;span&gt; {
&lt;/span&gt;&lt;span&gt; 9&lt;/span&gt;     &lt;span&gt;if&lt;/span&gt; (process.platform !== 'darwin'&lt;span&gt;) {
&lt;/span&gt;&lt;span&gt;10&lt;/span&gt; &lt;span&gt;        app.quit();
&lt;/span&gt;&lt;span&gt;11&lt;/span&gt; &lt;span&gt;    }
&lt;/span&gt;&lt;span&gt;12&lt;/span&gt; &lt;span&gt;});
&lt;/span&gt;&lt;span&gt;13&lt;/span&gt; 
&lt;span&gt;14&lt;/span&gt; app.on('ready', () =&amp;gt;&lt;span&gt; {
&lt;/span&gt;&lt;span&gt;15&lt;/span&gt;     mainWindow = &lt;span&gt;new&lt;/span&gt; BrowserWindow({autoHideMenuBar：&lt;span&gt;true&lt;/span&gt;&lt;span&gt;});
&lt;/span&gt;&lt;span&gt;16&lt;/span&gt;     mainWindow.loadURL(`file:&lt;span&gt;//&lt;/span&gt;&lt;span&gt;${app.getAppPath()}/index.html`);&lt;/span&gt;
&lt;span&gt;17&lt;/span&gt;     mainWindow.on('closed', () =&amp;gt;&lt;span&gt; {
&lt;/span&gt;&lt;span&gt;18&lt;/span&gt;         mainWindow = &lt;span&gt;null&lt;/span&gt;&lt;span&gt;;
&lt;/span&gt;&lt;span&gt;19&lt;/span&gt; &lt;span&gt;    });
&lt;/span&gt;&lt;span&gt;20&lt;/span&gt; });
&lt;/pre&gt;&lt;/div&gt;
&lt;span class=&quot;cnblogs_code_collapse&quot;&gt;View Code&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;另：自定义菜单设置，可参考&lt;a href=&quot;https://segmentfault.com/a/1190000008473121&quot; target=&quot;_blank&quot;&gt;https://segmentfault.com/a/1190000008473121&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;
&lt;h2&gt;窗口设置&lt;/h2&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt; 直接设置Window的属性&lt;/p&gt;
&lt;div readability=&quot;11&quot;&gt;
&lt;p&gt;mainWindow = new BrowserWindow({width:600,height:400,icon: 'images/myIcon.ico'});&lt;/p&gt;
&lt;p&gt;另：如果需要隐藏window的标题栏，可以设置frame:false;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/685541/201810/685541-20181014232135661-1087472351.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;图上：&lt;a href=&quot;https://files.cnblogs.com/files/kybs0/lorikeet-electron.zip&quot; target=&quot;_blank&quot;&gt;Demo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt; 注：&lt;a href=&quot;https://github.com/demopark/electron-api-demos-Zh_CN&quot; rel=&quot;nofollow noreferrer&quot; target=&quot;_blank&quot;&gt;GitHub官方文档与Demo: electron-api-demos-Zh_CN&lt;/a&gt;&lt;/p&gt;

</description>
<pubDate>Sun, 14 Oct 2018 15:24:00 +0000</pubDate>
<dc:creator>唐宋元明清2188</dc:creator>
<og:description>添加应用 首先添加一个Lorikeet版本的Electron应用。 菜单设置 1. 永久隐藏菜单 const electronMenu=electron.Menu; electronMenu.setA</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/kybs0/p/9788661.html</dc:identifier>
</item>
<item>
<title>Scheme来实现八皇后问题(1) - 窗户</title>
<link>http://www.cnblogs.com/Colin-Cai/p/9768105.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/Colin-Cai/p/9768105.html</guid>
<description>&lt;div class=&quot;cnblogs_Highlighter&quot; readability=&quot;34&quot;&gt;
&lt;pre class=&quot;brush:sql;gutter:true;&quot;&gt;
　　版权申明：本文为博主窗户(Colin Cai)原创，欢迎转帖。如要转贴，必须注明原文网址

　　http://www.cnblogs.com/Colin-Cai/p/9768105.html 

　　作者：窗户

　　QQ/微信：6679072

　　E-mail：6679072@qq.com 
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;　　看到有人写八皇后，那我就也写写这个吧。&lt;/p&gt;

&lt;p&gt;　　&lt;strong&gt;&lt;span&gt;八皇后问题&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　　这个问题大家应该都不陌生，很多计算机教程都以八皇后为例题。&lt;/p&gt;
&lt;p&gt;　　&lt;img src=&quot;https://img2018.cnblogs.com/blog/1151747/201810/1151747-20181011155033401-1675650246.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;　　上面是一个国际象棋棋盘，总共8X8个格子。&lt;/p&gt;
&lt;p&gt;　　皇后是国际象棋里杀力最强的子，它可以吃掉同一条横线、竖线上其他棋子，也可以吃掉所在的两条斜线上的其他棋子（当然在角上只有一条斜线）。&lt;/p&gt;
&lt;p&gt;　　&lt;img src=&quot;https://img2018.cnblogs.com/blog/1151747/201810/1151747-20181011155624161-1529676479.jpg&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;　　能否在棋盘上放更多的皇后，让彼此之间不能互相吃到？基于很显然一行或者一列最多只有一个皇后，那么这个8X8的棋盘是否可以放8个皇后？&lt;/p&gt;

&lt;p&gt;　　&lt;strong&gt;&lt;span&gt;解的表示&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　　8个皇后的表示可以用坐标，那么就是8个坐标的集合，其中行、列都是范围1~8的数字。&lt;/p&gt;
&lt;p&gt;　　考虑到每一行都只有一个，我们完全可以用让8个皇后按照行坐标进行从小到大排序，那么必然8个皇后的行坐标分别是1、2、3、4、5、6、7、8，于是这都是无用的信息。又因为只有8列，而且任意两个皇后都不能同列，从而每一列也有且只有一个，从而刚才排序之后的8个皇后的纵坐标序列是1、2、3、4、5、6、7、8的一个排列。于是每一种可行的解对应着1、2、3、4、5、6、7、8的一个排列。&lt;/p&gt;
&lt;p&gt;　　考虑更一般的情况，n皇后问题：nXn的棋盘上放n个皇后，要求彼此之间不互相吃。那么它的&lt;strong&gt;&lt;span&gt;每一个解对应着1~n的一个排列&lt;/span&gt;&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;　　&lt;span&gt;&lt;strong&gt;解法框架&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;　　一种做法就是先找到1~n的所有排列，然后筛选符合条件的结果。&lt;/p&gt;
&lt;p&gt; 　　那么利用filter算子最终代码很容易给出:&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
&lt;span&gt;(define (queen n)
 (filter
  valid&lt;/span&gt;?&lt;span&gt;
  (P n)
 )
)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;　　这里的(P n)是所有的1~n排列的集合，这里排列当然用list来表示，集合也用list来表示。&lt;/p&gt;
&lt;p&gt;　　集合的每个元素是没有序的关系，所以逻辑上表示集合的list我们应该忽略其各个元素的序的差别。&lt;/p&gt;
&lt;p&gt;　　比如(P 2)表示的是'((1 2) (2 1))，或者是'((2 1) (1 2))，无论哪种实现，都是可行的。&lt;/p&gt;

&lt;p&gt;　　valid?是个谓词函数(返回bool值的函数)，它的作用是对于某个具体排列，判断其表示的n个皇后有没有互相吃的情况：&lt;/p&gt;
&lt;p&gt;　　如果有两个皇后互相吃，那么这个排列不可以作为最后的解，应当返回假，Scheme里也就是#f；&lt;/p&gt;
&lt;p&gt;　　如果不存在两个皇后互相吃，那么这个排列可以作为最后的皆，从而应当返回真，Scheme里也就是#t。&lt;/p&gt;

&lt;p&gt;　　filter算子就是使用valid?这样的谓词函数来过滤后面的集合，&lt;/p&gt;
&lt;p&gt;　　比如(filter even? '(1 2 3 4 5 6 7 8 9 10))就是抓取其中为偶数的元素组成的集合，那么当然返回'(2 4 6 8 10)。&lt;/p&gt;
&lt;p&gt;　　filter这么常用的算子似乎并未出现在r5rs中，很奇怪，我在这里就给出一个实现如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
(define (filter boolf &lt;span&gt;set&lt;/span&gt;&lt;span&gt;)
 (cond
  ((&lt;/span&gt;&lt;span&gt;null&lt;/span&gt;? &lt;span&gt;set&lt;/span&gt;) &lt;span&gt;'&lt;/span&gt;&lt;span&gt;())&lt;/span&gt;
  ((boolf (car &lt;span&gt;set&lt;/span&gt;)) (cons (car &lt;span&gt;set&lt;/span&gt;) (filter boolf (cdr &lt;span&gt;set&lt;/span&gt;&lt;span&gt;))))
  (&lt;/span&gt;&lt;span&gt;else&lt;/span&gt; (filter boolf (cdr &lt;span&gt;set&lt;/span&gt;&lt;span&gt;)))
 )
)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;　　接下去就是P函数和valid?函数的实现。&lt;/p&gt;

&lt;p&gt;　　&lt;span&gt;&lt;strong&gt;全排列&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;　　第一个问题就是要解决1~n的所有排列，可能会有人考虑将所有的排列用字典排序依次输出。&lt;/p&gt;
&lt;p&gt;　　不过这一般是迭代的思想，而对于一种Lisp，我们第一反应一般是&lt;span&gt;&lt;strong&gt;递归&lt;/strong&gt;&lt;/span&gt;。&lt;/p&gt;

&lt;p&gt;　　假设我们已经有1~n-1的全排列了，那么我们怎么得到1~n的全排列呢？&lt;/p&gt;
&lt;p&gt;　　我们可以取1~n-1的一个排列，不妨用字母标注&lt;/p&gt;
&lt;p&gt;　　a&lt;sub&gt;1&lt;/sub&gt; a&lt;sub&gt;2&lt;/sub&gt; ... a&lt;sub&gt;n-1&lt;/sub&gt;&lt;/p&gt;
&lt;p&gt;　　我们希望找个位置插入n，得到新的1~n的排列。&lt;/p&gt;
&lt;p&gt;　　这个插入点一共有n个，分别为：&lt;/p&gt;
&lt;p&gt;　　&lt;span&gt;a&lt;sub&gt;1&lt;/sub&gt;之前&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　a&lt;sub&gt;1&lt;/sub&gt;和a&lt;sub&gt;2&lt;/sub&gt;之间&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　a&lt;sub&gt;2&lt;/sub&gt;和a&lt;sub&gt;3&lt;/sub&gt;之间&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　...&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　a&lt;sub&gt;n-2&lt;/sub&gt;和a&lt;sub&gt;n-1&lt;/sub&gt;之间&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　&lt;/span&gt;&lt;span&gt;a&lt;sub&gt;n-1&lt;/sub&gt;之后&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　从而可以得到n个1~n的排列。&lt;/p&gt;
&lt;p&gt;　　而对1~n-1的所有排列都这么做，则构成了1~n的所有排列，且不存在重复。&lt;/p&gt;

&lt;p&gt;　　比如1~2的所有排列组成的集合为&lt;/p&gt;
&lt;p&gt;　　((1 2) (2 1))&lt;/p&gt;
&lt;p&gt;　　现在我们要用它生成1~3的全排列&lt;/p&gt;
&lt;p&gt;　　对于(1 2)，有3个插入点，插入3，得到三个排列&lt;/p&gt;
&lt;p&gt;　　(3 1 2) (1 3 2) (1 2 3)&lt;/p&gt;
&lt;p&gt;　　对于(2 1)，有3个插入点，插入3，得到三个排列&lt;/p&gt;
&lt;p&gt;　　(3 2 1) (2 3 1) (2 1 3)&lt;/p&gt;
&lt;p&gt;　　以上6个排列组成的集合就是我们所需要的结果。&lt;/p&gt;

&lt;p&gt;　　首先，当然要建立一个往列表某个位置插值的函数list-insert,带三个参数，将列表lst的位置pos插入v。而对于位置的解释是，列表头之前的位置称为0，然后依次增加。比如(1 2 3)的位置1插入4，得到列表(1 4 2 3)。这个很容易用递归设计出来，如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
(define (list-&lt;span&gt;insert lst pos v)
 (&lt;/span&gt;&lt;span&gt;if&lt;/span&gt; (zero?&lt;span&gt; pos) (cons v lst)
  (cons (car lst) (list&lt;/span&gt;-insert (cdr lst) (- pos &lt;span&gt;1&lt;/span&gt;&lt;span&gt;) v))
 )
)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;　　上述(list-insert '(1 2 3) 1 4)，运算返回'(1 4 2 3)&lt;/p&gt;

&lt;p&gt;　　按照上面的递归思想，我们使用map算子先写一点测试测试，我们希望从1~2的全排列推到1~3的全排列&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　(map&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　 (lambda (x) (map (lambda (m) (list-insert x m 3)) '(0 1 2))) ;对于每个排列，给出0、1、2三个位置插入3&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　 '((1 2)(2 1))&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　结果为&lt;/p&gt;
&lt;p&gt;　　&lt;span&gt;'(((3 1 2) (1 3 2) (1 2 3)) ((3 2 1) (2 3 1) (2 1 3)))&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;　　这很像我们所要的，但似乎又不是，因为我们需要应该是'((3 1 2) (1 3 2) (1 2 3) (3 2 1) (2 3 1) (2 1 3))&lt;/p&gt;
&lt;p&gt;　　实际上，(apply append '(((3 1 2) (1 3 2) (1 2 3)) ((3 2 1) (2 3 1) (2 1 3))))就是我们需要的结果了。&lt;/p&gt;
&lt;p&gt;　　而apply是把最后一个参数(这个参数一定要是i列表)展开。&lt;/p&gt;
&lt;p&gt;　　于是上述就成了(append '((3 1 2) (1 3 2) (1 2 3)) '((3 2 1) (2 3 1) (2 1 3)) )，当然就是我们需要的结果了。&lt;/p&gt;

&lt;p&gt;　　而只有1个元1的全排列集合就是'((1))，这是递归的边界，&lt;/p&gt;
&lt;p&gt;　　结合上述，全排列的函数定义应该如下：&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;33&quot;&gt;
&lt;pre&gt;
&lt;span&gt;(define (P n)
 (&lt;/span&gt;&lt;span&gt;if&lt;/span&gt; (= n &lt;span&gt;1&lt;/span&gt;) &lt;span&gt;'&lt;/span&gt;&lt;span&gt;((1))&lt;/span&gt;
&lt;span&gt;  (apply append 
   (map 
    (lambda (x) (map (lambda (m) (list&lt;/span&gt;-insert x m n)) (range &lt;span&gt;0&lt;/span&gt;&lt;span&gt; n)))
    (P (&lt;/span&gt;- n &lt;span&gt;1&lt;/span&gt;&lt;span&gt;))
   )
  )
 )
)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;　　&lt;strong&gt;&lt;span&gt;判断合法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;　　目前只剩下valid?函数的实现了。实际上，在我们开始采用用1~n排序来作为最后的解的时候，已经把棋盘中同行同列的情况给排除了。于是，valid?函数实际上是要判断是否有两个棋子在同一个斜线上。&lt;/p&gt;
&lt;p&gt;　　比如'(1 3 6 4 2 5 8 7)表示如图的八个皇后，皇后的位置被打了红圈&lt;/p&gt;
&lt;p&gt;　　&lt;img src=&quot;https://img2018.cnblogs.com/blog/1151747/201810/1151747-20181014224713830-1552455456.jpg&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;　　其中存在着皇后互吃，&lt;/p&gt;
&lt;p&gt;　　&lt;img src=&quot;https://img2018.cnblogs.com/blog/1151747/201810/1151747-20181014224904802-1860372898.jpg&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;　　在数据上看，'(1 3 6 4 2 5 8 7)，其中&lt;/p&gt;
&lt;p&gt;　　1和4相差3，距离也为3(1在列表的第0个位置，4在列表的第3个位置，所以距离为3)；&lt;/p&gt;
&lt;p&gt;　　3和8相差5，距离也为5；&lt;/p&gt;
&lt;p&gt;　　8和7相差1，距离也为1。&lt;/p&gt;
&lt;p&gt;　　对应着上面三对互吃的皇后。&lt;/p&gt;

&lt;p&gt;　　我们这里可以用迭代来完成，这有点类似于过程式语言的循环了。&lt;/p&gt;
&lt;p&gt;　　从左到右先距离为1的，看看有没有值也相差1的，如果有，那么valid?返回假，也就是#f&lt;/p&gt;
&lt;p&gt;　　然后从左到右再扫距离为2的....&lt;/p&gt;
&lt;p&gt;　　...&lt;/p&gt;
&lt;p&gt;　　最后当距离到n的时候，直接返回真，也就是#f(因为最左边和最右边距离达到，也就是n-1，此时代表所有可能都已扫过)&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;35&quot;&gt;
&lt;pre&gt;
(define (_valid? x left-&lt;span&gt;pos distance)
 (cond
  ;当距离以及达到列表长度了，扫完了，返回真
  ((&lt;/span&gt;=&lt;span&gt; distance (length x)) #t)
  ;如果发现差值等于距离，这一对皇后互吃，返回假
  ((&lt;/span&gt;= distance (abs (- (list-&lt;span&gt;ref&lt;/span&gt; x left-pos) (list-&lt;span&gt;ref&lt;/span&gt; x (+ left-&lt;span&gt;pos distance))))) #f)
  ;如果这个距离还没扫完，那么往后推一个扫
  ((&lt;/span&gt;&amp;lt; (+ left-pos distance) (- (length x) &lt;span&gt;1&lt;/span&gt;)) (_valid? x (+ left-pos &lt;span&gt;1&lt;/span&gt;&lt;span&gt;) distance))
  ;否则，这个距离的已经扫完，距离加1，从最左边开始扫
  (&lt;/span&gt;&lt;span&gt;else&lt;/span&gt; (_valid? x &lt;span&gt;0&lt;/span&gt; (+ distance &lt;span&gt;1&lt;/span&gt;&lt;span&gt;)))
 )
)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;　　用它实现valid?，初始的时候，从left-pos为0，distance为1的一对皇后开始扫起&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;32&quot;&gt;
&lt;pre&gt;
(define (valid?&lt;span&gt; x)
 (_valid&lt;/span&gt;? x &lt;span&gt;0&lt;/span&gt; &lt;span&gt;1&lt;/span&gt;&lt;span&gt;)
)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;　　&lt;span&gt;&lt;strong&gt;运行&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;　　我们就拿8个皇后来测试一下，计算&lt;span&gt;(queen 8)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　得到　　&lt;/p&gt;
&lt;p&gt;&lt;span&gt;((4 7 3 8 2 5 1 6) (3 6 4 2 8 5 7 1) (3 5 2 8 6 4 7 1) (6 3 7 2 4 8 1 5) (3 6 8 2 4 1 7 5) (3 7 2 8 6 4 1 5) (3 5 2 8 1 7 4 6) (6 3 7 2 8 5 1 4) (3 6 2 7 5 1 8 4) (3 6 2 5 8 1 7 4) (7 3 8 2 5 1 6 4) (3 7 2 8 5 1 4 6) (3 6 2 7 1 4 8 5) (4 2 7 3 6 8 5 1) (4 2 7 3 6 8 1 5) (5 2 4 6 8 3 1 7) (5 2 4 7 3 8 6 1) (2 4 6 8 3 1 7 5) (5 7 2 6 3 1 8 4) (5 7 2 6 3 1 4 8) (8 2 5 3 1 7 4 6) (2 7 3 6 8 5 1 4) (7 2 6 3 1 4 8 5) (2 6 8 3 1 4 7 5) (4 7 5 2 6 1 3 8) (6 4 2 8 5 7 1 3) (4 2 5 8 6 1 3 7) (4 2 7 5 1 8 6 3) (7 4 2 5 8 1 3 6) (4 2 8 5 7 1 3 6) (4 6 8 2 7 1 3 5) (7 4 2 8 6 1 3 5) (4 2 8 6 1 3 5 7) (5 7 2 4 8 1 3 6) (2 5 7 4 1 8 6 3) (6 8 2 4 1 7 5 3) (7 2 4 1 8 5 3 6) (8 2 4 1 7 5 3 6) (5 2 6 1 7 4 8 3) (5 2 8 1 4 7 3 6) (2 7 5 8 1 4 6 3) (6 2 7 1 4 8 5 3) (2 6 1 7 4 8 3 5) (2 5 7 1 3 8 6 4) (6 2 7 1 3 5 8 4) (2 8 6 1 3 5 7 4) (4 7 5 3 1 6 8 2) (4 8 5 3 1 7 2 6) (4 6 8 3 1 7 5 2) (5 3 8 4 7 1 6 2) (3 5 8 4 1 7 2 6) (3 6 4 1 8 5 7 2) (6 3 7 4 1 8 2 5) (3 8 4 7 1 6 2 5) (6 3 5 7 1 4 2 8) (6 3 5 8 1 4 2 7) (3 5 7 1 4 2 8 6) (3 6 8 1 4 7 5 2) (6 3 1 8 4 2 7 5) (7 5 3 1 6 8 2 4) (5 3 1 6 8 2 4 7) (5 3 1 7 2 8 6 4) (6 3 1 7 5 8 2 4) (6 3 1 8 5 2 4 7) (3 6 8 1 5 7 2 4) (7 3 1 6 8 5 2 4) (3 1 7 5 8 2 4 6) (8 3 1 6 2 5 7 4) (5 7 4 1 3 8 6 2) (5 8 4 1 3 6 2 7) (4 1 5 8 6 3 7 2) (6 4 7 1 3 5 2 8) (8 4 1 3 6 2 7 5) (4 8 1 3 6 2 7 5) (5 7 1 3 8 6 4 2) (1 6 8 3 7 4 2 5) (7 1 3 8 6 4 2 5) (5 1 8 6 3 7 2 4) (1 5 8 6 3 7 2 4) (5 8 4 1 7 2 6 3) (6 4 1 5 8 2 7 3) (4 6 1 5 2 8 3 7) (4 7 1 8 5 2 6 3) (4 8 1 5 7 2 6 3) (4 1 5 8 2 7 3 6) (6 4 7 1 8 2 5 3) (5 1 4 6 8 2 7 3) (5 7 1 4 2 8 6 3) (5 1 8 4 2 7 3 6) (1 7 4 6 8 2 5 3) (1 7 5 8 2 4 6 3) (6 1 5 2 8 3 7 4))&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;　　一共92个解。&lt;/p&gt;
</description>
<pubDate>Sun, 14 Oct 2018 15:08:00 +0000</pubDate>
<dc:creator>窗户</dc:creator>
<og:description>看到有人写八皇后，那我就也写写这个吧。 八皇后问题 这个问题大家应该都不陌生，很多计算机教程都以八皇后为例题。 上面是一个国际象棋棋盘，总共8X8个格子。 皇后是国际象棋里杀力最强的子，它可以吃掉同一</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/Colin-Cai/p/9768105.html</dc:identifier>
</item>
<item>
<title>【自编码】变分自编码大杂烩 - 文字妖精</title>
<link>http://www.cnblogs.com/wzyj/p/9766655.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/wzyj/p/9766655.html</guid>
<description>&lt;p&gt;&lt;span&gt;&lt;strong&gt;1.&lt;span&gt;变分自编码&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;         变分是数学上的概念，大致含义是寻求一个中间的函数，通过改变中间函数来查看目标函数的改变。变分推断是变分自编码的核心，那么变分推断是要解决的是什么问题??&lt;/p&gt;
&lt;p&gt;问题描述如下，假如我们有一批样本X，这个时候，我们想生成一批和它类似的样本，且分布相同，这个时候我们该怎么办呢?&lt;/p&gt;
&lt;p&gt;1.如果我们知道样本的分布的情况下，这个事情就好办了，先生成一批均匀分布的样本，通过分布的具体形式与均匀分布之间的关系，生成一批新的样本。&lt;/p&gt;
&lt;p&gt;2.如果我们不知道样本分布的情况下，仍然可以通过一些方法得到类似的样本，例如MCMC过程，Gibbs-Sample等&lt;/p&gt;
&lt;p&gt;更加详细的推断和过程，可以在&lt;a href=&quot;http://vdisk.weibo.com/s/q0sGh/1360334108?utm_source=weibolife&quot; target=&quot;_blank&quot;&gt;LDA数学八卦&lt;/a&gt;来寻找答案。&lt;/p&gt;
&lt;p&gt;        神经网络拥有拟合出任意函数的特点，那么使用它来拟合我们的数据分布可以不?答案是肯定的，AutoEncoder的就是为了尽可能拟合原始数据而服务的，但是一般的AutoEncoder在工程中大部分只是被用来作为降维的手段，并没有产生新样本的功能，那就是VAE(变分自编码)的能解决的问题了。&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;1.1 &lt;span&gt;变分推断&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;限于本人数学造诣只是二把刀，下面的论断大部分来自网上相关博客，引用已经写在下面，有疑问，大家勿喷。下面的公式来自于 &lt;/span&gt;&lt;a href=&quot;https://www.dropbox.com/s/v6ua3d9yt44vgb3/cover_and_thesis.pdf?dl=0&amp;amp;utm_campaign=Revue+newsletter&amp;amp;utm_medium=Newsletter&amp;amp;utm_source=NLP+News&quot; target=&quot;_blank&quot;&gt;VARIATIONAL INFERENCE  &amp;amp; DEEP LEARNING: &lt;em id=&quot;__mceDel&quot;&gt;&lt;em id=&quot;__mceDel&quot;&gt;&lt;em id=&quot;__mceDel&quot;&gt;A NEW SYNTHESIS&lt;/em&gt;&lt;/em&gt;&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/377271/201810/377271-20181014180546391-1489863292.png&quot; alt=&quot;&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/377271/201810/377271-20181014180711867-1995520443.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;公式的推倒主要使用了贝叶斯公式和詹森不等式，有兴趣的同学可以尝试一下。下面我们只分析这个结果，以及用途。&lt;/p&gt;
&lt;p&gt;变分推断给出了一个下界，我们可以通过最大化$L_{\theta,\phi }(x)$ 来同时获得$\phi ,\theta $的最优解，从而获得需要逼近的数据分布。比较有趣的是$D_{KL}(q_{\phi }(z|x)||p_{\theta }(z|x))$决定了两个“距离”:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt; KL散度决定了先验假设的分布和真实分布的相似性。&lt;/li&gt;
&lt;li&gt;$L_{\theta,\phi }(x)$ 和最大相似似然函数$log p_{\theta }(x)$之间的间隔。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;所以最大化KL散度既能使得$q_{\phi }(z|x)$趋近于真实的分布$p_{\theta }(z|x)$,又使得$L_{\theta,\phi }(x)$和$log p_{\theta }(x)$的“间隔”变得更小。&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;1.2 &lt;span&gt;VAE的学习过程&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/377271/201810/377271-20181014211128557-1019145838.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;如上面幅图显示的如此，VAE包含两部分：Encoder和Decoder，前者又被成为推理模型，后者又被成为生成模型。正常来说$q_{D}(x)$的分布是比较复杂的，而映射之后的z空间在比较简单(通常假设为我们常见的分布，像高斯分布)，生成模型学习一个联合分布$p_{\theta }(x,z)=p_{\theta }(z)p_{\theta }(x|z)$，该联合分布分解为两部分，首先是z空间的先验分布$p_{\theta }(z)$和随机解码器$p_{\theta }(x|z)$。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/377271/201810/377271-20181014210925625-92440224.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;上面这张图给出了模型训练的伪代码，值得注意的是下面两点:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;在实际工程当中，我们经常假设z服从高斯分布，从理论上讲可以是任何分布，也可以是均匀分布，但是均匀分布在训练过程中计算KL散度可能得到无穷大值。&lt;/li&gt;
&lt;li&gt;中间采样的过程是怎么加入到神经网络的BP算法中的?这个地方是作者采用了 &lt;strong&gt;reparemerization&lt;/strong&gt; 的方法，在实际训练中$z=g(\varepsilon ,\phi ,x)$满足该分布，其中$\varepsilon$是独立于$\phi$和x的，经常把$\varepsilon$ 看作原始分布的一种噪音，这样随机独立的噪音可以与模型训练无关，我们只关注于$\phi$和x，抽样的过程则是不同$\varepsilon$ 的由z分布生成的样本。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;1.3 &lt;span&gt;比较常见的VAE模型&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;隐含变量z服从高斯分布，对应的目标函数变成&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/377271/201810/377271-20181014214848833-1926403688.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;中间的抽样过程如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/377271/201810/377271-20181014215019542-441298142.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;训练模型时值得注意&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;生成模型$p_{\theta }(x|z)$也经常假设一种分布，如果是二值的数据的话，我们经常假设为伯努利分布；如果是真实的数据的话，经常假设为高斯分布。&lt;/li&gt;
&lt;li&gt;生成模型的输出，都是独立的，也就是说神经网络的最后一层的每个输出不能使用softmax这种彼此互斥的输出的函数。&lt;/li&gt;
&lt;li&gt;输入模型的数据建议归一化一下，因为网上有些代码中会使用mse作为对$p_{\theta }(x)$的等价，其实拟合的输出是高斯分布的情况(&lt;span&gt;具体证明参见Pattern Recognition and Machine Learning 第5.2章节&lt;/span&gt;)，对于非归一化的数据，mse倾向于优化变化范围大的列。归一化之后，可以使用binary_cross-entropy (这里使用的是多输出的情况)。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;1.4 &lt;span&gt;网上的例子&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py&quot; target=&quot;_blank&quot;&gt;https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/vae.py&quot; target=&quot;_blank&quot;&gt;https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/vae.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;第一个着重于实践，是使用keras来写的，后者是使用tensorflow写的，比较符合数学上的公式推导，我们来一起分析一下代码的重点部分&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;62&quot;&gt;
&lt;pre&gt;
&lt;span&gt;def&lt;/span&gt;&lt;span&gt; make_encoder(activation, latent_size, base_depth):
  &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;Creates the encoder function.
  Args:
    activation: Activation function in hidden layers.
    latent_size: The dimensionality of the encoding.
    base_depth: The lowest depth for a layer.
  Returns:
    encoder: A `callable` mapping a `Tensor` of images to a
      `tfd.Distribution` instance over encodings.
  &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;
  conv &lt;/span&gt;=&lt;span&gt; functools.partial(
      tf.keras.layers.Conv2D, padding&lt;/span&gt;=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;SAME&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;, activation=&lt;span&gt;activation)

  encoder_net &lt;/span&gt;=&lt;span&gt; tf.keras.Sequential([
      conv(base_depth, &lt;/span&gt;5, 1&lt;span&gt;),
      conv(base_depth, &lt;/span&gt;5, 2&lt;span&gt;),
      conv(&lt;/span&gt;2 * base_depth, 5, 1&lt;span&gt;),
      conv(&lt;/span&gt;2 * base_depth, 5, 2&lt;span&gt;),
      conv(&lt;/span&gt;4 * latent_size, 7, padding=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;VALID&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;),
      tf.keras.layers.Flatten(),
      &lt;span&gt;tf.keras.layers.Dense(&lt;/span&gt;&lt;/span&gt;&lt;span&gt;2 * latent_size, activation=&lt;/span&gt;&lt;span&gt;&lt;span&gt;None),&lt;/span&gt;
  ])

  &lt;/span&gt;&lt;span&gt;def&lt;/span&gt;&lt;span&gt; encoder(images):
    images &lt;/span&gt;= 2 * tf.cast(images, dtype=tf.float32) - 1&lt;span&gt;
    net &lt;/span&gt;=&lt;span&gt; encoder_net(images)
    &lt;/span&gt;&lt;span&gt;return&lt;/span&gt;&lt;span&gt; tfd.&lt;span&gt;MultivariateNormalDiag&lt;/span&gt;(
        loc&lt;/span&gt;=&lt;span&gt;net[..., :latent_size],
        scale_diag&lt;/span&gt;=tf.nn.softplus(net[..., latent_size:] +&lt;span&gt;
                                  _softplus_inverse(&lt;/span&gt;1.0&lt;span&gt;)),
        name&lt;/span&gt;=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;code&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;)

  &lt;/span&gt;&lt;span&gt;return&lt;/span&gt; encoder
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;首先是encoder，encoder部分的最后一层的大小是2*lant_size，其中在构建高斯分布的时候，0~ lant_size个输出作为了Mean，后lant_size个输出作为了方差。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;69&quot;&gt;
&lt;pre&gt;
&lt;span&gt;def&lt;/span&gt;&lt;span&gt; make_decoder(activation, latent_size, output_shape, base_depth):
  &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;Creates the decoder function.
  Args:
    activation: Activation function in hidden layers.
    latent_size: Dimensionality of the encoding.
    output_shape: The output image shape.
    base_depth: Smallest depth for a layer.
  Returns:
    decoder: A `callable` mapping a `Tensor` of encodings to a
      `tfd.Distribution` instance over images.
  &lt;/span&gt;&lt;span&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;span&gt;
  deconv &lt;/span&gt;=&lt;span&gt; functools.partial(
      tf.keras.layers.Conv2DTranspose, padding&lt;/span&gt;=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;SAME&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;, activation=&lt;span&gt;activation)
  conv &lt;/span&gt;=&lt;span&gt; functools.partial(
      tf.keras.layers.Conv2D, padding&lt;/span&gt;=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;SAME&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;, activation=&lt;span&gt;activation)

  decoder_net &lt;/span&gt;=&lt;span&gt; tf.keras.Sequential([
      deconv(&lt;/span&gt;2 * base_depth, 7, padding=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;VALID&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;),
      deconv(&lt;/span&gt;2 * base_depth, 5&lt;span&gt;),
      deconv(&lt;/span&gt;2 * base_depth, 5, 2&lt;span&gt;),
      deconv(base_depth, &lt;/span&gt;5&lt;span&gt;),
      deconv(base_depth, &lt;/span&gt;5, 2&lt;span&gt;),
      deconv(base_depth, &lt;/span&gt;5&lt;span&gt;),
      conv(output_shape[&lt;/span&gt;-1], 5, activation=&lt;span&gt;None),
  ])

  &lt;/span&gt;&lt;span&gt;def&lt;/span&gt;&lt;span&gt; decoder(codes):
    original_shape &lt;/span&gt;=&lt;span&gt; tf.shape(codes)
    &lt;/span&gt;&lt;span&gt;#&lt;/span&gt;&lt;span&gt; Collapse the sample and batch dimension and convert to rank-4 tensor for&lt;/span&gt;
    &lt;span&gt;#&lt;/span&gt;&lt;span&gt; use with a convolutional decoder network.&lt;/span&gt;
    codes = tf.reshape(codes, (-1, 1, 1&lt;span&gt;, latent_size))
    logits &lt;/span&gt;=&lt;span&gt; decoder_net(codes)
    logits &lt;/span&gt;=&lt;span&gt; tf.reshape(
        logits, shape&lt;/span&gt;=tf.concat([original_shape[:-1], output_shape], axis=&lt;span&gt;0))
    &lt;/span&gt;&lt;span&gt;return&lt;/span&gt; tfd.&lt;span&gt;Independent&lt;/span&gt;(tfd.&lt;span&gt;Bernoulli&lt;/span&gt;(logits=&lt;span&gt;logits),
                           reinterpreted_batch_ndims&lt;/span&gt;=&lt;span&gt;len(output_shape),
                           name&lt;/span&gt;=&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;image&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;)

  &lt;/span&gt;&lt;span&gt;return&lt;/span&gt; decoder
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;在decoder中我们看到，输出都是彼此独立的，且是伯努利分布。&lt;/p&gt;
&lt;div class=&quot;cnblogs_code&quot; readability=&quot;38&quot;&gt;
&lt;pre&gt;
approx_posterior =&lt;span&gt; encoder(features)//数据进行encoder处理，得到高斯分布的参数
approx_posterior_sample &lt;/span&gt;= approx_posterior.sample(params[&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;n_samples&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;])//从得到的高斯函数中抽样，并没有使用重参数的方法
decoder_likelihood &lt;/span&gt;=&lt;span&gt; decoder(approx_posterior_sample)//将抽样的样本输入给decoder，得到伯努利分布的输出
distortion &lt;/span&gt;= -&lt;span&gt;decoder_likelihood.log_prob(features)//得到-logp(x)

latent_prior &lt;/span&gt;= make_mixture_prior(params[&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;latent_size&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;],
                                    params[&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;mixture_components&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;])//得到一个标准的多维高斯分布，主要是为了计算KL的时候使用

&lt;/span&gt;&lt;span&gt;if&lt;/span&gt; params[&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;analytic_kl&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;]:
    rate &lt;/span&gt;=&lt;span&gt; tfd.kl_divergence(approx_posterior, latent_prior)
  &lt;/span&gt;&lt;span&gt;else&lt;/span&gt;&lt;span&gt;:
    rate &lt;/span&gt;=&lt;span&gt; (approx_posterior.log_prob(approx_posterior_sample)
            &lt;/span&gt;-&lt;span&gt; latent_prior.log_prob(approx_posterior_sample))//该方法是指另外一种衡量生成分布和高斯分布之间距离的形式
  avg_rate &lt;/span&gt;=&lt;span&gt; tf.reduce_mean(rate)
  tf.summary.scalar(&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;rate&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt;&lt;span&gt;, avg_rate)

  elbo_local &lt;/span&gt;= -(rate +&lt;span&gt; distortion)

  elbo &lt;/span&gt;=&lt;span&gt; tf.reduce_mean(elbo_local)
  loss &lt;/span&gt;= -elbo
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;上述则是loss函数的组成了，其中features就是原始输入。&lt;/p&gt;

&lt;p&gt;注：&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;输入的数据是binarized_mnist，所以输出用的伯努利分布，伯努利之前的输出层并没有任何激活函数。&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;2.VAE条件自编码&lt;/p&gt;
&lt;p&gt;3.使用VAE做异常检测&lt;/p&gt;
&lt;p&gt;4.使用VAE做聚类&lt;/p&gt;

&lt;p&gt;待续&lt;/p&gt;
</description>
<pubDate>Sun, 14 Oct 2018 14:40:00 +0000</pubDate>
<dc:creator>文字妖精</dc:creator>
<og:description>1.变分自编码 变分是数学上的概念，大致含义是寻求一个中间的函数，通过改变中间函数来查看目标函数的改变。变分推断是变分自编码的核心，那么变分推断是要解决的是什么问题?? 问题描述如下，假如我们有一批样</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/wzyj/p/9766655.html</dc:identifier>
</item>
<item>
<title>机器学习排序算法：RankNet to LambdaRank to LambdaMART - RL-Learning</title>
<link>http://www.cnblogs.com/genyuan/p/9788294.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/genyuan/p/9788294.html</guid>
<description>&lt;p&gt;&lt;span class=&quot;html-tag&quot;&gt;使用机器学习排序算法LambdaMART有一段时间了，但一直没有真正弄清楚算法中的所有细节。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;html-tag&quot;&gt;学习过程中细读了两篇不错的博文，推荐给大家：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/pinard/p/6140514.html&quot; target=&quot;_blank&quot;&gt;&lt;span class=&quot;html-tag&quot;&gt;梯度提升树(GBDT)原理小结&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/starzhou/article/details/52413366&quot; target=&quot;_blank&quot;&gt;&lt;span class=&quot;html-tag&quot;&gt;徐博From RankNet to LambdaRank to LambdaMART: An Overview&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;但经过一番搜寻之后发现，目前网上并没有一篇透彻讲解该算法的文章，所以希望这篇文章能够达到此目的。&lt;/p&gt;
&lt;p&gt;本文主要参考微软研究院2010年发表的文章&lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/from-ranknet-to-lambdarank-to-lambdamart-an-overview/&quot; target=&quot;_blank&quot;&gt;From RankNet to LambdaRank to LambdaMART: An Overview&lt;/a&gt;$^1$，并结合自己的理解，试图将RankNet、LambdaRank和LambdaMART这三种算法的所有算法细节讲解透彻。&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;1. 概述&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;RankNet、LambdaRank和LambdaMART是三个关系非常紧密的机器学习排序算法。简而言之，RankNet是最基础，基于神经网络的排序算法；而LambdaRank在RankNet的基础上修改了梯度的计算方式，也即加入了lambda梯度；LambdaMART结合了lambda梯度和MART（另称为GBDT，梯度提升树）。这三种算法在工业界中应用广泛，在BAT等国内大厂和微软谷歌等世界互联网巨头内部都有大量应用，还曾经赢得“Yahoo！Learning To Rank Challenge(Track 1)&quot;的冠军。本人认为如果评选当今工业界中三种最重要的机器学习算法，以LambdaMART为首的集成学习算法肯定占有一席之地，另外两个分别是支持向量机和深度学习。&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;2. RankNet&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;2.1 算法基础定义&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;RankNet解决如下搜索排序问题：给定query集合，每个query都对应着一个文档集合，如何对每个query返回排序后的文档集合。可以想象这样的场景：某位高考生在得知自己的成绩后，准备报考志愿。听说最近西湖大学办得不错，所以就想到网上搜搜关于西湖大学的资料。他打开一个搜索引擎，输入“西湖大学”四个字，然后点击“搜索”，页面从上到下显示了10条搜索结果，他认为排在上面的肯定比下面的相关，所以就开始从上往下一个个地浏览。所以RankNet的目标就是对所有query，都能将其返回的文档按照相关性进行排序。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;RankNet网络将输入query的特征向量$x\in \mathbb{R}^n$映射为一个实数$f(x) \in \mathbb{R}$。RankNet采用pairwise的方法进行模型训练。具体地，给定特定query下的两个文档$U_i$和$U_j$，其特征向量分别为$x_i$和$x_j$，经过RankNet进行前向计算得到对应的分数为$s_i=f(x_i)$和$s_j=f(x_j)$。用$U_i \rhd U_j$表示$U_i$比$U_j$排序更靠前（如对某个query来说，$U_i$被标记为“good”，$U_j$被标记为“bad”）。继而可以用下面的公式来表示$U_i$应该比$U_j$排序更靠前的概率：$$P_{ij} \equiv P(U_i \rhd U_j) \equiv \frac{1}{1+e^{-\sigma(s_i-s_j)}}$$这个概率实际上就是深度学习中经常使用的sigmoid函数，参数$\sigma$决定sigmoid函数的形状。对于特定的query，定义$S_{ij} \in \{0,\pm1\}$为文档$i$和文档$j$被标记的标签之间的关联，即&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;$$ S_{ij}=\left\{&lt;br/&gt;\begin{aligned}&lt;br/&gt;1&amp;amp;&amp;amp;     文档i比文档j更相关\\&lt;br/&gt;0&amp;amp;&amp;amp;    文档i和文档j相关性一致\\&lt;br/&gt;-1&amp;amp;&amp;amp;   文档j比文档i更相关&lt;br/&gt;\end{aligned}&lt;br/&gt;\right.&lt;br/&gt;$$&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;html-tag&quot;&gt;定义$\overline{P}_{ij}=\frac{1}{2}(1+S_{ij})$表示$U_i$应该比$U_j$排序更靠前的已知概率，则可以用交叉熵定义优化目标的损失函数：$$C=-\overline{P}_{ij}log{P_{ij}}-(1-\overline{P}_{ij})log(1-P_{ij})$$&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;html-tag&quot;&gt;如果不太熟悉什么是交叉熵，可以参考宗成庆老师的《统计自然语言处理》2.2节“信息论基本概念”，里面将熵、联合熵、互信息、相对熵、交叉熵和困惑度等概念都讲得相当清楚。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;结合以上多个公式，可以改写损失函数$C$为：$$C=\frac{1}{2}(1-S_{ij})\sigma(s_i-s_j)+log(1+e^{-\sigma(s_i-s_j)})$$&lt;/p&gt;
&lt;p&gt;对于$S_{ij}=1$，$$C=log\left(1+e^{-\sigma(s_i-s_j)}\right)$$&lt;/p&gt;
&lt;p&gt;然而对于$S_{ij}=-1$，$$C=log\left(1+e^{-\sigma(s_j-s_i)}\right)$$&lt;/p&gt;
&lt;p&gt;可以看出损失函数$C$具有对称性，也即交换$i$和$j$的位置，损失函数的值不变。&lt;/p&gt;
&lt;p&gt;分析损失函数$C$的趋势发现，如果对文档$U_i$和$U_j$的打分可以正确地拟合标记的标签，则$C$趋向于0，否则$C$趋向于线性函数。具体地，假如$S_{ij}=1$，也即$U_i$应该比$U_j$排序高，如果$s_i&amp;gt;s_j$，则拟合的分数可以正确排序文档$i$和文档$j$，$$\lim \limits_{s_i-s_j\rightarrow\infty}C=\lim \limits_{s_i-s_j\rightarrow\infty}log\left(1+e^{-\sigma(s_i-s_j)}\right)=log1=0$$&lt;/p&gt;
&lt;p&gt;如果$s_i&amp;lt;s_j$，则拟合的分数不能正确排序文档$i$和文档$j$，$$\lim \limits_{s_i-s_j\rightarrow\infty}C=\lim \limits_{s_i-s_j\rightarrow\infty}log\left(1+e^{-\sigma(s_i-s_j)}\right)=log\left(e^{-\sigma(s_i-s_j)}\right)=-\sigma(s_i-s_j)$$&lt;/p&gt;
&lt;p&gt;利用神经网络对模型进行训练，目前最有效的方法就是反向传播算法。反向传播算法中最核心部分就是损失函数对模型参数的求导，然后可以使用下面的公式对模型参数进行迭代更新：&lt;/p&gt;
&lt;p&gt;$$w_k\rightarrow{w_k}-\eta\frac{\partial{C}}{\partial{w_k}}={w_k}-\eta\left(\frac{\partial{C}}{\partial{s_i}}\frac{\partial{s_i}}{\partial{w_k}}+\frac{\partial{C}}{\partial{s_j}}\frac{\partial{s_j}}{\partial{w_k}}\right)$$&lt;/p&gt;
&lt;p&gt;损失函数$C$对$s_i$和$s_j$的偏导数为：$$\frac{\partial{C}}{\partial{s_i}}=\sigma\left(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma(s_i-s_j)}}\right)=-\frac{\partial{C}}{\partial{s_j}}$$&lt;/p&gt;
&lt;p&gt;$s_i$和$s_j$对$w_k$的偏导数可根据神经网络求偏导数的方式求得。求得了损失函数$C$对神经网络模型参数$w_k$的偏导数之后，就可以使用梯度下降算法对其更新。这里的学习率$\eta$也是一个正数，因为$\eta$需要满足下面的不等式：$$\delta C=\sum_{k}\frac{\partial{C}}{\partial{w_k}}\delta w_k=\sum_{k}\frac{\partial{C}}{\partial{w_k}}\left(-\eta\frac{\partial{C}}{\partial{w_k}}\right)=-\eta\sum_{k}\left(\frac{\partial{C}}{\partial{w_k}}\right)^2&amp;lt;0$$&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2.2 RankNet分解形式：加速RankNet训练过程&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.1节中定义的RankNet，对于每一个文档对$(U_i$,$U_j)$都将计算损失函数对神经网络的参数$w_k$的偏导数，然后更新模型参数$w_k$。这样做的缺点在于，对模型参数更新慢，耗时长。所以本节讲解如何通过分解组合的方式加快这一训练过程。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对于给定的文档对$U_i$和$U_j$，损失函数$C$对参数$w_k$的偏导数为：$$\frac{\partial{C}}{\partial{w_k}}=\frac{\partial{C}}{\partial{s_i}}\frac{\partial{s_i}}{\partial{w_k}}+\frac{\partial{C}}{\partial{s_j}}\frac{\partial{s_j}}{\partial{w_k}}=\sigma\left(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma(s_i-s_j)}}\right)\left(\frac{\partial{s_i}}{\partial{w_k}}-\frac{\partial{s_j}}{\partial{w_k}}\right)=\lambda_{ij}\left(\frac{\partial{s_i}}{\partial{w_k}}-\frac{\partial{s_j}}{\partial{w_k}}\right)$$&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;其中：$$\lambda_{ij}=\frac{\partial{C(s_i-s_j)}}{\partial{s_i}}=\sigma\left(\frac{1}{2}(1-S_{ij})-\frac{1}{1+e^{\sigma(s_i-s_j)}}\right)$$&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;定义$I$为索引对$\{i,j\}$的集合，在不损失信息量的情况下，可以将集合$I$中的索引对都转换成满足$U_i \rhd U_j$的形式。另外集合$I$中的索引对还应该满足最多只出现一次的条件。在此基础上，累加权重参数$w_k$的更新量：$$\delta w_k=-\eta\sum_{(i,j) \in I}\left(\lambda_{ij}\frac{\partial{s_i}}{\partial{w_k}}-\lambda_{ij}\frac{\partial{s_j}}{\partial{w_k}}\right)=-\eta\sum_{i}\lambda_i\frac{\partial{s_j}}{\partial{w_k}}$$&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;其中：$$\lambda_i=\sum_{j:\{i,j\} \in I}\lambda_{ij}-\sum_{j:\{j,i\} \in I}\lambda_{ij}$$&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;通俗地说，$\lambda_i$就是集合$I$中所有$\{i,j\}$的$\lambda_{ij}$的和$-$集合$I$中所有$\{j,i\}$的$\lambda_{ij}$的和。如果还是不太明白，那看下面这个例子就明白了。集合$I=\{\{1,2\},\{2,3\},\{1,3\}\}$，则&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;$$\delta w_k=-\eta\sum_{\{i,j\}\in I}\left(\lambda_{ij}\frac{\partial{s_i}}{\partial{w_k}}-\lambda_{ij}\frac{\partial{s_j}}{\partial{w_k}}\right)=-\eta\left(\lambda_{12}\frac{\partial{s_1}}{\partial{w_k}}-\lambda_{12}\frac{\partial{s_2}}{\partial{w_k}}+\lambda_{13}\frac{\partial{s_1}}{\partial{w_k}}-\lambda_{13}\frac{\partial{s_3}}{\partial{w_k}}+\lambda_{23}\frac{\partial{s_2}}{\partial{w_k}}-\lambda_{23}\frac{\partial{s_3}}{\partial{w_k}}\right)=-\eta\left((\lambda_{12}+\lambda_{13})\frac{\partial{s_1}}{\partial{w_k}}+(\lambda_{23}-\lambda_{12})\frac{\partial{s_2}}{\partial{w_k}}+(-\lambda_{23}-\lambda_{13})\frac{\partial{s_3}}{\partial{w_k}}\right)$$&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;于是可以得到$\lambda_1=\lambda_{12}+\lambda_{13}$，$\lambda_2=\lambda_{23}-\lambda_{12}$，$\lambda_3=-\lambda_{23}-\lambda_{13}$&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;$\lambda_i$可以看成是作用在排序文档上的力，其正负代表了方向，长度代表了力的大小。最初的实现是对每个文档对，都计算一遍梯度并且更新神经网络的参数值，而这里则是将同一个query下的所有文档对进行叠加，然后更新一次网络的权重参数。这种分解组合形式实际上就是一种小批量学习方法，不仅可以加快迭代速度，还可以为后面使用非连续的梯度模型打下基础。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;2.3 模型训练过程示例&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;假设某个搜索系统中，文档用2维的特征向量表示。给定一个query下的三个文档向量分别为$x_1=(5,4.5)^T$，$x_2=(4,3.7)^T$和$x_3=(2,1.8)^T$，标记情况为$U_1 \rhd U_2 \rhd U_3$。为了简化训练过程，这里采用单层的神经网络模型，即输入层大小2，输出层大小为1，输出值为$f(x)=w_0+w_1x^{(1)}+w_2x^{(2)}$。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;初始化$\mathbf{w}=[0, -1, 1]$，控制sigmoid函数形状的$\sigma=0.1$，神经网络学习率$\eta=0.1$。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;根据以上初始值可以计算出$s_1=-0.5$，$s_2=-0.3$和$s_3=-0.2$，可见此时三个文档输出的分数并不满足标记$U_1 \rhd U_2 \rhd U_3$。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;计算$\lambda_1=\lambda_{12}+\lambda_{13}=-0.1012$，$\lambda_2=\lambda_{23}-\lambda_{12}=0.0002$，$\lambda_3=-\lambda_{23}-\lambda_{13}=-0.1010$。&lt;/p&gt;
&lt;p&gt;$\delta w_0=-\eta\left(\lambda_1\frac{\partial{s_1}}{\partial{w_0}}+\lambda_2\frac{\partial{s_2}}{\partial{w_0}}+\lambda_3\frac{\partial{s_3}}{\partial{w_0}}\right)=0$&lt;/p&gt;
&lt;p&gt;$\delta w_1=-\eta\left(\lambda_1\frac{\partial{s_1}}{\partial{w_1}}+\lambda_2\frac{\partial{s_2}}{\partial{w_1}}+\lambda_3\frac{\partial{s_3}}{\partial{w_1}}\right)=3.032$&lt;/p&gt;
&lt;p&gt;$\delta w_2=-\eta\left(\lambda_1\frac{\partial{s_1}}{\partial{w_2}}+\lambda_2\frac{\partial{s_2}}{\partial{w_2}}+\lambda_3\frac{\partial{s_3}}{\partial{w_2}}\right)=2.7286$&lt;/p&gt;
&lt;p&gt;更新网络权重:&lt;/p&gt;
&lt;p&gt;$w_0=w0+\delta w_0=0+0=0$&lt;/p&gt;
&lt;p&gt;$w_1=w1+\delta w_1=-1+3.032=2.032$&lt;/p&gt;
&lt;p&gt;$w_2=w2+\delta w_2=1+2.7286=3.7286$&lt;/p&gt;
&lt;p&gt;使用更新后的权重重新计算三个文档的分数，分别为$s_1=26.9387$，$s_2=21.92382$，$s_3=10.77548$。可见，经过一轮训练，单层神经网络的输出分数已经可以很好地拟合标记的标签。&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;3. 信息检索评分&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;信息检索研究者经常使用的排序质量评分指标有以下四种：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MRR&lt;/strong&gt;(Mean Reciprocal Rank)，平均倒数排名&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MAP&lt;/strong&gt;(Mean Average Precision)，平均正确率均值&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NDCG&lt;/strong&gt;(Normalized Discounted Cumulative Gain)，归一化折损累积增益&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ERR&lt;/strong&gt;(Expected Reciprocal Rank)，预期倒数排名&lt;/p&gt;
&lt;p&gt;&lt;span&gt;其中，MRR和MAP只能对二级的相关性（排序等级：相关和不相关）进行评分，而NDCG和ERR则可以对多级的相关性（排序等级&amp;gt;2）进行评分。NDCG和ERR的另一个优点是更关注排名靠前的文档，在计算分数时会给予排名靠前的文档更高的权重。但是这两种评分方式的缺点是函数不连续，不能进行求导，所以也就不能简单地将这两种评分方式加入到模型的损失函数中去。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;3.1 MRR&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对于一个查询$i$来说，$rank_i$表示第一个相关结果的排序位置，所以：$$MRR(Q)=\frac{1}{|Q|}\sum_{i=1}^{|Q|}\frac{1}{rank_i}$$&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;$|Q|$表示查询的数量，$MRR$表示搜索系统在查询集$Q$下的平均倒数排名值。$MRR$只能度量检索结果只有一个并且相关性等级只有相关和不相关两种的情况。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;举个简单例子：&lt;/span&gt;&lt;/p&gt;
&lt;table border=&quot;2&quot;&gt;&lt;tbody readability=&quot;3&quot;&gt;&lt;tr&gt;&lt;td&gt;查询语句&lt;/td&gt;
&lt;td&gt;查询结果&lt;/td&gt;
&lt;td&gt;正确结果&lt;/td&gt;
&lt;td&gt;排序位置&lt;/td&gt;
&lt;td&gt;排序倒数&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;机器学习&lt;/td&gt;
&lt;td&gt;快速排序，深度学习，并行计算&lt;/td&gt;
&lt;td&gt;深度学习&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1/2&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;苹果手机&lt;/td&gt;
&lt;td&gt;小米手机，华为手机，iphone 7&lt;/td&gt;
&lt;td&gt;iphone 7&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1/3&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;小米移动电源&lt;/td&gt;
&lt;td&gt;小米移动电源，华为充电器，苹果充电插头&lt;/td&gt;
&lt;td&gt;小米移动电源&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span&gt;所以$MRR(Q)=\frac{1/2+1/3+1}{3}=\frac{11}{18}$&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt; 3.2 MAP&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;假定信息需求$q_j \in Q$对应的所有相关文档集合为${d_{1},...,d_{mj}}$，$R_{jk}$是返回结果中直到遇到$d_k$后其所在位置前（含$d_k$）的所有文档的集合，则定义$MAP(Q)^2$如下：&lt;/p&gt;
&lt;p&gt;$$MAP(Q)=\frac{1}{|Q|}\sum_{j=1}^{|Q|}\frac{1}{m_j}\sum_{k=1}^{m_j}Precision(R_{jk})$$&lt;/p&gt;
&lt;p&gt;实际上有两种计算$MAP$的方法或者说有两种$MAP(Q)$的定义方法。第一种方法是在每篇相关文档所在位置上求正确率然后平均（参考上面的公式）。另一种是在每个召回率水平上计算此时的插值正确率，然后求11点平均正确率，最后在不同查询之间计算平均。前者也称为非插值$MAP(Q)$。一般提$MAP(Q)$都指前者，所有这里也只讨论前者。&lt;/p&gt;
&lt;p&gt;如果对定义的公式不太理解，可以结合下面的例子进行理解。&lt;/p&gt;
&lt;table border=&quot;2&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;查询1：机器学习&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;查询2：苹果手机&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;排序位置&lt;/td&gt;
&lt;td&gt;是否相关&lt;/td&gt;
&lt;td&gt;排序位置&lt;/td&gt;
&lt;td&gt;是否相关&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;是&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;否&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;是&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;是&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;否&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;是&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;否&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;否&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;是&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;否&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;否&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;是&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;否&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;是&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;针对上面检索的结果，可计算出&lt;/p&gt;
&lt;p&gt;$AP(1)=\left(1*1+1*1+2/3*0+2/4*0+3/5*1+3/6*0+3/7*0\right)/3=\frac{13}{15}$&lt;/p&gt;
&lt;p&gt;$AP(2)=\left(0*0+1/2*1+2/3*1+2/4*0+2/5*0+3/6*1+4/7*1\right)/4=\frac{47}{84}$&lt;/p&gt;
&lt;p&gt;$MAP(Q)=\frac{AP(1)+AP(2)}{2}=\frac{13/15+47/84}{2}=\frac{599}{420}$&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;3.3 NDCG&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;NDCG是基于前$k$个检索结果进行计算的。设$R(j,m)$是评价人员给出的文档$d$对查询$j$的相关性得分，那么有：&lt;/p&gt;
&lt;p&gt;$$NDCG(Q,k)=\frac{1}{|Q|}\sum_{j=1}^{|Q|}Z_{j,k}\sum_{m=1}^{k}\frac{2^{R(j,m)}-1}{log(1+m)}$$&lt;/p&gt;
&lt;p&gt;其中$$DCG_k=\sum_{m=1}^{k}\frac{2^{R(j,m)}-1}{log(1+m)}$$&lt;/p&gt;
&lt;p&gt;$Z_{j,k}$为第$j$个查询的DCG归一化因子，用于保证对于查询$j$最完美系统的$DCG_k$得分是1。$Z_{j,k}$也可以用$\frac{1}{IDCG_k}$表示。$m$是返回文档的位置。如果某查询返回的文档数$k'&amp;lt;k$，那么上述公式只需要计算到$k'$为止。&lt;/p&gt;
&lt;p&gt;修改上面简单的例子进行辅助理解：&lt;/p&gt;
&lt;table border=&quot;2&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;查询1：机器学习&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;查询2：苹果手机&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;排序位置&lt;/td&gt;
&lt;td&gt;相关程度&lt;/td&gt;
&lt;td&gt;排序位置&lt;/td&gt;
&lt;td&gt;相关程度&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;对于查询1：机器学习:&lt;/p&gt;
&lt;p&gt;$$DCG_7=\sum_{m=1}^{7}\frac{2^{R(j,m)}-1}{log(1+m)}=21.421516$$&lt;/p&gt;
&lt;p&gt;查询1返回结果的最佳相关程度排序为：3,3,2,2,2,1,0，所以，$IDCG_7=22.686817$，$NDCG_7=\frac{DCG_7}{IDCG_7}=0.944227$&lt;/p&gt;
&lt;p&gt;对于查询2：苹果手机:&lt;/p&gt;
&lt;p&gt;$$DCG_7=\sum_{m=1}^{7}\frac{2^{R(j,m)}-1}{log(1+m)}=18.482089$$&lt;/p&gt;
&lt;p&gt;查询2返回结果的最佳相关程度排序为：3,3,2,2,2,1,1，所以，$IDCG_7=23.167716$，$NDCG_7=\frac{DCG_7}{IDCG_7}=0.797752$&lt;/p&gt;
&lt;p&gt;最后可得：$NDCG(Q,7)=(0.944227+0.797752)/2=0.870990$&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;3.4 ERR&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;$ERR^3$旨在改善NDCG计算当前结果时未考虑排在前面结果的影响的缺点，提出了一种基于级联模型的评价指标。首先定义：&lt;/p&gt;
&lt;p&gt;$$R(g)=\frac{2^g-1}{2^{g_{max}}}, g \in \{0,1,...,g_{max}\}$$&lt;/p&gt;
&lt;p&gt;$g$代表文档的得分级别，$g_{max}$代表最大的分数级别。&lt;/p&gt;
&lt;p&gt;于是定义：&lt;/p&gt;
&lt;p&gt;$$ERR=\sum_{r=1}^{n}\frac{1}{r}\prod_{i=1}^{r-1}(1-R_i)R_r$$&lt;/p&gt;
&lt;p&gt;展开公式如下：&lt;/p&gt;
&lt;p&gt;$$ERR=R_1+\frac{1}{2}(1-R_1)R_2+\frac{1}{3}(1-R_1)(1-R_2)R_3+...+\frac{1}{n}(1-R_1)(1-R_2)...(1-R_{n-1})R_n$$ &lt;/p&gt;
&lt;p&gt;举例来说($g_{max}=3$):&lt;/p&gt;
&lt;table border=&quot;2&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;查询：机器学习&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;排序位置&lt;/td&gt;
&lt;td&gt;相关程度&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span&gt;$R_1=0.875,R2=0.375,R_3=0.875,R_4=0.125$&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;$ERR=0.875+\frac{1}{2}*0.125*0.375+\frac{1}{3}*0.125*0.625*0.875+\frac{1}{4}*0.125*0.625*0.125*0.125=0.913391$&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;4. LambdaRank&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;4.1 为什么需要LambdaRank&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;先看一张论文原文中的图，如下所示。这是一组用二元等级相关性进行排序的链接地址，其中浅灰色代表链接与query不相关，深蓝色代表链接与query相关。 对于左边来说，总的pairwise误差为13，而右边总的pairwise误差为11。但是大多数情况下我们更期望能得到左边的结果。这说明最基本的pairwise误差计算方式并不能很好地模拟用户对搜索引擎的期望。右边黑色箭头代表RankNet计算出的梯度大小，红色箭头是期望的梯度大小。NDCG和ERR在计算误差时，排名越靠前权重越大，可以很好地解决RankNet计算误差时的缺点。但是NDCG和ERR均是不可导的函数，如何加入到RankNet的梯度计算中去？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img title=&quot;图1&quot; src=&quot;https://img2018.cnblogs.com/blog/436630/201810/436630-20181008202412844-1605451333.png&quot; alt=&quot;图1&quot;/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;4.2  LambdaRank定义&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;RankNet中的$\lambda_{ij}$可以看成是$U_i$和$U_j$中间的作用力，如果$U_i \rhd U_j$，则$U_j$会给予$U_i$向上的大小为$|\lambda_{ij}$的推动力，而对应地$U_i$会给予$U_j$向下的大小为$|\lambda_{ij}$的推动力。如何将NDCG等类似更关注排名靠前的搜索结果的评价指标加入到排序结果之间的推动力中去呢？实验表明，直接用$|\Delta_{NDCG}|$乘以原来的$\lambda_{ij}$就可以得到很好的效果，也即：&lt;/p&gt;
&lt;p&gt;$$\lambda_{ij}=\frac{\partial{C(s_i-s_j)}}{\partial{s_i}}=\frac{-\sigma}{1+e^{\sigma(s_i-s_j)}}|\Delta_{NDCG}|$$&lt;/p&gt;
&lt;p&gt;其中$|\Delta_{NDCG}|$是交换排序结果$U_i$和$U_j$得到的NDCG差值。NDCG倾向于将排名高并且相关性高的文档更快地向上推动，而排名地而且相关性较低的文档较慢地向上推动。&lt;/p&gt;
&lt;p&gt;另外还可以将$|\Delta_{NDCG}|$替换成其他的评价指标。&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;5. LambdaMART&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;5.1 MART&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;LambdaMART是MART和LambdaRank的结合，所以要学习LambdaMART首先得了解什么是MART。MART是Multiple Additive Regession Tree的简称，很多时候又称为GBDT（Gradient Boosting Decision Tree）。MART是一种集成学习算法，不同于经典的集成学习算法Adaboost利用前一轮学习器的误差来更新下一轮学习的样本权重，MART每次都拟合上一轮分类器产生的残差。举个例子便于理解，比如一个人的年龄是50岁，第一棵树拟合的结果是35岁，第一轮的残差为15岁；然后第二棵数拟合的结果是10岁，两棵树相加总的拟合结果是45岁，第二轮的残差为5岁；第三棵数拟合的结果为2岁，三棵树相加拟合的结果是47岁，第三轮的残差是3岁......只要如此不断地进行下去，拟合结果就可以达到50岁，拟合残差的过程就是训练数据的过程。&lt;/p&gt;
&lt;p&gt;对于一个给定的数据集$\{x_i,y_i\}, i=1,2,...,m$，其中特征向量$x_i \in \mathbb{R}^n$，标签$y_i \in \mathbb{R}$，可以用$x_{ij}, j=1,2,...,d来代表x_i的第j个特征值$。对于一个典型的回归决策树问题，需要遍历所有特征$j$的全部阈值$t$，找到最优的$j$和$t$使下面的等式最小化：&lt;/p&gt;
&lt;p&gt;$$S_j=\sum_{i \in L}(y_i-\mu_L)^2+\sum_{i \in R}(y_i-\mu_R)^2$$&lt;/p&gt;
&lt;p&gt;其中$x_{ij} \leq t$的所有样本落入左子树$L$中，其中$x_{ij} &amp;gt; t$的所有样本落入右子树$R$中，$\mu_L(\mu_R)$表示左子树（右子树）所有样例标签值的均值。如果这就是一棵最简单的拥有一个根节点、两个叶子节点的二叉回归树，那么只需要根据最优阈值切分为左右子树，并且分别计算左右子树的值$\gamma_l,l=1,2$即可。如果将划分子树的过程继续进行$L-1$次即可得到一棵包含$L$个叶子节点的回归树。&lt;/p&gt;
&lt;p&gt;上面公式使用最小二乘法计算拟合误差，所以通过上面方法得到的模型又称为最小二乘回归树。其实不管误差的计算方式如何，我们都可以拟合出相应的回归树，唯一的区别是梯度的计算不同而已。&lt;/p&gt;
&lt;p&gt;MART使用线性组合的方式将拟合的树结合起来，作为最后的输出：&lt;/p&gt;
&lt;p&gt;$$F_n(x)=\sum_{i=1}^{N}\alpha_if_i(x)$$&lt;/p&gt;
&lt;p&gt;$f_i(x)$是单棵回归树函数，$\alpha_i$是第$i$棵回归树的权重。&lt;/p&gt;
&lt;p&gt;在这里我们需要弄清楚为什么拟合残差就能不断减少拟合误差。假设拟合误差$C$是拟合函数$F_n$的函数$C(F_n)$。那么：&lt;/p&gt;
&lt;p&gt;$$\delta C \approx \frac{\partial{C(F_n)}}{\partial{F_n}}\delta F_n$$&lt;/p&gt;
&lt;p&gt;如果取$\delta F_n=-\eta \frac{\partial{C}}{\partial{F_n}}$，就可以得到$\delta C&amp;lt;0$。其中$\eta$是学习率，为正实数。所以只要函数$F_n$拟合误差函数的负梯度就可以不断降低拟合误差的值。&lt;/p&gt;
&lt;p&gt;设标签向量$y=[y_1,y_2,...,y_m]^T$，如果用最小二乘的方式表示拟合误差，则：$$C=\frac{1}{2}(F_n-y)^2$$&lt;/p&gt;
&lt;p&gt;那么$\delta F_n=-\eta \frac{\partial{C}}{\partial{F_n}}=-\eta (F_n-y)$。这其实就是上面提到的残差，所以拟合残差可以不断减少拟合误差。&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;5.2 逻辑回归+MART进行二分类&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;了解了MART之后，下面举一个MART实际应用的例子：使用MART和逻辑回归进行二分类。用于分类的样本$x_i \in \mathbb{R}^n$，标签$y_i \in \{\pm1\}$，拟合函数$F(x)$。为了简化表示，我们表示条件概率如下：&lt;/p&gt;
&lt;p&gt;$$P_+ \equiv P(y=1|x)$$&lt;/p&gt;
&lt;p&gt;$$P_- \equiv P(y=-1|x)$$&lt;/p&gt;
&lt;p&gt;用交叉熵表示损失函数：$$L(y,F)=-ylog(P_+)-(1-y)log(P_-)$$&lt;/p&gt;
&lt;p&gt;逻辑回归使用对数机率（属于正例概率/属于负例概率）进行建模，&lt;/p&gt;
&lt;p&gt;$$F_n(x)=\frac{1}{2}log(\frac{P_+}{P_-})$$&lt;/p&gt;
&lt;p&gt;$$P_+=\frac{1}{1+e^{-2\sigma F_n(x)}}$$&lt;/p&gt;
&lt;p&gt;$$P_-=1-P_+=\frac{1}{1+e^{2\sigma F_n(x)}}$$&lt;/p&gt;
&lt;p&gt;将$P_+$和$P_-$带入$L(y,F)$中，得到：&lt;/p&gt;
&lt;p&gt;$$L(y,F_n)=log(1+e^{-2y\sigma F_n})$$&lt;/p&gt;
&lt;p&gt;$R_{jm}$表示落入第$m$棵树的第$j$个叶子节点中的样例集合，可以通过下式对该叶子节点的值进行优化：&lt;/p&gt;
&lt;p&gt;$$\gamma_{jm}=arg\min_{\gamma}\sum_{x_i \in R_{jm}}\log\left(1+e^{-2\sigma y_i\left(F_{m-1}\,\,\left({x_i}\right)+\gamma\right)\,}\right)$$&lt;/p&gt;
&lt;p&gt;上式可以使用Newton-Raphson方法按照下面的公式进行迭代求解：&lt;/p&gt;
&lt;p&gt;$$\gamma_{n+1}=\gamma_{n}-\frac{g'(\gamma_n)}{g''(\gamma_n)}$$&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;5.3 LambdaMART基本定义&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;LambdaMART基于MART，优化$\lambda$梯度。根据上面的定义，对于任意$U_i$和$U_j$，有：&lt;/p&gt;
&lt;p&gt;$$\lambda_{ij}=\frac{\partial{C(s_i-s_j)}}{\partial{s_i}}=\frac{-\sigma |\Delta_{Z_{ij}}|}{1+e^{\sigma(s_i-s_j)}}$$&lt;/p&gt;
&lt;p&gt;$|\Delta_{Z_{ij}}|$表示交换$U_i$和$U_j$的位置产生的评价指标差值，$Z$可以是$NDCG$或者$ERR$等。对于特定$U_i$，累加其他所有排序项的影响，得到：&lt;/p&gt;
&lt;p&gt;$$\lambda_i=\sum_{j:\{i,j\} \in I}\lambda_{ij}-\sum_{j:\{j,i\} \in I}\lambda_{ij}$$&lt;/p&gt;
&lt;p&gt; 为了简化表示：&lt;/p&gt;
&lt;p&gt;$$\sum_{\{i,j\}\rightleftharpoons I}=\sum_{j:\{i,j\} \in I}\lambda_{ij}-\sum_{j:\{j,i\} \in I}\lambda_{ij}$$&lt;/p&gt;
&lt;p&gt;于是我们可以更新损失函数：&lt;/p&gt;
&lt;p&gt;$$\frac{\partial{C}}{\partial{s_i}} = \sum_{j:\{i,j\} \in I} \frac{-\sigma |\Delta_{Z_{ij}}|}{1+e^{\sigma(s_i-s_j)}} = \sum_{j:\{i,j\} \in I} -\sigma |\Delta_{Z_{ij}}| \rho_{ij}$$&lt;/p&gt;
&lt;p&gt;其中，我们定义：&lt;/p&gt;
&lt;p&gt;$$\rho_{ij}=\frac{1}{1+e^{\sigma(s_i-s_j)}}=\frac{-\lambda_{ij}}{\sigma |\Delta_{Z_{ij}}|}$$&lt;/p&gt;
&lt;p&gt;然后可以得到：&lt;/p&gt;
&lt;p&gt;$$\frac{\partial{^2C}}{\partial{s_i^2}}=\sum_{\{i,j\}\rightleftharpoons I}\sigma^2|\Delta_{Z_{ij}}|\rho{ij}(1-\rho_{ij})$$&lt;/p&gt;
&lt;p&gt;所以我们可以用下面的公式计算第$k$棵树的第$k$个叶子节点上的值：&lt;/p&gt;
&lt;p&gt;$$\gamma_{km}=\frac{\sum_{x_i \in R_{km}}\frac{\partial{C}}{\partial{s_i}}}{\sum_{x_i \in R_{km}}\frac{\partial{^2C}}{\partial{s_i^2}}}=\frac{-\sum_{x_i \in R_{km}}\sum_{\{i,j\}\rightleftharpoons I}|\Delta_{Z_{ij}}|\rho_{ij}}{\sum_{x_i \in R_{km}}\sum_{\{i,j\}\rightleftharpoons I}|\Delta_{Z_{ij}}|\rho_{ij}(1-\rho_{ij})}$$&lt;/p&gt;
&lt;p&gt;所以总结&lt;strong&gt;LambdaMART&lt;/strong&gt;算法如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/436630/201810/436630-20181014200717864-340780855.png&quot; alt=&quot;&quot; width=&quot;845&quot; height=&quot;335&quot;/&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;6. 参考文献&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;1. Christopher J.C. Burges. From RankNet to LambdaRank to LambdaMART: An Overview. Microsoft Research Technical Report MSR-TR-010-82.&lt;/p&gt;
&lt;p&gt;2. Chrisopher D.Manning, Prabhakar Raghavan, Hinrich Schutze著, 王斌译. Introduction to Information Retrieval, 8.4 有序检索结果的评价方法, 2017年10月北京第11次印刷.&lt;/p&gt;
&lt;p&gt;3. Olivier Chapelle, Ya Zhang, Pierre Grinspan. Expected Recipocal Rank for Graded Relevance. CIKM 2009.&lt;/p&gt;
</description>
<pubDate>Sun, 14 Oct 2018 14:01:00 +0000</pubDate>
<dc:creator>RL-Learning</dc:creator>
<og:description>RankNet、LambdaRank和LambdaMART是三个关系非常紧密的机器学习排序算法。简而言之，RankNet是最基础，基于神经网络的排序算法；而LambdaRank在RankNet的基础上</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/genyuan/p/9788294.html</dc:identifier>
</item>
<item>
<title>Effective Java 第三版——48. 谨慎使用流并行 - 林本托</title>
<link>http://www.cnblogs.com/IcanFixIt/p/9788231.html</link>
<guid isPermaLink="true" >http://www.cnblogs.com/IcanFixIt/p/9788231.html</guid>
<description>&lt;blockquote readability=&quot;8.2216981132075&quot;&gt;
&lt;p&gt;Tips&lt;br/&gt;《Effective Java, Third Edition》一书英文版已经出版，这本书的第二版想必很多人都读过，号称Java四大名著之一，不过第二版2009年出版，到现在已经将近8年的时间，但随着Java 6，7，8，甚至9的发布，Java语言发生了深刻的变化。&lt;br/&gt;在这里第一时间翻译成中文版。供大家学习分享之用。&lt;br/&gt;书中的源代码地址：&lt;a href=&quot;https://github.com/jbloch/effective-java-3e-source-code&quot; class=&quot;uri&quot;&gt;https://github.com/jbloch/effective-java-3e-source-code&lt;/a&gt;&lt;br/&gt;注意，书中的有些代码里方法是基于Java 9 API中的，所以JDK 最好下载 JDK 9以上的版本。但是Java 9 只是一个过渡版本，所以建议安装JDK 10。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upload_images/4366140-8966e457a14bc8b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&quot; alt=&quot;Effective Java, Third Edition&quot;/&gt;&lt;/p&gt;

&lt;p&gt;在主流语言中，Java一直处于提供简化并发编程任务的工具的最前沿。 当Java于1996年发布时，它内置了对线程的支持，包括同步和wait / notify机制。 Java 5引入了&lt;code&gt;java.util.concurrent&lt;/code&gt;类库，带有并发集合和执行器框架。 Java 7引入了fork-join包，这是一个用于并行分解的高性能框架。 Java 8引入了流，可以通过对&lt;code&gt;parallel&lt;/code&gt;方法的单个调用来并行化。 用Java编写并发程序变得越来越容易，但编写正确快速的并发程序还像以前一样困难。 安全和活跃度违规（liveness violation）是并发编程中的事实，并行流管道也不例外。&lt;/p&gt;
&lt;p&gt;考虑条目 45中的程序：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;// Stream-based program to generate the first 20 Mersenne primes

public static void main(String[] args) {

    primes().map(p -&amp;gt; TWO.pow(p.intValueExact()).subtract(ONE))

        .filter(mersenne -&amp;gt; mersenne.isProbablePrime(50))

        .limit(20)

        .forEach(System.out::println);

}

static Stream&amp;lt;BigInteger&amp;gt; primes() {

    return Stream.iterate(TWO, BigInteger::nextProbablePrime);

}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;在我的机器上，这个程序立即开始打印素数，运行到完成需要12.5秒。假设我天真地尝试通过向流管道中添加一个到&lt;code&gt;parallel()&lt;/code&gt;的调用来加快速度。你认为它的表现会怎样?它会快几个百分点吗?慢几个百分点?遗憾的是，它不会打印任何东西，但是CPU使用率会飙升到90%，并且会无限期地停留在那里(liveness failure:活性失败)。这个程序可能最终会终止，但我不愿意去等待；半小时后我强行阻止了它。&lt;/p&gt;
&lt;p&gt;这里发生了什么？简而言之，流类库不知道如何并行化此管道并且启发式失败（heuristics fail.）。即使在最好的情况下，&lt;strong&gt;如果源来自&lt;code&gt;Stream.iterate&lt;/code&gt;方法，或者使用中间操作&lt;code&gt;limit&lt;/code&gt;方法，并行化管道也不太可能提高其性能&lt;/strong&gt;。这个管道必须应对这两个问题。更糟糕的是，默认的并行策略处理不可预测性的&lt;code&gt;limit&lt;/code&gt;方法，假设在处理一些额外的元素和丢弃任何不必要的结果时没有害处。在这种情况下，找到每个梅森素数的时间大约是找到上一个素数的两倍。因此，计算单个额外元素的成本大致等于计算所有先前元素组合的成本，并且这种无害的管道使自动并行化算法瘫痪。这个故事的寓意很简单：不要无差别地并行化流管道（stream pipelines）。性能后果可能是灾难性的。&lt;/p&gt;
&lt;p&gt;通常，&lt;strong&gt;并行性带来的性能收益在ArrayList、HashMap、HashSet和ConcurrentHashMap实例、数组、int类型范围和long类型的范围的流上最好&lt;/strong&gt;。这些数据结构的共同之处在于，它们都可以精确而廉价地分割成任意大小的子程序，这使得在并行线程之间划分工作变得很容易。用于执行此任务的流泪库使用的抽象是&lt;code&gt;spliterator&lt;/code&gt;，它由&lt;code&gt;spliterator&lt;/code&gt;方法在Stream和Iterable上返回。&lt;/p&gt;
&lt;p&gt;所有这些数据结构的共同点的另一个重要因素是它们在顺序处理时提供了从良好到极好的引用位置（ locality of reference）：顺序元素引用在存储器中存储在一块。 这些引用所引用的对象在存储器中可能彼此不接近，这降低了引用局部性。 对于并行化批量操作而言，引用位置非常重要：没有它，线程大部分时间都处于空闲状态，等待数据从内存传输到处理器的缓存中。 具有最佳引用位置的数据结构是基本类型的数组，因为数据本身连续存储在存储器中。&lt;/p&gt;
&lt;p&gt;流管道终端操作的性质也会影响并行执行的有效性。 如果与管道的整体工作相比，在终端操作中完成了大量的工作，并且这种操作本质上是连续的，那么并行化管道的有效性将是有限的。 并行性的最佳终操作是缩减（reductions），即使用流的&lt;code&gt;reduce&lt;/code&gt;方法组合管道中出现的所有元素，或者预先打包的reduce(如min、max、count和sum)。短路操作&lt;code&gt;anyMatch&lt;/code&gt;、&lt;code&gt;allMatch&lt;/code&gt;和&lt;code&gt;noneMatch&lt;/code&gt;也可以支持并行性。由Stream的&lt;code&gt;collect&lt;/code&gt;方法执行的操作，称为可变缩减（mutable reductions），不适合并行性，因为组合集合的开销非常大。&lt;/p&gt;
&lt;p&gt;如果编写自己的Stream，Iterable或Collection实现，并且希望获得良好的并行性能，则必须重写&lt;code&gt;spliterator方&lt;/code&gt;法并广泛测试生成的流的并行性能。 编写高质量的spliterator很困难，超出了本书的范围。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;并行化一个流不仅会导致糟糕的性能，包括活性失败（liveness failures）;它会导致不正确的结果和不可预知的行为(安全故障)&lt;/strong&gt;。使用映射器（mappers），过滤器（filters）和其他程序员提供的不符合其规范的功能对象的管道并行化可能会导致安全故障。 Stream规范对这些功能对象提出了严格的要求。 例如，传递给Stream的&lt;code&gt;reduce&lt;/code&gt;方法操作的累加器（accumulator）和组合器（combiner）函数必须是关联的，非干扰的和无状态的。 如果违反了这些要求（其中一些在第46项中讨论过），但按顺序运行你的管道，则可能会产生正确的结果; 如果将它并行化，它可能会失败，也许是灾难性的。&lt;/p&gt;
&lt;p&gt;沿着这些思路，值得注意的是，即使并行的梅森素数程序已经运行完成，它也不会以正确的(升序的)顺序打印素数。为了保持顺序版本显示的顺序，必须将&lt;code&gt;forEach&lt;/code&gt;终端操作替换为&lt;code&gt;forEachOrdered&lt;/code&gt;操作，它保证以遇出现顺序（encounter order）遍历并行流。&lt;/p&gt;
&lt;p&gt;即使假设正在使用一个高效的可拆分的源流、一个可并行化的或廉价的终端操作以及非干扰的函数对象，也无法从并行化中获得良好的加速效果，除非管道做了足够的实际工作来抵消与并行性相关的成本。作为一个非常粗略的估计，流中的元素数量乘以每个元素执行的代码行数应该至少是100,000 [Lea14]。&lt;/p&gt;
&lt;p&gt;重要的是要记住并行化流是严格的性能优化。 与任何优化一样，必须在更改之前和之后测试性能，以确保它值得做（第67项）。 理想情况下，应该在实际的系统设置中执行测试。 通常，程序中的所有并行流管道都在公共fork-join池中运行。 单个行为不当的管道可能会损害系统中不相关部分的其他行为。&lt;/p&gt;
&lt;p&gt;如果在并行化流管道时，这种可能性对你不利，那是因为它们确实存在。一个认识的人，他维护一个数百万行代码库，大量使用流，他发现只有少数几个地方并行流是有效的。这并不意味着应该避免并行化流。&lt;strong&gt;在适当的情况下，只需向流管道添加一个&lt;code&gt;parallel&lt;/code&gt;方法调用，就可以实现处理器内核数量的近似线性加速&lt;/strong&gt;。某些领域，如机器学习和数据处理，特别适合这些加速。&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;作为并行性有效的流管道的简单示例，请考虑此函数来计算π(n)，素数小于或等于n：
// Prime-counting stream pipeline - benefits from parallelization
static long pi(long n) {
    return LongStream.rangeClosed(2, n)
        .mapToObj(BigInteger::valueOf)
        .filter(i -&amp;gt; i.isProbablePrime(50))
        .count();
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;在我的机器上，使用此功能计算π（10&lt;sup&gt;8&lt;/sup&gt;）需要31秒。 只需添加&lt;code&gt;parallel()&lt;/code&gt;方法调用即可将时间缩短为9.2秒：&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;// Prime-counting stream pipeline - parallel version
static long pi(long n) {
    return LongStream.rangeClosed(2, n)
        .parallel()
        .mapToObj(BigInteger::valueOf)
        .filter(i -&amp;gt; i.isProbablePrime(50))
        .count();
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;换句话说，在我的四核计算机上，并行计算速度提高了3.7倍。值得注意的是，这不是你在实践中如何计算π(n)为n的值。还有更有效的算法，特别是Lehmer’s formula。&lt;/p&gt;
&lt;p&gt;如果要并行化随机数流，请从&lt;code&gt;SplittableRandom&lt;/code&gt;实例开始，而不是&lt;code&gt;ThreadLocalRandom&lt;/code&gt;（或基本上过时的&lt;code&gt;Random&lt;/code&gt;）。 &lt;code&gt;SplittableRandom&lt;/code&gt;专为此用途而设计，具有线性加速的潜力。&lt;code&gt;ThreadLocalRandom&lt;/code&gt;设计用于单个线程，并将自身适应作为并行流源，但不会像&lt;code&gt;SplittableRandom&lt;/code&gt;一样快。Random实例在每个操作上进行同步，因此会导致过度的并行杀死争用（ parallelism-killing contention）。&lt;/p&gt;
&lt;p&gt;总之，甚至不要尝试并行化流管道，除非你有充分的理由相信它将保持计算的正确性并提高其速度。不恰当地并行化流的代价可能是程序失败或性能灾难。如果您认为并行性是合理的，那么请确保您的代码在并行运行时保持正确，并在实际情况下进行仔细的性能度量。如果您的代码是正确的，并且这些实验证实了您对性能提高的怀疑，那么并且只有这样才能在生产代码中并行化流。&lt;/p&gt;
</description>
<pubDate>Sun, 14 Oct 2018 13:51:00 +0000</pubDate>
<dc:creator>林本托</dc:creator>
<og:description>Tips 《Effective Java, Third Edition》一书英文版已经出版，这本书的第二版想必很多人都读过，号称Java四大名著之一，不过第二版2009年出版，到现在已经将近8年的时间</og:description>
<dc:language>zh-cn</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.cnblogs.com/IcanFixIt/p/9788231.html</dc:identifier>
</item>
</channel>
</rss>