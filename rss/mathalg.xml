<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fmathalg-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/mathalg-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>算法与数学之美</title>
<link>http://www.jintiankansha.me/column/c9dZ5TM2aS</link>
<description>算法与数学之美 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>入门 | 一文看懂卷积神经网络</title>
<link>http://www.jintiankansha.me/t/sfqMtrY0RE</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/sfqMtrY0RE</guid>
<description>&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;&lt;span&gt;本文选自Medium,主要介绍了神经网络中的卷积神经网络，适合初学者阅读。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;概述&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;深度学习和人工智能是 2016 年的热词；2017 年，这两个词愈发火热，但也更加容易混淆。我们将深入深度学习的核心，也就是神经网络。大多数神经网络的变体是难以理解的，并且它们的底层结构组件使得它们在理论上和图形上是一样的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;下图展示了最流行的神经网络变体，可参考这篇博客 (http://www.asimovinstitute.org/neural-network-zoo/)。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.5&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEHnWIVJzibiaezDsicIIribM6qT9iaPXTp0Z6QlUD5RibEIA28MQV8CQglbiagg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;800&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;本文介绍卷积神经网络（CNN）。在开始之前，我们首先了解一下感知机。神经网络是一些被称作感知机的单元的集合，感知机是二元线性分类器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.4117647058823529&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEHia0CHP4BD5RllHBFbrG3Qnh9VkehjpRzSyG9jflospdvRIfaaelPhQg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;391&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如上图所示，输入 x1 和 x2 分别和各自的权重 w1 和 w2 相乘、求和，所以函数 f=x1*w1+x2*w2+b（偏置项，可以选择性地添加）。函数 f 可以是任意的运算，但是对于感知机而言通常是求和。函数 f 随后会通过一个激活函数来评估，该激活函数能够实现期望分类。Sigmoid 函数是用于二元分类的最常见的激活函数。如果您想进一步了解感知机，推荐阅读这篇文章（https://appliedgo.net/perceptron/）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如果我们把多个输入堆叠在一起，并且使用函数 f 将其与位于另一层的多个堆叠在一起的单元连接在一起，这就形成了多个全连接的感知机，这些单元（隐藏层）的输出成为最后一个单元的输入，再通过函数 f 和激活函数得到最终的分类。如下图所示，这个就是最简单的神经网络。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.684931506849315&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEHHjicJQXfpP96ccUX4HYxXuENlF4FGYSfuDeeB2BkOgEEp3UdhCS413w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;511&quot; width=&quot;353px&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;神经网络有一个独特的能力，被称作「泛逼近函数」（Universal Approximation function），所以神经网络的拓扑和结构变体是很多样化的。这本身就是一个很大的话题，Michael Nielsen 在文章中做了详细的描述（http://neuralnetworksanddeeplearning.com/chap4.html）。读完这个我们可以相信：神经网络可以模拟任何函数，不管它是多么的复杂。上面提到的神经网络也被称为前馈神经网络（FFNN），因为信息流是单向、无环的。现在我们已经理解了感知机和前馈神经网络的基本知识，我们可以想象，数百个输入连接到数个这样的隐藏层会形成一个复杂的神经网络，通常被称为深度神经网络或者深度前馈神经网络。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5205047318611987&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEH4PUP9iaZfwmFaVL3hl0S7ibHpkQbUTDZRkicCibkhzb0ICycEfdG5AnuEA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;634&quot; width=&quot;486px&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;那么深度神经网络和卷积神经网络有什么不同呢？让我们来探讨一下。&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;CNN 由于被应用在 ImageNet 等竞赛中而广受欢迎，最近也被应用在自然语言处理和语音识别中。需要记住的关键点是，其他的变体，如 RNN、LSTM、GRU 等，基于和 CNN 类似的结构，不过架构存在一些差异。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.615&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEHd1fpNjfJHKgeOB6FKx4Eibr0Bl0FVVfkqKa2KSY4wEPnvWHibbCQ06eQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;800&quot; width=&quot;393px&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;CNN 由三种不同的层组成，即「卷积层」、「池化层」、「密集层或全连接层」。我们之前的神经网络都是典型的全连接层神经网络。如果想了解更多卷积和池化层的知识，可以阅读 Andrej Karpathy 的解释（https://cs231n.github.io/convolutional-networks/）。现在继续我们关于层的讨论，下面我们来看一下卷积层。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;（在下面的内容里，我们会以图像分类为例来理解卷积神经网络，后面再转移到自然语言处理和视频任务中。）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;卷积层：假设一张图像有 5*5 个像素，1 代表白，0 代表黑，这幅图像被视为 5*5 的单色图像。现在用一个由随机地 0 和 1 组成的 3*3 矩阵去和图像中的子区域做乘法，每次迭代移动一个像素，这样该乘法会得到一个新的 3*3 的矩阵。下面的动图展示了这个过程。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;__bg_gif&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7300380228136882&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/KmXPKA19gWicj8FxAzKushnDznWX0ppEH5fM3ImjW6PsNFGXGNdT6CciaI4tpCcIZ9I5yUfiaibhQGqqhmpuIo3V5w/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;526&quot; width=&quot;349px&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;上述的 3*3 的矩阵被称作「滤波器」，它的任务是提取图像特征，它使用「优化算法」来决定 3*3 矩阵中具体的 0 和 1。我们在神经网络的卷积层中使用好几个这样的滤波器来提取多个特征。3*3 矩阵的每一个单个步骤被称作「步幅」（stride）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;下图展示了使用两个三通道滤波器从三通道（RGB）图像中生成两个卷积输出的详细过程。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.8352201257861636&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEHQwic8z4WtRgmNiaZajFyOPncugSkyX2JWfa69ibI30EX9C24u3krNpiaaA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;795&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;滤波器 w0 和 w1 是「卷积」，输出是提取到的特征，包含这些滤波器的层叫做卷积层。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;池化层：这个层主要使用不同的函数为输入降维。通常，最大池化层（max-pooling layer）出现在卷积层之后。池化层使用 2*2 的矩阵，以卷积层相同的方式处理图像，不过它是给图像本身降维。下面分别是使用「最大池化」和「平均池化」的示例。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.9039855072463768&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEHeYu181H4eXCJRav1iaqXIk6CZRphV5VPTnNYkvkyzRmvcygGoicKE4ew/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;552&quot; width=&quot;369px&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;全连接层：这个层是位于之前一层和激活函数之间的全连接层。它和之前讨论过的简单「神经网络」是类似的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;注意：卷积神经网络结果也会使用正则化层，不过本文将分开讨论。此外，池化层会损失信息，所以也不是首选的。通常的做法是在卷机层中使用一个较大的步幅。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;ILSVRC 2014 的亚军 VGGNet 是一个流行的卷积神经网络，它使用 16 个层来帮助我们理解 CNN 中深度的重要性，AlexNet 是 ILSVRC 2012 的冠军，它只有 8 层。Keras 中有可以直接使用的模型 VGG-16。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5626959247648903&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEHSHoXeZiakwQOnLVP92zvjibjDTIqcDrl4z7yFzJiaeVpAz9ia7ibgibtApXw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;638&quot; width=&quot;433px&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在 Keras 中加载了这个模型之后，我们可以观察每一层的「output shape」来理解张量维度，观察「Param#」来了解如何计算参数来得到卷积特征。「Param#」是每一次获取卷积特征时的所有权重更新。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.53375&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEHEJUc3rOEqnI9icFReZVG7biasSicHWY7vpNgSanY6sXahDBRcsciaWJwgA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;现在我们已经熟悉了卷积神经网络的结构，理解了每一层都是如何运行的，那么我们可以进一步去理解它是如何用在自然语言处理和视频处理中的了。您可以在这个链接中了解自 2012 年以来所有获得 ImageNet 竞赛冠军的 CNN 模型（https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html）。&lt;/span&gt;&lt;/p&gt;
&lt;section data-style=&quot;white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;∑编辑&lt;/span&gt;&lt;span&gt; | &lt;/span&gt;&lt;span&gt;Gemini&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;来源 | 机器之心&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;img class=&quot;__bg_gif&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.9366666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/951TjTgiabkwJ4BpvBcQhGAbtWZZvV69s7GickZGibsKgYkTQkiaZfLYOmGS9iaaoibadibGJhT18OVZkfeJmCSUSD0zw/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;600&quot; width=&quot;auto&quot;/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;/section&gt;&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域&lt;br/&gt;稿件一经采用，我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 22 Feb 2018 20:12:47 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/sfqMtrY0RE</dc:identifier>
</item>
<item>
<title>寒假无聊？玩点带颜色的刺激刺激吧！</title>
<link>http://www.jintiankansha.me/t/wKHmfg5Wtd</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/wKHmfg5Wtd</guid>
<description>&lt;p&gt;诶诶诶，别打别打，有话好好说——&lt;/p&gt;&lt;p&gt;&lt;span&gt;我不只是来逗大家开心的&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我是想告诉大家&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;绘画其实是一门历久弥新的艺术&lt;/p&gt;

&lt;p&gt;它古老&lt;/p&gt;
&lt;p&gt;因为每一个人&lt;span&gt;在学会写字前便会涂涂画画&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;将脑中所见所想以画作的形式记录和表达出来&lt;/p&gt;
&lt;p&gt;某种意义上来说&lt;/p&gt;
&lt;p&gt;画画其实是人类的一种本能&lt;/p&gt;

&lt;p&gt;它年轻&lt;/p&gt;
&lt;p&gt;因为在科技发达的现代&lt;span&gt;也依旧熠熠生辉&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;游戏、动漫、设计……&lt;/p&gt;
&lt;p&gt;即便在人工AI取代大多数工作的未来&lt;/p&gt;
&lt;p&gt;绘画作为人文艺术的精粹也无可替代&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6272727272727273&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkzZ5uce4fqax5Zb8OofnQs4UWPZCXN37ZSthUCTMicKdf0ULfAyu20EIoEoAiaKGYWibh7dLY4xibuWBg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;220&quot;/&gt;&lt;/p&gt;
&lt;p&gt;对于大学生而言&lt;/p&gt;
&lt;p&gt;学习绘画其实有太多的好处&lt;/p&gt;

&lt;p&gt;提升审美&lt;/p&gt;
&lt;p&gt;不俗的品味对于人生的优化&lt;/p&gt;
&lt;p&gt;真的非常惊人&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;丰富个人生活&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;比起游戏&lt;/p&gt;
&lt;p&gt;绘画会让你内心更加充实&lt;/p&gt;

&lt;p&gt;而这些从来都&lt;strong&gt;不是艺术生的专属&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;学习绘画&lt;/p&gt;
&lt;p&gt;并不是因为它有“钱途”&lt;/p&gt;
&lt;p&gt;也并非一定要成为名家大咖&lt;/p&gt;

&lt;p&gt;培养对生活热忱&lt;/p&gt;
&lt;p&gt;和高雅的情趣&lt;/p&gt;
&lt;p&gt;这样即使在再寒冷的冬天&lt;/p&gt;
&lt;p&gt;也不会忘记梅花的芬芳&lt;/p&gt;

&lt;p&gt;而这也是我送给大家真正的福利——&lt;/p&gt;
</description>
<pubDate>Thu, 22 Feb 2018 20:12:46 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/wKHmfg5Wtd</dc:identifier>
</item>
</channel>
</rss>