<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fmathalg-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/mathalg-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>算法与数学之美</title>
<link>http://www.jintiankansha.me/column/c9dZ5TM2aS</link>
<description>算法与数学之美 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>入门 | 一文看懂卷积神经网络</title>
<link>http://www.jintiankansha.me/t/sfqMtrY0RE</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/sfqMtrY0RE</guid>
<description>&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;&lt;span&gt;本文选自Medium,主要介绍了神经网络中的卷积神经网络，适合初学者阅读。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;概述&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;深度学习和人工智能是 2016 年的热词；2017 年，这两个词愈发火热，但也更加容易混淆。我们将深入深度学习的核心，也就是神经网络。大多数神经网络的变体是难以理解的，并且它们的底层结构组件使得它们在理论上和图形上是一样的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;下图展示了最流行的神经网络变体，可参考这篇博客 (http://www.asimovinstitute.org/neural-network-zoo/)。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.5&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEHnWIVJzibiaezDsicIIribM6qT9iaPXTp0Z6QlUD5RibEIA28MQV8CQglbiagg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;800&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;本文介绍卷积神经网络（CNN）。在开始之前，我们首先了解一下感知机。神经网络是一些被称作感知机的单元的集合，感知机是二元线性分类器。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.4117647058823529&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEHia0CHP4BD5RllHBFbrG3Qnh9VkehjpRzSyG9jflospdvRIfaaelPhQg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;391&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如上图所示，输入 x1 和 x2 分别和各自的权重 w1 和 w2 相乘、求和，所以函数 f=x1*w1+x2*w2+b（偏置项，可以选择性地添加）。函数 f 可以是任意的运算，但是对于感知机而言通常是求和。函数 f 随后会通过一个激活函数来评估，该激活函数能够实现期望分类。Sigmoid 函数是用于二元分类的最常见的激活函数。如果您想进一步了解感知机，推荐阅读这篇文章（https://appliedgo.net/perceptron/）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如果我们把多个输入堆叠在一起，并且使用函数 f 将其与位于另一层的多个堆叠在一起的单元连接在一起，这就形成了多个全连接的感知机，这些单元（隐藏层）的输出成为最后一个单元的输入，再通过函数 f 和激活函数得到最终的分类。如下图所示，这个就是最简单的神经网络。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.684931506849315&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEHHjicJQXfpP96ccUX4HYxXuENlF4FGYSfuDeeB2BkOgEEp3UdhCS413w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;511&quot; width=&quot;353px&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;神经网络有一个独特的能力，被称作「泛逼近函数」（Universal Approximation function），所以神经网络的拓扑和结构变体是很多样化的。这本身就是一个很大的话题，Michael Nielsen 在文章中做了详细的描述（http://neuralnetworksanddeeplearning.com/chap4.html）。读完这个我们可以相信：神经网络可以模拟任何函数，不管它是多么的复杂。上面提到的神经网络也被称为前馈神经网络（FFNN），因为信息流是单向、无环的。现在我们已经理解了感知机和前馈神经网络的基本知识，我们可以想象，数百个输入连接到数个这样的隐藏层会形成一个复杂的神经网络，通常被称为深度神经网络或者深度前馈神经网络。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5205047318611987&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEH4PUP9iaZfwmFaVL3hl0S7ibHpkQbUTDZRkicCibkhzb0ICycEfdG5AnuEA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;634&quot; width=&quot;486px&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;那么深度神经网络和卷积神经网络有什么不同呢？让我们来探讨一下。&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;CNN 由于被应用在 ImageNet 等竞赛中而广受欢迎，最近也被应用在自然语言处理和语音识别中。需要记住的关键点是，其他的变体，如 RNN、LSTM、GRU 等，基于和 CNN 类似的结构，不过架构存在一些差异。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.615&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEHd1fpNjfJHKgeOB6FKx4Eibr0Bl0FVVfkqKa2KSY4wEPnvWHibbCQ06eQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;800&quot; width=&quot;393px&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;CNN 由三种不同的层组成，即「卷积层」、「池化层」、「密集层或全连接层」。我们之前的神经网络都是典型的全连接层神经网络。如果想了解更多卷积和池化层的知识，可以阅读 Andrej Karpathy 的解释（https://cs231n.github.io/convolutional-networks/）。现在继续我们关于层的讨论，下面我们来看一下卷积层。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;（在下面的内容里，我们会以图像分类为例来理解卷积神经网络，后面再转移到自然语言处理和视频任务中。）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;卷积层：假设一张图像有 5*5 个像素，1 代表白，0 代表黑，这幅图像被视为 5*5 的单色图像。现在用一个由随机地 0 和 1 组成的 3*3 矩阵去和图像中的子区域做乘法，每次迭代移动一个像素，这样该乘法会得到一个新的 3*3 的矩阵。下面的动图展示了这个过程。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;__bg_gif&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7300380228136882&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/KmXPKA19gWicj8FxAzKushnDznWX0ppEH5fM3ImjW6PsNFGXGNdT6CciaI4tpCcIZ9I5yUfiaibhQGqqhmpuIo3V5w/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;526&quot; width=&quot;349px&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;上述的 3*3 的矩阵被称作「滤波器」，它的任务是提取图像特征，它使用「优化算法」来决定 3*3 矩阵中具体的 0 和 1。我们在神经网络的卷积层中使用好几个这样的滤波器来提取多个特征。3*3 矩阵的每一个单个步骤被称作「步幅」（stride）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;下图展示了使用两个三通道滤波器从三通道（RGB）图像中生成两个卷积输出的详细过程。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.8352201257861636&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEHQwic8z4WtRgmNiaZajFyOPncugSkyX2JWfa69ibI30EX9C24u3krNpiaaA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;795&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;滤波器 w0 和 w1 是「卷积」，输出是提取到的特征，包含这些滤波器的层叫做卷积层。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;池化层：这个层主要使用不同的函数为输入降维。通常，最大池化层（max-pooling layer）出现在卷积层之后。池化层使用 2*2 的矩阵，以卷积层相同的方式处理图像，不过它是给图像本身降维。下面分别是使用「最大池化」和「平均池化」的示例。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.9039855072463768&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEHeYu181H4eXCJRav1iaqXIk6CZRphV5VPTnNYkvkyzRmvcygGoicKE4ew/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;552&quot; width=&quot;369px&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;全连接层：这个层是位于之前一层和激活函数之间的全连接层。它和之前讨论过的简单「神经网络」是类似的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;注意：卷积神经网络结果也会使用正则化层，不过本文将分开讨论。此外，池化层会损失信息，所以也不是首选的。通常的做法是在卷机层中使用一个较大的步幅。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;ILSVRC 2014 的亚军 VGGNet 是一个流行的卷积神经网络，它使用 16 个层来帮助我们理解 CNN 中深度的重要性，AlexNet 是 ILSVRC 2012 的冠军，它只有 8 层。Keras 中有可以直接使用的模型 VGG-16。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5626959247648903&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEHSHoXeZiakwQOnLVP92zvjibjDTIqcDrl4z7yFzJiaeVpAz9ia7ibgibtApXw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;638&quot; width=&quot;433px&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在 Keras 中加载了这个模型之后，我们可以观察每一层的「output shape」来理解张量维度，观察「Param#」来了解如何计算参数来得到卷积特征。「Param#」是每一次获取卷积特征时的所有权重更新。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.53375&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicj8FxAzKushnDznWX0ppEHEJUc3rOEqnI9icFReZVG7biasSicHWY7vpNgSanY6sXahDBRcsciaWJwgA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;800&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;现在我们已经熟悉了卷积神经网络的结构，理解了每一层都是如何运行的，那么我们可以进一步去理解它是如何用在自然语言处理和视频处理中的了。您可以在这个链接中了解自 2012 年以来所有获得 ImageNet 竞赛冠军的 CNN 模型（https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html）。&lt;/span&gt;&lt;/p&gt;
&lt;section data-style=&quot;white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;∑编辑&lt;/span&gt;&lt;span&gt; | &lt;/span&gt;&lt;span&gt;Gemini&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;来源 | 机器之心&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;img class=&quot;__bg_gif&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.9366666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/951TjTgiabkwJ4BpvBcQhGAbtWZZvV69s7GickZGibsKgYkTQkiaZfLYOmGS9iaaoibadibGJhT18OVZkfeJmCSUSD0zw/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;600&quot; width=&quot;auto&quot;/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;/section&gt;&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域&lt;br/&gt;稿件一经采用，我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 22 Feb 2018 20:12:47 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/sfqMtrY0RE</dc:identifier>
</item>
<item>
<title>现代中国第一位数学博士是谁？</title>
<link>http://www.jintiankansha.me/t/R2R8WBT50A</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/R2R8WBT50A</guid>
<description>&lt;section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;1.5&quot;&gt;&lt;section&gt;&lt;span mpa-none-contnet=&quot;t&quot;&gt;“&lt;/span&gt;
&lt;section/&gt;&lt;/section&gt;&lt;section readability=&quot;3&quot;&gt;&lt;p&gt;&lt;span&gt;胡明复（1891~1927）初名胡孔孙，后改名为胡达，江苏无锡人，现代中国第一个数学博士，中国科学社创始人&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;/section&gt;&lt;section&gt;&lt;section/&gt;&lt;span mpa-none-contnet=&quot;t&quot;&gt;”&lt;/span&gt;&lt;/section&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.3652482269503545&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkwSZ2GQM1psEfuibiaRWCkbNBM7o8YvictCX3nXkVO90r4XlYnicuI6CF8XQMFBBrYqGxUIlGHoppvbEw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;282&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;19岁的胡适到北京参加留美公费考试。发榜那夜，他借着微弱的马灯，在史家胡同的录取榜上忐忑不安地瞅，终于看到了自己的名字，可仔细一看，却是&lt;span lang=&quot;EN-US&quot;&gt;“&lt;/span&gt;胡达&lt;span lang=&quot;EN-US&quot;&gt;”&lt;/span&gt;，再往上看，相隔很近，便是&lt;span lang=&quot;EN-US&quot;&gt;“&lt;/span&gt;胡适&lt;span lang=&quot;EN-US&quot;&gt;”&lt;/span&gt;，他终于长出一口气，心中暗想：&lt;span lang=&quot;EN-US&quot;&gt;“&lt;/span&gt;那个胡达不知是谁，几乎害我空欢喜一场。&lt;span lang=&quot;EN-US&quot;&gt;”&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;发生于&lt;span lang=&quot;EN-US&quot;&gt;1910&lt;/span&gt;年&lt;span lang=&quot;EN-US&quot;&gt;7&lt;/span&gt;月&lt;span lang=&quot;EN-US&quot;&gt;11&lt;/span&gt;日的这张榜单上的巧合，仿佛是中国近现代知识分子追求&lt;span lang=&quot;EN-US&quot;&gt;“&lt;/span&gt;德先生&lt;span lang=&quot;EN-US&quot;&gt;”&lt;/span&gt;和&lt;span lang=&quot;EN-US&quot;&gt;“&lt;/span&gt;塞先生&lt;span lang=&quot;EN-US&quot;&gt;”&lt;/span&gt;的坎坷道路的一个缩影。此后数年，胡适因提倡文学革命，成为新文化运动的领袖，而胡达，则是中国第一个综合性科学团体&lt;span lang=&quot;EN-US&quot;&gt;“&lt;/span&gt;中国科学社&lt;span lang=&quot;EN-US&quot;&gt;”&lt;/span&gt;最早的&lt;span lang=&quot;EN-US&quot;&gt;9&lt;/span&gt;名发起人之一，更多时候，人们叫他胡明复。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;1914年&lt;span lang=&quot;EN-US&quot;&gt;6&lt;/span&gt;月&lt;span lang=&quot;EN-US&quot;&gt;10&lt;/span&gt;日的黄昏，美国康奈尔大学的图书馆走廊上，胡明复、赵元任、任鸿隽、秉志、杨杏佛等几个中国留学生聚在一起，讨论起中国的未来。有人提出：中国缺乏的莫过于科学，我们为什么不出版一种专门向中国介绍科学的杂志呢？随后，这个提议获得了一致赞同。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在这本名为《科学》的杂志和以其为核心组建的中国科学社里，胡明复一直是核心领导人之一。在中国科学社创立之初，许多章程都由他独立设计。&lt;span lang=&quot;EN-US&quot;&gt;1916&lt;/span&gt;年，中国科学会在美国开第一次年会，社长任鸿隽查询了英国皇家学会的资料，才发现这个老牌科学组织的运行，与胡明复的设计，&lt;span lang=&quot;EN-US&quot;&gt;“&lt;/span&gt;竟有许多不约而同的地方&lt;span lang=&quot;EN-US&quot;&gt;”&lt;/span&gt;。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这是胡明复第一次显露出在数学之外的才能。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在此之前的&lt;span lang=&quot;EN-US&quot;&gt;1900&lt;/span&gt;年，德国数学家希尔伯特在巴黎数学家大会上提出&lt;span lang=&quot;EN-US&quot;&gt;23&lt;/span&gt;个著名的数学问题，当时的中国，没有一个数学家能听懂这些问题的意思。&lt;/span&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;1917年，胡明复在哈佛大学获得博士学位，被近代数学史家们视为&lt;span lang=&quot;EN-US&quot;&gt;“&lt;/span&gt;中国现代数学真正开始的标志&lt;span lang=&quot;EN-US&quot;&gt;”&lt;/span&gt;，当时，他的博士论文《具有边界条件的线性微分&lt;span lang=&quot;EN-US&quot;&gt;—&lt;/span&gt;积分方程》，被发表在美国的一流数学杂志上，引起了国际数学界的广泛注目。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;不过，在后来的中国科学社里，他的数学才能被更多地发挥在会计上。虽然经费窘迫，但在胡明复的理财本领下，始终都能正常运转。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这个数学博士也有算错的时候，曾有人留心过科学社的账目，发现有时候数目对不上，当然，吃亏的从来不会是科学社，而都是胡会计自己。根据胡明复的统计，在&lt;span lang=&quot;EN-US&quot;&gt;10&lt;/span&gt;多年中，他为科学社捐付的款项，达到两万多银元，在当时，这笔钱可以在北京买到&lt;span lang=&quot;EN-US&quot;&gt;50&lt;/span&gt;个四合院。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;结束美国留学生涯后，中国科学社核心成员陆续回国。在成员分散于全国各地的情况下，胡明复在上海坚守阵地，主持中国科学社的社务长达&lt;span lang=&quot;EN-US&quot;&gt;10&lt;/span&gt;年。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为了科学社的正常运转，胡明复常在几个大学里兼课赚钱。在人们记忆中，寒冷的早晨，胡明复总穿着一身洗得发白的蓝色长衫，搭乘电车去上海的徐家汇赶第一堂课，因衣服单薄，常常冻得清鼻涕直流。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;他讲普通话略带无锡口音，声音不大，却有着一种&lt;span lang=&quot;EN-US&quot;&gt;“&lt;/span&gt;简洁的力量&lt;span lang=&quot;EN-US&quot;&gt;”&lt;/span&gt;。很少有人见过胡明复高兴的样子，不过，&lt;span lang=&quot;EN-US&quot;&gt;1919&lt;/span&gt;年&lt;span lang=&quot;EN-US&quot;&gt;,&lt;/span&gt;中国科学社在西湖边上组织了第一次国内年会，人们却见到了一个在讲台上兴高采烈的胡明复。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在演讲中，他说：&lt;span lang=&quot;EN-US&quot;&gt;“……&lt;/span&gt;研究科学的人最爱自然，故在美丽的杭州西湖举行科学年会极为相宜。古代诗人来游西湖，歌咏名篇甚多，科学家虽不同于诗人&lt;span lang=&quot;EN-US&quot;&gt;……&lt;/span&gt;未始不可为西湖增色也。&lt;span lang=&quot;EN-US&quot;&gt;”&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;但那一刻，当胡明复沉醉于西湖的湖光山色之中，憧憬着他所献身的科学事业时，绝不会想到，他的身后将会永与西湖为伴。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;1929年，年仅&lt;span lang=&quot;EN-US&quot;&gt;36&lt;/span&gt;岁的胡明复在家乡游泳意外溺水而亡。他的科学社同人们，把他安葬在了西湖边一处名为烟霞洞的小山坡上。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;生前，胡明复最喜欢说这样一句话：&lt;span lang=&quot;EN-US&quot;&gt;“&lt;/span&gt;一个人倘若在一日之中能做&lt;span lang=&quot;EN-US&quot;&gt;3&lt;/span&gt;日的工作，他&lt;span lang=&quot;EN-US&quot;&gt;30&lt;/span&gt;岁死，也同活到&lt;span lang=&quot;EN-US&quot;&gt;90&lt;/span&gt;岁无异。&lt;span lang=&quot;EN-US&quot;&gt;”&lt;/span&gt;于是，有人在悼词中这样怀念道：&lt;span lang=&quot;EN-US&quot;&gt;“&lt;/span&gt;他恐怕没有&lt;span lang=&quot;EN-US&quot;&gt;1&lt;/span&gt;天不做别人&lt;span lang=&quot;EN-US&quot;&gt;5&lt;/span&gt;天的工作，这么说，他便是我们当中最为长寿的人。&lt;span lang=&quot;EN-US&quot;&gt;”&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;宜统二年（1910）七月，我到北京考留美官费。那一天，有人来说，发榜了。我坐了人力车去看榜，到史家胡同时，天已黑了。我拿了车上的灯，从榜尾倒看上去（因为我自信我考的很不好），看完了一张榜，没有我的名字，我很失望。看过头上，才知道那一张是&quot;备取&quot;的榜。我再拿灯照读那&quot;正取&quot;的榜，仍是倒读上去。看到我的名字了！仔细一看，却是&quot;胡达&quot;，不是&quot;胡适&quot;。我再看上去，相隔很近，便是我的姓名了。我抽了一口气，放下灯，仍坐原车回去了，心里却想着，&quot;那个胡达不知是推，几乎害我空高兴一场！&quot;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;那个胡达便是胡明复。后来我和他和宪生都到康南耳大学，中国同学见了我们的姓名，总以为胡达胡适是兄弟，却不知道宪生和他是堂兄弟，我和他却全无亲属的关系。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;hr/&gt;&lt;p&gt;&lt;span&gt;那年我们同时放洋的共有七十一人，此外还有胡敦复先生，唐孟伦先生，严约冲先生。船上十多天，大家都熟了。但是那时已可看出许多人的性情嗜好。我是一个爱玩的人，也吸纸烟，也爱喝柠檬水，也爱学打&quot;五百&quot;及&quot;高低，杰克&quot;等等纸牌。在吸烟室里，我认得了宪生，常同他打&quot;Shuffle　Board&quot;；我又常同严约冲张彭春王鸿卓打纸牌。明复从不同我们玩。他和赵元任周仁总是同胡敦复在一块谈天；我们偶然听见他们谈话，知道他们谈的是算学问题，我们或是听不懂，或是感觉没有趣味，只好走开，心里都恭敬这一小群的学者。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;到了绮色佳（Ithaca）之后，明复与元任所学相同，最亲热；我在农科，同他们见面时很少。到了一九一二年以后，我改入文科，方才和明复元任同在克雷登（Prot.J.E. Creighton）先生的哲学班上。我们三个人同坐一排，从此我们便很相熟了。明复与元任的成绩相差最近，竞争最烈。他们每学期的总平均总都在九十分以上；大概总是元任多着一分或半分，有一年他们差只有几厘。他们在康南耳四年，每年的总成绩都是全校最高的。一九一三年，我们三人同时被举为Phi Beta Kappa会员；因为我们同在克雷登先生班上，又同在一排，故同班的人都很欣羡；其实我的成绩远不女问他们两位。一九一四年，他们二人又同时被举为Sigma Xi会员，这是理科的名誉学会，得之很难；他们两人间时已得Phi BetaKappa的&quot;会钥&quot;，又得Sigma Xi &quot;会钥&quot;，更是全校稀有的荣誉。（郭复先生也是Phi　Beta kappa的会员。）&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;明复是科学社的发起人，这是大家知道的。这件事的记载，我在我的《藏晖室札记》里居然留和一点材料，现在摘已在此，也许可供将来科学社修史的人参考。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;科学社发起的人是赵元任胡达（明复）周仁秉志过探先杨铨任鸿隽金邦正章元善。他们有一天（1914）聚在世界会（Cosmopolitan　Club）的一个房间里，——似是过探先所住，——商量要办一个月报，名为&quot;科学&quot;。后来他们公推明复与杨铨任鸿隽等起草，拟定&quot;科学社&quot;的招股章程。最初的章程是杨铨手写付印的，其全文如下：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　&lt;strong&gt;　科学社招股章程&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　  （1）定名　本社定名科学社（Science　Society）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　 &lt;span&gt;（2）宗旨　本社发起&quot;科学&quot;（Science）月刊，以提倡科学，鼓吹实业，审定名词，传播知识，为宗旨。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　（3）资本　本社暂时以美金四百元为资本。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　（4）股份　本社发行股份票四十份，每份美金十元。其二十份由发起人担任，余二十份发售。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　（5）交股法　购一股者，限三期交清，以一月为一期：第一期五元，第二期三元，第三期二元。购二股者，限五期交清：第一期六元，第二三期各四元，第四五期各三元。每股东以三股为限，购三股者其二股依上述二股例交付，余一股照单购法办理。凡股东入股，转股，均须先经本社认可。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　 &lt;span&gt;（6）权利　 股东有享受赢余及选举被选举权。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　（7）总事务所在地　本社总事务所暂设美国以萨克（Ithaca）城。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　（8）期限　营业期限无定。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　（9）通讯处　美国过探先。（住址从略）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;当时的目的只想办一个&quot;科学&quot;月刊，资本只要美金四百元。后来才放手做去，变成今日的科学社，&quot;科学&quot;月刊的发行只成为社中的一件附属事业了。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;当时大家决定，先须收齐三个月的稿子，然后赶送出付印。明复在编辑上的功劳最大；他不但自己撰译了不少稿子，还担任整理别人的稿件，统一行款，改换标点，故他最辛苦。他在社中后来的贡献与劳绩，是许多朋友都知道的，不用我说了。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;hr/&gt;&lt;p&gt;&lt;span&gt;明复学的是数学物理，但他颇注意于他所专习的科学以外的事情。我住在世界会，常见明复到会里来看杂志；别的科学学生很少来的。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;有一件事可以作证。民国元年（1912）十一月里，明复和我发起一个政治研究会。那时在革命之后，大家都注意政治问题，故有这个会的组织。第一次组织会在我的房间里开会，会员共十人，议决：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　（1）每两星期开会一次。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　（2）每会讨论一个问题，由会员二人轮次预备论文宣读。论文完后，由会员讨论。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　（3）每会由会员一人轮当主席。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　（4）会期在星期六下午二时。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;第一次论会的论题为&quot;美国议会&quot;，由过探先与我担任。第二次论题为，&quot;租税制度&quot;，由胡明复与尤怀皋担任。我的日记有这一条：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;十二月念一日，中国学生政治研究会第二次会，论&quot;租税&quot;。胡明复尤怀皋二君任讲演，甚有兴味。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;二君所预备演稿俱极精详，费时当不少，其热心可佩也。&lt;/span&gt;&lt;/p&gt;

&lt;hr/&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;明复与元任后来都到哈佛去了。那时杏佛（杨铨）编辑&quot;科学&quot;，常向他们催稿子。民国五年（1916）六月间，杏佛作了一首白话打油诗寄给明复：——&lt;/span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　寄胡 明 复&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　自从老胡去，这城天气凉。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　新屋有风阁，清福过帝王。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　境闲心不闲，手忙脚更忙。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　为我告&quot;夫子&quot;，&quot;科学&quot;要文章。&lt;/span&gt;&lt;/p&gt;

&lt;hr/&gt;&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;元任见此诗，也和了一首：——&lt;/span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　&lt;span&gt;　寄杨 杏 佛&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　  &lt;span&gt;自从老胡来，此地暖如汤。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　&quot;科学&quot;稿已去，&quot;夫子&quot;不敢当。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　才完就要做，忙似阎罗王。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　幸有&quot;辟克匿，那时波士顿肯里白奇的社友还可大大的乐一场！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　这也可以表示当时的朋友之乐，与科学社编辑部工作的状况。&lt;/span&gt;&lt;/p&gt;

&lt;hr/&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;民国三年（1941) 明复得盲肠炎，幸早去割了，才得无事。民国五年（1916），元任也得盲肠炎，也得割治。那时我在纽约，作了一首打油诗寄给元任，并寄给明复看：——&lt;/span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　闻道先生病了，叫我吓了一跳。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　&quot;阿彭底赛梯斯！&quot;这事有点不妙！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　依我仔细看来，这病该怪胡达。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　作和他两口儿，可算得亲热杀：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　同学同住同事，今又同到哈，&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　同时&quot;西葛玛鳃&quot;，同时&quot;斐贝卡拔&quot;。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　&lt;span&gt;　  前年胡达破肚，今年&quot;先生&quot;该割。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　莫怪胡适无礼，嘴里夹七夹八。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　要&quot;先生&quot;开口笑，病中快活快活。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　更望病早早好，阿弥陀佛菩萨！&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;那时候我正开始作白话诗，常同一班朋友讨论文学问题。明复有一天忽然寄了两首打油诗来，不但是白话的，竟是土白的。第一首是：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　纽约城里，&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　&lt;span&gt;　  有个胡适，&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;　　　　白话连篇，&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　成啥样式！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第二首是一首&quot;宝塔诗&quot;：--&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　痴！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　适之！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　勿读书！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　&lt;span&gt;　　　香烟一支！&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　单做白话诗！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　说时快，做时迟。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　一做就是三小时！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我也答他一首&quot;宝塔诗&quot;：--&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　咦！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　&lt;span&gt;　　　希奇！&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　胡格哩，&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　我做诗！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　这话不须提。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　我做诗快得希，&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　从来不用三小时。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　提起笔何用费心思，&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　笔尖儿嗤嗤嗤嗤地飞，&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　也不管宝塔诗有几层儿！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　　　这种朋友游戏的乐处，可怜如今都成永不回来的陈迹了！&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;去年五月底，我从外国回来，住在沧州旅馆。有一天，吴稚晖先生在我的房里大谈。门外有客来了，我开门看时，原来是明复同周子竞（仁）两位。我告诉他们，里面是稚晖先生。他们怕打断吴先生的谈话，不肯进来，说&quot;过几天再来谈&quot;，都走了。我以为，大家同在上海，相见很容易的。谁知不多时明复遂死了，那一回竟是我同他的永诀了。他永永不再来谈了！&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;section data-style=&quot;white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);&quot;&gt;&lt;p&gt;&lt;span&gt;∑编辑&lt;span&gt; | &lt;/span&gt;Gemini&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;来源 | 数学e点通&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;section data-style=&quot;white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);&quot;&gt;&lt;p&gt;&lt;img class=&quot;__bg_gif&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.9366666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/951TjTgiabkwJ4BpvBcQhGAbtWZZvV69s7GickZGibsKgYkTQkiaZfLYOmGS9iaaoibadibGJhT18OVZkfeJmCSUSD0zw/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;600&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;
&lt;/section&gt;&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域&lt;br/&gt;稿件一经采用，我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Wed, 21 Feb 2018 19:34:23 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/R2R8WBT50A</dc:identifier>
</item>
</channel>
</rss>