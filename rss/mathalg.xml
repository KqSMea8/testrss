<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fmathalg-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/mathalg-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>算法与数学之美</title>
<link>http://www.jintiankansha.me/column/c9dZ5TM2aS</link>
<description>算法与数学之美 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>数学家张益唐妹妹首次感人披露其家庭：清歌如烟，我的哥哥我的家</title>
<link>http://www.jintiankansha.me/t/AcjxKluqPO</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/AcjxKluqPO</guid>
<description>&lt;section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;1.5&quot;&gt;
&lt;section&gt;&lt;span mpa-none-contnet=&quot;t&quot;&gt;“&lt;/span&gt;
&lt;section/&gt;&lt;/section&gt;&lt;section readability=&quot;3&quot;&gt;&lt;p mpa-is-content=&quot;t&quot;&gt;&lt;span&gt;2013年，张益唐在沉默20多年后横空出世，在孪生素数猜想上取得突破。传奇还在江湖流传，他的妹妹首次披露他的哥哥和他的家庭：&lt;/span&gt;&lt;/p&gt;
&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;1.5&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;3&quot;&gt;
&lt;p&gt;&lt;span&gt;一位历经坎坷的数学家，背后一个普通的家，一双把对儿子的爱揉进生命中的父母。他们，共同谱出父爱如山，母爱似海的清歌一曲。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;故事非常感人。&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section/&gt;&lt;span mpa-none-contnet=&quot;t&quot;&gt;”&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.734375&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/HJqsswHdM2lkqscujicQsCc62LRpzaTI7ekxIRV5krcUHw1CyTuoXWZicYA2hJbsuRXJqLUwkoeCHLu3cDBvryhw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1280&quot; width=&quot;auto&quot;/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;数学家张益唐&lt;/span&gt;&lt;/p&gt;

&lt;section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;给数学家当妹妹&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;张盈唐&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我有个当数学家的老哥，叫张益唐。他的名字在全球的数学界算得如雷贯耳，因为他对孪生素数猜想的研究取得了历史性的突破。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;社会上充满了关于我老哥的传说，一个穷其毕生，无视一切艰难，不懈追求的形象傲视着这个物质的社会。他是一个数学天才，他的数学成就，远不是一个好学生靠勤奋苦读，头悬梁锥刺股所能够成就的。那需要传奇般的天分，和发自内心对数学的无限热爱。正当少年求学的最好年龄，他却跟着妈妈到湖北干校锻炼改造，后来又独自回到北京当一名制锁厂的工人。文革结束，好不容易在北大度过了几年最美好的与数学相伴的岁月，当他雄心满满地踏上美国的土地，期望在那里再展宏图时，却遇到心胸狭窄自私的导师，让他的求职路充满坎坷曲折。他经受住了，其实以他的数学才能，在美国硅谷任何一家公司或金融公司都能轻松地获得不菲的收入，但是他根本无视这些物质上的诱惑，数学之美，才是他毕生的追求。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;度过20多年清贫寂寞的日子，他终于成功了！首次证明了弱版本的孪生素数猜想。他的成功，是等待小鹿光临时的灵光一闪，更是他苦苦追求多年的厚积薄发。他终于实现了他的梦想。当各种各样的光环向他袭来，他无措，甚至烦恼。但他值得，这么多的荣誉和称号！&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;他是我的老哥，大我十一岁的哥哥。常常有人用崇拜的口气来对我说：张益唐是你哥哥啊？真了不起！我们居然和名人离的这么近，和名人的妹妹是朋友，是同事。太荣幸了！是的，张益唐的名字现在成了充满正能量的传奇，成了很多知识分子崇拜的偶像。在这个浮躁的社会，很少有人还能像他这样坚守、执着，甘于寂寞清贫，不受物质世界的诱惑。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;span&gt;&lt;strong&gt;给这样一个数学家当妹妹，对于我，亦福亦难。&lt;/strong&gt;我当然爱他，父母已逝，这个世界上除了女儿以外，他是我血脉相连，最亲的亲人了。但是回想过去的岁月，我往往忍不住的潸然泪下。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;那对他来说最艰难的20多年岁月，对我们，对他后面的这个家，也曾是那么煎熬的时光。&lt;/strong&gt;20多年，他没有回国，甚至杳无音信，一对父母对儿子的挂念无从寄托，父亲早早地走了，走的时候没有哥哥的半点音讯。病弱的母亲，20年里最大的牵挂就是她最爱的儿子，但却一遍遍的希望，又一次次的失望，以至于最后当儿子终于站在她的病榻前时，她已经平静得好像儿子昨天刚刚来过。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;很多人想从我嘴里听到我老哥的传奇，他的奋斗，他的成功，和我们家的故事。我时常一笑而过，打住话题。因为对我而言，这是一个不容易展开的话题。这几年，我多想云淡风轻地回看过去的岁月，用几句轻松潇洒的话去笑谈我们走过的那些日子。但是我发现我根本做不到。父母临走前期盼的眼神总在我眼前闪现，我无法轻描淡写地替他们表述那曾经揪心的感情，那曾经多少年得不到回应的牵挂和挚爱。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;所以，我还是决定把这篇文章发出来，即使我写完后根本就不忍去读它，因为每次读到后面，我就会泪流满面。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;


&lt;section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;我的哥哥我的家 &lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt; 张盈唐&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我敢说，全世界也找不出与我和我哥哥同名的人。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我出生在1966年。在那个红色的年代，父母往往给女孩子起名叫红，华，梅……，男孩子则往往叫兵，军，刚……，都带着那个时代的浓厚烙印。 我和我哥哥的名字却大相径庭。哥哥叫张益唐，我叫张盈唐，字面和读音上非常相像的两个名字，都出自我那才华横溢的爸爸。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;爸爸姓张，妈妈姓唐。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;哥哥名字中的“益”与“一”谐音，寓意这是家里的第一个孩子，也暗含了父母希望这个孩子长大后做些对社会有益的事情。&lt;/strong&gt;我是家里第二个孩子，爸爸妈妈认为两个孩子足够了，“盈”寓意着满足。还有父母的朋友的诠释：“你这个名字的意思，其实就是你爸爸笑盈盈地看着你妈妈”，一句话把我家的生活日常勾勒得淋漓尽致。 &lt;/span&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;我出身在一个典型的中国知识分子家庭。爸爸妈妈都在通信行业从事科研工作。哥哥比我大十一岁。我一直很不理解，父母生我哥哥的时候，还是二十五岁正当年，却为什么又在长长的十一年后，在当时已经算是高龄的三十六岁，生下我这个女儿。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;直到现在，我才后知后觉地感觉到，也许冥冥中父母早已料到，&lt;strong&gt;一个注定要为事业献身的儿子，是不能指望他在父母身边扇枕温衾的，所以他们要有一个女儿来承欢膝下。 &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我的哥哥张益唐，从小就是个数学天才。为追求他心目中的数学之美，他默默耕耘三十年之久。在美国历经坎坷曲折，倾尽一生最好的年华，痴心不改。终于，在孪生素数研究方面取得了突破性进展，首次成功证明了弱版本的孪生素数猜想。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;其证明存在无穷多对素数相差都小于7000万的论文&lt;span&gt;Bounded gaps between primes&lt;/span&gt;，在2013年的5月被世界最权威的数学杂志《数学年刊》接受，《数学年刊》审稿人高度评价说：“这项研究是第一流的，作者成功证明了一个关于素数分布的里程碑式的定理。”&lt;strong&gt;因此，我哥哥的成就被誉为“敲开了世纪数学猜想的大门”，“是中国人有史以来在数学领域对世界的最大贡献”。随之，大小奖项扑面而来。 &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;哥哥的研究成就，给了科学界一个巨大的惊喜。一个一直默默无闻，根本不被数学界知晓的普通教师，在沉寂多年后，突然取得了轰动世界的大成就，全球科学界震惊了，华人圈更震惊了。各种媒体，报道纷纷去探究他这么多年的生活历程。于是，少儿时代数学小天才的故事；北大校友对数学学霸的回忆；普渡求学期间的艰辛与倔强；找不到工作的几年在清贫中的坚定和坚守；一个普通数学老师的职业操守与教学生涯；朋友花园里的灵光闪现和豁然开朗；直到孪生素数猜想证明的诞生……一个个生动的故事串起了数学家坎坷的经历和曲折的人生，迎合了一切传奇所需要的元素。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当2013年的5月，有亲戚把哥哥的新闻传到我耳中的时候，我的第一个反应是不相信。在我的心里早已认定，我的哥哥，注定会为他视为比生命还重要的数学而倾其所有，穷其毕生。而我知道，从古到今，为数学献身者千千万万，但是真能登上数学顶峰的人却少之又少，绝大多数人会奋斗终身却一无所获，默默无闻，甚至穷困潦倒地度过一生。我和妈妈已经坦然接受了这样的事实，既然哥哥这么热爱数学，就让他沉醉其中吧，只要他身体健康，生活安定，做自己喜欢做的事情，我们就放心了。其他的一切，在经历了这么多年揪心的期盼、等待和失落后，我们已经都不指望了。就像有一句话说的：你若安好，我便晴天。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;而当我从网上真真切切地了解到，哥哥的确取得了一个举世瞩目的成就，而并非八卦消息的炒作夸大后，我不禁一夜失眠，止不住的泪如雨下。记得我当时在微博上写了这样一段话：“从没有期待过，因此当好消息不期而至时，我的第一反应却是迟钝。九泉之下的老爸，终该欣慰了吧！重病的妈妈，还有精神为此而骄傲吗？万里之外的老哥，又该是怎样的心情啊！即使是我，回首20多年的艰辛和隐忍，也止不住的潸然泪下！Congratulations, My dear brother! Congratulations, My family!” &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;每一个人的身后，都有一个家。因此我始终以为，每一个人的奋斗都不是孤单的。在你觉得最孤独无助，觉得全世界都抛弃了你的时候，其实还会有两双关心和关爱的眼睛始终在注视着你，那是你的爸爸，你的妈妈。无论他们在哪里。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如今，一切都归于平静，宛如一曲清歌，如烟飘逝。我的哥哥，已度过了他一生中最艰难的岁月，可以更加自如地徜徉在他热爱的数学天地。我们至亲至爱的爸爸妈妈，都已长眠在故乡苏州的凤凰山下。只有我，虽然一直不忍回视，不想落泪，但终究还是硬逼着自己，去回望那过去近三十年的思念与亲情。让一个女儿，告慰天堂里的爸爸妈妈，你们为世界贡献了一个值得骄傲的儿子，一切的惦念担心都有了答案，你们可以安心了；让一个妹妹，告诉哥哥，你有一个多么爱你的家，你今天的成功里，他们的牵挂和承受，远远超出了你的想象。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;一位历经坎坷的数学家，背后一个普通的家，一双把对儿子的爱揉进生命中的父母。他们，共同谱出父爱如山，母爱似海的清歌一曲。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;亲情与爱，是我要为哥哥传奇的人生故事中，增加的一抹色彩。 &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;


&lt;section class=&quot;&quot; readability=&quot;1&quot;&gt;&lt;section label=&quot;Powered by xmyeditor.com&quot; class=&quot;&quot; data-tools=&quot;&amp;#x5C0F;&amp;#x8682;&amp;#x8681;&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; readability=&quot;2&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.15818181818181817&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/AtX6zYuGjynLUdO7MMfotQulBYNZY9MFibzmHlMTjgbaj90O59WznW6m3Wdgkia417lTp0oKh70H4sR8SyuV0icRg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;550&quot; width=&quot;100%&quot;/&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;小时候，记忆中的家 &lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;我说的小时候，从我有记忆起，一直到我高中毕业，我哥哥出国那年。——1985年。其实不算小了！ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;因为那是我最无忧无虑的时光。虽然家里也遇到各种困难，坎坷，但是，那是大人的事情，与我这个最小的女儿无关。那时候我的脑子里，只负责存放我自己的各种幼稚而又浪漫的小女生心思，家里的大小事情，是不用我费心的。 现在回首看看，那真是无忧无虑的小时候啊！ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;印象中，家里的大小事情，都是爸爸在操心。说我名字的另一个含义是：“爸爸笑盈盈地看着妈妈”，其实一点也不为过。因为它生动地再现了我家当时的生活场景。我的妈妈身体一直不好，除了喜欢做菜，包揽了家里主要的做饭任务之外，其他大小事情都是爸爸在照顾。而我的爸爸宽容，大度，幽默，细心，多才多艺，无所不能。他操持着家里的一切大事小事，小到柴米油盐，阳台上的花，窗台上的灰尘；大到老妈的身体，哥哥的学业，我的功课，一切尽在他的操心之中。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我的爸爸妈妈，也算得那个时代的有为青年了。他们都是建国以前入党的老党员，尤其我爸爸，还曾以年轻的19岁地下党员的身份，参加过解放上海保护电台的斗争。解放后，为了支持首都北京的建设，在上海邮电管理局工作的这对年轻人离开故乡亲朋，只身来到北京，投身到国家的邮电事业中。和当时大批年轻忠诚的布尔什维克一样，他们把祖国的建设看得高于一切，襁褓中的儿子被托付给上海的妈妈，两个好强的人废寝忘食地扑在工作上。当日常工作走上正轨时，一对年轻夫妻又不甘平庸，先后分别考上了清华大学和北京邮电学院。大学毕业后，爸爸凭借优异的学习成绩留在清华无线电系当了老师，妈妈回到邮电部工作。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;直到我出生那年，文化大革命来了。我们那个年轻而又美满幸福的家，和当时中国的千千万万个家庭一样，被席卷进文革的滚滚热浪中。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;爸爸因为解放前地下党的历史，理所当然地被认定为叛徒特务，和清华大学众多教授们一起，被发配到江西鲤鱼洲劳动改造，那是一个血吸虫病泛滥成灾，当地农民都望而却步的地方。不久，妈妈也接到通知到湖北干校下放。单枪匹马的妈妈很要强也很能干，先是把全家的家具打包送进了邮电部的仓库，上交了家门钥匙，然后一手抱着我，一手牵着哥哥登上了火车。先把我送到上海的外婆家，后带着14岁的哥哥到了湖北干校，在那里哥哥度过了他本该黄金的中学时代。后来又独自回到北京当上了工人。而我一直寄居在上海外婆家直到74年随爸爸妈妈回京。最艰难的几年中，我们家四个人，分别生活在北京、上海、江西和湖北四个地方。可想当时父母身在农场干校，对我们这对小儿女的牵肠挂肚。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;74年回到北京，刚开始全家团聚的生活，但却好景不长，76年的一场唐山大地震又引起上海外婆的担心。一次次来信来电，把我和多病的妈妈召回上海。一年多以后我和妈妈才又回到北京，全家人终于安定了下来。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;文革后全家在北京团聚的几年是我们家最幸福的几年。爸爸妈妈和当时所有知识分子的想法一样，为了把文革失去的时间夺回来而拼命工作。哥哥在北京制锁厂当工人，把所有的业余时间花在了他钟爱的数学上。我从小学生到中学生，轻轻松松地上学，快快乐乐地享受着来自父母兄长的溺爱。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;1978年哥哥考上北京大学，离开家去学校住校时，我还在上小学。1985年哥哥离开北大到美国留学，那一年我考上了大学。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;妈妈总说，咱们全家在一起的那几年是最幸福的几年，可惜时间太短了。我算了算，不到10年的幸福光阴。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;section class=&quot;&quot;&gt;&lt;section label=&quot;Powered by xmyeditor.com&quot; class=&quot;&quot; data-tools=&quot;&amp;#x5C0F;&amp;#x8682;&amp;#x8681;&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section label=&quot;Powered by xmyeditor.com&quot; class=&quot;&quot; data-tools=&quot;&amp;#x5C0F;&amp;#x8682;&amp;#x8681;&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot;&gt;&lt;section powered-by=&quot;xmyeditor.com&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section label=&quot;Powered by xmyeditor.com&quot; class=&quot;&quot; data-tools=&quot;&amp;#x5C0F;&amp;#x8682;&amp;#x8681;&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot;&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.15818181818181817&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/AtX6zYuGjynLUdO7MMfotQulBYNZY9MFbTn93Ijtz1sO4M2UQ3SH4GuZxDWgv8Xiasd1LUdiajcp1fBibM6TolUXQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;550&quot; width=&quot;100%&quot;/&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;哥哥印象 &lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;在我从小的记忆中，哥哥和我，根本就不是同一代人。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我小时候和哥哥生活在一起的时间并不长。3岁那年，我被妈妈送到了上海外婆家。这之前虽然和哥哥在一起生活过一年，但一点印象都没有了。我人生的第一个记忆还是从刚到上海那天的淅淅小雨开始的。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;上海的亲戚们告诉了我很多哥哥的童年往事。我外婆家是上海的一个工人大家庭。妈妈是老大，我还有两个舅舅，两个阿姨。最小的小阿姨比我哥也就大个五、六岁，所以哥哥的童年基本上是和我的小舅舅、小阿姨们一起摸爬滚打过来的。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;虽然长着一个智慧的大脑袋，但是在调皮淘气方面，哥哥绝对比不上那些上海小孩，而且他年龄还是小，所以基本上就是跟着大孩子屁股后面乱跑乱跳。这时候，我那外婆就会颠着小脚在后面使劲追，嘴里嚷着：宝宝，宝宝（哥哥的小名），别跟着他们乱跑，小心摔！你们几个孩子，不许欺负宝宝！然后一把搂过我哥哥，像老母鸡护犊子那样搂进怀里。时间长了，大家就说，外婆是我哥的“保护阳伞”。哥哥对外婆的感情也最深。妈妈去世后我们一起整理妈妈的遗物，他把外婆留下的一对耳环珍藏起来带回了美国。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;工人家庭没有那么好的学习环境满足哥哥强烈的求知欲。几个舅舅阿姨的学校读本，几本《十万个为什么》很快就被哥哥翻烂了，当几个大孩子学习成绩不好时，哥哥还能煞有其事地给他们补课。很快，这些都满足不了哥哥探索知识的欲望，他很快盯上了大舅舅的一个好朋友，姚先生。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;姚先生是个数学老师，每次来我家做客，哥哥就缠着他问这问那，甚至不让他和舅舅聊天，小小的年纪就开始和他讨论起高等数学的问题。大舅舅有个经典的段子，时不时拿出来调侃一番。叫做“舅舅大喜之日，外甥大哭一场”。讲的是在我大舅舅的婚礼上，按家里的习俗小孩子要单独坐一桌。但我这个倔强的哥哥一定要和姚先生坐在一起请教数学问题。大人不同意，他就在舅舅的婚礼上大哭了一场，生生搅乱了一场喜事。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;大一点的时候哥哥被接回北京，那时我们家住在清华园里。校园里满是满腹经纶的学者教授。哥哥如鱼得水，到处问大人问题。慢慢的这个奇怪的小孩子在清华园就有了点小名气。再后来，跟着妈妈到了湖北干校，他还是到处问问题，好在干校里的知识分子多，大家也都喜欢这个小小年纪脑子里却装满了高深知识的孩子。所以哥哥基本没有上过中学，他的那些知识，都是自学来的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.7438271604938271&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkwOayrDhTlDh3Emhn9EPDFCT2BTiaLAmXYCs2xje10IVTpmDBiaJfJETdHaibDp7u5WJ7AgnO4iapibLicA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;324&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.7975460122699387&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkwOayrDhTlDh3Emhn9EPDFCpRXCVT8dMn4aUHjAT1ToRFDsfZh5VKQ96tLUXrYIdGbHxwmFBv1W3Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;326&quot;/&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;我的发小，小时候写的关于我哥哥的作文&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1974年回到北京后，妈妈被爸爸在文革中的遭遇吓怕了，坚决不同意他再回清华大学当老师。因此，爸爸又调回了邮电部，和妈妈一起，在邮电部传输研究所工作。初回北京，父母单位分给我家的房子是分开的两个单间。我还小，所以和爸妈住在一间，哥哥住在单独的另外一间。年少的我，只记得哥哥的工作总是三班倒，剩下的时间也都窝在他那个小房间中捣鼓他的数学，只有吃饭的时候才能看到他。自然，大我十一岁的他，也是不屑与我这个小毛孩子多说话的。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;后来搬到传输所的后院宿舍，房间大了点，但还是分开在同一幢楼不同单元的两个单间。哥哥继续把下班后的所有时间献给他的数学，他喜欢他的小屋，安静不受干扰；他宝贝他的时间，除了吃饭时间外，最多也就是逗逗我们几个小孩子玩，一会儿就不见人影了。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;其实对哥哥的印象，更多的来自于妈妈。我一向知道，在家里爸爸更多喜欢我一些，而妈妈，就简直是太偏爱她的儿子了。在妈妈的口中，我总是那个贪玩，爱看电视，爱读小说，就是不努力学习的孩子，她总是要求我向哥哥学习。“你看你哥，从来不看电视。一心钻研学习。再看看你，总是坐不住，总想着出去玩。”这是我妈妈的老生常谈了。我不服气，谁也不可能整天像我哥哥那样学习啊！同时我也不在乎，只要有我爸爸喜欢我就行了。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当然，哥哥对妈妈也一直很孝顺。有时候妈妈生病上医院，他会背着妈妈跑上跑下。记得我们家有几年住在复兴门的12层高楼，那时的电梯到晚上11:00就停运了，有几次回家晚没有赶上电梯，哥哥就一直背着妈妈爬了12层楼。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;印象最深的是哥哥报考大学。1977年，文革后恢复高考的第一年，哥哥的政治考试成绩不够，没有上成理想的大学。1978年，又到了大学报考的时间。我这倔强的哥哥，却不愿报考北京大学的数学系了，他要直接报考另一所大学的数学系研究生。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我妈妈的一贯思想是学习要循序渐进，扎实基础。记得当时还是小学生的我，靠着小聪明学习成绩还不错，老师几次建议我跳级，都被妈妈果断地反对掉了。到了我哥哥这里，一个连中学都几乎没有上过的人，居然想直接跳过大学，读研究生，妈妈觉得这知识学的太不扎实了。反对是肯定的，难就难在妈妈和哥哥，骨子里都是很倔的人，两个人都不肯让步，终于大吵了起来，这是我记忆中家里爆发的第一次大吵。即便好脾气的爸爸在旁再三调解，也无济于事。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;问题的解决是妈妈拿出了杀手锏，她病了！ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;从小，爸爸就用他的身体力行，告诉我和哥哥，妈妈身体不好，我们都要照顾她，让着她。这一次，妈妈可能真的是急火攻心，一下子倒在床上起不来了。嘴里喃喃着：“我被你气死了，我病了，我不行了，我得上医院了”。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;一看这架势，我们三个人都慌了。急急忙忙要扶妈妈去医院。然而妈妈又说：“不行，你不答应报北大，我就不去医院。” &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;面对妈妈的病，倔强的哥哥终于败下阵来，答应了妈妈。于是，神奇般的，妈妈的病不治而愈。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我有时候想，今天的北大，以有一位名叫张益唐的校友为荣。其实他们，还有我哥哥，都应该感谢我的妈妈，&lt;strong&gt;如果没有我妈妈对北大至始至终的敬仰和坚持，没有哥哥对妈妈的孝顺，很有可能，哥哥就与北大失之交臂了。 &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6905487804878049&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkwOayrDhTlDh3Emhn9EPDFC57bFxMIO369TdsXIMlN7BtIDVbsgZArCAaqSkwrVvpaKSz51NdnQuQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;656&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;哥哥和妈妈的合影&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;一直以为我家是爸爸更喜欢我，妈妈更喜欢哥哥。直到去年，我在搬家的时候翻出了妈妈一直保存着的爸爸的日记。我读的时候很好奇，想知道在我的孩童时代，我的父母兄长们都在想什么做什么。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;结果我发现，其实爸爸对我的爱，只是一个大人对小孩子的爱，溺爱和娇宠。而爸爸对哥哥的爱其实更加深沉，那已经是两个男人之间的交流和切磋，关注他的学业，关心他的发展，探讨他的未来。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;做过教师的爸爸，深知自己儿子的数学天赋，但是因为身处那个时代，20多岁的年轻人一心扑在事业和工作上，没有谁会像当今的父母一样去刻意地为孩子设计未来，还有不得不经历的各种政治风雨，他没有过多的时间倾注在儿子的学业上，但他是知道自己儿子的与众不同的。父亲含蓄，他把情感的表露，把与儿子的交流，把对儿子深切的期盼和在他成长过程中一步一步的指点都写在了日记里，我读着爸爸遗留下来的日记，看着他写的或细腻周全或深思熟虑的文字，忍不住热泪盈眶。如果父亲地下有知，知道他的儿子终于实现了自己的理想，登上了数学研究的顶峰，他该是多么的欣慰和自豪啊！ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;而我，有时会懊恼，觉得哥哥和我的年纪差的太多了，没什么共同语言，我这个妹妹体会不到兄长的关怀。直到1984年我上高三的那年，才改变了这个印象。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7492753623188406&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkwOayrDhTlDh3Emhn9EPDFCTwsiaPDTrt5OwcmiaWZiceJ6oGkJZia24NVtFp8xPY1JuIcjUhhczmYh9w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;690&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;和哥哥的合影&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;高三的运动会上我跳高把腿摔了，很严重，只能拄根拐杖，一瘸一拐地走几步。正值高考前夕，上学成了负担。那时候哥哥研究生快毕业了，在家里住的时间多一些，于是担负起推着自行车送我上下学的任务，同时帮我复习数学。虽然每次他都会和妈妈吐槽这个妹妹太笨了，但他还是勉为其难地帮我一直复习到高考。那段时间，18岁的我，在懊恼自己受伤影响高考复习的同时，也还有点小确幸，有个哥哥的好处终于显现了。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;虽然孩童时的我对哥哥不太关心，但我还是知道哥哥的性格不像我们家人。我的爸爸妈妈，在单位里都是典型的好人缘。爸爸待人随和，幽默开朗，多才多艺，单位上上下下都喜欢和他交往，那时候，家里经常坐着三三两两找爸爸谈工作聊天的人。而我妈妈，就是典型的党员唐大姐了。本来就是研究室支部书记的她，热衷于关心单位同事的工作和生活。我呢，虽然是个性格比较内向的孩子，但还算聪明灵巧，朋友师长们的一致评价是，这姑娘情商挺高。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;而我哥哥的性格却是典型的清高自傲，“道不同不相为谋”。哥哥自有他的一帮志同道合的朋友，经常聚在他那小屋里侃侃而谈，半天都不出来，那时候，会看出哥哥的兴奋。但是，对于他看不上的人，谈不到一起的话题，或是些家长里短的闲聊，他却一句话回应都没有，转身就走。毫不顾及所谓的礼仪礼貌。常常让爸妈在外人面前觉得尴尬。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;而这，确是一位数学大师的真性格。靠着这样不合世俗常规的秉性，我的哥哥才能一路坚持住他的梦想，不屑于物质世界的诱惑，经历常人难以忍受的艰辛与寂寞，走到成功的今天。  &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;section label=&quot;Powered by xmyeditor.com&quot; class=&quot;&quot; data-tools=&quot;&amp;#x5C0F;&amp;#x8682;&amp;#x8681;&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot;&gt;&lt;section powered-by=&quot;xmyeditor.com&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section label=&quot;Powered by xmyeditor.com&quot; class=&quot;&quot; data-tools=&quot;&amp;#x5C0F;&amp;#x8682;&amp;#x8681;&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot;&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.15818181818181817&quot; data-type=&quot;jpeg&quot; data-w=&quot;550&quot; width=&quot;100%&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/AtX6zYuGjynLUdO7MMfotQulBYNZY9MFVZfPicDSmVd2EfWZPxunUqianzHIWgk7sibdJRicg7vyLxykkDICgNPOibg/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;爸爸，病中的坚强 &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;从小到大，在我的心目中，爸爸是一个无所不能的神话。19岁时，他就是上海南翔电讯台唯一的一名地下党员，领导了保护电台迎接上海解放的斗争。因为地下党的经历，文革中他被当成叛徒特务，和清华大学的很多教授走资派一起，被送到江西鲤鱼洲那个血吸虫病高发区劳动改造了好几年，手上一直留有肝掌的红色斑块。但是文革一过，他就无怨无悔、废寝忘食地投入到工作中，要把失去的时间补回来。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;他是中国第一批研究移动通信的专家，是他们那代人把当时国外最先进的移动技术引进到中国，中国的移动通信事业蓬蓬勃勃发展至今，爸爸也算得奠基人之一了。后来中国移动通信大发展时期很多叱咤风云的技术精英，都曾是爸爸的下属或学生。工作上，我眼见周围的叔叔阿姨们对他的敬重和佩服，我知道爸爸以自己的学识和钻研奠定了在事业上的权威；生活中，他是家里的顶梁柱，家里的一切柴米油盐，大事小事，都是爸爸在张罗忙活。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;至今，我都无比羡慕我的妈妈，能有这样一个坚实的依靠。然后，他还写得一笔好字，舒展典雅的隶书像极了他沉静淡泊的性格。他喜爱中国的古代文学，唐诗宋词都造诣非浅，《稼轩长短句》和《白香词谱》两卷册子在他深受病痛折磨的日子里一直伴随在他的床头。他幽默诙谐，满腹经纶冒出一二，就会吸引住四邻老小。甚至，他还有闲暇时间和心情去为隔壁一个爱美的阿姨做条花裙子。 &lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.280373831775701&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkwOayrDhTlDh3Emhn9EPDFCZsnd9CeqS7qbT55emKqiamkAqunBfxibZgO2MlvPsVk41pbRm0DF1RVw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;428&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;爸爸贺长辈生日作的词，爸爸的书法&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我总在想，哥哥也算是我家的奇才了，但他的聪明和天赋，绝对应该来自爸爸的遗传。哥哥的成果出来后，很多人都很佩服他，一个埋头数字的数学家却对文学、音乐等有着如此深的造诣与喜爱，&lt;strong&gt;只有我知道，那是因为我们从小浸染在爸爸营造的氛围中。&lt;/strong&gt;我们兄妹俩最大的遗憾，就是没能把爸爸一手漂亮的隶书继承下来。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;多少年后，当我看到张艺谋的电影《归来》中陈道明演的陆焉识，身上那种荣辱不惊的隐忍、巧手体贴的平和，却掩盖不住内心的满腹才华和铮铮傲骨，那眼神那气质让我立刻想起了我的爸爸。那样一种经历过文化大革命的中国知识分子的沉淀与从容。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在全家人的心中，爸爸都是一棵大树，为我们遮风挡雨，给我们支撑依靠。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;当爸爸突然病了，我才知道，家里的大树倒了；而我，必须马上长成那颗大树。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;一切的改变开始于27年前的那个夏天。那时候，哥哥在美国留学已经5年了，而我，大学毕业不久，一门心思想和哥哥一样出国留学。 &lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;span&gt;那个夏天，报了一个业余的外语培训班，计划考托福。记得那天下课后还和同学一起跑到北师大旁边的蓟门烟树去疯玩，晚上回到家时已经半夜了，却见爸爸铁青着脸在家里坐着，见我回来，很生气地问我知不知道几点了，为什么这么晚回家，还记不记得明天一早要陪他上医院。我有点心虚，为自己的贪玩，为爸爸从来没有这么严厉地批评过我。也有点委屈，我觉得晚点回家并不影响第二天陪爸爸到医院看病啊，其实我根本就没把上医院这件事情放在心上。在我的印象中，爸爸的身体超好，多少年也没个感冒发烧的，他怎么可能生病呢？ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;但是第二天和爸爸一起到了肿瘤医院，找到已经联系好的一位专家，做了很多检查，我才懵懵懂懂地知道，爸爸的病很重。食管癌，必须马上手术。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;但即使如此，我还是没有意识到这个病的严重性，没有意识到这将给我亲爱的爸爸，给我们的家，和我的生活带来多么大的变化。当时才20来岁的我，心里对“癌症”这个词实在是没有什么概念。爸爸在医院里跑来跑去，做住院登记，和医生商量手术的时间。而我，只是傻傻地跟在后面，什么忙也帮不上。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;天翻地覆的改变是从三天后的手术开始的。那天的手术，因为妈妈一直身体不好，爸爸不让她到医院陪伴，所以是我和爸爸单位的同事一起在手术室外等候。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;早晨八点多推进手术室，大夫预测手术要5、6个小时，让我们耐心等待。然而，不到3个小时，爸爸就被推出了手术室。糊里糊涂的我被叫进医生办公室，却听到了一个天大的噩耗：爸爸的食管癌已经转移，手术切除已经没有意义了，所以又原封不动地合上了。术后大概只有几个月的时间了。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;医生、单位同事和我商量，怕影响爸爸继续治疗的信心，也怕影响多病的妈妈的身体，暂时不告诉他们真实病情，只是说手术很顺利，肿瘤都被切除了。而病情真相和治疗方案，只是我知道。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我点头，眼泪决堤而下。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;回到病房，我拼命忍住满眼的泪，强颜欢笑地面对刚从麻醉中醒过来的爸爸，和从家里赶来的妈妈，告诉他们手术很成功，病灶已经清除干净了。爸爸虚弱而欣慰地笑了，我的内心突然涌动起一种心疼，从未有过的，对爸爸的心疼！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;27年前要和美国通电话是件很麻烦的事情。傍晚时分，我跑到在农业部工作的堂姐那里，用她的办公电话给远在美国的哥哥通了长途。 电话通了，当遥远的“hello”传过来时，我满心的害怕紧张，满腹的委屈心酸，好像都找到了出口，我哭着把爸爸的病情，手术的情况告诉了哥哥，抽泣着叫哥哥“你快回来一趟吧！” &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;哥哥的沉默让我的情绪也慢慢沉淀下来。他说让我好好照顾爸爸妈妈，说他马上寄钱回来让我给爸爸买药和补品，说他会争取机会回来一趟。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;回到病房，我执意让爸爸单位的同事陪妈妈回家，由我留下来陪伴刚手术完的爸爸。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这一夜，爸爸虚弱地时睡时醒。我时而趴在病床边，时而躲到病房的窗旁，任眼泪流了一夜。生命中，爸爸是我最爱，最依赖的人了，我从来以为，他会为我铺设好一切，而我，只需要在爸爸的呵护下快乐生活就行了。却从未想过，爸爸也会生病，也会需要我的照顾。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;而我，一点准备都没有！ &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;爸爸术后，马上开始了放疗。放疗的不良反应，使得他的身体迅速消瘦下去，虚弱，呕吐，脱发，稍做活动就气喘吁吁，食物难以下咽。我家的大部分亲戚住在上海和江浙一带，北京能够帮上忙的亲戚很少，那时也没有护工。单位同事虽然都很热情，但毕竟不好意思长时间请他们帮忙护理，妈妈的身体又不好，所以往返医院照顾爸爸的任务都落到了我的身上。单位很照顾我，同意我请假一段时间。我每天的行程是固定的，一早带着妈妈在家精心做好的饭菜，辗转几趟公交车到位于北京东南边的肿瘤医院，晚上，再回到位于西北边的家。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;每天这样往返跑着，我的心里却充满了惶恐。20来岁的我，从来没有为家里的什么事情操过心，现在，爸爸的病像一座大山压在我的身上，医生三天两头告诫我这种危险那种可能，我却不能对自己的爸妈吐露实情，还要编出各种好消息去安慰他们。尤其是看到躺在病床上的爸爸，原来结实强壮的身体，一天天变的虚弱，我心疼的要命，却还要在他们面前强作欢颜，连哭都要偷偷找个他们看不见的地方。每天晚上，当爸爸催着我快回家时，我都是百般的不舍。看医院里别的病人，家里有亲人白天黑夜轮番护理，而我，就一个人，晚上没人照顾爸爸，万一有点事情可怎么办？可是，爸爸惦记独自在家的妈妈，怕她一个人在家胡思乱想或生病，每晚一定要我早早回家。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我急切地盼着，哥哥快回来吧，有哥哥在，我就有了依靠，一切就有了担当。 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;但是我打了好多次电话，哥哥的答复和第一次没什么两样。让我好好照顾爸爸妈妈，说他会多寄点钱回来让我给爸爸买药和补品，说他争取机会回来一趟。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;模棱两可的回答让我听不懂他的真正意思。慢慢地我心凉了，我发现我真的不了解哥哥。我不知道，自己最亲的亲人病重，他为什么就不能回来一趟呢？什么事情比爸爸生病更重要呢？ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;来探望的同事朋友也每到必问，哥哥知道了吗？哥哥什么时候回来？我和妈妈无言以答，只好含糊：快了，快了…… &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;慢慢的，我开始有点怨恨起哥哥来。 这时，病中的爸爸向我展示了他的豁达，他的乐观，他的坚强。 &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;病情稍好一些，爸爸又担起当家作主的角色。他安慰妈妈和我，从不向我们抱怨他的病痛，反而时不时用风趣和幽默来减缓我们心中的压力。他镇定地为自己安排治疗方案。在朋友亲人的热心帮助下，他从肿瘤医院出院后，又先后到中日友好医院进行中西医结合的化疗，到解放军304医院接受当时最新的一种生物疗法。治疗期间，他向病友学会了郭林气功，每天到家后面的小树林锻炼。虽然他的身体越来越虚弱，走路的速度越来越慢，说话时的气喘越来越厉害，但根本见不到他叹气发愁的时候。他向我们传递的，从来都是他的身体越来越好的信息。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;他一刻都没有忘记他的工作，病前他刚刚出版了一本书《移动通信》，由于反响热烈，出版社邀请他出第二版。他就在病床上反复改稿，和同事、编辑讨论修订，使那本书很快就得以再印出版。病稍好一些，出院后他马上又回到了工作岗位，那时候他是电信技术研究院研究生部的领导，他忘不了他的学生。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;与我和妈妈焦急地盼望哥哥能回国一趟相反，爸爸却安慰我们说，随他吧！他一个人在外的日子不容易，不回来，肯定有他的难处，我们不要给他增加压力。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;后来的日子里，爸爸身体稍好的时候，还是会一切如常地给哥哥写信，描述家里的生活，尽量轻描淡写地说起自己的病情，问候哥哥的情况。哥哥也会回信，信很短，只是说自己一切都好，但雷打不动地，会每月附上几百美元的一张支票，是给爸爸买补品的钱。再到后来，哥哥的来信越来越少，通信地址变来变去，电话也找不到他了。我们只是从时间上知道他应该是毕业离开了学校，但是他始终没有告诉我们他去了哪里。联系就这样慢慢地中断了。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;爸爸和哥哥之间的沟通，对我来说始终是个谜。我是那种在爸爸面前，有什么话都要啪啪啪地说出来，然后等着爸爸来给我出主意想办法的孩子，但是哥哥相反，他很少把他的情况一五一十地说给家里，他更习惯于报喜不报忧，有好消息时就会来信告诉家里，自己遇到难处了，他就不说了，来信也少了。研究不顺利，不肯发表自己认为不完美的论文，与导师关系破裂，毕业拿不到推荐信，没有工作只能打临工，居无定所甚至住在房车上，这些都是我后来从网上知道，但是他从来没有对我们讲过。他人生最低谷的那段日子，也正好是爸爸病重的两年。现在的我能体会他当时的举步维艰，还有无法面对家人的落寞，所以他选择了沉默。他的逃避曾经让当时的我很气愤。但是我猜想爸爸能够感受到儿子所处的困境，所以爸爸也选择了沉默。一个病重的父亲，已经没有能力去帮助自己远隔重洋的儿子，那么只能选择不再给他增加负担，只能期待他自己度过难关。 &lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;span&gt;手术打开又关上的时候，医生告诉我，爸爸只有几个月的时间了。但是我那坚强的爸爸，凭借自己的乐观和勇敢，与疾病斗争了两年多。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;1993年的3月，春天即将到来的日子，才只有63岁的爸爸带着他对妻子的惦念，对儿子的牵挂，和对小女儿的不放心，带着满满的不舍，终于离开了我们。 &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7463479415670651&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkwOayrDhTlDh3Emhn9EPDFCzxegDRRLxBtfBiaXutJ4CmgUvHQC8LhSRUvWauKfMGv0vUTyRqvgY1w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;753&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;爸爸在日内瓦国际电联会场&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;1990年到93年，电视剧《渴望》持续热播，我每天傍晚从医院回家的路上，都能听到从家家户户窗口传来的“悠悠岁月，欲说当年好困惑”的旋律。我永远忘不了这个旋律。每当听到它，我就会想起陪伴爸爸度过的最后的日子。想起我倚在爸爸身边，对他说：“爸爸，我怕以后对你的回忆，全都是你在病床上的样子”时，爸爸的神情黯然。想起1993年的除夕之夜，妈妈做了几个好菜，我们一起在病房，陪爸爸度过的最后一个春节。吃完饭后，妈妈坚持要我回家，她自己在病房陪爸爸过夜。我一个人，在除夕夜空寂无人的马路上等车，再回到空无一人的家里呆坐，那种回天无术的孤苦，永远地印刻在我的记忆里。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;爸爸是一个深夜走的，第二天，是北京初春一个晴朗的日子。我望着湛蓝湛蓝的天空，却知道，天空再也不是以前的天空了，没有了爸爸的日子，和以前再也不一样了。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;爸爸走的时候，我们已经联系不上哥哥了。我不知道，正在遥远的异国他乡苦苦挣扎的哥哥，他能否感受到爸爸越走越远，再也不回头的背影，他能否听到我和妈妈的哭泣和呼唤？！ &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;section class=&quot;&quot;&gt;&lt;section label=&quot;Powered by xmyeditor.com&quot; class=&quot;&quot; data-tools=&quot;&amp;#x5C0F;&amp;#x8682;&amp;#x8681;&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot;&gt;&lt;section powered-by=&quot;xmyeditor.com&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section label=&quot;Powered by xmyeditor.com&quot; class=&quot;&quot; data-tools=&quot;&amp;#x5C0F;&amp;#x8682;&amp;#x8681;&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot;&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.15818181818181817&quot; data-type=&quot;jpeg&quot; data-w=&quot;550&quot; width=&quot;100%&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/AtX6zYuGjynLUdO7MMfotQulBYNZY9MF8ib8TUdzibv3CelC2yibgkACSsUvFYtaPyS8NhwA09mquqyO9aLuU4npw/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;和妈妈一起走过的日子 &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;爸爸的生病，让我彻底断了出国留学的梦想。爸爸走了，我知道，他万般不放心地把照顾妈妈的任务交给了我。我在心里发誓，即使是为了爸爸的嘱托，我也会照顾好妈妈！ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;妈妈从30多岁开始，身体就一直不好。有人说她是年轻时太勤奋，太好强了。比如，她在北邮上大学时，每天晚上宿舍熄灯后，还要躲在楼道里学习到深夜2、3点钟。长期以往，把自己的身体拖垮了。 &lt;/span&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;妈妈身上的病，多得每次陪她上医院，我都很难向医生介绍全。上湖北干校的那段日子，条件太艰苦，心脏病多次发作，不得不送回上海病休。长期高血压演化成后来的脑梗，几次脑梗突发，送医院急救，把全家人吓出一身冷汗，幸好抢救都很及时没有留下什么后遗症。又不知什么时候妈妈开始乏力，消瘦，没胃口，结果一查，是肝出了毛病，这是一种自身免疫系统出问题导致的肝损伤，治疗中用到激素，结果肝没治好，却导致股骨头坏死，几近瘫痪，大夫说需要手术置换，但是妈妈的身体综合状况使得我们不敢轻易同意手术，到处打听，做激光治疗配合中医，半年后老妈居然可以拄着拐自己慢慢走路了。但是从那时候起，妈妈就离不开轮椅和拐杖了。除此以外，还有风湿性关节炎，干燥症，白内障，……大大小小的病总是不断，往往是这个病稍微好一点，那个病就又冒头了。协和医院的病例，厚厚的一大本。平时，妈妈最注意自己的血压，最怕的是脑梗，但最后，却是肝损伤导致的肝硬化夺走了妈妈的生命。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;妈妈身体不好，我不可能让她一个人生活。20多年来，妈妈一直和我住在一起。曾经一直以为，这么多年都是我在照顾体弱多病的妈妈，而等到妈妈走后我才豁然发现，其实，是妈妈一直陪着我，走过了这20多年的路。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;她陪着我结婚生子，看我从一个女儿成长为一个妻子，一个母亲。帮着我一起把女儿带大。当我逐渐变成家里的主要劳动力后，各种家务琐事有时会惹得我心烦上火，冲老公发脾气，每逢这时，她必定坚决地站在我这一边，更加严厉地数落我老公的不是。慢慢地，我们全家人都知道，一旦我妈妈开始找女婿的各种不是并开始批评他，就说明这段时间老妈的身体状况不错，精力充沛。我们也就心安了。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;她做得一手好菜，每天变着花样的饭菜让我老公婚后迅速从豆芽菜体型发展成了啤酒肚，不得不计划减肥。即使是后来实在做不动了，她也要打起精神调教家里每一个新换的阿姨，直到做出的饭菜合乎她的要求。这就导致我成了那个最不会做饭的人。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;她看着我20年间，从一个青涩的职场小白，成长为一个项目骨干，专业带头人和科研管理者。我的工作越来越忙，出差越来越多，妈妈每天简短的日记上，便充满着“盈**号出差**地方，**号回京”“盈今天加班到*点才回”的字样。每次我出差，当我拎着箱子跨出楼道门后，回头总会看到妈妈站在窗边望着我冲我招手的身影。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;她是个爱炫耀的妈妈，以前，儿子的数学天才曾给过她丰富的吹嘘素材，现在，这个话题再难以启齿，她只好退而求其次，朋友聊天时总要夸大我女儿多么能干，工作多么繁忙，对我多么孝顺…… &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;退休的妈妈喜欢读报，订阅了每天都是厚厚一沓的《北京青年报》。因此她知道的新闻很多，甚至还是姚明的粉丝。每天晚饭的时候，是我们家最热闹的时间，妈妈要告诉我她从报纸上看到的各种新鲜事情，女儿要给我讲她学校里的大小趣闻，我坐在她们俩人之间，两个声音经常同时往我耳朵里灌，而我的脑子里却往往还想着白天工作上的事情，时间长了，就会不耐烦地止住她俩：“你们俩一个一个说，我没法同时听两个人说话。”这时，妈妈就会拿出长辈的尊严，对外孙女说：“你小孩子懂什么？不要打断大人说话”。而我那委屈的女儿，就只好默默地掉小金豆了。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;年轻的时候，她曾经是个工作狂，玩乐享受对她来说简直是犯罪。年纪大了，身体差了，她却喜欢跟着我们到处去玩。冬天到郊区泡温泉，夏天到海边游泳，每到这时，她就会喃喃地说：“现在的日子多好啊！可惜你爸和你哥没福气享受这样的生活”。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;曾经以为多余的身影，再也不会出现在窗前；曾经以为嘈杂的声音，再也不会想起在耳边。这时我才知道，曾经的一切，是多么弥足珍贵！ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;20多年，妈妈没有停止过思念儿子，每年哥哥的生日那天，她必定要煮上一碗长寿面。&lt;/strong&gt;不像爸爸的含蓄，妈妈是要和我念叨的：“你哥在外面不知道怎么样了？” “这么多年怎么就没有个消息”“什么时候能回来看看？”“不会出什么事情吧？”……而每当新闻里传来美国有什么天灾人祸的事情，她就总是很紧张地联想到儿子。总之一个妈妈能想到的儿子的各种情况她都想到了，但是没有一点办法，因为从93年爸爸去世那年，到2000年左右，我们干脆就找不到哥哥了。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;各种亲朋好友的问候也让妈妈格外心烦意乱。说实话，天才的哥哥，在父母的朋友圈里还是有点名气的，时不时有人来看望妈妈时要问起：“儿子怎么样了？什么时候回来啊？”妈妈其实是个好面子的人，她生有一个优秀的儿子，却根本不知道他在哪里，在做什么，根本无从回答大家的关心。这对一个母亲来说是何等的煎熬。慢慢地，我和妈妈都开始害怕遇到这类问题，都学会了含糊其辞，一带而过。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;那些年，与妈妈的心情不太一样，其实我是有点怨恨哥哥的。和爸爸、妈妈一样，我也是个很恋家的女儿，我根本就不能理解，为什么哥哥这么多年会不给我们丝毫消息，难道他忘记了在北京还有他的一个家？还有妈妈和妹妹在担心他？ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我会有很多胡思乱想。我猜想，也许哥哥和爸爸的感情不深？因为从小他和爸爸一起生活的时间不长，他会不会有点责怪爸爸对他关心不够？但是妈妈对哥哥的偏心是人人皆知的，哥哥不可能不爱妈妈。所以，这一切是为什么呢？或者，是不是哥哥在西方的时间长了，中国传统文化中的父母亲情，忠孝礼仪在他的脑海里已经很淡泊了？又或者，就是哥哥在美国的事业发展遇到了挫折，他觉得无颜见江东父老？我觉得这种可能性比较大。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;虽然朦朦胧胧想到哥哥在美国肯定过的不顺利，以我和妈妈对哥哥的了解，我们也知道哥哥绝对不会改变初衷，为了生活去从事别的工作。但越是知道他的脾气秉性，我们心里的牵挂就越多。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这期间，我们找过一切可能的关系帮我们寻找，但是，杳无音信。网上讲我哥哥的故事，总会扯出一条：2000年，妹妹在网上发寻人告示，家里找不到哥哥了。其实这件事情我已经记不太清楚了，因为这么多年来，多少次各种途径的寻找，找他湖北干校一起长大的发小帮着打听，找他大学的同学多方联系，各种找不到，我都已经数不过来了。如果不是顾及哥哥会有意见，我都要去找大使馆帮忙了。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;最终找到哥哥，也是2001年左右，感谢万能的互联网，让我终于大海捞针般地在美国一个很不起眼的大学，新罕布什尔大学网站的教职员名单中，找到了哥哥的名字和他的邮箱。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当试探发出的邮件终于收到了哥哥的回复时，妈妈欣喜若狂。八年了，终于又和儿子联系上了。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;那时候哥哥已经在新罕布什尔大学安顿下来了，看得出他很喜欢他的工作。也很高兴和我们联系上，他寄来了他的近照，学校的风景照，那的确是一个美丽的校园。妈妈流着眼泪，心疼地说，你看，这照片上的毛背心，还是他出国时我亲手给他织的。这手表，也还是出国的时候带的。你哥哥这些年过的是什么日子啊！ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;和哥哥联系上的几年，是妈妈又开心起来的几年。哥哥结婚了，寄来的结婚照也让妈妈放下了又一桩心事。岁月的沉淀，生活的磨难，使得这个曾经倔强一根筋的毛头小伙，开始体会到家庭的亲情。当我把妈妈和我女儿的一张合影寄给他时，他高兴地把照片放大了好几倍贴在自己的办公桌前，到处和人炫耀，说张家有后人了。我女儿中学时有一年到美国参加冬令营，他冒着大雪从新罕布什尔跑到波士顿，早早地等在孩子们将要下榻的宾馆大厅，当一大群孩子从门外涌入时，他一眼在人群中找到了自己的外甥女。“那就是我们张家的孩子”。这是他在远离家乡20多年后，第一次在异国他乡见到了自己的后代，激动溢于言表。他对外甥女的喜爱出乎我的意料，以至于最近这两年遇到他美国的同事，听大家聊起来，我发现在他们当中，我女儿比我名气大多了。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;和哥哥联系上后，妈妈又开始催着哥哥回国探亲。毕竟她已经太长的时间没有见过自己的儿子了。她的年纪越来越大，身体也越来越不好。能见到儿子，已经成了她生命中最大的期盼。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;那些年，给哥哥写信，或是寄去妈妈的亲笔信，问他什么时候回国，做着这些，我心里愈发地不理解。为什么，哥哥回趟国这么艰难呢？ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;按理说这个时候出入国门已经是件很简单的事情了，美国再也不是以前那个让人神往的神秘国度，哥哥也很多次流露出回家探亲的想法。但是好几次，回国的手续办着办着就无疾而终了，哥哥只好找个借口，告诉我们他回不来了。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;希望最大的一次就是2009年暑假，哥哥拿着中科院的邀请函去办签证，我们都以为这次是板上钉钉的事情了。妈妈精心地为儿子儿媳准备好一套全新的餐具，甚至想好了欢迎家宴上的菜单。又专门买了一个杯子，上面一个羊的图案，因为哥哥属羊。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;但是又收到哥哥的邮件，大使馆需要他提供以前的各种材料，还需要等待几个星期，这样就错过了暑期，他又回不来了。 &lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;span&gt;我的涵养还是不够，我又一次急了，通信中流露出我的不满和责怪。他为什么就不能拿出他钻研数学的韧劲，去把回国签证搞定呢？按理说一个20年未曾回国的人，第一次拿着美国护照要办中国签证，大使馆有审查要材料还要走一系列流程，也是可以理解的，但这个太犟的哥哥，除了数学以外不愿迁就任何琐事，不愿迂回任何障碍，在办回国手续时遇到麻烦扭头就走。结果把回国变成了比破解孪生素数更难的难题。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;就这样，在经历一次又一次殷切的期盼，焦急的等待和越来越深的失望后，尤其是2009年的那次打击之后，妈妈不再幻想哥哥还会回国看她。&lt;/strong&gt;她一遍遍地对我，其实也是安慰她自己说：只要你哥哥在那里生活安定，身体健康，能做自己喜欢做的事情，我也就安心了。也不求其他了。那个画着小羊的杯子，最终哥哥也没有拿它喝过水。那杯子成了妈妈的喝水杯，直到生命的最后一刻，她都是拿着它喝水吃药。我不忍细想，每一次举起这个杯子的时候，老人家会是一种怎样的心境？！ &lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.3459715639810426&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkwOayrDhTlDh3Emhn9EPDFCZIDT1gNxoG4e2y4rfBgsOsVrLgg3qUVjosSpsnKuNibg1UeHK9vFQtw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;422&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;妈妈给哥哥买的羊图案的杯子，后来妈妈一直用它喝水&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;有人说如果张益唐在国内，肯定会有来自亲朋好友，四面八方的各种世俗压力，他不会取得这样的成就，但不管外界如何，起码我们家人从来不曾给哥哥施加过任何事业上的压力。一是我们家经济条件一直还可以，还没有遇到过什么大的财务困难，所以全家老小都普遍缺乏经济头脑，钱财在我们脑海里就是有则多花，无则少用的身外之物，那些所谓富贵奢华的生活品质，也根本不是家人的追求。二则父母都很了解哥哥的秉性，他们知道他的倔强，知道他绝不会为五斗米折腰。其实家里对哥哥的最大心愿，就是他能在一个自己喜欢的大学或者研究院所工作，专心地研究学问，时不时能回国看看逐渐年老的父母，让他们放心。但是就是这点最普通的想法，这么多年却成了奢望。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;哥哥1985年出国，87年回过一次国探亲，然后就是25年的漫长离别，直到2013年8月，他才功成名就，再一次踏上祖国的土地。真真是“少小离乡老大回，乡音难改鬓毛衰”。这时候，爸爸已经离开我们20年了，妈妈也已无力再为儿子再做一顿家乡的饭菜。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;2013年的8月20日，哥哥终于回国，那天正是妈妈的83岁生日。哥哥的第一次回国特意选在这天，也是想把儿子迟到的回归作为礼物献给生日和生病的妈妈吧！ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;无数次想象母亲与儿子在阔别25年后的第一次见面，该是怎样的百感交集，心潮澎湃。为此我特意提前买了速效救心丸备着，怕妈妈过于激动出现意外。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;但令我吃惊的是妈妈，终于见到了天天念日日盼的儿子，她却如此的淡定，没有眼泪，没有伤心，没有激动，她呈现给儿子的是一幅欢喜的笑容。&lt;span class=&quot;&quot; lang=&quot;ZH-CN&quot; xml:lang=&quot;ZH-CN&quot;&gt; &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;blockquote readability=&quot;13&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;“儿子，你回来了”；&lt;/span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;“姆妈，儿子回来看你了”；&lt;/span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;&lt;span class=&quot;&quot;&gt;“益唐，你回来姆妈就放心了。身体好吗？”；&lt;/span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;“姆妈，我身体好的很，一点病都没有”；&lt;/span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;“让姆妈看看你，嗯，我儿子没有瘦，也没有老”&lt;/span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;……&lt;/span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span class=&quot;&quot; lang=&quot;ZH-CN&quot; xml:lang=&quot;ZH-CN&quot;&gt;&lt;span class=&quot;&quot;&gt;平平常常的对话，好像儿子只是出了个短差，又回家了。&lt;/span&gt; &lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我至今也没有想清楚，为什么妈妈在间隔20多年后见到她最爱的儿子，心情却如此平静。要知道妈妈以前一直是个感情浓烈的人，每每和我提起他的宝贝儿子，那份担心和无助的神情总是让我揪心。那天夜里我怕妈妈因激动而睡不着，仔细观察了妈妈的睡眠，却发现她睡的比往常都要香甜，酣然如婴儿。是再没有了牵挂而放心了？ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;2012年的秋天，妈妈肝硬化住院。对这样一个多病的老人来说已找不到根治的办法了，只能用保守疗法维持病情。2013年的8月，妈妈的身体已经极度虚弱，浮肿，少尿，低烧，肝中毒引发的脑中毒……每日的活动仅限于床和轮椅，日常生活完全离不开保姆。但是儿子在国内的那10天，是妈妈精神状态最好的十天，她聚拢起全部的精神来迎接她亲爱的儿子，甚至坐着轮椅到儿子的住处巡视，怕他住的有任何的不舒服；还精神抖擞地参加了酒店的亲戚聚会。要知道这对于她的体力来说几乎是不可能的事情。儿子在的时候，她始终用幸福、满足的眼神追随着儿子的一举一动。她和我说，她最大的愿望就是躺在床上，儿子坐在床边，手握着手和她轻轻地聊天。就他们两个人。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;而对于儿子取得的辉煌成就，她并没有表现出特别的激动。别人对她说：“你真了不起，生出这么棒的一个儿子，一个伟大的数学家”。她说，我只要看到我的儿子，看到他健康快乐就够了，别的我不关心。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;有一天，她坚持唤回了在外忙得不可开交的儿子，在家里摆上爸爸的遗像，插上三柱香，她和儿子坐在一起，握着儿子的手，絮絮叨叨地给他讲那些年的辛苦和思念。她告诉自己的丈夫，儿子终于回来了！终于…… &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;她说，我安心了！ &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;等儿子回美国后，妈妈用全身力量强打起的精神终于散了，勉强支撑了不久，她就住进了医院，与病魔挣扎了2个多月后，2013年年底，妈妈走了！&lt;/strong&gt;  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;那也是个夜半时分。妈妈平静、安详地离开了。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在告别妈妈的时候，有老朋友感叹说，以前我们总说你妈妈是有福气的人，还真是这样，她终于在有生之年，等到了日思夜盼的儿子。 &lt;/span&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.8631123919308358&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkwOayrDhTlDh3Emhn9EPDFCXs6vuwWAzic1NY4gkicMNRMBnNiakFicfQbgYC2wWkicbNIgd1z1BtVY8ag/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;694&quot;/&gt;&lt;/p&gt;

</description>
<pubDate>Thu, 15 Feb 2018 14:02:04 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/AcjxKluqPO</dc:identifier>
</item>
<item>
<title>陶哲轩，其人其事</title>
<link>http://www.jintiankansha.me/t/NTxrI3Pt4Q</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/NTxrI3Pt4Q</guid>
<description>&lt;p&gt;&lt;span&gt;14岁时正式进入弗林德斯大学就读；&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;16岁获得该校荣誉理科学位，仅一年后就取得了硕士学位；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17岁进入普林斯顿大学就读；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;21岁获得该校博士学位；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;24岁被加利福尼亚大学洛杉矶分校聘为正教授，成为加利福尼亚大学洛杉矶分校有史以来最年轻的正教授；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2006年夏，获得麦克阿瑟基金（MacArthur Foundation）天才奖和数学界的诺贝尔奖“菲尔兹”奖（继丘成桐之后获此殊荣的第二位华人）；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2008年获得美国国家科学基金会（NSF）的艾伦沃特曼奖（Alan T. Waterman Award）；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;IQ达到230，远超爱因斯坦、牛顿、霍金。被公认应该是史上智商最高、最聪明的人物。&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 12 Feb 2018 17:19:45 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/NTxrI3Pt4Q</dc:identifier>
</item>
<item>
<title>神经网络和深度学习简史（全）</title>
<link>http://www.jintiankansha.me/t/GSJsyHC1Yt</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/GSJsyHC1Yt</guid>
<description>&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;&lt;span&gt;如今，深度学习浪潮拍打计算机语言的海岸已有好几年，但是，2015年似乎才是这场海啸全力冲击自然语言处理（NLP）会议的一年。——Dr. Christopher D. Manning, Dec 2015&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;整个研究领域的成熟方法已经迅速被新发现超越，这句话听起来有些夸大其词，就像是说它被「海啸」袭击了一样。但是，这种灾难性的形容的确可以用来描述深度学习在过去几年中的异军突起——显著改善人们对解决人工智能最难问题方法的驾驭能力，吸引工业巨人（比如谷歌等）的大量投资，研究论文的指数式增长（以及机器学习的研究生生源上升）。在听了数节机器学习课堂，甚至在本科研究中使用它以后，我不禁好奇：这个新的「深度学习」会不会是一个幻想，抑或上世纪80年代已经研发出来的「人工智能神经网络」扩大版？让我告诉你，说来话长——这不仅仅是一个有关神经网络的故事，也不仅仅是一个有关一系列研究突破的故事，这些突破让深度学习变得比「大型神经网络」更加有趣，而是一个有关几位不放弃的研究员如何熬过黑暗数十年，直至拯救神经网络，实现深度学习梦想的故事。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiaFYzxDHNZ8Zzq4Xy9dQM6T7ZNNYcH9exUnpeAb77IG178cTDDBSL3ibg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;线性回归&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;br /&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;首先简单介绍一下机器学习是什么。从二维图像上取一些点，尽可能绘出一条拟合这些点的直线。你刚才做的就是从几对输入值（x）和输出值（y）的实例中概括出一个一般函数，任何输入值都会有一个对应的输出值。这叫做线性回归，一个有着两百年历史从一些输入输出对组中推断出一般函数的技巧。这就是它很棒的原因：很多函数难以给出明确的方程表达，但是，却很容易在现实世界搜集到输入和输出值实例——比如，将说出来的词的音频作为输入，词本身作为输出的映射函数。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;线性回归对于解决语音识别这个问题来说有点太无用，但是，它所做的基本上就是监督式机器学习：给定训练样本，「学习」一个函数，每一个样本数据就是需要学习的函数的输入输出数据（无监督学习，稍后在再叙）。尤其是，机器学习应该推导出一个函数，它能够很好地泛化到不在训练集中的输入值上，既然我们真的能将它运用到尚未有输出的输入中。例如，谷歌的语音识别技术由拥有大量训练集的机器学习驱动，但是，它的训练集也不可能大到包含你手机所有语音输入。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;泛化能力机制如此重要，以至于总会有一套测试数据组（更多的输入值与输出值样本）这套数据组并不包括在训练组当中。通过观察有多少个正确计算出输入值所对应的输出值的样本，这套单独数据组可以用来估测机器学习技术有效性。概括化的克星是过度拟合——学习一个对于训练集有效但是却在测试数据组中表现很差的函数。既然机器学习研究者们需要用来比较方法有效性的手段，随着时间的推移，标准训练数据组以及测试组可被用来评估机器学习算法。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;好了，定义谈得足够多了。重点是——我们绘制线条的联系只是一个非常简单的监督机器学习例子：要点在于训练集（X为输入，Y为输出），线条是近似函数，用这条线来为任何没有包含在训练集数据里的X值（输入值）找到相应的Y值（输出值）。别担心，接下来的历史就不会这么干巴巴了。让我们继续吧。&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;&lt;strong&gt;虚假承诺的荒唐&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;显然这里话题是神经网络，那我们前言里为何要扯线性回归呢？呃, 事实上线性回归和机器学习一开始的方法构想,弗兰克· 罗森布拉特(Frank Rosenblatt)的感知机, 有些许相似性。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.3015625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCia0L9y4KSGcicaibIwqDF2NciaG7SCqicbDWj32hrYOC4FVI5L9tHRKgOnnQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Perceptron&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;心理学家Rosenblatt构想了感知机，它作为简化的数学模型解释大脑神经元如何工作：它取一组二进制输入值（附近的神经元），将每个输入值乘以一个连续值权重（每个附近神经元的突触强度），并设立一个阈值，如果这些加权输入值的和超过这个阈值，就输出1，否则输出0（同理于神经元是否放电）。对于感知机，绝大多数输入值不是一些数据，就是别的感知机的输出值。但有一个额外的细节：这些感知机有一个特殊的，输入值为1的，「偏置」输入，因为我们能补偿加权和，它基本上确保了更多的函数在同样的输入值下是可计算的。这一关于神经元的模型是建立在沃伦·麦卡洛克(Warren McCulloch)和沃尔特·皮兹(Walter Pitts)工作上的。他们曾表明，把二进制输入值加起来，并在和大于一个阈值时输出1，否则输出0的神经元模型，可以模拟基本的或/与/非逻辑函数。这在人工智能的早期时代可不得了——当时的主流思想是,计算机能够做正式的逻辑推理将本质上解决人工智能问题。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5703125&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiaGdhnv39m6VFN4YfWAdGsTlyELa7sGnjMYMjbpJGiaKC6ZpOfht5cIRA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;另一个图表，显示出生物学上的灵感。激活函数就是人们当前说的非线性函数，它作用于输入值的加权和以产生人工神经元的输出值——在罗森布拉特的感知机情况下，这个函数就是输出一个阈值操作&lt;br data-filtered=&quot;filtered&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;然而，麦卡洛克-皮兹模型缺乏一个对AI而言至关重要的学习机制。这就是感知机更出色的地方所在——罗森布拉特受到唐纳德·赫布(Donald Hebb) 基础性工作的启发，想出一个让这种人工神经元学习的办法。赫布提出了一个出人意料并影响深远的想法，称知识和学习发生在大脑主要是通过神经元间突触的形成与变化，简要表述为赫布法则：&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;span&gt;当细胞A的轴突足以接近以激发细胞B，并反复持续地对细胞B放电，一些生长过程或代谢变化将发生在某一个或这两个细胞内，以致A作为对B放电的细胞中的一个，效率增加。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;感知机并没有完全遵循这个想法，但通过调输入值的权重，可以有一个非常简单直观的学习方案：给定一个有输入输出实例的训练集，感知机应该「学习」一个函数：对每个例子，若感知机的输出值比实例低太多，则增加它的权重，否则若设比实例高太多，则减少它的权重。更正式一点儿的该算法如下:&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot; list-paddingleft-2&quot; readability=&quot;2&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;从感知机有随机的权重和一个训练集开始。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;对于训练集中一个实例的输入值，计算感知机的输出值。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;span&gt;如若感知机的输出值和实例中默认正确的输出值不同：(1)若输出值应该为0但实际为1，减少输入值是1的例子的权重。(2)若输出值应该为1但实际为0，增加输入值是1的例子的权重。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;对于训练集中下一个例子做同样的事，重复步骤2-4直到感知机不再出错。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;&lt;span&gt;这个过程很简单，产生了一个简单的结果：一个输入线性函数（加权和），正如线性回归被非线性激活函数「压扁」了一样（对带权重求和设定阈值的行为）。当函数的输出值是一个有限集时（例如逻辑函数，它只有两个输出值True/1 和 False/0），给带权重的和设置阈值是没问题的，所以问题实际上不在于要对任何输入数据集生成一个数值上连续的输出（即回归类问题），而在于对输入数据做好合适的标签（分类问题）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.2307692307692308&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiawjACl91rSOR9NsZBiaBgkwKU1lrS3vMf2ofDPybicZItluAfRILaGcgQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;312&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;康奈尔航天实验室的Mark I 感知机，第一台感知机的硬件 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;罗森布拉特用定制硬件的方法实现了感知机的想法（在花哨的编程语言被广泛使用之前），展示出它可以用来学习对20×20像素输入中的简单形状进行正确分类。自此，机器学习问世了——建造了一台可以从已知的输入输出对中得出近似函数的计算机。在这个例子中，它只学习了一个小玩具般的函数，但是从中不难想象出有用的应用，例如将人类乱糟糟的手写字转换为机器可读的文本。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;很重要的是，这种方法还可以用在多个输出值的函数中，或具有多个类别的分类任务。这对一台感知机来说是不可能完成的，因为它只有一个输出，但是，多输出函数能用位于同一层的多个感知机来学习，每个感知机接收到同一个输入，但分别负责函数的不同输出。实际上，神经网络（准确的说应该是「人工神经网络（ANN，Artificial Neural Networks）」）就是多层感知机（今天感知机通常被称为神经元）而已，只不过在这个阶段，只有一层——输出层。所以，神经网络的典型应用例子就是分辨手写数字。输入是图像的像素，有10个输出神经元，每一个分别对应着10个可能的数字。在这个案例中，10个神经元中，只有1个输出1，权值最高的和被看做是正确的输出，而其他的则输出0。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.0076923076923077&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiae8VqYibQiadiaiaic7TGGZs9TAnAX5XiaMufPo3yZuQtmZU38GeWlqpkJrPQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;260&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;多层输出的神经网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;也可以想象一个与感知机不同的人工神经网络。例如，阈值激活函数并不是必要的； 1960年，Bernard Widrow和Tedd Hoff很快开始探索一种方法——采用适应性的「自适应（ADALINE）」神经元来输出权值的输入，这种神经元使用化学「 存储电阻器」，并展示了这种「自适应线性神经元」能够在电路中成为「 存储电阻器」的一部分（存储电阻器是带有存储的电阻）。他们还展示了，不用阈值激活函数，在数学上很美，因为神经元的学习机制是基于将错误最小化的微积分，而微积分我们都很熟悉了。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;如果我们多思考一下 「自适应（ADALINE）」，就会有进一步的洞见：为大量输入找到一组权重真的只是一种线性回归。再一次，就像用线性回归一样，这也不足以解决诸如语音识别或计算机视觉这样的人工智能难题。McCullough，Pitts和罗森布拉特真正感到兴奋的是联结主义（Connectionism）这个宽泛的想法：如此简单计算机单元构成的网络，其功能会大很多而且可以解决人工智能难题。而且罗森布拉特说的和（坦白说很可笑的）《纽约时报》这段引文的意思差不多：&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;span&gt;海军披露了一台尚处初期的电子计算机，期待这台电子计算机能行走，谈话，看和写，自己复制出自身存在意识…罗森布拉特博士，康奈尔航空实验室的一位心理学家说，感知机能作为机械太空探险者被发射到行星上。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;这种谈话无疑会惹恼人工领域的其他研究人员，其中有许多研究人员都在专注于这样的研究方法，它们以带有具体规则（这些规则遵循逻辑数学法则）的符号操作为基础。MIT人工智能实验室创始人Marvin Minsky和Seymour Paper就是对这一炒作持怀疑态度研究人员中的两位，1969年，他们在一本开创性著作中表达了这种质疑，书中严谨分析了感知机的局限性，书名很贴切，叫《感知机》。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;他们分析中，最被广为讨论的内容就是对感知机限制的说明，例如，他们不能学习简单的布尔函数XOR，因为它不能进行线性分离。虽然此处历史模糊，但是，人们普遍认为这本书对人工智能步入第一个冬天起到了推波助澜的作用——大肆炒作之后，人工智能进入泡沫幻灭期，相关资助和出版都遭冻结。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.4725490196078431&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCia4MvXNK2YYT5GNJmofZO7vQza4kOcIutP6pbxrsl559fUgdv4M0bQMw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;510&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;感知机局限性的视觉化。找到一个线性函数，输入X，Y时可以正确地输出+或-，就是在2D图表上画一条从+中分离出-的线；很显然，就第三幅图显示的情况来看，这是不可能的&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;人工智能冬天的复苏&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;因此，情况对神经网络不利。但是，为什么？他们的想法毕竟是想将一连串简单的数学神经元结合在一起，完成一些复杂任务，而不是使用单个神经元。换句话说，并不是只有一个输出层，将一个输入任意传输到多个神经元（所谓的隐藏层，因为他们的输出会作为另一隐藏层或神经元输出层的输入）。只有输出层的输出是「可见」的——亦即神经网络的答案——但是，所有依靠隐藏层完成的间接计算可以处理复杂得多的问题，这是单层结构望尘莫及的。 &lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.490625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiaia6c1h8AAPavf6Iwibv9ct0S6exaic9YibrbibM0V8nWpCSIgiagbRhC2Qdw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;有两个隐藏层的神经网络 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;言简意赅地说，多个隐藏层是件好事，原因在于隐藏层可以找到数据内在特点，后续层可以在这些特点（而不是嘈杂庞大的原始数据）基础上进行操作。以图片中的面部识别这一非常常见的神经网络任务为例，第一个隐藏层可以获得图片的原始像素值，以及线、圆和椭圆等信息。接下来的层可以获得这些线、圆和椭圆等的位置信息，并且通过这些来定位人脸的位置——处理起来简单多了！而且人们基本上也都明白这一点。事实上，直到最近，机器学习技术都没有普遍直接用于原始数据输入，比如图像和音频。相反，机器学习被用于经过特征提取后的数据——也就是说，为了让学习更简单，机器学习被用在预处理的数据上，一些更加有用的特征，比如角度，形状早已被从中提取出来。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.3578125&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiaaazQicT9IBR98N7VtWqPvicfQGZLX01RzKhAueZEuMmYXAgwWjpq78EA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;传统的特征的手工提取过程的视觉化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;因此，注意到这一点很重要：Minsky和Paper关于感知机的分析不仅仅表明不可能用单个感知机来计算XOR，而且特别指出需要多层感知机——亦即现在所谓的多层神经网络——才可以完成这一任务，而且罗森布拉特的学习算法对多层并不管用。那是一个真正的问题：之前针对感知机概括出的简单学习规则并不是适用于多层结构。想知道原因？让我们再来回顾一下单层结构感知机如何学习计算一些函数：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot; list-paddingleft-2&quot; readability=&quot;1.5&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;和函数输出数量相等的感知机会以小的初始权值开始（仅为输入函数的倍数）&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;选取训练集中的一个例子作为输入，计算感知机的输出&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;对于每一个感知机，如果其计算结果和该例子的结果不匹配，调整初始权值&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;继续采用训练集中的下一个例子，重复过程2到4次，直到感知机不再犯错。&lt;/span&gt;&lt;/p&gt;

&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;&lt;span&gt;这一规则并不适用多层结构的原因应该很直观清楚了：选取训练集中的例子进行训练时，我们只能对最终的输出层的输出结果进行校正，但是，对于多层结构来说，我们该如何调整最终输出层之前的层结构权值呢？答案（尽管需要花时间来推导）又一次需要依赖古老的微积分：链式法则。这里有一个重要现实：神经网络的神经元和感知机并不完全相同，但是，可用一个激活函数来计算输出，该函数仍然是非线性的，但是可微分，和Adaline神经元一样；该导数不仅可以用于调整权值，减少误差，链式法则也可用于计算前一层所有神经元导数，因此，调整它们权重的方式也是可知的。说得更简单些：我们可以利用微积分将一些导致输出层任何训练集误差的原因分配给前一隐藏层的每个神经元，如果还有另外一层隐藏层，我们可以将这些原因再做分配，以此类推——我们在反向传播这些误差。而且，如果修改了神经网络（包括那些隐藏层）任一权重值，我们还可以找出误差会有多大变化，通过优化技巧（时间长，典型的随机梯度下降）找出最小化误差的最佳权值。&lt;br data-filtered=&quot;filtered&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.4265625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiaUzNePLdDTEPfEDjLgC4Pv3wE6tLDRgDibiaFIYXyUGFDL6MmO27Xkcibg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;反向传播的基本思想 &lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;反向传播由上世纪60年代早期多位研究人员提出，70年代，由Seppo Linnainmaa引入电脑运行，但是，Paul Werbos在1974年的博士毕业论文中深刻分析了将之用于神经网络方面的可能性，成为美国第一位提出可以将其用于神经网络的研究人员。有趣的是，他从模拟人类思维的研究工作中并没有获得多少启发，在这个案例中，弗洛伊德心理学理论启发了他，正如他自己叙述：&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;9&quot;&gt;
&lt;p&gt;&lt;span&gt;1968年，我提出我们可以多少模仿弗洛伊德的概念——信度指派的反向流动（ a backwards flow of credit assignment,），指代从神经元到神经元的反向流动…我解释过结合使用了直觉、实例和普通链式法则的反向计算，虽然它正是将弗洛伊德以前在心理动力学理论中提出的概念运用到数学领域中！&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;尽管解决了如何训练多层神经网络的问题，在写作自己的博士学位论文时也意识到了这一点，但是，Werbos没有发表将BP算法用于神经网络这方面的研究，直到1982年人工智能冬天引发了寒蝉效应。实际上，Werbos认为，这种研究进路对解决感知机问题是有意义的，但是，这个圈子大体已经失去解决那些问题的信念。&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;&lt;span&gt;Minsky的书最著名的观点有几个：（1）我们需要用MLPs[多层感知机，多层神经网络的另一种说法）来代表简单的非线性函数，比如XOR 映射；而且（2）世界上没人发现可以将MLPs训练得够好，以至于可以学会这么简单的函数的方法。Minsky的书让世上绝大多数人相信，神经网络是最糟糕的异端，死路一条。Widrow已经强调，这种压垮早期『感知机』人工智能学派的悲观主义不应怪在Minsky的头上。他只是总结了几百位谨慎研究人员的经验而已，他们尝试找出训练MLPs的办法，却徒劳无功。也曾有过希望，比如Rosenblatt所谓的backpropagation（这和我们现在说的 backpropagation并不完全相同！），而且Amari也简短表示，我们应该考虑将最小二乘（也是简单线性回归的基础）作为训练神经网络的一种方式（但没有讨论如何求导，还警告说他对这个方法不抱太大期望）。但是，当时的悲观主义开始变得致命。上世纪七十年代早期，我确实在MIT采访过Minsky。我建议我们合著一篇文章，证明MLPs实际上能够克服早期出现的问题…但是，Minsky并无兴趣（14）。事实上，当时的MIT，哈佛以及任何我能找到的研究机构，没人对此有兴趣。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;我肯定不能打保票，但是，直到十年后，也就是1986年，这一研究进路才开始在David Rumelhart, Geoffrey Hinton和Ronald Williams合著的《Learning representations by back-propagating errors》中流行开来，原因似乎就是缺少学术兴趣。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;尽管研究方法的发现不计其数（论文甚至清楚提道，David Parker 和 Yann LeCun是事先发现这一研究进路的两人），1986年的这篇文章却因其精确清晰的观点陈述而显得很突出。实际上，学机器学习的人很容易发现自己论文中的描述与教科书和课堂上解释概念方式本质上相同。&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;10&quot;&gt;
&lt;p&gt;&lt;span&gt;不幸的是，科学圈里几乎无人知道Werbo的研究。1982年，Parker重新发现了这个研究办法[39]并于1985年在M.I.T[40]上发表了一篇相关报道。就在Parker报道后不久，Rumelhart, Hinton和Williams [41], [42]也重新发现了这个方法， 他们最终成功地让这个方法家喻户晓，也主要归功于陈述观点的框架非常清晰。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;但是，这三位作者没有止步于介绍新学习算法，而是走得更远。同年，他们发表了更有深度的文章《Learning internal representations by error propagation》。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;文章特别谈到了Minsky在《感知机》中讨论过的问题。尽管这是过去学者的构想，但是，正是这个1986年提出的构想让人们广泛理解了应该如何训练多层神经网络解决复杂学习问题。而且神经网络也因此回来了！&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;&lt;strong&gt;神经网络获得视觉&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;随着训练多层神经网络的谜题被揭开，这个话题再一次变得空前热门，罗森布拉特的崇高雄心似乎也将得以实现。直到1989年另一个关键发现被公布，现在仍广为教科书及各大讲座引用。&lt;br data-filtered=&quot;filtered&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;span&gt;多层前馈神经网络是普适模拟器（ universal approximators）。」本质上，可以从数学证明多层结构使得神经网络能够在理论上执行任何函数表达，当然包括XOR（异或）问题。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;然而，这是数学，你可以在数学中畅想自己拥有无限内存和所需计算能力——反向传播可以让神经网络被用于世界任何角落吗？噢，当然。也是在1989年，Yann LeCunn在AT&amp;amp;T Bell实验室验证了一个反向传播在现实世界中的杰出应用，即「反向传播应用于手写邮编识别（Backpropagation Applied to Handwritten Zip Code Recognition）」。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;你或许会认为，让计算机能够正确理解手写数字并没有那么了不起，而且今天看来，这还会显得你太过大惊小怪，但事实上，在这个应用公开发布之前，人类书写混乱，笔画也不连贯，对计算机整齐划一的思维方式构成了巨大挑战。这篇研究使用了美国邮政的大量数据资料，结果证明神经网络完全能够胜任识别任务。更重要的是，这份研究首次强调了超越普通（plain）反向传播 、迈向现代深度学习这一关键转变的实践需求。&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;9&quot;&gt;
&lt;p&gt;&lt;span&gt;传统的视觉模式识别工作已经证明，抽取局部特征并且将它们结合起来组成更高级的特征是有优势的。通过迫使隐藏单元结合局部信息来源，很容易将这样的知识搭建成网络。一个事物的本质特征可以出现在输入图片的不同位置。因此，拥有一套特征探测器，可以探测到位于输入环节任何地方的某个具体特征实例，非常明智。既然一个特征的精准定位于分类无关，那么，我们可以在处理过程中适当舍弃一些位置信息。不过，近似的位置信息必须被保留，从而允许下面网络层能够探测到更加高级更加复杂的特征。（Fukushima1980，Mozer,1987）&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.3&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiaJB8Es8RcuoTuxiaN2EAXeGjl9kEETvxZel2LiaQvLOyheJQFTicajoQLQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;440&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;         一个神经网络工作原理的可视化过程&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;或者，更具体的：神经网络的第一个隐层是卷积层——不同于传统网络层，每个神经元对应的一个图片像素都相应有一个不同的权值（40*60=2400个权值），神经元只有很少一部分权值（5*5=25）以同样的大小应用于图像的一小个完整子空间。所以，比如替换了用四种不同的神经元来学习整个输入图片4个角的45度对角线探测，一个单独的神经元能通过在图片的子空间上学习探测45度对角线，并且照着这样的方法对整张图片进行学习。每层的第一道程序都以相类似的方式进行，但是，接收的是在前一隐藏层找到的「局部」特征位置而不是图片像素值，而且，既然它们正在结合有关日益增大的图片子集的信息，那么，它们也能「看到」其余更大的图片部分。最后，倒数的两个网络层利用了前面卷积抽象出来的更加高级更加明显的特征来判断输入的图像究竟该归类到哪里。这个在1989年的论文里提出的方法继续成为举国采用的支票读取系统的基础，正如LeCun在如下小视频中演示的：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.743161094224924&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiaw7bj0BZM41QgLXgSKott9ZrU64dx39II5n5PKousn2oMPuqrXibo03w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;658&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这很管用，为什么？原因很直观，如果数学表述上不是那么清楚的话：没有这些约束条件，网络就必须学习同样的简单事情（比如，检测45°角的直线和小圆圈等），要花大把时间学习图像的每一部分。但是，有些约束条件，每一个简单特征只需要一个神经元来学习——而且，由于整体权值大量减少，整个过程完成起来更快。而且，既然这些特征的像素确切位置无关紧要，那么，基本上可以跳过图像相邻子集——子集抽样，一种共享池手段（a type of pooling）——当应用权值时，进一步减少了训练时间。多加了这两层——（卷积层和汇集层）——是卷积神经网络（CNNs/ConvNets）和普通旧神经网络的主要区别。&lt;/span&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.4370257966616085&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiajVQEkuGELZypT7PApF1FpfWwSrjb6VbkdzIibS2iblibP29uPGHwibF1icw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;659&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;卷积神经网络（CNN）的操作过程&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  那时，卷积的思想被称作「权值共享」，也在1986年Rumelhart、Hinton和Williams关于反向传播的延伸分析中得到了切实讨论。显然，Minsky和Papert在1969年《感知机》中的分析完全可以提出激发这一研究想法的问题。但是，和之前一样，其他人已经独立地对其进行了研究——比如，Kunihiko Fukushima在1980年提出的 Neurocognitron。而且，和之前一样，这一思想从大脑研究汲取了灵感：&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;&lt;span&gt;根据Hubel和Wiesel的层级模型，视觉皮层中的神经网络具有一个层级结构：LGB（外侧膝状体）→样品细胞→复杂细胞→低阶超复杂细胞-&amp;gt;高阶超复杂细胞。低阶超复杂细胞和高阶超复杂细胞之间的神经网络具有一个和简单细胞与复杂细胞之间的网络类似的结构。在这种层状结构中，较高级别的细胞通常会有这样的倾向，即对刺激模式的更复杂的特征进行选择性响应，同时也具有一个更大的接收域，而且对刺激模式位置的移动更不敏感。因此，在我们的模型中就引入了类似于层级模型的结构。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;LeCun也在贝尔实验室继续支持卷积神经网络，其相应的研究成果也最终在上世纪90年代中期成功应用于支票读取——他的谈话和采访通常都介绍了这一事实：「在上世纪90年代后期，这些系统当中的一个读取了全美大约10%到20%的支票。」&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;神经网络进入无监督学习时期&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;将死记硬背，完全无趣的支票读取工作自动化，就是机器学习大展拳脚的例子。也许有一个预测性小的应用？ 压缩。即指找到一种更小体量的数据表示模式，并从其可以恢复数据原有的表示形态，通过机器学习找到的压缩方法有可能会超越所有现有的压缩模式。当然，意思是在一些数据中找到一个更小的数据表征，原始数据可以从中加以重构。学会压缩这一方案远胜于常规压缩算法，在这种情况下，学习算法可以找到在常规压缩算法下可能错失的数据特征。而且，这也很容易做到——仅用训练带有一个小隐藏层的神经网络就可以对输入进行输出。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.9066265060240963&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiawezMvpAzU1IxAlOR9ic7EHkCAUKdXxzRqHNso48qgKnS6zFopQyniaPA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;332&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;strong&gt;&lt;span&gt;自编码神经网络&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;这是一个自编码神经网络，也是一种学习压缩的方法——有效地将数据转换为压缩格式，并且自动返回到本身。我们可以看到，输出层会计算其输出结果。由于隐藏层的输出比输入层少，因此，隐藏层的输出是输入数据的一个压缩表达，可以在输出层进行重建。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5420054200542005&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiawn3t7l1zspc6oE1oazXnicWQttjJy0N2dBg3yUfCj0GQSyiaVHgYibXAQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;369&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;更明确地了解自编码压缩&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;注意一件奇妙的事情：我们训练所需的唯一东西就是一些输入数据。这与监督式机器学习的要求形成鲜明的对比，监督式机器学习需要的训练集是输入-输出对（标记数据），来近似地生成能从这些输入得到对应输出的函数。确实，自编码器并不是一种监督式学习；它们实际上是一种非监督式学习，只需要一组输入数据（未标记的数据），目的是找到这些数据中某些隐藏的结构。换句话说，非监督式学习对函数的近似程度不如它从输入数据中生成另一个有用的表征那么多。这样一来，这个表征比原始数据能重构的表征更小，但它也能被用来寻找相似的数据组（聚类）或者潜在变量的其他推论（某些从数据看来已知存在但数值未知的方面）。&lt;br data-filtered=&quot;filtered&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.403125&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiaNTHRyzjPcgt6faN2xKxNBiapG1QkTy9Y99j67ytvvXDNklsblUXII0A/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;聚类，一种很常用的非监督式学习应用&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;在反向传播算法发现之前和之后，神经网络都还有其他的非监督式应用，最著名的是自组织映射神经网络（SOM，Self Organizing Maps）和自适应共振理论（ART，Adapative Resonance Theory）。SOM能生成低维度的数据表征，便于可视化，而ART能够在不被告知正确分类的情况下，学习对任意输入数据进行分类。如果你想一想就会发现，从未标记数据中能学到很多东西是符合直觉的。假设你有一个数据集，其中有一堆手写数字的数据集，并没有标记每张图片对应着哪个数字。那么，如果一张图片上有数据集中的某个数字，那它看起来与其他大多数拥有同样数字的图片很相似，所以，尽管计算机可能并不知道这些图片对应着哪个数字，但它应该能够发现它们都对应着同一个数字。这样，模式识别就是大多数机器学习要解决的任务，也有可能是人脑强大能力的基础。但是，让我们不要偏离我们的深度学习之旅，回到自编码器上。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.615625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiaGls6eWqhOv7lKLKfUiaj4sibKlId6X8LKrNabQ0PWfm3tlK2Yu40uwjA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;自组织映射神经网络：将输入的一个大向量映射到一个神经输出的网格中，在其中，每个输出都是一个聚类。相邻的神经元表示同样的聚类。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;正如权重共享一样，关于自编码器最早的讨论是在前面提到过的1986年的反向传播分析中所进行。有了权重共享，它在接下来几年中的更多研究中重新浮出了水面，包括Hinton自己。这篇论文，有一个有趣的标题：《自编码器，最小描述长度和亥姆霍兹自由能》（Autoencoders, Minimum Description Length, and Helmholts Free Energy），提出「最自然的非监督式学习方法就是使用一个定义概率分布而不是可观测向量的模型」，并使用一个神经网络来学习这种模型。所以，还有一件你能用神经网络来做的奇妙事：对概率分布进行近似。&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;神经网络迎来信念网络&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;事实上，在成为1986年讨论反向传播学习算法这篇有重大影响力论文的合作者之前，Hinton在研究一种神经网络方法，可以学习1985年「 A Learning Algorithm for Boltzmann Machines」中的概率分布。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;玻尔兹曼机器就是类似神经网络的网络，并有着和感知器（Perceptrons）非常相似的单元，但该机器并不是根据输入和权重来计算输出，在给定相连单元值和权重的情况下，网络中的每个单元都能计算出自身概率，取得值为1或0。因此，这些单元都是随机的——它们依循的是概率分布而非一种已知的决定性方式。玻尔兹曼部分和概率分布有关，它需要考虑系统中粒子的状态，这些状态本身基于粒子的能量和系统本身的热力学温度。这一分布不仅决定了玻尔兹曼机器的数学方法，也决定了其推理方法——网络中的单元本身拥有能量和状况，学习是由最小化系统能量和热力学直接刺激完成的。虽然不太直观，但这种基于能量的推理演绎实际上恰是一种基于能量的模型实例，并能够适用于基于能量的学习理论框架，而很多学习算法都能用这样的框架进行表述。&lt;/span&gt;&lt;span&gt;&lt;br data-filtered=&quot;filtered&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5605095541401274&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCia8hxnQP91lSUUaqQgCpbHiclhzxv7TRpZKZmxiafXYFcpQcYwJpjDammw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;628&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;一个简单的信念，或者说贝叶斯网络——玻尔兹曼机器基本上就是如此，但有着非直接/对称联系和可训练式权重，能够学习特定模式下的概率。&lt;br data-filtered=&quot;filtered&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;回到玻尔兹曼机器。当这样的单元一起置于网络中，就形成了一张图表，而数据图形模型也是如此。本质上，它们能够做到一些非常类似普通神经网络的事：某些隐藏单元在给定某些代表可见变量的可见单元的已知值（输入——图像像素，文本字符等）后，计算某些隐藏变量的概率（输出——数据分类或数据特征）。以给数字图像分类为例，隐藏变量就是实际的数字值，可见变量是图像的像素；给定数字图像「1」作为输入，可见单元的值就可知，隐藏单元给图像代表「1」的概率进行建模，而这应该会有较高的输出概率。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;玻尔兹曼机器实例。每一行都有相关的权重，就像神经网络一样。注意，这里没有分层——所有事都可能跟所有事相关联。我们会在后文讨论这样一种变异的神经网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;因此，对于分类任务，现在有一种计算每种类别概率的好方法了。这非常类似正常分类神经网络实际计算输出的过程，但这些网络有另一个小花招：它们能够得出看似合理的输入数据。这是从相关的概率等式中得来的——网络不只是会学习计算已知可见变量值时的隐藏变量值概率，还能够由已知隐藏变量值反推可见变量值概率。所以，如果我们想得出一幅「1」数字图像，这些跟像素变量相关的单元就知道需要输出概率1，而图像就能够根据概率得出——这些网络会再创建图像模型。虽然可能能够实现目标非常类似普通神经网络的监督式学习，但学习一个好的生成模型的非监督式学习任务——概率性地学习某些数据的隐藏结构——是这些网络普遍所需要的。这些大部分都不是小说，学习算法确实存在，而使其成为可能的特殊公式，正如其论文本身所描述的：&lt;br data-filtered=&quot;filtered&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;&lt;span&gt;或许，玻尔兹曼机器公式最有趣的方面在于它能够引导出一种（与领域无关的）一般性学习算法，这种算法会以整个网络发展出的一种内部模型（这个模型能够捕获其周围环境的基础结构）的方式修改单元之间的联系强度。在寻找这样一个算法的路上，有一段长时间失败的历史（Newell，1982），而很多人（特别是人工智能领域的人）现在相信不存在这样的算法。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;我们就不展开算法的全部细节了，就列出一些亮点：这是最大似然算法的变体，这简单意味着它追求与已知正确值匹配的网络可见单元值（visible unit values）概率的最大化。同时计算每个单元的实际最有可能值 ，计算要求太高，因此，训练吉布斯采样（training Gibbs Sampling）——以随机的单元值网络作为开始，在给定单元连接值的情况下，不断迭代重新给单元赋值——被用来给出一些实际已知值。当使用训练集学习时，设置可见单位值（ visible units）从而能够得到当前训练样本的值，这样就通过抽样得到了隐藏单位值。一旦抽取到了一些真实值，我们就可以采取类似反向传播的办法——针对每个权重值求偏导数，然后估算出如何调整权重来增加整个网络做出正确预测的概率。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;和神经网络一样，算法既可以在监督（知道隐藏单元值）也可以在无监督方式下完成。尽管这一算法被证明有效（尤其是在面对自编码神经网络解决的「编码」问题时），但很快就看出不是特别有效。Redford M. Neal1992年的论文《Connectionist learning of belief networks》论证了需要一种更快的方法，他说：「这些能力使得玻耳兹曼机在许多应用中都非常有吸引力——要不是学习过程通常被认为是慢的要命。」因此，Neal引入了类似信念网络的想法，本质上就像玻耳兹曼机控制、发送连接（所以又有了层次，就像我们之前看过的神经网络一样，而不像上面的玻耳兹曼机控制机概念）。跳出了讨厌的概率数学，这一变化使得网络能以一种更快的学习算法得到训练。洒水器和雨水那一层上面可以视为有一个信念网络——这一术语非常严谨，因为这种基于概率的模型，除了和机器学习领域有着联系，和数学中的概率领域也有着密切的关联。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;尽管这种方法比玻尔兹曼机进步，但还是太慢了，正确计算变量间的概率关系的数学需求计算量太大了，而且还没啥简化技巧。Hinton、Neal和其他两位合作者很快在1995年的论文《 The wake-sleep algorithm for unsupervised neural networks》中提出了一些新技巧。这次他们又搞出一个和上个信念网络有些不一样的网络，现在被叫做「亥姆霍兹机」。再次抛开细节不谈，核心的想法就是对隐含变量的估算和对已知变量的逆转生成计算采取两套不同的权重，前者叫做recognition weights，后者叫做generative weights，保留了Neal's信念网络的有方向的特性。这样一来，当用于玻尔兹曼机的那些监督和无监督学习问题时，训练就快得多。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;最终，信念网络的训练多少会快些！尽管没那么大的影响力，对信念网络的无监督学习而言，这一算法改进是非常重要的进步，堪比十年前反向传播的突破。不过，目前为止，新的机器学习方法也开始涌现，人们也与开始质疑神经网络，因为大部分的想法似乎基于直觉，而且因为计算机仍旧很难满足它们的计算需求，人工智能寒冬将在几年内到来。&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;神经网络做决定&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;神经网络运用于无监督学习的发现之旅结束后，让我们也快速了解一下它们如何被用于机器学习的第三个分支领域：强化学习。正规解释强化学习需要很多数学符号，不过，它也有一个很容易加以非正式描述的目标：学会做出好决定。给定一些理论代理（比如，一个小软件），让代理能够根据当前状态做出行动，每个采取行动会获得一些奖励，而且每个行动也意图最大化长期效用。&lt;br data-filtered=&quot;filtered&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;因此，尽管监督学习确切告诉了学习算法它应该学习的用以输出的内容，但是，强化学习会过一段时间提供奖励，作为一个好决定的副产品，不会直接告诉算法应该选择的正确决定。从一开始，这就是一个非常抽象的决策模型——数目有限的状态，并且有一组已知的行动，每种状态下的奖励也是已知的。为了找到一组最优行动，编写出非常优雅的方程会因此变得简单，不过这很难用于解决真实问题——那些状态持续或者很难界定奖励的问题。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7231121281464531&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiaicd3glnJojMqJGASeRHicfaPib0TPgUjvtCYxNaolDMB5YN7A9RTNUviaw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;437&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;强化学习&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这就是神经网络流行起来的地方。机器学习大体上，特别是神经网络，很善于处理混乱的连续性数据 ，或者通过实例学习很难加以定义的函数。尽管分类是神经网络的饭碗，但是，神经网络足够普适（general），能用来解决许多类型的问题——比如，Bernard Widrow和Ted Hoff的Adaline后续衍生技术被用于电路环境下的自适应滤波器。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;因此，BP研究复苏之后，不久，人们就设计了利用神经网络进行强化学习的办法。早期例子之一就是解决一个简单却经典的问题：平衡运动着的平台上的棍子，各地控制课堂上学生熟知的倒立摆控制问题。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;__bg_gif&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiadqYruuHXfrMicZicWpl8kcSd9MvvyFS2YiazUdAic7bXjazKzEliaGINFqw/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;768&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;双摆控制问题——单摆问题进阶版本，是一个经典的控制和强化学习任务。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;因为有自适应滤波，这项研究就和电子工程领域密切相关，这一领域中，在神经网络出现之前的几十年当中，控制论已经成为一个主要的子领域。虽然该领域已经设计了很多通过直接分析解决问题的办法，也有一种通过学习解决更加复杂状态的办法，事实证明这一办法有用——1990年，「Identification and control of dynamical systems using neural networks」的7000次高被引就是证明。或许可以断定，另有一个独立于机器学习领域，其中，神经网络就是有用的机器人学。用于机器人学的早期神经网络例子之一就是来自CMU的NavLab，1989年的「Alvinn: An autonomous land vehicle in a neural network」：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;1. “NavLab 1984 - 1994”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;正如论文所讨论的，这一系统中的神经网络通过普通的监督学习学会使用传感器以及人类驾驶时记录下的驾驶数据来控制车辆。也有研究教会机器人专门使用强化学习，正如1993年博士论文「Reinforcement learning for robots using neural networks」所示例的。论文表明，机器人能学会一些动作，比如，沿着墙壁行走，或者在合理时间范围内通过门，考虑到之前倒立摆工作所需的长得不切实际的训练时间，这真是件好事。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这些发生在其他领域中的运用当然很酷，但是，当然多数强化学习和神经网络的研究发生在人工智能和机器学习范围内。而且，我们也在这一范围内取得了强化学习史上最重要的成绩之一：一个学习并成为西洋双陆棋世界级玩家的神经网络。研究人员用标准强化学习算法来训练这个被称为TD-Gammon的神经网络，它也是第一个证明强化学习能够在相对复杂任务中胜过人类的证据。而且，这是个特别的加强学习办法，同样的仅采用神经网络（没有加强学习）的系统，表现没这么好。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.55078125&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCia5J1ic40BlSp4wmAVo7umNibLLWHtfje58Rl294uI3maHWPwCOic90icYQg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;512&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;西洋双陆棋游戏中，掌握专家级别水平的神经网络&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;但是，正如之前已经看到，接下来也会在人工智能领域再次看到，研究进入死胡同。下一个要用TD-Gammnon办法解决的问题，Sebastian Thrun已经在1995年「Learning To Play the Game of Chess」中研究过了，结果不是很好..尽管神经网络表现不俗，肯定比一个初学者要好，但和很久以前实现的标准计算机程序GNU-Chess相比，要逊色得多。人工智能长期面临的另一个挑战——围棋，亦是如此。这样说吧，TD-Gammon 有点作弊了——它学会了精确评估位置，因此，无需对接下来的好多步做任何搜索，只用选择可以占据下一个最有利位置的招数。但是，在象棋游戏和围棋游戏里，这些游戏对人工智能而言是一个挑战，因为需要预估很多步，可能的行动组合如此之巨。而且，就算算法更聪明，当时的硬件又跟不上，Thrun称「NeuroChess不怎么样，因为它把大部分时间花在评估棋盘上了。计算大型神经网络函数耗时是评价优化线性评估函数（an optimized linear evaluation function），比如GNU-Chess，的两倍。」当时，计算机相对于神经网络需求的不足是一个很现实的问题，而且正如我们将要看到的，这不是唯一一个…&lt;/span&gt;&lt;span&gt;&lt;br data-filtered=&quot;filtered&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;神经网络变得呆头呆脑&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;尽管无监督学习和加强学习很简洁，监督学习仍然是我最喜欢的神经网络应用实例。诚然，学习数据的概率模型很酷，但是，通过反向传播解决实际问题更容易让人兴奋。我们已经看到了Yann Lecun成功解决了识别手写的问题（这一技术继续被全国用来扫描支票，而且后来的使用更多），另一项显而易见且相当重要的任务也在同时进行着：理解人类的语音。&lt;br data-filtered=&quot;filtered&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;和识别手写一样，理解人类的语音很难，同一个词根据表达的不同，意思也有很多变化。不过，还有额外的挑战：长序列的输入。你看，如果是图片，你就可以把字母从图片中切出来，然后，神经网络就能告诉你这个字母是啥，输入-输出模式。但语言就没那么容易了，把语音拆成字母完全不切实际，就算想要找出语音中的单词也没那么容易。而且你想啊，听到语境中的单词相比单个单词，要好理解一点吧！尽管输入-输出模式用来逐个处理图片相当有效，这并不适用于很长的信息，比如音频或文本。神经网络没有记忆赖以处理一个输入能影响后续的另一个输入的情况，但这恰恰是我们人类处理音频或者文本的方式——输入一串单词或者声音，而不是单独输入。要点是：要解决理解语音的问题，研究人员试图修改神经网络来处理一系列输入（就像语音中的那样）而不是批量输入（像图片中那样）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Alexander Waibel等人（还有Hinton）提出的解决方法之一，在1989年的「 Phoneme recognition using time-delay neural networks」中得到了介绍。这些时延神经网络和通常意义上的神经网络非常类似，除了每个神经元只处理一个输入子集，而且为不同类型的输入数据延迟配备了几套权重。易言之，针对一系列音频输入，一个音频的「移动窗口」被输入到神经网络，而且随着窗口移动，每个带有几套不同权重的神经元就会根据这段音频在窗口中位置，赋予相应的权重，用这种方法来处理音频。画张图就好理解了：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7756813417190775&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiaHX9qKf8B87bqGAmhG4icdWj0QmYP8dSaBvPHEhYPicAY9KQFKicwiaOAbQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;477&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;时延神经网络&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;从某种意义上来说，这和卷积神经网络差不多——每个单元一次只看一个输入子集，对每个小子集进行相同的运算，而不是一次性计算整个集合。不同之处在于，在卷积神经网络中不存在时间概念， 每个神经元的输入窗形成整个输入图像来计算出一个结果，而时延神经网络中有一系列的输入和输出。一个有趣的事实：据Hinton说，时延神经网络的理念启发了LeCun开发卷积神经网络。但是，好笑的是，积卷神经网络变得对图像处理至关重要，而在语音识别方面，时延神经网络则败北于另一种方法——递归神经网络(RNNs)。你看，目前为止讨论过的所有神经网络都是前归网络，这意味着某神经元的输出是下一层神经元的输入。但并不一定要这样，没有什么阻止我们勇敢的计算机科学家将最后一层的输出变成第一层的输入，或者将神经元的输出连接到神经元自身。将神经元回路接回神经网络，赋予神经网络记忆就被优雅地解决了。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.795&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiarLbryxm8YPd3vBxCaiaWLiaDpfstQaTh9oWYcxaicj103DLm5jW7MWbTQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;400&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;递归神经网络图。还记得之前的玻尔兹曼机吗？大吃一惊吧！那些是递归性神经网络。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;然而，这可没有那么容易。注意这个问题——如果反向传播需要依赖『正向传播』将输出层的错误反馈回来，那么，如果第一层往回连接到输出层，系统怎么工作？错误会继续传到第一层再传回到输出层，在神经网络中循环往复，无限次地。解决办法是，通过多重群组独立推导，通过时间进行反向传播。基本来说，就是将每个通过神经网络的回路做为另一个神经网络的输入，而且回路次数有限，通过这样的办法把递归神经网络铺开。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.33125&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiawHU1jD8KHXL93kiamjHZwAicfqpErIBiaD8XBiciazFKHNQ7hjIiccYyoxZg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;通过时间概念反向传播的直观图解。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  这个很简单的想法真的起作用了——训练递归神经网络是可能的。并且，有很多人探索出了RNN在语言识别领域的应用。但是，你可能也听说过其中的波折：这一方法效果并不是很好。为了找出原因，让我们来认识另一位深度学习的巨人：Yoshua Bengion。大约在1986年，他就开始进行语言识别方向的神经网络研究工作，也参与了许多使用ANN和RNN进行语言识别的学术论文，最后进入AT&amp;amp;T BELL实验室工作，Yann LeCun正好也在那里攻克CNN。 实际上，1995年，两位共同发表了文章「Convolutional Networks for Images, Speech, and Time-Series」，这是他们第一次合作，后来他们也进行了许多合作。但是，早在1993年，Bengio曾发表过「A Connectionist Approach to Speech Recognition」。其中，他对有效训练RNN的一般错误进行了归纳：&lt;br data-filtered=&quot;filtered&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;&lt;span&gt;尽管在许多例子中，递归网络能胜过静态网络，但是，优化训练起来也更有难度。我们的实验倾向于显示（递归神经网络）的参数调整往往收敛在亚优化的解里面，（这种解）只考虑了短效应影响因子而不计长效影响因子。例如，在所述实验中我们发现，RNN根本捕获不到单音素受到的简单时间约束…虽然这是一个消极的结果，但是，更好地理解这一问题可以帮助设计替代系统来训练神经网络，让它学会通过长效影响因子，将输出序列映射到输入序列（map input sequences to output sequences with long term dependencies ），比如，为了学习有限状态机，语法，以及其他语言相关的任务。既然基于梯度的方法显然不足以解决这类问题，我们要考虑其他最优办法，得出可以接受的结论，即使当判别函数（criterion function）并不平滑时。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;新的冬日黎明&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;因此，有一个问题。一个大问题。而且，基本而言，这个问题就是近来的一个巨大成就：反向传播。卷积神经网络在这里起到了非常重要的作用，因为反向传播在有着很多分层的一般神经网络中表现并不好。然而，深度学习的一个关键就是——很多分层，现在的系统大概有20左右的分层。但是，二十世纪八十年代后期，人们就发现，用反向传播来训练深度神经网络效果并不尽如人意，尤其是不如对较少层数的网络训练的结果。原因就是反向传播依赖于将输出层的错误找到并且连续地将错误原因归类到之前的各个分层。然而，在如此大量的层次下，这种数学基础的归咎方法最终产生了不是极大就是极小的结果，被称为『梯度消失或爆炸的问题』，Jurgen Schmidhuber——另一位深度学习的权威，给出了更正式也更深刻的归纳：&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;&lt;span&gt;一篇学术论文（发表于1991年，作者Hochreiter）曾经对深度学习研究给予了里程碑式的描述。文中第五、第六部分提到：二十世纪九十年代晚期，有些实验表明，前馈或递归深度神经网络是很难用反向传播法进行训练的（见5.5）。Horchreiter在研究中指出了导致问题的一个主要原因：传统的深度神经网络遭遇了梯度消失或爆炸问题。在标准激活状态下（见1），累积的反向传播错误信号不是迅速收缩，就是超出界限。实际上，他们随着层数或CAP深度的增加，以几何数衰减或爆炸（使得对神经网络进行有效训练几乎是不可能的事）。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;通过时间顺序扁平化BP路径本质上跟具有许多层的神经网络一样，所以，用反向传播来训练递归神经网络是比较困难的。由Schmidhuber指导的Sepp Hochreiter及Yoshua Bengio都写过文章指出，由于反向传播的限制，学习长时间的信息是行不通的。分析问题以后其实是有解决办法的，Schmidhuber 及 Hochreiter在1997年引进了一个十分重要的概念，这最终解决了如何训练递归神经网络的问题，这就是长短期记忆（Long Short Term Memory, LSTM）。简言之，卷积神经网络及长短期记忆的突破最终只为正常的神经网络模型带来了一些小改动：&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;&lt;span&gt;LSTM的基本原理十分简单。当中有一些单位被称为恒常误差木马（Constant Error Carousels， CECs）。每个CEC使用一個激活函数 f，它是一个恒常函数，並有一个与其自身的连接，其固定权重为1.0。由於 f 的恒常导数为1.0，通过CEC的误差反向传播将不会消失或爆炸（5.9节），而是保持原状（除非它们从CEC「流出」到其他一些地方，典型的是「流到」神经网络的自适应部分）。CEC被连接到许多非线性自适应单元上（有一些单元具有乘法的激活函数），因此需要学习非线性行为。单元的权重改变经常得益于误差信号在时间里通过CECs往后传播。为什么LSTM网络可以学习探索发生在几千个离散时间步骤前的事件的重要性，而之前的递归神经网络对于很短的时间步骤就已经失败了呢？CEC是最主要的原因。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;但这对于解决更大的知觉问题，即神经网络比较粗糙、没有很好的表现这一问题是没有太大帮助的。用它们来工作是十分麻烦的——电脑不够快、算法不够聪明，人们不开心。所以在九十年代左右，对于神经网络一个新的AI寒冬开始来临——社会对它们再次失去信心。一个新的方法，被称为支持向量机（Support Vector Machines），得到发展并且渐渐被发现是优于先前棘手的神经网络。简单的说，支持向量机就是对一个相当于两层的神经网络进行数学上的最优训练。事实上，在1995年，LeCun的一篇论文，「 Comparison of Learning Algorithms For Handwritten Digit Recognition」，就已经讨论了这个新的方法比先前最好的神经网络工作得更好，最起码也表现一样。&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;&lt;span&gt;支持向量机分类器具有非常棒的准确率，这是最显著的优点，因为与其他高质量的分类器比，它对问题不包含有先验的知识。事实上，如果一个固定的映射被安排到图像的像素上，这个分类器同样会有良好的表现。比起卷积网络，它依然很缓慢，并占用大量内存。但由于技术仍较新，改善是可以预期的。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;另外一些新的方法，特别是随机森林（Random Forests），也被证明十分有效，并有强大的数学理论作为后盾。因此，尽管递归神经网络始终有不俗的表现，但对于神经网络的热情逐步减退，机器学习社区再次否认了它们。寒冬再次降临。&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;&lt;span&gt;深度学习的密谋&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;当你希望有一场革命的时候，那么，从密谋开始吧。随着支持向量机的上升和反向传播的失败，对于神经网络研究来说，上世纪早期是一段黑暗的时间。Lecun与Hinton各自提到过，那时他们以及他们学生的论文被拒成了家常便饭，因为论文主题是神经网络。上面的引文可能夸张了——当然机器学习与AI的研究仍然十分活跃，其他人，例如Juergen Schmidhuber也正在研究神经网络——但这段时间的引用次数也清楚表明兴奋期已经平缓下来，尽管还没有完全消失。在研究领域之外，他们找到了一个强有力的同盟：加拿大政府。CIFAR的资助鼓励还没有直接应用的基础研究，这项资助首先鼓励Hinton于1987年搬到加拿大，然后一直资助他的研究直到九十年代中期。…Hinton 没有放弃并改变他的方向，而是继续研究神经网络，并努力从CIFAR那里获得更多资助，正如这篇例文（http://www.thestar.com/news/world/2015/04/17/how-a-toronto-professors-research-revolutionized-artificial-intelligence.html）清楚道明的：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;「但是，在2004年，Hinton要求领导一项新的有关神经计算的项目。主流机器学习社区对神经网络兴趣寡然。」&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;「那是最不可能的时候」Bengio是蒙特利尔大学的教授，也是去年重新上马的CIFAR项目联合主管，「其他每个人都在做着不同的事。莫名其妙地，Geoff说服了他们。」&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;「我们应该为了他们的那场豪赌大力赞许CIFAR。」&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;CIFAR「对于深度学习的社区形成有着巨大的影响。」LeCun补充道，他是CIFAR项目的另一个联合主管。「我们像是广大机器学习社区的弃儿：无法发表任何文章。这个项目给了我们交流思想的天地。」&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;资助不算丰厚，但足够让研究员小组继续下去。Hinton和这个小组孕育了一场密谋：用「深度学习」来「重新命名」让人闻之色变的神经网络领域。接下来，每位研究人员肯定都梦想过的事情真的发生了：2006年，Hinton、Simon Osindero与Yee-Whye Teh发表了一篇论文，这被视为一次重要突破，足以重燃人们对神经网络的兴趣：A fast learning algorithm for deep belief nets（论文参见：https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;正如我们将要看到的，尽管这个想法所包含的东西都已经很古老了，「深度学习」的运动完全可以说是由这篇文章所开始。但是比起名称，更重要的是如果权重能够以一种更灵活而非随机的方式进行初始化，有着多层的神经网络就可以得以更好地训练。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;「历史上的第一次，神经网络没有好处且不可训练的信念被克服了，并且这是个非常强烈的信念。我的一个朋友在ICML（机器学习国际会议）发表了一篇文章，而就在这不久之前，选稿编辑还说过ICML不应该接受这种文章，因为它是关于神经网络，并不适合ICML。实际上如果你看一下去年的ICML，没有一篇文章的标题有『神经网络』四个字，因此ICML不应该接受神经网络的文章。那还仅仅只是几年前。IEEE期刊真的有『不接收你的文章』的官方准则。所以，这种信念其实非常强烈。」&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5517241379310345&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiaXacQyibfI3Wic9tY2bbaQo35GTIjVbNGFt3yYl94oL5G8a9tLCkw9ibyQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;203&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;受限的玻尔兹曼机器&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;那么什么叫做初始化权重的灵活方法呢？实际上，这个主意基本就是利用非监督式训练方式去一个一个训练神经层，比起一开始随机分配值的方法要更好些，之后以监督式学习作为结束。每一层都以受限波尔兹曼机器（RBM）开始，就像上图所显示的隐藏单元和可见单元之间并没有连接的玻尔兹曼机器（如同亥姆霍兹机器），并以非监督模式进行数据生成模式的训练。事实证明这种形式的玻尔兹曼机器能够有效采用2002年Hinton引进的方式「最小化对比发散专家训练产品（Training Products of Experts by Minimizing Contrastive Divergence）」进行训练。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;基本上，除去单元生成训练数据的可能，这个算法最大化了某些东西，保证更优拟合，事实证明它做的很好。因此，利用这个方法，这个算法如以下：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot; list-paddingleft-2&quot; readability=&quot;3&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;利用对比发散训练数据训练RBM。这是信念网络（belief net）的第一层。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;span&gt;生成训练后RBM数据的隐藏值，模拟这些隐藏值训练另一个RBM，这是第二层——将之「堆栈」在第一层之上，仅在一个方向上保持权重直至形成一个信念网络。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;根据信念网络需求在多层基础上重复步骤2。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;span&gt;如果需要进行分类，就添加一套隐藏单元，对应分类标志，并改变唤醒-休眠算法「微调」权重。这样非监督式与监督式的组合也经常叫做半监督式学习。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5278969957081545&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiah7FCzxW6sgMW1HXbHUoAiaMUGuDLUNKdBj7McicCcHHZ8fFDKuYOmIxQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;466&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Hinton引入的层式预训练&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这篇论文展示了深度信念网络（DBNs）对于标准化MNIST字符识别数据库有着完美的表现，超越了仅有几层的普通神经网络。Yoshua Bengio等在这项工作后于2007年提出了「深层网络冗余式逐层训练（ “Greedy Layer-Wise Training of Deep Networks）」，其中他们表达了一个强有力的论点，深度机器学习方法（也就是有着多重处理步骤的方法，或者有着数据等级排列特征显示）在复杂问题上比浅显方法更加有效（双层ANNs或向量支持机器）。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;2.231132075471698&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiaWj9A12ZIFznqRQCCjrTBflWYCN1Hf6t2s0DcYsYRV201EYc9dR8xEA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;212&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;关于非监督式预训练的另一种看法，利用自动代码取代RBM。&lt;br data-filtered=&quot;filtered&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;他们还提出了为什么附加非监督式预训练，并总结这不仅仅以更优化的方式初始权重，而且更加重要的是导致了更有用的可学习数据显示，让算法可以有更加普遍化的模型。实际上，利用RBM并不是那么重要——普通神经网络层的非监督式预训练利用简单的自动代码层反向传播证明了其有效性。同样的，与此同时，另一种叫做分散编码的方法也表明，非监督式特征学习对于改进监督式学习的性能非常有力。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;因此，关键在于有着足够多的显示层，这样优良的高层数据显示能够被学习——与传统的手动设计一些特征提取步骤并以提取到的特征进行机器学习方式完全不同。Hinton与Bengio的工作有着实践上的证明，但是更重要的是，展示了深层神经网络并不能被训练好的假设是错误的。LeCun已经在整个九十年代证明了CNN，但是大部分研究团体却拒绝接受。Bengio与Yann LeCun一起，在「实现AI的算法（Scaling Algorithms Towards AI）」研究之上证明了他们自己：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;「直至最近，许多人相信训练深层架构是一个太过困难的优化问题。然而，至少有两个不同的方法对此都很有效：应用于卷积神经网络的简单梯度下降[LeCun et al., 1989, LeCun et al., 1998]（适用于信号和图像），以及近期的逐层非监督式学习之后的梯度下降[Hinton et al., 2006, Bengio et al., 2007, Ranzato et al., 2006]。深层架构的研究仍然处于雏形之中，更好的学习算法还有待发现。从更广泛的观点来看待以发现能够引出AI的学习准则为目标这事已经成为指导性观念。我们希望能够激发他人去寻找实现AI的机器学习方法。」&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;他们的确做到了。或者至少，他们开始了。尽管深度学习还没有达到今天山呼海应的效果，它已经如冰面下的潜流，不容忽视地开始了涌动。那个时候的成果还不那么引人注意——大部分论文中证明的表现都限于MNIST数据库，一个经典的机器学习任务，成为了十年间算法的标准化基准。Hinton在2006年发布的论文展现出惊人的错误率，在测试集上仅有1.25%的错误率，但SVMs已经达到了仅1.4%的错误率，甚至简单的算法在个位数上也能达到较低的错误率，正如在论文中所提到的，LeCun已经在1998年利用CNNs表现出0.95%的错误率。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;因此，在MNIST上做得很好并不是什么大事。意识到这一点，并自信这就是深度学习踏上舞台的时刻的Hinton与他的两个研究生，Abdel-rahman Mohamed和George Dahl，展现了他们在一个更具有挑战性的任务上的努力：语音识别（ Speech Recognition）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;利用DBN，这两个学生与Hinton做到了一件事，那就是改善了十年间都没有进步的标准语音识别数据集。这是一个了不起的成就，但是现在回首来看，那只是暗示着即将到来的未来——简而言之，就是打破更多的记录。&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;&lt;span&gt;蛮力的重要性&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;上面所描述的算法对于深度学习的出现有着不容置疑的重要性，但是自上世纪九十年代开始，也有着其他重要组成部分陆续出现：纯粹的计算速度。随着摩尔定律，计算机比起九十年代快了数十倍，让大型数据集和多层的学习更加易于处理。但是甚至这也不够——CPU开始抵达速度增长的上限，计算机能力开始主要通过数个CPU并行计算增长。为了学习深度模型中常有的数百万个权重值，脆弱的CPU并行限制需要被抛弃，并被具有大型并行计算能力的GPUs所代替。意识到这一点也是Abdel-rahman Mohamed，George Dahl与Geoff Hinton做到打破语音识别性能记录的部分原因：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;「由Hinton的深度神经网络课堂之一所激发，Mohamed开始将它们应用于语音——但是深度神经网络需要巨大的计算能力，传统计算机显然达不到——因此Hinton与Mohamed招募了Dahl。Dahl是Hinton实验室的学生，他发现了如何利用相同的高端显卡（让栩栩如生的计算机游戏能够显示在私人计算机上）有效训练并模拟神经网络。」&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;「他们用相同的方法去解决时长过短的语音中片段的音素识别问题，」Hinton说道，「对比于之前标准化三小时基准的方法，他们有了更好的成果。」&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;在这个案例中利用GPU而不是CPU到底能变得有多快很难说清楚，但是同年《Large-scale Deep Unsupervised Learning using Graphics Processors》这篇论文给出了一个数字：70倍。是的，70倍，这使得数以周记的工作可以被压缩到几天就完成，甚至是一天。之前研发了分散式代码的作者中包括高产的机器学习研究者吴恩达，他逐渐意识到利用大量训练数据与快速计算的能力在之前被赞同学习算法演变愈烈的研究员们低估了。这个想法在2010年的《Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition》（作者之一J. Schimidhuber正是递归LTSM网络（recurrent LTSM networks）的投资者）中也得到了大力支持，展示了MNIST数据库能够达到令人惊叹的0.35%错误率，并且除去大型神经网络、输入的多个变量、以及有效的反向传播GPU实现以外没有任何特殊的地方。这些想法已经存在了数十年，因此尽管可以说算法的改进并不那么重要，但是结果确实强烈表明大型训练数据集与快速腭化计算的蛮力方法是一个关键。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Dahl与Mohamed利用GPU打破记录是一个早期且相对有限的成功，但是它足以激励人们，并且对这两人来说也为他们带来了在微软研究室实习的机会。在这里，他们可以享受到那时已经出现的计算领域内另一个趋势所带来的益处：大数据。这个词语定义宽松，在机器学习的环境下则很容易理解——大量训练数据。大量的训练数据非常重要，因为没有它神经网络仍然不能做到很好——它们有些过拟合了（完美适用于训练数据，但无法推广到新的测试数据）。这说得通——大型神经网络能够计算的复杂度需要许多数据来使它们避免学习训练集中那些不重要的方面——这也是过去研究者面对的主要难题。因此现在，大型公司的计算与数据集合能力证明了其不可替代性。这两个学生在三个月的实习期中轻易地证明了深度学习的能力，微软研究室也自此成为了深度学习语音识别研究的前沿地带。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;微软不是唯一一个意识到深度学习力量的大公司（尽管起初它很灵巧）。Navdeep Jaitly是Hinton的另一个学生，2011年曾在谷歌当过暑假实习生。他致力于谷歌的语音识别项目，通过结合深度学习能够让他们现存的设备大大提高。修正后的方法不久就加强了安卓的语音识别技术，替代了许多之前的解决方案。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;除了博士实习生给大公司的产品带来的深刻影响之外，这里最著名的是两家公司都在用相同的方法——这方法对所有使用它的人都是开放的。实际上，微软和谷歌的工作成果，以及IBM和Hinton实验室的工作成果，在2012 年发布了令人印象深刻的名为「深层神经网络语音识别的声学建模：分享四个研究小组的观点」的文章。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;这四个研究小组——有三个是来自企业，确定能从伤脑筋的深度学习这一新兴技术专利中获益，而大学研究小组推广了技术——共同努力并将他们的成果发布给更广泛的研究社区。如果有什么理想的场景让行业接受研究中的观念，似乎就是这一刻了。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这并不是说公司这么做是为了慈善。这是他们所有人探索如何把技术商业化的开始，其中最为突出的是谷歌。但是也许并非Hinton，而是吴恩达造成了这一切，他促使公司成为世界最大的商业化采用者和技术用户者。在2011年，吴恩达在巡视公司时偶遇到了传说中的谷歌人Jeff Dean，聊了一些他用谷歌的计算资源来训练神经网络所做的努力。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这使Dean着迷，于是与吴恩达一起创建了谷歌大脑（Google Brain）——努力构建真正巨大的神经网络并且探索它们能做什么。这项工作引发了一个规模前所未有的无监督式神经网络学习——16000个CPU核，驱动高达10亿权重的学习（作为比较，Hinton在2006年突破性的DBN大约有100万权重）。神经网络在YouTube视频上被训练，完全无标记，并且学着在这些视频中去辨认最平常的物体——而神经网络对于猫的发现，引起了互联网的集体欢乐。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.86875&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiaPwmibTnSZog3KXJLpHEMQHSGibFriaKjkVkwzhVY1rvvmjPCPN0jmL3vw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;谷歌最著名的神经网络学习猫。这是输入到一个神经元中最佳的一张。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;它很可爱，也很有用。正如他们常规发表的一篇论文中所报道的，由模型学习的特征能用来记录标准的计算机视觉基准的设置性能。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这样一来，谷歌训练大规模的神经网络的内部工具诞生了，自此他们仅需继续发展它。深度学习研究的浪潮始于2006年，现在已经确定进入行业使用。&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;深度学习的上升&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;当深度学习进入行业使用时，研究社区很难保持平静。有效的利用GPU和计算能力的发现是如此重要，它让人们检查长久存疑的假设并且问一些也许很久之前被提及过的问题——也就是，反向传播到底为何没什么用呢？为什么旧的方法不起作用，而不是新的方法能奏效，这样的问题观点让Xavier Glort 和 Yoshua Bengio在2010年写了「理解训练深度前馈神经网络的难点」（Understanding the difficulty of training deep feedforward neural networks）一文。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;在文中，他们讨论了两个有重大意义的发现：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot; list-paddingleft-2&quot; readability=&quot;1&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;为神经网络中神经元选取的特定非线性激活函数，对性能有巨大影响，而默认使用的函数不是最好的选择。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;&lt;span&gt;相对于随机选取权重，不考虑神经层的权重就随机选取权重的问题要大得多。以往消失的梯度问题重现，根本上，由于反向传播引入一系列乘法，不可避免地导致给前面的神经层带来细微的偏差。就是这样，除非依据所在的神经层不同分别选取不同的权重 ——否则很小的变化会引起结果巨大变化。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.75&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiaib3TbNqwBtRBqv7Ta1TPRFT2CtTeGwD9oN4mmlgCwlDNKfscJubKsbw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;560&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;不同的激活函数。ReLU是**修正线性单元**&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第二点的结论已经很清楚了，但是第一点提出了这样的问题：『然而，什么是最好的激活函数？』有三个不同的团队研究了这个问题：LeCun所在的团队，他们研究的是「针对对象识别最好的多级结构是什么？」；另一组是Hinton所在的团队，研究「修正的线性单元改善受限玻尔兹曼机器」；第三组是Bengio所在的团队——「深度稀缺的修正神经网络」。他们都发现惊人的相似结论：近乎不可微的、十分简单的函数f(x)=max(0,x)似乎是最好的。令人吃惊的是，这个函数有点古怪——它不是严格可微的，确切地说，在零点不可微，因此 就 数学而言论文看起来很糟糕。但是，清楚的是零点是很小的数学问题——更严重的问题是为什么这样一个零点两侧导数都是常数的简单函数，这么好用。答案还未揭晓，但一些想法看起来已经成型：&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;修正的激活导致了表征稀疏，这意味着在给定输入时，很多神经元实际上最终需要输出非零值。这些年的结论是，稀疏对深度学习十分有利，一方面是由于它用更具鲁棒性的方式表征信息，另一方面由于它带来极高的计算效率（如果大多数的神经元在输出零，实际上就可以忽略它们，计算也就更快）。顺便提一句，计算神经科学的研究者首次在大脑视觉系统中引入稀疏计算，比机器学习的研究早了10年。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;相比指数函数或者三角函数，简单的函数及其导数，使它能非常快地工作。当使用GPU时，这就不仅仅是一个很小的改善，而是十分重要，因为这能规模化神经网络以很好地完成极具挑战的问题。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;后来吴恩达联合发表的「修正的非线性改善神经网络的语音模型 」（Rectifier Nonlinearities Improve Neural Network Acoustic Models）一文，也证明了ReLU导数为常数0或1对学习并无害处。实际上，它有助于避免梯度消失的问题，而这正是反向传播的祸根。此外，除了生成更稀疏的表征，它还能生成更发散的表征——这样就可以结合多个神经元的多重值，而不局限于从单个神经元中获取有意义的结论。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;目前，结合2006年以来的这些发现，很清楚的是非监督预训练对深度学习来说不是必要的。虽然，它的确有帮助，但是在某些情况下也表明，纯粹的监督学习（有正确的初始权重规模和激活函数）能超越含非监督训练的学习方式。那么，到底为什么基于反向传播的纯监督学习在过去表现不佳？Geoffrey Hinton总结了目前发现的四个方面问题：&lt;/span&gt;&lt;/p&gt;
&lt;ol class=&quot; list-paddingleft-2&quot; readability=&quot;1&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;带标签的数据集很小，只有现在的千分之一.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;计算性能很慢，只有现在的百万分之一.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;权重的初始化方式笨拙.&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;使用了错误的非线性模型。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;&lt;span&gt;好了，就到这里了。深度学习。数十年研究的积累，总结成一个公式就是：&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;span&gt;深度学习=许多训练数据+并行计算+规模化、灵巧的的算法&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.2265625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCiahkUAt630Kqrc8OeicdnBqAicrYjNXEfVkbU0hicZckXKfTvRY88advZKg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我希望我是第一个提出这个赏心悦目的方程的，但是看起来有人走在我前面了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;更不要说这里就是希望弄清楚这点。差远了！被想通的东西刚好是相反的：人们的直觉经常出错，尤其是一些看似没有问题的决定及假设通常都是没有根据的。问简单的问题，尝试简单的东西——这些对于改善最新的技术有很大的帮助。其实这一直都在发生，我们看到更多的想法及方法在深度学习领域中被发掘、被分享。例如 G. E. Hinton等的「透过预防特征检测器的互相适应改善神经网络」（ Improving neural networks by preventing co-adaptation of feature detectors）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;其构思很简单：为了避免过度拟合，我们可以随机假装在训练当中有些神经元并不在那儿。想法虽然非常简单——被称为丢弃法（dropout）——但对于实施非常强大的集成学习方法又非常有效，这意味着我们可以在训练数据中实行多种不同的学习方法。随机森林——一种在当今机器学习领域中占主导地位的方法——主要就是得益于集成学习而非常有效。训练多个不同的神经网络是可能的，但它在计算上过于昂贵，而这个简单的想法在本质上也可取得相同的结果，而且性能也可有显著提高。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;然而，自2006年以来的所有这些研究发现都不是促使计算机视觉及其他研究机构再次尊重神经网络的原因。这个原因远没有看来的高尚：在现代竞争的基准上完全摧毁其他非深度学习的方法。Geoffrey Hinton召集与他共同写丢弃法的两位作家，Alex Krizhevsky 与 Ilya Sutskever，将他们所发现的想法在ILSVRC-2012计算机视觉比赛中创建了一个条目。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对于我来说，了解他们的工作是非常惊人的，他们的「使用深度卷积神经网络在ImageNet上分类」（ImageNet Classification with deep convolutional neural networks）一文其实就是将一些很旧的概念（例如卷积神经网络的池化及卷积层，输入数据的变化）与一些新的关键观点（例如十分高性能的GPU、ReLU神经元、丢弃法等）重新组合，而这点，正是这一点，就是现代深度网络的所有深意了。但他们如何做到的呢？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;远比下一个最近的条目好：它们的误差率是15.3%，第二个最近的是26.2%。在这点上——第一个及唯一一个在比赛中的CNN条目——对于CNNs及深度学习整体来说是一个无可争议的标志，对于计算机视觉，它应该被认真对待。如今，几乎所有的比赛条目都是CNNs——这就是Yann LeCun自1989年以来在上面花费大量心血的神经网络模型。还记得上世纪90年代由Sepp Hochreiter 及 Jürgen Schmidhuber为了解决反向传播问题而开发的LSTM循环神经网络吗？这些在现在也是最新的连续任务比如语音处理的处理方法。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这就是转折点。一波对于其可能发展的狂欢在其无可否认的成绩中达到了高潮，这远远超过了其他已知方法所能处理的。这就是我们在第一部分开头所描写的山呼海应比喻的起点，而且它到如今还一直在增长，强化。深度学习就在这儿，我们看不到寒冬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6591836734693878&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/VYh0ayiawQ8ez3iahhLY6ENbvzBJJ63MCia6cKrWVpQp9hhia0gKiaB8yB485NM5CofrclglSpFlrmdlHCLQP3Wcdrw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;490&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; 
</description>
<pubDate>Sat, 10 Feb 2018 15:55:04 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/GSJsyHC1Yt</dc:identifier>
</item>
</channel>
</rss>