<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fmathalg-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/mathalg-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>算法与数学之美</title>
<link>http://www.jintiankansha.me/column/c9dZ5TM2aS</link>
<description>算法与数学之美 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>贝叶斯法则：预测未来</title>
<link>http://www.jintiankansha.me/t/TacNBf37xX</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/TacNBf37xX</guid>
<description>&lt;p&gt;&lt;span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.2654867256637168&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkymKiawEBibkSJiaNJ0qFH5q5oxZ6ic9At9iapD7UVKAAeg92xR6pjtyUjNzFuhpftiaLZYvxY6ZR9kYuKg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;113&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;本文节选自《算法之美：指导工作与生活的算法》&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;中信出版集团，2018年05月出版&lt;/span&gt;&lt;/p&gt;


&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;span&gt;人类获得的所有知识都是不确定的、不准确的和不全面的。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;——伯特兰·罗素&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;明天太阳会照常升起。你可以用你的一切来打赌太阳会出来。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;——安妮&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;1969年，J. 理查德·戈特三世在普林斯顿攻读天体物理博士学位之前，他去欧洲旅行了一趟。他看见了柏林墙，那是8年前建成的。站在墙的影子下，这仿佛是冷战的一个鲜明象征，他开始思索这墙会将东德和西德地区继续分割多久。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;从表面上看，试图做出这种预测有些荒谬。即使撇开地缘政治的不可预测性不说，这个问题仅在数学上似乎就很可笑：因为它试图从一个单一数据点进行预测。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;但是，尽管这看起来很可笑，但我们总是会根据需要做出这样的预测。你到了一个外国城市的公共车站，也许其他游客已经站在那里等了7分钟。下一班车什么时候到？继续等待是否值得？如果是这样的话，在放弃之前你应该再那等多久？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;或者你的一个朋友已经和某人约会了一个月，希望得到你的建议：邀请他们一起参加即将到来的人的婚礼是否太早？这种关系已经有了一个良好的开端，但是什么时候开始制订计划比较合适呢？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;谷歌的研究部主任彼得·诺维德曾进行过一次题为“数据的不合理有效性”的著名演讲，该演讲深究了“数十亿琐碎的数据点最终如何能被理解”。媒体不断告诉我们，我们生活在一个“大数据时代”，计算机可以筛选这数十亿的数据点并发现一些肉眼看不到的细节。但跟日常生活联系最密切的问题往往是另一种极端。我们的生活充满“小数据”，我们就像看到柏林墙的戈特一样，也就是通过一个单一的观察，做一个推论。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;那么我们一般怎么做呢？我们又应该怎样做？&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;故事发生在18 世纪的英国，那时，有一个研究领域对伟大的数学思想家来说是不可抗拒的（对那些神职人员也是如此），那就是赌博。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;贝叶斯牧师的倒推理&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;&lt;span&gt;因此，如果我们相信过去的经验，并把它作为我们判断未来的标准，那这些标准就一定不是确定的。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;——大卫•休谟&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;250 年前 ，贝叶斯牧师就很重视小数据预测问题，他来自英国迷人的温泉城镇坦布里奇韦尔斯，是一位长老会的牧师。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;贝叶斯设想，如果我们买10 张新的、不熟悉的抽奖彩票，其中有5 张中奖，那么要估计中奖概率就似乎相对容易：5/10，或50%。但是，如果我们只买了一张彩票，并赢得奖品呢？我们真的认为中奖的概率就是1/1，或是100%的？这似乎过于乐观，不是吗？如果是这样的话，那中奖概率应该是多少？我们应该猜多少呢？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;对于那些曾在不确定性推理历史上产生如此重大影响的人来说，贝叶斯自己的故事也具有讽刺的不确定性。他出生于1701年或者1702年，出生地是英国的赫特福德郡，或是伦敦。在1746年，或1748年，或1747年，抑或是1749年，他写了一篇在数学界最具影响力的论文，他却未将它发表，并继续做其他事情。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在这两个事件之间我们有了更多的把握。作为牧师的儿子，贝叶斯去爱丁堡大学学习神学，并像他父亲一样被任命为牧师。他对数学和神学感兴趣，并在1736年为牛顿全新的 “微积分”理论写了一篇慷慨激昂的辩护书，以回应乔治伯克利主教对牛顿的攻击。这使他在1742年当选为皇家学会的成员，并被赞誉为“擅长几何、数学和哲学学习的绅士”。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;1761年贝叶斯去世后，他的朋友理查德·普莱斯被要求整理他的数学论文，看是否有可发布的内容。一篇文章引起了他的兴趣，并令他特别兴奋——他说这篇文章“极为出色，值得保存”。这篇论文就论述了本文所讨论的彩票问题：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;span&gt;让我们想象一个人在抽奖的时候，对会不会中奖完全不知道，也不知道中奖和无奖的比例如何。让我们进一步假设，他要从他之前了解到的无奖的数量来推测相对的中奖数量，并询问他在这些情况下能做出什么合理的结论。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;贝叶斯的关键见解是，试图使用我们看到的中奖和未中奖彩票来分析彩票来源于整体彩票池的方法，本质上是在倒推。他说，要做到这一点，我们需要先用假设向前推理。换句话说，我们首先需要确定，如果各种可能场景都成真的情况下，我们中奖的可能性有多少。这个被现代统计学家称为“可能性”的概率给了我们解决问题所需要的信息。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;例如，假设我们买了三张彩票，三张都中奖了。现在，如果这种彩票中奖率特别高，所有彩票都能中奖，那我们的买三中三的中奖率就肯定会一直发生，在这种情况下就是100% 的概率。但如果只有一半的彩票能中奖，那我们三张彩票的中奖率就是1/2×1/2×1/2, 也就是1/8。如果1 000 张彩票只有一张能中奖，那么我们的中奖率将是1/1 000×1/1 000×1/1 000，也就是1×10–9。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;贝叶斯认为，因此我们应该判断如何能让所有彩票都尽可能中奖而不是一半能中奖，或者尽可能使一半的彩票中奖而不是1/1 000。也许我们生来便拥有这种直觉，但贝叶斯的逻辑思维却给我们提供了为这种直觉定量的方法。在同等条件下，我们应该想象成所有彩票都中奖的概率比一半中奖的概率要高8 倍，因为我们在这种情况下买的彩票正好是8 倍多的中奖概率（100% 与1/8）。同样的，一半的彩票中奖的概率正好是1 000 张中一张中奖的1.25 亿倍，我们已经通过比较1/8 和1×10–9 而得知其中的原因。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这是贝叶斯论证的关键所在。从假设的过去向前推理，并奠定了理论基础，让我们可以向后找到最大的可能性。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  这是一个巧妙和创新的方法，但它对抽奖问题没能提供一个完整的答案。普莱斯在向皇家学会提交贝叶斯的研究结果时，他能够确定，如果你买了一张彩票并中奖了，那么至少有一半的彩票都能中奖的概率是75%。但是，考虑概率的概率问题会让人有点儿头晕。更重要的是，如果有人在催促我们：“好吧，但是你认为彩票的中奖率到底是多少？”我们仍然不知道该说什么。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如何将所有可能的假设提取到单一的期望值，这一问题将在短短几年后由法国数学家皮埃尔·西蒙·拉普拉斯解答。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;拉普拉斯定理&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1749年，拉普拉斯生于诺曼底，他父亲送他到一所天主教学校，并希望他成为神职人员。拉普拉斯继续在卡昂大学学习神学，他不像贝叶斯那样一生都能平衡对神学和科学的奉献，因此他最终放弃了做牧师，而专攻数学。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;1774年，在完全不知道贝叶斯以前做的工作的情况下，拉普拉斯发表了一篇雄心勃勃的论文，名为“事件原因的概率论”。在这篇论文中，拉普拉斯终于解决了如何从观察到的效果向后推理并找出可能的原因这一问题。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如我们所见，贝叶斯找到了一种比较两种假设的相对可能性的方法。但是在彩票这一问题上，这里的假设几乎就是无穷的——每一个中奖彩票可能的比例。利用微积分这一曾备受争议却受到贝叶斯坚决拥护的数学学科，拉普拉斯能够证明这个巨大范围的可能性，这可以提取成一个单一的预估值和一个非常简洁的数字。他表示，如果我们提前真的不知道彩票的情况，然后当我们第一次买的三张彩票中的一张彩票中奖了，我们可以推测奖池里彩票的总中奖比例为2 / 3。如果我们买三张彩票，都中奖了，那我们可以推测总中奖比例正好是4/5。事实上，如果买n 张彩票共w 张中奖，那么中奖率就是中奖数加1，除以所购买的数目加2，即w+1/n+2。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这种令人难以置信的简单的方法估计概率的简单方法被称为拉普拉斯定律，它很容易就能适用于任何你需要通过历史事件来评估概率的情况。如果你做了10 次尝试，其中有5 次成功，拉普拉斯定律估计你的整体成功概率是6/12 或50%，这符合我们的直觉。如果你只试一次便取得成功，拉普拉斯给的估计是2/3，这比假设你每次都赢更合理，也比普莱斯的观点更具可操作性（它告诉我们，50% 或更大的成功概率有75% 的元概率）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;拉普拉斯继续将他的统计方法应用到广泛的时间问题上，包括评估男孩和女孩的出生率是否真正平均。（他发现，男婴其实比女婴的出生率稍高。）他还写了关于概率的哲学论文，可以说这是给大众读者的第一本关于概率的书，也是最好的概率书之一，此书奠定了他的理论基础并讲述了这些理论在法律、科学与日常生活上的应用。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;拉普拉斯定律为我们在现实世界中面对小数据时提供了第一种简单的经验法则。即使我们只进行了一些或一次观察，它也都能给予我们实际指导。想知道你的车晚点的概率吗？你的垒球队会赢吗？数一数过去已经发生的数量再加一，然后除以可能的机会数再加2。拉普拉斯定律的精髓就在于无论我们有一个单独的数据点或数以百万计的数据，它都同样适用。小安妮相信太阳明天会升起是有道理的，这句话告诉我们：地球已经连续看到太阳上升约1.6 万亿天，在下一次的“尝试”中看见太阳不升起来的机会，几乎没有可能。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;贝叶斯法则与先验信念&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;&lt;span&gt;可以想象，所有这些假设都是一致并可以想象的。为什么我们要偏向其中一种，而这一种并不比其余的更一致或可以想象？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;——大卫•休谟&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;拉普拉斯也考虑了另一种修饰贝叶斯理论的方法，这将被证明是至关重要的：那就是如何处理那些比其他假设可能性更大的假设。例如，买彩票时，99%的中奖率是有可能的，但我们可以假设中奖率更有可能只有1%。这一假设应该体现在我们的估算过程中。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;说得更具体点儿，例如有一个朋友给你看两个不同的硬币。一个是正常的“公平”硬币，正反两面都具有50–50的概率，另一种是两面都是头像的硬币。他把它们扔到一个袋子里，然后随意地拿出一个，他将硬币旋转一次：是头像。你认为你的朋友旋转的是哪个硬币？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;贝叶斯的反向工作方案使这个问题变得简单。那个公平硬币转到头像的概率是50%，另一个双头硬币转到头像的概率是100%。因此，我们可以自信地断言，转到这个硬币的概率是100%除以50%，或朋友掏出双头硬币的概率是它的两倍。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;现在考虑下面一次的旋转。这一次，朋友给你看9个公平硬币和一个双头像硬币，把所有10枚硬币都装进袋子，随机抽取一个，并翻转它：还是头像。现在你怎么想？这次是公平硬币还是双头像硬币？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;拉普拉斯预料到了这一点，而且答案又一次简单得令人印象深刻。如果和以前一样，一枚公平硬币转到头像的概率正好是一枚双头像硬币的一半。但现在，首先公平的硬币被抽到的概率就是双头像硬币的9 倍。事实证明，我们可以把这两个不同的概率都考虑进去，并把它们相乘：这就是说，你朋友持有一个公平的硬币的概率是双头像硬币的4.5 倍。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;描述这种关系的数学公式，将我们先前持有的观念和我们眼前的证据结合起来，就形成了后来的贝叶斯法则。有点儿讽刺的是，真正重要的工作却是由拉普拉斯完成的。它提供了一个非常简单的解决方案来如何处理现有的信念与观察到的证据：将它们的概率相乘。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;值得注意的是，有一些预先存在的信念，在计算这个公式时至关重要。如果你的朋友只是走近你说：“我从这个袋子里翻出了一枚硬币，最后转出头像那面。你认为这是一枚公平硬币的概率有多大？”除非你最开始就对袋子里是什么硬币有一定了解，否则你完全无法回答这个问题。（当你对任何一个概率都无从得知的时候，你便无法将两个概率相乘），在硬币翻转之前，你对“袋子里”是什么的感觉，或是说在你看到任何数据之前，每个假设的概率都是真实可能的，这就是所谓的先验概率，或者简称为“先验”。贝叶斯法则总是需要一些先验，即使它只是一个猜测。有多少枚双头像硬币？抽到他们的概率有多大？那么，你的朋友有多大可能是一个骗子呢？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;贝叶斯法则依赖于先验概率，这一点在历史上的某些时刻被认为是有争议的、有偏见的，甚至是不科学的。但在现实中，我们的头脑实际上很少会进入一个完全空白甚至停滞的状况。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当你对先验概率有一定的预估时，贝叶斯法则也适用于各种各样的预测问题，无论它们是大数据类型还是更常见的小数据排序。计算彩票获奖概率或扔硬币的概率仅仅是开始。由贝叶斯和拉普拉斯研究出的方法可以在任何时候帮助我们，尤其是当我们遇到不确定性或数据不足的问题和工作时。这正是我们试图预测未来时所面对的情况。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;哥白尼原则&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;span&gt;预测本就是一件难事，预测未来尤其如此。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;——谚语&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;当理查德·戈特看到柏林墙时，他问了自己一个非常简单的问题：我在哪？也就是说，在这一人工建筑存在的全过程中，我是否恰好已经到达了呢？简而言之，他是在从时间角度问一个空间问题，而这一问题正是在400年前深深吸引着天文学家尼古拉·哥白尼的问题：我们在哪？地球在宇宙的什么位置？与前人不同，哥白尼激进地以为地球不是宇宙的中心，也就是说地球没有什么特别的。戈特决定采取同样的关于时间的分析步骤。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;他设想，他到达柏林墙的那一刻并不特别，因为这只是柏林墙整个历史中的一瞬。如果有任何一个时刻都有同样的可能性，那么平均来说，他的到来应该是在一个精确的中间点（因为他有50%概率是在此之前到来，或50%的概率是在此之后）。更普遍的是，除非我们确定我们在某个特定时间现象中出现的特定中间点。a如果我们假设我们到达的中间点有精确的时间，那么对于它在未来还可以持续多久的最佳猜测就变得很明显：确切地说就是它已经存在的时间。戈特看到柏林墙时已经建成8 年了，所以他最好的猜测是，它将再存在8 年。（最终，这个数字是20 年。）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  这个简单的推理，被戈特称为哥白尼原则，它可以得出一个简单的算法，能为各类事件做出预测判断。在没有任何先入为主的预测时，我们不仅可以用它来获得对柏林墙终结时间的预测，同时也可以预测任何其他短期和长期现象。哥白尼原则预测道，美利坚合众国作为一个国家将一直持续到2255 年左右，谷歌将持续到大约2032 年，你与你的朋友一个月前开始的一段关系将可能再持续约一个月（也许你该告诉他不要参加刚收到的婚礼邀请呢）。同样，它告诉我们要持怀疑态度，例如，《纽约客》杂志封面是一个人拿着一个6 英寸的智能手机，上面有大家熟悉的网格正方形应用程序图标，标题为“2525”。但这是令人怀疑的。据我们所知，智能手机刚诞生10 年，哥白尼原则告诉我们，它不可能出现在2025 年，更别说5 世纪后了。到2525年，即使还有一个纽约市存在，也会让人感到吃惊。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;更实际地说，如果我们正在考虑一份建筑工地的工作，他们的标牌表明“上一次工程事故发生在7 天前”，我们可能会想离开，除非这是一份我们计划做得特别短的工作。如果一个城市的公交系统承担不起可以告诉乘客下一班车什么时候会到来这一非常有用却很昂贵的实时提醒系统的话，哥白尼原则表明，可能有一个更简单也更便宜的替代品。那就是简单地显示前一辆公交车到达此处的时间距离现在有多久，这可以为判断下一辆公交车到来的时间提供一个实质性的提示。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;但是哥白尼原则就一定正确吗？当戈特在《自然》杂志上发表了他的猜想之后，该杂志收到了很多重要信件。当我们尝试将规则应用到一些比较熟悉的例子时，很容易理解这是为什么。如果你遇到一个90岁的男子，哥白尼原则预测他会活到180岁。同时，每个6岁的男孩都会被预测将在12岁时早逝。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;要理解为什么哥白尼原则是合理的，以及为什么它有时不合理，我们需要回归到贝叶斯法则。因为，哥白尼原则尽管具有明显的简单性，但其的确是贝叶斯法则的一个实例。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;贝叶斯与哥白尼&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;在预测未来时，如柏林墙的寿命这类问题，我们需要评估的假设是所有手头上掌握的现象的持续时间：它会持续一个星期，一个月，一年，还是十年？正如我们已经看到的，要应用贝叶斯法则，我们首先需要给每个现象的持续时间分配一个先验概率。事实证明，哥白尼原则正是应用贝叶斯法则并使用了所谓的无信息先验的结果。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;起初，这似乎是一个矛盾。如果贝叶斯法则总是要求我们明确事先的预测和想法，我们又怎么能告诉它，我们没有任何预测结果呢？在彩票抽奖的情况下，为无知进行辩护的一个方法就是被称为“统一先验”的方法，这就是认为每个中奖彩票的比例都是相同的。在柏林墙这一例子中，无信息先验意味着：我们对将要预测的时间范畴一无所知：墙可能会在接下来的5 分钟或5 年后倒塌。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;除了这些无信息先验，如我们所见，我们供应给贝叶斯法则的唯一一部分数据，事实上就是我们到达柏林墙的时候，它已经存在了8 年。任何预测它小于8 年寿命的假设都可以被排除，因为这些假设不能解释我们这里的情况。（同样的，一枚双头像硬币就可以排除字那面的可能性。）任何超过8 年的预测都是有可能的，但是如果柏林墙要存在100 万年，那它将是一个很大的巧合，表明我们几乎是接近它存在的最初起点。因此，即使特别长的寿命不能排除，但它也不大可能出现。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当贝叶斯法则与所有这些概率结合——更有可能的短时限就拉低了平均预测，可能性更小但也有一定可能性的长时限又将其拉高，哥白尼原则便出现了：如果我们要预测某个事物还将持续存在多久（在对它没有其他任何了解时），我们可以做出的最好的猜测就是，它将再持续已经存在的时间。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;事实上，戈特并不是第一个提出类似哥白尼原则的人。20 世纪20 年代中期，贝叶斯统计学家哈罗德·杰佛利曾考虑仅仅通过一辆城市有轨电车的序号来确定一个城市有轨电车的数量，并得出了相同的答案：该数字的双倍。一个类似的问题出现得更早，在第二次世界大战期间，同盟国试图估计由德国制造的坦克数量。他们通过所捕获的坦克的序列号，在纯数学估计的基础上进行预测，得出的结果是德国每月生产246 辆坦克，而通过广泛的（高度危险的）空中侦察所获得的估计表明，这个数字更接近于1 400。而战后，德国记录显示的真实数字是：245。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在认识到哥白尼原则是无信息先验基础上的贝叶斯法则之后，就可以回答很多关于其有效性的问题。哥白尼原则在我们什么都不知道的情况下似乎是合理的、准确的，如在1969年看到的柏林墙，我们不确定什么时间范畴是合适的。同时，在我们对某一对象的确有所了解时，就会感觉这是完全错误的。预测一个90岁的人能活到180岁是不合理的，这恰恰是因为我们关于人类寿命已经了解了很多——在这种情况下，我们就可以预测得更好。我们给贝叶斯法则带来的先验信息越丰富，我们便能从中得到越有用的预测。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;真实世界的先验……&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;从广义上讲，世界上有两种类型的事物：倾向于（或围绕）某种“自然”价值的事物，以及与之相反的事物。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;人类的生命跨度显然是属于前一类。它大体遵循所谓的“正态”的分布，也被称为“高斯”分布（这是以德国数学家卡尔·弗里德里希·高斯命名的），同时因其分布的形状特征也被形象地称为“钟形曲线”。这种形状能很好地表现人类的寿命，例如，美国男性的平均寿命集中在76岁左右，曲线顶端的两边呈现急剧下降的趋势。正态分布往往都有一个适当的比例：一位数的寿命往往会被认为是悲惨的，三位数的寿命是非凡的。自然世界的许多其他事情也都呈现正态分布的趋势，从人的身高、体重、血压，到城市正午的温度，或是果园的果实直径。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;世界上有许多事物看起来似乎并不呈现正态分布，但这只是因为你没有长远地看。例如，美国一个城镇的平均人口是8 226 人。但是如果你要按人口统计该城镇数量图表，你就不会看到像钟形曲线那样长远才能实现的东西。还有很多小镇的人口远不足8 226 人，同时，某些重要城镇的人口会比平均人口要大得多。这种模式就是所谓的“幂律分布”，也被称为“无标度分布”，因为他们可以在多个尺度的范围表达数量：一个城市能有几十，数百，数千，数万，数十万，甚至数百万名的居民，所以我们不能以一个单一的数值来定义一个“正常”的城镇有多大。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;幂律分布可以描述在日常生活中一系列与城镇人口分布类似的现象：大多数都低于平均值，少数是超过的。电影的票房收入，其范围可以是从4~10 位的数字，这是另一个例子。有些电影根本挣不了那么多钱，但偶尔也有像《泰坦尼克号》这样的高票房电影。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;事实上，一般来说，货币是一个充满权力法则的领域。幂律分布可以描述人民的财富和人民的收入。例如，美国的人均收入是55 68美元，但由于收入大致是呈幂律分布的，这样我们便会得知，平均值以下的人会比平均值以上的要多，而平均值以上的人的收入可能高得几乎偏离了图表。事实也的确如此：美国2/3 的人口收入低于平均收入，但前1% 的人的收入几乎是平均水平的10 倍。这1% 中的前1%的人的收入又是其余99% 的10 倍。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;人们常常感叹“富人会变得更富有”，实际上“偏好依附”的过程是产生幂律分布的最可靠的方法之一。我们使用最多的网站往往就是最有可能获得导入链接的网站，拥有最多人追随的网络红人就是最有可能获得新支持者的人，最有声望的公司就是最有可能吸引新客户的公司，最大的城市就是最有可能吸引新居民的城市。在这每一种情况下，幂律分布都会得出这个结果。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;贝叶斯法则告诉我们，在基于有限的证据进行预测时，很少有事情是和好的先验一样重要的，也就是说，我们期望证据可以从分布结果中得出。因此，良好的预测最开始要有良好的直觉，要能感觉到我们何时在处理一个正态分布，何时在处理一个幂律分布。事实证明，贝叶斯法则为我们处理这些情况各提供了一个简单但显著不同的预测经验法则。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;他们的预测规则&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;span&gt;你是指“这会一直”朝好的方向发展吗？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;——本•勒纳&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span&gt;为了验证哥白尼原则，我们看到，当给贝叶斯法则一个无信息先验时，它会一直预测事物的总寿命为目前寿命的两倍。事实上，无信息先验的可能性有很宽泛的尺度，柏林墙可能继续存在几个月或几千年，这个尺度就是幂律分布。对于任何幂律分布，贝叶斯法则表明，一个合适的预测策略就是相乘法则：将迄今观察到的数量乘以一些常数。对于无信息先验，这个常数一般是2，哥白尼预测的方法由此得来；在其他幂律的情况下，所乘的数将取决于你工作的精确分布。例如，对于电影票房，它正好是1.4。所以，如果你听到一部电影到目前为止已经赚了600万美元，那么你可以猜测，它总共将赚840万美元。如果它现在赚了9 000万美元，那么可以预计的最高票房将是1.26亿美元。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;幂律分布不能表明它们所描述的现象的自然范畴，这就直接导致了相乘法则的出现。因此，唯一能给我们的预测提供一些关于范畴的想法的就是我们所拥有的单一数据点，比如柏林墙已经存在8 年了。单一数据点的值越大，我们可能要处理的范畴也就越大，反之亦然。当然这种情况也是有可能的：这部电影的票房现在是600 万美元，而实际上它只是在第一个小时票房惊人，它更可能是一个只有几百万美元票房的电影。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;另一方面，当我们将正态分布作为贝叶斯法则的先验时，我们会得到一个非常不同的指导。我们会得到一个“平均”规则，而不是相乘法则：使用分布的“自然”平均数作为指导。例如，如果有人还没达到平均寿命，那么就直接将其年龄预测为平均值。随着他们的年龄增长并超过平均水平，就预测他们还会再活几年。遵循这一规律为90岁和6 岁的两个人给出的合理预测年龄分别为94 岁和77 岁。（6 岁的孩子的预测寿命比76 岁的平均寿命略高是因为他已经顺利度过了婴儿期：这样我们就知道他不处于分布的尾端。）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  电影的时长就像人类的寿命，也遵循正态分布：大多数电影都在100 分钟左右，某些特殊的电影时长处于分布的两端。但并不是所有的人类活动都是这样的。诗人迪安·杨曾经说过，每当他听一首带编号的诗时，如果读者开始念第四节，他的心就会一沉：如果有三个以上的部分，杨就会重新开始，静坐细听。事实证明，杨的沮丧完美体现了贝叶斯法则。通过对诗的分析可发现，它不同于电影的时长，诗歌更接近于幂率分布而不是正态分布：因为大部分诗是短的，除了某些史诗。所以说到诗歌，首先你要确保有一个舒适的座位。正态分布的东西似乎太长了，最后必然会很快结束。但幂律分布的东西存在的时间越长，你可以预测它继续下去的时间就越长。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在这两个极端之间，生活中实际上还有第三种事物：那些不具有更大或更小可能性结束的事物，只因为他们已经持续存在了一段时间。有时候事情是简单的、不变的。丹麦数学家瓦格纳·厄兰研究了这种现象，他将独立事件之间的间隔形式化并推导出带有他名字的函数：厄兰分布。这条曲线的形状不同于正态分布或幂律分布：它有一个类似翅膀的形状，峰值上升较缓，尾部下降的趋势比幂律分布得快，但比正态分布得缓。在20世纪初，他为哥本哈根电信公司工作，用这种分布曲线来模拟在电话网络中连续通话的时间。自那以后，厄兰分布也被用于城市规划以及汽车和行人交通的建设模型中，并被网络工程师在设计互联网的基础设施时使用。自然世界中存在多个维度，其中发生的事件彼此也是完全独立的，它们之间的间隔从而就落在了厄兰曲线上。放射性衰变就是一个例子，这意味着厄兰分布完美地预测了盖革计数器的下一次提示声何时会发出。其在描述例如政客在众议院的任职时间这类的人类活动时也表现不俗。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;厄兰分布给出了第三种预测法则——相加法则：总是预测事物只会再持续一个常量。我们经常听到的“只需5分钟！……（5分钟后）再给我5分钟！”这往往表现了人们的某种特征，比如说，当一个人准备离开房子或办公室，或完成一些任务的最后时间，这似乎预示着在对现实做出估计时可能出现的一些慢性故障。不过，在一个人不符合厄兰分布的情况下，无论如何，这种话都可能是正确的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;例如，如果一个赌场纸牌爱好者告诉他不耐烦的配偶，他会在赢得一次21 点后就停手（赢的概率约为20∶1），他会很高兴地预测：“我再买20 次就会赢了！”20 次后她又回来，问他要让她再等多久，那么，他的答案将是不变的：“我再买大约20 次就会赢！”这听起来像是我们这位不懈的赌鬼已经进入短期记忆丧失模式了，但事实上，他的预测是完全正确的。事实上，无论他们过去或目前的状态是怎样的，分布结果会产生相同的预测，这一结果被统计学家称为“无记忆性”。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这三个非常不同的最佳预测模式——相乘法则、平均法则和相加法则都是通过将贝叶斯法则应用到幂律、正态和厄兰分布上得出结果的。因为这些预测的出现，这三种分布也给我们提供了不同的指导，让我们知道对某些事件应该有多惊讶。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在幂律分布中，某个事物已经存在的时间越长，我们可以预测它继续存在的时间也就越长。因此，幂律事件让我们等待的时间越长，就会让我们更加惊奇，尤其在它发生前的一刻。一个国家、一个公司或一个机构，年复一年地变得更加强大，所以当它崩溃时总是令人震惊。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在正态分布中，如果事件提前发生就会令人惊讶，因为我们期望它们达到平均水平，但当它们推迟发生时不会如此。的确，到了这一点，它们似乎推迟发生了，所以我们等待的时间越长，我们就会越期待。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在厄兰分布中，通过定义的事件无论何时发生都不会给我们带来更多或更少的意外。任何事情的状态都有可能结束，不管它已经持续了多久。毫无疑问，政治家总是会对他们下一次的选举进行准备。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;赌博的特点类似于稳态预期。例如，如果你所等待的轮盘赌注的胜利是呈正态分布的，那么平均法则将适用于此：在一个坏运气后，它会告诉你，你的号码应该会随时中奖，在输了更多次之后会更快出现。（在这种情况下，它的影响会持续到下一次胜利，然后停止。）相反，如果你等待的胜利呈现幂律分布，那么相乘法则会告诉你胜出盘会一次接着一次出现。（在这种情况下，如果你这局胜出了就应该继续下注，如果长时间没有胜出就该停手。）然而，当面对无记忆分布时，你就进退两难了。相加法则告诉你，现在赢的机会和一小时前一样，一小时后也如此。一切都没有什么变化。你没有因为长时间的等待而得到大奖，也没有一个转折点会告诉你何时应该停止你的损失。在电影《赌棍》中，肯尼·罗杰斯提出了一个著名的建议，他说，你必须“知道什么时候走开，或知道什么时候继续”，但对于无记忆分布而言，没有一个绝对正确的退出时间。这可能就是为什么这些游戏会让人上瘾的部分原因。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;知道你所面对的是什么样的分布十分重要。当哈佛大学的生物学家和作家斯蒂芬·杰伊·古尔德发现自己得了癌症后，他的第一个念头就是去阅读相关的医学文献。然后他发现为什么他的医生会劝阻他这样做：患他这种癌症的病人有一半在确诊8 个月内死亡。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;但是这一个统计数字（8 个月）并没有告诉他任何关于幸存者的分布。如果这是一个正态分布，那么平均法则将给出一个相当明确的预测，告诉他还可以活多久：约8 个月。但是，如果它是幂律分布，尾部延伸到右侧，那么情况就会大不相同：相乘法则会告诉他，他活得越久，就会有越多的证据证明他能活得更长。进一步阅读后，古尔德发现：“分布确实是强烈右偏，长（但比较小的）尾巴延长数年以上，都超过8个月的中位数。我看不出我为什么不应该待在那条小尾巴上，我长长地松了一口气。”古尔德在确诊后又活了20年。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;小数据与思维&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;三个预测法则——相乘、平均和相加适用于日常生活的各个方面。在这种情况下，人们一般都非常善于使用正确的预测法则。汤姆在读研究生时，和麻省理工学院的乔希·特南鲍姆一起进行了一个实验，实验要求人们对生活中的各种常量进行预测，如人类的寿命、电影的票房以及众议院议员任职时间等，每个问题只提供一条信息：现年龄、现票房或现任职时间。然后，他们比较了人们所预测的结果和应用贝叶斯法则的结果。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;事实证明，人们所做的预测与贝叶斯法则所得出的预测非常接近。直觉上，人们做出不同类型的预测也是遵循在现实世界中的不同分布——幂律、正态和厄兰分布。换句话说，虽然你可能不知道或不清楚某种情况是需要用相乘法则、平均法则，还是相加法则，但你每天做的预测往往隐含在这些分布中，它反映了日常生活中出现的不同情况，以及不同的行为方式。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;根据我们对贝叶斯法则的了解，这一出色的人类表现显示了可以帮助我们进行预测的重要因素。小数据是大数据的变相。往往，我们能从少量的或一个单一的观察结果得出正确预测结果的原因是，我们在这方面的先验如此丰富。不管我们是否知道，我们似乎已经在头脑中储存下惊人准确的先验，例如关于电影的票房和时长、诗的长度，以及任职时间，更不用说人类的寿命。我们不需要特意收集这些先验，因为我们从这个世界中不停地吸收着它们。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;事实上，就整体而言，人们的直觉似乎接近于贝叶斯法则的预测，也可以将各种先验分布逆向转换，即使这很难得到权威的真实数据。例如，对客户服务保持不变是人类经验中一个相当常见的一面，但没有公开的数据集表明好莱坞票房收入的保持时间。但是，如果人们是通过他们的经验进行预测，我们就可以使用贝叶斯法则，通过挖掘人们的期望对世界进行间接探测。当汤姆和乔希要求人们从一个单一的数据点来预测保持时间时，结果表明受试者使用的是相乘法则：人们预计的总等待时间是他们等待时间的一倍多。这与将幂律分布作为先验相一致，其中广泛的尺度也是可能的。只希望你不要因为等待时间而终结在“泰坦尼克号”上。在过去的10 年中，这样的方法使认知科学家能够从视觉、语言等各个领域识别人类的先验分布。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;然而这里有一个关键的警示。在我们没有良好先验的情况下，我们就无法很好地预测。例如，在汤姆和乔希的研究中有一个主题，人们的预测在这个主题上全都系统地偏离了贝叶斯法则，那就是预言埃及法老统治的长度。（恰巧，法老王的统治遵循厄兰分布。）在这个问题中，人们只是没有足够的日常接触以产生一个直观的感觉范围的价值观，所以他们的预测肯定也十分困难。准确的预测需要充足的先验知识。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这具有许多重要的含义。我们的判断背叛了我们的预期，我们的期望又背叛了我们的经验。我们对未来的计划揭示了我们生活的世界以及我们自己经历过的方方面面。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我们的预测体现出我们自己&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;20世纪70年代初期，沃尔特·米歇尔在其著名的“棉花糖实验”中曾试图分析延迟满足的能力是如何随着年龄的增长而发展的。在斯坦福大学的一所幼儿园里，研究者对一组3~5岁的孩子进行了意志力测试。每一个孩子面前都会出现一种美食，如棉花糖，并被告知参与实验的成人马上要离开一会儿。如果他们想吃那些糖，可以马上吃。但是，如果他们忍着不吃等到实验者回来，便会得到多一颗糖。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;有些孩子由于抵制不了美食的诱惑，就立即吃了。有些孩子坚持了整整15分钟，直到实验者返回，并得到了两颗糖。但也许最有趣的就是那些等待了一会儿但后来还是没忍住吃掉糖的孩子。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在这种情况下，这些孩子在努力抗争，抵制诱惑，但最终还是败下阵来，失去了额外的棉花糖，这被解释为体现出一种非理性。如果你要屈服，为什么不立即屈服并免受折磨？但这完全取决于孩子认为自己处于什么样的状况。正如宾夕法尼亚大学的乔·麦奎尔和乔·凯布尔所指出的，如果需要大人回来的时间呈幂律分布（逾期缺席意味着比预想的等待时间更长），那么在某个时候减少损失就是完美决定。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;换句话说，抵制诱惑的能力至少部分取决于预期而不是意志力。如果你预测大人会在很短的时间后回来（有点类似正态分布），那么你就应该能够坚持下去。平均法则表明，经过痛苦的等待，要做的事情还是在那里：实验者应该随时会返回。但是，如果你不知道消失的时间会有多长（与幂律分布一致），那么这就是一场艰苦的战斗。相乘法则表明，现在漫长的等待还只是未来漫长等待的开头。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这次棉花糖实验后的几十年，沃尔特·米歇尔和他的同事们又重新观察当时的参与者在后来的生活中表现如何。令人惊讶的是，当时等到两颗糖的孩子长大后比其他人更成功，甚至他们的学术能力评估测试成绩也更高。如果棉花糖实验测试的是意志力，那么这就是一个强有力的证据，证明了学习自我控制可以对一个人的生活有多大的影响。但是，如果测试是关于意愿，而不是预期，那么这就体现了一个完全不同的，也许更凄美的故事。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; 
&lt;p&gt;&lt;span&gt;罗切斯特大学的一组研究者最近研究了先验经验在棉花糖实验中是如何影响人们的行为的。在提到棉花糖之前，实验中的孩子们先进行了一个艺术项目。实验者给了他们一些平常的艺术品，并承诺很快会有更好的东西给他们。但是，他们并不知道，孩子们被分为两组。其中一组的实验者很诚信，返回时履行承诺，带来了更好的艺术品。而另一组的实验者并未信守承诺，回来时只给孩子们一个道歉，什么都没带回来。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;艺术项目完成后，孩子们接下来就去参加标准棉花糖实验。在这个实验中，之前认为实验者是不可靠的孩子更可能在大人回来之前就吃掉棉花糖，失去获得第二颗糖的机会。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在棉花糖实验中失败，并在以后的生活也没那么成功的人可能跟缺乏毅力没什么关系。可能是因为当时那些孩子认为大人是不可靠的：他们说的话不能相信，他们离开的时间长度也是随意的。学习自我控制是一个重要的问题，但在一个成年人始终能信赖的环境中成长也是同样重要的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;机械复制时代的先验&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;10&quot;&gt;
&lt;p&gt;&lt;span&gt;这就好像有人要买好几份同样的晨报来确保报纸上说的是真的。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;——路德维希•维特根斯坦&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;他仔细看他所读的内容，因为那是他要写的东西。他很认真学习他所学的内容，因为那是他将会懂得的东西。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;——安妮•迪拉德&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;正如贝叶斯法则告诉我们的，做出准确预测的最好方法就是准确地了解你所预测的事情。这就是为什么我们能很好地预测人类的寿命，但是当被问及预测法老的统治时间时却不尽如人意。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;作为贝叶斯法则的一种好方法，它以正确的比例表现世界——具有充分合理的先验，并适当校准。总的来说，对于人类和其他动物来说，这种情况是自然发生的。通常，当有什么东西使我们感到惊奇时，它应该让我们吃惊，而当它不应该让我们吃惊的时候，它就不会。即使我们所积累的偏见不是客观正确的，这些偏见通常还是会合理地反映我们所生活的世界的特定部分。例如，生活在沙漠气候中的人可能高估了世界上的沙量，而生活在极地的人可能高估了雪的总量。但他们都能很好地适应自己的生态环境。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;然而，当一个物种学会使用语言时，一切就开始瓦解。我们所谈论的并不是我们所经历的事情——我们主要谈论的是有趣的事情，而这些事往往也是不寻常的。根据其定义，事件总是或多或少地在其适当的频率发生，但语言并不完全是这样。任何经历过蛇咬伤或雷击的人，都会在他们余下的生命中复述那些奇异的故事。这些故事是如此不寻常，因此会被人不断谈起。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;之后，在与他人沟通和保持准确的先验世界之间有一种奇怪的压力。当人们谈论感兴趣的事或说一些他们认为听众也会感兴趣的故事时，就偏离了我们的经验统计。这使得经验统计很难保持适当的先验分布。而随着印刷术、新闻和社交媒体的发展，这种挑战会不断增加，并使我们人类这个物种能够机械地传播语言。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;想想你见过多少次失事的飞机或汽车。你完全可能看过以下某个场景——失事的汽车可能就在你旁边的道路上，而飞机坠毁可能发生在另一个大陆，这些消息都是通过互联网或电视传输给你的。例如，在美国，从2000 年起到现在，在商业飞机上失去生命的总人数不足以填满卡耐基音乐厅，甚至一半都没有。相比之下，美国在同一时间段死于车祸的人数就超过了怀俄明州的全部人口。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;简单地说，媒体对事件的报道并不与其在世界上发生的频率相符。社会学家巴里·格拉斯纳指出，在20 世纪90 年代美国的谋杀率下降了20%，然而在那段时间里，美国新闻中所报道的枪支暴力事件却增加了600%。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如果你想成为一个具有准确直觉的贝叶斯主义者——如果你想自然地做出准确的预测，而不必考虑什么样的预测规则是适当的，你就需要保护你的先验。相反，这可能意味着要关闭消息来源渠道。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;∑编辑 | Gemini&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;__bg_gif&quot; data-backh=&quot;222&quot; data-backw=&quot;400&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/951TjTgiabkxticOZ0wU6P1wUrXXVzVVUpiaOV3PWHITXENP3ELfOCQIO7Kia05kuIia0TFEuKbc60QZfuB2csicWe4w/640?wx_fmt=gif&quot; data-ratio=&quot;0.555&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/951TjTgiabkxticOZ0wU6P1wUrXXVzVVUpiaOV3PWHITXENP3ELfOCQIO7Kia05kuIia0TFEuKbc60QZfuB2csicWe4w/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;400&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;粉丝福利&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;送书！&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;143&quot; data-backw=&quot;113&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkymKiawEBibkSJiaNJ0qFH5q5oxZ6ic9At9iapD7UVKAAeg92xR6pjtyUjNzFuhpftiaLZYvxY6ZR9kYuKg/640?wx_fmt=png&quot; data-copyright=&quot;0&quot; data-cropselx1=&quot;0&quot; data-cropselx2=&quot;558&quot; data-cropsely1=&quot;0&quot; data-cropsely2=&quot;707&quot; data-ratio=&quot;1.40234375&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkxMuY8wyjZdlFNVMfLAUVbsrGEcbk8eNLLnTdJ6HWQHTdeC75m3cHd8xHv9AoCAUR7Cpd7VOwvQnQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1280&quot; width=&quot;558px&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;想获得此书，&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;文章底部留言，&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;strong&gt;&lt;span&gt;留言点赞前四名的粉丝（24小时计），&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;免费获得此书！&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;__bg_gif&quot; data-backh=&quot;172&quot; data-backw=&quot;480&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/951TjTgiabkxN5SJPzhu6icTXrIpMZqSdFzG0y6ib1c9enWGK3GxfHTRIN7ich2kzqepNvMHfktp4Ir88ibolsDBuhQ/640?wx_fmt=gif&quot; data-ratio=&quot;0.35833333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/951TjTgiabkxN5SJPzhu6icTXrIpMZqSdFzG0y6ib1c9enWGK3GxfHTRIN7ich2kzqepNvMHfktp4Ir88ibolsDBuhQ/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;480&quot; width=&quot;auto&quot; /&gt;&lt;/p&gt;

</description>
<pubDate>Thu, 12 Jul 2018 14:38:03 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/TacNBf37xX</dc:identifier>
</item>
<item>
<title>想转行人工智能，机会来了！！！</title>
<link>http://www.jintiankansha.me/t/AUv6dvJjdF</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/AUv6dvJjdF</guid>
<description>&lt;p data-mpa-powered-by=&quot;yiban.io&quot;&gt;&lt;strong&gt;&lt;span&gt;一个坏消息：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2018年1月 教育部印发的《普通高中课程方案和语文等学科课程标准》新加入了数据结构、人工智能、开源硬件设计等 AI 相关的课程。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这意味着职场新人和准备找工作的同学们，为了在今后十年内不被淘汰，你们要补课了，从初中开始。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;一个好消息：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;人工智能尖端人才远远不能满足需求。行业风口的人工智能，在中国人才缺口将超过500万人，而中国人工智能人才数量目前只有5万（数据来自工信部教育考试中心）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;并且目前岗位溢价相当严重，2017年人工智能在互联网岗位薪酬中位列第三，月薪20.1k，如果按照普遍的16月薪酬计算，那么人工智能在2017年一年的薪酬就是2.01*16=32.16万。那么再来看一组2018的薪酬数据： &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6058890147225368&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/4I1d5nkX8941l91Oibjp6yWFsDogPKGMXoqiacL7UgDtUjVyeaU9PUu8xIvsFFQSLNDgv9diagdjrTHbicc2yETXag/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;883&quot; width=&quot;auto&quot; /&gt; &lt;/p&gt;
&lt;p&gt;&lt;span&gt;所以如果你对自己的专业/工作不满意，现在正是进入人工智能领域学习就业/转业的最佳时机。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在面对众多的数学知识和编程知识里，自学会让大家耗费大量的时间金钱。因此，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;&lt;strong&gt;云&lt;strong&gt;&lt;strong&gt;博士&lt;/strong&gt;&lt;/strong&gt;（美国哈佛大学博士后 &amp;amp;&amp;amp; 浙江大学博士）领衔的人工智能博士团队&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;开发推出了人工智能&lt;/span&gt;&lt;span&gt;&lt;strong&gt;机器学习365天特训营（第二期）&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;课程。（扫描最底部二维码联系助教）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;310&quot; data-backw=&quot;558&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/4I1d5nkX896uAibiaNJG7nukribQhjuZmEwfGzeS2m0B3GSvevicy7xDk8bb5CibbAmXUSs2VRYFCt3Pqictk5zoRfmA/0?wx_fmt=png&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5548098434004475&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/4I1d5nkX896uAibiaNJG7nukribQhjuZmEwfGzeS2m0B3GSvevicy7xDk8bb5CibbAmXUSs2VRYFCt3Pqictk5zoRfmA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;894&quot; width=&quot;415px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为了保证大家的学习效果和就业情况&lt;/span&gt;&lt;span&gt;，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;幂次学院提供4项课程服务&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;，从发展历程、概念、基本名词、术语、评估方法讲起，到算法模型与实战演练：&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;1、名校大牛讲师授课：&lt;/strong&gt;&lt;strong&gt;&lt;strong&gt;云博士（美国哈佛大学博士后 &amp;amp;&amp;amp; 浙江大学博士）授课&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;；&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; 
&lt;p&gt;&lt;span&gt;&lt;strong&gt;2、3&lt;/strong&gt;&lt;strong&gt;65天的系统学习周期&lt;/strong&gt;&lt;strong&gt;：直播学习，&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;4年内随时随地回看&lt;strong&gt;&lt;span&gt;直播&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;，在线答疑；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;3、优质的答疑服务：&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;全天24小时课程问答与社群交流服务，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;让你的每一个问题都能够得到解答&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;，课程资料随时下载。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;4、颁发培训结业证书：&lt;/strong&gt;通过幂&lt;/span&gt;&lt;/span&gt;&lt;span&gt;次学院的阶段测试和毕业测试，并颁发幂次学院人工智能培训结业证书。&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;strong&gt;合计&lt;/strong&gt;365+天，每周两次直播&lt;/strong&gt;&lt;span&gt;&lt;span&gt;，&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;365天130+小时（理论+6个企业级项目实战）&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;课&lt;/span&gt;&lt;span&gt;程&lt;/span&gt;&lt;span&gt;（讲师直播答疑，课程7*24小时问答服务，学院社群7*24小时交流，课程资料随时下载）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;购买课程另赠送2门辅助课程：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1. &lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;现在报名&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;免费赠送售价899元&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;的 机器学习之Python编程基础与数据分析 课程，课程内容由&lt;/span&gt;&lt;span&gt;&lt;strong&gt;清华大学python大牛&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;主讲，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;课程内容&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;包括：python基础，python数据分析，python机器学习基础与python在机器学习中的实践案例。&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2. &lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;现在报名&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;免费赠送售价899元&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;的 人工智能数学基础8天集训营 课程，由&lt;/span&gt;&lt;span&gt;&lt;strong&gt;中国科学院计算技术研究所博士团队&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;主讲，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;课程内容&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;包括：矩阵论基础，概率与信息论，数值计算三部分&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5102803738317757&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/4I1d5nkX894Kar3aI8C9PoBa0jB21ev4ZywEDJLDznI2UicZ4fjWUAlabF9yUUJDQUHdNRPCMtAIth8d2aFpHAA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1070&quot; width=&quot;668px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;助力您解决人工智能学习中所需要用到的数学知识、Python编程知识。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;立即开始体系化学习，所有知识一步到位！&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;直播 + 回放：合计365+天，每周二19：00-20：00，20&lt;strong&gt;&lt;span&gt;：00-21：00&lt;/span&gt;&lt;/strong&gt;开课，直播回放4年内随时随地回看。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;strong&gt;讲师团队介绍：&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;strong&gt;云博士：美国哈佛大学大数据分析方向博士后，&lt;strong&gt;&lt;strong&gt;浙江大学计算机科学与技术专业博士，曾任&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;华为高级软件工程师/项目经理&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;发明专利多项，软件著作权多项，国际重要期刊论文数十篇，国家及省部级项目多项，横向项目数十项。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;张&lt;strong&gt;&lt;strong&gt;博士：&lt;/strong&gt;&lt;/strong&gt;中国科学院计算技术研究所机器学习方向博士&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;专注于人机交互、机器学习等领域研究。曾在国内外知名会议期刊发表多篇论文，并荣获人工智能领域会议“最佳论文提名奖”，目前拥有国家发明专利2项、软件著作权1项。拥有机器学习、数据挖掘领域实战经验，曾参与项目：&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;1、面向病症的多模态在线预警方法研究—国家自然科学基金;&lt;br /&gt;2、基于人机交互技术的安全驾驶映射系统—国家国际科技合作专项;&lt;br /&gt;3、散发性病症风险基因图谱与预警评估方法研究—北京市科学技术委员会北京脑科学研究项目;&lt;br /&gt;4、广东省大数据科学中心项目“基于多模态大数据的复杂疾病临床诊断标准及应用”—广东省科技计划项目NSFC等国家级项目。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;李金老师：清华大学机器学习方向本硕双清华毕业生，阿里巴巴机器学习方向算法工程师&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;研究方向为：推荐系统，计算机视觉，自然语言处理，深度学习等，在TNNLS，PR等杂志上发表过多篇论文，著有《自学Python—编程基础科学计算及数据分析》一书，P&lt;/span&gt;&lt;span&gt;ython笔记3K+Star，知乎python及机器学习板块12K+ zan，幂次学院签约讲师。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;附：机器学习365天特训营 - 课程大纲：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;第一部分 基础篇&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第1章&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.1 引言&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.2 基本术语&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.3 假设空间&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.4 归纳偏好&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.5 发展历程&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.6 应用现状&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第2章 模型评估与选择&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.1 经验误差与过拟合&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2 评估方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2.1 留出法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2.2 交叉验证法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2.3 自助法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2.4 调参与最终模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.3 性能度量&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.3.1 错误率与精度&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.3.2 查准率、查全率与F1&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.3.3 ROC与AUC&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.3.4 代价敏感错误率与代价曲线&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.4 比较检验&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.4.1 假设检验&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;2.4.2 交叉验证t检验&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.4.3 McNemar检验&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.4.4 Friedman检验与后续检验&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.5 偏差与方差&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第3章 线性模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.1 基本形式&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.2 线性回归&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.3 对数几率回归&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.4 线性判别分析&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.5 多分类学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.6 类别不平衡问题&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第4章 决策树&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.1 基本流程&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.2 划分选择&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.2.1 信息增益&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.2.2 增益率&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.2.3 基尼指数&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.3 剪枝处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.3.1 预剪枝&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.3.2 后剪枝&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.4 连续与缺失值&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.4.1 连续值处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.4.2 缺失值处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.5 多变量决策树&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第5章 神经网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.1 神经元模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.2 感知机与多层网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.3 误差逆传播算法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.4 全局最小与局部极小&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5 其他常见神经网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5.1 RBF网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5.2 ART网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5.3 SOM网络&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;5.5.4 级联相关网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5.5 Elman网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5.6 Boltzmann机&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.6 深度学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第6章 支持向量机&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6.1 间隔与支持向量&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6.2 对偶问题&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6.3 核函数&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6.4 软间隔与正则化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6.5 支持向量回归&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6.6 核方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第7章 贝叶斯分类器&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.1 贝叶斯决策论&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.2 极大似然估计&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.3 朴素贝叶斯分类器&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.4 半朴素贝叶斯分类器&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.5 贝叶斯网&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.5.1 结构&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.5.2 学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.5.3 推断&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.6 EM算法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第8章 集成学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.1 个体与集成&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.2 Boosting&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.3 Bagging与随机森林&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.3.1 Bagging&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.3.2 随机森林&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.4 结合策略&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.4.1 平均法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.4.2 投票法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.4.3 学习法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.5 多样性&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.5.1 误差--分歧分解&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;8.5.2 多样性度量&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.5.3 多样性增强&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第9章 聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.1 聚类任务&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.2 性能度量&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.3 距离计算&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.4 原型聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.4.1 k均值算法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.4.2 学习向量量化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.4.3 高斯混合聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.5 密度聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.6 层次聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第10章 降维与度量学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.1 k近邻学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.2 低维嵌入&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.3 主成分分析&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.4 核化线性降维&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.5 流形学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.5.1 等度量映射&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.5.2 局部线性嵌入&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.6 度量学习&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;第二部分 进阶篇&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第11章 特征选择与稀疏学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.1 子集搜索与评价&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.2 过滤式选择&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.3 包裹式选择&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.4 嵌入式选择与L_1正则化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.5 稀疏表示与字典学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.6 压缩感知&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第12章 计算学习理论&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.1 基础知识&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.2 PAC学习&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;12.3 有限假设空间&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.3.1 可分情形&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.3.2 不可分情形&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.4 VC维&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.5 Rademacher复杂度&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.6 稳定性&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第13章 半监督学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;13.1 未标记样本&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;13.2 生成式方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;13.3 半监督SVM&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;13.4 图半监督学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;13.5 基于分歧的方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;13.6 半监督聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第14章 概率图模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.1 隐马尔可夫模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.2 马尔可夫随机场&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.3 条件随机场&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.4 学习与推断&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.4.1 变量消去&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.4.2 信念传播&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.5 近似推断&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.5.1 MCMC采样&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.5.2 变分推断&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.6 话题模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第15章 规则学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.1 基本概念&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.2 序贯覆盖&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.3 剪枝优化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.4 一阶规则学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.5 归纳逻辑程序设计&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.5.1 最小一般泛化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.5.2 逆归结&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第16章 强化学习&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;16.1 任务与奖赏&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.2 K-摇臂赌博机&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.2.1 探索与利用&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.2.2 ε-贪心&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.2.3 Softmax&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.3 有模型学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.3.1 策略评估&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.3.2 策略改进&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.3.3 策略迭代与值迭代&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.4 免模型学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.4.1 蒙特卡罗强化学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.4.2 时序差分学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.5 值函数近似&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.6 模仿学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.6.1 直接模仿学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.6.2 逆强化学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第17章 增量学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.1 被动攻击学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.1.1 梯度下降量的抑制&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.1.2 被动攻击分类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.1.3 被动攻击回归&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.2 适应正则化学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.2.1 参数分布的学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.2.2 适应正则化分类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.2.3 适应正则化回归&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.3 增量随机森林&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第18章 迁移学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.1 迁移学习简介&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.1.1 什么是迁移学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.1.2 迁移学习VS传统机器学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.1.3 应用领域&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2 迁移学习的分类方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2.1 按迁移情境&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;18.2.2 按特征空间&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2.3 按迁移方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.3 代表性研究成果&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2.1 域适配问题&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2.2 多源迁移学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2.3 深度迁移学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第19章 主动学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;19.1 主动学习简介&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;19.2 主动学习思想&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;19.3 主动学习VS半监督学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;19.4 主动学习VS Self-Learning&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第20章 多任务学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;20.1 使用最小二乘回归的多任务学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;20.2 使用最小二乘概率分类器的多任务学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;20.3 多次维输出函数的学习&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;第三部分 实战篇&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第21章 机器学习应用场景介绍&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;21.1 机器学习经典应用场景&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;21.2 头脑风暴：挖掘身边的应用场景&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第22章 数据预处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;22.1 数据降噪&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;22.2 数据分割&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第23章 特征提取&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;23.1 时域特征&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;23.2 频域特征&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;23.3 自动特征提取&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第24章 机器学习方法应用&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;24.1 应用机器学习方法之前的处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;24.2 使用机器学习分类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;24.3 机器学习调参&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;24.4 分类结果展示&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;&lt;span&gt;&lt;span&gt;第25章&lt;/span&gt;&lt;/span&gt; - 机器学习企业级项目实战&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;25.1 O2O优惠券使用预测&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;25.2 鲍鱼年龄预测&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;25.3 机器恶意流量识别&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;25.4 根据用户轨迹进行精准营销&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;25.5 根据搜狗输入进行用户画像&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;25.6 美国债务违约预测&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;报名费用及优惠详情：&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;br /&gt;原价16800元：&lt;/span&gt;&lt;span&gt;&lt;strong&gt;折后特惠价：2999元。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;208&quot; data-backw=&quot;208&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/4I1d5nkX896cbVeGItvDKsPGhluU4vhO5vfooqXr8Ik9XssHL935SFKmmic8xyE9nqRNY25icibEnnWfsbQjicZ56A/640?wx_fmt=jpeg&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/4I1d5nkX896cbVeGItvDKsPGhluU4vhO5vfooqXr8Ik9XssHL935SFKmmic8xyE9nqRNY25icibEnnWfsbQjicZ56A/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;800&quot; width=&quot;210px&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;长按二维码 - 咨询助教微信&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/4I1d5nkX896uAibiaNJG7nukribQhjuZmEw8bgJwliarCb5LUNZOCCPmibtVFgaaicS5VFtiaOqribTNyr0gfqxMcX6Zjg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;500&quot; width=&quot;226px&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;长按二维码 - 进入课程详情&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Wed, 11 Jul 2018 03:48:42 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/AUv6dvJjdF</dc:identifier>
</item>
</channel>
</rss>