<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fmathalg-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/mathalg-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>算法与数学之美</title>
<link>http://www.jintiankansha.me/column/c9dZ5TM2aS</link>
<description>算法与数学之美 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>一个女程序员征男友的需求说明书</title>
<link>http://www.jintiankansha.me/t/wcVXPPbqpm</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/wcVXPPbqpm</guid>
<description>&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;回复1：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;项目完成后强烈要求其公布开发文档、测试文档和维护文档。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;回复2：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;不合适你直接把人家GOTO到：不会联系你，十分抱歉，希望你会有更好的缘分！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;&lt;strong&gt;&lt;span&gt;回复3:&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;寻男友过程一定要遵照CMM5规范来执行,争取这个项目要成为CMM5模范工程!&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;现在成立CMM评审小组,愿意参加的报名…..&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;回复4：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;你的文档不能通过ISO2002-SW-CMM1，项目不能通过，去问问技术总监吧！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;回复5：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;CMM小组一至决定需求不通过，完全不能对需求方所提供资料进行分析（比如说：需求方条件，照片等），所以这个评审失败。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;回复6:&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;强烈要求公布开发文档、测试文档和维护文档、如果可能也公开源代码。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;回复7:&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;//本程序在Microsoft VisualC++ .NET 55601-652-0000007-18074下编译通过&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;//版本1.0 共享软件(C)版权所有 2003&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;BOOL IfYourWantToFindSomeOne(){&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;do{&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;If(Has_Photos()){ //有照片&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;if(身高==My.男友.身高.180CM&amp;amp;&amp;amp;　相貌==My.男友.相貌 &amp;amp;&amp;amp; OtherConditions()){&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;return TRUE;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;}&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;else{&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;return FALSE;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;}&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;}&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;else{&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;if(That_Man_Is_Good_Man()){ //好人还是坏人都很难说&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;return TRUE;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;}&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;else{&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Your_Meet_A_Bad_Man();//坏人多多，还是在身边找吧&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;return FALSE;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;}&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;}&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;}&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;while(_404_No_Found_Boy_Friend() &amp;amp;&amp;amp;My.精力– &amp;amp;&amp;amp; My.信心–)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;}&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;回复8:&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这个需求太简单了，说明你没有认真做需求分析，估计你的需求在你的“设计阶段”还会变更。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;回复9：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;TO 7:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;程序错误：at line 18 of FindFriend.cpp:my.精力 no initlizeted&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;程序错误：at line 18 of FindFriend.cpp:my.信心 to initlizeted&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;呵呵可能是个warning&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;不过很危险哟搞不好会系统崩溃哟&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;回复10：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;流程过于简单，异常分支考虑不严密，另外需求分析，对立项的目的和项目风险估算不够。不能算一份合格的需求说明。&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 10 Sep 2018 04:49:07 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/wcVXPPbqpm</dc:identifier>
</item>
<item>
<title>如何优雅地从四个方面加深对深度学习的理解</title>
<link>http://www.jintiankansha.me/t/tG8EBnPeSk</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/tG8EBnPeSk</guid>
<description>&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; background-color:=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;在今年的 ICML 上，深度学习理论成为最大的主题之一。会议第一天，Sanjeev Arora 就展开了关于深度学习理论理解的教程，并从四个方面分析了关于该领域的研究：非凸优化、超参数和泛化、深度的意义以及生成模型。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; box-sizing:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;534&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp6Ybbr97W3HMw9iaR5RHE3hBsWgyzZlNz03qicp4EQeDuQIiacMT5DEbJA/0?wx_fmt=png&quot; data-ratio=&quot;0.64765625&quot; data-type=&quot;png&quot; data-w=&quot;1280&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp6Ybbr97W3HMw9iaR5RHE3hBsWgyzZlNz03qicp4EQeDuQIiacMT5DEbJA/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;2017 年 12 月 NIPS 的 Test-of-Time Award 颁奖典礼上，Ali Rahimi 这样呼吁人们加深对深度学习的理解：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;我希望生活在这样的一个世界，它的系统是建立在严谨可靠而且可证实的知识之上，而非炼金术。[……] 简单的实验和定理是帮助理解复杂大现象的基石。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Ali 的目标不是解散各个领域，而是「展开对话」。这个目标已经实现了，但对于目前的深度学习应被视为炼金术还是工程或科学，人们仍存在分歧。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;7 个月后，在斯德哥尔摩举行的国际机器学习会议 (ICML) 上，机器学习社区又聚焦了这个问题。此次大会与会者有 5000 多名，并累计发表论文 629 篇，这是基础机器学习研究的「年度大戏」。而深度学习理论已成为此次会议的最大主题之一。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;619&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp1dIWuDmhoB7L7gpic4EteZT5IqDf9zib9vhQPialJNYphHicLMTkRXkKlg/0?wx_fmt=png&quot; data-ratio=&quot;0.75&quot; data-type=&quot;png&quot; data-w=&quot;1200&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp1dIWuDmhoB7L7gpic4EteZT5IqDf9zib9vhQPialJNYphHicLMTkRXkKlg/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;会议第一天，最大的房间里就挤满了机器学习相关人员，他们准备聆听 Sanjeev Arora 关于深度学习理论理解的教程。这位普林斯顿大学计算机科学教授在演讲中总结了目前的深度学习理论研究领域，并将其分成四类：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;非凸优化：如何理解与深度神经网络相关的高度非凸损失函数？为什么随机梯度下降法会收敛？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;超参数和泛化：在经典统计理论中，为什么泛化依赖于参数的数量而非深度学习？存在其它较好的泛化方法吗？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;深度的意义：深度如何帮助神经网络收敛？深度和泛化之间的联系是什么？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;生成模型：为什么生成对抗网络（GAN）效果非常好？有什么理论特性能使模型稳定或者避免模式崩溃？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;在这一系列的文章中，我们将根据最新的论文（尤其是 ICML2018 的论文），帮助大家直观理解这四个方面。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;第一篇文章将重点讨论深度网络的非凸优化问题。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt; 非凸优化 &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;416&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpfuicL2hzO7alSeeyoSvrSoLF6RJTnuBicrFh1E1eibZae2dCDBXEJkE4w/0?wx_fmt=png&quot; data-ratio=&quot;0.5035405192761605&quot; data-type=&quot;png&quot; data-w=&quot;1271&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpfuicL2hzO7alSeeyoSvrSoLF6RJTnuBicrFh1E1eibZae2dCDBXEJkE4w/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;我敢打赌，你们很多人都曾尝试过训练自己的「深度网络」，结果却因为无法让它发挥作用而陷入自我怀疑。这不是你的错。我认为都是梯度下降的错。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Ali Rahimi 在 NIPS 演讲中曾说，随机梯度下降 (SGD) 的确是深度学习的基石，它应该解决高度非凸优化问题。理解它何时起作用，以及为什么起作用，是我们在深度学习的基本理论中一定会提出的最基本问题之一。具体来说，对于深度神经网络的非凸优化研究可以分为两个问题：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;损失函数是什么样的？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;SGD 为什么收敛？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt; 损失函数是什么样的？ &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;如果让你想象一个全局最小值，很可能你脑海中出现的第一幅图是这样的：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;274&quot; data-backw=&quot;436&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpjLVcBpDV1mVZQBBS933UEh2A6Za0Vn4HKPkiaMOtUroOjuP4psTvxHA/0?wx_fmt=png&quot; data-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpYosC1qjoB8iaobVth7kTklNOePv4I2T6cibO3kJ5zuldJ4uMKvzWAQyQ/0?wx_fmt=jpeg&quot; data-ratio=&quot;0.6284403669724771&quot; data-type=&quot;jpeg&quot; data-w=&quot;436&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpYosC1qjoB8iaobVth7kTklNOePv4I2T6cibO3kJ5zuldJ4uMKvzWAQyQ/640?wx_fmt=jpeg&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;二维世界中的全局最小值附近，函数是严格凸的（这意味着 hessian 矩阵的两个特征值都是正数）。但在一个有着数十亿参数的世界里，就像在深度学习中，全局最小值附近的方向都不平坦的可能性有多大？或者 hessian 中一个为零（或近似为零）的特征值都没有的概率有多大？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Sanjeev Arora 在教程中写的第一个评论是：损失函数的可能方向数量会随着维度的增长呈指数增长。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;218&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpGdVWAVt0fp4EicEZ7XLfL5dV5z385g3NqW1mP4OjjZGy3ZKKv7Qib9iag/0?wx_fmt=png&quot; data-ratio=&quot;0.26450344149459193&quot; data-type=&quot;png&quot; data-w=&quot;1017&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpGdVWAVt0fp4EicEZ7XLfL5dV5z385g3NqW1mP4OjjZGy3ZKKv7Qib9iag/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;直观上看，全局最小值似乎不是一个点而是一个连接管（connected manifold）。这意味着如果找到了全局最小值，你就能够穿过一条平坦的路径，在这条道路上，所有的点都是最小值。海德堡大学的一个研究团队在论文《Essentially No Barriers in Neural Network Energy Landscape》中证明了这一点。他们提出了一个更常规的说法，即任何两个全局最小值都可以通过一条平坦的路径连接。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;628&quot; data-backw=&quot;674&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpLL8vQpKoC4DluRJFwuQeLxTic5lm9YjpulFue2IUHy3Ly3GqaK3H0JA/0?wx_fmt=png&quot; data-ratio=&quot;0.9317507418397626&quot; data-type=&quot;png&quot; data-w=&quot;674&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpLL8vQpKoC4DluRJFwuQeLxTic5lm9YjpulFue2IUHy3Ly3GqaK3H0JA/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  在 MNIST 上的 CNN 或在 PTB 上的 RNN 已经是这样的情况，但是该项研究将这种认知扩展到了在更高级的数据集（CIFAR10 和 CIFAR100）上训练的更大网络（一些 DenseNet 和 ResNet）上。为了找到这条路径，他们使用了一种来自分子统计力学的启发式方法，叫做 AutoNEB。其思想是在两个极小值之间创建一个初始路径（例如线性），并在该路径上设置中心点。然后迭代地调整中心点的位置，以最小化每个中心点的损失，并确保中心点之间的距离保持不变（通过用弹簧建模中心点之间的空间）。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;虽然他们没有从理论上证明这个结果，但他们对为什么存在这样的路径给出了一些直观的解释：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;如果我们扰乱单个参数，比如添加一个小常数，然后让其它部分去自适应这种变化，仍然可以使损失最小化。因此可以认为，通过微调，无数其它参数可以「弥补」强加在一个参数上的改变。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;因此，本文的结果可以帮助我们通过超参数化和高维空间，以不同的方式看待极小值。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;通俗来说，当考虑神经网络的损失函数时，你应该牢记一个给定的点周围可能有非常多的方向。由此得出另一个结论，鞍点肯定比局部最小值多得多：在给定的关键点上，在数十亿个可能的方向中，很可能会找到一个向下的方向（如果不是在全局最小值上）。这种认知在 NIPS 2014 年发表的论文《Identifying and attacking the saddle point problem in high-dimensional non-convex optimization》中被严格规范化，并得到了实证证明。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;为什么 SGD 收敛（或不收敛）？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;深度神经网络优化的第二个重要问题与 SGD 的收敛性有关。虽然这种算法长期以来被看做是一种快速的近似版梯度下降，但我们现在可以证明 SGD 实际上收敛于更好、更一般的最小值。但我们能否将其规范化并定量地解释 SGD 脱离局部极小值或鞍点的能力？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;SGD 修改了损失函数&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;论文《An Alternative View: When Does SGD Escape Local Minima?》表明，实施 SGD 相当于在卷积（所以平滑）的损失函数上进行常规梯度下降。根据这一观点并在某些假设下，他们证明了 SGD 将设法脱离局部最小值，并收敛到全局最小值附近的一个小区域。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt; SGD 由随机微分方程控制 &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;连续 SGD 彻底改变了我对这个算法的看法。在 ICML 2018 关于非凸优化的研讨会上，Yoshua Bengio 在他关于随机梯度下降、平滑和泛化的演讲中提出了这个想法。SGD 不是在损失函数上移动一个点，而是一片点云或者说一个分布。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;532&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpVp9Fh7rOaLEfLv0gnnQpxHacwrTOV7Xd2ibaAJAmczu0RmxC6yvmu3g/0?wx_fmt=png&quot; data-ratio=&quot;0.644747393744988&quot; data-type=&quot;png&quot; data-w=&quot;1247&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpVp9Fh7rOaLEfLv0gnnQpxHacwrTOV7Xd2ibaAJAmczu0RmxC6yvmu3g/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;幻灯片摘自 Y. Bengio 在 ICML 2018 发表的演讲。他提出用分布（或点云）代替点来看待 SGD。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;这个点云的大小（即相关分布的方差）与 learning_rate / batch_size 因子成正比。Pratik Chaudhari 和 Stefano Soatto 在论文《Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks》中证明了这一点。这个公式非常直观：较低的 batch size 意味着梯度非常混乱（因为要在数据集一个非常小的子集上计算），高学习率意味着步骤混乱。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;将 SGD 视为随时间变化的分布可以得出：控制下降的方程现在是随机偏微分方程。更准确地说，在某些假设下，论文表明控制方程实际上是一个 Fokker-Planck 方程。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-backh=&quot;588&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpNKIctvKJ5QUYGIibribA3R1XCRBRK3JOlKWGa4KV3kZZt8FYb1I9OmBw/0?wx_fmt=png&quot; data-ratio=&quot;0.713&quot; data-type=&quot;png&quot; data-w=&quot;1000&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpNKIctvKJ5QUYGIibribA3R1XCRBRK3JOlKWGa4KV3kZZt8FYb1I9OmBw/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;幻灯片摘自 P. Chaudhari 和 S. Soatto 在 ICML 2018 发表的演讲——《High-dimensional Geometry and Dynamics of Stochastic Gradient Descent for Deep Networks》。他们展示了如何从离散系统过渡到 Fokker-Plank 方程所描述的连续系统。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;在统计物理学中，这种类型的方程描述了暴露在曳力 (使分布推移，即改变平均值) 和随机力 (使分布扩散，即增加方差) 下的粒子的演化。在 SGD 中，曳力由真实梯度建模，而随机力则对应算法的内在噪声。正如上面的幻灯片所示，扩散项与温度项 T = 1 /β= learning_rate /(2 * batch_size) 成正比，这再次显示了该比值的重要性！&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;__bg_gif&quot; data-ratio=&quot;0.65&quot; data-type=&quot;gif&quot; data-w=&quot;200&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/ymzg67DoLHL0GAVyghIBeu2spTC1GJNplshR7jFFrYVEDRyqFgzPxth1ic3t5SlI8wx1x26CF7B0sgV3icyoJFLA/640?wx_fmt=gif&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Fokker-Planck 方程下分布的演化。它向左漂移，随时间扩散。图源：维基百科&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;通过这个框架，Chaudhari 和 Soatto 证明了我们的分布将单调地收敛于某个稳定的分布（从 KL 散度的意义来说）：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;197&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpkfAupiaScZjtY8P0KyUkcibmFr9S43kpPy4TXN6g91HwYPia6yFoIzZrg/0?wx_fmt=png&quot; data-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp77ic5V3v7dQJzqOWUGRpOic4htm7HNiaT0DHAZibLwkibyggphmBF6R0VDA/0?wx_fmt=jpeg&quot; data-ratio=&quot;0.23870220162224798&quot; data-type=&quot;jpeg&quot; data-w=&quot;863&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp77ic5V3v7dQJzqOWUGRpOic4htm7HNiaT0DHAZibLwkibyggphmBF6R0VDA/640?wx_fmt=jpeg&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Pratik Chaudhari 和 Stefano Soatto 论文的一个主要定理，证明了分布的单调会收敛到稳定状态（在 KL 散度意义中）。第二个方程表明，使 F 最小化相当于最小化某个潜在的ϕ以及扩大熵的分布（温度 1 /β控制的权衡）。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;在上面的定理中有几个有趣的观点：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;SGD 最小化的函数可以写成两项之和（Eq. 11）：潜在Φ和熵的分布。温度 1 /β控制这两项的权衡。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;潜在Φ只取决于数据和网络的架构（而非优化过程）。如果它等于损失函数，SGD 将收敛到全局最小值。然而, 本文表明这种情况比较少见。而如果知道Φ与损失函数的距离，你将可以知道 SGD 收敛的概率。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;最终分布的熵取决于 learning_rate/batch_size（温度）的比例。直观上看，熵与分布的大小有关，而高温会导致分布具有更大的方差，这意味着一个平坦的极小值。平坦极小值的泛化能力更好，这与高学习率和低 batch size 能得到更优最小值的经验是一致的。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;因此，将 SGD 看作是一个随时间变化的分布表明，在收敛性和泛化方面，learning_rate/batch_size 比每个独立的超参数更有意义。此外，它还引入了与收敛相关的网络潜力，为架构搜索提供了一个很好的度量。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt; 结论 &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;探索深度学习理论的过程可以分为两部分：首先，通过简单的模型和实验，建立起关于深度学习理论如何及其为什么起作用的认知，然后将这些理念以数学形式呈现，以帮助我们解释当前的结论并得到新的结果。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;在第一篇文章中，我们试图传达更多关于神经网络高维损失函数和 SGD 解说的直观认知，同时表明新的形式主义正在建立，目的是建立一个关于深层神经网络优化的真正数学理论。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;然而，虽然非凸优化是深度学习的基石并且拥有大量的层数和参数，但它取得的成功大部分源于其优秀的泛化能力。这将是下一篇文章将分享的内容。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.34&quot; data-type=&quot;png&quot; data-w=&quot;300&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpU34THcyJPz4XtAkBib0gUCJG9eB5pnHm9fHK3KzANrXenNeBLNslY4w/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Sanjeev Arora：印度裔美国理论计算机科学家，他以研究概率可检验证明，尤其是PCP定理而闻名。研究兴趣包括计算复杂度理论、计算随机性、概率可检验证明等。他于2018年2月被推选为美国国家科学院院士，目前是普林斯顿大学计算机科学系教授。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;∑编辑 | Gemini&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;来源 | 数盟&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.0437601296596435&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;617&quot; width=&quot;auto&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky7x6u1VxMVMia4MLibNzC2nrumY3zDflTsCeoM04M1BrkvPny8tsw6hYkIicUr42iarLmadL2x6JwV6A/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域，经采用我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 10 Sep 2018 04:49:06 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/tG8EBnPeSk</dc:identifier>
</item>
<item>
<title>世上描述宇宙最精确的科学理论</title>
<link>http://www.jintiankansha.me/t/McO1WtOiBO</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/McO1WtOiBO</guid>
<description>&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;宇宙万物是由什么构成的？它们又是如何结合在一起的？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;标准模型&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;是人类已知最精确的科学理论，在上个世纪的诺贝尔物理学奖中，超过四分之一都与标准模型有关。自建立以来，标准模型在过去的50年中经受住了重重严峻的考验，它描述了除引力之外的其它基本力以及组成所有物质的基本粒子。任何试图超越它的尝试，都必须付出巨大的努力，而且很多都已经失败了。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们周围的世界是由分子组成的，而分子又是由原子组成的。门捷列夫在19世纪60年代就已经认识到这一点，并将所有原子排列到周期表中。但是，元素周期表中含有的不同化学元素种类多达118种。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5348837209302325&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;860&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/Ikuml0ziaic9L1G97VkGPhwW0bqccONv5icvq6dJBq79Hqv9JwXpxar6j18FgiahU6SI1BSUCQfhVoDvkM5pnf57jw/640?wx_fmt=jpeg&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;○ &lt;/span&gt;&lt;span&gt;元素周期表包含了118个元素，这些元素还可以进一步被分割。&lt;/span&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;物理学家向来喜欢简洁，他们想要看到事物的本质、并揭开它们最基本的成分。到了1932年，科学家了解到所有的这些原子都由三种粒子——中子、质子和电子——构成的。中子和质子紧密地结合在原子核内，而质量更轻的电子则绕着原子核极速运动。于是，物理学家普朗克、玻尔、薛定谔、海森堡等人发展了一种新的科学——量子力学，以解释这种运动。粒子的数量一下子缩减到了3个，这似乎是个令人满意的止步之处。但它们是如何结合在一起的？带负电的电子和带正电的质子通过电磁力束缚在一起。但是所有的质子全挤在核中，它们所带的正电应该将它们有力地分开才对。而中性的中子并不能起到什么作用。那么将质子和中子结合在一起的是什么？或许有人会认为“这是神的旨意”。但是，这种情形即使对神来说也是极大的挑战，因为祂需要密切关注宇宙中的10⁸⁰个质子和中子，并按照祂的意志将它们结合在一起。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span&gt;大自然并不满足于它的粒子动物园只有这3种粒子。算上构成光的粒子——光子的话，其实是4种。而当安德森探测到从外太空击中地球的正电子（带正电的电子）时，4又变成了5。安德森所探测到的正电子正是狄拉克曾经预言过的反物质粒子。后来，5个又变成了6个，这是因为汤川秀树预言可以将原子核束缚在一起的π介子被发现了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;再后来，拉比发现了比电子重200倍的μ子，它的性质跟电子完全相同。这时候，这一数字已经上升到7了，似乎已经谈不上简洁了。然而，事情却远远没有结束。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;到了20世纪60年代，已经出现了数百个“基本”粒子了。排列整齐的元素周期表，被长长的重子（如质子和中子）、介子（如π介子）和轻子（如电子和中微子）列表代替——杂乱且无指导原则可言。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;br /&gt;现在我们知道，夸克有6种“味”，就像对应于冰淇淋中的香草、巧克力等不同味一样，夸克有上、下、奇、粲、底和顶。 1964年，盖尔曼和茨威格为我们揭示了“食谱”：混合搭配任意三个夸克可以构成重子。例如，原本我们以为是基本粒子的质子事实上是由更小的两个上夸克和一个下夸克结合而成的；中子则是由两个下夸克和一个上夸克组成的。一个夸克和一个反夸克则可结合成一个介子。一个π介子是一个上夸克或下夸克、与一个反上夸克或反下夸克的结合。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.8902116402116402&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;756&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/Ikuml0ziaic9L1G97VkGPhwW0bqccONv5icdxDCWAgX1fQq3ibe9nuA4t46Q6abxWZz40ZdR19Mb6dc8rlWicSV3yMg/640?wx_fmt=jpeg&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;○ &lt;/span&gt;&lt;span&gt;标准模型中的基本粒子：夸克、轻子（比如电子和中微子）和载力粒子（比如光子）。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;有趣的是，这些夸克彼此之间总是紧密相连，你永远不会看到一个单独的夸克或反夸克。负责传递夸克之间的力的粒子被称为胶子（就好比是光子之于电磁力），而描述夸克和胶子之间相互作用的理论被称为量子色动力学。这是标准模型中至关重要的一部分，但其背后的数学却是极其困难的，以至于相关问题被列为千禧年七大数学难题之一。物理学家一直在尽全力用它来进行计算，但仍处于学习如何使用的过程中。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;标准模型的另一个核心是“轻子模型”。1967年11月20日，温伯格在《物理评论快报》上发表的这篇标志性的论文：《轻子模型》，为高能粒子物理学在20世纪后半叶的发展指明了方向。在只有两页半纸的论文中（算上参考文献和致谢在内），温伯格优雅而简洁地书写了宇宙中最深层的秘密。他将我们熟悉的电磁力和会导致特定放射性衰变的“弱核力”统一在一起，并将它们描述为同一种力的不同方面。它结合了赋予基本粒子质量的希格斯机制。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.9885714285714285&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;700&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/Ikuml0ziaic9L1G97VkGPhwW0bqccONv5iccXEs8B924xWFr9mn9RUTNLNsQJJenRfExUnADlPwZX3b1WEUPwbAWA/640?wx_fmt=jpeg&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span class=&quot;&quot;&gt;○ &lt;/span&gt;&lt;span&gt;温伯格发表的论文：《轻子模型》。他的成果在当时无疑是革命性的，但却被忽略了许多年，如今，这篇论文每周至少会被引用三次。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;自那以后，标准模型预测了一次又一次的实验结果，包括发现几种夸克、以及传递弱核力的W和Z玻色子。虽然在20世纪60年代，人们曾忽略了中微子具有质量的可能性。但到20世纪90年代，中微子也被纳入了标准模型，只比其他的粒子晚了几十年加入这场粒子盛宴。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;2012年，标准模型走上巅峰，迎来了一次伟大的胜利。科学家苦苦追寻的希格斯粒子终于在粒子加速器中被找到了！这是标准模型的最后一块拼图。然而，这并不意味着结束，而是一个新的开始。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;物理学家一直在担心标准模型没有充分体现他们对简洁性的期望，担心其数学的自洽性，还有期待将引力也引入标准模型中的最终必要性。因此为了超越标准模型，物理学家已提出了很多的理论。它们都有着十分酷炫的名字，比如大统一理论、超对称和弦论等等。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;但可惜的是，这些所谓的超越标准模型的理论还没能成功地预测到任何新的实验现象。在50年后的今天，标准模型似乎还未到升级更新的时刻，这对标准模型来说是件值得庆祝的事，它仍然是迄今描述（几乎）万物最完美的理论。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;∑编辑 | Gemini&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;来&lt;/span&gt;&lt;span&gt;源 | 树脑智能科技&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7509529860228716&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;787&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkz1SRWmm2kJgtV2NTQtdSgtyl7nJbJm8xS78Td2LBbJQKKqyE54oaOO9upMribZagMKYJVBaEDyKtA/640?wx_fmt=jpeg&quot; width=&quot;auto&quot; /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域，经采用我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Sun, 09 Sep 2018 10:05:16 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/McO1WtOiBO</dc:identifier>
</item>
<item>
<title>诗人也出数学题，出的有趣又深刻</title>
<link>http://www.jintiankansha.me/t/rMayFkcg2m</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/rMayFkcg2m</guid>
<description>&lt;p&gt;&lt;span&gt;莱蒙托夫活了多大年纪呢?&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;由于莱蒙托夫一生喜爱数学，后来有人用他的生卒年代编了一道数学题:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; “伟大的俄国诗人莱蒙托夫是19世纪的人。他生于19世纪，也死于19世纪。根据下述条件，说出他生于哪一年？死于哪一年？ &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;  (1) 他诞生与逝世的年份，都是四个相同的阿拉伯数字组成，但排列位置不同；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;  (2 ) 他诞生的那一年，四个阿拉伯数字之和为14；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;  (3) 他逝世的年份，其阿拉伯数字的十位数是个位数的四倍。”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;可以这样来考虑：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;既然“生于19世纪，死于19世纪”，那么他生和死的年代的前两位数字一定是18；根据“他诞生的那一年， 四个阿拉伯数字之和为14。” 而百位和千位数字之和为1+8=9，可知个位和十位数字之和是14-9=5；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;由于“诞生和逝世的年份，都是四个相同的阿拉伯数字组成”，又由于“他逝世的年份，其阿拉伯数字的十位数是个位数的四倍。”可以得知，莱蒙托夫死于1841年，生于1814年,仅仅活了27岁！&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.6666666666666666&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/xxljFkocqpeE1zICricNlyxDESppWBLxAYv0DRVnecnEJ2ice9jmzHMFSdl5mXxiaX7c4duZickSsIgv9Z6VzLL2VA/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;俄国另一位大诗人贝涅吉克托夫也非常喜欢数学题，他曾写过一本数学题集。这本书中有些很有趣的数学题，请看其中的一道题:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;“卖鸡蛋的某妇人，派她三个女儿去市场出售她的90个鸡蛋。她给最聪明的大女儿10个鸡蛋，给了二女儿30个鸡蛋，小女儿50个鸡蛋。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;妇人对三个女儿说：‘你们先商量好售价，然后就不要让步。你们都得坚持同样的价钱，但是我希望我的大女儿运用她的智慧，即便是按照你们共同商定的价钱，仍能把她自已那10个鸡蛋卖得的钱数同二女儿卖掉30个鸡蛋的钱数相同，并教会二女儿把她那30个鸡蛋卖得的钱数同三女儿卖掉50个鸡蛋的钱数相同。注意，你们三个人所卖的总钱数和每次鸡蛋卖的钱数都要相同。还有,我希望你们卖鸡蛋时,90个鸡蛋不能低于90分钱。’&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;你说说，这三个女儿是怎样完成这个任务的?”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;诗人是这样来解决这个问题的：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;三个女儿在去市场的路上边走边商量。后来二女儿、三女儿都请大女儿出主意。大女儿想了想说：“妹妹们，咱们以前都是10个蛋10个蛋地卖，这次咱们不这样干了，改成7个蛋7个蛋地卖。每7个蛋一份，咱们给每一份订一个价钱，按照妈妈的嘱咐，咱们三个人都得遵守，一分钱也不让价，每份卖3分钱，你们说怎样?”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;二女儿说：“那可太便宜了。”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;大女儿说：“我们把7个一份按份出售的鸡蛋卖完后，提高剩下来的蛋的价钱呀！我已经注意到，今天市场上卖鸡蛋的除你我三人外，再没有别人卖了。因此，不会有人压低我们的价钱了。那么，剩下的这些蛋，只要有人急用，货又剩得不多了，价钱自然要上涨。咱们就是要在剩下的那几个蛋上赚回来。”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;三女儿问：“剩下那几个蛋卖什么价钱呢?”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;大女儿果断地说:“每个蛋卖9分。就这个价。急等鸡蛋下锅的买主是会出这个价钱的。”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;二女儿摇摇头说:“太贵了点。”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; “那有什么！”大女儿回答说，“可是我们7个份的鸡蛋卖的不是太便宜了吗?两者刚好抵消。”大家都同意了。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;到了市场，姐妹三人各找地方坐下来卖她们的鸡蛋。买东西的男男女女，看到鸡蛋如此便宜，都跑到三女儿那儿去买。三女儿把50个鸡蛋分成7个一份共分成七份， 每份3分共卖了21分，筐子里还剩下一个鸡蛋；二女儿有30个鸡蛋，7个一份共卖给了四个顾客，筐子里还剩下两个鸡蛋，卖了12分；大女儿则卖了一份7个蛋，卖了3分钱，还剩下了3个蛋。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这时，市场上赶来了一位女厨师，是奉主人之命来买鸡蛋的，她的任务是必须买到10个鸡蛋，因为主人的儿子特别喜欢吃鸡蛋，而他刚刚回来探亲。女厨师在市场上转来转去，可是鸡蛋都已卖光，卖鸡蛋的三个摊子上一共只剩下6个鸡蛋：一个摊子只有1个，另一个摊子只有2个，还有一个摊子只有3个。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;女厨师先跑到大女儿的摊子前问：“这三个鸡蛋卖多少钱?”大女儿回答：“每个鸡蛋9分钱。”女厨师瞪着大眼睛说:“你怎么啦?发疯啦?要这么多钱!”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;大女儿爱理不理地说:“随您的便，少一个钱也不卖，就剩这几个了。”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;女厨师又跑到只剩下2个鸡蛋的二女儿摊前问:“什么价钱?”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;二女儿说:“9分钱一个。言不二价。蛋都卖光了。”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;女厨师赌气又跑到三女儿摊前问:“你这个鸡蛋卖多少钱?”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;三女儿回答:“9分钱一个。”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;一点办法也没有，女厨师只好用高价买下了这仅有的6个鸡蛋。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;于是，女厨师付了27分给大女儿，买下她的三个鸡蛋。这样连同原先卖出的3分，大女儿共卖了30分钱；二女儿的2个鸡蛋拿到了18分，连同以前卖的12分钱，共得30分钱；三女儿剩下的一个蛋卖了9分，连同以前卖的21分，也拿到了30分钱。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;三姐妹回到家里，每人交了30分钱给妈妈，并向妈妈讲述她们是怎样卖的。的确，她们在价钱上遵守了共同的条件：不论是10个鸡蛋还是50个鸡蛋，都卖得同样的钱数。妈妈非常满意，特别为大女儿的智力出众感到高兴。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;你看，贝涅吉克托夫编的数学题，不仅有趣，而且故事还挺曲折。&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Sun, 09 Sep 2018 10:05:15 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/rMayFkcg2m</dc:identifier>
</item>
</channel>
</rss>