<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fmathalg-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/mathalg-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>算法与数学之美</title>
<link>http://www.jintiankansha.me/column/c9dZ5TM2aS</link>
<description>算法与数学之美 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>如何选择合适的损失函数，请看......</title>
<link>http://www.jintiankansha.me/t/cD8l43kowg</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/cD8l43kowg</guid>
<description>&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6669106881405563&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FqujqaY0DKkzblVl4UkDWLEy3wT25iaialRCRANzQ15cqicxpuXBibnOPibzSA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1366&quot; width=&quot;auto&quot;/&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;机器学习中的所有算法都依赖于最小化或最大化某一个函数，我们称之为“&lt;strong&gt;目标函数&lt;/strong&gt;”。最小化的这组函数被称为“&lt;strong&gt;损失函数&lt;/strong&gt;”。&lt;strong&gt;损失函数是衡量预测模型预测期望结果表现的指标&lt;/strong&gt;。寻找函数最小值的最常用方法是“&lt;strong&gt;梯度下降&lt;/strong&gt;”。把损失函数想象成起伏的山脉，梯度下降就像从山顶滑下，目的是到达山脉的最低点。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;没有一个损失函数可以适用于所有类型的数据&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。损失函数的选择取决于许多因素，包括是否有离群点，机器学习算法的选择，运行梯度下降的时间效率，是否易于找到函数的导数，以及预测结果的置信度。这个博客的目的是&lt;strong&gt;帮助你了解不同的损失函数&lt;/strong&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;损失函数可以大致分为两类：&lt;strong&gt;分类损失&lt;/strong&gt;（Classification Loss）和&lt;strong&gt;回归损失&lt;/strong&gt;（Regression Loss）。下面这篇博文，就将重点介绍5种回归损失。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;1.376543209876543&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquB8tibeKa8RuhOZvUxj92HARhJVCcgrXoicd8nYKCD8xUzIZB0KibfxHibA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;486&quot; data-width=&quot;364px&quot; height=&quot;auto&quot; width=&quot;364px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;回归函数预测实数值，分类函数预测标签&lt;/span&gt;&lt;/p&gt;

&lt;h2&gt;&lt;span&gt;▌&lt;/span&gt;&lt;span&gt;&lt;strong&gt;回归损失&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1、均方误差，二次损失，L2损失（Mean Square Error, Quadratic Loss, L2 Loss）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;均方误差（MSE）是最常用的回归损失函数。MSE是目标变量与预测值之间距离平方之和。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.396078431372549&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquVNzmvkzX8KNw7ZvbM3OnGGXbPiasp3ZXXvT58B6wXcnibDNAKMrKchqQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;255&quot; data-width=&quot;191px&quot; height=&quot;auto&quot; width=&quot;191px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;下面是一个MSE函数的图，其中真实目标值为100，预测值在-10,000至10,000之间。预测值（X轴）= 100时，MSE损失（Y轴）达到其最小值。损失范围为0至∞。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquiaF1WHnpwIvsO0VyibxoHcEibMiasqCRIlR7gKAXqic53jEI3dCxrMrFnHw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;576&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;MSE损失（Y轴）与预测值（X轴）关系图&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2、平均绝对误差，L1损失（Mean Absolute Error, L1 Loss）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;平均绝对误差（MAE）是另一种用于回归模型的损失函数。MAE是目标变量和预测变量之间差异绝对值之和。因此，它在一组预测中衡量误差的平均大小，而不考虑误差的方向。（如果我们也考虑方向，那将被称为平均偏差（Mean Bias Error, MBE），它是残差或误差之和）。损失范围也是0到∞。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.36964980544747084&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquxoU6zTBkGUiaNBpUjMRffsgE06XEullzicNgeqAytM4cTPiblqewGRGfw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;257&quot; data-width=&quot;192px&quot; height=&quot;auto&quot; width=&quot;192px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.625&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquI1BBUkaFWFy47B3OAYGk9c2OHaM5UrsM6AjM3eSmXQc9D9GUXOEG1g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;576&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;MAE损失（Y轴）与预测值（X轴）关系图&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;3、MSE vs MAE （L2损失 vs L1损失）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;简而言之， 使用平方误差更容易求解，但使用绝对误差对离群点更加鲁棒。但是，知其然更要知其所以然！&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;每当我们训练机器学习模型时，我们的目标就是找到最小化损失函数的点。当然，当预测值正好等于真实值时，这两个损失函数都达到最小值。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;下面让我们快速过一遍两个损失函数的Python代码。我们可以编写自己的函数或使用sklearn的内置度量函数：&lt;/span&gt;&lt;/p&gt;


&lt;section class=&quot;&quot; readability=&quot;7&quot;&gt;&lt;pre&gt;
&lt;br/&gt;＃&lt;span class=&quot;&quot;&gt;true&lt;/span&gt;：真正的目标变量数组&lt;br/&gt;＃pred：预测数组&lt;br/&gt;**&lt;br/&gt;&lt;span class=&quot;&quot;&gt;&lt;span class=&quot;&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;&quot;&gt;mse&lt;/span&gt;&lt;span class=&quot;&quot;&gt;(&lt;span class=&quot;&quot;&gt;true&lt;/span&gt;, pred)&lt;/span&gt;&lt;/span&gt;:&lt;br/&gt;&lt;span class=&quot;&quot;&gt;return&lt;/span&gt; np.sum(((&lt;span class=&quot;&quot;&gt;true&lt;/span&gt; – pred)**&lt;span class=&quot;&quot;&gt;2&lt;/span&gt;))&lt;br/&gt;**&lt;br/&gt;&lt;span class=&quot;&quot;&gt;&lt;span class=&quot;&quot;&gt;def&lt;/span&gt;&lt;/span&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;
&lt;span class=&quot;&quot;&gt;mae&lt;/span&gt;&lt;span class=&quot;&quot;&gt;(&lt;span class=&quot;&quot;&gt;true&lt;/span&gt;, pred)&lt;/span&gt;:&lt;br/&gt;&lt;span class=&quot;&quot;&gt;return&lt;/span&gt; np.sum(np.abs(&lt;span class=&quot;&quot;&gt;true&lt;/span&gt; – pred))&lt;br/&gt;**&lt;br/&gt;＃也可以在sklearn中使用&lt;br/&gt;**&lt;br/&gt;from sklearn.metrics import mean_squared_error&lt;br/&gt;from sklearn.metrics import mean_absolute_error&lt;br/&gt;&lt;/pre&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;让我们来看看两个例子的MAE值和RMSE值（RMSE，Root Mean Square Error，均方根误差，它只是MSE的平方根，使其与MAE的数值范围相同）。在第一个例子中，预测值接近真实值，观测值之间误差的方差较小。第二个例子中，有一个异常观测值，误差很高。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.31107954545454547&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjxVJ8ickgPg8afDRIjCmDNT7iaJcGNIuGjib3EUaJOS1jibyxAz09RqugzNwibFDZf9icVYEGibnRMLsfG1A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;704&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;左：误差彼此接近  右：有一个误差和其他误差相差很远&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们从中观察到什么？我们该如何选择使用哪种损失函数？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;由于MSE对误差（e）进行平方操作（y - y_predicted = e），如果e&amp;gt; 1，误差的值会增加很多。如果我们的数据中有一个离群点，e的值将会很高，将会远远大于|e|。这将使得和以MAE为损失的模型相比，以MSE为损失的模型会赋予更高的权重给离群点。在上面的第二个例子中，以RMSE为损失的模型将被调整以最小化这个离群数据点，但是却是以牺牲其他正常数据点的预测效果为代价，这最终会降低模型的整体性能。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;MAE损失适用于训练数据被离群点损坏的时候（即，在训练数据而非测试数据中，我们错误地获得了不切实际的过大正值或负值）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;直观来说，我们可以像这样考虑：对所有的观测数据，如果我们只给一个预测结果来最小化MSE，那么该预测值应该是所有目标值的均值。但是如果我们试图最小化MAE，那么这个预测就是所有目标值的中位数。我们知道中位数对于离群点比平均值更鲁棒，这使得MAE比MSE更加鲁棒。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;使用MAE损失（特别是对于神经网络）的一个大问题是它的梯度始终是相同的，这意味着即使对于小的损失值，其梯度也是大的。这对模型的学习可不好。为了解决这个问题，我们可以使用随着接近最小值而减小的动态学习率。MSE在这种情况下的表现很好，即使采用固定的学习率也会收敛。MSE损失的梯度在损失值较高时会比较大，随着损失接近0时而下降，从而使其在训练结束时更加精确（参见下图）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.31&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquYuZpIqhpB3XxGtmQgSgJOdy0VB0dX54BiaB0bLicbxxsIBibDLQOpCKzA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;800&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;决定使用哪种损失函数？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如果离群点是会影响业务、而且是应该被检测到的异常值，那么我们应该使用MSE。另一方面，如果我们认为离群点仅仅代表数据损坏，那么我们应该选择MAE作为损失。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  我建议阅读下面这篇文章，其中有一项很好的研究，比较了在存在和不存在离群点的情况下使用L1损失和L2损失的回归模型的性能。请记住，L1和L2损失分别是MAE和MSE的另一个名称而已。&lt;/span&gt;&lt;/p&gt;

&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;地址：&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;http://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;L1损失对异常值更加稳健，但其导数并不连续，因此求解效率很低。L2损失对异常值敏感，但给出了更稳定的闭式解（closed form solution）（通过将其导数设置为0）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;两种损失函数的问题：可能会出现这样的情况，即任何一种损失函数都不能给出理想的预测。例如，如果我们数据中90％的观测数据的真实目标值是150，其余10％的真实目标值在0-30之间。那么，一个以MAE为损失的模型可能对所有观测数据都预测为150，而忽略10％的离群情况，因为它会尝试去接近中值。同样地，以MSE为损失的模型会给出许多范围在0到30的预测，因为它被离群点弄糊涂了。这两种结果在许多业务中都是不可取的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在这种情况下怎么做？一个简单的解决办法是转换目标变量。另一种方法是尝试不同的损失函数。这是我们的第三个损失函数——Huber Loss——被提出的动机。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;3、Huber Loss，平滑的平均绝对误差&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Huber Loss对数据离群点的敏感度低于平方误差损失。它在0处也可导。基本上它是绝对误差，当误差很小时，误差是二次形式的。误差何时需要变成二次形式取决于一个超参数，(delta)，该超参数可以进行微调。当  𝛿 ~ 0时， Huber Loss接近MAE，当  𝛿 ~ ∞（很大的数）时，Huber Loss接近MSE。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.1580952380952381&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquAVbCpzRJ14mgyMRF38PKd8XVPlNBCrFIiaECMhuJu2KJIO6xV5YMUPA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;525&quot; data-width=&quot;393px&quot; height=&quot;auto&quot; width=&quot;393px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.7142857142857143&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquKyy9J1HF1VJ4Yuwe36k3y0jntFmsF37VNKqsc9JVw9Lw8x8hscNAZA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;504&quot; data-width=&quot;378px&quot; height=&quot;auto&quot; width=&quot;378px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Huber Loss（Y轴）与预测值（X轴）关系图。真值= 0&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;delta的选择非常重要，因为它决定了你认为什么数据是离群点。大于delta的残差用L1最小化（对较大的离群点较不敏感），而小于delta的残差则可以“很合适地”用L2最小化。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;为什么使用Huber Loss？&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;使用MAE训练神经网络的一个大问题是经常会遇到很大的梯度，使用梯度下降时可能导致训练结束时错过最小值。对于MSE，梯度会随着损失接近最小值而降低，从而使其更加精确。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在这种情况下，Huber Loss可能会非常有用，因为它会使最小值附近弯曲，从而降低梯度。另外它比MSE对异常值更鲁棒。因此，它结合了MSE和MAE的优良特性。但是，Huber Loss的问题是我们可能需要迭代地训练超参数delta。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;4、Log-Cosh Loss&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Log-cosh是用于回归任务的另一种损失函数，它比L2更加平滑。Log-cosh是预测误差的双曲余弦的对数。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.20642201834862386&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquvdgibUuMpBgbJLS7UeicibPwXJhw8PuVFvzqCWMDRyRsEzXsPyaxdnzPw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;436&quot; data-width=&quot;327px&quot; height=&quot;auto&quot; width=&quot;327px&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7142857142857143&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquZGJ8QVibRPmobQMzibuEc3DVszmBiaFemAibaf75C2yfjA1IF5EbfIbvng/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;504&quot; width=&quot;auto&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Log-cosh Loss（Y轴）与预测值（X轴）关系图。真值= 0&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;优点： log(cosh(x))对于小的x来说，其大约等于 (x ** 2) / 2，而对于大的x来说，其大约等于 abs(x) - log(2)。这意味着'logcosh'的作用大部分与均方误差一样，但不会受到偶尔出现的极端不正确预测的强烈影响。它具有Huber Loss的所有优点，和Huber Loss不同之处在于，其处处二次可导。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为什么我们需要二阶导数？许多机器学习模型的实现（如XGBoost）使用牛顿方法来寻找最优解，这就是为什么需要二阶导数（Hessian）的原因。对于像XGBoost这样的机器学习框架，二阶可导函数更有利。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.3815261044176707&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquMWJ8VlRvBSuldIuTfLHeeyDQl4xSyrX0qJGV9YGEicxGNeLehGR9Sqg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;747&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;XGBoost中使用的目标函数。注意其对一阶和二阶导数的依赖性。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;但Log-chsh Loss并不完美。它仍然存在梯度和Hessian问题，对于误差很大的预测，其梯度和hessian是恒定的。因此会导致XGBoost中没有分裂。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Huber和Log-cosh损失函数的Python代码：&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot; readability=&quot;10.5&quot;&gt;&lt;pre readability=&quot;7&quot;&gt;
&lt;br/&gt;&lt;span class=&quot;&quot;&gt;&lt;span class=&quot;&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;&quot;&gt;sm_mae&lt;/span&gt;&lt;span class=&quot;&quot;&gt;(true, pred, delta)&lt;/span&gt;:&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;&quot; readability=&quot;2&quot;&gt;&quot;&quot;&quot;&lt;br/&gt;true: array of true values    &lt;br/&gt;pred: array of predicted values&lt;p&gt;returns: smoothed mean absolute error loss&lt;br/&gt;&quot;&quot;&quot;&lt;/p&gt;&lt;/span&gt;&lt;br/&gt;loss = np.where(np.abs(true-pred) 0.5*((true-pred)**&lt;span class=&quot;&quot;&gt;2&lt;/span&gt;), delta*np.abs(true - pred) - &lt;span class=&quot;&quot;&gt;0.5&lt;/span&gt;*(delta**&lt;span class=&quot;&quot;&gt;2&lt;/span&gt;))&lt;br/&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;
&lt;span class=&quot;&quot;&gt;return&lt;/span&gt; np.sum(loss)&lt;p&gt;&lt;span class=&quot;&quot;&gt;&lt;span class=&quot;&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;&quot;&gt;logcosh&lt;/span&gt;&lt;span class=&quot;&quot;&gt;(true, pred)&lt;/span&gt;:&lt;/span&gt;&lt;br/&gt;loss = np.log(np.cosh(pred - true))&lt;br/&gt;&lt;span class=&quot;&quot;&gt;return&lt;/span&gt; np.sum(loss)&lt;br/&gt;&lt;/p&gt;&lt;/pre&gt;&lt;/section&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;5、Quantile Loss（分位数损失）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在大多数真实预测问题中，我们通常想了解我们预测的不确定性。了解预测值的范围而不仅仅是单一的预测点可以显着改善许多业务问题的决策过程。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当我们有兴趣预测一个区间而不仅仅是预测一个点时，Quantile Loss函数就很有用。最小二乘回归的预测区间是基于这样一个假设：残差（y - y_hat）在独立变量的值之间具有不变的方差。我们不能相信线性回归模型，因为它违反了这一假设。当然，我们也不能仅仅认为这种情况一般使用非线性函数或基于树的模型就可以更好地建模，而简单地抛弃拟合线性回归模型作为基线的想法。这时，Quantile Loss就派上用场了。因为基于Quantile Loss的回归模型可以提供合理的预测区间，即使是对于具有非常数方差或非正态分布的残差亦是如此。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;让我们看一个有效的例子，以更好地理解为什么基于Quantile Loss的回归模型对异方差数据表现良好。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Quantile 回归 vs 普通最小二乘（Ordinary Least Square, OLS）回归&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.4525&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquJHcasjaM2L6EfcmNArbJZGiaL8tQzBqkwo1l4rbb3lH2KRKLVlVFuvg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;800&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;左：线性关系b/w X1和Y，残差的方差恒定。右：线性关系b/w X2和Y，但Y的方差随着X2增加而变大（异方差）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.4525&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquujCpkT2Qx2A8HvibBVjbtC5yvhDQeSyhgzRavLMeh5x3hcOhiaKPC72Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;800&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;橙线表示两种情况下的OLS估计&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.7464788732394366&quot; data-type=&quot;png&quot; data-w=&quot;568&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquVzstKrVfnOibicvY1BQBwPqoqk5WgeIuvSt5SQLSghn6AtsibAkQcfXzw/640?wx_fmt=png&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Quantile回归：虚线表示基于0.05和0.95 分位数损失函数的回归估计&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如上所示的Quantile回归代码在下面这个notebook中。&lt;/span&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;地址：&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;https://github.com/groverpr/Machine-Learning/blob/master/notebooks/09_Quantile_Regression.ipynb&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;了解Quantile Loss函数&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;基于Quantile回归的目的是，在给定预测变量的某些值时，估计因变量的条件“分位数”。Quantile Loss实际上只是MAE的扩展形式（当分位数是第50个百分位时，Quantile Loss退化为MAE）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Quantile Loss的思想是根据我们是打算给正误差还是负误差更多的值来选择分位数数值。损失函数根据所选quantile (γ)的值对高估和低估的预测值给予不同的惩罚值。举个例子，γ= 0.25的Quantile Loss函数给高估的预测值更多的惩罚，并试图使预测值略低于中位数。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.13807531380753138&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquZbv72jVjocwNFC8FHV9fic8EEbqWFNRLBfIDcCwEh8oXGELve8Tnkbg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;717&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;γ 是给定的分位数，其值介于0和1之间。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7142857142857143&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquFC5OKMicPfvP9jwqLNMCApYnavRXFpRn2QxVhaXxibs6qFGRN5gJ8fDg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;504&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Quantile Loss（Y轴）与预测值（X轴）关系图。真值为Y= 0&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们也可以使用这个损失函数来计算神经网络或基于树的模型的预测区间。下图是sklearn实现的梯度提升树回归。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.75&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquA8JafWRFGibIA96IDFWcKeqTWwpbEiboCYYKd9BZZw4HU1PLias5malqQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;使用Quantile Loss的预测区间（梯度提升回归）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;上图显示的是sklearn库的GradientBoostingRegression中的quantile loss函数计算的90％预测区间。上限的计算使用了γ = 0.95，下限则是使用了γ = 0.05。&lt;/span&gt;&lt;/p&gt;

&lt;h2&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;▌&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&lt;span&gt;比较研究&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;“Gradient boosting machines, a tutorial”中提供了一个很好的比较研究。为了演示上述所有的损失函数的性质，研究人员创造了一个人工数据集，数据集从sinc(x)函数中采样，其中加入了两种人造模拟噪声：高斯噪声分量和脉冲噪声分量。脉冲噪声项是用来展示结果的鲁棒效果的。以下是使用不同损失函数来拟合GBM（Gradient Boosting Machine, 梯度提升回归）的结果。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  ﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.425&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9Fqu16eo4ia9fCPNia0Yo0XovgZU0Y1ibooaFQHq15Eiciaaib0ywGianlBwhuBPA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;800&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;连续损失函数：（A）MSE损失函数; （B）MAE损失函数; （C）Huber损失函数; （D）Quantile损失函数。用有噪声的sinc(x)数据来拟合平滑GBM的示例：（E）原始sinc(x)函数; （F）以MSE和MAE为损失拟合的平滑GBM; （G）以Huber Loss拟合的平滑GBM， = {4,2,1}; （H）以Quantile Loss拟合的平滑GBM。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;模拟实验中的一些观察结果：&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span&gt;一张图画出所有损失函数&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;img class=&quot;&quot; data-height=&quot;auto&quot; data-ratio=&quot;0.65&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BnSNEaficFAZPBQNcxWVdicvljIT6d9FquFEmVhlvKbbicSJkWIKVaI6dbpQYoOYyGIV8dW8kIOI3MSibZOt63ib7fA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;720&quot; data-width=&quot;415px&quot; height=&quot;auto&quot; width=&quot;415px&quot;/&gt;&lt;span&gt;﻿&lt;/span&gt;&lt;/p&gt;

&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;原文链接：&lt;br/&gt;https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span&gt;∑编辑 | Gemini&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;来源 | 人工智能头条&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.0437601296596435&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky7x6u1VxMVMia4MLibNzC2nrumY3zDflTsCeoM04M1BrkvPny8tsw6hYkIicUr42iarLmadL2x6JwV6A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;617&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域，经采用我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Wed, 04 Jul 2018 01:46:28 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/cD8l43kowg</dc:identifier>
</item>
<item>
<title>感动 ∣她养我长大，我陪她到老！90后女孩带痴呆症养母读研</title>
<link>http://www.jintiankansha.me/t/i6VQh6PG9i</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/i6VQh6PG9i</guid>
<description>&lt;p&gt;&lt;span&gt;2018年5月30日，西北工业大学长安校区，1991年出生的孙玉晴和母亲正在租住的房子里聊天。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;在此之前，她刚刚完成了一场令她担惊受怕却又习以为常的寻人行动&lt;/span&gt;&lt;span&gt;——母亲又走丢了，这是她不得不面对的日常“小状况”。她最担心的就是母亲喜欢出去逛，却总回不了家。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;2&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;4&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;　　&lt;/span&gt;&lt;span&gt;享受旅行、肆意追梦、甜蜜恋爱，这些同龄人的正常生活距离孙玉晴都太过遥远。自高三那年养父过世，摆在孙玉晴面前的只有身兼数职“吃饱饭”，攒钱给养母看病，不负养父嘱托完成学业。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;355&quot; data-backw=&quot;558&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/iahMqGfic6iczicD1rMJ83vPAJHicF7vBv8x1rzPB0icgXyaV9Ngey4k3ENsMXWq6Wk01ueeYiahicib89sVC8dJWnK41sg/640?wx_fmt=png&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6354430379746835&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/iahMqGfic6iczicD1rMJ83vPAJHicF7vBv8x1rzPB0icgXyaV9Ngey4k3ENsMXWq6Wk01ueeYiahicib89sVC8dJWnK41sg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;790&quot; width=&quot;100%&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;孙玉晴勇敢、坚定，8年来她自强奋进，不仅完成了从高职到本科再到研究生的课业，还带着养母畅游北京。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;生活窘迫稍有缓解时，年近八旬的养母患上了老年痴呆症，为照顾养母，她只能把自己的活动范围限定在校内。尽管还有很多挑战，孙玉晴却坚定如初：“养母陪我长大，我陪她到老。没有她，就没有今天的我。”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;400&quot; data-backw=&quot;600&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutIFfnicXTxWzazLlIibVXE5fqM4vM3YATdYbXictLJ18CYUibaSSZj4moWZQ/640?&quot; data-ratio=&quot;0.6666666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutIFfnicXTxWzazLlIibVXE5fqM4vM3YATdYbXictLJ18CYUibaSSZj4moWZQ/640?&quot; data-type=&quot;png&quot; data-w=&quot;600&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;“中国大学生自强之星”孙玉晴与养母。本人供图&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot; data-mpa-template-id=&quot;769609&quot; data-mpa-color=&quot;null&quot; data-mpa-category=&quot;fav&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;1&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;2&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;一条用兼职搭建的&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;升本、考研路&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;section mpa-is-content=&quot;t&quot;&gt;&lt;br/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;养父母捡到孙玉晴时已是65岁和51岁高龄，虽是养女，他们却视如己出，邻居们说“能养活就不错了”，他们却靠捡卖废品全力支持女儿上学。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;5岁起，孙玉晴放学的第一件事就是拿好袋子跟养母一起去捡拾废品，和养父一起卖废品，给养父母做饭、洗脚、洗衣服、捶背都是她最爱做的事。&lt;/span&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  虽然日子过得紧巴，但那却是孙玉晴心底最温暖的回忆。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;饱受高血压和风湿煎熬的年迈养父，最终还是没能等到女儿考上大学，在孙玉晴高三那年离世。&lt;/span&gt;&lt;span&gt;“那时候特别自责，一想到我妈一个人孤零零在医院照顾我爸，就痛恨自己什么也做不了。”&lt;/span&gt;&lt;span&gt;忆起往事，孙玉晴泪水哗然。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;至亲离世，还没来得及处理好悲伤的情绪，孙玉晴便不得不面对日益逼近的高考。遗憾的是，那一年她只考上了高职。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;432&quot; data-backw=&quot;540&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutIN59fqLwkbFGe5nAqqEVB5ficV3sCD8bBxyOqGzZvumTaIRH8wt1iaGEA/640?&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.66640625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutIN59fqLwkbFGe5nAqqEVB5ficV3sCD8bBxyOqGzZvumTaIRH8wt1iaGEA/640?&quot; data-type=&quot;jpeg&quot; data-w=&quot;1280&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;“我一定要上大学！给我爸妈争气。”&lt;/span&gt;&lt;span&gt;踏入学校的第一天，孙玉晴就下了这样的决心。然而彼时，孙玉晴面临的更迫切的问题是“生活费”。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;去食堂打工，是孙玉晴初入学校时唯一能找到的兼职。卖饭、洗碗、擦餐桌、倒剩饭，早中晚三个时段的忙碌，换来80块的月薪，&lt;/span&gt;&lt;span&gt;“很多同学不明白我为什么要去干这个，但这可以省去一日三餐的生活费”，&lt;/span&gt;&lt;span&gt;孙玉晴说这份兼职她干了整整一年。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;做销售、翻译、发传单、一天三个家教……在同学眼中，孙玉晴是一个不会玩的“怪人”，因为除去兼职时间，她几乎都在图书馆度过。　&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;“我舍不得睡觉，从来没午休过，专升本只有一次机会，我必须确保万无一失。”&lt;/span&gt;&lt;span&gt;提起大学的日子，孙玉晴再次泪目，尽管她已说不清为什么当时会有那样的勇气，但对那份在她心中“必须承担的责任”记忆深刻。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;功夫不负有心人，孙玉晴不仅成功考取了本科，还在2016年考取了西北工业大学的研究生。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;　　&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;407&quot; data-backw=&quot;556&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/iahMqGfic6iczicD1rMJ83vPAJHicF7vBv8x16O7Bf5iaq3CwRm3qeGBkEr4xqVyrJTPl4OeqESRml8Qjia0YnC7VT5Fw/640?wx_fmt=png&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6120906801007556&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/iahMqGfic6iczicD1rMJ83vPAJHicF7vBv8x16O7Bf5iaq3CwRm3qeGBkEr4xqVyrJTPl4OeqESRml8Qjia0YnC7VT5Fw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;794&quot; width=&quot;100%&quot;/&gt;&lt;/p&gt;

&lt;section class=&quot;&quot; data-mpa-template-id=&quot;769609&quot; data-mpa-color=&quot;null&quot; data-mpa-category=&quot;fav&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;1&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;2&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;攒钱圆了养母北京旅行梦&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;除了克服学习、生活上的压力，养母每况愈下的身体状况是孙玉晴最忧心的事。&lt;/span&gt;&lt;span&gt;大学那几年，养母每年都要住院三四次，孙玉晴每半月都要从学校回家一趟，火车票放了满满一抽屉。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;子欲养而亲不待，养父离世是孙玉晴心中永远的痛。为了在还来得及的时候报恩尽孝，孙玉晴决定奢侈一次，帮养母完成“到北京看看”的心愿。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;370&quot; data-backw=&quot;556&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutI5sTabyOgAwW67icxD8icFYBPVyjyB5T2hreUxeleib4RDn4LYXXs5knYw/640?&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.66625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutI5sTabyOgAwW67icxD8icFYBPVyjyB5T2hreUxeleib4RDn4LYXXs5knYw/640?&quot; data-type=&quot;jpeg&quot; data-w=&quot;800&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如今患上了老年痴呆症的养母，头脑不清醒，总跟孙玉晴闹脾气，还不时走丢。但她却也时不时无意提起“我去过北京”“坐过飞机”的事，这让孙玉晴在疲惫中多了一丝安慰。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot; data-mpa-template-id=&quot;769609&quot; data-mpa-color=&quot;null&quot; data-mpa-category=&quot;fav&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;1&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;2&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;“感谢所有帮助过我的人”&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;一路走来，外界的每一份关怀于她而言都弥足珍贵。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;收到西北工业大学研究生录取通知书的当天，孙玉晴一边是欣喜，一边是忐忑。&lt;/span&gt;&lt;span&gt;时年76岁的养母身体越来越差，她怎能把养母独自留在湖北老家？思考再三，孙玉晴拨通了辅导员的电话。素未谋面，辅导员却在了解了情况后，立即伸以援手，不仅帮她找房子，还帮她申请一系列补助，解决经济上的困难。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;542&quot; data-backw=&quot;400&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutI8MXadOWhDSv9qtRnKSibdSF0uctQ34ppw2P12xL0SNtPBoeKUxlToiaQ/640?&quot; data-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/azXQmS1HA7kibSETb6V7f2dPARZ1HvFc3k5GNc4GmMq3abgicp0QxoPkMBFJica9n5xL8Mm15xlCtfLp2QMjOIRPg/0?wx_fmt=jpeg&quot; data-ratio=&quot;1.355&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/azXQmS1HA7kibSETb6V7f2dPARZ1HvFc3k5GNc4GmMq3abgicp0QxoPkMBFJica9n5xL8Mm15xlCtfLp2QMjOIRPg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;400&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;孙玉晴本科毕业时与养母合影。本人供图&lt;/strong&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;学校的关怀让孙玉晴至今感动不已，她说：“这是当时我面临的最大的困难，在陕西我人生地不熟，都是学校帮着出力。所以我发誓一定要好好学习。”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;读硕士期间，孙玉晴一边细心照顾养母，一边砥砺前行。&lt;/span&gt;&lt;span&gt;在学习科研上，她赴新加坡、印度尼西亚参加外语类顶级国际会议并作口头汇报；以第一作者的身份在核心期刊等发表多篇论文并获得了多项奖学金；被评选为第十二届　“中国大学生年度人物”；在由共青团中央、全国学联主办的“青春自强·励志华章”主题活动中，获得2016年“中国大学生自强之星”称号。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;394&quot; data-backw=&quot;540&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/iahMqGfic6iczicD1rMJ83vPAJHicF7vBv8x1bDTeMWgxIjicqQdAL31gW9HmxiakJbLn3myQDcLIT7PpqT39gh0pP26w/640?wx_fmt=png&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6068702290076335&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/iahMqGfic6iczicD1rMJ83vPAJHicF7vBv8x1bDTeMWgxIjicqQdAL31gW9HmxiakJbLn3myQDcLIT7PpqT39gh0pP26w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;786&quot; width=&quot;100%&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;“一路上我需要感谢的人太多了，我觉得自己所有的努力就是为了回馈那些帮助过我的人。”时序更迭，孙玉晴仍记得高中时期那些关于“一件衣服”“一顿饭”的关切，“可能老师觉得微不足道，但对当时的我来说是莫大的安慰”。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot; data-mpa-template-id=&quot;769609&quot; data-mpa-color=&quot;null&quot; data-mpa-category=&quot;fav&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;1&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;2&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;梦想当教师回馈社会&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;2018年，孙玉晴27岁了，生活的磨砺让她考虑事情比同龄人更加长远。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;孙玉晴对从业方向有着清晰的规划。养父曾是小学老师，成长过程中蒙受师恩，让孙玉晴对教师这个职业怀有特殊的情结。她说：&lt;/span&gt;&lt;span&gt;“我想当老师，想去关心那些有困难的学生，想让那些遇到困难的学生能从我身上学到一些东西。”&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;319&quot; data-backw=&quot;542&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/iahMqGfic6iczicD1rMJ83vPAJHicF7vBv8x1eeoI4muhnedGcviclQTVWlD248ZO8NDicXDefzNfDDCI2ep61QsYMK0A/640?wx_fmt=png&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5891276864728192&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/iahMqGfic6iczicD1rMJ83vPAJHicF7vBv8x1eeoI4muhnedGcviclQTVWlD248ZO8NDicXDefzNfDDCI2ep61QsYMK0A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;791&quot; width=&quot;100%&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在实现职业理想之前，孙玉晴还有一个读博的愿望，一些了解她艰辛生活的同学对此并不能理解——“为什么不去工作？”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;孙玉晴坦言，她对学术有着一份特殊的热爱，此外在她看来，只有留在学校，才有更多时间守着母亲。&lt;/span&gt;&lt;span&gt;“我知道未来还有更多的困难，但那么难的日子都过来了，车到山前必有路，所以我会一直坚持、努力，去实现自己的梦想。”&lt;/span&gt;&lt;span&gt;说这些话时，孙玉晴笑了。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;361&quot; data-backw=&quot;542&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutIEGEfNH46I1zEEWax56uLHLO2lxSF36ma6EpFseXbvXLdeLKaN21l0Q/640?&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.66625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/SyoZQ3Y6TvfA5wnf6tG7xlAwCZSJbutIEGEfNH46I1zEEWax56uLHLO2lxSF36ma6EpFseXbvXLdeLKaN21l0Q/640?&quot; data-type=&quot;jpeg&quot; data-w=&quot;800&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;养母陪我长大，我要陪她到老&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;有孝心的人，最美！&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;∑编辑 | Gemini&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;来源 | 中国青年网&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.0437601296596435&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky7x6u1VxMVMia4MLibNzC2nrumY3zDflTsCeoM04M1BrkvPny8tsw6hYkIicUr42iarLmadL2x6JwV6A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;617&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域，经采用我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Wed, 04 Jul 2018 01:46:30 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/i6VQh6PG9i</dc:identifier>
</item>
<item>
<title>想转行人工智能？机会来了！！！</title>
<link>http://www.jintiankansha.me/t/xVIuJSuWzF</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/xVIuJSuWzF</guid>
<description>&lt;p data-mpa-powered-by=&quot;yiban.io&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;一个坏消息：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2018年1月 教育部印发的《普通高中课程方案和语文等学科课程标准》新加入了数据结构、人工智能、开源硬件设计等 AI 相关的课程。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这意味着职场新人和准备找工作的同学们，为了在今后十年内不被淘汰，你们要补课了，从初中开始。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;一个好消息：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;人工智能尖端人才远远不能满足需求。行业风口的人工智能，在中国人才缺口将超过500万人，而中国人工智能人才数量目前只有5万（数据来自工信部教育考试中心）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;并且目前岗位溢价相当严重，2017年人工智能在互联网岗位薪酬中位列第三，月薪20.1k，如果按照普遍的16月薪酬计算，那么人工智能在2017年一年的薪酬就是2.01*16=32.16万。那么再来看一组2018的薪酬数据： &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6058890147225368&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/4I1d5nkX8941l91Oibjp6yWFsDogPKGMXoqiacL7UgDtUjVyeaU9PUu8xIvsFFQSLNDgv9diagdjrTHbicc2yETXag/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;883&quot; width=&quot;auto&quot; /&gt; &lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;所以如果你对自己的专业/工作不满意，现在正是进入人工智能领域学习就业/转业的最佳时机。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在面对众多的数学知识和编程知识里，自学会让大家耗费大量的时间金钱。因此，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;中国科学院计算技术研究所人工智能博士团队&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;开发推出了人工智能&lt;/span&gt;&lt;span&gt;&lt;strong&gt;机器学习365天特训营&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;课程。（扫描最底部二维码联系助教）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为了保证大家的&lt;/span&gt;&lt;span&gt;&lt;strong&gt;学习效果&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;和&lt;/span&gt;&lt;span&gt;&lt;strong&gt;就业情况&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;幂次学院提供4项课程服务，&lt;/strong&gt;&lt;strong&gt;从发展历程、概念、基本名词、术语、评估方法讲起，到算法模型与实战演练：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;1、名校大牛讲师授课：&lt;/strong&gt;&lt;strong&gt;中国科学院计算技术研究所人工智能博士团队&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;开发课程，名校大牛授课；&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;2、3&lt;/strong&gt;&lt;strong&gt;65天的系统学习&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;跟老师直播学习，&lt;strong&gt;&lt;span&gt;直播回放4年内随时随地回看&lt;/span&gt;&lt;/strong&gt;，在线答疑，并完成课后作业；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;3、优质的售后答疑：&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;全天24小时课程问答与社群交流服务，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;让你的每一个问题都能够得到解答，课程资料随时下载；&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;4、颁发培训结业证书：通过幂&lt;/span&gt;&lt;/span&gt;&lt;span&gt;次学院的阶段测试和毕业测试，并颁发幂次学院人工智能培训结业证书。&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;strong&gt;合计&lt;/strong&gt;365+天，每周两次直播&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;365天100+小时（理论+实战）课程&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;（讲师直播答疑，课程7*24小时问答服务，学院社群7*24小时交流，课程资料随时下载）&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;购买课程另赠送2门辅助课程：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1. &lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;现在报名&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;免费赠送售价899元&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;的 机器学习之Python编程基础与数据分析 课程，课程内容由&lt;/span&gt;&lt;span&gt;&lt;strong&gt;清华大学python大牛&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;与&lt;/span&gt;&lt;span&gt;&lt;strong&gt;美国普渡大学算法工程师&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;主讲，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;课程内容&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;包括：python基础，python数据分析，python机器学习基础与python在机器学习中的实践案例（详细课程大纲见幂次学院主页）。&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2. &lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;&lt;strong&gt;现在报名&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;免费赠送售价899元&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;的 人工智能数学基础8天集训营 课程，由&lt;/span&gt;&lt;span&gt;&lt;strong&gt;中国科学院计算技术研究所博士团队&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;主讲，&lt;/span&gt;&lt;span&gt;&lt;strong&gt;课程内容&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;包括：矩阵论基础，概率与信息论，数值计算三部分&lt;span&gt;（详细课程大纲见幂次学院主页）。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5102803738317757&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/4I1d5nkX894Kar3aI8C9PoBa0jB21ev4ZywEDJLDznI2UicZ4fjWUAlabF9yUUJDQUHdNRPCMtAIth8d2aFpHAA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1070&quot; width=&quot;668px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;助力您解决人工智能学习中所需要用到的数学知识、Python编程知识。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;立即开始体系化学习，所有知识一步到位！&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;直播 + 回放：合计365+天，每周六19：00-21：00开课，直播回放随时随地回看。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;讲师团队介绍&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;（更多讲师详情请关注幂次学院主页：https://mici.jiqishidai.com)&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;张老师，中国科学院计算技术研究所机器学习方向博士&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;专注于人机交互、机器学习等领域研究。曾在国内外知名会议期刊发表多篇论文，并荣获人工智能领域会议“最佳论文提名奖”，目前拥有国家发明专利2项、软件著作权1项。拥有机器学习、数据挖掘领域实战经验，曾参与项目：&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1、面向病症的多模态在线预警方法研究—国家自然科学基金;&lt;br /&gt;2、基于人机交互技术的安全驾驶映射系统—国家国际科技合作专项;&lt;br /&gt;3、散发性病症风险基因图谱与预警评估方法研究—北京市科学技术委员会北京脑科学研究项目;&lt;br /&gt;4、广东省大数据科学中心项目“基于多模态大数据的复杂疾病临床诊断标准及应用”—广东省科技计划项目NSFC等国家级项目。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;李金老师，清华大学机器学习方向本硕双清华毕业生，阿里巴巴机器学习方向算法工程师；&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;研究方向为：推荐系统，计算机视觉，自然语言处理，深度学习等，在TNNLS，PR等杂志上发表过多篇论文，著有《自学Python—编程基础科学计算及数据分析》一书，P&lt;/span&gt;&lt;span&gt;ython笔记3K+Star，知乎python及机器学习板块12K+ zan，幂次学院签约讲师。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;赵朗老师，美国普渡大学硕士毕业生，机器学习工程师/算法工程师，曾参与研究：&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  1.参与克莱斯勒公司“合金发展”项目，应用机器学习分析产品合格率与合金成分等因素之间的关系；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.参与浙江大学关于研究材料渗透率的项目，应用机器学习分析材料渗透率与材料结构之间的关系；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.应用机器学习研究各大风场的风机故障问题，在机器学习，数据挖掘等方面有丰富的实战经验，善于用简单的例子来描述复杂的机器学习概念，善于对学生进行启发，帮助学生掌握机器学习的核心概念与算法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;附：机器学习365天特训营 - 课程大纲：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;第一部分 基础篇&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第1章&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.1 引言&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.2 基本术语&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.3 假设空间&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.4 归纳偏好&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.5 发展历程&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.6 应用现状&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第2章 模型评估与选择&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.1 经验误差与过拟合&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2 评估方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2.1 留出法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2.2 交叉验证法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2.3 自助法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2.4 调参与最终模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.3 性能度量&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.3.1 错误率与精度&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.3.2 查准率、查全率与F1&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.3.3 ROC与AUC&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.3.4 代价敏感错误率与代价曲线&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.4 比较检验&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.4.1 假设检验&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.4.2 交叉验证t检验&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.4.3 McNemar检验&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.4.4 Friedman检验与后续检验&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.5 偏差与方差&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第3章 线性模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.1 基本形式&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;3.2 线性回归&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.3 对数几率回归&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.4 线性判别分析&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.5 多分类学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.6 类别不平衡问题&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第4章 决策树&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.1 基本流程&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.2 划分选择&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.2.1 信息增益&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.2.2 增益率&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.2.3 基尼指数&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.3 剪枝处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.3.1 预剪枝&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.3.2 后剪枝&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.4 连续与缺失值&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.4.1 连续值处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.4.2 缺失值处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4.5 多变量决策树&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第5章 神经网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.1 神经元模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.2 感知机与多层网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.3 误差逆传播算法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.4 全局最小与局部极小&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5 其他常见神经网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5.1 RBF网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5.2 ART网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5.3 SOM网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5.4 级联相关网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5.5 Elman网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.5.6 Boltzmann机&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.6 深度学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第6章 支持向量机&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6.1 间隔与支持向量&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6.2 对偶问题&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6.3 核函数&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;6.4 软间隔与正则化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6.5 支持向量回归&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6.6 核方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第7章 贝叶斯分类器&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.1 贝叶斯决策论&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.2 极大似然估计&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.3 朴素贝叶斯分类器&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.4 半朴素贝叶斯分类器&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.5 贝叶斯网&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.5.1 结构&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.5.2 学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.5.3 推断&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;7.6 EM算法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第8章 集成学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.1 个体与集成&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.2 Boosting&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.3 Bagging与随机森林&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.3.1 Bagging&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.3.2 随机森林&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.4 结合策略&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.4.1 平均法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.4.2 投票法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.4.3 学习法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.5 多样性&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.5.1 误差--分歧分解&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.5.2 多样性度量&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;8.5.3 多样性增强&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第9章 聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.1 聚类任务&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.2 性能度量&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.3 距离计算&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.4 原型聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.4.1 k均值算法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.4.2 学习向量量化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  9.4.3 高斯混合聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.5 密度聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;9.6 层次聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第10章 降维与度量学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.1 k近邻学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.2 低维嵌入&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.3 主成分分析&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.4 核化线性降维&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.5 流形学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.5.1 等度量映射&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.5.2 局部线性嵌入&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;10.6 度量学习&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;第二部分 进阶篇&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第11章 特征选择与稀疏学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.1 子集搜索与评价&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.2 过滤式选择&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.3 包裹式选择&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.4 嵌入式选择与L_1正则化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.5 稀疏表示与字典学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;11.6 压缩感知&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第12章 计算学习理论&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.1 基础知识&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.2 PAC学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.3 有限假设空间&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.3.1 可分情形&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.3.2 不可分情形&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.4 VC维&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.5 Rademacher复杂度&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;12.6 稳定性&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第13章 半监督学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;13.1 未标记样本&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;13.2 生成式方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;13.3 半监督SVM&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;13.4 图半监督学习&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;13.5 基于分歧的方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;13.6 半监督聚类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第14章 概率图模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.1 隐马尔可夫模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.2 马尔可夫随机场&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.3 条件随机场&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.4 学习与推断&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.4.1 变量消去&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.4.2 信念传播&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.5 近似推断&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.5.1 MCMC采样&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.5.2 变分推断&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;14.6 话题模型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第15章 规则学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.1 基本概念&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.2 序贯覆盖&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.3 剪枝优化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.4 一阶规则学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.5 归纳逻辑程序设计&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.5.1 最小一般泛化&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;15.5.2 逆归结&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第16章 强化学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.1 任务与奖赏&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.2 K-摇臂赌博机&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.2.1 探索与利用&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.2.2 ε-贪心&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.2.3 Softmax&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.3 有模型学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.3.1 策略评估&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.3.2 策略改进&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.3.3 策略迭代与值迭代&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.4 免模型学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.4.1 蒙特卡罗强化学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.4.2 时序差分学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.5 值函数近似&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;16.6 模仿学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.6.1 直接模仿学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;16.6.2 逆强化学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第17章 增量学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.1 被动攻击学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.1.1 梯度下降量的抑制&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.1.2 被动攻击分类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.1.3 被动攻击回归&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.2 适应正则化学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.2.1 参数分布的学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.2.2 适应正则化分类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.2.3 适应正则化回归&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;17.3 增量随机森林&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第18章 迁移学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.1 迁移学习简介&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.1.1 什么是迁移学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.1.2 迁移学习VS传统机器学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.1.3 应用领域&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2 迁移学习的分类方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2.1 按迁移情境&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2.2 按特征空间&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2.3 按迁移方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.3 代表性研究成果&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2.1 域适配问题&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2.2 多源迁移学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;18.2.3 深度迁移学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第19章 主动学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;19.1 主动学习简介&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;19.2 主动学习思想&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;19.3 主动学习VS半监督学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;19.4 主动学习VS Self-Learning&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第20章 多任务学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;20.1 使用最小二乘回归的多任务学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;20.2 使用最小二乘概率分类器的多任务学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;span&gt;20.3 多次维输出函数的学习&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;第三部分 实战篇&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第21章 机器学习应用场景介绍&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;21.1 机器学习经典应用场景&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;21.2 头脑风暴：挖掘身边的应用场景&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第22章 数据预处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;22.1 数据降噪&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;22.2 数据分割&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第23章 特征提取&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;23.1 时域特征&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;23.2 频域特征&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;23.3 自动特征提取&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第24章 机器学习方法应用&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;24.1 应用机器学习方法之前的处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;24.2 使用机器学习分类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;24.3 机器学习调参&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;24.4 分类结果展示&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;报名费用及优惠详情：&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;br /&gt;原价16800元：&lt;/span&gt;&lt;span&gt;&lt;strong&gt;折后特惠价：2999元。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/4I1d5nkX896cbVeGItvDKsPGhluU4vhO5vfooqXr8Ik9XssHL935SFKmmic8xyE9nqRNY25icibEnnWfsbQjicZ56A/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;800&quot; width=&quot;210px&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;长按二维码咨询助教微信&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/gDmjibFWrFyPmsrETMZjrlkZibLNK8T98q6lWfibkWDwVu5VuMFgEB8nGcWkntxuOedFmmoLPm07wafujgxWYgmtA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;500&quot; width=&quot;252px&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  长按二维码进入报名页面&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
<pubDate>Wed, 04 Jul 2018 01:46:29 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/xVIuJSuWzF</dc:identifier>
</item>
<item>
<title>机器学习 —— 浅谈贝叶斯和MCMC</title>
<link>http://www.jintiankansha.me/t/8rXKQMi9Yd</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/8rXKQMi9Yd</guid>
<description>&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5348837209302325&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjyw8LR5jvj4RcfXWgKBHMZcibOISbpiaAVAqUoF2HQzCdqmt405HTCCRneUWiamLVX4gQUY0gcibeEZ0g/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;860&quot;/&gt;&lt;span&gt;&lt;span&gt;‍‍Abstract：&lt;/span&gt;&lt;span&gt;最近课业内的任务不是很多，又临近暑假了，就在网上搜了一些有关于机器学习和深度学习的课程进行学习。网上的资料非常繁多，很难甄别，我也是货比三家进行学习。这是这个系列的第一个笔记，是关于贝叶斯和MCMC一些数学原理的讲解和代码的实现，希望能够深入浅出，叙述的容易让人理解。…&lt;/span&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;



&lt;p&gt;&lt;span&gt;不论是学习概率统计还是机器学习的过程中，贝叶斯总是是绕不过去的一道坎，大部分人在学习的时候都是在强行地背公式和套用方法，没有真正去理解其牛逼的思想内涵。我看了一下 Chalmers 一些涉及到贝叶斯统计的课程，content 里的第一条都是 Philosophy of Bayesian statistics。&lt;/span&gt;&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;▌&lt;/span&gt;历史背景&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;什么事都要从头说起，贝叶斯全名为托马斯·贝叶斯(Thomas Bayes，1701-1761),是一位与牛顿同时代的牧师，是一位业余数学家，平时就思考些有关上帝的事情，当然，统计学家都认为概率这个东西就是上帝在掷骰子。当时贝叶斯发现了古典统计学当中的一些缺点，从而提出了自己的“贝叶斯统计学”，但贝叶斯统计当中由于引入了一个主观因素（先验概率，下文会介绍），一点都不被当时的人认可。直到20世纪中期，也就是快200年后了，统计学家在古典统计学中遇到了瓶颈，伴随着计算机技术的发展，当统计学家使用贝叶斯统计理论时发现能解决很多之前不能解决的问题，从而贝叶斯统计学一下子火了起来，两个统计学派从此争论不休。&lt;/span&gt;&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;▌&lt;/span&gt;什么是概率？&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;什么是概率这个问题似乎人人都觉得自己知道，却有很难说明白。比如说我问你 掷一枚硬币为正面的概率为多少？，大部分人第一反应就是50%的几率为正。不好意思，首先这个答案就不正确，只有当材质均匀时硬币为正面的几率才是50%（所以不要觉得打麻将的时候那个骰子每面的几率是相等的，万一被做了手脚呢）。好，那现在假设硬币的材质是均匀的，那么为什么正面的几率就是50%呢？有人会说是因为我掷了1000次硬币，大概有492次是正面，508次是反面，所以近似认为是50%，说得很好（掷了1000次我也是服你）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;br/&gt;掷硬币的例子说明了古典统计学的思想，就是概率是基于大量实验的，也就是 大数定理。那么现在再问你，有些事件，例如：明天下雨的概率是30%；A地会发生地震的概率是 5%；一个人得心脏病的概率是 40%…… 这些概率怎么解释呢？难道是A地真的 100 次的机会里，地震了 5 次吗？肯定不是这样，所以古典统计学就无法解释了。再回到掷硬币的例子中，如果你没有机会掷 1000 次这么多次，而是只掷了 3 次，可这 3 次又都是正面，那该怎么办？难道这个正面的概率就是 100% 了吗？这也是古典统计学的弊端。&lt;/span&gt;&lt;/p&gt;

&lt;h2&gt;&lt;span&gt;举个例子：生病的几率&lt;/span&gt;&lt;/h2&gt;

&lt;blockquote readability=&quot;10&quot;&gt;
&lt;p&gt;&lt;span&gt;一种癌症，得了这个癌症的人被检测出为阳性的几率为 90%，未得这种癌症的人被检测出阴性的几率为 90%，而人群中得这种癌症的几率为 1%，一个人被检测出阳性，问这个人得癌症的几率为多少？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;br/&gt;猛地一看，被检查出阳性，而且得癌症的话阳性的概率是 90%，那想必这个人应该是难以幸免了。那我们接下来就算算看。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;我们用 &lt;/span&gt;&lt;span&gt;A表示事件 “测出为阳性”, 用 &lt;/span&gt;&lt;span&gt;B&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;&lt;span&gt; 表示“得癌症”, &lt;/span&gt;&lt;span&gt;B&lt;/span&gt;&lt;span&gt;2&lt;/span&gt;&lt;span&gt; 表示“未得癌症”。根据题目，我们知道如下信息:&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.061511423550087874&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bXRoHL3VibTYic7OSwrjbBBDQNUwuOTfMibDx7yib6W7DpuwP7kibYibNcLd5g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;569&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;那么我们现在想得到的是已知为阳性的情况下，得癌症的几率 &lt;/span&gt;&lt;span&gt;P&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;B&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;&lt;span&gt;,&lt;/span&gt;&lt;span&gt;A&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt; :&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.058577405857740586&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bX8DfIH4QtN6CN1NoXoGeCvNviavjmDX0VKPuUml3trYCdLxzotsqg7WA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;478&quot;/&gt;&lt;/p&gt;


&lt;p&gt;&lt;span&gt;P&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;B&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;&lt;span&gt;,&lt;/span&gt;&lt;span&gt;A&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt; 表示的是联合概率，得癌症且检测出阳性的概率是人群中得癌症的概率乘上得癌症时测出是阳性的几率，是 0.009。同理可得得癌症且检测出阳性的概率：&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.07296137339055794&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bXHFcawCh1t8SLMibrHgSnGIKWkEDo9KuKR318AibUCldwtpGvrib5AXhYw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;466&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这个概率是什么意思呢？其实是指如果人群中有 1000 个人，检测出阳性并且得癌症的人有 9 个，检测出阳性但未得癌症的人有 99 个。可以看出，检测出阳性并不可怕，不得癌症的是绝大多数的，这跟我们一开始的直觉判断是不同的！可直到现在，我们并没有得到所谓的“在检测出阳性的前提下得癌症的 概率 ”，怎么得到呢？很简单，就是看被测出为阳性的这 108(9+99) 人里，9 人和 99 人分别占的比例就是我们要的,也就是说我们只需要添加一个归一化因子(normalization)就可以了。所以阳性得癌症的概率&lt;/span&gt;&lt;span&gt;P&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;B&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;&lt;span&gt;|&lt;/span&gt;&lt;span&gt;A&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt; 为：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.25609756097560976&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bXgubmJpjWpserJ6UeDy4FT44P1csd5wbeWRy12ljdEMuZlgBSlXhw2Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;164&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt; 阳性未得癌症的概率 &lt;/span&gt;&lt;span&gt;P&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;B&lt;/span&gt;&lt;span&gt;2&lt;/span&gt;&lt;span&gt;|&lt;/span&gt;&lt;span&gt;A&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt; 为：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.25&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bX4v75JI0KbtT12nPavic0jQEVE6hDXONVIFlibxlUbBWoibQPFAVBX5iagQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;168&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这里 &lt;/span&gt;&lt;span&gt;P&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;B&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;&lt;span&gt;|&lt;/span&gt;&lt;span&gt;A&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt; ，&lt;/span&gt;&lt;span&gt;P&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;B&lt;/span&gt;&lt;span&gt;2&lt;/span&gt;&lt;span&gt;|&lt;/span&gt;&lt;span&gt;A&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt; 中间多了这一竖线&lt;/span&gt;&lt;span&gt;|&lt;/span&gt;&lt;span&gt;|&lt;/span&gt;&lt;span&gt;成为了条件概率，而这个概率就是贝叶斯统计中的&lt;strong&gt;后验概率&lt;/strong&gt;！而人群中患癌症与否的概率 &lt;/span&gt;&lt;span&gt;P&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;B&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt;,&lt;/span&gt;&lt;span&gt;P&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;B&lt;/span&gt;&lt;span&gt;2&lt;/span&gt;&lt;span&gt;) &lt;/span&gt;&lt;span&gt;就是&lt;strong&gt;先验概率&lt;/strong&gt;！我们知道了先验概率，根据观测值(observation)，也可称为 test evidence：是否为阳性，来判断得癌症的后验概率，这就是基本的贝叶斯思想，我们现在就能得出本题的后验概率的公式为：&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.14666666666666667&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bXFiaiaE28M4uKY0qLB6x6hfGA7S06oNWgtO3icAmkpAADAMnvLlU2hBYiag/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;450&quot;/&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;span&gt;由此就能得到如下的贝叶斯公式的一般形式。&lt;/span&gt;&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;▌&lt;/span&gt;‍‍贝叶斯公式&lt;/span&gt;&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;span&gt;我们把上面例题中的 &lt;/span&gt;&lt;span&gt;A 变成样本(sample) &lt;/span&gt;&lt;span&gt;x&lt;/span&gt;&lt;span&gt; , 把&lt;/span&gt;&lt;span&gt;B&lt;/span&gt;&lt;span&gt; 变成参数(parameter) &lt;/span&gt;&lt;span&gt;θ&lt;/span&gt;&lt;span&gt;, 我们便得到我们的贝叶斯公式：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.24096385542168675&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bXvIYic8j4WlmJDFAoCDj3U6WuN9HrM2QJZ8wWRLguEaABX9e7H0mBuyQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;249&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;‍‍可以看出上面这个例子中，&lt;/span&gt;&lt;span&gt;B&lt;/span&gt;&lt;span&gt; 事件的分布是离散的，所以在分母用的是求和符号 &lt;/span&gt;&lt;span&gt;∑&lt;/span&gt;&lt;span&gt; 。那如果我们的参数 &lt;/span&gt;&lt;span&gt;θ&lt;/span&gt;&lt;span&gt; 的分布是连续的呢？没错，那就要用积分，于是我们终于得到了真正的&lt;strong&gt;贝叶斯公式&lt;/strong&gt; ：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.23106060606060605&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bXIdyjdv8O2gtxuTgugYiarGib4a2zSQ0qZ47ZZk6RKfXraStdq7ccDDicA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;264&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;其中 π 指的是参数的概率分布，π(θ) 指的是先验概率，π(θ|x) 指的是后验概率，f(x|θ)指的是我们观测到的样本的分布，也就是似然函数(likelihood)，记住&lt;strong&gt;竖线 |左边的才是我们需要的&lt;/strong&gt;。其中积分求的区间&lt;/span&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.8888888888888888&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bXwZbLFDoC2ZOujzic8EZCUVHElbxibdmD7o1dByqlP5CyqOBnB64PAqmA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;18&quot;/&gt;&lt;span&gt;指的是参数 θ 所有可能取到的值的域，所以可以看出后验概率 π(θ|x) 是在知道 x 的前提下在&lt;/span&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.8888888888888888&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bXwZbLFDoC2ZOujzic8EZCUVHElbxibdmD7o1dByqlP5CyqOBnB64PAqmA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;18&quot;/&gt;&lt;span&gt;域内的一个关于θ 的概率密度分布，每一个θ 都有一个对应的可能性(也就是概率)。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;&lt;span&gt;▌&lt;/span&gt;理解贝叶斯公式&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这个公式应该在概率论书中就有提到，反正当时我也只是死记硬背住，然后遇到题目就套用。甚至在 Chalmers 学了一门统计推断的课讲了贝叶斯，大部分时间我还是在套用公式，直到后来结合了一些专门讲解贝叶斯的课程和资料才有了一些真正的理解。要想理解这个公式，首先要知道这个竖线 | 的两侧一会是 x|θ ，一会是 θ|x 到底指的是什么，或者说似然函数和参数概率分布到底指的是什么。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;▌&lt;/span&gt;似然函数&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;首先来看似然函数 f(x|θ)，似然函数听起来很陌生，其实就是我们在概率论当中看到的各种概率分布 f(x)，那为什么后面要加个参数|θ 呢？我们知道，掷硬币这个事件是服从伯努利分布的 Ber(p) , n次的伯努利实验就是我们熟知的二项分布 Bin(n,p), 这里的p就是一个参数，原来我们在做实验之前，这个参数就已经存在了(可以理解为上帝已经定好了)，我们抽样出很多的样本 x 是为了找出这个参数，我们上面所说的掷硬币的例子，由于我们掷了 1000 次有 492 次是正面，根据求期望的公式 n⋅p=μ (492就是我们的期望)可以得出参数 p 为 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.3017241379310345&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bX2Vv3yT6iaBkdpNkn83yiaF9cULSzmQGedicRgNj740MBvtotU51domU2Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;116&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;所以我们才认为正面的概率是近似 50% 的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;现在我们知道了，其实我们观测到样本 x 的分布是在以某个参数 θ 为前提下得出来的，所以我们记为 f(x|θ)，只是我们并不知道这个参数是多少。所以&lt;strong&gt;参数估计&lt;/strong&gt;成为了统计学里很大的一个课题，古典统计学中常用的方法有两种：&lt;strong&gt;矩方法(momnet)和最大似然估计(maximum likelihood estimate, mle)&lt;/strong&gt; ，我们常用的像上面掷硬币例子中求均值的方法，本质就是矩估计方法，这是基于大数定理的。而统计学中更广泛的是使用最大似然估计的方法，原理其实很简单，在这简单说一下：假设我们有 n 个样本 x1,x2,x3,…,xn，它们每一个变量都对应一个似然函数:&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.12109375&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bXSsnKEuZ3G98NuO0bNnAP9rfFknTFCY4sxkZMdwjgI9xETbcqalT4gQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;256&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们现在把这些似然函数乘起来:&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.37988826815642457&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bX0z82H2Z7qQEXQj6hXuFZtbj6rsA76TZibnDuAnibIY91JL7CGF3dicTAg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;179&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们只要找到令 lik(θ)  这个函数最大的 θ 值，便是我们想要的参数值(具体计算参考[2]中p184)。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;▌&lt;/span&gt;后验分布(Posterior distribution)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;现在到了贝叶斯的时间了。以前我们想知道一个参数，要通过大量的观测值才能得出，而且是只能得出一个参数值。而现在运用了贝叶斯统计思想，这个后验概率分布 π(θ|x) 其实是一系列参数值 θ 的概率分布，再说简单点就是我们得到了许多个参数 θ 及其对应的可能性，我们只需要从中选取我们想要的值就可以了：有时我们想要概率最大的那个参数，那这就是 &lt;strong&gt;后验众数估计&lt;/strong&gt;(posterior mode estimator)；有时我们想知道参数分布的中位数，那这就是&lt;strong&gt;后验中位数估计&lt;/strong&gt;(posterior median estimator);有时我们想知道的是这个参数分布的均值，那就是&lt;strong&gt;后验期望估计&lt;/strong&gt;。这三种估计没有谁好谁坏，只是提供了三种方法得出参数，看需要来选择。现在这样看来得到的参数是不是更具有说服力？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;strong&gt;&lt;span&gt;&lt;span&gt;▌&lt;/span&gt;置信区间和可信区间&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在这里我想提一下&lt;strong&gt;置信区间&lt;/strong&gt;(confidence interval, CI) 和&lt;strong&gt;可信区间&lt;/strong&gt;(credibility interval,CI),我觉得这是刚学贝叶斯时候非常容易弄混的概念。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;再举个例子：一个班级男生的身高可能服从某种正态分布 N(μ,σ2),然后我们把全班男生的身高给记录下来，用高中就学过的求均值和方差的公式就可以算出来这两个参数，要知道我们真正想知道的是这个参数 μ,σ2，当然样本越多，得出的结果就接近真实值(其实并没有人知道什么是真实值，可能只有上帝知道)。等我们算出了均值和方差，我们这时候一般会构建一个95%或者90%的置信区间，这个置信区间是对于&lt;strong&gt;样本 x &lt;/strong&gt;来说的，我只算出了一个 μ 和 一个 σ 参数值的情况下，95% 的置信区间意味着在这个区间里的样本是可以相信是服从以 μ,σ 为参数的正态分布的，一定要记住置信区间的概念中是指&lt;strong&gt;一个参数值&lt;/strong&gt;的情况下！&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;而我们也会对我们得到的后验概率分布构造一个 90% 或 95% 的区间，称之为可信区间。这个可信区间是对于&lt;strong&gt;参数 θ &lt;/strong&gt;来说的，我们的到了 很多的参数值，取其中概率更大一些的90%或95%，便成了可信区间。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;▌&lt;/span&gt;先验分布(Prior distribution)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;说完了后验分布，现在就来说说先验分布。先验分布就是你在取得实验观测值以前对一个参数概率分布的&lt;strong&gt;主观判断&lt;/strong&gt;，这也就是为什么贝叶斯统计学一直不被认可的原因，统计学或者数学都是客观的，怎么能加入主观因素呢？但事实证明这样的效果会非常好！&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;再拿掷硬币的例子来看(怎么老是拿这个举例，是有多爱钱。。。)，在扔之前你会有判断正面的概率是50%，这就是所谓的先验概率，但如果是在打赌，为了让自己的描述准确点，我们可能会说正面的概率为 0.5 的可能性最大，0.45 的几率小点，0.4 的几率再小点，0.1 的几率几乎没有等等，这就形成了一个先验概率分布。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;那么现在又有新的问题了，如果我告诉你这个硬币的材质是不均匀的，那正面的可能性是多少呢？这就让人犯糊涂了，我们想有主观判断也无从下手，于是我们就想说那就先认为 0~1 之间每一种的可能性都是相同的吧，也就是设置成 0~1 之间的均匀分布Uni(0,1) 作为先验分布吧，这就是贝叶斯统计学当中的&lt;strong&gt;无信息先验&lt;/strong&gt;(noninformative prior)！那么下面我们就通过不断掷硬币来看看，这个概率到是多少，贝叶斯过程如下&lt;/span&gt;&lt;span&gt;：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.3599137931034483&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjy4ALbecuZ91UsBZPQP54bXXxlTML2ECIgv81dXms8PMXqsbpqcSN1Zr6wcib3QlHrpN6OrppTpcfg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;928&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.33938100320170755&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/ptp8P184xjy4ALbecuZ91UsBZPQP54bXQvicCas5bR3GjyZPWXagEtnaMuP72Ma6OMqfsxmoyndBEqRYRoc0ianw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;937&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;从图中我们可以看出，0 次试验的时候就是我们的先验假设——均匀分布，然后掷了第一次是正面，于是概率分布倾向于 1，第二次又是正，概率是 1的可能性更大了，但&lt;strong&gt;注意：这时候在 0.5 的概率还是有的，只不过概率很小，在 0.2 的概率变得更小&lt;/strong&gt;。第三次是反面，于是概率分布被修正了一下，从为1的概率最大变成了 2/3 左右最大(3次试验，2 次正 1 次反当然概率是2/3的概率最大)。再下面就是进行更多次的试验，后验概率不断根据观测值在改变，当次数很大的时候，结果趋向于 0.5 (哈哈，结果这还是一枚普通的硬币，不过这个事件告诉我们，直觉是不可靠的，一定亲自实验才行~)。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;有的人会说，这还不是在大量数据下得到了正面概率为 0.5 嘛，有什么好稀奇的？&lt;strong&gt;注意了！画重点了！(敲黑板) 记住&lt;/strong&gt;，不要和一个统计学家或者数学家打赌！跑题了，跑题了。。。说回来，我们上面就说到了古典概率学的弊端就是如果掷了 2 次都是正面，那我们就会认为正面的概率是 1，而在贝叶斯统计学中，如果我们掷了 2 次都是正面，只能说明正面是1的可能性最大，但还是有可能为 0.5, 0.6, 0.7 等等的，这就是对古典统计学的一种完善和补充，于是我们也就是解释了，我们所谓的&lt;strong&gt;地震的概率为 5%；生病的概率为 10% 等等这些概率的意义了&lt;/strong&gt;，这就是贝叶斯统计学的哲学思想。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;▌&lt;/span&gt;共轭先验(Conjugate prior)&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;共轭先验应该是每一个贝叶斯统计初学者最头疼的问题，我觉得没有“之一”。这是一个非常大的理论体系，我试着用一些简单的语言进行描述，关键是去理解其思想。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;继续拿掷硬币的例子，这是一个二项试验 Bin(n,p)，所以其似然函数为:&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.27232142857142855&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bX84e0oKx4aXfOeVvyDxBcmanPJnpPibWcuCHy6oBshBEriae7sm22hD3w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;224&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在我们不知道情况时就先假设其先验分布为均匀分布 Uni(0,1)，即：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.2441860465116279&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bXpfO1eaicoDj7ia7I76neNEgHy2mib1yFG5bt98edHKNUIdenXH2w2ibia9w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;172&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;那现在根据贝叶斯公式求后验概率分布：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.24028268551236748&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bXKUzDJU9pjHwSzTQv1cNOM1XbW94NAAiaaGhBK5RRUcUMxV8YPBAn6Wg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;283&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们得到结果为:&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.12673267326732673&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bXMicZhIIiaoX4DmlAO3OU8jfzPZVtEtPeB4zoLzZnoGT0LWxIuzBBqbOQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;505&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这么一大串是什么呢？其实就是大名鼎鼎的贝塔分布(Beta distribution)。 简写就是 Be(x+1,n−x+1)。 比如我掷了10 次(n=10)，5次正(x=5),5 次反，那么结果就是 Be(6,6), 这个分布的均值就是&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.42105263157894735&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bXYXPT3ypMusZn5HaojoRfLvpVhGEicicHwAZnQVxUOIrDA1NEfgKqPEUg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;76&quot; width=&quot;75px&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;很符合我们想要的结果。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;现在可以说明，我们把主观揣测的先验概率定为均匀分布是合理的，因为我们在对一件事物没有了解的时候，先认为每种可能性都一样是非常说得通的。有人会认为，既然无信息先验是说得通的，而且贝叶斯公式会根据我们的观测值不断更新后验概率，那是不是我们随便给一个先验概率都可以呢？当然……不行！！这个先验概率是不能瞎猜的，是需要根据一些前人的经验和常识来判断的。比如我随便猜先验为一个分段函数：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.13447432762836187&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bX8mE3ZviaaQciajNQ0B7VJmUNicTxQPuVEIm5R22T5eUmEiaR8I1E9ql01Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;409&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;靠，是不是很变态的一个函数…就是假设一个极端的情况，如果你把这个情况代入贝叶斯公式，结果是不会好的(当然我也不知道该怎么计算)。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这个例子中，我看到了可能的后验分布是 Beta 分布，看起来感觉有点像正态分布啊，那我们用正态分布作为先验分布可以吗？这个是可以的(所以要学会观察)。可如果我们把先验分布为正态分布代入到贝叶斯公式，那计算会非常非常麻烦，虽然结果可能是合理的。那怎么办？不用担心，因为我们有共轭先验分布！&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;继续拿上面这个例子，如果我们把先验分布 π(θ) 设为贝塔分布 Beta(a,b)，结果是什么呢？我就不写具体的计算过程啦，直接给结果：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.10689655172413794&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bXffxqSNm0XYCMg2rqR9IQkcH94wHatjAvBibdjXpmicq8oJQDAKvY2PeA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;290&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;有没有看到，依然是贝塔分布，结果只是把之前的 1 换成了 a 和 b(聪明的你可能已经发现，其实我们所说的均匀分布 Uni(0,1) 等价于 Beta(1,1),两者是一样的)。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;由此我们便可以称&lt;strong&gt;二项分布的共轭先验分布为贝塔分布！注意！接着画重点！：共轭先验这个概念必须是基于似然函数来讨论的，否则没有意义！&lt;/strong&gt;好，那现在有了共轭先验，然后呢？作用呢？这应该是很多初学者的疑问。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;现在我们来看，如果你知道了一个观测样本的似然函数是二项分布的，那我们把先验分布直接设为 Beta(a,b) ，于是我们就&lt;strong&gt;不用计算复杂的含有积分的贝叶斯公式&lt;/strong&gt;便可得到后验分布 Beta (x+a,n−x+b) 了！！！只需要记住试验次数n，和试验成功事件次数x就可以了！互为共轭的分布还有一些，但都很复杂，用到的情况也很少，推导过程也极其复杂，有兴趣的可以自行搜索。我说的这个情况是最常见的！&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;注意一下，很多资料里会提到一个概念叫伪计数(pseudo count),这里的伪计数值得就是a,b对后验概率分布的影响，我们会发现如果我们取 Beta(1,1) ，这个先验概率对结果的影响会很小，可如果我们设为 Beta(100,100)，那么我们做 10 次试验就算是全是正面的，后验分布都没什么变化。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;▌&lt;/span&gt;朴素贝叶斯分类器(Naive Bayes classifier)和scikit-learn的简单使用&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在机器学习中你应该会看到有一个章节是讲朴素贝叶斯分类器的(把naive翻译成朴素我也是服了啊，以后我们可以“夸”某某人好朴素啊)。具体的数学原理在周志华老师的西瓜书《机器学习》的第7章有详细解释，其实就是利用了基本的贝叶斯理论，跟上面说的差不多，只不过更加说明的怎样去实践到机器学习中。下面就直接简单说一下Python中有个机器学习库 scikit-learn 中朴素贝叶斯分类器的简单实用。例子参考的是 scikit-learn 官网的GaussianNB 页面。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;直接看代码：&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot; readability=&quot;24&quot;&gt;&lt;pre&gt;
&lt;br/&gt;import numpy as np&lt;br/&gt;X = np.array(&lt;span class=&quot;&quot;&gt;[[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]&lt;/span&gt;)&lt;br/&gt;Y = np.array([&lt;span class=&quot;&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;&quot;&gt;1&lt;/span&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;

    
, &lt;span class=&quot;&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;&quot;&gt;2&lt;/span&gt;])&lt;br/&gt;from sklearn.naive_bayes import GaussianNB #导入GaussianNB&lt;br/&gt;clf = GaussianNB() #设置clf为高斯朴素贝叶斯分类器&lt;br/&gt;clf.fit(X, Y) #训练数据&lt;br/&gt;&lt;span class=&quot;&quot;&gt;print&lt;/span&gt;(clf.predict(&lt;span class=&quot;&quot;&gt;[[-1, 0]]&lt;/span&gt;)) #预测数据[&lt;span class=&quot;&quot;&gt;-1&lt;/span&gt;,&lt;span class=&quot;&quot;&gt;0&lt;/span&gt;]属于哪一类&lt;br/&gt;&lt;/pre&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;输出结果为: [1]。 这里面我们可以看到有 X,Y 两个变量，X是我们要训练的数据特征，而Y给的是对应数据的标签，分成 [1],[2] 两类。用 clf.fit 训练好了之后，用 clf.predict 预测新数据[-1,0]，结果是被分为第一类，说明结果是令人满意的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;MCMC(Markov chain Monte Carlo)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;你以为说到这贝叶斯的事情就结束了？那你真的就是太 naive 了。贝叶斯公式里的 θ 只是一个参数，有没有想过有两个参数怎么办？还能怎么办，分布的积分改成双重积分呗。可以可以，那如果有 3个、5个、10 个参数呢？还有十重积分嘛？很显然积分这个工具只适合我们在一维和二维的情况下进行计算，三维以上的效果就已经不好了；其实不仅仅在于多维情况，就算是在一维情况很多积分也很难用数值方法计算出来，那该怎么办？于是便有了 MCMC 方法，全称是&lt;strong&gt;马尔科夫链蒙特卡洛方法&lt;/strong&gt;。大家别指望在下文里看到详细的计算过程和推导，我还是按照我的理解，简单地从原理出发进行描述，让大家有一个感性的认识。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;第二个MC：蒙特卡洛方法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;虽然蒙特卡洛方法是 MCMC 中的第二个 MC，但先解释蒙特卡洛方法会更加容易理解。蒙特卡洛方法也称蒙特卡洛抽样方法，其基本思想是通过大量取样来近似得到想要的答案。有一个经典的试验就是计算圆周率，在一个边上为1的正方形中画一个内切圆，圆的面积就是 π，圆面积比上整体的正方形面积也是 ππ, 现在在正方形内产生大量随机数，最后我们只需要计算在圆内点的个数比上总体点的个数，便近似得到了圆周率 π 的值(这些统计学家是真聪明啊。。。)。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;现在回到贝叶斯公式，我们现在有一个后验概率 π(θ|x) ，但我们其实最想知道的是 h(θ) 的后验期望：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.20622568093385213&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bXfkMBNvchHHmoCglQbPY6gnGHZcxktp0hnHKO7icia0G7vZ6OBib7b5bTQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;257&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;怎么又跑出来一个 h(θ) ？不要着急，如果我们令 h(θ)=θ , 那上面这个积分求得就是我们想要的后验期望估计了！(当然 h(θ) 还可以是其他情况，会得到其他我们想要的结果，例如后验最大估计，后验方差等等，这里就不赘述了) 蒙特卡洛方法指出：如果我们可以从后验概率分布 π(θ|x) 中抽取大量的独立同分布(i.i.d)的观测值 θ1,θ2,…,θm ,于是我们可以用如下公式：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.3230769230769231&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bX1alSUfQmuicQbKf6ibFSnFNdVUJQtJm9c6kRuaA5TZuWy2CbSuTvCvFA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;195&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在大数定理的支持下，hm 就可看作是 E[h(θ)|x] 的近似值。但是这个方法在多维和后验分布形式未知的情况下，很难抽样，于是便有了第一个 MC，马尔科夫链的方法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;第一个MC：马尔科夫链&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;马尔科夫链也称之为马氏链，先来看一下数学定义：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.09624413145539906&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bX1u3S4WpVEoKNkt2SC97lQvoMO14uia4X99GibS3k3EhWVWFf2OKH5PXQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;426&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;意思就是，从 Xn,Xn−1,…,X0 到 Xn+1 的转移概率只与 Xn+1 的前一个状态 Xn 有关。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如果条件概率 P(Xn+1|Xn) 与 n 无关，成为一个固定值，那么就称这个马氏链有平稳转移概率，记为 pij 。并且我们称 P=(pij) 为马氏链的转移矩阵，且满足条件：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.31413612565445026&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bXHiaSodNRfibp8Pcd1rY9XicOvvevQJqFfpe7DibUbvHa0wVia0P0ZGNBF5w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;191&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;怎么一个概率变成一个矩阵了？？？其实这个转移概率 pij 指的只是状态 i 中的一个观测值 Xn 到状态 j 中的另一个观测值 Xn+1 的概率，其实我们在每个状态下许许多多的观测值。我随便举一例子：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;现有一个转换矩阵：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.45241379310344826&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjy4ALbecuZ91UsBZPQP54bXEQrLtV531WJXiaxWTGts4N3ibFbrnSTktI3vCojE6wwGBukyo1zpr1qw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;725&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;可以看出状态 i 中的一个观测值转移到下个状态 j 的分别三个观测值的概率和为1。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;下面就是最最重要的马氏链的平稳性(也可称之为收敛性)：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;设马尔科夫链有转移概率矩阵&lt;/span&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.3048780487804878&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjyw8LR5jvj4RcfXWgKBHMZcjZBraicQ2bcp4bhDQWGiaXRY8eMcibIiaUsvcK5gj7TsasHXQ9pClJzicTA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;82&quot; width=&quot;58px&quot;/&gt;&lt;span&gt;, 一个概率分布&lt;/span&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.18803418803418803&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjyw8LR5jvj4RcfXWgKBHMZcH5IuTmiaWSe2PueicMlQXQ4wwv5QZdjKyEr1tF5jsu5MX9ia7WLOiaicp6g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;117&quot;/&gt;&lt;span&gt;如果满足 &lt;/span&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.2032520325203252&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ptp8P184xjyw8LR5jvj4RcfXWgKBHMZcKXABiaAKAPB6V93WxysFV2ibZBaFWkCOEhWo8SZdLl2sL7OUCULeUFibg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;123&quot; width=&quot;102px&quot;/&gt;&lt;span&gt;，&lt;span&gt;则称之为此马尔科夫链的平稳分布。（取自[1]中定义6.3.2）&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;可能这么看这个定义还是有点绕，这里的 i,j 并不是指从 i 一步就到了 j ,求和符号 ∑ 的意思就是能让概率分布 π(i) 经过 n 步之后成为平稳分布 π(j) 。我们得到的平稳分布 π(j)=[π(1),π(2),…,π(j)] 里面各个概率的和也为1。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  现在我们就要把这个马尔科夫链和贝叶斯联系起来，按照我的理解，π(i) 就是我们的先验分布，如果我们能找到一个转移矩阵，那么我们就会在n步之后就会收敛到一个平稳分布，而这个分布就是我们要的后验分布。得到平稳分布后，根据平稳性，继续乘上这个转移概率矩阵，平稳分布依然不会改变，所以我们就从得到平稳分布开始每次对其中抽样 1 个出来，再经过 m 步之后，我们就得到了 m 个服从后验分布的 i.i.d 样本，便可按照第二个 MC 蒙特卡洛方法进行计算了！&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当然，还有很多理论是关于如何找这个转移概率矩阵的，和算法如何实现的，但这些都太复杂了，就不在这里说了。MCMC 是一个有着完整体系的东西，市面上都很少有详细介绍其理论过程的资料，有的话那都不是几十页纸能讲完的。有一些很好的讲解可参考资料[1][4]，尤其是[4]是个很有名的文档，讲得非常通俗易懂、深入浅出，我在这两个部分写的少的原因就是如果写多了也就是把这些资料对应部分重新说一遍，所以有兴趣的就直接看这些资料吧。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;MCMC 的 Python 实现——Pymc&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;原本想在这里详细介绍一个例子的，但终究还是别人的例子，还是去看原资料比较好，见[4]。注意文件的文件是 ipython 的格式，用 anaconda 里的 Jupyter notebook 打开就行。还有要注意的是每个章节的内容分为了pymc2 和 pymc3 两个库的实现。pymc3 需要依赖项 theano 才行，我一开始就一直运行不起来，原来 theano 只能在Python&amp;lt;3.6和scipy&amp;lt;0.17.1 才能运行。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;▌&lt;/span&gt;结尾&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这篇文章主要介绍了贝叶斯统计的数学思想，这篇文章从头到尾写了有近十个小时了，希望能对大家有所帮助，如果有任何错误和解释不当的地方，请给我评论，我也只是个初学者，也希望能得到大神的指点。&lt;/span&gt;&lt;/p&gt;

&lt;blockquote readability=&quot;16&quot;&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;References&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;[1]韦来生，《贝叶斯统计》，高等教育出版社，2016&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;[2]John A.Rice, 《数理统计与数据分析》(原书第三版), 机械工业出版社, 2016&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;[3]Cameron Davidson-Pilon, Probabilistic Programming and Bayesian Methods for Hackers, 2016&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;[4]Rickjin(靳志辉)，《LDA数学八卦》，2013&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;

&lt;/blockquote&gt;
&lt;p&gt;&lt;span&gt;（编辑注：文中题图来自sciencenews）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;∑编辑 | Gemini&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;来源 | 徐炎琨的知乎问答&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.0437601296596435&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky7x6u1VxMVMia4MLibNzC2nrumY3zDflTsCeoM04M1BrkvPny8tsw6hYkIicUr42iarLmadL2x6JwV6A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;617&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域，经采用我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Tue, 03 Jul 2018 13:16:54 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/8rXKQMi9Yd</dc:identifier>
</item>
</channel>
</rss>