<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fmathalg-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/mathalg-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>算法与数学之美</title>
<link>http://www.jintiankansha.me/column/c9dZ5TM2aS</link>
<description>算法与数学之美 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>谷歌背后的数学</title>
<link>http://www.jintiankansha.me/t/ySGWo6QCUc</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/ySGWo6QCUc</guid>
<description>&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;92498&quot;&gt;&lt;section&gt;&lt;section background-size=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot; fmt=&quot;png&quot; https=&quot;&quot; important=&quot;&quot; left=&quot;&quot; mmbiz=&quot;&quot; no-repeat=&quot;&quot; png=&quot;&quot; px=&quot;&quot; tjtgiabky1osjzibhbaikrwhib9yrxicav6silioptqyydibibp2zyukuwxkkhc4miasz5h2pxiac07jt6vzoriaqbhw=&quot;&quot; top=&quot;&quot; wx=&quot;&quot;&gt;&lt;section background-size=&quot;&quot; bottom=&quot;&quot; break-word=&quot;&quot; fmt=&quot;png&quot; https=&quot;&quot; important=&quot;&quot; mmbiz=&quot;&quot; no-repeat=&quot;&quot; png=&quot;&quot; px=&quot;&quot; right=&quot;&quot; tjtgiabky1osjzibhbaikrwhib9yrxica7hx90xnibebfloa2f2ojzjrufhrcvfoian9k9woxhpoxoayqqlgtddka=&quot;&quot; wx=&quot;&quot;&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot;&gt;一. 引言&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;在如今这个互联网时代， 有一家公司家喻户晓——它自 1998 年问世以来， 在极短的时间内就声誉鹊起， 不仅超越了所有竞争对手， 而且彻底改观了整个互联网的生态。 这家公司就是当今互联网上的第一搜索引擎： 谷歌 (Google)。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在这样一家显赫的公司背后， 自然有许许多多商战故事， 也有许许多多成功因素。 但与普通商战故事不同的是， 在谷歌的成功背后起着最关键作用的却是一个数学因素。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;本文要谈的就是这个数学因素。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;谷歌作为一个搜索引擎， 它的核心功能顾名思义， 就是网页搜索。 说到搜索， 我们都不陌生， 因为那是凡地球人都会的技能。 我们在字典里查个生字， 在图书馆里找本图书， 甚至在商店里寻一种商品， 等等， 都是搜索。 只要稍稍推究一下， 我们就会发现那些搜索之所以可能， 并且人人都会， 在很大程度上得益于以下三条：&lt;/span&gt;&lt;/p&gt;

&lt;blockquote readability=&quot;13&quot;&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;搜索对象的数量较小——比如一本字典收录的字通常只有一两万个， 一家图书馆收录的不重复图书通常不超过几十万种， 一家商店的商品通常不超过几万种， 等等。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;span&gt;搜索对象具有良好的分类或排序——比如字典里的字按拼音排序， 图书馆里的图书按主题分类， 商店里的商品按品种或用途分类， 等等。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;span&gt;搜索结果的重复度较低——比如字典里的同音字通常不超过几十个， 图书馆里的同名图书和商店里的同种商品通常也不超过几十种， 等等。&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;但互联网的鲜明特点却是以上三条无一满足。 事实上， 即便在谷歌问世之前， 互联网上的网页总数就已超过了诸如图书馆藏书数量之类传统搜索对象的数目。 而且这还只是冰山一角， 因为与搜索图书时单纯的书名搜索不同， 互联网上的搜索往往是对网页内容的直接搜索， 这相当于将图书里的每一个字都变成了搜索对象， 由此导致的数量才是真正惊人的， 它不仅直接破坏了上述第一条， 而且连带破坏了二、 三两条。 在互联网发展的早期， 象雅虎 (Yahoo) 那样的门户网站曾试图为网页建立分类系统， 但随着网页数量的激增， 这种做法很快就 “挂一漏万” 了。 而搜索结果的重复度更是以快得不能再快的速度走向失控。 这其实是可以预料的， 因为几乎所有网页都离不开几千个常用词， 因此除非搜索生僻词， 否则出现几十万、 几百万、 甚至几千万条搜索结果都是不足为奇的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;互联网的这些 “不良特点” 给搜索引擎的设计带来了极大的挑战。 而在这些挑战之中， 相对来说， 对一、 二两条的破坏是比较容易解决的， 因为那主要是对搜索引擎的存储空间和计算能力提出了较高要求， 只要有足够多的钱来买 “装备”， 这些都还能算是容易解决的——套用电视连续剧《蜗居》中某贪官的台词来说， “能用钱解决的问题就不是大问题”。 但对第三条的破坏却要了命了， 因为无论搜索引擎的硬件如何强大， 速度如何快捷， 要是搜索结果有几百万条， 那么任何用户想从其中 “海选” 出自己真正想要的东西都是几乎不可能的。 这一点对早期搜索引擎来说可谓是致命伤， 而且它不是用钱就能解决的问题。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.0902061855670102&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicafbpKtB9Sic4ViaARTMmWgJeIbeDA7BPZ05TW10VtpsdNqrYby4icVy3jQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;388&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这致命伤该如何治疗呢？ 药方其实很简单， 那就是对搜索结果进行排序， 把用户最有可能需要的网页排在最前面， 以确保用户能很方便地找到它们。 但问题是： 网页的水平千差万别， 用户的喜好更是万别千差， 互联网上有一句流行语叫做： “在互联网上， 没人知道你是一条狗” (On the Internet, nobody knows you're a dog)。 连用户是人是狗都 “没人知道”， 搜索引擎又怎能知道哪些搜索结果是用户最有可能需要的， 并对它们进行排序呢？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在谷歌主导互联网搜索之前， 多数搜索引擎采用的排序方法， 是以被搜索词语在网页中的出现次数来决定排序——出现次数越多的网页排在越前面。 这个判据不能说毫无道理， 因为用户搜索一个词语， 通常表明对该词语感兴趣。 既然如此， 那该词语在网页中的出现次数越多， 就越有可能表示该网页是用户所需要的。 可惜的是， 这个貌似合理的方法实际上却行不大通。 因为按照这种方法， 任何一个象祥林嫂一样翻来复去倒腾某些关键词的网页， 无论水平多烂， 一旦被搜索到， 都立刻会 “金榜题名”， 这简直就是广告及垃圾网页制造者的天堂。 事实上， 当时几乎没有一个搜索引擎不被 “祥林嫂” 们所困扰， 其中最具讽刺意味的是： 在谷歌诞生之前的 1997 年 11 月， 堪称早期互联网巨子的当时四大搜索引擎在搜索自己公司的名字时， 居然只有一个能使之出现在搜索结果的前十名内， 其余全被 “祥林嫂” 们挤跑了。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;92498&quot;&gt;&lt;section&gt;&lt;section background-size=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot; fmt=&quot;png&quot; https=&quot;&quot; important=&quot;&quot; left=&quot;&quot; mmbiz=&quot;&quot; no-repeat=&quot;&quot; png=&quot;&quot; px=&quot;&quot; tjtgiabky1osjzibhbaikrwhib9yrxicav6silioptqyydibibp2zyukuwxkkhc4miasz5h2pxiac07jt6vzoriaqbhw=&quot;&quot; top=&quot;&quot; wx=&quot;&quot;&gt;&lt;section background-size=&quot;&quot; bottom=&quot;&quot; break-word=&quot;&quot; fmt=&quot;png&quot; https=&quot;&quot; important=&quot;&quot; mmbiz=&quot;&quot; no-repeat=&quot;&quot; png=&quot;&quot; px=&quot;&quot; right=&quot;&quot; tjtgiabky1osjzibhbaikrwhib9yrxica7hx90xnibebfloa2f2ojzjrufhrcvfoian9k9woxhpoxoayqqlgtddka=&quot;&quot; wx=&quot;&quot;&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot;&gt;二. 基本思路&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;正是在这种情况下， 1996 年初， 谷歌公司的创始人， 当时还是美国斯坦福大学 (Stanford University) 研究生的佩奇 (Larry Page) 和布林 (Sergey Brin) 开始了对网页排序问题的研究。 这两位小伙子之所以研究网页排序问题， 一来是导师的建议 (佩奇后来称该建议为 “我有生以来得到过的最好建议”)， 二来则是因为他们对这一问题背后的数学产生了兴趣。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;网页排序问题的背后有什么样的数学呢？ 这得从佩奇和布林看待这一问题的思路说起。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在佩奇和布林看来， 网页的排序是不能靠每个网页自己来标榜的， 无论把关键词重复多少次， 垃圾网页依然是垃圾网页。 那么， 究竟什么才是网页排序的可靠依据呢？ 出生于书香门第的佩奇和布林 (两人的父亲都是大学教授) 想到了学术界评判学术论文重要性的通用方法， 那就是看论文的引用次数。 在互联网上， 与论文的引用相类似的是显然是网页的链接。 因此， 佩奇和布林萌生了一个网页排序的思路， 那就是通过研究网页间的相互链接来确定排序。 具体地说， 一个网页被其它网页链接得越多， 它的排序就应该越靠前。 不仅如此， 佩奇和布林还进一步提出， 一个网页越是被排序靠前的网页所链接， 它的排序就也应该越靠前。 这一条的意义也是不言而喻的， 就好比一篇论文被诺贝尔奖得主所引用， 显然要比被普通研究者所引用更说明其价值。 依照这个思路， 网页排序问题就跟整个互联网的链接结构产生了关系， 正是这一关系使它成为了一个不折不扣的数学问题。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;思路虽然有了， 具体计算却并非易事， 因为按照这种思路， 想要知道一个网页&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.1363636363636365&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaKEZma3uMK0m3MnpR4pM9cgRia5UzXOatMKvHlOISCjYjeugCnb5CGVw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;22&quot;/&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  的排序， 不仅要知道有多少网页链接了它， 而且还得知道那些网页各自的排序——因为来自排序靠前网页的链接更有分量。 但作为互联网大家庭的一员，&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.1363636363636365&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaKEZma3uMK0m3MnpR4pM9cgRia5UzXOatMKvHlOISCjYjeugCnb5CGVw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;22&quot;/&gt;本身对其它网页的排序也是有贡献的， 而且基于来自排序靠前网页的链接更有分量的原则， 这种贡献与&lt;/span&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.1363636363636365&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaKEZma3uMK0m3MnpR4pM9cgRia5UzXOatMKvHlOISCjYjeugCnb5CGVw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;22&quot;/&gt;&lt;span&gt;本身的排序也有关。 这样一来， 我们就陷入了一个 “先有鸡还是先有蛋” 的循环： 要想知道&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.1363636363636365&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaKEZma3uMK0m3MnpR4pM9cgRia5UzXOatMKvHlOISCjYjeugCnb5CGVw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;22&quot;/&gt;的排序， 就得知道与它链接的其它网页的排序， 而要想知道那些网页的排序， 却又首先得知道&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.1363636363636365&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaKEZma3uMK0m3MnpR4pM9cgRia5UzXOatMKvHlOISCjYjeugCnb5CGVw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;22&quot;/&gt;的排序。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为了打破这个循环， 佩奇和布林采用了一个很巧妙的思路， 即分析一个虚拟用户在互联网上的漫游过程。 他们假定： 虚拟用户一旦访问了一个网页后， 下一步将有相同的几率访问被该网页所链接的任何一个其它网页。 换句话说， 如果网页 &lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.1363636363636365&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaKEZma3uMK0m3MnpR4pM9cgRia5UzXOatMKvHlOISCjYjeugCnb5CGVw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;22&quot;/&gt;有&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.25&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxica6PvHjoEyCCnfibuv7sriaWRb4CUcMa27uzkoib9laxfl4Vrfyud4MzGcA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;16&quot;/&gt;&lt;span&gt;个对外链接， 则虚拟用户在访问了&lt;/span&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.1363636363636365&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaKEZma3uMK0m3MnpR4pM9cgRia5UzXOatMKvHlOISCjYjeugCnb5CGVw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;22&quot;/&gt;&lt;span&gt;之后， 下一步点击那些链接当中的任何一个的几率均为&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.696969696969697&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicacJwogTiboC1hcg2QHjgMQFSYnOG8xQa9FvxbRAvbB3O6Bt3LXNYt3yQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;33&quot;/&gt;&lt;span&gt;。 初看起来， 这一假设并不合理， 因为任何用户都有偏好， 怎么可能以相同的几率访问一个网页的所有链接呢？ 但如果我们考虑到佩奇和布林的虚拟用户实际上是对互联网上全体用户的一种平均意义上的代表， 这条假设就不象初看起来那么不合理了。 那么网页的排序由什么来决定呢？ 是由该用户在漫游了很长时间——理论上为无穷长时间——后访问各网页的几率分布来决定， 访问几率越大的网页排序就越靠前。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为了将这一分析数学化， 我们用&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7647058823529411&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxica1BeibrxfjKGQ3SWZWwrpHuddokXSTvOAl24lgtOLJRs3tcNvpDjUa3w/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;34&quot;/&gt;&lt;span&gt;表示虚拟用户在进行第 n 次浏览时访问网页&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.1363636363636365&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaKEZma3uMK0m3MnpR4pM9cgRia5UzXOatMKvHlOISCjYjeugCnb5CGVw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;22&quot;/&gt;&lt;span&gt;的几率。 显然， 上述假设可以表述为 (请读者自行证明)：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.18032786885245902&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaRCGQ0unyH7WCc7ezn748Hc1DEZQgmbLbBKhJRAt0PqLCXSv4JliaEJg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;183&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这里 pj→i 是一个描述互联网链接结构的指标函数 (indicator function)， 其定义是： 如果网页&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.15&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaJX9xuD51ffMUz9Uonp551NcjLRdHUZbGYnfWoEu1hlNnQLx7Y2TzOA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;20&quot;/&gt;&lt;span&gt; 有链接指向网页&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.1363636363636365&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaKEZma3uMK0m3MnpR4pM9cgRia5UzXOatMKvHlOISCjYjeugCnb5CGVw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;22&quot;/&gt;&lt;span&gt;， 则&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.263157894736842&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaRUtZXya46LNOwvXD4VG4ySNw9vIenymibHcWcCXZgy8giaVuG4ahzzpw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;19&quot;/&gt;&lt;span&gt;→i 取值为 1， 反之则为 0。 显然， 这条假设所体现的正是前面提到的佩奇和布林的排序原则， 因为右端求和式的存在表明与&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.1363636363636365&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaKEZma3uMK0m3MnpR4pM9cgRia5UzXOatMKvHlOISCjYjeugCnb5CGVw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;22&quot;/&gt;&lt;span&gt;有链接的所有网页&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.15&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaJX9xuD51ffMUz9Uonp551NcjLRdHUZbGYnfWoEu1hlNnQLx7Y2TzOA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;20&quot;/&gt;&lt;span&gt; 都对&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.1363636363636365&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaKEZma3uMK0m3MnpR4pM9cgRia5UzXOatMKvHlOISCjYjeugCnb5CGVw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;22&quot;/&gt;&lt;span&gt;的排名有贡献， 而求和式中的每一项都正比于 pj， 则表明来自那些网页的贡献与它们的自身排序有关， 自身排序越靠前 (即&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.263157894736842&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaRUtZXya46LNOwvXD4VG4ySNw9vIenymibHcWcCXZgy8giaVuG4ahzzpw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;19&quot;/&gt;&lt;span&gt;越大)， 贡献就越大。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为符号简洁起见， 我们将虚拟用户第 n 次浏览时访问各网页的几率合并为一个列向量&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9523809523809523&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxica5SgedaCP2ooVgJKepk2NmeJ0wYcQGczbZmU7CQfYy91MpNGYZjGGLg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;21&quot;/&gt;&lt;span&gt;， 它的第 i 个分量为&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7058823529411765&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaIXFw4AUQarvlicMWF948KMtxp6MTiaD0p8Etvc87xQCdTACPP3793ZFA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;34&quot;/&gt;&lt;span&gt;， 并引进一个只与互联网结构有关的矩阵 H， 它的第 i 行 j 列的矩阵元为&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.30434782608695654&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxica77HGe3Q35mmtel1gxjOP8HyGPUt1TW05XfaBO3LrvsEy83ZISlVxnw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;92&quot;/&gt;&lt;span&gt;， 则上述公式可以改写为：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.2558139534883721&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaRaSPUBcibbQwPEx8LXDGibicPZWJg6xUN65zRibkRcG5BzBnlQ8hsZ9ibibw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;86&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这就是计算网页排序的公式。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;熟悉随机过程理论的读者想必看出来了， 上述公式描述的是一种马尔可夫过程 (Markov process)， 而且是其中最简单的一类， 即所谓的平稳马尔可夫过程 (stationary Markov process)[注一]， 而 H 则是描述马尔可夫过程中的转移概率分布的所谓转移矩阵 (transition matrix)。 不过普通马尔可夫过程中的转移矩阵通常是随机矩阵 (stochastic matrix)， 即每一列的矩阵元之和都为 1 的矩阵 (请读者想一想， 这一特点的 “物理意义” 是什么？)[注二]。 而我们的矩阵 H 却可能有一些列是零向量， 从而矩阵元之和为 0， 它们对应于那些没有对外链接的网页， 即所谓的 “悬挂网页” (dangling page)[注三]。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;上述公式的求解是简单得不能再简单的事情， 即：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.29213483146067415&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaI4yYspPAhbYopwHK4G4W3qJLjcIrQ4bZOTYBFD2rUafv20LMrIwTvQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;89&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;其中 p0 为虚拟读者初次浏览时访问各网页的几率分布 (在佩奇和布林的原始论文中， 这一几率分布被假定为是均匀分布)。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;92498&quot;&gt;&lt;section&gt;&lt;section background-size=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot; fmt=&quot;png&quot; https=&quot;&quot; important=&quot;&quot; left=&quot;&quot; mmbiz=&quot;&quot; no-repeat=&quot;&quot; png=&quot;&quot; px=&quot;&quot; tjtgiabky1osjzibhbaikrwhib9yrxicav6silioptqyydibibp2zyukuwxkkhc4miasz5h2pxiac07jt6vzoriaqbhw=&quot;&quot; top=&quot;&quot; wx=&quot;&quot;&gt;&lt;section background-size=&quot;&quot; bottom=&quot;&quot; break-word=&quot;&quot; fmt=&quot;png&quot; https=&quot;&quot; important=&quot;&quot; mmbiz=&quot;&quot; no-repeat=&quot;&quot; png=&quot;&quot; px=&quot;&quot; right=&quot;&quot; tjtgiabky1osjzibhbaikrwhib9yrxica7hx90xnibebfloa2f2ojzjrufhrcvfoian9k9woxhpoxoayqqlgtddka=&quot;&quot; wx=&quot;&quot;&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot;&gt;三. 问题及解决&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;如前所述， 佩奇和布林是用虚拟用户在经过很长——理论上为无穷长——时间的漫游后访问各网页的几率分布， 即&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3561643835616438&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxica5aALSO4jXxBlyXcJswazRzKqwsgBxPicmkQgRicBjcLGQrRlKPUxxd9Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;73&quot;/&gt;&lt;span&gt;， 来确定网页排序的。 这个定义要想管用， 显然要解决三个问题：&lt;/span&gt;&lt;/p&gt;

&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;&lt;span&gt;极限&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3561643835616438&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxica5aALSO4jXxBlyXcJswazRzKqwsgBxPicmkQgRicBjcLGQrRlKPUxxd9Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;73&quot;/&gt;&lt;span&gt;是否存在？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如果极限存在， 它是否与&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.2&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaBNzw4ib4T56mdYyjaVJvP0JDt2GnsOUAgjL65VYfiaMwn5DJkjre4erQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;20&quot;/&gt;&lt;span&gt;的选取无关？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如果极限存在， 并且与&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.2&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaBNzw4ib4T56mdYyjaVJvP0JDt2GnsOUAgjL65VYfiaMwn5DJkjre4erQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;20&quot;/&gt;&lt;span&gt;的选取无关， 它作为网页排序的依据是否真的合理？&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;如果这三个问题的答案都是肯定的， 那么网页排序问题就算解决了。 反之， 哪怕只有一个问题的答案是否定的， 网页排序问题也就不能算是得到了满意解决。 那么实际答案如何呢？ 很遗憾， 是后一种， 而且是其中最糟糕的情形， 即三个问题的答案全都是否定的。 这可以由一些简单的例子看出。 比方说， 在只包含两个相互链接网页的迷你型互联网上， 如果&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.30120481927710846&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaXTdyUw8vz8ibkrYxMeFdqQgUhHW5l7Mo7fAExzee4wnq2IQf5BpF55Q/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;83&quot;/&gt;&lt;span&gt;， 极限就不存在 (因为几率分布将在&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5531914893617021&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaHgwwyVx8pCQ2MZZw0tbK0icqDSojibXZy6zBEvMIMFe9YJicu66UfxOOg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;47&quot;/&gt;&lt;span&gt;和&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.574468085106383&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicadcvM0EVG4kvL0zvFbY7ftILEZtCsvaHribiaIkicic5ykKicXPGH3Pys4Jw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;47&quot;/&gt;&lt;span&gt;之间无穷振荡)。 而存在几个互不连通 (即互不链接) 区域的互联网则会使极限——即便存在——与 p0 的选取有关 (因为把 p0 选在不同区域内显然会导致不同极限)。 至于极限存在， 并且与 p0 的选取无关时它作为网页排序的依据是否真的合理的问题， 虽然不是数学问题， 答案却也是否定的， 因为任何一个 “悬挂网页” 都能象黑洞一样， 把其它网页的几率 “吸收” 到自己身上 (因为虚拟用户一旦进入那样的网页， 就会由于没有对外链接而永远停留在那里)， 这显然是不合理的。 这种不合理效应是如此显著， 以至于在一个连通性良好的互联网上， 哪怕只有一个 “悬挂网页”， 也足以使整个互联网的网页排序失效， 可谓是 “一粒老鼠屎坏了一锅粥”。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为了解决这些问题， 佩奇和布林对虚拟用户的行为进行了修正。 首先， 他们意识到无论真实用户还是虚拟用户， 当他们访问到 “悬挂网页” 时， 都不应该也不会 “在一棵树上吊死”， 而是会自行访问其它网页。 对于真实用户来说， 自行访问的网页显然与各人的兴趣有关， 但对于在平均意义上代表真实用户的虚拟用户来说， 佩奇和布林假定它将会在整个互联网上随机选取一个网页进行访问。 用数学语言来说， 这相当于是把 H 的列向量中所有的零向量都换成 e/N (其中 e 是所有分量都为 1 的列向量， N 为互联网上的网页总数)。 如果我们引进一个描述 “悬挂网页” 的指标向量 (indicator vector) a， 它的第 i 个分量的取值视 Wi 是否为 “悬挂网页” 而定——如果是 “悬挂网页”， 取值为 1， 否则为 0——并用 S 表示修正后的矩阵， 则：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.2743362831858407&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicarSmaZlZEbmMz44j5LggjzKvsDxXCDMYHO7ImxS8FIE0n0wF546bxibw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;113&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;显然， 这样定义的 S 矩阵的每一列的矩阵元之和都是 1， 从而是一个不折不扣的随机矩阵。 这一修正因此而被称为随机性修正 (stochasticity adjustment)。 这一修正相当于剔除了 “悬挂网页”， 从而可以给上述第三个问题带来肯定回答 (当然， 这一回答没有绝对标准， 可以不断改进)。 不过， 这一修正解决不了前两个问题。 为了解决那两个问题， 佩奇和布林引进了第二个修正。 他们假定， 虚拟用户虽然是虚拟的， 但多少也有一些 “性格”， 不会完全受当前网页所限， 死板地只访问其所提供的链接。 具体地说， 他们假定虚拟用户在每一步都有一个小于 1 的几率 α 访问当前网页所提供的链接， 同时却也有一个几率 1-α 不受那些链接所限， 随机访问互联网上的任何一个网站。 用数学语言来说 (请读者自行证明)， 这相当于是把上述 S 矩阵变成了一个新的矩阵 G：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.14814814814814814&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxica1opicavxoQlY3TornPet5NWbogACGr6pZ3lrNYRwrDSTRicoemXxRFvQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;162&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这个矩阵不仅是一个随机矩阵， 而且由于第二项的加盟， 它有了一个新的特点， 即所有矩阵元都为正 (请读者想一想， 这一特点的 “物理意义” 是什么？)， 这样的矩阵是所谓的素矩阵 (primitive matrix)[注四]。 这一修正因此而被称为素性修正 (primitivity adjustment)。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;经过这两类修正， 网页排序的计算方法就变成了：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.4166666666666667&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicaYDx96TibJT2FyHliaUO8dnGicSPwiclFXGFKZwPia1CTcF3XAagf5jicbkCQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;84&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这个算法能给上述问题提供肯定答案吗？ 是的， 它能。 因为随机过程理论中有一个所谓的马尔可夫链基本定理 (fundamental theorem of Markov chains)， 它表明在一个马尔可夫过程中， 如果转移矩阵是素矩阵， 那么上述前两个问题的答案就是肯定的。 而随机性修正已经解决了上述第三个问题， 因此所有问题就都解决了。 如果我们用 p 表示&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9545454545454546&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicamOfTxJx8M9cJpWC6zWteEkCFzdIgexU27tDYmYxiceMgjYD5Icx4GBQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;22&quot;/&gt;&lt;span&gt;的极限[注五]， 则 p 给出的就是整个互联网的网页排序——它的每一个分量就是相应网页的访问几率， 几率越大， 排序就越靠前。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这样， 佩奇和布林就找到了一个不仅含义合理， 而且数学上严谨的网页排序算法， 他们把这个算法称为 PageRank， 不过要注意的是， 虽然这个名称的直译恰好是 “网页排序”， 但它实际上指的是 “佩奇排序”， 因为其中的 “Page” 不是指网页， 而是佩奇的名字。 这个算法就是谷歌排序的数学基础， 而其中的矩阵 G 则被称为谷歌矩阵 (Google matrix)。 &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;细心的读者可能注意到了， 我们还遗漏了一样东西， 那就是谷歌矩阵中描述虚拟用户 “性格” 的那个 α 参数。 那个参数的数值是多少呢？ 从理论上讲， 它应该来自于对真实用户平均行为的分析， 不过实际上另有一个因素对它的选取产生了很大影响， 那就是&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;&quot; data-ratio=&quot;0.55&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicasibsLXstibrcJVCgtTLn9SMpSpKf8k6LVt15xIde68n9u2SqrrBZoIrQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;40&quot;/&gt;&lt;span&gt;收敛于 p 的快慢程度。 由于 G 是一个 N×N 矩阵， 而 N 为互联网上——确切地说是被谷歌所收录的——网页的总数， 在谷歌成立之初为几千万， 目前为几百亿 (并且还在持续增加)， 是一个极其巨大的数字。 因此 G 是一个超大型矩阵， 甚至很可能是人类有史以来处理过的最庞大的矩阵。 对于这样的矩阵，&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.55&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicasibsLXstibrcJVCgtTLn9SMpSpKf8k6LVt15xIde68n9u2SqrrBZoIrQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;40&quot;/&gt;&lt;span&gt;收敛速度的快慢是关系到算法是否实用的重要因素， 而这个因素恰恰与 α 有关。 可以证明， α 越小，&lt;/span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.55&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicasibsLXstibrcJVCgtTLn9SMpSpKf8k6LVt15xIde68n9u2SqrrBZoIrQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;40&quot;/&gt;&lt;span&gt;的收敛速度就越快。 但 α 也不能太小， 因为太小的话， “佩奇排序” 中最精华的部分， 即以网页间的彼此链接为基础的排序思路就被弱化了 (因为这部分的贡献正比于 α)， 这显然是得不偿失的。 因此， 在 α 的选取上有很多折衷的考虑要做， 佩奇和布林最终选择的数值是 α = 0.85。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;以上就是谷歌背后最重要的数学奥秘。 与以往那种凭借关键词出现次数所作的排序不同， 这种由所有网页的相互链接所确定的排序是不那么容易做假的， 因为做假者再是把自己的网页吹得天花乱坠， 如果没有真正吸引人的内容， 别人不链接它， 一切就还是枉然[注六]。 而且 “佩奇排序” 还有一个重要特点， 那就是它只与互联网的结构有关， 而与用户具体搜索的东西无关。 这意味着排序计算可以单独进行， 而无需在用户键入搜索指令后才临时进行。 谷歌搜索的速度之所以快捷， 在很大程度上得益于此。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;92498&quot;&gt;&lt;section&gt;&lt;section background-size=&quot;&quot; border-box=&quot;&quot; break-word=&quot;&quot; fmt=&quot;png&quot; https=&quot;&quot; important=&quot;&quot; left=&quot;&quot; mmbiz=&quot;&quot; no-repeat=&quot;&quot; png=&quot;&quot; px=&quot;&quot; tjtgiabky1osjzibhbaikrwhib9yrxicav6silioptqyydibibp2zyukuwxkkhc4miasz5h2pxiac07jt6vzoriaqbhw=&quot;&quot; top=&quot;&quot; wx=&quot;&quot;&gt;&lt;section background-size=&quot;&quot; bottom=&quot;&quot; break-word=&quot;&quot; fmt=&quot;png&quot; https=&quot;&quot; important=&quot;&quot; mmbiz=&quot;&quot; no-repeat=&quot;&quot; png=&quot;&quot; px=&quot;&quot; right=&quot;&quot; tjtgiabky1osjzibhbaikrwhib9yrxica7hx90xnibebfloa2f2ojzjrufhrcvfoian9k9woxhpoxoayqqlgtddka=&quot;&quot; wx=&quot;&quot;&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot;&gt;四. 结语&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.794344473007712&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicauvbgGCBwwiaQMsN1C5Hxic0YyrKtpXEnmiabFdK2SR04iaE45g7icJW3sQw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;389&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在本文的最后， 我们顺便介绍一点谷歌公司的历史。 佩奇和布林对谷歌算法的研究由于需要收集和分析大量网页间的相互链接， 从而离不开硬件支持。 为此， 早在研究阶段， 他们就四处奔走， 为自己的研究筹集资金和硬件。 1998 年 9 月， 他们为自己的试验系统注册了公司——即如今大名鼎鼎的谷歌公司。 但这些行为虽然近乎于创业， 他们两人当时却并无长期从商的兴趣。 1999 年， 当他们觉得打理公司干扰了自己的研究时， 甚至萌生了卖掉公司的想法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;他们的开价是 100 万美元。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;与谷歌在短短几年之后的惊人身价相比， 那简直就是 “跳楼大甩卖”。 可惜当时却无人识货。 佩奇和布林在硅谷 “叫卖” 了一圈， 连一个买家都没找到。 被他们找过的公司包括了当时搜索业巨头之一的 Excite (该公司后来想必连肠子都悔青了)。 为了不让自己的心血荒废， 佩奇和布林只得将公司继续办了下去， 一直办到今天， 这就是谷歌的 “发家史”。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;谷歌成立之初跟其它一些 “发迹于地下室” (one-man-in-basement) 的 IT 公司一样寒酸： 雇员只有一位 (两位老板不算)， 工作场所则是一位朋友的车库。 但它出类拔萃的排序算法很快为它赢得了声誉。 公司成立仅仅 3 个月，《PC Magzine》杂志就把谷歌列为了年度最佳搜索引擎。 2001 年， 佩奇为 “佩奇排序” 申请到了专利， 专利的发明人为佩奇， 拥有者则是他和布林的母校斯坦福大学。 2004 年 8 月， 谷歌成为了一家初始市值约 17 亿美元的上市公司。 不仅公司高管在一夜间成为了亿万富翁， 就连当初给过他们几十美元 “赞助费” 的某些同事和朋友也得到了足够终身养老所用的股票回报。 作为公司摇篮的斯坦福大学则因拥有 “佩奇排序” 的专利而获得了 180 万股谷歌股票。 2005 年 12 月， 斯坦福大学通过卖掉那些股票获得了 3.36 亿美元的巨额收益， 成为美国高校因支持技术研发而获得的有史以来最巨额的收益之一[注七]。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;谷歌在短短数年间就横扫整个互联网， 成为搜索引擎业的新一代霸主， 佩奇和布林的那个排序算法无疑居功至伟， 可以说， 是数学成就了谷歌[补注一]。 当然， 这么多年过去了， 谷歌作为 IT 界研发能力最强的公司之一， 它的网页排序方法早已有了巨大的改进， 由当年单纯依靠 “佩奇排序” 演变为了由 200 多种来自不同渠道的信息——其中包括与网页访问量有关的统计数据——综合而成的更加可靠的方法。 而当年曾给佩奇和布林带来过启示的学术界， 则反过来从谷歌的成功中借鉴了经验， 如今一些学术机构对论文影响因子 (impact factor) 的计算已采用了类似 “佩奇排序” 的算法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在本文的最后， 还有一件事情在这里提一下， 那就是与佩奇和布林研究排序算法几乎同时， 有另外几人也相互独立地沿着类似的思路从事着研究[注八]。 他们中有一位是当时在美国新泽西州工作的中国人， 他的算法后来也成就了一家公司——一家中国公司。 此人的名字叫做李彦宏 (Robin Li)， 他所成就的那家公司就是百度[补注二]。 这些新公司的发展极好地印证了培根 (Francis Bacon) 的一句名言： 知识就是力量。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;注释&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;38&quot;&gt;
&lt;p&gt;&lt;span&gt;马尔可夫过程， 也称为马尔可夫链 (Markov chain)， 是一类离散随机过程， 它的最大特点是每一步的转移概率分布都只与前一步有关。 而平稳马尔可夫过程则是指转移概率分布与步数无关的马尔可夫过程 (体现在我们的例子中， 即 H 与 n 无关)。 另外要说明的是， 本文在表述上不同于佩奇和布林的原始论文， 后者并未使用诸如 “马尔可夫过程” 或 “马尔可夫链” 那样的术语， 也并未直接运用这一领域内的数学定理。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在更细致的分类中， 这种每一列的矩阵元之和都为 1 的随机矩阵称为左随机矩阵 (left stochastic matrix)， 以区别于每一行的矩阵元之和都等于 1 的所谓右随机矩阵 (right stochastic matrix)。 这两者在应用上基本是等价的， 区别往往只在于约定。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这种几乎满足随机矩阵条件， 但有些列 (或行) 的矩阵元之和小于 1 的矩阵也有一个名称， 叫做亚随机矩阵 (substochastic matrix)。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;确切地说， 这种所有矩阵元都为正的矩阵不仅是素矩阵， 而且还是所谓的正矩阵 (positive matrix)。 这两者的区别是： 正矩阵要求所有矩阵元都为正， 而素矩阵只要求自己的某个正整数次幂为正矩阵。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;读者们想必看出来了， p 其实是矩阵 G 的本征值为 1 的本征向量， 而利用虚拟用户确定网页排序的思路其实是在用迭代法解决上述本征值问题。 在数学上可以证明， 上述本征向量是唯一的， 而且 G 的其它本征值 λ 全都满足 |λ|&amp;lt;1 (更准确地说，=&quot;&quot; 是=&quot;&quot; |λ|≤α=&quot;&quot; ——这也正是下文即将提到的&amp;lt;=&quot;&quot; span=&quot;&quot;&amp;gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.55&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky1oSJzibhBaIkrwHib9YRxicasibsLXstibrcJVCgtTLn9SMpSpKf8k6LVt15xIde68n9u2SqrrBZoIrQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;40&quot;/&gt;&lt;span&gt;的收敛速度与 α 有关的原因)。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当然， 这绝不意味着在网页排序上已不可能再做假。 相反， 这种做假在互联网上依然比比皆是， 比如许多广告或垃圾网页制造者用自动程序到各大论坛发贴， 建立对自己网页的链接， 以提高排序， 就是一种常见的做假手法。 为了遏制做假， 谷歌采取了很多技术手段， 并对有些做假网站采取了严厉的惩罚措施。 这种惩罚 (有时是误罚) 对于某些靠互联网吃饭的公司有毁灭性的打击力。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;从投资角度讲， 斯坦福大学显然是过早卖掉了股票， 否则获利将更为丰厚。 不过， 这正是美国名校的一个可贵之处， 它们虽擅长从支持技术研发中获利， 却并不唯利是图。 它们有自己的原则， 那就是不能让商业利益干扰学术研究。 为此， 它们通常不愿长时间持有特定公司的股票， 以免在无形中干扰与该公司存在竞争关系的学术研究的开展。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;那些研究与 “佩奇排序” 的类似仅仅在于大方向 (即都利用互联网的链接结构来决定网页排序)， 而非具体算法类似。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;参考文献&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;D. Austin, How Google Finds Your Needle in the Web's Haystack.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;em&gt;&lt;span&gt;J. Battelle, The Birth of Google, Wired (August 2005).&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;S. Brin and L. Page, The Anatomy of a Large-Scale Hypertextual Web Search Engine, Seventh International World-Wide Web Conference, April 14-18, 1998, Brisbane, Australia.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;O. Ibe, Markov Processes for Stochastic Modeling, (Elsevier Academic Press, 2009).&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;A. N. Langville and C. D. Meyer, Google's PageRank and Beyond: The Science of Search Engine Rankings, (Princeton University Press, 2006).&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;C. Rousseau and Y. Saint-Aubin, Mathematics and Technology, (Springer, 2008).&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;二零一零年十二月四日写于纽约&lt;br/&gt;二零一零年十二月五日发表于本站&lt;br/&gt;http://www.changhai.org/&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;补注&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;有些读者对 “是数学成就了谷歌” 这一说法不以为然， 认为是佩奇和布林的商业才能， 或将数学与商业结合起来的才能成就了谷歌。 这是一个见仁见智的问题， 看法不同不足为奇。 我之所以认为是数学成就了谷歌， 是因为谷歌当年胜过其它搜索引擎的地方只有算法。 除算法外， 佩奇和布林当年并无其它胜过竞争对手的手段， 包括商业手段。 如果让他们去当其它几家搜索引擎公司的老总， 用那几家公司的算法， 他们是不可能脱颖而出的； 而反过来， 如果让其它几家搜索引擎公司的老总来管理谷歌， 用谷歌的算法， 我相信谷歌依然能超越对手。 因此， 虽然谷歌后来确实用过不少出色的商业手段 (任何一家那样巨型的公司都必然有商业手段上的成功之处)， 而当年那个算法在今天的谷歌——如正文所述——则早已被更复杂的算法所取代， 但我认为谷歌制胜的根基和根源在于那个算法， 而非商业手段， 因此我说 “是数学成就了谷歌”。 [2011-01-01]&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;本文被收录于《因为星星在那里： 科学殿堂的砖与瓦》 (清华大学出版社 2015 年 6 月出版) 一书时未保留有关百度的这几句话， 因为实在是越来越觉得百度不配被提及。 不过本站版本由于在 “网友讨论选录” 中已作过说明， 将继续保留之。 [2016-05-03]&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;∑编辑 | Gemini&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;来源 | 卢昌海主页&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.0437601296596435&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;617&quot; width=&quot;auto&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky7x6u1VxMVMia4MLibNzC2nrumY3zDflTsCeoM04M1BrkvPny8tsw6hYkIicUr42iarLmadL2x6JwV6A/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域，经采用我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Tue, 11 Sep 2018 09:04:16 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/ySGWo6QCUc</dc:identifier>
</item>
<item>
<title>为什么有那么多人选择“人工智能”，真的有那么好吗？</title>
<link>http://www.jintiankansha.me/t/YczFxCWNIk</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/YczFxCWNIk</guid>
<description>&lt;p&gt;&lt;span&gt;从17年开始，各大高校的数据科学与大数据技术专业持续火爆，2018年，北京大学、西安交通大学等高校更在本科阶段设立人工智能一级学科，中国顶尖人才的流向在悄然改变……&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5217391304347826&quot; data-type=&quot;png&quot; data-w=&quot;529&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/pojyAtdhQhNzlDz7vTFHpoP1iaw20qxeNoNe6KSn4nziaZiaTCAxdQekJ04BUrXIj1K1WYHZh2MOu38dic5a51OSGw/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;据目前最新的数据显示，AI行业开发人员的月薪基本上维持在10K~50K之间，人工智能岗位的薪酬水平明显高于其他职能岗位。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当看到这些消息，你是否有些心生落寞。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这个时代似乎正在以我们越来越看不懂的方式在发展。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;于你而言，人工智能，是一个闪耀而遥远的字眼吗？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5746691871455577&quot; data-type=&quot;png&quot; data-w=&quot;529&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/pojyAtdhQhNzlDz7vTFHpoP1iaw20qxeNkXCToNAblsm99qpEXBTdh2zNlT5ZOdGZfksXHIwbzIicq1XCKYUwMyg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;处于行业风口的人工智能，岗位溢价让人咋舌；其高级岗位高出整体水平55%，中级岗位高出90%，而初级岗位更是高达110%。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;span&gt;好在，人工智能还处在萌芽阶段。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;学习人工智能最好的时间是五年前，其次就是现在。&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;&quot;&gt;&lt;img class=&quot;__bg_gif&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5625&quot; data-type=&quot;gif&quot; data-w=&quot;480&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/4I1d5nkX896Z3ibGC97fFjWWr8zUlqTqMyxVgwcHhSvdsEg0lUx2ITibwgdZw0VnM5WUQRtLoLjQ6ChPzch29dTA/640?wx_fmt=gif&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;国家需要人工智能人才，世界更需要人工智能人才。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;从战略科学家，到科技公司顾问，再到创业CTO，亦或如你我这样的普通人。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们都在追赶这个时代。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;我们&lt;/span&gt;&lt;span&gt;蛰伏着，一边汲取知识，一边等待时机。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;无论你是其他专业的在校生，还是已经工作的职业人。这都是应该去把握的机会。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;span&gt;第一次工业革命是机械化，它开创了以机器代替劳动的时代。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第二次工业革命是电气化，它促成了世界殖民体系的形成。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第三次工业革命是自动化，它开创了空间、原子能、计算机技术发展的新纪元。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第四次工业革命是智能化，它将促成ABC（人工智能、大数据、云服务）等技术的形成。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;和菜头说：“顺着大浪游泳，怎么都能游得更快一点”。&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;如果你想拥有更多机会&lt;strong&gt;&lt;span&gt;，如果你想要高薪，那就&lt;/span&gt;&lt;/strong&gt;现在开始学习人工智能吧。&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;云博士（美国哈佛大学博士后 &amp;amp;&amp;amp; 浙江大学博士）领衔的人工智能博士团队&lt;/strong&gt;&lt;/span&gt;开发推出了&lt;strong&gt;&lt;span&gt;人工智能机器学习365天特训营（第二期）（实时直播+回放+答疑服务）课程&lt;/span&gt;&lt;/strong&gt;。（扫描最底部二维码联系助教或直接报名课程）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;310&quot; data-backw=&quot;558&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/4I1d5nkX896uAibiaNJG7nukribQhjuZmEwfGzeS2m0B3GSvevicy7xDk8bb5CibbAmXUSs2VRYFCt3Pqictk5zoRfmA/0?wx_fmt=png&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5548098434004475&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;894&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/4I1d5nkX896uAibiaNJG7nukribQhjuZmEwfGzeS2m0B3GSvevicy7xDk8bb5CibbAmXUSs2VRYFCt3Pqictk5zoRfmA/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;section class=&quot;&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section class=&quot;&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section class=&quot;&quot;&gt;&lt;section readability=&quot;1&quot;&gt;&lt;section readability=&quot;2&quot;&gt;&lt;p&gt;&lt;strong&gt;课程提供什么服务？&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;充分考虑到同学们的学习效果和就业情况，&lt;/span&gt;&lt;strong&gt;&lt;span&gt;幂次学院提供4项课程服务&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;。&lt;/span&gt;&lt;span&gt;从发展历程、概念、基本名词、术语、评估方法讲起，到算法模型与实战演练：&lt;/span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;1、名校大牛讲师授课：&lt;/strong&gt;&lt;strong&gt;&lt;strong&gt;云博士（美国哈佛大学博士后 &amp;amp;&amp;amp; 浙江大学博士）授课&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;2、3&lt;/strong&gt;&lt;strong&gt;65天的系统学习周期&lt;/strong&gt;&lt;strong&gt;：&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;直播&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;学习，&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;4年内随时随地回看&lt;strong&gt;&lt;span&gt;直播&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;，&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;在线答疑；&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;3、优质的答疑服务：&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;全天24小时课程问答与社群交流服务，&lt;/span&gt;&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;&lt;strong&gt;让你的每一个问题都能够得到解答&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;，课程资料随时下载。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;4、颁发培训结业证书：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;通过幂&lt;/span&gt;次学院的阶段测试和毕业测试，并颁发幂次学院人工智能培训结业证书。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section class=&quot;&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section class=&quot;&quot;&gt;&lt;section readability=&quot;1&quot;&gt;&lt;section readability=&quot;2&quot;&gt;&lt;p&gt;&lt;strong&gt;怎么学？学多久？什么时候学？&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;合计365+天，每周两次（&lt;strong&gt;&lt;span&gt;每周二19：00-20：00，20：00-21：00&lt;/span&gt;&lt;/strong&gt;）直播，365天130+小时（理论+6个企业级项目实战）&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;课程（讲师直播答疑，课程7*24小时问答服务，学院社群7*24小时交流，课程资料随时下载）&lt;/span&gt;&lt;strong&gt;&lt;span&gt;直播回放4年内随时随地回看。&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section class=&quot;&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section class=&quot;&quot;&gt;&lt;section readability=&quot;1&quot;&gt;&lt;section readability=&quot;2&quot;&gt;&lt;p&gt;&lt;strong&gt;现在报名赠送价值1798元的2门基础课程&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;1. &lt;/span&gt;现在报名免费赠送售价899元&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;的 机器学习之Python编程基础与数据分析 课程，课程内容由&lt;/span&gt;&lt;strong&gt;&lt;span&gt;清华大学python大牛&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;主讲，&lt;/span&gt;&lt;strong&gt;&lt;span&gt;课程内容&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;包括：python基础，python数据分析，python机器学习基础与python在机器学习中的实践案例。&lt;br/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;2. &lt;/span&gt;现在报名免费赠送售价899元&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;的 人工智能数学基础8天集训营 课程，由&lt;/span&gt;&lt;strong&gt;&lt;span&gt;中国科学院计算技术研究所博士团队&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;主讲，&lt;/span&gt;&lt;strong&gt;&lt;span&gt;课程内容&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;包括：矩阵论基础，概率与信息论，数值计算三部分&lt;span&gt;。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5102803738317757&quot; data-s=&quot;300,640&quot; data-type=&quot;jpeg&quot; data-w=&quot;1070&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/4I1d5nkX894Kar3aI8C9PoBa0jB21ev4ZywEDJLDznI2UicZ4fjWUAlabF9yUUJDQUHdNRPCMtAIth8d2aFpHAA/640?wx_fmt=jpeg&quot; width=&quot;668&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;助力您解决人工智能学习中所需要用到的数学知识、Python编程知识。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;立即开始体系化学习，所有知识一步到位，不再需要报名其他任何课程！&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;section class=&quot;&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section class=&quot;&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section class=&quot;&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;strong&gt;跟谁学？&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;strong&gt;云博士：美国哈佛大学大数据分析方向博士后，&lt;strong&gt;&lt;strong&gt;浙江大学计算机科学与技术专业博士，曾任&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;华为高级软件工程师/项目经理&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;发明专利多项，软件著作权多项，国际重要期刊论文数十篇，国家及省部级项目多项，横向项目数十项。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;张博士：中国科学院计算技术研究所机器学习方向博士&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;专注于人机交互、机器学习等领域研究。曾在国内外知名会议期刊发表多篇论文，并荣获人工智能领域会议“最佳论文提名奖”，目前拥有国家发明专利2项、软件著作权1项。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;李金老师：清华大学机器学习方向本硕双清华毕业生，阿里巴巴机器学习方向算法工程师&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;研究方向为：推荐系统，计算机视觉，自然语言处理，深度学习等，在TNNLS，PR等杂志上发表过多篇论文，著有《自学Python—编程基础科学计算及数据分析》一书，P&lt;/span&gt;&lt;span&gt;ython笔记3K+Star，知乎python及机器学习板块12K+ zan，幂次学院签约讲师。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section class=&quot;&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;section&gt;&lt;section&gt;&lt;section class=&quot;&quot;&gt;&lt;section readability=&quot;1&quot;&gt;&lt;section readability=&quot;2&quot;&gt;&lt;p&gt;&lt;strong&gt;学什么？（直播+直播回放+答疑课程大纲）&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;第一部分 基础篇&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;第1章 初识机器学习（直播课程+直播回放+答疑）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.1 引言&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.2 基本术语&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.3 假设空间&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.4 归纳偏好&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.5 发展历程&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1.6 应用现状&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;第2章 模型评估与选择&lt;span&gt;（直播课程+直播回放&lt;span&gt;+答疑&lt;/span&gt;）&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;2.1 经验误差与过拟合&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2 评估方法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2.1 留出法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.2.2 交叉验证法&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Tue, 11 Sep 2018 09:04:14 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/YczFxCWNIk</dc:identifier>
</item>
<item>
<title>一个女程序员征男友的需求说明书</title>
<link>http://www.jintiankansha.me/t/wcVXPPbqpm</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/wcVXPPbqpm</guid>
<description>&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;目的：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;征男友一名&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;概述：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;要求身高1.76以上（因为本人身高1.70），精通C++编程（起码要比我水平高）， 24岁以上因为本人&amp;gt;23岁&amp;amp;&amp;amp;本人&amp;lt;24岁），身体强壮（这样会有安全感），在北京工作（因为本人不打算到别处去），本次征友的主要原因：受不了老妈的热心，次要原因：想找一个志同道和的人。&amp;lt; span=&quot;&quot;&amp;gt;24岁），身体强壮（这样会有安全感），在北京工作（因为本人不打算到别处去），本次征友的主要原因：受不了老妈的热心，次要原因：想找一个志同道和的人。&amp;lt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;本人简介：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;在北京从事计算机业两年，虽然水平不高，但有志于成为一个专家，坚持认为只有从coder做起才会真正成为高手，崇拜c++高手，业余时间喜欢音乐和足球。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;UseCase1：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;基本路径：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1：你是一个真诚的人，不是玩玩而已&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2：留给我你的基本条件及基本联系方式&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3：我认为合适会联络你&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4：尝试成为朋友&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5：成为恋人&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;6：结婚&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;异常路径：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1：第3步我认为不合适&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2：不会联系你，十分抱歉，希望你会有更好的缘分！&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 10 Sep 2018 04:49:07 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/wcVXPPbqpm</dc:identifier>
</item>
<item>
<title>如何优雅地从四个方面加深对深度学习的理解</title>
<link>http://www.jintiankansha.me/t/tG8EBnPeSk</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/tG8EBnPeSk</guid>
<description>&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; background-color:=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;在今年的 ICML 上，深度学习理论成为最大的主题之一。会议第一天，Sanjeev Arora 就展开了关于深度学习理论理解的教程，并从四个方面分析了关于该领域的研究：非凸优化、超参数和泛化、深度的意义以及生成模型。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; box-sizing:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;534&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp6Ybbr97W3HMw9iaR5RHE3hBsWgyzZlNz03qicp4EQeDuQIiacMT5DEbJA/0?wx_fmt=png&quot; data-ratio=&quot;0.64765625&quot; data-type=&quot;png&quot; data-w=&quot;1280&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp6Ybbr97W3HMw9iaR5RHE3hBsWgyzZlNz03qicp4EQeDuQIiacMT5DEbJA/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;2017 年 12 月 NIPS 的 Test-of-Time Award 颁奖典礼上，Ali Rahimi 这样呼吁人们加深对深度学习的理解：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;我希望生活在这样的一个世界，它的系统是建立在严谨可靠而且可证实的知识之上，而非炼金术。[……] 简单的实验和定理是帮助理解复杂大现象的基石。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Ali 的目标不是解散各个领域，而是「展开对话」。这个目标已经实现了，但对于目前的深度学习应被视为炼金术还是工程或科学，人们仍存在分歧。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;7 个月后，在斯德哥尔摩举行的国际机器学习会议 (ICML) 上，机器学习社区又聚焦了这个问题。此次大会与会者有 5000 多名，并累计发表论文 629 篇，这是基础机器学习研究的「年度大戏」。而深度学习理论已成为此次会议的最大主题之一。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;619&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp1dIWuDmhoB7L7gpic4EteZT5IqDf9zib9vhQPialJNYphHicLMTkRXkKlg/0?wx_fmt=png&quot; data-ratio=&quot;0.75&quot; data-type=&quot;png&quot; data-w=&quot;1200&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp1dIWuDmhoB7L7gpic4EteZT5IqDf9zib9vhQPialJNYphHicLMTkRXkKlg/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;会议第一天，最大的房间里就挤满了机器学习相关人员，他们准备聆听 Sanjeev Arora 关于深度学习理论理解的教程。这位普林斯顿大学计算机科学教授在演讲中总结了目前的深度学习理论研究领域，并将其分成四类：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;非凸优化：如何理解与深度神经网络相关的高度非凸损失函数？为什么随机梯度下降法会收敛？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;超参数和泛化：在经典统计理论中，为什么泛化依赖于参数的数量而非深度学习？存在其它较好的泛化方法吗？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;深度的意义：深度如何帮助神经网络收敛？深度和泛化之间的联系是什么？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;生成模型：为什么生成对抗网络（GAN）效果非常好？有什么理论特性能使模型稳定或者避免模式崩溃？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;在这一系列的文章中，我们将根据最新的论文（尤其是 ICML2018 的论文），帮助大家直观理解这四个方面。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;第一篇文章将重点讨论深度网络的非凸优化问题。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt; 非凸优化 &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;416&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpfuicL2hzO7alSeeyoSvrSoLF6RJTnuBicrFh1E1eibZae2dCDBXEJkE4w/0?wx_fmt=png&quot; data-ratio=&quot;0.5035405192761605&quot; data-type=&quot;png&quot; data-w=&quot;1271&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpfuicL2hzO7alSeeyoSvrSoLF6RJTnuBicrFh1E1eibZae2dCDBXEJkE4w/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;我敢打赌，你们很多人都曾尝试过训练自己的「深度网络」，结果却因为无法让它发挥作用而陷入自我怀疑。这不是你的错。我认为都是梯度下降的错。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Ali Rahimi 在 NIPS 演讲中曾说，随机梯度下降 (SGD) 的确是深度学习的基石，它应该解决高度非凸优化问题。理解它何时起作用，以及为什么起作用，是我们在深度学习的基本理论中一定会提出的最基本问题之一。具体来说，对于深度神经网络的非凸优化研究可以分为两个问题：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;损失函数是什么样的？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;SGD 为什么收敛？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt; 损失函数是什么样的？ &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;如果让你想象一个全局最小值，很可能你脑海中出现的第一幅图是这样的：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;274&quot; data-backw=&quot;436&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpjLVcBpDV1mVZQBBS933UEh2A6Za0Vn4HKPkiaMOtUroOjuP4psTvxHA/0?wx_fmt=png&quot; data-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpYosC1qjoB8iaobVth7kTklNOePv4I2T6cibO3kJ5zuldJ4uMKvzWAQyQ/0?wx_fmt=jpeg&quot; data-ratio=&quot;0.6284403669724771&quot; data-type=&quot;jpeg&quot; data-w=&quot;436&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpYosC1qjoB8iaobVth7kTklNOePv4I2T6cibO3kJ5zuldJ4uMKvzWAQyQ/640?wx_fmt=jpeg&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;二维世界中的全局最小值附近，函数是严格凸的（这意味着 hessian 矩阵的两个特征值都是正数）。但在一个有着数十亿参数的世界里，就像在深度学习中，全局最小值附近的方向都不平坦的可能性有多大？或者 hessian 中一个为零（或近似为零）的特征值都没有的概率有多大？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Sanjeev Arora 在教程中写的第一个评论是：损失函数的可能方向数量会随着维度的增长呈指数增长。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;218&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpGdVWAVt0fp4EicEZ7XLfL5dV5z385g3NqW1mP4OjjZGy3ZKKv7Qib9iag/0?wx_fmt=png&quot; data-ratio=&quot;0.26450344149459193&quot; data-type=&quot;png&quot; data-w=&quot;1017&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpGdVWAVt0fp4EicEZ7XLfL5dV5z385g3NqW1mP4OjjZGy3ZKKv7Qib9iag/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;直观上看，全局最小值似乎不是一个点而是一个连接管（connected manifold）。这意味着如果找到了全局最小值，你就能够穿过一条平坦的路径，在这条道路上，所有的点都是最小值。海德堡大学的一个研究团队在论文《Essentially No Barriers in Neural Network Energy Landscape》中证明了这一点。他们提出了一个更常规的说法，即任何两个全局最小值都可以通过一条平坦的路径连接。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;628&quot; data-backw=&quot;674&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpLL8vQpKoC4DluRJFwuQeLxTic5lm9YjpulFue2IUHy3Ly3GqaK3H0JA/0?wx_fmt=png&quot; data-ratio=&quot;0.9317507418397626&quot; data-type=&quot;png&quot; data-w=&quot;674&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpLL8vQpKoC4DluRJFwuQeLxTic5lm9YjpulFue2IUHy3Ly3GqaK3H0JA/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  在 MNIST 上的 CNN 或在 PTB 上的 RNN 已经是这样的情况，但是该项研究将这种认知扩展到了在更高级的数据集（CIFAR10 和 CIFAR100）上训练的更大网络（一些 DenseNet 和 ResNet）上。为了找到这条路径，他们使用了一种来自分子统计力学的启发式方法，叫做 AutoNEB。其思想是在两个极小值之间创建一个初始路径（例如线性），并在该路径上设置中心点。然后迭代地调整中心点的位置，以最小化每个中心点的损失，并确保中心点之间的距离保持不变（通过用弹簧建模中心点之间的空间）。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;虽然他们没有从理论上证明这个结果，但他们对为什么存在这样的路径给出了一些直观的解释：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;如果我们扰乱单个参数，比如添加一个小常数，然后让其它部分去自适应这种变化，仍然可以使损失最小化。因此可以认为，通过微调，无数其它参数可以「弥补」强加在一个参数上的改变。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;因此，本文的结果可以帮助我们通过超参数化和高维空间，以不同的方式看待极小值。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;通俗来说，当考虑神经网络的损失函数时，你应该牢记一个给定的点周围可能有非常多的方向。由此得出另一个结论，鞍点肯定比局部最小值多得多：在给定的关键点上，在数十亿个可能的方向中，很可能会找到一个向下的方向（如果不是在全局最小值上）。这种认知在 NIPS 2014 年发表的论文《Identifying and attacking the saddle point problem in high-dimensional non-convex optimization》中被严格规范化，并得到了实证证明。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;为什么 SGD 收敛（或不收敛）？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;深度神经网络优化的第二个重要问题与 SGD 的收敛性有关。虽然这种算法长期以来被看做是一种快速的近似版梯度下降，但我们现在可以证明 SGD 实际上收敛于更好、更一般的最小值。但我们能否将其规范化并定量地解释 SGD 脱离局部极小值或鞍点的能力？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;SGD 修改了损失函数&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;论文《An Alternative View: When Does SGD Escape Local Minima?》表明，实施 SGD 相当于在卷积（所以平滑）的损失函数上进行常规梯度下降。根据这一观点并在某些假设下，他们证明了 SGD 将设法脱离局部最小值，并收敛到全局最小值附近的一个小区域。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt; SGD 由随机微分方程控制 &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;连续 SGD 彻底改变了我对这个算法的看法。在 ICML 2018 关于非凸优化的研讨会上，Yoshua Bengio 在他关于随机梯度下降、平滑和泛化的演讲中提出了这个想法。SGD 不是在损失函数上移动一个点，而是一片点云或者说一个分布。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;532&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpVp9Fh7rOaLEfLv0gnnQpxHacwrTOV7Xd2ibaAJAmczu0RmxC6yvmu3g/0?wx_fmt=png&quot; data-ratio=&quot;0.644747393744988&quot; data-type=&quot;png&quot; data-w=&quot;1247&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpVp9Fh7rOaLEfLv0gnnQpxHacwrTOV7Xd2ibaAJAmczu0RmxC6yvmu3g/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;幻灯片摘自 Y. Bengio 在 ICML 2018 发表的演讲。他提出用分布（或点云）代替点来看待 SGD。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;这个点云的大小（即相关分布的方差）与 learning_rate / batch_size 因子成正比。Pratik Chaudhari 和 Stefano Soatto 在论文《Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks》中证明了这一点。这个公式非常直观：较低的 batch size 意味着梯度非常混乱（因为要在数据集一个非常小的子集上计算），高学习率意味着步骤混乱。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;将 SGD 视为随时间变化的分布可以得出：控制下降的方程现在是随机偏微分方程。更准确地说，在某些假设下，论文表明控制方程实际上是一个 Fokker-Planck 方程。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-backh=&quot;588&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpNKIctvKJ5QUYGIibribA3R1XCRBRK3JOlKWGa4KV3kZZt8FYb1I9OmBw/0?wx_fmt=png&quot; data-ratio=&quot;0.713&quot; data-type=&quot;png&quot; data-w=&quot;1000&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpNKIctvKJ5QUYGIibribA3R1XCRBRK3JOlKWGa4KV3kZZt8FYb1I9OmBw/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;幻灯片摘自 P. Chaudhari 和 S. Soatto 在 ICML 2018 发表的演讲——《High-dimensional Geometry and Dynamics of Stochastic Gradient Descent for Deep Networks》。他们展示了如何从离散系统过渡到 Fokker-Plank 方程所描述的连续系统。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;在统计物理学中，这种类型的方程描述了暴露在曳力 (使分布推移，即改变平均值) 和随机力 (使分布扩散，即增加方差) 下的粒子的演化。在 SGD 中，曳力由真实梯度建模，而随机力则对应算法的内在噪声。正如上面的幻灯片所示，扩散项与温度项 T = 1 /β= learning_rate /(2 * batch_size) 成正比，这再次显示了该比值的重要性！&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;__bg_gif&quot; data-ratio=&quot;0.65&quot; data-type=&quot;gif&quot; data-w=&quot;200&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/ymzg67DoLHL0GAVyghIBeu2spTC1GJNplshR7jFFrYVEDRyqFgzPxth1ic3t5SlI8wx1x26CF7B0sgV3icyoJFLA/640?wx_fmt=gif&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Fokker-Planck 方程下分布的演化。它向左漂移，随时间扩散。图源：维基百科&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;通过这个框架，Chaudhari 和 Soatto 证明了我们的分布将单调地收敛于某个稳定的分布（从 KL 散度的意义来说）：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;197&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpkfAupiaScZjtY8P0KyUkcibmFr9S43kpPy4TXN6g91HwYPia6yFoIzZrg/0?wx_fmt=png&quot; data-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp77ic5V3v7dQJzqOWUGRpOic4htm7HNiaT0DHAZibLwkibyggphmBF6R0VDA/0?wx_fmt=jpeg&quot; data-ratio=&quot;0.23870220162224798&quot; data-type=&quot;jpeg&quot; data-w=&quot;863&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp77ic5V3v7dQJzqOWUGRpOic4htm7HNiaT0DHAZibLwkibyggphmBF6R0VDA/640?wx_fmt=jpeg&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Pratik Chaudhari 和 Stefano Soatto 论文的一个主要定理，证明了分布的单调会收敛到稳定状态（在 KL 散度意义中）。第二个方程表明，使 F 最小化相当于最小化某个潜在的ϕ以及扩大熵的分布（温度 1 /β控制的权衡）。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;在上面的定理中有几个有趣的观点：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;SGD 最小化的函数可以写成两项之和（Eq. 11）：潜在Φ和熵的分布。温度 1 /β控制这两项的权衡。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;潜在Φ只取决于数据和网络的架构（而非优化过程）。如果它等于损失函数，SGD 将收敛到全局最小值。然而, 本文表明这种情况比较少见。而如果知道Φ与损失函数的距离，你将可以知道 SGD 收敛的概率。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;最终分布的熵取决于 learning_rate/batch_size（温度）的比例。直观上看，熵与分布的大小有关，而高温会导致分布具有更大的方差，这意味着一个平坦的极小值。平坦极小值的泛化能力更好，这与高学习率和低 batch size 能得到更优最小值的经验是一致的。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;因此，将 SGD 看作是一个随时间变化的分布表明，在收敛性和泛化方面，learning_rate/batch_size 比每个独立的超参数更有意义。此外，它还引入了与收敛相关的网络潜力，为架构搜索提供了一个很好的度量。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt; 结论 &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;探索深度学习理论的过程可以分为两部分：首先，通过简单的模型和实验，建立起关于深度学习理论如何及其为什么起作用的认知，然后将这些理念以数学形式呈现，以帮助我们解释当前的结论并得到新的结果。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;在第一篇文章中，我们试图传达更多关于神经网络高维损失函数和 SGD 解说的直观认知，同时表明新的形式主义正在建立，目的是建立一个关于深层神经网络优化的真正数学理论。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;然而，虽然非凸优化是深度学习的基石并且拥有大量的层数和参数，但它取得的成功大部分源于其优秀的泛化能力。这将是下一篇文章将分享的内容。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.34&quot; data-type=&quot;png&quot; data-w=&quot;300&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpU34THcyJPz4XtAkBib0gUCJG9eB5pnHm9fHK3KzANrXenNeBLNslY4w/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Sanjeev Arora：印度裔美国理论计算机科学家，他以研究概率可检验证明，尤其是PCP定理而闻名。研究兴趣包括计算复杂度理论、计算随机性、概率可检验证明等。他于2018年2月被推选为美国国家科学院院士，目前是普林斯顿大学计算机科学系教授。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;∑编辑 | Gemini&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;来源 | 数盟&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.0437601296596435&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;617&quot; width=&quot;auto&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky7x6u1VxMVMia4MLibNzC2nrumY3zDflTsCeoM04M1BrkvPny8tsw6hYkIicUr42iarLmadL2x6JwV6A/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域，经采用我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 10 Sep 2018 04:49:06 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/tG8EBnPeSk</dc:identifier>
</item>
</channel>
</rss>