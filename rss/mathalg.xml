<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fmathalg-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/mathalg-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>算法与数学之美</title>
<link>http://www.jintiankansha.me/column/c9dZ5TM2aS</link>
<description>算法与数学之美 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>浅析PageRank算法</title>
<link>http://www.jintiankansha.me/t/KZpXP4mFRe</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/KZpXP4mFRe</guid>
<description>&lt;section class=&quot;_135editor&quot;&gt;&lt;section class=&quot;layout&quot;&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;浅析PageRank算法&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;机器学习&lt;/span&gt;&lt;span&gt;很早就对Google的PageRank算法很感兴趣，但一直没有深究，只有个轮廓性的概念。前几天趁团队outing的机会，在动车上看了一些相关的资料（PS：在动车上看看书真是一种享受），趁热打铁，将所看的东西整理成此文。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;本文首先会讨论搜索引擎的核心难题，同时讨论早期搜索引擎关于结果页面重要性评价算法的困境，借此引出PageRank产生的背景。第二部分会详细讨论PageRank的思想来源、基础框架，并结合互联网页面拓扑结构讨论PageRank处理Dead Ends及平滑化的方法。第三部分讨论Topic-Sensitive PageRank算法。最后将讨论对PageRank的Spam攻击方法：Spam Farm以及搜索引擎对Spam Farm的防御。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;86516&quot;&gt;&lt;section&gt;&lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;86516&quot;&gt;&lt;section&gt;&lt;p class=&quot;135brush&quot; data-brushtype=&quot;text&quot;&gt;1 搜索引擎的难题&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;Google早已成为全球最成功的互联网搜索引擎，但这个当前的搜索引擎巨无霸却不是最早的互联网搜索引擎，在Google出现之前，曾出现过许多通用或专业领域搜索引擎。Google最终能击败所有竞争对手，很大程度上是因为它解决了困扰前辈们的最大难题：对搜索结果按重要性排序。而解决这个问题的算法就是PageRank。毫不夸张的说，是PageRank算法成就了Google今天的低位。要理解为什么解决这个难题如此重要，我们先来看一下搜索引擎的核心框架。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;86516&quot; readability=&quot;1&quot;&gt;&lt;section readability=&quot;2&quot;&gt;&lt;p class=&quot;135brush&quot; data-brushtype=&quot;text&quot;&gt;2 搜索引擎的核心框架&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;section/&gt;
&lt;p&gt;&lt;span&gt;虽然搜索引擎已经发展了很多年，但是其核心却没有太大变化。从本质上说，搜索引擎是一个资料检索系统，搜索引擎拥有一个资料库（具体到这里就是互联网页面），用户提交一个检索条件（例如关键词），搜索引擎返回符合查询条件的资料列表。理论上检索条件可以非常复杂，为了简单起见，我们不妨设检索条件是一至多个以空格分隔的词，而其表达的语义是同时含有这些词的资料（等价于布尔代数的逻辑与）。例如，提交“张洋 博客”，意思就是“给我既含有‘张洋’又含有‘博客’词语的页面”，以下是Google对这条关键词的搜索结果：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;image&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g8aBYPCNDBp60cJdUGsy8v02jVoJUpvVRTOfZJXwgY3VGen8MkyV2URg/0?wx_fmt=png&quot; border=&quot;0&quot; data-ratio=&quot;0.9185185185185185&quot; data-w=&quot;675&quot; data-type=&quot;png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;可以看到我的博客出现在第五条，而第四条是我之前在博客园的博客。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;当然，实际上现在的搜索引擎都是有分词机制的，例如如果以“张洋的博客”为关键词，搜索引擎会自动将其分解为“张洋 的 博客”三个词，而“的”作为停止词（Stop Word）会被过滤掉。关于分词及词权评价算法（如TF-IDF算法）是一个很大的话题，这里就不展开讨论了，为了简单此处可以将搜索引擎想象为一个只会机械匹配词语的检索系统。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这样看来，建立一个搜索引擎的核心问题就是两个：1、建立资料库；2、建立一种数据结构，可以根据关键词找到含有这个词的页面。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt;  第一个问题一般是通过一种叫爬虫（Spider）的特殊程序实现的（当然，专业领域搜索引擎例如某个学术会议的论文检索系统可能直接从数据库建立资料库），简单来说，爬虫就是从一个页面出发（例如新浪首页），通过HTTP协议通信获取这个页面的所有内容，把这个页面url和内容记录下来（记录到资料库），然后分析页面中的链接，再去分别获取这些链接链向页面的内容，记录到资料库后再分析这个页面的链接……重复这个过程，就可以将整个互联网的页面全部获取下来（当然这是理想情况，要求整个Web是一个强连通（Strongly Connected），并且所有页面的robots协议允许爬虫抓取页面，为了简单，我们仍然假设Web是一个强连通图，且不考虑robots协议）。抽象来看，可以将资料库看做一个巨大的key-value结构，key是页面url，value是页面内容。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第二个问题是通过一种叫倒排索引（inverted index）的数据结构实现的，抽象来说倒排索引也是一组key-value结构，key是关键词，value是一个页面编号集合（假设资料库中每个页面有唯一编号），表示这些页面含有这个关键词。本文不详细讨论倒排索引的建立方法。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;有了上面的分析，就可以简要说明搜索引擎的核心动作了：搜索引擎获取“张洋 博客”查询条件，将其分为“张洋”和“博客”两个词。然后分别从倒排索引中找到“张洋”所对应的集合，假设是{1， 3， 6， 8， 11， 15}；“博客”对应的集合是{1， 6， 10， 11， 12， 17， 20， 22}，将两个集合做交运算（intersection），结果是{1， 6， 11}。最后，从资料库中找出1、6、11对应的页面返回给用户就可以了。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;86516&quot; readability=&quot;1&quot;&gt;&lt;section readability=&quot;2&quot;&gt;&lt;p class=&quot;135brush&quot; data-brushtype=&quot;text&quot;&gt;3 搜索引擎的核心难题&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;section/&gt;
&lt;p&gt;&lt;span&gt;上面阐述了一个非常简单的搜索引擎工作框架，虽然现代搜索引擎的具体细节原理要复杂的多，但其本质却与这个简单的模型并无二异。实际Google在上述两点上相比其前辈并无高明之处。其最大的成功是解决了第三个、也是最为困难的问题：如何对查询结果排序。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我们知道Web页面数量非常巨大，所以一个检索的结果条目数量也非常多，例如上面“张洋 博客”的检索返回了超过260万条结果。用户不可能从如此众多的结果中一一查找对自己有用的信息，所以，一个好的搜索引擎必须想办法将“质量”较高的页面排在前面。其实直观上也可以感觉出，在使用搜索引擎时，我们并不太关心页面是否够全（上百万的结果，全不全有什么区别？而且实际上搜索引擎都是取top，并不会真的返回全部结果。），而很关心前一两页是否都是质量较高的页面，是否能满足我们的实际需求。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;因此，对搜索结果按重要性合理的排序就成为搜索引擎的最大核心，也是Google最终成功的突破点。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;86516&quot; readability=&quot;1&quot;&gt;&lt;section readability=&quot;2&quot;&gt;&lt;p class=&quot;135brush&quot; data-brushtype=&quot;text&quot;&gt;4 早期搜索引擎的做法&lt;/p&gt;
&lt;/section&gt;&lt;section&gt;
&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;不评价&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这个看起来可能有点搞笑，但实际上早期很多搜索引擎（甚至包括现在的很多专业领域搜索引擎）根本不评价结果重要性，而是直接按照某自然顺序（例如时间顺序或编号顺序）返回结果。这在结果集比较少的情况下还说得过去，但是一旦结果集变大，用户叫苦不迭，试想让你从几万条质量参差不齐的页面中寻找需要的内容，简直就是一场灾难，这也注定这种方法不可能用于现代的通用搜索引擎。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;基于检索词的评价&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;后来，一些搜索引擎引入了基于检索关键词去评价搜索结构重要性的方法，实际上，这类方法如TF-IDF算法在现代搜索引擎中仍在使用，但其已经不是评价质量的唯一指标。完整描述TF-IDF比较繁琐，本文这里用一种更简单的抽象模型描述这种方法。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;基于检索词评价的思想非常朴素：和检索词匹配度越高的页面重要性越高。“匹配度”就是要定义的具体度量。一个最直接的想法是关键词出现次数越多的页面匹配度越高。还是搜索“张洋 博客”的例子：假设A页面出现“张洋”5次，“博客”10次；B页面出现“张洋”2次，“博客”8次。于是A页面的匹配度为5 + 10 = 15，B页面为2 + 8 = 10，于是认为A页面的重要性高于B页面。很多朋友可能意识到这里的不合理性：内容较长的网页往往更可能比内容较短的网页关键词出现的次数多。因此，我们可以修改一下算法，用关键词出现次数除以页面总词数，也就是通过关键词占比作为匹配度，这样可以克服上面提到的不合理。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;早期一些搜索引擎确实是基于类似的算法评价网页重要性的。这种评价算法看似依据充分、实现直观简单，但却非常容易受到一种叫“Term Spam”的攻击。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;Term Spam&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt; &lt;span&gt;其实从搜索引擎出现的那天起，spammer和搜索引擎反作弊的斗法就没有停止过。Spammer是这样一群人——试图通过搜索引擎算法的漏洞来提高目标页面（通常是一些广告页面或垃圾页面）的重要性，使目标页面在搜索结果中排名靠前。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;现在假设Google单纯使用关键词占比评价页面重要性，而我想让我的博客在搜索结果中排名更靠前（最好排第一）。那么我可以这么做：在页面中加入一个隐藏的html元素（例如一个div），然后其内容是“张洋”重复一万次。这样，搜索引擎在计算“张洋 博客”的搜索结果时，我的博客关键词占比就会非常大，从而做到排名靠前的效果。更进一步，我甚至可以干扰别的关键词搜索结果，例如我知道现在欧洲杯很火热，我就在我博客的隐藏div里加一万个“欧洲杯”，当有用户搜索欧洲杯时，我的博客就能出现在搜索结果较靠前的位置。这种行为就叫做“Term Spam”。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;早期搜索引擎深受这种作弊方法的困扰，加之基于关键词的评价算法本身也不甚合理，因此经常是搜出一堆质量低下的结果，用户体验大大打了折扣。而Google正是在这种背景下，提出了PageRank算法，并申请了专利保护。此举充分保护了当时相对弱小Google，也使得Google一举成为全球首屈一指的搜索引擎。&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;86516&quot;&gt;&lt;section&gt;&lt;p class=&quot;135brush&quot; data-brushtype=&quot;text&quot;&gt;5 PageRank算法&lt;/p&gt;
&lt;/section&gt;&lt;section&gt;
&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;上文已经说到，PageRank的作用是评价网页的重要性，以此作为搜索结果的排序重要依据之一。实际中，为了抵御spam，各个搜索引擎的具体排名算法是保密的，PageRank的具体计算方法也不尽相同，本节介绍一种最简单的基于页面链接属性的PageRank算法。这个算法虽然简单，却能揭示PageRank的本质，实际上目前各大搜索引擎在计算PageRank时链接属性确实是重要度量指标之一。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;简单PageRank计算&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;首先，我们将Web做如下抽象：1、将每个网页抽象成一个节点；2、如果一个页面A有链接直接链向B，则存在一条有向边从A到B（多个相同链接不重复计算边）。因此，整个Web被抽象为一张有向图。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;现在假设世界上只有四张网页：A、B、C、D，其抽象结构如下图：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;image&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g8cbJXgI4IZS1icq0zkticVA44UVyZciaSrureKFGQhQMExu8WxvbuyXyyQ/0?wx_fmt=png&quot; border=&quot;0&quot; data-ratio=&quot;0.9438502673796791&quot; data-w=&quot;374&quot; data-type=&quot;png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;显然这个图是强连通的（从任一节点出发都可以到达另外任何一个节点）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;然后需要用一种合适的数据结构表示页面间的连接关系。其实，PageRank算法是基于这样一种背景思想：被用户访问越多的网页更可能质量越高，而用户在浏览网页时主要通过超链接进行页面跳转，因此我们需要通过分析超链接组成的拓扑结构来推算每个网页被访问频率的高低。最简单的，我们可以假设当一个用户停留在某页面时，跳转到页面上每个被链页面的概率是相同的。例如，上图中A页面链向B、C、D，所以一个用户从A跳转到B、C、D的概率各为1/3。设一共有N个网页，则可以组织这样一个N维矩阵：其中i行j列的值表示用户从页面j转到页面i的概率。这样一个矩阵叫做转移矩阵（Transition Matrix）。下面的转移矩阵M对应上图：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;\large M=\begin{bmatrix} \0 &amp;amp; 1/2 &amp;amp; 0 &amp;amp; 1/2\\ 1/3 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1/2\\ 1/3 &amp;amp; 1/2 &amp;amp; 0 &amp;amp; 0\\ 1/3 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \end{bmatrix}&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g8g2dAziaAkkicfvkrLk9QA1sFHsZvagFiaITGJ3xFIXIBzVOUAAwN6ZwvQ/0?wx_fmt=gif&quot; data-ratio=&quot;0.4669603524229075&quot; data-w=&quot;227&quot; data-type=&quot;gif&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;然后，设初始时每个页面的rank值为1/N，这里就是1/4。按A-D顺序将页面rank为向量v：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;\large v=\begin{bmatrix} 1/4\\ 1/4\\ 1/4\\ 1/4 \end{bmatrix}&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g8uRlku96pQKyxCJCxSZO8pkHbOLaNc8Wh9EiaZauygtpLW479GDs3tcg/0?wx_fmt=gif&quot; data-ratio=&quot;1.1157894736842104&quot; data-w=&quot;95&quot; data-type=&quot;gif&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;注意，M第一行分别是A、B、C和D转移到页面A的概率，而v的第一列分别是A、B、C和D当前的rank，因此用M的第一行乘以v的第一列，所得结果就是页面A最新rank的合理估计，同理，Mv的结果就分别代表A、B、C、D新rank：&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt; &lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;\large Mv=\begin{bmatrix} 1/4\\ 5/24\\ 5/24\\ 1/3 \end{bmatrix}&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g8I4Wicyp6UVia38RUiaRZpDE7guJQVhKFV0v6yjvIwKzQLlolBzqRI0kIA/0?wx_fmt=gif&quot; data-ratio=&quot;0.828125&quot; data-w=&quot;128&quot; data-type=&quot;gif&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;然后用M再乘以这个新的rank向量，又会产生一个更新的rank向量。迭代这个过程，可以证明v最终会收敛，即v约等于Mv，此时计算停止。最终的v就是各个页面的pagerank值。例如上面的向量经过几步迭代后，大约收敛在（1/4, 1/4, 1/5, 1/4），这就是A、B、C、D最后的pagerank。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;处理Dead Ends&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;上面的PageRank计算方法假设Web是强连通的，但实际上，Web并不是强连通（甚至不是联通的）。下面看看PageRank算法如何处理一种叫做Dead Ends的情况。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;所谓Dead Ends，就是这样一类节点：它们不存在外链。看下面的图：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;image&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g8P3076Op8OYPUqsOPU20mFx7Kw67Knhu0EOb9UPG4CqW0bvneQlQ27Q/0?wx_fmt=png&quot; border=&quot;0&quot; data-ratio=&quot;1.0292397660818713&quot; data-w=&quot;342&quot; data-type=&quot;png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;注意这里D页面不存在外链，是一个Dead End。上面的算法之所以能成功收敛到非零值，很大程度依赖转移矩阵这样一个性质：每列的加和为1。而在这个图中，M第四列将全为0。在没有Dead Ends的情况下，每次迭代后向量v各项的和始终保持为1，而有了Dead Ends，迭代结果将最终归零（要解释为什么会这样，需要一些矩阵论的知识，比较枯燥，此处略）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;处理Dead Ends的方法如下：迭代拿掉图中的Dead Ends节点及Dead Ends节点相关的边（之所以迭代拿掉是因为当目前的Dead Ends被拿掉后，可能会出现一批新的Dead Ends），直到图中没有Dead Ends。对剩下部分计算rank，然后以拿掉Dead Ends逆向顺序反推Dead Ends的rank。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;以上图为例，首先拿到D和D相关的边，D被拿到后，C就变成了一个新的Dead Ends，于是拿掉C，最终只剩A、B。此时可很容易算出A、B的PageRank均为1/2。然后我们需要反推Dead Ends的rank，最后被拿掉的是C，可以看到C前置节点有A和B，而A和B的出度分别为3和2，因此C的rank为：1/2 * 1/3 + 1/2 * 1/2 = 5/12；最后，D的rank为：1/2 * 1/3 + 5/12 * 1 = 7/12。所以最终的PageRank为（1/2, 1/2, 5/12, 7/12）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;Spider Traps及平滑处理&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;可以预见，如果把真实的Web组织成转移矩阵，那么这将是一个极为稀疏的矩阵，从矩阵论知识可以推断，极度稀疏的转移矩阵迭代相乘可能会使得向量v变得非常不平滑，即一些节点拥有很大的rank，而大多数节点rank值接近0。而一种叫做Spider Traps节点的存在加剧了这种不平滑。例如下图：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;image&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g8z8P22svLQdAN3hQZbOU7QBovroXN2iboWibg3CGnzKTkVHwetVej7zGg/0?wx_fmt=png&quot; border=&quot;0&quot; data-ratio=&quot;0.9646739130434783&quot; data-w=&quot;368&quot; data-type=&quot;png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;D有外链所以不是Dead Ends，但是它只链向自己（注意链向自己也算外链，当然同时也是个内链）。这种节点叫做Spider Trap，如果对这个图进行计算，会发现D的rank越来越大趋近于1，而其它节点rank值几乎归零。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;为了克服这种由于矩阵稀疏性和Spider Traps带来的问题，需要对PageRank计算方法进行一个平滑处理，具体做法是加入“心灵转移（teleporting）”。所谓心灵转移，就是我们认为在任何一个页面浏览的用户都有可能以一个极小的概率瞬间转移到另外一个随机页面。当然，这两个页面可能不存在超链接，因此不可能真的直接转移过去，心灵转移只是为了算法需要而强加的一种纯数学意义的概率数字。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;加入心灵转移后，向量迭代公式变为：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;\large {v}'=(1-\beta)Mv+e\frac{\beta}{N}&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g88CoZFLg892tt4eAXE8nGZXnfBW0JKWTsZ623PndSfEMuZ5rD7HkRAQ/0?wx_fmt=gif&quot; data-ratio=&quot;0.22439024390243903&quot; data-w=&quot;205&quot; data-type=&quot;gif&quot;/&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt; &lt;p&gt;&lt;span&gt;其中β往往被设置为一个比较小的参数（0.2或更小），e为N维单位向量，加入e的原因是这个公式的前半部分是向量，因此必须将β/N转为向量才能相加。这样，整个计算就变得平滑，因为每次迭代的结果除了依赖转移矩阵外，还依赖一个小概率的心灵转移。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;以上图为例，转移矩阵M为：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;\large M=\begin{bmatrix} 0 &amp;amp; 1/2 &amp;amp; 0 &amp;amp; 0\\ 1/3 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ 1/3 &amp;amp; 1/2 &amp;amp; 0 &amp;amp; 0\\ 1/3 &amp;amp; 0 &amp;amp; 1 &amp;amp; 1 \end{bmatrix}&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g8bqaibrnKkwXjKUHibZZotIpzmibk4PibcBrYkGt0EDfYwEET1vLPBTvndg/0?wx_fmt=gif&quot; data-ratio=&quot;0.5170731707317073&quot; data-w=&quot;205&quot; data-type=&quot;gif&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;设β为0.2，则加权后的M为：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;\large M=\begin{bmatrix} 0 &amp;amp; 2/5 &amp;amp; 0 &amp;amp; 0\\ 4/15 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ 4/15 &amp;amp; 2/5 &amp;amp; 0 &amp;amp; 0\\ 4/15 &amp;amp; 0 &amp;amp; 4/5 &amp;amp; 4/5 \end{bmatrix}&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g8DgeqqdzkSk9NiaGNibt0ToibvoLJ7nZbA9OPX6zzbH0dUzPibrqQxP5mdw/0?wx_fmt=gif&quot; data-ratio=&quot;0.4092664092664093&quot; data-w=&quot;259&quot; data-type=&quot;gif&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;因此：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;\large {v}'=\begin{bmatrix} 0 &amp;amp; 2/5 &amp;amp; 0 &amp;amp; 0\\ 4/15 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ 4/15 &amp;amp; 2/5 &amp;amp; 0 &amp;amp; 0\\ 4/15 &amp;amp; 0 &amp;amp; 4/5 &amp;amp; 4/5 \end{bmatrix}v+\begin{bmatrix} 1/20\\ 1/20\\ 1/20\\ 1/20 \end{bmatrix}v&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g8HZxOWriaKyHvq1VeMrQOAjbWKBic8e0iaYB7d6QWcnoZRQNKUNYxWqXdw/0?wx_fmt=gif&quot; data-ratio=&quot;0.2896174863387978&quot; data-w=&quot;366&quot; data-type=&quot;gif&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;如果按这个公式迭代算下去，会发现Spider Traps的效应被抑制了，从而每个页面都拥有一个合理的pagerank。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;86516&quot; readability=&quot;1&quot;&gt;&lt;section readability=&quot;2&quot;&gt;&lt;p class=&quot;135brush&quot; data-brushtype=&quot;text&quot;&gt;6 Topic-Sensitive PageRank&lt;/p&gt;
&lt;/section&gt;&lt;section&gt;
&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;其实上面的讨论我们回避了一个事实，那就是“网页重要性”其实没一个标准答案，对于不同的用户，甚至有很大的差别。例如，当搜索“苹果”时，一个数码爱好者可能是想要看iphone的信息，一个果农可能是想看苹果的价格走势和种植技巧，而一个小朋友可能在找苹果的简笔画。理想情况下，应该为每个用户维护一套专用向量，但面对海量用户这种方法显然不可行。所以搜索引擎一般会选择一种称为Topic-Sensitive的折中方案。Topic-Sensitive PageRank的做法是预定义几个话题类别，例如体育、娱乐、科技等等，为每个话题单独维护一个向量，然后想办法关联用户的话题倾向，根据用户的话题倾向排序结果。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Topic-Sensitive PageRank分为以下几步：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1、确定话题分类。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;一般来说，可以参考Open Directory（DMOZ）的一级话题类别作为topic。目前DMOZ的一级topic有：Arts（艺术）、Business（商务）、Computers（计算机）、Games（游戏）、Health（医疗健康）、Home（居家）、Kids and Teens（儿童）、News（新闻）、Recreation（娱乐修养）、Reference（参考）、Regional（地域）、Science（科技）、Shopping（购物）、Society（人文社会）、Sports（体育）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2、网页topic归属。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这一步需要将每个页面归入最合适的分类，具体归类有很多算法，例如可以使用TF-IDF基于词素归类，也可以聚类后人工归类，具体不再展开。这一步最终的结果是每个网页被归到其中一个topic。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt; &lt;span&gt;3、分topic向量计算。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;在Topic-Sensitive PageRank中，向量迭代公式为&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;\large {v}'=(1-\beta)Mv+s\frac{\beta}{|s|}&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g89iaSZVpUAUElPo2mV9YqaHa4mlA9WRbR8hy8icaKEsbw5icQLv7QHeCBQ/0?wx_fmt=gif&quot; data-ratio=&quot;0.24519230769230768&quot; data-w=&quot;208&quot; data-type=&quot;gif&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;首先是单位向量e变为了s。s是这样一个向量：对于某topic的s，如果网页k在此topic中，则s中第k个元素为1，否则为0。注意对于每一个topic都有一个不同的s。而|s|表示s中1的数量。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;还是以上面的四张页面为例，假设页面A归为Arts，B归为Computers，C归为Computers，D归为Sports。那么对于Computers这个topic，s就是：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;\large s=\begin{bmatrix} 0\\ 1\\ 1\\ 0 \end{bmatrix}&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g8cN0qR55LFQX01tJSsCIVqdGlpTICjxdqDpww9GENLMVAwK794R2WMA/0?wx_fmt=gif&quot; data-ratio=&quot;1.4722222222222223&quot; data-w=&quot;72&quot; data-type=&quot;gif&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;而|s|=2。因此，迭代公式为：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;\large {v}'=\begin{bmatrix} 0 &amp;amp; 2/5 &amp;amp; 0 &amp;amp; 0\\ 4/15 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0\\ 4/15 &amp;amp; 2/5 &amp;amp; 0 &amp;amp; 0\\ 4/15 &amp;amp; 0 &amp;amp; 4/5 &amp;amp; 4/5 \end{bmatrix}v+\begin{bmatrix} 0\\ 1/10\\ 1/10\\ 0 \end{bmatrix}v&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g8f84LyUylMtibm9hXOSAQj4k9qyeyrkslEsSaR1zQZiaoodWy0iaib7br1w/0?wx_fmt=gif&quot; data-ratio=&quot;0.2896174863387978&quot; data-w=&quot;366&quot; data-type=&quot;gif&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;最后算出的向量就是Computers这个topic的rank。如果实际计算一下，会发现B、C页在这个topic下的权重相比上面非Topic-Sensitive的rank会升高，这说明如果用户是一个倾向于Computers topic的人（例如程序员），那么在给他呈现的结果中B、C会更重要，因此可能排名更靠前。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4、确定用户topic倾向。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;最后一步就是在用户提交搜索时，确定用户的topic倾向，以选择合适的rank向量。主要方法有两种，一种是列出所有topic让用户自己选择感兴趣的项目，这种方法在一些社交问答网站注册时经常使用；另外一种方法就是通过某种手段（如cookie跟踪）跟踪用户的行为，进行数据分析判断用户的倾向，这本身也是一个很有意思的话题，按时这个话题超出本文的范畴，不再展开细说。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;86516&quot; readability=&quot;1&quot;&gt;&lt;section readability=&quot;2&quot;&gt;&lt;p class=&quot;135brush&quot; data-brushtype=&quot;text&quot;&gt;7 针对PageRank的Spam攻击与反作弊&lt;/p&gt;
&lt;/section&gt;&lt;section&gt;
&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;上文说过，Spammer和搜索引擎反作弊工程师的斗法从来就没停止过。实际上，只要是算法，就一定有spam方法，不存在无懈可击的排名算法。下面看一下针对PageRank的spam。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;Link Spam&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;回到文章开头的例子，如果我想让我的博客在搜索“张洋 博客”时排名靠前，显然在PageRank算法下靠Term Spam是无法实现的。不过既然我明白了PageRank主要靠内链数计算页面权重，那么我是不是可以考虑建立很多空架子网站，让这些网站都链接到我博客首页，这样是不是可以提高我博客首页的PageRank？很不幸，这种方法行不通。再看下PageRank算法，一个页面会将权重均匀散播给被链接网站，所以除了内链数外，上游页面的权重也很重要。而我那些空架子网站本身就没啥权重，所以来自它们的内链并不能起到提高我博客首页PageRank的作用，这样只是自娱自乐而已。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;所以，Spam PageRank的关键就在于想办法增加一些高权重页面的内链。下面具体看一下Link Spam怎么做。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt; &lt;p&gt;&lt;span&gt;首先明确将页面分为几个类型：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1、目标页&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;目标页是spammer要提高rank的页面，这里就是我的博客首页。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2、支持页&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;支持页是spammer能完全控制的页面，例如spammer自己建立的站点中页面，这里就是我上文所谓的空架子页面。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3、可达页&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;可达页是spammer无法完全控制，但是可以有接口供spammer发布链接的页面，例如天涯社区、新浪博客等等这种用户可发帖的社区或博客站。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4、不可达页&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这是那些spammer完全无法发布链接的网站，例如政府网站、百度首页等等。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;作为一个spammer，我能利用的资源就是支持页和可达页。上面说过，单纯通过支持页是没有办法spam的，因此我要做的第一件事情就是尽量找一些rank较高的可达页去加上对我博客首页的链接。例如我可以去天涯、猫扑等地方回个这样的贴：“楼主的帖子很不错！精彩内容：http://codinglabs.org”。我想大家一定在各大社区没少见这种帖子，这就是有人在做spam。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;然后，再通过大量的支持页放大rank，具体做法是让每个支持页和目标页互链，且每个支持页只有一条链接。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这样一个结构叫做Spam Farm，其拓扑图如下：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;image&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g88wWE4vnSmMft7eXCpLwzzw4IhEcXNhIFHHDUsTjzu2yzLYheykZ1BA/0?wx_fmt=png&quot; border=&quot;0&quot; data-ratio=&quot;0.9365558912386707&quot; data-w=&quot;331&quot; data-type=&quot;png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;其中T是目标页，A是可达页，S是支持页。下面计算一下link spam的效果。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;设T的总rank为y，则y由三部分组成：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1、可达页的rank贡献，设为x。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2、心灵转移的贡献，为β/n。其中n为全部网页的数量，β为转移参数。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3、支持页的贡献：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;设有m个支持页，因为每个支持页只和T有链接，所以可以算出每个支持页的rank为：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt; &lt;img class=&quot;aligncenter&quot; title=&quot;\large \frac{(1-\beta)y}{m}+\frac{\beta}{n}&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g8Y8gEEBygKvqLvoicWmzZO6RErsIKribFPdE4rBqrfpEv78MmyT1PPdUw/0?wx_fmt=gif&quot; data-ratio=&quot;0.37398373983739835&quot; data-w=&quot;123&quot; data-type=&quot;gif&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;则支持页贡献的全部rank为：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;\large m(1-\beta)(\frac{(1-\beta)y}{m}+\frac{\beta}{n})&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g8hFb0OkJyeSuhQhPrPuymutjVYxfdzRsMqVKYRbR4QXWka2GWFZdPBg/0?wx_fmt=gif&quot; data-ratio=&quot;0.20175438596491227&quot; data-w=&quot;228&quot; data-type=&quot;gif&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;因此可以得到：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;\large y=m(1-\beta)(\frac{(1-\beta)y}{m}+\frac{\beta}{n})+x+\frac{\beta}{n}&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g80gnLXmHPKjM3ZuQ8A2eUoEia3t4BMGQMzXah6WAFYRKiaC4eDoRvHRtQ/0?wx_fmt=gif&quot; data-ratio=&quot;0.13142857142857142&quot; data-w=&quot;350&quot; data-type=&quot;gif&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;由于相对β，n非常巨大，所以可以认为β/n近似于0。 简化后的方程为：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;\large y=m(1-\beta)(\frac{(1-\beta)y}{m})+x&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g8EzCW5V1Q5D9EmXBnVtYEibsSffic39pCAnibxYfHhbkFdUKIsT7C7O6Mw/0?wx_fmt=gif&quot; data-ratio=&quot;0.17424242424242425&quot; data-w=&quot;264&quot; data-type=&quot;gif&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;解方程得：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter&quot; title=&quot;\large y=x\frac{1}{2\beta-\beta^2}&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkytibcGFTC8eTVzLLcUe65g8ZLpCPgduIGUUibjJuIFAUwswptWsug3zLtnC6dIwCmIj0mPiazyZQvqg/0?wx_fmt=gif&quot; data-ratio=&quot;0.3937007874015748&quot; data-w=&quot;127&quot; data-type=&quot;gif&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;假设β为0.2，则1/(2β-β^2) = 2.77则这个spam farm可以将x约放大2.7倍。因此如果起到不错的spam效果。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;Link Spam反作弊&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;针对spammer的link spam行为，搜索引擎的反作弊工程师需要想办法检测这种行为，一般来说有两类方法检测link spam。&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;网&lt;/span&gt;&lt;span&gt;络拓扑分析&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;一种方法是通过对网页的图拓扑结构分析找出可能存在的spam farm。但是随着Web规模越来越大，这种方法非常困难，因为图的特定结构查找是时间复杂度非常高的一个算法，不可能完全靠这种方法反作弊。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;TrustRank&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;更可能的一种反作弊方法是叫做一种TrustRank的方法。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;说起来TrustRank其实数学本质上就是Topic-Sensitive Rank，只不过这里定义了一个“可信网页”的虚拟topic。所谓可信网页就是上文说到的不可达页，或者说没法spam的页面。例如政府网站（被黑了的不算）、新浪、网易门户首页等等。一般是通过人力或者其它什么方式选择出一个“可信网页”集合，组成一个topic，然后通过上文的Topic-Sensitive算法对这个topic进行rank计算，结果叫做TrustRank。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt; &lt;p&gt;&lt;span&gt;TrustRank的思想很直观：如果一个页面的普通rank远高于可信网页的topic rank，则很可能这个页面被spam了。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;设一个页面普通rank为P，TrustRank为T，则定义网页的Spam Mass为：(P – T)/P。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Spam Mass越大，说明此页面为spam目标页的可能性越大。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;86516&quot;&gt;&lt;section&gt;&lt;p class=&quot;135brush&quot; data-brushtype=&quot;text&quot;&gt;8 总结&lt;/p&gt;
&lt;/section&gt;&lt;section&gt;
&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;这篇文章是我对一些资料的归纳汇总，简单介绍了PageRank的背景、作用、计算方法、变种、Spam及反作弊等内容。为了突出重点我简化了搜索引擎的模型，当然在实际中搜索引擎远没有这么简单，真实算法也一定非常复杂。不过目前几乎所有现代搜索引擎页面权重的计算方法都基于PageRank及其变种。因为我没做过搜索引擎相关的开发，因此本文内容主要是基于现有文献的客观总结，稍加一点我的理解。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;文中的图使用PGF/TikZ for Tex绘制：http://www.texample.net/tikz/。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;文中公式由codecogs在线LaTeX公式编辑器生成：http://www.codecogs.com/latex/eqneditor.php。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;参考文献&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;[1] Anand Rajaraman, Jeffrey D. Ullman, Mining of Massive Datasets. 2010-2011&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;[2] S. Brin and L. Page, “Anatomy of a large-scale hypertextual web search engine,” Proc. 7th Intl. World-Wide-Web Conference, pp. 107–117, 1998.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;[3] A. Broder, R. Kumar, F. Maghoul, P. Raghavan, S. Rajagopalan, R. Stata, A. Tomkins, and J. Weiner, “Graph structure in the web,” Computer Networks 33:1–6, pp. 309–320, 2000.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;[4] T.H. Haveliwala, “Topic-sensitive PageRank,” Proc. 11th Intl. World-Wide-Web Conference, pp. 517–526, 2002&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;[5] Z. Gy¨ongi, H. Garcia-Molina, and J. Pedersen, “Combating link spam with trustrank,” Proc. 30th Intl. Conf. on Very Large Databases, pp. 576–587, 2004.&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span&gt;作者：张洋&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;来源：&lt;/span&gt;&lt;span&gt;伯乐在线&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt; &lt;strong&gt;&lt;span&gt;---&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&lt;span&gt;------&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre accuse=&quot;aContent&quot;&gt;
&lt;br/&gt;&lt;strong&gt;&lt;span&gt;等的就是你，真的超有趣！高能金融抱团群发车啦~&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;&lt;strong&gt;加我拉你进群呦&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;
&lt;/pre&gt;
&lt;pre accuse=&quot;aContent&quot;&gt;
&lt;br/&gt;&lt;strong&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkyJCDnjGtGFUOmOAE2SZY1zvIueyhic76aNgYN38xwV6XE9y6tSzCTFYianCf1YgCrfeNqconWV5RhQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;512&quot; width=&quot;auto&quot;/&gt;&lt;/strong&gt;
&lt;/pre&gt;
&lt;pre readability=&quot;6&quot;&gt;
&lt;br/&gt;&lt;strong&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;/span&gt;&lt;/strong&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域。&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;strong&gt;&lt;span&gt;稿件一经采用，我们将奉上稿酬。&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;strong&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;br/&gt;商务合作：联系微信号hengzi5809&lt;/span&gt;&lt;/strong&gt;
&lt;/p&gt;&lt;/pre&gt;</description>
<pubDate>Sun, 19 Nov 2017 07:16:48 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/KZpXP4mFRe</dc:identifier>
</item>
<item>
<title>从概念到案例：初学者须知的十大机器学习算法</title>
<link>http://www.jintiankansha.me/t/dgh0Jb8Epx</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/dgh0Jb8Epx</guid>
<description>&lt;p&gt;&lt;span&gt;本文先为初学者介绍了必知的十大机器学习（ML）算法，并且我们通过一些图解和实例生动地解释这些基本机器学习的概念。我们希望本文能为理解机器学习基本算法提供简单易读的入门概念。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;89226&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot;&gt;&lt;span&gt;机器学习模型&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;section/&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;

&lt;p&gt;&lt;span&gt;在《哈佛商业评论》发表「数据科学家是 21 世纪最性感的职业」之后，机器学习的研究广受关注。所以，对于初入机器学习领域的学习者，我们放出来一篇颇受欢迎的博文——《初学者必知的十大机器学习算法》，尽管这只是针对初学者的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;机器学习算法就是在没有人类干预的情况下，从数据中学习，并在经验中改善的一种方法，学习任务可能包括学习从输入映射到输出的函数，学习无标签数据的隐含结构；或者是「基于实例的学习」，通过与存储在记忆中的训练数据做比较，给一个新实例生成一个类别标签。基于实例的学习（instance-based learning）不会从具体实例中生成抽象结果。&lt;/span&gt;&lt;/p&gt;


&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;89226&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot;&gt;&lt;span&gt;机器学习算法的类型&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;section/&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;

&lt;p&gt;&lt;span&gt;有三类机器学习算法：&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;39&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot; placeholder=&quot;&amp;#x8BF7;&amp;#x8F93;&amp;#x5165;&amp;#x6807;&amp;#x9898;&quot;&gt;&lt;span&gt;1. 监督学习:&lt;/span&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;可以这样来描述监督学习：使用有标签的训练数据去学习从输入变量（X）到输出变量（Y）的映射函数。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Y = f (X)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;它分为两种类型：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;a. 分类：通过一个给定的输入预测一个输出，这里的输出变量以类别的形式展示。例如男女性别、疾病和健康。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt; 
&lt;p&gt;&lt;span&gt;b. 回归：也是通过一个给定的输入预测一个输出，这里的输出变量以实数的形式展示。例如预测降雨量、人的身高等实数值。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;本文介绍的前 5 个算法就属于监督学习：线性回归、Logistic 回归、CART、朴素贝叶斯和 KNN。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;集成学习也是一种监督学习方法。它意味着结合多种不同的弱学习模型来预测一个新样本。本文介绍的第 9、10 两种算法–随机森林 Bagging 和 AdaBoost 提升算法就是集成学习技术。&lt;/span&gt;&lt;/p&gt;


&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;39&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot; placeholder=&quot;&amp;#x8BF7;&amp;#x8F93;&amp;#x5165;&amp;#x6807;&amp;#x9898;&quot;&gt;&lt;span&gt;2. 非监督学习:&lt;/span&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;非监督学习问提仅仅处理输入变量（X），但不会处理对应的输出（也就是说，没有标签）。它使用无标签的训练数据建模数据的潜在结构。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;非监督学习可以分为 2 种类型：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;a. 关联：就是去发觉在同一个数据集合中不同条目同时发生的概率。广泛地用于市场篮子分析。例如：如果一位顾客买了面包，那么他有 80% 的可能性购买鸡蛋。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;b. 聚类：把更加相似的对象归为一类，而不是其他类别对象。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;c. 降维：顾名思义，降维就是减少数据集变量，同时要保证重要信息不丢失。降维可以通过使用特征提取和特征选择方法来完成。特征选择方法会选择原始变量的一个子集。特征提取完成了从高维空间到低维空间的数据变换。例如，主成分分析（PCA）就是一个特征提取方法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;本文介绍的算法 6-8 都是非监督学习的例子：包括 Apriori 算法、K-均值聚类、主成分分析（PCA）。&lt;/span&gt;&lt;/p&gt;


&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;39&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot; placeholder=&quot;&amp;#x8BF7;&amp;#x8F93;&amp;#x5165;&amp;#x6807;&amp;#x9898;&quot;&gt;&lt;span&gt;3. 强化学习:&lt;/span&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt;  强化学习是这样一种学习方法，它允许智能体通过学习最大化奖励的行为，并基于当前状态决定下一步要采取的最佳行动。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;强化学习一般通过试错学习到最佳行动。强化学习应用于机器人，机器人在碰到障碍物质之后会收到消极反馈，它通过这些消极反馈来学会避免碰撞；也用在视频游戏中，通过试错发现能够极大增长玩家回报的一系列动作。智能体可以使用这些回报来理解游戏中的最佳状态，并选择下一步的行动。&lt;/span&gt;&lt;/p&gt;


&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;89226&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot;&gt;&lt;span&gt;监督学习&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;section/&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;39&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot; placeholder=&quot;&amp;#x8BF7;&amp;#x8F93;&amp;#x5165;&amp;#x6807;&amp;#x9898;&quot;&gt;&lt;span&gt;1. 线性回归&lt;/span&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;在机器学习中，我们用输入变量 x 来决定输出变量 y。输入变量和输出变量之间存在一个关系。机器学习的目标就是去定量地描述这种关系。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;


&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;39&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot; placeholder=&quot;&amp;#x8BF7;&amp;#x8F93;&amp;#x5165;&amp;#x6807;&amp;#x9898;&quot;&gt;&lt;span&gt;2.Logistic 回归&lt;/span&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;线性回归预测是连续值（如厘米级的降雨量），logistic 回归预测是使用了一种变换函数之后得到的离散值（如一位学生是否通过了考试）。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Logistic 回归最适合于二元分类问题（在一个数据集中，y=0 或者 1，1 代表默认类。例如：在预测某个事件是否会发生的时候，发生就是 1。在预测某个人是否患病时，患病就是 1）。这个算法是拿它所使用的变换函数命名的，这个函数称为 logistics 函数（logistics function，h(x)= 1/ (1 + e^x)），它的图像是一个 S 形曲线。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在 logistic 回归中，输出是默认类别的概率（不像线性回归一样，输出是直接生成的）。因为是概率，所以输出的值域是 [0,1]。输出值 y 是通过输入值 x 的对数变换 h(x)= 1/ (1 + e^ -x) 得到的。然后使用一个阈值强制地让输出结果变成一个二元分类问题。&lt;/span&gt;&lt;/p&gt;


&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;39&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot; placeholder=&quot;&amp;#x8BF7;&amp;#x8F93;&amp;#x5165;&amp;#x6807;&amp;#x9898;&quot;&gt;&lt;span&gt;3. 分类和回归树&lt;/span&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;分类和回归树（CART）是决策树的一种补充。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt; 
&lt;p&gt;&lt;span&gt;非终端节点（non-terminal node）包含根节点 (root node) 和中间节点 (internal node)。每一个非终端节点代表一个单独的输入变量 x 和这个变量的分支节点；叶节点代表的是输出变量 y。这个模型按照以下的规则来作出预测：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;526&quot; data-backw=&quot;556&quot; data-ratio=&quot;0.946875&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/jAqr3XzCYibxJicly0CljNCg7m6yudL2fpY7PdyuOarUkwdicicWb5WicDw1hcNJST59aSZbSh3aia8ZGZXBnnysGdBA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; height=&quot;606&quot; sizes=&quot;(max-width: 640px) 100vw, 640px&quot; width=&quot;640&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;决策树的一些部分&lt;/span&gt;&lt;/p&gt;


&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;39&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot; placeholder=&quot;&amp;#x8BF7;&amp;#x8F93;&amp;#x5165;&amp;#x6807;&amp;#x9898;&quot;&gt;&lt;span&gt;4. 朴素贝叶斯法&lt;/span&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;在给定一个早已发生的事件的概率时，我们用贝叶斯定理去计算某个事件将会发生的概率。在给定一些变量的值时，我们也用贝叶斯定理去计算某个结果的概率，也就是说，基于我们的先验知识（d）去计算某个假设（h）为真的概率。计算方法如下：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;P(h|d)= (P(d|h) * P(h)) / P(d)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;其中，&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot; list-paddingleft-2&quot; readability=&quot;1.5&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt;P(h|d) = 后验概率。就是假设 h 为真概率，给定的数据相当于先验知识 d。其中 P(h|d)= P(d1| h)* P(d2| h)*….*P(dn| h)* P(d)。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;P(d|h) = 似然度。假设 h 正确时，数据 d 的概率。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;P(h) = 类先验概率。假设 h 正确的额概率。(无关数据)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;P(d) = 预测器先验概率。数据的概率（无关假设)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span&gt;这个算法被称为「朴素」的原因是：它假设所有的变量是相互独立的，这也是现实世界中做出的一个朴素的假设。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;1175&quot; data-backw=&quot;556&quot; data-ratio=&quot;2.1125&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/jAqr3XzCYibxJicly0CljNCg7m6yudL2fp1DaUic0G1THEpuxyT691cicv3aiauLScnlVQzgXH8cgrz8cnZ5sdzxdnw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; height=&quot;1352&quot; sizes=&quot;(max-width: 640px) 100vw, 640px&quot; width=&quot;640&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;使用朴素贝叶斯法来预测变量「天气」变化状态&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt; 
&lt;p&gt;&lt;span&gt;以上图为例，如果天气=晴天，那么输出是什么呢？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在给定变量天气=晴天时，为了判断结果是或者否，就要计算 P(yes|sunny) 和 P(no|sunny)，然后选择概率较大的结果。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;计算过程如下：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;-&amp;gt;P(yes|sunny)= (P(sunny|yes) * P(yes)) / P(sunny) = (3/9 * 9/14 ) / (5/14) = 0.60 -&amp;gt; P(no|sunny)= (P(sunny|no) * P(no)) / P(sunny) = (2/5 * 5/14 ) / (5/14) =&lt;span&gt; 0.40&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;所以，天气=晴天时，结果为是。&lt;/span&gt;&lt;/p&gt;


&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;39&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot; placeholder=&quot;&amp;#x8BF7;&amp;#x8F93;&amp;#x5165;&amp;#x6807;&amp;#x9898;&quot;&gt;&lt;span&gt;5.KNN&lt;/span&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;KNN 使用了整个数据集作为训练集，而不是将它分为训练集和测试集。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当给定的一个数据实例时，KNN 算法会在整个数据集中寻找 k 个与其新样本距离最近的，或者 k 个与新样本最相似的，然后，对于回归问题，输出结果的平均值，或者对于分类问题，输出频率最高的类。k 的值是用户自定义的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;样本之间的相似性是用欧氏距离或者汉明（Hamming）距离来计算的。&lt;/span&gt;&lt;/p&gt;


&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;89226&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot;&gt;&lt;span&gt;非监督学习算法&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;section/&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;


&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;39&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt; &lt;section class=&quot;&quot; data-brushtype=&quot;text&quot; placeholder=&quot;&amp;#x8BF7;&amp;#x8F93;&amp;#x5165;&amp;#x6807;&amp;#x9898;&quot;&gt;&lt;span&gt;6.Apriori 算法&lt;/span&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;Apriori 算法被用来在交易数据库中进行挖掘频繁的子集，然后生成关联规则。常用于市场篮子分析，分析数据库中最常同时出现的交易。通常，如果一个顾客购买了商品 X 之后又购买了商品 Y，那么这个关联规则就可以写为：X -&amp;gt; Y。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;例如：如果一位顾客购买了牛奶和甜糖，那他很有可能还会购买咖啡粉。这个可以写成这样的关联规则： {牛奶，甜糖} -&amp;gt; 咖啡粉。关联规则是交叉了支持度（support）和置信度（confidence）的阈值之后产生的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;支持度的程度帮助修改在频繁的项目集中用来作为候选项目集的数量。这种支持度的衡量是由 Apriori 原则来指导的。Apriori 原则说明：如果一个项目集是频繁的，那么它的所有子集都是频繁的。&lt;/span&gt;&lt;/p&gt;


&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;39&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot; placeholder=&quot;&amp;#x8BF7;&amp;#x8F93;&amp;#x5165;&amp;#x6807;&amp;#x9898;&quot;&gt;&lt;span&gt;7.K-均值聚类算法&lt;/span&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;K-均值是一个对相似的数据进行聚类的迭代算法。它计算出 k 个聚类的中心点，并给某个类的聚类分配一个与其中心点距离最近的数据点。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;419&quot; data-backw=&quot;556&quot; data-ratio=&quot;0.753125&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/jAqr3XzCYibxJicly0CljNCg7m6yudL2fp8yxIEQTKawg8m4LoDwa5LIPvaFIibRhicacxBRYQBOSkF7AvoxIQZyyA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; height=&quot;482&quot; sizes=&quot;(max-width: 640px) 100vw, 640px&quot; width=&quot;640&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;K-均值算法的步骤&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;步骤 1：K-均值初始化&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;a) 选择一个 k 值。这里我们令 k=3。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;b) 将数据点随机地分配给三个聚类。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;c) 计算出每个聚类的中心点。图中的红色、蓝色和绿色的星分别代表三个聚类的中心点。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt;  步骤 2：将每一个观测值与一个聚类关联起来&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;将每一个数据点重新分配给离它最近的一个聚类中心点。如图所示，上边的五个数据点被分配给了蓝星代表的聚类。按照相同的步骤将数据点分配给红色和绿色星代表的聚类中心点。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;步骤 3：重新计算中心点&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;计算新聚类的中心点。如图所示，旧中心点是灰色的，新中心点是红色、蓝色和绿色的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;步骤 4：迭代，然后在数据点所属的聚类不变的时候退出整个过程&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;重复步骤 2-3，直至每一个聚类中的点不会被重新分配到另一个聚类中。如果在两个连续的步骤中不再发生变化，那么就退出 K-均值算法。&lt;/span&gt;&lt;/p&gt;


&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;39&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot; placeholder=&quot;&amp;#x8BF7;&amp;#x8F93;&amp;#x5165;&amp;#x6807;&amp;#x9898;&quot;&gt;&lt;span&gt;8. 主成分分析（PCA）&lt;/span&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;主成分分析（PCA）通过减少变量的数目来使数据变得更加易于探索和可视化。这通过将数据中拥有最大方差的数据抽取到一个被称为「主成分」的新坐标系中。每一个成分都是原始变量的一个新的线性组合，且是两两统计独立的。统计独立意味着这些成分的相关系数是 0。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;第一主成分捕获的是数据中最大方差的数据。第二主成分捕获的是剩下的数据中方差最大但是与第一主成分相互独立的数据。相似地，后续的主成分（例如 PC3、PC4）都是剩下的数据中方差最大的但是与之前的主成分保持独立的数据。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;集成意味着通过投票或者取平均值的方式，将多个学习器（分类器）结合起来以改善结果。在分类的时候进行投票，在回归的时候求平均值。核心思想就是集成多个学习器以使性能优于单个学习器。有三种集成学习的方法：装袋（Bagging）、提升（Boosting）和堆叠（Stacking）。本文不涉及堆叠。&lt;/span&gt;&lt;/p&gt;


&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;39&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot; placeholder=&quot;&amp;#x8BF7;&amp;#x8F93;&amp;#x5165;&amp;#x6807;&amp;#x9898;&quot;&gt;&lt;span&gt;9. 随机森林 Bagging&lt;/span&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt; &lt;span&gt;随机森林（多个学习器）是在装袋决策树（单个学习器）上做的改进。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Bagging：Bagging 的第一步就是在使用 Bootstrap 采样方法得到的数据库中创建多个模型，每个生成的训练集都是原始数据集的子集。每个训练集都有相同的大小，但是有些样本重复出现了很多次，有些样本一次未出现。然后，整个原始数据集被用为测试集。那么，如果原始数据集的大小为 N，则每个生成的训练集的大小也是 N，唯一（没有重复）样本的大小大约是 2*N/3；测试集的大小也是 N。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Bagging 的第二步就是使用同一个算法在不同的数据集上生成多个模型。然后，我们讨论一下随机森林。在决策树中，每个节点都在最好的、能够最小化误差的最佳特征上进行分支，而随机森林与之不同，我们选择随机分支的特征来构建最佳的分支。进行随机处理的原因在于：即便使用了 Bagging，当决策树选择最佳特征来分支的时候，它们最终会有相似的模型和相关联的预测结果。但是用随机子集上的特征进行分支意味着子树做的预测是没有多少相关性的。&lt;/span&gt;&lt;/p&gt;



&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;39&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot; placeholder=&quot;&amp;#x8BF7;&amp;#x8F93;&amp;#x5165;&amp;#x6807;&amp;#x9898;&quot;&gt;&lt;span&gt;10. AdaBoost 提升算法&lt;/span&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;a）Bagging 是并行集成，因为每个模型都是独立建立的。然而，提升是一个顺序性集成，每个模型都要纠正前面模型的错误分类。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;b）Bagging 主要涉及到「简单投票」，每个分类器都投票得到一个最终结果，这个分类结果是由并行模型中的大多数模型做出的；提升则使用「权重投票」。每个分类器都会投票得到一个由大多数模型做出的结果—但是建立这些顺序性模型的时候，给之前误分类样本的模型赋予了较大的权重。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Adaboost 指的是适应性提升。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;481&quot; data-backw=&quot;556&quot; data-ratio=&quot;0.865625&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/jAqr3XzCYibxJicly0CljNCg7m6yudL2fpEBBwmytG3LSPap98sHR55ZkzR14Ozn2Yx7ziatxMvRaLRNKqmRvuQvw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;640&quot; height=&quot;554&quot; sizes=&quot;(max-width: 640px) 100vw, 640px&quot; width=&quot;640&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;一个决策树的 Adaboost&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在上图 中，步骤 1、2、3 指的是被称为决策桩（decision stump）的弱学习器（是一个仅依靠一个输入作出决策的 1 级决策树；是一种根节点直接连接到叶节点的决策树）。构造决策树的过程会一直持续，直到用户定义了一个弱学习器的数目，或者训练的时候再也没有任何提升的时候。步骤 4 结合了之前模型中的 3 个决策桩（所以在这个决策树中就有 3 种分支规则）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;步骤 1：开始用 1 个决策桩来根据 1 个输入变量做决策&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;数据点的大小说明我们应用了等权重来将它们分为圆形或者三角形。决策桩在图的上半部分用一条水平线来对这些点进行分类。我们可以看到，有两个圆被误分为三角形。所以，我们会赋予这两个圆更大的权重，然后使用另一个决策桩（decision stump）。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt; 
&lt;p&gt;&lt;span&gt;&lt;strong&gt;步骤 2：转向下一个决策桩，对另一个输入变量进行决策&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们可以看到，之前的步骤中误分类的两个圆要比其余数据点大。现在，第二个决策桩要尝试正确地预测这两个圆。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;赋予更大权重的结果就是，这两个圆被左边的竖线正确地分类了。但是这又导致了对上面 3 个小圆的误分类。因此，我们要在另一个决策桩对这三个圆赋予更大的权重。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;步骤 3：训练另一个决策桩对下一个输入变量进行决策。&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;上一步误分类的 3 个圆要比其他的数据点大。现在，在右边生成了一条竖线，对三角形和圆进行分类。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;步骤 4：结合决策桩&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们结合了之前 3 步的决策桩，然后发现一个复杂的规则将数据点正确地分类了，性能要优于任何一个弱学习器。&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;89226&quot; data-color=&quot;rgb(250, 78, 12)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-brushtype=&quot;text&quot;&gt;&lt;span&gt;结语&lt;/span&gt;&lt;/section&gt;&lt;section&gt;&lt;section/&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;回顾一下，本文主要学到的内容：&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;

&lt;ol class=&quot; list-paddingleft-2&quot; readability=&quot;0.5&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;span&gt; 5 种监督学习技术：线性回归、Logistic 回归、CART（分类和决策树）、朴素贝叶斯法和 KNN。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;3 种非监督学习技术：Apriori 算法、K-均值聚类、主成分分析（PCA）。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;span&gt;两种集成学习方法：Bagging 随机森林、AdaBoost 提升。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span&gt;综上所述，机器学习的基本属性可以总结为以下：&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot; list-paddingleft-2&quot; readability=&quot;3&quot;&gt;&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;6787195013&quot; data-ad-format=&quot;auto&quot;/&gt; &lt;span&gt;机器学习是（使用样本获取近似函数的）统计学的一个分支。我们有一个确实存在的理论函数或分布用以生成数据，但我们目前不知道它是什么。我们可以对这个函数进行抽样，这些样本选自我们的训练集。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;以图片描述任务为例：函数：f⋆（图片）→图片描述，样本：data∈（image，description）。注意：由于一个物体有许多有效的描述，所以描述是文本空间中的一个分布：图片描述〜文本。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;span&gt;机器的目标是找到模型：有足够的表现力来逼近真正的函数，找到一个高效的算法，它使用训练数据找到函数最优解。而且此最优解必须对未知输入有良好的泛化能力。&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span&gt;来源：36大数据&lt;/span&gt;&lt;/p&gt;

&lt;section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;3&quot;&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot; readability=&quot;7.5&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;---&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;strong&gt;&lt;span&gt;------&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre accuse=&quot;aContent&quot;&gt;
&lt;br/&gt;&lt;strong&gt;&lt;span&gt;等的就是你，真的超有趣！高能金融抱团群发车啦~&lt;br/&gt;&lt;/span&gt;&lt;/strong&gt;&lt;strong&gt;&lt;span&gt;&lt;strong&gt;加我拉你进群呦&lt;/strong&gt;&lt;/span&gt;&lt;/strong&gt;
&lt;/pre&gt;
&lt;pre accuse=&quot;aContent&quot;&gt;
&lt;br/&gt;&lt;strong&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1&quot; data-src=&quot;http://img2.100weidu.com/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkyJCDnjGtGFUOmOAE2SZY1zvIueyhic76aNgYN38xwV6XE9y6tSzCTFYianCf1YgCrfeNqconWV5RhQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;512&quot; width=&quot;auto&quot;/&gt;&lt;/strong&gt;
&lt;/pre&gt;
&lt;pre readability=&quot;6&quot;&gt;
&lt;br/&gt;&lt;strong&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;/span&gt;&lt;/strong&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域。&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;strong&gt;&lt;span&gt;稿件一经采用，我们将奉上稿酬。&lt;/span&gt;&lt;/strong&gt;&lt;br/&gt;&lt;strong&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;br/&gt;商务合作：联系微信号hengzi5809&lt;/span&gt;&lt;/strong&gt;
&lt;/p&gt;&lt;/pre&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;

</description>
<pubDate>Sat, 18 Nov 2017 06:58:10 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/dgh0Jb8Epx</dc:identifier>
</item>
</channel>
</rss>