<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fmathalg-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/mathalg-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>算法与数学之美</title>
<link>http://www.jintiankansha.me/column/c9dZ5TM2aS</link>
<description>算法与数学之美 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>生活中处处的贝叶斯</title>
<link>http://www.jintiankansha.me/t/dDaqksMXNH</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/dDaqksMXNH</guid>
<description>&lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;88661&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section/&gt;&lt;section&gt;&lt;section/&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;section&gt;&lt;section/&gt;&lt;section&gt;&lt;section/&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;section/&gt;&lt;section readability=&quot;2.5&quot;&gt;&lt;section class=&quot;135brush&quot; readability=&quot;5&quot;&gt;&lt;p&gt;&lt;strong&gt;&lt;span&gt;摘要：&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;贝叶斯方法对于由证据的积累来推测一个事物发生的概率具有重大作用，它告诉我们当我们要预测一个事物，我们需要的是首先根据已有的经验和知识推断一个先验概率，然后在新证据不断积累的情况下调整这个概率。用贝叶斯分析的方法，可以帮助我们解决生活中方方面面的问题，尤其在我们未来将有可能深入了解的机器学习，大数据挖掘，以及相关工程性问题中，有着极其重要的地位，接下来就让我们走进贝叶斯方法，通过一系列的例子来了解其含义及应用。&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;section&gt;&lt;section/&gt;&lt;section/&gt;&lt;section/&gt;&lt;section/&gt;&lt;/section&gt;&lt;section&gt;&lt;section/&gt;&lt;section/&gt;&lt;section/&gt;&lt;section/&gt;&lt;/section&gt;&lt;section/&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot; class=&quot;_135editor&quot;&gt;
&lt;/section&gt;&lt;blockquote readability=&quot;10&quot;&gt;
&lt;p&gt;&lt;span&gt;文章主线：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;①引出贝叶斯方法的含义（1）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;②通过模型比较理论体现出贝叶斯方法的优势所在（2）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;③最后通过中文分词、机器翻译、最大似然与最小二乘、机器学习这几个实例来说明贝叶斯方法运用的普遍性（3）。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1. 贝叶斯学派与频率主义学派&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;简单说来，贝叶斯学派认为，概率是一个人对于一件事的信念强度，概率是主观的。但频率主义学派所持的是不同的观念：他们认为参数是客观存在的，即使是未知的，但都是固定值，不会改变。我参阅了一些资料，尝试以我们以前课堂上所学的概率论来解释一下，频率学派认为进行一定数量的重复实验后，如果出现某个现象的次数与总次数趋于某个值，那么这个比值就会倾向于固定。最简单的例子就是抛硬币了，在理想情况下，我们知道抛硬币正面朝上的概率会趋向于1/2。非常好理解不是么？但贝叶斯提出了一种截然不同的观念，他认为概率不应该这么简单地计算，而需要加入先验概率的考虑。先验概率也就是说，我们先设定一个假设（或信念，belief）。然后我们通过一定的实验来证明/推翻这个假设，这就是后验。随后，旧的后验会成为一个新的先验，如此重复下去。而归根结底，就得到了这样一个著名的公式：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;P( A | B ) = P(B | A ) * P( A ) / P( B )&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;（A | B表示A给定B的概率，也就是说，如果B发生，A发生的可能性有多大。反之亦然。）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2. 模型比较理论（ModelComparasion）&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2.1模型比较&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;实际上，模型比较就是去比较哪个模型（猜测）更可能隐藏在观察数据的背后。我们对用户实际想输入的单词的猜测就是模型，用户输错的单词就是观测数据。通过P(h | D) ∝ P(h) * P(D | h) 我们可以比较哪个模型最为靠谱。有时候光靠 P(D | h) （即“似然”）是不够的，有时候还需要引入 P(h) 这个先验概率。因为最大似然的猜测，其可能先验概率非常小。但有些时候，我们对于先验概率一无所知，只能假设每种猜测的先验概率是均等的，这个时候就只有用最大似然了。实际上，统计学家和贝叶斯学家有一个有趣的争论，统计学家说：我们让数据自己说话。言下之意就是要摒弃先验概率。而贝叶斯支持者则说：数据会有各种各样的偏差，而一个靠谱的先验概率则可以对这些随机噪音做到健壮。事实证明贝叶斯派胜利了，胜利的关键在于所谓先验概率其实也是经验统计的结果，譬如为什么我们会认为绝大多数硬币是基本公平的？为什么我们认为大多数人的肥胖适中？为什么我们认为肤色是种族相关的，而体重则与种族无关？先验概率里面的“先验”并不是指先于一切经验，而是仅指先于我们“当前”给出的观测数据而已，在硬币的例子中先验指的只是先于我们知道投掷的结果这个经验，而并非“先天”。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;不过有时候我们必须得承认，就算是基于以往的经验，我们手头的“先验”概率还是均匀分布，这个时候就必须依赖用最大似然。可以用一个自然语言二义性问题来说明这一点：The girl saw theboy with a telescope.到底是 The girlsaw-with-a-telescope the boy 这一语法结构，还是 The girl sawthe-boy-with-a-telescope 呢？两种语法结构的常见程度都差不多。如果语法结构是 The girl sawthe-boy-with-a-telecope 的话，怎么那个男孩偏偏手里拿的就是望远镜？这也太小概率了吧。所以唯一的解释是，这个“巧合”背后肯定有它的必然性，这个必然性就是，如果我们将语法结构解释为 The girlsaw-with-a-telescope the boy 的话，就跟数据完美吻合了——既然那个女孩是用某个东西去看这个男孩的，那么这个东西是一个望远镜就完全可以解释了（不再是小概率事件了）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;2.2 最小描述长度原则&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;贝叶斯模型比较理论与信息论有一个有趣的关联：P(h | D)∝P(h) * P(D | h)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;两边求对数，将右式的乘积变成相加：ln P(h | D) ∝ ln P(h) + ln P(D | h)。显然，最大化 P(h | D) 也就是最大化 ln P(h | D)。而 ln P(h) + ln P(D | h) 则可以解释为模型（或者称“假设”、“猜测”）h 的编码长度加上在该模型下数据 D 的编码长度。使这个和最小的模型就是最佳模型。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2.3 最优贝叶斯推理&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;推理分为两个过程，第一步是对观测数据建立一个模型。第二步则是使用这个模型来推测未知现象发生的概率。前面讲的都是对于观测数据给出最靠谱的那个模型。然而很多时候，虽然某个模型是所有模型里面最靠谱的，但是别的模型也并不是一点机会都没有。很多时候我们建立模型是为了推测未知的事情的发生概率，这个时候，不同的模型对未知的事情发生的概率都会有自己的预测，仅仅因为某一个模型概率稍大就只取它一个就太不科学了。所谓的最优贝叶斯推理就是将多个模型对于未知数据的预测结论加权平均起来（权值就是模型相应的概率）。这个推理已经把所有可能性都考虑进去，但由于计算模型可能非常费时间，它仅仅被视为一个理论基准。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;3. 无处不在的贝叶斯&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3.1 中文分词&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  贝叶斯是机器学习的核心方法之一。比如中文分词领域就用到了贝叶斯。Google 研究员吴军在《数学之美》系列中就有一篇是介绍中文分词的：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;分词问题的描述为：给定一个句子（字串），如：南京市长江大桥&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;span&gt;1. 南京市/长江大桥&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;2. 南京/市长/江大桥&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span&gt;这两个分词，到底哪个更靠谱呢？我们用贝叶斯公式来形式化地描述这个问题，令 X 为字串，Y 为词串。我们就是需要寻找使得 P(Y|X) 最大的Y，使用一次贝叶斯可得：P(Y|X)∝P(Y)*P(X|Y)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;用自然语言来说就是：这种分词方式的可能性乘以这个词串生成我们的句子的可能性。进一步容易看到：可以近似地将 P(X|Y) 看作是恒等于 1 的，因为任意假想的一种分词方式之下生成我们的句子总是精准地生成的（只需把分词之间的分界符号扔掉即可）。于是，我们就变成了去最大化 P(Y) ，也就是寻找一种分词使得这个词串的概率最大化。而如何计算一个词串：W1, W2, W3, W4 ..的可能性呢？我们知道，根据联合概率的公式展开：P(W1, W2, W3, W4 ..)=P(W1)*P(W2|W1)*P(W3|W2, W1)*P(W4|W1,W2,W3) *... 可以通过一系列的条件概率（右式）的乘积来求整个联合概率。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;不幸的是，随着条件数目的增加，数据稀疏问题也会越来越严重，即便语料库再大也无法统计出一个靠谱的 P(Wn|Wn-1,Wn-2,..,W1)来。为了缓解这个问题，计算机科学家们用了“有限地平线”假设：假设句子中一个词的出现概率只依赖于它前面的有限的 k 个词（k一般不超过 3）。虽然这个假设很天真，但结果却表明它的结果往往是很好很强大的。有了这个假设，刚才那个乘积就可以改写成： P(W1) *P(W2|W1) * P(W3|W2) * P(W4|W3) .. （假设每个词只依赖于它前面的一个词）。统计 P(W2|W1) 就不再受到数据稀疏问题的困扰了。对于我们上面提到的例子“南京市长江大桥”，如果按照自左到右的贪婪方法分词的话，结果就成了“南京市长/江大桥”。但如果按照贝叶斯分词的话（假设使用 3-gram），由于“南京市长”和“江大桥”在语料库中一起出现的频率为 0 ，这个整句的概率便会被判定为 0 。从而使得“南京市/长江大桥”这一分词方式胜出。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;有人可能会疑惑，难道我们人类也是基于这些天真的假设来进行推理的？不是的。事实上，统计机器学习方法所统计的东西往往处于相当表层的层面，在这个层面机器学习只能看到一些非常表面的现象，有一点科学研究的理念的人都知道：越是往表层去，世界就越是繁复多变。从机器学习的角度来说，特征就越多，成百上千维度都是可能的。特征一多，高维诅咒就产生了，数据就很稀疏，不够用。而人类的观察水平显然比机器学习的观察水平要更深入一些，为了避免数据稀疏我们不断地发明各种装置（最典型就是显微镜），来帮助我们直接深入到更深层的事物层面去观察更本质的联系，而不是在浅层对表面现象作统计归纳。举一个简单的例子，通过对大规模语料库的统计，机器学习可能会发现这样一个规律：所有的“他”都是不会穿裙子的，所有的“她”则都是会穿的。然而，作为一个男人，却完全无需进行任何统计学习，因为深层的规律就决定了他们根本不会去穿裙子。至于机器学习能不能完成后者的推理，则是人工智能领域的经典问题。至少在那之前，统计学习方法是不可能终结科学研究的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;3.2 统计机器翻译&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;统计机器翻译的问题可以描述为：给定一个句子e，它的可能的外文翻译f 中哪个是最靠谱的。即我们需要计算：P(f|e) 。一旦出现条件概率，贝叶斯总是挺身而出：P(f|e) ∝ P(f) * P(e|f)，这个式子的右端很容易解释：那些先验概率较高，并且更可能生成句子 e 的外文句子 f 将会胜出。我们只需简单统计就可以得到任意一个外文句子 f 的出现概率。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;然而 P(e|f) 却不是那么好求的。假设 e 为：John loves Mary，首选f是：Jean aime Marie。为了求出 P(e|f)，我们需要考虑 e 和 f 有多少种对齐的可能性。为什么要对齐，是因为一旦对齐了之后，就可以容易地计算在这个对齐之下的 P(e|f) 是多大，只需计算：P(John|Jean) * P(loves|aime) * P(Marie|Mary)即可。然后我们遍历所有的对齐方式，并将每种对齐方式之下的翻译概率∑求和。便可以获得整个的 P(e|f) 是多大。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;3.3 最大似然与最小二乘&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;学过线性代数的同学都知道用经典的最小二乘方法来做线性回归。问题描述是：给定平面上N 个点，找出一条最佳描述了这些点的直线。一个接踵而来的问题就是，如何定义最佳？我们设每个点的坐标为 (Xi, Yi)。如果直线为 y = f(x)，那么 (Xi, Yi) 跟直线对这个点的“预测”：(Xi, f(Xi)) 就相差了一个ΔYi = |Yi–f(Xi)|。最小二乘就是说寻找直线使得误差的平方和 (ΔY1)^2 + (ΔY2)^2 + ..最小，关于为什么是误差的平方和而不是误差的绝对值和这个问题，贝叶斯方法提供一个完美的解释。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我们假设直线对于坐标 Xi 给出的预测 f(Xi) 是最靠谱的预测，所有纵坐标偏离f(Xi)的那些数据点都含有噪音，是噪音使得它们偏离了完美的一条直线，一个合理的假设就是偏离路线越远的概率越小，具体小多少，可以用一个正态分布曲线来模拟，这个分布曲线以直线对 Xi 给出的预测 f(Xi) 为中心，实际纵坐标为 Yi 的点 (Xi, Yi) 发生的概率就正比于 EXP[-(ΔYi)^2]。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们要想最大化的后验概率是：P(h|D) ∝ P(h) * P(D|h)。这里 h 就是指一条特定的直线，D 就是指这 N 个数据点。我们需要寻找一条直线 h 使得 P(h) * P(D|h) 最大。很显然，P(h) 这个先验概率是均匀的，因为哪条直线也不比另一条更优越。所以只需看 P(D|h) 这一项，它是指这条直线生成这些数据点的概率，前面说生成数据点 (Xi, Yi) 的概率为 EXP[-(ΔYi)^2] 乘以一个常数。而 P(D|h) =P(d1|h) * P(d2|h) * .. 即假设各个数据点是独立生成的，所以可以把每个概率乘起来。因此生成 N 个数据点的概率为 EXP[-(ΔY1)^2] * EXP[-(ΔY2)^2] * EXP[-(ΔY3)^2] * .. = EXP{-[(ΔY1)^2 + (ΔY2)^2 + (ΔY3)^2 + ..]} 最大化这个概率就是要最小化 (ΔY1)^2 + (ΔY2)^2 + (ΔY3)^2 + .. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;3.4统计建模&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我们先将贝叶斯方法分为两类：一为统计建模，另一个为概率机器学习。后者包括了所谓的非参数方法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;建模通常在数据稀缺且难以获得时得以应用，比如在社会科学和其它难以进行大规模对照实验的环境中。想象一下，如果一个数据学家手头只拥有少量的数据，那么他会不遗余力地对算法进行调整，以期让每个数据都发挥最大的功用。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;此外，对于小数据而言，最重要的是量化不确定性，这也正是贝叶斯方法所擅长的。而贝叶斯方法——尤其是MCMC——通常计算量巨大，这又与小数据是共存的。在名为《Data Analysis Using Regression Analysis andMultilevel /Hierarchical Models》（http://www.stat.columbia.edu/~gelman/arm/）的书中，介绍了从一个没有预测变量的线性模型开始，不断增加到11个预测变量的情况并进行讨论。这种劳动密集性模式实际上与我们的机器学习方向背道而驰，我们还是希望能使用数据，让计算机自动学习。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;3.5 概率机器学习&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我们现在尝试把“概率”一词替换“贝叶斯”。从这个角度而言，它与其它分类方法并没有区别。如果从分类考虑，大多数分类器都能够输出概率预测，比如最经典的SVM（支持变量机）。但需要指出的是，这些概率只是源于分类器的信念陈述，而它们是否符合真实的概率则完全是另一回事了，这也就是所谓的校准&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;贝叶斯非参数模型：接下来我们要说说贝叶斯非参数模型的一些内容，顾名思义，这个名字代表着模型中的参数数量可以随着数据的增大或减少而自适应模型的变化。这与SVM有些相似，它能在训练点中选择支持向量，而贝叶斯非参数模型也能根据数据多少来选择参数确定模型。比较流行的贝叶斯非参数模型包括高斯回归过程，还有隐含狄里克雷分布（LDA）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;高斯回归过程：高斯回归过程有点类似于SVM——采用内核并具有类似的可伸缩性。其最大的特点在于回归特性，分类做为事后的判断，而对于SVM而言，这个过程是相反的。此外，GP是从头开始进行概率判断，而SVM不是。大多数的常规方法只提供数据点估计，而贝叶斯非参数模型则会输出不确定性估计。高斯回归过程的流行主要应用于机器学习算法的超参数优化上。数据非常小，也只有几个参数需要调整。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;后记：偶然间读了数学之美上的一篇文章——《平凡而又神奇的贝叶斯方法》，在被贝叶斯方法这一工具惊艳到的同时，也让我明白了概率论与数理统计与实际生产生活的联系之紧密。通过资料的查找，将概率论与未来可能深入学习的机器学习、大数据挖掘分析及互联网相关领域联系起来，让我更加明晰其内在含义与运行机制。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;参考资料：《数学之美番外篇——平凡而又神奇的贝叶斯方法》，以及Google、Wikipedia 上关于机器学习，概率统计的示例及例子的条目。&lt;/span&gt;&lt;/p&gt;
&lt;section data-style=&quot;white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);&quot;&gt;&lt;p&gt;&lt;span&gt;∑编辑&lt;span&gt; | &lt;/span&gt;Gemini&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;来源 | G先生&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;__bg_gif&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.9366666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/951TjTgiabkwJ4BpvBcQhGAbtWZZvV69s7GickZGibsKgYkTQkiaZfLYOmGS9iaaoibadibGJhT18OVZkfeJmCSUSD0zw/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;600&quot; width=&quot;auto&quot;/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;/section&gt;&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域&lt;br/&gt;稿件一经采用，我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 19 Feb 2018 17:31:46 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/dDaqksMXNH</dc:identifier>
</item>
<item>
<title>桥牌中的概率问题</title>
<link>http://www.jintiankansha.me/t/R7QFTOI5ys</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/R7QFTOI5ys</guid>
<description>&lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;86425&quot;&gt;&lt;section readability=&quot;2.5&quot;&gt;&lt;section data-width=&quot;65%&quot;&gt;&lt;span&gt;“&lt;/span&gt;&lt;/section&gt;&lt;section class=&quot;135brush&quot; data-style=&quot;text-align: justify;&quot; readability=&quot;5&quot;&gt;&lt;p&gt;&lt;span&gt;桥牌是一种极具魅力与技术的牌类游戏，同时具有科学性。概率在桥牌中有着极其广泛和重要的作用，深刻影响着牌手的策略，甚至能够决定成败。其中，最基本也最关键的，就是对手各花色的牌型分布问题。依据概率理论分析，我们能够得到普遍情况下牌型分布的规律，而这些规律也在世界牌手的反复实践中得到了验证。可以说，概率成就了桥牌。而我们同样可以认为，桥牌也是概率在生活中得到发扬和应用的明证。&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;section data-width=&quot;65%&quot;&gt;&lt;span&gt;”&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot; class=&quot;_135editor&quot;&gt;
&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;关键词：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;概率  桥牌  牌型分布&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;正文： &lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;    &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;在桥牌比赛中，概率扮演着一个极其重要的角色。其中，对除自己和明手外另外两家牌型分布的推算是一个非常有趣的课题。对牌型分布概率有所了解的朋友们很可能会对那些看起来比较复杂的数字感到迷惑不解——例如，1-1分布的概率为什么是52%而2-0分布是48%之类，甚至可能会觉得这个48%是不是四舍五入得来的。其实，这些数字背后的理论说起来挺简单，更值得注意的是在运用这些数字的时候要万分小心。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;所有为我们所熟知的牌型分布概率都是建立在一个条件上的：对所关心的那两家手里的牌我们事先没有获得任何信息，也就是说对那26张牌我们一无所知。如果在这个条件不能得到满足的情况下机械地运用表格里那些枯燥的数字，误入歧途的可能性是很大的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们从最简单的有价值情况入手。当自己和明手一共持有一套花色的11张的时候，另外2张牌分布的概率是怎么样的呢？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;由基本的组合理论所得出的结论非常简单：从2张牌中取出0、1和2张的方式各为1、2和1，分别对应2-0、1-1和2-0分布——也就是说，2-0分布和1-1分布的概率皆为2/4=50%. 很遗憾，这个结论是不正确的，原因在于它是一个独立事件概率理论，并没有考虑到2张牌之间的相关。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;正确的分析方法应该如下：两家暗手一共有26张牌，在零信息的条件下它们为这套花色余下的2张牌提供了26个位置。第1张牌（这种表达方式并没有人为带来2张牌“地位”上的区别，证明很简单，就是把连乘式两个因子的分子交换一下位置而已）在某一家的概率是显而易见的：13/26. 这时分析第2张牌——这时一共余下25个位置：&lt;/span&gt;&lt;/p&gt;
&lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;7&quot; data-color=&quot;rgb(12, 137, 24)&quot; readability=&quot;3&quot;&gt;&lt;blockquote readability=&quot;9&quot;&gt;
&lt;p&gt;&lt;span&gt;①2-0分布对应的情况是第2张牌也在第1张牌所在的一家，一共有12种可能，其概率为12/25；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;②1-1分布对应的情况是第2张牌在另一家，一共有13种可能，其概率为13/25. 可见1-1分布的概率比2-2分布大。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/section&gt;&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;具体的数字计算如下（对非严格等式，单独概率保留三位有效数字，总概率保留到小数点后第三位）：&lt;/span&gt;&lt;/p&gt;
&lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;7&quot; data-color=&quot;rgb(12, 137, 24)&quot; readability=&quot;4&quot;&gt;&lt;blockquote readability=&quot;11&quot;&gt;
&lt;p&gt;&lt;span&gt;①2-0分布一共有2种情况（根据独立事件组合理论，下同），各自对应概率13/26 * 12/25 = 0.24, 总概率为 2*0.24=0.48；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;②1-1分布一共也有2种情况，各自对应概率13/26* 13/25 = 0.26, 总概率为 2*0.26=0.52.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/section&gt;&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;3张牌的情况如下：&lt;/span&gt;&lt;/p&gt;
&lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;7&quot; data-color=&quot;rgb(12, 137, 24)&quot; readability=&quot;4&quot;&gt;&lt;blockquote readability=&quot;11&quot;&gt;
&lt;p&gt;&lt;span&gt;①3-0分布一共有2种情况，各自对应概率13/26* 12/25 * 11/24 = 0.11, 总概率为 2*0.11=0.22；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;②2-1分布一共有6种情况，各自对应概率13/26* 13/25 * 12/24 = 0.13, 总概率为 6*0.13=0.78.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/section&gt;&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;4张牌的情况如下：&lt;/span&gt;&lt;/p&gt;
&lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;7&quot; data-color=&quot;rgb(12, 137, 24)&quot; readability=&quot;5.5&quot;&gt;&lt;blockquote readability=&quot;14&quot;&gt;
&lt;p&gt;&lt;span&gt;①4-0分布一共有2种情况，各自对应概率13/26* 12/25 * 11/24 * 10/23 = 0.0478, 总概率为 2*0.0478=0.096；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;②3-1分布一共有8种情况，各自对应概率13/26* 13/25 * 12/24 * 11/23 = 0.0622, 总概率为 8*0.0622=0.497；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;③2-2分布一共有6种情况，各自对应概率13/26* 13/25 * 12/24 * 12/23 = 0.0678，总概率为 6*0.0678=0.407.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/section&gt;&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5张牌的情况如下：&lt;/span&gt;&lt;/p&gt;
&lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;7&quot; data-color=&quot;rgb(12, 137, 24)&quot; readability=&quot;5.5&quot;&gt;&lt;blockquote readability=&quot;14&quot;&gt;
&lt;p&gt;&lt;span&gt;①5-0分布一共有2种情况，各自对应概率13/26* 12/25 * 11/24 * 10/23 * 9/22 = 0.0196, 总概率为2*0.0196=0.039；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;②4-1分布一共有10种情况，各自对应概率13/26* 13/25 * 12/24 * 11/23 *10/22 = 0.0283, 总概率为 10*0.0283=0.283；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;③3-2分布一共有20种情况，各自对应概率13/26* 13/25 * 12/24 * 12/23 * 11/22 = 0.0339，总概率为20*0.0229=0.678.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot; class=&quot;_135editor&quot; data-color=&quot;rgb(12, 137, 24)&quot;&gt;
&lt;/section&gt;&lt;p&gt;&lt;span&gt;6张牌的情况如下：&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;7&quot; data-color=&quot;rgb(12, 137, 24)&quot; readability=&quot;7&quot;&gt;&lt;blockquote readability=&quot;17&quot;&gt;
&lt;p&gt;&lt;span&gt;①6-0分布一共有2种情况，各自对应概率13/26* 12/25 * 11/24 * 10/23 * 9/22 * 8/21 = 0.00745, 总概率为2*0.00745=0.015；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;②5-1分布一共有12种情况，各自对应概率13/26* 13/25 * 12/24 * 11/23 *10/22 * 9/21 = 0.0121, 总概率为12*0.0121=0.145；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;③4-2分布一共有30种情况，各自对应概率13/26* 13/25 * 12/24 * 12/23 * 11/22 * 10/21 = 0.0161，总概率为30*0.0161=0.484；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;④3-3分布一共有20种情况，各自对应概率13/26* 13/25 * 12/24 * 12/23 * 11/22 * 11/21 = 0.0178，总概率为20*0.0178=0.355.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot; class=&quot;_135editor&quot; data-color=&quot;rgb(12, 137, 24)&quot;&gt;
&lt;/section&gt;&lt;p&gt;&lt;span&gt;等等等等。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;以上的计算都是建立在“第1张牌有26个位置可供放置”这个条件上的，如果这个条件本身不成立，这些数字就没有了意义。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;举一个简单的例子：东家曾经作过1黑桃5张高花开叫，最后北家主打方块，庄家手里有6张将牌，东家作长4首攻黑桃3后庄家明手有3张方块，此外庄家和明手黑桃一共5张，也就是说西家有3张黑桃（这里暂且排除东家在首攻时欺诈的情况——如果东家作出长5首攻而并未事先声明ta们的首攻不是长4，也就是说东家违反了约定，但是如果这个首攻能把同伴也骗倒，那就不犯规的）。在这一瞬间，一个优秀的庄家应该先规划好做庄路线然后再命令同伴——ta也许在为大家削苹果——出牌。庄家应怎么分析外面4张将牌的分布概率呢？&lt;/span&gt;&lt;/p&gt;
&lt;section class=&quot;_135editor&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;7&quot; data-color=&quot;rgb(12, 137, 24)&quot; readability=&quot;6&quot;&gt;&lt;blockquote readability=&quot;15&quot;&gt;
&lt;p&gt;&lt;span&gt;目前为止全部已知信息如下：东家有5张黑桃，西家有3张。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;东4-西0的概率：8/18 * 7/17* 6/16 * 5/15 = 0.0229，可能性为1，总概率为0.023；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;东3-西1的概率：8/18 * 10/17* 7/16 * 6/15 = 0.0458，可能性为4，总概率为0.183；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;东2-西2的概率：8/18 * 10/17* 7/16 * 9/15 = 0.0686，可能性为6，总概率为0.412；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;东1-西3的概率：8/18 * 10/17* 9/16 * 8/15 = 0.0784，可能性为4，总概率为0.314；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;东0-西4的概率：10/18 * 9/17* 8/16 * 7/15 = 0.0686，可能性为1，总概率为0.069.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/section&gt;&lt;section data-role=&quot;paragraph&quot; class=&quot;_135editor&quot; data-color=&quot;rgb(12, 137, 24)&quot;&gt;
&lt;/section&gt;&lt;p&gt;&lt;span&gt;如果东家首攻不是黑桃，而是一门看起来像双张的花色，情况又不一样。总而言之，在计算外手将牌分布概率时一定要考虑这个问题“我已经知道这两家分别已经有什么牌”？而不是机械地去套书上写的“3-2分布概率”诸如此类的数字。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当然，真正打牌的时候没有那么多时间去算得那么精确，但是作出一个大致的判断是没有问题的。就拿上面那个例子来说，我们知道在零条件下4张牌的分布概率为0.096 (4-0)，0.497 (3-1) 和0.407 (2-2). 现在已知东家手里比西家多2张黑桃，那么认为ta手里将牌更可能比西家少是非常合理的。这时候东3-西1和东1-西3分布不再是各有0.249的概率——东3-西1要低一些，西3-东1要高一些。因此清将的时候主要应考虑2-2分布和西家有3张的情况，而不是机械地按照3-1分布来打。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这些概率分析都属于“静态概率分析”，但随着出牌的进行，牌手们在不断地获取信息，这时剩余牌的分布概率就会不断发生变化，同样牌手也会调整自己的打牌策略。正因如此，桥牌才会有场上的千变万化，桥牌历久弥新的魅力也在于此。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;参考文献：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;https://bbs.hupu.com/2216705.html&lt;/span&gt;&lt;/p&gt;

&lt;section data-style=&quot;white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);&quot;&gt;&lt;p&gt;&lt;span&gt;∑编辑&lt;span&gt; | &lt;/span&gt;Gemini&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;来源 | 航天爱好者&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;img class=&quot;__bg_gif&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.9366666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/951TjTgiabkwJ4BpvBcQhGAbtWZZvV69s7GickZGibsKgYkTQkiaZfLYOmGS9iaaoibadibGJhT18OVZkfeJmCSUSD0zw/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;600&quot; width=&quot;auto&quot;/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域&lt;br/&gt;稿件一经采用，我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Tue, 13 Feb 2018 18:01:33 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/R7QFTOI5ys</dc:identifier>
</item>
</channel>
</rss>