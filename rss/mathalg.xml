<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fmathalg-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/mathalg-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>算法与数学之美</title>
<link>http://www.jintiankansha.me/column/c9dZ5TM2aS</link>
<description>算法与数学之美 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>李承鹏｜写在512的爱国帖</title>
<link>http://www.jintiankansha.me/t/TA2HwutyKa</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/TA2HwutyKa</guid>
<description>&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6616541353383458&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkwzgCbBf5dL7871BESIetJXfbenmYtg3aJK15Ym1KJQCguLYPLvNztnLSaek3SoianLK5kaYN0FJug/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;798&quot; /&gt;&lt;/p&gt;



&lt;p&gt;&lt;span&gt;那年油菜花比往年晚开了整整一个月，人们并没有意识到什么。那时人们还相信专家，专家说花期推迟很正常，青蛙上街很正常。那天我正在书房赶一篇文章，地动时还以为家猫在脚下调皮。直到满书架的书往外飞，才明白是地震。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;大楼摇晃、灯杆倾斜、天边发出异光，总之那个景象十分特殊，像末日降临。我拼命冲下楼，地面像煮沸了一样抖动，地面下像有无数双手在抓脚后跟，好容易跟一 些邻居逃到小区外空地……慢慢地才知道都江堰死了很多人，北川已封路了，血浆都不够用了。那时我正处于一个爱国青年的尾声，纠结处热情最为猛烈，我认为报 效国家的时候到了，要用我们的血肉筑起新的长城。我在头晚到处张罗捐款后，次日清晨与唐建光、郑褚进到北川。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;可是，我在北川一中面临着人生很大的一个困扰。我无法解释为什么五层高的新楼倒塌后只有半个篮球场那么大，而几十年前修的旧楼竟没有倒塌。我也无法解释为 什么楼房脆得像饼干一样且建渣里面没什么钢筋，连一楼的学生都没来得及逃脱。一个妇人一直在我身边走来走去，她已不太哭得出声，只指着那堆很渺小的建渣： 看，那是我娃娃呀，手还在动，她还没死，但是我扯不出来她啊……那个情景令人崩溃，我看得见那个女娃娃碎花衣服的一角，还有其他孩子的衣角，他们中很多还 在动，可按部队命令我们不能上前，因为过脆的废墟不能轻易站人，否则会引起二次崩塌。就这样眼看孩子们的身体还在动，与那些石头一起，慢慢变冷，而我们无 能为力。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;在此之前我还是个爱国青年，我相信生活的很多不幸是敌对势力造成的。我在球评里写“大刀向鬼子头上砍去”，这些总打败中国队的家伙是南京大屠杀的后裔。我骂过CNN长了口蹄疫，因为蒂弗莱说中国人几千年来都是暴民和垃圾。我也不反对抵制家乐福，觉得这一个侧面也可唤醒民主意识。我家离美领馆很近，99年美国导弹轰炸我驻南大使馆时，我也在美领馆外高举过抗议的拳头。同年前往美国采访时，我写过一句“像一枚导弹打进美国本土”，深觉这句子十分有力。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5372208436724566&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkwzgCbBf5dL7871BESIetJXkpasRwibZpOYZ2lwpiaBTqQEhl5tnZCJEVz8fUO9MXDJeOpplPknzsbQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;806&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;可站在北川学校废墟前，我很困惑。我还坚持过去一些爱国观点，但开始明白建渣里的钢筋并不是帝国主义悄悄抽走的，那些孩子也不是死于侵略者的魔爪，而死于自己人的脏手。我更困惑的是，为什么911死难者都有名字，而我们的孩子没有名字。我认为我们当然要用血肉筑起新的长城，可另一方面，长城也应该要保护我们的血肉。爱国主义应该是双向的，单向收费的不是爱国主义，是向君主效忠。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我从2008发生变化，如果晚年写自传，我会以2008为 基点，在此之前我是一个混蛋。那段时间与其他一些志愿者天天在北川山里晃，救了一些老人和小孩，无意发现有一所希望小学远好无损甚至连玻璃窗都没怎么震 碎，最后学生们在老师带领下翻过三座大山逃到山外。我问过校长和老师为什么出现这个奇迹，他们说得感谢那个监工。那个监工是捐款企业派来的，工程兵出身， 修建过程中天天用小锤子敲水泥柱子听声音，他能从声音里听出有没有多掺沙子，圆石比例、水泥标号是否匹配，如果不合格就责令返工。老师告诉我，那些日子工 地上除了施工声音就是这个监工跟人吵架的声音，除因质量问题吵，就因向当地政府追款吵。因为，企业捐助希望小学的款都要先交当地政府掌握，再由政府拨给具 体施工单位……最后一架是关于操场的，终于成功追款修起了操场。大地震发生时，正是这个操场庇护了几百名孩子。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我问过这所希望小学是不是用了特殊标准才修得这么坚固。这个监工说：不，只是按国家普通建筑标准修建的。我又得知，这个监工监理了五所学校，在那场大地震中奇迹般地无一垮塌。他说：没什么奇迹，所谓奇迹，就是你修房子时，能在十年之前想到十年之后的事情。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;可是他从来不能被主流媒体宣传，名字也一直不能公布，后又传出他所属的企业其实涉黑。前两年的一天晚上，他打来电话，说正在被精神病医生治疗着，老婆也离婚了，他现在想带着女儿逃出四川，问我能不能帮他远离这是非之地，在北方找一个工作……后来我们就断了联系。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我从2008年开始变化，一个人生平第一次看到无数的冤魂，肯定会变化。我持续四年的困惑：我们不仅不能公布那些死去孩子的名字，也不能公布救了很多孩子的监工的名字。今天是汶川大震四周年，这里正式公布他的名字：句艳东。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;最近大家很爱谈爱国主义。在我看来，不要狭隘理解爱国主义就是敢于抵御外敌，爱国主义更是敢于抗争内贼，这如同你爱你们村，不仅表现在敢于同别村抢水源时 打架，更表现平时勤恳耕种、爱护资源、不对本村妇女耍流氓……一方面欺负本村人民，一方面为了财主利益勇敢跟别村打架，这不叫爱国主义，这叫勇当家丁。所 以我认为句艳东是十足的爱国者，他没去攻打钓鱼岛黄岩岛，可他救了很多孩子，他应当得到彰显，可弘扬名望的舞台被骗子占领着，我在灾区一月见闻，多少骗子 假太阳光辉之名横行……我们深爱的国家正在逆淘汰、逆宣传、逆袭真相，如果一个国家的爱国主义宣传着一些骗子，这个爱国主义本身就是骗局。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;5.13下 午再次强烈余震，接命令必须外撤，走了几公里撤到山口时正碰到央视张泉灵在时空连线，无意中我一身雨水的形象被摄进镜头。刚到山下，一个素以厚道著称的央 视记者打来电话：你丫真会出风头，没事儿你跑北川干嘛呀，抢我们台镜头。我说：日你妈。绝交至今。一月后回京碰一著名央视仁义大哥。聊起豆腐渣工程，我 说：贪官该杀几个。仁义大哥深邃地看着我：不，中国的事情要慢慢来，否则又会乱，毕竟重建还要靠他们呀。又过三年，我不小心批评了倪萍“共和国嵴梁”，该 名仁义大哥电话里斥：你丫骂人倪大姐干什么呢，她可是好人哪。我在香港书展调侃于丹余秋雨伪善，仁义大哥再斥：想不到这几年你变成这种人，承鹏，咱不能只 破坏不建设，不能见政府干的事都是错的。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6685393258426966&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkwzgCbBf5dL7871BESIetJX0RpVykVESiafBdRD1icibbQ5rNBptuAiaaUhnHbsQHJByE7GMKZiaMbV9sA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;712&quot; /&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;我曾经如此欣赏仁义大哥，现在大家天各一方，形同陌路。他那些不知是矫造还是表演的关于公平正义的话在微博流传着，星光灿烂，粉丝推崇。以及类似仁义大哥 这样的爱国者总说：不管国家有这样或那样的问题，可我们仍要爱这个国。我觉得这是个病句，我爱这个国，可我不能去爱制造豆腐渣工程的政府，更不能去爱给学 校修豆腐渣给自己修豪华办公楼的政府官员。&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我认为我仍是一个爱国者，可历经2008年 的奥运、毒牛奶特别是汶川大地震，我对爱国主义重新定义。爱国主义肯定不是一边说是外人抢劫了我们，一边亲自掠夺国人财富的主义；不是一边说恶邻让我们石 油紧缺，一边派出发改委只涨不降的主义；不是一边号召不要让强盗欺负我们的母亲，一边在大地震里让很多的母亲被欺侮的主义，她们看得见自己孩子的手还在 动，却无能为力。那天我发了一条很爱国的微博：爱国主义就是，你并不拥有一寸私土，却宣称用生命保卫这片领土。这情形就像你并不在银行里拥有一分存款，却 宣布誓死捍卫里面的金库……而且，此时你并不知道劫匪在哪里，银行保安是否把你当成劫匪。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这条微博伤害了很多爱国者的感情，纷纷斥责我为汉奸。我认为这又是个病句，在中国官不至厅局级，财产不过一个亿，每年不去开几个峰会哪好意思夸自己是汉 奸。又说我是带路党，可是不拿几张绿卡儿女不开着法拉利在名校上学不在美国置几处房产哪有资格带路。还有说，母亲无论怎样打骂过我们，可毕竟是生我养我的 亲妈啊。我就突然想起爱国者曲啸了——尼玛谁见过这么下毒手打骂自己孩子的亲妈？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我其实相当地不反对打黄岩的，可反对只打黄岩不打黄贼。可爱国者逻辑是：打黄贼得给政府一些时间，打黄岩迫不及待。对此我只有一个解析：多少黄贼，假打黄岩之名逃于法网之外。就想起五四运动中的梅思平，假爱国之名火烧曹家，可日本人打来时第一批参加了汪伪政府。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这样比爱国主义胸大肌其实很难证明真伪，说实话这三十年中国实力取得不小进步，至少近期不太可能有大批日本鬼子打进家门，所以那些组织义勇军半夜去炸碉堡的行为基本属于自我催眠的英雄幻想，不如让我们谈谈务实的爱国主义：爱 国主义是给孩子修校舍时少一分回扣，多几根钢筋；爱国主义是少修点豪华办公楼，多建些实用农舍；爱国主义是少喝点爱心茅台，多吐槽些醒世真言；爱国主义是 少宣传些虚假的英雄，多公布些逝去的名字；爱国主义是能让国民在这个国自由迁徙、念书，而不是平民子弟五证齐全才能就读京城；爱国主义爱的不是国家专政机 器，而是去爱一种共同价值观……重要的不是拥护广袤的领土，更重要的是拥有生活的尊严。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;小小黄岩，以我军威武几排炮就打成粉齑，收回失地指日可待，以壮国威；重重汶川，多少魂灵在飞，不惩前毖后，君将空负民心。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我是一个爱国者，所以，我在乎庞大的领土多一个小岛的名字，更在乎小小的纪念碑上回归数万亡灵的真实姓名&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;——是为写在5.12的爱国帖。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;⊙来源|池见新草&lt;/span&gt;&lt;/p&gt;
&lt;pre accuse=&quot;aContent&quot;&gt;
&lt;/pre&gt;

</description>
<pubDate>Sat, 12 May 2018 14:18:34 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/TA2HwutyKa</dc:identifier>
</item>
<item>
<title>一起读懂传说中的经典：受限玻尔兹曼机</title>
<link>http://www.jintiankansha.me/t/3NOltErbYk</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/3NOltErbYk</guid>
<description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section data-id=&quot;85660&quot; data-custom=&quot;rgb(117, 117, 118)&quot; data-color=&quot;rgb(117, 117, 118)&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section data-id=&quot;85660&quot; data-custom=&quot;rgb(117, 117, 118)&quot; data-color=&quot;rgb(117, 117, 118)&quot; readability=&quot;2.5&quot;&gt;&lt;section readability=&quot;5&quot;&gt;&lt;p&gt;&lt;span&gt;尽管性能没有流行的生成模型好，但受限玻尔兹曼机还是很多读者都希望了解的内容。这不仅是因为深度学习的复兴很大程度上是以它为前锋，同时它那种逐层训练与重构的思想也非常有意思。本文介绍了什么是受限玻尔兹曼机，以及它的基本原理，并以非常简单的语言描述了它的训练过程。虽然本文不能给出具体的实现，但这些基本概念还是很有意思的。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; &lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;定义 &amp;amp; 结构&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;受限玻尔兹曼机（RBM，Restricted Boltzmann machine）由多伦多大学的 Geoff Hinton 等人提出，它是一种可以用于降维、分类、回归、协同过滤、特征学习以及主题建模的算法。更多关于如何部署诸如 RBM 这样的神经网络的具体例子，请参阅 deeplearning4j 关于深度学习用例的内容。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;本文将从受限玻尔兹曼机的关系和历史重要性出发，首先讨论什么是 RBM。随后，我们会使用图表和浅显的语言来描述它们的运行原理。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;RBM 是两层神经网络，这些浅层神经网络是 DBN（深度信念网络）的构建块。RBM 的第一层被称为可见层或者输入层，它的第二层叫做隐藏层。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.9519832985386222&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8dh3Upofh0sm4KejGwQ50ob0oFB1u09GwKVliamcUb377XRsOj7VKejBTGmfPUlTxliadrk0AMdgwg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;479&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;上图中的每个圆圈代表一个类似于神经元的节点，这些节点通常是产生计算的地方。相邻层之间是相连的，但是同层之间的节点是不相连的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;也就是说，不存在层内通信，这就是 RBM 中的限制所在。每一个节点都是处理输入数据的单元，每个节点通过随机决定是否传递输入。随机意味着「随机判断」，这里修改输入的参数都是随机初始化的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;每个输入单元以数据集样本中的低级特征作为输入。例如，对于一个由灰度图组成的数据集，每个输入节点都会接收图像中的一个像素值。MNIST 数据集有 784 个像素点，所以处理它们的神经网络必须有 784 个输入节点。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;现在让我们跟随单像素穿过这两层网络。在隐藏层的节点 1，x 和一个权重相乘，然后再加上一个偏置项。这两个运算的结果可作为非线性激活函数的输入，在给定输入 x 时激活函数能给出这个节点的输出，或者信号通过它之后的强度。这里其实和我们常见的神经网络是一样的过程。&lt;/span&gt;&lt;/p&gt;

&lt;pre&gt;
&lt;code class=&quot;&quot;&gt;activation f((weight w * input x) + bias b ) = output a&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7154213036565977&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8dh3Upofh0sm4KejGwQ50oIW6Dm7e2pltCDHftoVribTtSylhhwo7GF8ajNziazgTnvibTqYHW3VKjw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;629&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;接下来，让我们看一下多个输入单元是如何结合在一个隐藏节点的。每个 x 乘以一个独立的权重，然后相加后再加一个偏置项，最后将结果传递到激活函数来产生输出。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7013996889580093&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8dh3Upofh0sm4KejGwQ50oLJnG16LyZFiatFODzE0T5kOje115Ipvy1v9xRib2u5XAumZMV7cdp4PQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;643&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;因为所有可见（或输入）节点的输入都被传递到所有的隐藏节点了，所以 RBM 可以被定义为对称二分图（symmetrical bipartite graph）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;对称意味着每个可见节点都与一个隐藏节点相连（如下所示）。二分则意味着它具有两部分，或者两层。图是一个数学术语，指的是由节点和边组成的网络。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在每一个隐藏节点，每个输入 x 都与对应的权重 w 相乘。也就是说，一个输入 x 会拥有 12 个权重（4 个输入节点×3 个输出节点）。两层之间的权重总会形成一个矩阵，矩阵的行数等于输入节点的个数，列数等于输出节点的个数。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;每个隐藏节点会接收 4 个与对应权重相乘的输入。这些乘积的和再一次与偏置相加，并将结果馈送到激活函数中以作为隐藏单元的输出。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6508422664624809&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8dh3Upofh0sm4KejGwQ50oYrAm3aiaMZCq1VhkKs5qKR1hnl5rmB6pOXEWZVVCpHics2LMLZbOibu9g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;653&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如果这两层是更深网络的一部分，那么第一个隐藏层的输出会被传递到第二个隐藏层作为输入，从这里开始就可以有很多隐藏层，直到它们增加到最终的分类层。对于简单的前馈网络，RBM 节点起着自编码器的作用，除此之外，别无其它。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5500667556742324&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8dh3Upofh0sm4KejGwQ50oIZQ54mxbAMqYn8Q9j6paN86ibTTOPInDaLm4kibk0hCy6WHFgrvl0q7g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;749&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;重建（Reconstruction）&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;但是在本文关于 RBM 的介绍中，我们会集中讨论它们如何以一种无监督的方式通过自身来重建数据，这使得在不涉及更深层网络的情况下，可见层和第一个隐藏层之间会存在数次前向和反向传播。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在重建阶段，第一个隐藏层的激活状态变成了反向传递过程中的输入。它们与每个连接边相同的权重相乘，就像 x 在前向传递的过程中随着权重调节一样。这些乘积的和在每个可见节点处又与可见层的偏置项相加，这些运算的输出就是一次重建，也就是对原始输入的一个逼近。这可以通过下图表达：&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5923392612859097&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8dh3Upofh0sm4KejGwQ50oibSjvicUv7IlRciaF1qIcibcQMhW4jJiboeIicjOUfWO9Ct4iczibLpUtAkyJQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;731&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;因为 RBM 的权重是随机初始化的，所以，重建结果和原始输入的差距通常会比较大。你可以将 r 和输入值之间的差值看做重建误差，然后这个误差会沿着 RBM 的权重反向传播，以一个迭代学习的过程不断反向传播，直到达到某个误差最小值。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;正如你所看到的，在前向传递过程中，给定权重的情况下 RBM 会使用输入来预测节点的激活值，或者输出的概率 x:p(a|x; w)。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;但是在反向传播的过程中，当激活值作为输入并输出原始数据的重建或者预测时，RBM 尝试在给定激活值 a 的情况下估计输入 x 的概率，它具有与前向传递过程中相同的权重参数。这第二个阶段可以被表达为 p(x|a; w)。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这两个概率估计将共同得到关于输入 x 和激活值 a 的联合概率分布，或者 p(x, a)。重建与回归有所不同，也不同于分类。回归基于很多输入来估计一个连续值，分类预测出离散的标签以应用在给定的输入样本上，而重建是在预测原始输入的概率分布。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这种重建被称之为生成学习，它必须跟由分类器执行的判别学习区分开来。判别学习将输入映射到标签上，有效地在数据点与样本之间绘制条件概率。若假设 RBM 的输入数据和重建结果是不同形状的正态曲线，它们只有部分重叠。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为了衡量输入数据的预测概率分布和真实分布之间的距离，RBM 使用 KL 散度来度量两个分布的相似性。KL 散度测量的是两条曲线的非重叠区域或者说发散区域，RBM 的优化算法尝试最小化这些区域，所以当共享权重与第一个隐藏层的激活值相乘时就可以得出原始输入的近似。图的左边是一组输入的概率分布 p 及其重构分布 q，图的右侧是它们的差的积分。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.3477851083883129&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8dh3Upofh0sm4KejGwQ50ofSyj9FF31T7Nn6b4TlN0U01MoJfjACfhIxPPNMg3tmRtk8jFxWRCmg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1061&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;迭代地根据它们产生的误差来调节权重，RBM 学会了逼近原始数据。你可以说权重在慢慢地反映输入数据的结构，并通过隐藏层的激活值进行编码，学习过程就像两个概率分布在逐步重合。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6020260492040521&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8dh3Upofh0sm4KejGwQ50oMgh0diblP0XHuQZ19TmeK1KiawFQQKAPEI9qUVNX5WQjM9iaMSU9eicEIA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;691&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;概率分布&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;让我们来讨论一下概率分布。如果你在掷两个骰子，所有结果的概率分布如下：&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7157057654075547&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8dh3Upofh0sm4KejGwQ50oCI8KMsNTr2sUMOe86ndBReq4msGJK5COFvfgA2pWHfzklicM2VYaHwQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1509&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;也就是说，和为 7 的结果是最有可能出现的，因为相比于 2 到 12 等其它结果，有更多的抛掷组合可以得到 7 这个结果（3+4,1+6,2+5）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;或者举另一个例子：语言是字母的特定概率分布，因为每一种语言会使用一些字母较多，而另一些较少。在英语中，字母 e、t 以及 a 是最常见的，然而在冰岛语中，最常见的字母是 a、t 和 n。因此尝试使用基于英语的权重集合来重建冰岛语将会导致较大的差异。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;同样，图像数据集拥有像素值的唯一概率分布，这取决于数据集中图像的种类。像素值的分布取决于数据集中的图像类别，例如 MNIST：&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.593128390596745&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8dh3Upofh0sm4KejGwQ50ogbP5IkEEXjLYCPl5D3S15SzrHdLGVt8Q1DWKibT7CWFxJWgOEfc1pibQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;553&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;或者 Faces in the Wild 数据集中标记的头像：&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.2414721723518851&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8dh3Upofh0sm4KejGwQ50oibZBxwwEWJ5mq3xTVIcX8aPByBLOCKn8yrHEZF576jRC6KvAlb8tk6A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1114&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;想象一下仅输入狗和大象图片的 RBM，它只有两个输出节点，每个结点对应一种动物。在前向传递的过程中 RBM 会问自己这样的问题：在给定的这些像素下，我应该向哪个节点发送更强的信号呢，大象节点还是狗的节点？在反向传递的过程中 RBM 的问题是：给定一头大象的时候，应该期望那种像素分布？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;那就是联合概率分布：给定 a 时 x 的概率以及给定 x 时 a 的概率，可以根据 RBM 两层之间的共享权重而确定。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;从某种意义上而言，学习重建的过程就是学习在给定的图像集合下，哪些像素会倾向于同时出现。由深层网络的隐藏层节点所产生的激活状态表现出来的共现现象：例如，「非线性灰色管＋大的、松软的耳朵＋皱纹」可以作为一个分布。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在上面的两幅图像中，你看到了用 Deeplearning4j 实现的 RBM。这些重建代表着 RBM 的激活值所「认为」输入数据看起来的样子，Geoff Hinton 将其称为机器「做梦」。当被呈现在神经网络在训练过程时，这种可视化是非常有用的启发，它让人确信 RBM 确实在学习。如果不是，那么它的超参数应该被调整。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;最后一点：你会注意到 RBM 有两个偏置项。这是有别于其它自动编码器的一个方面。隐藏层的偏置项有助于 RBM 在前向传递中获得非零激活值，而可见层的偏置有助于 RBM 学习后向传递中的重建。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;多层受限玻尔兹曼机&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;一旦 RBM 学到了与第一隐藏层激活值有关的输入数据的结构，那么数据就会沿着网络向下传递一层。你的第一个隐藏层就成为了新的可见层或输入层。这一层的激活值会和第二个隐藏层的权重相乘，以产生另一组的激活。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这种通过特征分组创建激活值集合序列，并对特征组进行分组的过程是特征层次结构的基础，通过这个过程，神经网络学到了更复杂的、更抽象的数据表征。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  对于每一个新的隐藏层，权重都会通过迭代反复调整，直至该层能够逼近来自于前一层的输入。这是贪婪的、逐层的、无监督的预训练。它不需要使用标签来改善网络的权重，这意味着我们可以在无标签的数据集上进行训练，而这些数据没有经过人工处理，这是现实中绝大多数的数据。通常，拥有更多数据的算法会产生更准确的结果，这也是深层学习算法崛起的原因之一。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;因为这些权重早已接近数据的特征，所以在使用深度信念网络进行图像分类的时候，后续的监督学习阶段可以更简单地学习。尽管 RBM 有很多用途，但合适的权重初始化以方便以后的分类是其主要优点之一。从某种程度而言，它们完成了某种类似于反向传播的功能：它们很好地调整了权重，以对数据进行更好的建模。你可以说预训练和反向传播是达到相同目的的可替代方法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;为了在一个图中展示受限玻尔兹曼机，我们需要使用对称二分双向图表示：&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6711309523809523&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8dh3Upofh0sm4KejGwQ50oekL0aYknd4M1tcDphTF9K3PVYfcskyJGIFicKSwkoxTl0JuOXL9aIOg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;672&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;对于那些对深入研究 RBM 结构感兴趣的人而言，它们是一种无向图模型，也被称作马尔科夫随机场。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;代码实例：Stacked RBMS&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;GitHub 链接：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span&gt;https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/unsupervised/deepbelief/DeepAutoEncoderExample.java&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;参数 &amp;amp; K&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;变量 k 是运行对比散度（Contrastive Divergence）的次数。对比散度是用来计算梯度（该斜率表示网络权重与其误差之间的关系）的方法，没有这种方法，学习就无法进行。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在上面的例子中，你可以看到如何将 RBM 创建为具有更通用多层配置的层。在每个点处，你会发现一个可以影响深度神经网络结构和性能的额外参数。大多数这些参数都是在这里定义的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;参数初始化（weightInit 或者 weightInitialization）表示放大或者抑制到达每个节点的输入信号的系数的初始值。合适的权重初始化可以节省大量的训练时间，因为训练一个网络只不过是调整系数来传递最佳信号，从而使网络能够准确分类。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;激活函数（activationFunction）是一组函数中的一个，用于确定每个节点处的激活阈值，高于阈值的信号可以通过，低于阈值的信号就被阻止。如果一个节点传递了一个信号，则它被「激活」。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;优化算法（optimizationAlgo）指神经网络最小化误差或者找到最小误差轨迹的方式，它是一步一步调整参数的。LBFGS 是一种优化算法，它利用二阶导数来计算梯度的斜率，系数将沿着梯度的斜率进行调整。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;正则化（regularization）方法（如 L2）有助于防止神经网络中的过拟合。正则化本质上会惩罚较大的系数，因为大系数意味着网络已经学会将结果锁定在几个高权值的输入上了。过强的权重会使网络模型在面对新数据的时候难以泛化。&lt;/span&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;显元/隐元（VisibleUnit/HiddenUnit）指神经网络的层。显元或者可见层，是输入到达的层，隐元或者隐藏层，是输入被结合成更复杂特征的层。这两种单元都有各自所谓的变换，在这里，可见层是高斯变换，隐藏层是整流线性单元，它们将来自它们对应层的信号映射到新的空间。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;损失函数（lossFunction）是测量误差的方法，或者测量网络预测和测试集包含的正确的标签之间差距的方法。我们在这里使用的是 SQUARED_ERROR，它使所有的误差都是正值，因此可以被求和并反向传播。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;学习率（learningRate，如 momentum）会影响神经网络在每次迭代中校正误差时调整系数的程度。这两个参数有助于确定网络将梯度降低到局部最优时的步长。较大的学习率会使网络学习得更快，并且可能越过最佳值。较小的学习率可能减慢学习，而且可能是低效的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;连续 RBM&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;连续 RBM 是受限玻尔兹曼机的一种形式，它通过不同类型的对比散度采样接受连续的输入（也就是比整数切割得更细的数字）。这允许 CRBM 处理图像像素或字数向量这类被归一化到 0 到 1 之间的小数的向量。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;应该注意，深度学习网络的每一层都需要四个元素：输入、系数、偏置项以及变换（激活算法）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;输入是数值数据，是一个来自于前面层（或者原始数据）的向量。系数是通过每个节点层的特征的权重。偏置项确保部分节点无论如何都能够被激活。变换是一种额外的算法，它在数据通过每一层以后以一种使梯度（梯度是网络必须学习的）更容易被计算的方式压缩数据。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这些额外算法和它们的组合可以逐层变化。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;一种有效的连续 RBM 在可见（或者输入）层上使用高斯变换，在隐藏层上使用整流线性单元（ReLU）变换。这在面部重建中特别有用。对于处理二进制数据的 RBM 而言，只需要进行二进制转换即可。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;高斯变换在 RBM 的隐藏层上的表现不好。相反，使用 ReLU 变换能够表示比二进制变换更多的特征，我们在深度置信网络中使用了它。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;总结 &amp;amp; 下一步工作&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;你可以将 RBM 的输出解释为百分比。每次重建的数字不为零，这是 RBM 学习输入的良好指示。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;应当指出的是，RBM 并不能生成所有的浅层前馈网络中最稳定、最一致的结果。在很多情况下，密集层自编码器性能较好。事实上，业界正在转向变分自编码器和 GAN 等工具。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;下一步，我们将会展示如何实现深度置信网络（https://deeplearning4j.org/deepbeliefnetwork.html），它由许多受限玻尔兹曼机堆叠而成。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;∑编辑 | Gemini&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;来源 | DL4J 机器之心编译&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.7509529860228716&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkz1SRWmm2kJgtV2NTQtdSgtyl7nJbJm8xS78Td2LBbJQKKqyE54oaOO9upMribZagMKYJVBaEDyKtA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;787&quot; width=&quot;auto&quot;/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;经采用我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Sat, 12 May 2018 14:18:32 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/3NOltErbYk</dc:identifier>
</item>
<item>
<title>纪念数学泰斗吴文俊先生诞辰99周年</title>
<link>http://www.jintiankansha.me/t/lpd3XNHxsQ</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/lpd3XNHxsQ</guid>
<description>&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;400&quot; data-backw=&quot;500&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.8&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/BiamlbbuowzGApHxqpHkl8QbIlSxUzTXoRFoIlGO6kvTuPsLWvGsjtAySUe9yiaxRZjaqmmSmvBIuXialGqyL3wtg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;500&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;吴文俊（1919年5月12日－2017年5月7日），世界著名数学家、中国科学院院士，曾任中国数学会理事长，中国科学院数理学部主任，全国政协常委，2002年国际数学家大会主席，中国人工智能学会名誉理事长，中国科学院系统所名誉所长。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;666&quot; data-backw=&quot;500&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.332&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/BiamlbbuowzGApHxqpHkl8QbIlSxUzTXotibMic28TY8DHVcUANReLj6n3ia6HCF0v50LU1ibFib1fOD4aoXHojjSE8w/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;500&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;生平介绍&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;吴文俊是我国最具国际影响的数学家之一，他对数学的核心领域拓扑学做出了重大贡献、开创了数学机械化新领域，对数学与计算机科学研究影响深远。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;794&quot; data-backw=&quot;530&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.4981132075471697&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/BiamlbbuowzGApHxqpHkl8QbIlSxUzTXo9cqdw1lz3ouGtMDsWmTh6CJbbU3hrEWNTEDK0kM2siaU8t9gEhyiaVoQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;530&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;年轻时代的吴文俊&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;吴文俊1919年出生于上海，1940年本科毕业于上海交通大学，1946年在中央研究院数学所工作、在陈省身先生指导下开始从事拓扑学研究，1947年赴法留学，师从埃里斯曼与嘉当，1949年毕业于法国斯特拉斯堡大学，获得法国国家博士学位，随后在法国国家科学中心任研究员。新中国成立后，吴文俊于1951年回国工作，先在北京大学数学系任教授，1952年到中国科学院数学研究所任研究员,直到1980年转入中国科学院系统科学所，1998年转入新成立的中国科学院数学与系统科学研究院。他曾任中国数学会理事长（1985-1987），中国科学院数理学部主任（1992-1994），全国政协委员、常委（1979-1998），2002年国际数学家大会主席，1993年开始任中国科学院系统所名誉所长。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;371&quot; data-backw=&quot;558&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.665625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/BiamlbbuowzGApHxqpHkl8QbIlSxUzTXoPsm5sNsOoBILBwfwI2snMlEzBIxwx7m7c7N6X9iauoox36icqLP22W5g/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;吴文俊在他的家乡朱家角&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;从1946年到1951年，吴文俊主要从事施蒂费尔-惠特尼示性类的研究工作；从1953年到1957年，他主要从事庞特里亚金示性类的研究工作。其后，吴文俊转向示嵌类的研究。由于他在拓扑学示性类及示嵌类方面的出色工作，吴文俊与华罗庚、钱学森一起荣获1956年国家第一届自然科学奖的最高奖——一等奖，并于1957年增选为中国科学院学部委员(院士)。1958年吴文俊被邀请到国际数学家大会作分组报告（因故未能成行）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.665625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/BiamlbbuowzGApHxqpHkl8QbIlSxUzTXogQqQjWdBYmqiaicTmCib91lA0SxIWt6JkrBb83eG1Yy5l25iaFOoG5iapbg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;吴文俊在数学所作拓扑学的学术报告（1955年）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;1976年，吴文俊在中国古算研究的基础上，开拓了机械化数学的崭新领域。1986年吴文俊被邀请到国际数学家大会作分组报告，1990年荣获第三世界科学院数学奖，1993年获陈嘉庚数理科学奖，1994年获首届香港求是杰出科学家奖，1997年获得国际自动推理最高奖厄布朗（Herbrand）自动推理杰出成就奖。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  2000年，吴文俊由于对拓扑学与数学机械化的贡献，获得首届最高国家科学技术奖。2006年吴文俊由于 “对数学机械化新兴交叉学科的贡献 ”与美国数学家David Mumford共同获得了有东方诺贝尔奖之称的 “邵逸夫数学奖”及一百万美元的奖金。评奖委员会认为：“通过引入深邃的数学思想，吴开辟了一种全新的方法，该方法被证明在解决一大类问题上都是极为有效的。”“吴的方法使该领域发生了一次彻底的革命性变化，并导致了该领域研究方法的变革。” 他的工作“揭示了数学的广度，为未来的数学家们树立了新的榜样。”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;吴文俊的研究工作涉及代数拓扑学、微分拓扑学、代数几何学、对策论、中国数学史、数学机械化等多个数学领域并在其中做出了独特的贡献。现介绍最重要的两个领域：拓扑学与数学机械化。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span&gt;&lt;strong&gt;对拓扑学的重大贡献&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;拓扑学是现代数学的主要领域之一。法国现代数学家狄多奈称拓扑学是现代数学的女王。陈省身先生称拓扑的发展是二十世纪上半世纪在纯粹数学的最大成就。示性类是拓扑学中最基本的整体不变量。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;349&quot; data-backw=&quot;558&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6254416961130742&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/BiamlbbuowzGApHxqpHkl8QbIlSxUzTXonrJzklxZg6hQiaOibewKdVibd0J5x3GStBkia9MC6rbxSdoJZiaQvgAia0WA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;566&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;上世纪50年代前后，示性类研究还处在起步阶段。吴文俊将示性类概念由繁化简，由难变易，引入新的方法和手段，形成了系统的理论。他引入了一类示性类，被称为吴示性类。他还给出了刻画各种示性类之间关系的吴公式。在他的工作之前，示性类的计算有极大的困难。吴的工作给出了示性类之间的关系与计算方法。由此拓扑学和数学的其他分支结合得更加紧密，许多新的研究领域应运而生。这最终使示性类理论成为拓扑学中最完美的一章。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;拓扑学中最基本问题之一是嵌入问题。在吴的工作之前，嵌入理论只有零散的结果。吴提出了吴示嵌类等一系列拓扑不变量，研究了嵌入理论的核心问题，并由此发展了统一的嵌入理论。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.665625&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/BiamlbbuowzGApHxqpHkl8QbIlSxUzTXo9XRe79OlHeCkYea7fAIIVN1O1270O8rd4yqhSEejibhzINrb5RwxAJA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;中国科学院系统科学研究所聘请陈省身教授为名誉学术委员，吴文俊教授为陈省身教授颁发聘书（1980年）&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在拓扑学研究中，吴起到了承前启后的作用。在他的工作的影响下，研究拓扑学的武器库得以形成，这极大地推进了拓扑学的发展。许多著名数学家从吴的工作中受到启发或直接以吴的成果为起始点之一，获得了一系列重大成果。例如，吴的工作被五位国际数学最高奖-菲尔兹奖-得主引用，他们分别是法国数学家托姆、美国数学家米尔诺、斯梅尔、维腾，英国数学家阿提亚，其中三位还在他们的获奖工作中使用了吴的结果。数学大师陈省身先生称赞吴“对纤维丛示性类的研究做出了划时代的贡献。”由于以上两项工作，1956年吴文俊获首届“国家自然科学一等奖。” &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;吴文俊的工作是50年代前后拓扑学的重大突破之一，产生了重大影响，成为影响深远的经典性成果, 被写进多种教科书，至今还在前沿研究中使用。&lt;/span&gt; &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;开创数学机械化领域&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  上世纪七十年代末，吴文俊用算法的观点对中国古算作了正本清源的分析，认为中国古算是算法化的数学。由此，开辟了中国数学史研究的新思路与新方法，在数学史领域产生了重大影响。1986年吴文俊被邀请到国际数学家大会作分组报告，介绍他在中国古代数学史研究中的成果。不仅如此，他又在中国古算研究的启发下，开拓了机械化数学的崭新领域。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;1977年他在初等几何定理的机械化证明方面首先取得成功，提出了几何定理机器证明的吴方法。此后，相继提出微分几何的定理机械化证明方法，方程组符号求解的吴消元法，全局优化的有限核定理，建立了数学机械化体系。他不仅建立数学机械化的基础，而且将这一理论应用于多个高技术领域，解决了曲面拼接、机构设计、计算机视觉、机器人等高技术领域核心问题。这样走出了完全是中国人自己开拓的新的数学道路，产生了巨大的国际影响。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;388&quot; data-backw=&quot;558&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.6942028985507246&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/BiamlbbuowzGApHxqpHkl8QbIlSxUzTXoC62Ea5hQKGw7ibiaF6aQ7gD5bibjlBT7KDTh8uESZ4MOZgyARMYOuve8g/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;690&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;1997年吴文俊获得国际自动推理最高奖“Herbrand自动推理杰出成就奖”。授奖词中提到，几何定理自动证明在“吴方法”出现之前进展甚微,“在不多的自动推理领域中，这种被动局面是由一个人完全扭转的。吴文俊很明显是这样一个人。”吴的工作使得“几何定理证明的研究已全面复兴，变为自动推理界最活跃与成功的领域之一。” &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;几十年来，吴文俊一贯重视人才的培养。50至60年代，他在数学所培养的学生和长期在他影响或帮助下工作的同志，有些已经成为有名的数学家。从1960年起，吴文俊担任中国科学技术大学数学系60级学生的主讲教师，在中国科学技术大学培养了80多名学生，有的已成为所在领域的领军人物，并涌现出多名国际著名学者。&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;1990年，以吴文俊为首的“中国科学院数学机械化中心”正式成立。2003年,数学机械化中心与信息安全中心联合成立了中国科学院数学机械化重点实验室。该实验室目前已经成为国际计算机数学领域最著名的领军团队之一。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;吴文俊的各项独创性研究工作使他在国际、国内产生广泛的影响，享有很高的声誉。2010年，经国际天文学联合会小天体命名委员会批准，将国际编号第7683号小行星永久命名为“吴文俊星”。 2011年，中国人工智能学会发起设立“吴文俊人工智能科学技术奖”；这是我国智能科学技术领域唯一依托社会力量设立的科学技术奖，具备直接推荐国家科学技术奖资格，被誉为“中国智能科技最高奖”。2011年，中国科学技术大学以中国科学技术大学数学所为基础组建了中国科学院吴文俊数学重点实验室。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;吴文俊治学严谨，学术思想活跃，但从来不注意个人名利，无论获得多么高的声誉，他总是勤奋地在科研第一线工作，一生积极进取、锲而不舍，不断取得新的成就。他读庞特里亚金的俄文原文完全是靠字典一个字一个字查出来的，其刻苦精神由此可见一斑。在开始从事机器证明时，他已近花甲之年，为了验证自己所提方法的有效性，他从零开始学习编写计算机程序，用Fortran语言实现了符号计算和几何定理证明的算法。编程的工作量是巨大的，他每天十多个小时在机房连续工作，终于取得成功。他平易近人，乐于助人，乐于宣传其他人的成绩，学术作风民主。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;吴文俊具有强烈的爱国心，于1951年放弃在法国的优越条件，回到祖国参加社会主义建设。1980年4月加入中国共产党，并表示“作为一个从事科学工作的知识分子，我决心在党的领导下，为党的四个现代化这一新的伟大历史任务克尽绵薄。”他对祖国的经济建设和国内重大建设项目十分关心。他对中国文化有着深刻的认识并通过自己的科研工作为复兴中国文化做出了重要贡献。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;266&quot; data-backw=&quot;400&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.665&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/BiamlbbuowzGApHxqpHkl8QbIlSxUzTXobnK5FzUtQDXxJrGibWXk75IOuCdwB52AKZibuyicm1DJTMIkaibl6CHqpg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;400&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在1994年香港求是科技基金会的“杰出科学家”奖的颁奖典礼上，陈省身先生介绍吴文俊的学术成就，盛赞他保持了历史上的许多大数学家对纯粹数学与应用数学都有贡献的传统，他的工作一般来说都是“独出蹊径，不袭前人，富创造性”，他的机器证明理论“保持了中国数学的传统”，盛赞“这是一个十分杰出的数学家！”  &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-backh=&quot;418&quot; data-backw=&quot;558&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.75&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/BiamlbbuowzGApHxqpHkl8QbIlSxUzTXoHicIPQKNbvTZ2n6u6Z5bXYVX34mVtjZkgvdjwYMibpX8ELo4HIXhGcxw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;640&quot; width=&quot;100%&quot; /&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p&gt;&lt;span&gt;吴文俊及其夫人陈丕和&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;⊙来源|中国数学会&lt;/span&gt;&lt;/p&gt;
&lt;pre accuse=&quot;aContent&quot;&gt;
&lt;br /&gt;&lt;img class=&quot;__bg_gif&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.9366666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/951TjTgiabkwJ4BpvBcQhGAbtWZZvV69s7GickZGibsKgYkTQkiaZfLYOmGS9iaaoibadibGJhT18OVZkfeJmCSUSD0zw/640?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;600&quot; width=&quot;auto&quot; /&gt;&lt;br /&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域&lt;br /&gt;稿件一经采用，我们将奉上稿酬。&lt;/span&gt;&lt;br /&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;
&lt;/pre&gt;

</description>
<pubDate>Sat, 12 May 2018 14:18:31 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/lpd3XNHxsQ</dc:identifier>
</item>
<item>
<title>Matlab高级绘图功能</title>
<link>http://www.jintiankansha.me/t/rDE3p1LWmT</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/rDE3p1LWmT</guid>
<description>&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;span&gt;温馨提示福利在最下方&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/uJcKyGGBIvEQs6EOouMOCnHLsHncfVicTmunOde3d11MmnaA2hxbew2aiakKH9a9CiarZzetFEtaWdSdQia2m2c4FQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;546&quot; width=&quot;220px&quot;/&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;23&quot;&gt;
&lt;p&gt;&lt;span&gt;d=[-1 1]; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;[x,y,z]=meshgrid(d,d,d);%定义一个立方体&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;x=[x(:);0];&lt;br/&gt;y=[y(:);0]; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;z=[z(:);0];%[x,y,z]分别为加上中心的立方体顶点&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;X=[x(:) y(:) z(:)];&lt;br/&gt;Tes=delaunayn(X);%返回 m×n 的数组值&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;tetramesh(Tes,X);%绘制四面体图&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;camorbit(20,0);%旋转摄像目标位置&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6332288401253918&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/uJcKyGGBIvEQs6EOouMOCnHLsHncfVicTLwjFUcd0x9DydkXXbF9GXaVcUCLNaXZO4aHylCxaBPUGNiax0cgvowg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;638&quot; width=&quot;338px&quot;/&gt;&lt;/p&gt;

&lt;blockquote readability=&quot;48&quot;&gt;
&lt;p&gt;&lt;span&gt;load wind%打开保存的数据&lt;br/&gt;lims=[100.64 116.67 17.25 28.75 -0.02 6.86];%定义坐标轴范围&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;[x,y,z,u,v,w]=subvolume(x,y,z,u,v,w,lims);%lims 来定义数据子集&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;[sx sy sz]=meshgrid(110,20:5:30,1:5);%定义网格点verts=stream3(x,y,z,u,v,w,sx,sy,sz,.5);%计算彩带顶点&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;cav=curl(x,y,z,u,v,w);%计算卷曲角速度&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;wind_speed=sqrt(u.^2+v.^2+w.^2);%计算流速h=streamribbon(verts,x,y,z,cav,wind_speed,2);%绘制流彩带图&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;view(3)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.9310344827586207&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/uJcKyGGBIvEQs6EOouMOCnHLsHncfVicTdxSeX4MBKfUnJasysMU6wdPUL7xOCugicDZCWR4OuMB977rcXayCTQQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;406&quot; width=&quot;266px&quot;/&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;15&quot;&gt;
&lt;p&gt;&lt;span&gt;n=6%定义轮数&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;r=(0:n)'/n;%定义轮的半径&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;theta=pi*(-n:n)/n;%定义轮的扇区角&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;X=r*cos(theta); &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Y=r*sin(theta);%定义网格顶点&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;C=r*cos(2*theta);%定义色图&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;pcolor(X,Y,C)%绘制伪彩图 axis equal tight&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;更多高级功能等你来解锁&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/uJcKyGGBIvEcIAcib5306USRvUH17CgUytN0dLuicGI8xSmjW3Myx9ZsyPbxn5PSHFBq3VCexRHpQ85tfFXRP4dQ/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1280&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;
&lt;section class=&quot;&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;p class=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;讲师介绍&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.0008481764206956&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/uJcKyGGBIvFALGIl7ZMnnGia1Aj11KWiaU7braArjSKhQPVG55ibSuIMdfF1u5XNSy41EJCMGRibFRzlKaX97IGjpg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;1179&quot; width=&quot;120px&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;董辰辉&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Matlab 畅销书主编、上市公司高级算法工程师。&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;使用 MATLAB 超过&lt;/span&gt;&lt;span&gt;&lt;strong&gt; 16 &lt;/strong&gt;&lt;/span&gt;&lt;span&gt;年，精通各种算法及 MATLAB算法工具箱，出版有《MATLAB从入门到精通》、《MATLAB2008全程指南》、《MATLAB/Simulink通信系统建模与仿真实例精讲》等教程。2009年研究生毕业从事算法工程师工作至今，主要工作内容为数学建模、优化算法、预测算法等。在算法工作方面积累了非常丰富的经验。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;span&gt;&lt;strong&gt;课程特色&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;1、&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;以全新的 &lt;/span&gt;&lt;span&gt;Matlab2018a&lt;/span&gt;&lt;span&gt; 为讲课软件；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2、&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;涵盖 &lt;/span&gt;&lt;/span&gt;&lt;span&gt;10大常用算法 +绘图+图像处理；&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;3、&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;10套 原创作业习题+&lt;/span&gt;&lt;/span&gt;&lt;span&gt;作业讲解&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;4、&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;超&lt;/span&gt;&lt;span&gt; 30学时 &lt;/span&gt;&lt;span&gt;课程&lt;/span&gt;&lt;span&gt;+ 6个月 &lt;/span&gt;&lt;span&gt;超长答疑服务&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;strong&gt;&lt;span&gt;5、&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;累计超过&lt;/span&gt;&lt;span&gt;1500多名&lt;/span&gt;&lt;span&gt;学员，学员的好评，值得信赖&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;span&gt;&lt;strong&gt;课程目录&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p helvetica=&quot;&quot; neue=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;第一章：MATLAB基础技能&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第一节&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 MATLAB入门&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 数据类型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 矩阵和数组技巧&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第二节&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 可视化及其控制&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 新版本绘图的功能&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 绘图控件如何设置属性&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第三节&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 图形对象&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第四节&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 流程控制&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 函数的类型&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 串演算函数&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第五节&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 函数变量&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 错误和异常的处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 matlab计算效率提升&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 调试模式，断点设置，如何查找bug修改bug&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 MATLAB编程规范&lt;/span&gt;&lt;/p&gt;

&lt;p helvetica=&quot;&quot; neue=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;第二章：Matlab常用算法及实践&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;第六节&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 数据拟合、概率统计、随机数的产生、灵敏性检验&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 数据文件io、大数据处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第七节     &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 优化工具箱介绍&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 模拟退火算法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第八节     &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;                 遗传算法实践&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 蒙特卡罗算法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第九节     &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 蚁群算法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 tsp问题演示&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 时间序列&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第十节     &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 神经网络&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 SVM算法 &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第十一节     &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 图论&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 图像处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 经济与金融&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;第十二节&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;                 AppDesigner&lt;/span&gt;&lt;/p&gt;

&lt;section class=&quot;&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;p class=&quot;&quot;&gt;&lt;strong&gt;开课时间&lt;/strong&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1、&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;全程直播课程&lt;span&gt; 5月12日 至 6月16日 &lt;/span&gt; 连续 6 周（每周六、周日晚19-21点）；&lt;/span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2、&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;直播过后有视频回放。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;section class=&quot;&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;span&gt;&lt;strong&gt;限时优惠&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;1、&lt;/span&gt;&lt;/strong&gt;添加极值学院助教微信（jizhi-xingchen），发送“ &lt;/span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;Matlab &lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;”，现在课程已经超过100人，需要参加拼团享受&lt;span&gt; &lt;/span&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt; 150元&lt;/span&gt;&lt;/strong&gt;&lt;span&gt; &lt;/span&gt;&lt;/span&gt;&lt;span&gt; &lt;/span&gt;优惠（仅剩 40 个课报名名额，先到先得！）；&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;img class=&quot;&quot; data-ratio=&quot;1&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/uJcKyGGBIvHflaGck6705pYXceGu5jOyfghleqEbz9hhkEekleVdmq7ZtW1DBdrKONAHnVmlrR0SsA9lXl93Ig/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;430&quot; width=&quot;170px&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;极值学院助教微信&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2、&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;购课即可获赠 &lt;span&gt; &lt;strong&gt;100个G的 MATLAB 资料大礼包&lt;/strong&gt; &lt;/span&gt;。资料预览图如下：&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;精心整理的安装包和教程包▼&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.1930635838150289&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/uJcKyGGBIvFyXib5jJSuHVpH46riborRRFgN3oicqAQaz6GZPGicUmuBuiblTpeI37L3ntGnQiaib4D4zWFPTnU9J9hHw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;865&quot; height=&quot;89&quot; width=&quot;459&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;各种教程资料▼&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.37109826589595374&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/uJcKyGGBIvFyXib5jJSuHVpH46riborRRFC5sCibYKk5bwI500vG15Exw8xMJiaRwtApbEFcULlRk0wPEy37ictZzow/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;865&quot; height=&quot;170&quot; width=&quot;460&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;section class=&quot;&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;span&gt;&lt;strong&gt;往期精彩回顾&lt;/strong&gt;&lt;/span&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;p&gt;&lt;span&gt;「Matlab从入门到算法实践」系列已经进行了四期。课程受到学员们的一直好评，董老师独特的授课方式和周到的答疑服务，让每个同学都受益匪浅，课程质量领先市场上同类课程，课后答疑服务周到全面，学员的问题全部能得到解决，而且绝大部分疑问在 1 小时之内就得到了老师的答复。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;&lt;span&gt;1、部分课件预览&lt;/span&gt;：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5696594427244582&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/uJcKyGGBIvHxyRYAibAm7dJ7Ezia0kUytwodpBbIYhKg2n6mGpqc6qyRLQnd0bXUVibkxib12rzNcEy4A29ibkzSib7A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;646&quot; height=&quot;198&quot; width=&quot;347&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;SVM算法&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5634674922600619&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/uJcKyGGBIvHxyRYAibAm7dJ7Ezia0kUytwPDBiaWfrWx0tgZf6PBdZ7GmaQbmoxcXX5UdicQeZ1b79SGBa7SrTfKbQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;646&quot; height=&quot;189&quot; width=&quot;334&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;神经网络算法&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6519916142557652&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/uJcKyGGBIvHxyRYAibAm7dJ7Ezia0kUytwHkcLjnBb5G5ScFvhXBiaexoG4HecTgLwYSTmSskxCBTXAD8INzKLEsw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;477&quot; height=&quot;218&quot; width=&quot;334&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;蚁群算法&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7791411042944786&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/uJcKyGGBIvFBeSHrus4VsDkS18NhUMZVqLkAfFsg7O2HibqHnehdKx9cupaSHlzpH4XjwwHM3f0icW3xfBCuxs7g/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;652&quot; width=&quot;351px&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;最短路径问题&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.3780487804878049&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/uJcKyGGBIvFBeSHrus4VsDkS18NhUMZVtxFH8ibcdehH3GH16ja6DJKwl4j4Pta6gmbPr3n6GFCkhUV13J6XrOg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;656&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;图像处理&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;2、课程配套作业预览&lt;/span&gt;&lt;span&gt;：&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.3168539325842696&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/uJcKyGGBIvEBLFNVoJNp3kIBGvzWmv4I7fticxicaGEQic7SpYej3Z9xj1NSQBibRmiaZiaIpYAfzNaDjLUUl1IJvraQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;890&quot; height=&quot;505&quot; width=&quot;383&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.2333333333333334&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/uJcKyGGBIvEBLFNVoJNp3kIBGvzWmv4IRyyiasUy2ia3RgicF65HFyL2j4JjKm5wfawQ1x7e0WdQVldHBYdp5ZCaw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;900&quot; height=&quot;487&quot; width=&quot;395&quot;/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;看到这里的都有福利&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;扫描下方二维码，发送“matlab”&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;免费领取电子资料&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;▼&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-croporisrc=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/uJcKyGGBIvFT7dPotZlRJoeicQUzwq7rr7KJNc3QgyyohZlggLbokhjIiam4gmF45slu1oFd5KwnD6ORbVzYe7fQ/640?wx_fmt=png&quot; data-cropx1=&quot;0&quot; data-cropx2=&quot;600&quot; data-cropy1=&quot;31.343283582089555&quot; data-cropy2=&quot;371.64179104477614&quot; data-ratio=&quot;0.5666666666666667&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/uJcKyGGBIvEQs6EOouMOCnHLsHncfVicTqlK945qk96iccJD1BstMSNvad9rmDFIdLanCryAHQNIuh3Lx0d25aNA/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;600&quot; width=&quot;402px&quot;/&gt;&lt;/p&gt;

</description>
<pubDate>Fri, 11 May 2018 13:29:22 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/rDE3p1LWmT</dc:identifier>
</item>
</channel>
</rss>