<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fmathalg-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/mathalg-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>算法与数学之美</title>
<link>http://www.jintiankansha.me/column/c9dZ5TM2aS</link>
<description>算法与数学之美 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>从香农熵到手推KL散度：一文带你纵览机器学习中的信息论</title>
<link>http://www.jintiankansha.me/t/zP8wIsaxKs</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/zP8wIsaxKs</guid>
<description>&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section data-id=&quot;85660&quot; data-custom=&quot;rgb(117, 117, 118)&quot; data-color=&quot;rgb(117, 117, 118)&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section&gt;&lt;section data-id=&quot;85660&quot; data-custom=&quot;rgb(117, 117, 118)&quot; data-color=&quot;rgb(117, 117, 118)&quot;&gt;&lt;section&gt;&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;89091&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;2.5&quot;&gt;&lt;section class=&quot;&quot; https:=&quot;&quot; fill=&quot;&quot; break-word=&quot;&quot; readability=&quot;5&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;信息论与信息熵是 AI 或机器学习中非常重要的概念，我们经常需要使用它的关键思想来描述概率分布或者量化概率分布之间的相似性。在本文中，我们从最基本的自信息和信息熵到交叉熵讨论了信息论的基础，再由最大似然估计推导出 KL 散度而加强我们对量化分布间相似性的理解。最后我们简要讨论了信息熵在机器学习中的应用，包括通过互信息选择决策树的特征、通过交叉熵衡量分类问题的损失和贝叶斯学习等。&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;信息论是应用数学的一个分支，主要研究的是对一个信号包含信息的多少进行量化。它最初被发明是用来研究在一个含有噪声的信道上用离散的字母表来发送消息，例如通过无线电传输来通信。而本文主要探讨信息熵在 AI 或机器学习中的应用，一般在机器学习中，我们可以将信息论应用在连续型变量上，并使用信息论的一些关键思想来描述概率分布或者量化概率分布之间的相似性。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;因此在机器学习中，通常要把与随机事件相关信息的期望值进行量化，此外还要量化不同概率分布之间的相似性。在这两种情况下，香农熵都被用来衡量概率分布中的信息内容。香农熵是以信息论之父 Claude Shannon 的名字命名的，也称为信息熵或微分熵（连续）。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;自信息&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;香农熵的基本概念就是所谓的一个事件背后的自信息（self-information），有时候也叫做不确定性。自信息的直觉解释如下，当某个事件（随机变量）的一个不可能的结果出现时，我们就认为它提供了大量的信息。相反地，当观察到一个经常出现的结果时，我们就认为它具有或提供少量的信息。将自信息与一个事件的意外性联系起来是很有帮助的。例如，一个极其偏畸的硬币，每一次抛掷总是正面朝上。任何一次硬币抛掷的结果都是可以完全预测的，这样的话我们就永远不会对某次结果感到惊奇，也就意味着我们从这个实验中得到的信息是 0。换言之，它的自信息是 0。如果硬币的偏畸程度稍微小一些，这样的话，尽管看到正面朝上的概率超过了 50%，每次抛掷还会有一些信息。因此，它的自信息大于 0。如果硬币的偏畸程度是导致反面朝上的结果，我们得到的自信息还是 0。在使用一个没有偏畸的硬币做实验时，每次抛掷得到正面朝上和反面朝上的概率都是 50%，我们会得到最大的意外性，因为在这种情况下硬币抛掷的结果的可预测性是最小的。我们也可以说，均匀分布的熵最大，确定事件的熵最小。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;基于以上的非正式需求，我们可以找到一个合适的函数来描述自信息。对于一个可能取值为 x_1,x_2,...,x_n 的离散随机变量 X，它的概率质量函数 P（X），以及任何正的取值在 0 到 1 之间的单调递减函数 I(p_i) 都可以作为信息的度量。此外，还有另外一个关键的属性就是独立事件的可加性；两次连续硬币抛掷的信息应该是一次单独抛掷的 2 倍。这对独立变量而言是有意义的，因为在这种情况下意外性或者不可预测性会增大为之前的两倍。形式上，对于独立事件 x_i 和 x_j 而言，我们需要 I(p_i * p_j) = I(p_i) + I(p_j)。满足所有这些要求的函数就是负对数，因此我们可以使用负对数表示自信息：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.1388888888888889&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicTbLBDavAOicicia0kk2Mea3QcWsQa9F24NW4heG1Qu9fdImr1icuT8RJMBSG7ZibUyYyxE2BBgoh9jRA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;360&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;图 1 所示是自信息 I(p)。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.71375&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicTbLBDavAOicicia0kk2Mea3Q8kbyjG0XTAx9Lza2xcmwstqmuqRhByfU3b2uQdIMqpCgCcd0LiaQ0Qw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;800&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;图 1：函数 I(p) 的自信息。小概率对应着较高的自信息，反之亦然。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们继续回到简单的硬币抛掷实验中。在信息论中，1bit（也叫做 Shannon）信息代表一次单独硬币抛掷的两种可能结果。相似地，对于两次连续抛掷而言，就需要 4 bit 来描述 4 中可能的结果。通常，用 log_2(n)（2 的对数）bit 来描述 n 个连续的独立随机事件的结果，或者是自信息。下面我们来验证一下一次连续三次的实验中自信息的计算：总共有 2^3=8 种可能的结果，每种结果的概率都是 0.5^3=0.125。所以，这次实验的自信息就是 I（0.125）= -log_2(0.125) = 3。我们需要 3bit 来描述这些所有可能的结果，那么，任何一次连续三次的硬币抛掷的自信息等于 3.0。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们也可以计算连续随机变量的自信息。图 2 展示了三种不同的概率密度函数及其对应的信息函数。图 2（A）所示的 Dirac delta 对应着很强的偏差，总是同一面朝上的偏畸硬币对应着零熵。所有 p(x)= 0 的地方都对应着无限高的信息量。然而，由于这些零概率的事件永远不会发生，所以这只是一个假设。图 2（B）中的高斯概率密度函数就是对那种经常同一面朝上，但不总是同一面朝上的情况的模拟。最后，图 2（C）描述的是一个均匀分布概率密度函数，它对应着均匀的信息量，和我们没有偏畸的硬币是类似的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.3566666666666667&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicTbLBDavAOicicia0kk2Mea3QO0FNurliaEVxQh2JQbD5GqaBXYC5u2XeWe5xhPSvxBARw4JXAoibOMzw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1200&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;图 2. [-3,3] 上的三种不同的概率密度函数及其自信息 I（p）。(A)Dirac δ函数（完全确定）；（B）μ = 0,σ = 0.5 的高斯分布；（C）均匀分布&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;熵&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;到目前为止我们只讨论了自信息。在正常的硬币实验中，自信息实际上都等于香农熵，因为所有的结果都是等概率出现的。通常，香农熵是 X 的所有可能结果的自信息期望值：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.09833333333333333&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicTbLBDavAOicicia0kk2Mea3QPZDUaBtOLcsPGg83Kic5icBMMROKJgD97xx5zD06JqF1BBc4wayPhUog/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1200&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;其中 b 是对数的底数。上面我们使用的是 b=2，其他常见的选择还有 b=10，以及 e。其实这个影响不大，因为不同底数的对数之间存在一个常数的关系。我们这里仍然假设底数为 2，所以我们将省略下面公式中的 b。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;如果仔细注意的话，你可能会疑惑，当 p(x_i) = 0 的时候会发生什么，因为这种情况下我们必须计算 0 · log(0)。事实上，我们需要计算的是一个极限：lim_(p→0) p*log(p(x_i))=0。使用洛必达法则或泰勒展开式求解的过程读者可以查阅书籍自行完成。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当香农熵泛化到连续域的时候，通常它指的是一种微分熵，对于连续的随机变量 x 及其概率密度函数 p(x)，它的香农熵定义如下：&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.08083333333333333&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicTbLBDavAOicicia0kk2Mea3QPVScJ6d24zOpZMnMkMUBh88TfJspNicWUibibDuAPVibN1WxJ4xOZKy6AA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1200&quot;/&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;span&gt;我们上述三个分布的熵分别是 0（狄拉克δ分布），174（高斯分布）以及 431（均匀分布）。在我们的实验中出现的模式是：越宽广的分布对应着越高的信息熵。仔细观察图 2（B）和图 2（C）有助于你的理解。尽管高斯分布中 I（p）曲线下面的面积要远大于均匀分布，然而它的信息熵要远小于均匀分布，因为信息熵 I（P）是按照概率密度 p 加权的，在高斯分布的两侧，p 接近于 0。更广的概率密度对应着更大的信息熵，有一个很好的比喻帮助记住这个：想象某种气体充满了一个储罐。从物理学中我们可以知道，一个封闭系统中的熵会随着时间增加，而且从来不会减少。在我们从储罐的另一侧注入气体之后，气体粒子的分布会收敛于一个均匀值。低熵意味着高密度的气体粒子聚集在某个特定的区域，而这是永远不会自发发生的。很多气体粒子聚集在某个小面积区域对应的还早呢故事我们的高斯概率密度函数，在狄拉克δ分布中是一个极端粒例子，所有的气体都被压缩在一个无限小的区域。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;交叉熵&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;交叉熵是一个用来比较两个概率分布 p 和 q 的数学工具。它和熵是类似的，我们计算 log(q) 在概率 p 下的期望，而不是反过来：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.12017167381974249&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicTbLBDavAOicicia0kk2Mea3Qp6uWAKBFAsPkbY7Xx3qDko2EiaMHxGk8ByvMD3NFIT2Vndzvb58Cibpg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;932&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;在信息论中，这个量指的是：如果用「错误」的编码方式 q（而不是 p）去编码服从 q 分布的事件，我们所需要的 bit 数。在机器学习中，这是一个衡量概率分布相似性的有用工具，而且经常作为一个损失函数。因为交叉熵等于 KL 散度加上一项信息熵，即 D_KL(p||q) = H(p, q) - H(p)。而当我们针对 Q 最小化交叉熵时，H(p) 为常量，因此它能够被省略。交叉熵在这种情况下也就等价于 KL 散度，因为 KL 散度可以简单地从最大似然估计推导出来，因此下文详细地以 GAN 为例利用 MLE 推导 KL 散度的表达式。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;KL 散度&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;与交叉熵紧密相关，KL 散度是另一个在机器学习中用来衡量相似度的量：从 q 到 p 的 KL 散度如下:D_KL(p||q)。在贝叶斯推理中，DKL(p||q) 衡量当你修改了从先验分布 q 到后验分布 p 的信念之后带来的信息增益，或者换句话说，就是用后验分布 q 来近似先验分布 p 的时候造成的信息损失。例如，在训练一个变分自编码器的隐藏空间表征时就使用了 KL 散度。KL 散度可以用熵和交叉熵表示：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.08361204013377926&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicTbLBDavAOicicia0kk2Mea3QlG7qOpwc8ibBtqEOr8ZbaolAKSl9EWQom60nN7u0hgb4jXN03s3kFicg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;598&quot; width=&quot;382px&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;交叉熵衡量的是用编码方案 q 对服从 p 的事件进行编码时所需 bit 数的平均值，而 KL 散度给出的是使用编码方案 q 而不是最优编码方案 p 时带来的额外 bit 数。从这里我们可以看到，在机器学习中，p 是固定的，交叉熵和 KL 散度之间只相差一个常数可加项，所以从优化的目标来考虑，二者是等价的。而从理论角度而言，考虑 KL 散度仍然是有意义的，KL 散度的一个属性就是，当 p 和 q 相等的时候，它的值为 0。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;KL 散度有很多有用的性质，最重要的是它是非负的。KL 散度为 0 当且仅当 P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是 『几乎 处处』 相同的。因为 KL 散度是非负的并且衡量的是两个分布之间的差异，它经常 被用作分布之间的某种距离。然而，它并不是真的距离因为它不是对称的：对于某 些 P 和 Q，D_KL(P||Q) 不等于 D_KL(Q||P)。这种非对称性意味着选择 D_KL(P||Q) 还是 D_KL(Q||P) 影响很大。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在李弘毅的讲解中，KL 散度可以从极大似然估计中推导而出。若给定一个样本数据的分布 P_data(x) 和生成的数据分布 P_G(x;θ)，那么 GAN 希望能找到一组参数θ使分布 P_g(x;θ) 和 P_data(x) 之间的距离最短，也就是找到一组生成器参数而使得生成器能生成十分逼真的图片。&lt;/span&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;现在我们可以从训练集抽取一组真实图片来训练 P_G(x;θ) 分布中的参数θ使其能逼近于真实分布。因此，现在从 P_data(x) 中抽取 m 个真实样本 {𝑥^1,𝑥^2,…,𝑥^𝑚}，其中符号「^」代表上标，即 x 中的第 i 个样本。对于每一个真实样本，我们可以计算 P_G(x^i;θ)，即在由θ确定的生成分布中，x^i 样本所出现的概率。因此，我们就可以构建似然函数：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.4343065693430657&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicTbLBDavAOicicia0kk2Mea3QPdFGXaX7Ssf3ERiaVe00rW9Tgm8YTYoscAmR3VtvHAMsjadxDqRuzVA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;274&quot; width=&quot;226px&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;其中「∏」代表累乘、P_G(x^i;θ) 代表第 i 个样本在生成分布出现的概率。从该似然函数可知，我们抽取的 m 个真实样本在 P_G(x;θ) 分布中全部出现的概率值可以表达为 L。又因为若 P_G(x;θ) 分布和 P_data(x) 分布相似，那么真实数据很可能就会出现在 P_G(x;θ) 分布中，因此 m 个样本都出现在 P_G(x;θ) 分布中的概率就会十分大。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;下面我们就可以最大化似然函数 L 而求得离真实分布最近的生成分布（即最优的参数θ）：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.5828759604829857&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicTbLBDavAOicicia0kk2Mea3QDcNRhhWXvp6EjgFHWvfic1GY477F4kJgVvmo1IqyQibh2oUiaiassHWyzQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;911&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在上面的推导中，我们希望最大化似然函数 L。若对似然函数取对数，那么累乘 ∏ 就能转化为累加 ∑，并且这一过程并不会改变最优化的结果。因此我们可以将极大似然估计化为求令 log[P_G(x;θ)] 期望最大化的θ，而期望 E[logP_G(x;θ)] 可以展开为在 x 上的积分形式：∫P_data(x)logP_G(x;θ)dx。又因为该最优化过程是针对θ的，所以我们添加一项不含θ的积分并不影响最优化效果，即可添加 -∫P_data(x)logP_data(x)dx。添加该积分后，我们可以合并这两个积分并构建类似 KL 散度的形式。该过程如下：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.18292682926829268&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicTbLBDavAOicicia0kk2Mea3QicoiaiaJFjNPCUkhKWOzpiaGxTt7qvHDydHw5Xs9GiaZlk2jfIEMAL8lxMQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;902&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这一个积分就是 KL 散度的积分形式，因此，如果我们需要求令生成分布 P_G(x;θ) 尽可能靠近真实分布 P_data(x) 的参数θ，那么我们只需要求令 KL 散度最小的参数θ。此外，我们可以将 KL 散度的积分形式转换为我们熟悉的 KL 散度表达式：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.1332560834298957&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicTbLBDavAOicicia0kk2Mea3QyicnVH3N7dfiapnaHgR5wwWF7u6yuUOhL8F6IL8Y2HfQT9Cnw79Ny8Yg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;863&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在离散型变量的情况下，KL 散度衡量的是，当我们使用一种被设计成能够使得概率分布 Q 产生的消息的长度最小的编码，发送包含由概率分布 P 产生的符号消息时，所需要的额外信息量。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;在机器学习中的使用&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;你或许疑问，这里的熵和机器学习是如何相关的。下面我们看一下一些具体的领域。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;贝叶斯学习&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;首先，上面描述的高斯分布的例子是很重要的，因为在机器学习应用中，高斯分布是一个很常见的建模选择。机器学习的目标就是减少熵。我们希望做一些预测，而且我们必须对自己的预测比较确定。而熵正好可以用来衡量这个置信度。在贝叶斯学习中，经常假设一个先验分布具有较宽广的概率密度函数，这反映了随机变量在观测之前的不确定性。当数据来了以后，熵会减小，并且让后验分布在最可能的参数值周围形成峰值。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;决策树学习&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在决策树的学习算法中，一般包含了特征选择、决策树的生成与决策树的剪枝过程。决策树的特征选择在于选取对训练数据有分类能力的特征，而通常特征选择的准则是信息增益或信息增益比。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在李航的统计学习方法中，一般熵 H(Y) 与条件熵 H(Y|X) 之差可以称为互信息（Mutual Information），决策树学习中的信息增益等价于训练数据中类与特征的互信息。若给定训练数据集 D 和特征 A，经验熵 H(D) 表示对数据集 D 进行分类的不确定性。而经验条件熵 H(D|A) 表示在特征 A 给定的条件下对数据集 D 进行分类的不确定性。那么它们的差，即信息增益，就表示由于特征 A 而使得对数据集 D 的分类的不确定性减少的程度。显然，对于数据集 D 而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;根据信息增益准则的特征选择方法是：对训练数据集（或子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;因此在决策树学习中，熵被用来构建树。通过将数据集 S 根据可能的「最佳」属性分成一些子数据集，从根节点开始构建决策树，「最佳」属性也就是能够将得到的子数据集的熵最小化的属性。这个过程被递归地重复，直到没有更多的属性来分割。此过程被称为 ID3 算法，由此可见 ID3 算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;分类&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;不管是在二分类问题还是多分类问题中，交叉熵是 logistic 回归和神经网络中的标准损失函数。通常，p 是真实分布，q 是模型描述的分布。让我们来看一个二分类 logistic 回归中的一个例子。两个类别的标签分别是 0 和 1，logistic 模型给每一个输入赋予以下概率：q_(y=1) =y_hat，q_(y=0) = 1- y_hat。这个可以简写为 q ∈ {y_hat, 1 − y_hat}。尽管真实标签是精确的 0 和 1，但是这里还是写成 p ∈ {y, 1 − y}，因此不要被这个表达方式搞混。在这个标记下，每个样本的真实值和估计分布之间的交叉熵如下：&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.09122203098106713&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicTbLBDavAOicicia0kk2Mea3QNRMRPVw6Ydd1tR3OSwC88NMrEZsnjhmV6RfOe18avlzysSVwn0usdA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1162&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;当它被作为一个损失函数使用的时候，我们用的是 N 个样本的交叉熵均值，&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.105&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gWicTbLBDavAOicicia0kk2Mea3Q4lG56DcsT1u1cUsia1GBhcnGwgzA5YPsKJvibsZY0VsSGZicuar3NIXGg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;1200&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;结语&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;以上基本上来说就是机器学习中所涉及的信息论基础，虽然我们并不怎么使用信息论中关于消息长度的解释，但机器学习主要使用用信息论的一些关键思想来描述概率分布或者量化概率分布之间的相似性。信息论是我们构建损失函数随必须要考虑的，而且它的对数形式很容易与输出单元一般采用的指数形式相结合而提高学习的效率。此外，现代深度网络的成功与最大似然估计的流行，很大程度是因为有像信息熵那样对数形式的损失函数而取得极大的提升。&lt;/span&gt;&lt;/p&gt;
&lt;section data-style=&quot;white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);&quot; readability=&quot;2&quot;&gt;&lt;p&gt;&lt;span&gt;机器之心编译&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;参与：Nurhachu Null、蒋思源&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;p&gt;&lt;img class=&quot;__bg_gif&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.9366666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/951TjTgiabkwJ4BpvBcQhGAbtWZZvV69s7GickZGibsKgYkTQkiaZfLYOmGS9iaaoibadibGJhT18OVZkfeJmCSUSD0zw/0?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;600&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域&lt;br/&gt;稿件一经采用，我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Mon, 15 Jan 2018 05:11:55 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/zP8wIsaxKs</dc:identifier>
</item>
<item>
<title>苏步青谈读书与做题</title>
<link>http://www.jintiankansha.me/t/ELkZ3spfcx</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/ELkZ3spfcx</guid>
<description>&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.7295081967213115&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/4ulCFFymNR5P3qaP0Dm5ymfnrSAaibEjICVAhXcpwznZDoI3Vb0XaH7pkMYibYhVdzTKz7XMUmWQfcsNbY4JLhtw/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;244&quot; width=&quot;100%&quot;/&gt;&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;2.5&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;5&quot;&gt;&lt;section class=&quot;&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot; data-id=&quot;89091&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;2.5&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;5&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;学数学就是除了做题还是做题吗？为什么不少同学做了那么多题，成绩还是难以提高？看数学书是浪费时间、影响做题吗？下面将苏步青的读书经历分享给大家，相信定会让你得到启发：原来，读书和解题都是数学学习的重要环节，两者相辅相成，互相促进。&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;&lt;span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;在中学的数学课本里，一些基本的概念是逐步地被引导进来的，要把基本的概念了解清楚，可以说是学好数学的第一个步骤。如果概念还没有理解清楚，就急急忙忙地去证明定理、做习题，那是没有不碰壁的。有些同学在课堂里听了老师的讲课以后，回到家里就拿起笔来做习题，这时大概对以下两类习题的演算不大会感到困难：一类是用到的基本概念已经正确理解了的习题。由于正确理解了概念，解答所配的习题就比较容易，而通过习题的演算反过来还可以进一步明确概念以及从概念导出来的结论——定理。另一类是同课堂里老师做给大家看过的例题类似的习题。对这类只要“依样画葫芦”的习题。即使基本的概念还没有理解清楚，也可以做出来，但是如果遇到习题稍有更改，就会感到无从下手。像这种看来似乎能演算而实际是“描红”的情况，在今天的中学生里并不是罕见的。不少同学对数学竞赛的试题感到困难。原因不是别的，就是从来没有见过这类题目。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;7&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;14&quot;&gt;&lt;p&gt;&lt;span&gt;正确地理解数学的基本概念之所以重要，是因为它是掌握数学基础知识的前提。犹如造房屋那样，基础打得牢靠些，将来在它的上面造起来的房屋就不会坍毁。因此，正确理解基本概念的好处不仅仅在于能解出几个习题。打基础的惟一方法是不厌其烦地反复学习。既不要以为基本的概念很抽象，不易理解，就干脆把它放过去，又不要以为它很容易懂，就不去深人理解。在高中学习的有些数学内容，由于以前在初中里学过一点，往往就容易忽视它的重要性。没看到，这些内容外表上好像同初中阶段学过的有些内容是重复的，而实际上却是螺旋式上升的。从有理数的加法发展为整式、分式的加法，又发展为函数的加法，后来在物理学里发展为力、速度矢量的加法，这是一个具体的例子。不要怕做这些课程的计算题，不要不耐烦。凡是基础的东西总不免有些单调，缺乏变化，容易使人感到厌倦，以致产生“现在不去重视它，也没有什么关系”的不正确想法。事实恰恰相反，今天基础打得不好，明天就会发现缺陷。&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;我在1924年当学生的时候，曾经做过一万道微积分的题目。&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;我为什么要做这样多的题目呢?当时我是这样想的：要真正学到手，只学一遍恐怕太少，一定的重复是很有必要的。有的人念书，念—遍就够了，我自己往往不是那么快。怎么办呢?那就多看、多念、多想，一直到把它弄懂为止。我过去念—本书或阅读一本论著，从来没有念一遍就让它过去的。要么不念，要念就念个透，一次、两次，多到五次、六次，每次念的时候总觉得比前—次有新的体会。这里可以看出，平常所谓“懂了”，中间还有深浅之分，甚至有“真懂”与“假懂”之分。我们对怎样才算学好了、真懂了，要有一个高的标准。多一分耕耘，就多一分收获。我们要把基础知识扎扎实实地学到手，就要舍得下功夫。我念外文总是念懂了才译出来。我念过的书都有笔记，并且注明某月某日看的。这些笔记我都保存着，有的笔记现在还常常用到。由于念的次数多，又通过手、脑的劳动，所以印象是深刻的。有时学生来问我什么问题，我往往可以讲出来有关这个问题的答案在哪—本书、哪一卷、哪一页里，并且还可以从书架的某一处立刻拿出来。我不相信，人的脑力有那么厉害，学了一遍，做了很少习题，很少甚至没有一点实际形象化的东西，就会都理解透了，巩固了，一辈子也不会走样了。求学问，从不知到知，从没有印象到有印象，而且还要“印”得正确，“印”的清楚，决不是较而易举的，一定要经过艰巨的劳动，通过多次反复的钻研和练习，才能达到这样的境界。学习数学，宁可多花一些时间，学得精—些、深一些、透一些，学到的知识也就扎实些、牢靠些，“有备无患或少患”，“以防万一”。对学习中的困难要有足够的估计，多做一些准备，不要贪眼前的快，学得太多、太粗，而长期下去将造成一生的慢。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;科学研究，首先是“实事求是、循序前进”，然后，在这个基础上才能“齐头并进、迎头赶&lt;br/&gt;上”。没有基础，就没有得以进一步飞跃的土壤，那怎么能够开花结果呢？&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这样说，并不反对同学们在完成自己的作业的前提下阅读课外读物；不但不反对，而且还要鼓励。只是要注意，即使在这种情况下也不要贪多冒进，囫囵吞枣，食而不化。想看这本课外读物，又想找另一本，这容易引起阅读不精，概念模糊，思路混乱等毛病。原来想看一点课外读物来帮助提高业务水平，而结果可能恰恰相反。所以，我们大学里担任一年级教学的老师经常说：“补基础，炒夹生饭，不好办。”从这一点看来，我从前在中学里念书时看不到一本数学课外读物，或许倒是一件好事！我希望成绩比较优秀的同学，在可能的条件下选定一本程度恰当的数学书籍、精读细算，踏踏实实做好、做完习题，然后考虑第二本。在阅读课外读物的时候，要练手——多做习题，又要练脑——多加思索。因为，要认识数学里的基本概念和推导得来的定理，必须经过实际演算，否则，就不可能获得念好这本书的经验，但是如果念了书，做了习题不想一想，只满足于做过算数，这同样也不可能积累经验，提高认识和掌握数学的本质。要学好数学，要善于使用思想器官，必须提倡思索，学会分析事物的方法，养成分析的习惯。数学，特别是高等数学，包括越来越多的抽象概念，尽管对一个一个的概念一读就觉得“懂了”，如果对概念的发展以及概念之间的联系不加思索和分析，往往在念完一本书或学完一门分支，回顾一下，会觉得局部是“明了’的，可是整体上不大懂，甚至莫名其妙。这样．将来把这分支的知识应用到另一理论上或建设事业的实际问题中，就会出毛病了。总之，要学好数学，方法不外是打好基础、多做习题、多加思索和分折等。学习数学除了书本知识以外，还备要同实际联系，也只有这样，才能生根壮大。发挥作用。限于篇幅，这里不详谈了。&lt;/span&gt;&lt;/p&gt;


&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;section class=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.4133333333333333&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/4ulCFFymNR5P3qaP0Dm5ymfnrSAaibEjIXU0F8HRTSCoG1CSFkaPL0wcL8Lvb2SFulfcwoiaVQrufNv0s3gliaBeg/640?wx_fmt=jpeg&quot; data-type=&quot;jpeg&quot; data-w=&quot;300&quot; width=&quot;100%&quot;/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;苏步青&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;2&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;4&quot;&gt;&lt;p&gt;&lt;span&gt;（1902~2003），中国著名的数学家、教育家，中国微分几何学派创始人，被誉为“东方第一几何学家”。苏步青将自己的毕生精力无私地奉献给了人民的教育事业，为祖国培养了一大批优秀的数学人才，包括中国科学院院士。&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;
&lt;/section&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;1.5&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;3&quot;&gt;&lt;p&gt;&lt;span&gt;本系列文章选自《数学阅读精粹》（第四册，七年级下适用），江苏凤凰教育出版社，2016.12&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section data-style=&quot;white-space: normal; text-align: left;font-size: 14px;line-height: 1.5em; color: rgb(12, 12, 12);&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;span&gt;来源：好玩的数学&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;p&gt;&lt;img class=&quot;__bg_gif&quot; data-copyright=&quot;0&quot; data-ratio=&quot;0.9366666666666666&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/951TjTgiabkwJ4BpvBcQhGAbtWZZvV69s7GickZGibsKgYkTQkiaZfLYOmGS9iaaoibadibGJhT18OVZkfeJmCSUSD0zw/0?wx_fmt=gif&quot; data-type=&quot;gif&quot; data-w=&quot;600&quot; width=&quot;auto&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域&lt;br/&gt;稿件一经采用，我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 15 Jan 2018 05:11:54 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/ELkZ3spfcx</dc:identifier>
</item>
</channel>
</rss>