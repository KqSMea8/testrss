<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fmathalg-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/mathalg-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>算法与数学之美</title>
<link>http://www.jintiankansha.me/column/c9dZ5TM2aS</link>
<description>算法与数学之美 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>丘成桐演讲全文：几何与计算数学的关系</title>
<link>http://www.jintiankansha.me/t/gpubO7qazH</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/gpubO7qazH</guid>
<description>&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot;/&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot;&gt;&lt;section class=&quot;&quot; powered-by=&quot;xiumi.us&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;1.5&quot;&gt;&lt;section class=&quot;&quot; readability=&quot;3&quot;&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;2017年10月26日上午，第十四届中国计算机大会（CNCC 2017）正式在福州海峡国际会展中心开幕&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;section class=&quot;&quot;&gt;&lt;section/&gt;&lt;section&gt;&lt;br/&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;在大会第一天，菲尔兹奖获得者、哈佛大学终身教授丘成桐在会上作为特邀嘉宾做了首个演讲报告，报告主题为《现代几何学在计算机科学中的应用》。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;报告中丘成桐先生首先介绍了现代几何的发展历史，随后介绍了他与他的学生及朋友在计算机与几何交叉方面的一些研究。对于人工智能，丘成桐先生认为现代以神经网络为代表的统计方法及机器学习在工程实践中取得了很大的成功，但其理论基础非常薄弱，是一个黑箱算法；人工智能需要一个可以被证明的理论作为基础。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;胡事民（大会程序主席，清华大学教授）：&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;大家都知道，计算机科学离不开数学，早期的计算机都是数学家帮我们奠定了基础。今天的第一个报告，我们非常荣幸地邀请到了著名的数学家、数学界最高奖菲尔兹奖获得者、哈佛大学教授丘成桐。丘老师不仅是伟大的数学家，他也在计算机方面做了很多工作。他开创了计算共形几何，广泛地应用在图形学、视觉传感器等方面。最近丘先生还在Nature上发表了一篇文章，研究社交网络。下面我们有请丘先生。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6518771331058021&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkw0ZE91yg4ukVZIfiaDVnUdXfT2MicHicPZbgzTEpK08DMISdTialtr2saxgIvelclpTYH1NyGcwo5iblw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;293&quot;/&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;&lt;span&gt;丘成桐演讲全文：&lt;/span&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span&gt;今天很荣幸地收到你们的邀请来做一个演讲。我本人在数学上的贡献不在计算机数学，最近这十多年来，由于我的学生顾险峰以及其他朋友的缘故，他们叫我帮忙做些跟计算机有关的学问。我发觉，纯数学，尤其是几何学在计算机方面有很大的应用。所以我今天就滥竽充数，讲讲几何跟计算机数学的关系。 &lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;一、现代几何的历史&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;首先，前面几分钟讲讲几何学历史。几何学一开始，就类似今天的人工智能，有很多工程上的应用以及产生的很多定理。不过随后欧几里得将当时主要的平面定理组合以后发现这些定理都可以由5个公理推出来。这是人类历史上很重要的一个里程碑，在很繁复的现象里，他找到了很简单但却很基本的五个公理，从而能将原来的这些公理全部推出来。我是很鼓励我们做人工智能的也能重复这个做法——从现在复杂多样的网络中找到它最简单的公理。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;由于希腊人的工具不够，所以除了二次方程定义的图形（圆形、直线、椭圆等）以外，他们没有能力处理更一般的图形。一直到阿基米德，才开始做微积分的无限算法（积分体积），同时他们也开始做射影几何的算法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;微积分的出现使几何学进入了新纪元，微分几何也因此诞生。几何学在欧拉和高斯手上突飞猛进，变分方法和组合方法被大量地引入到几何学当中。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;现代几何（近两百年的几何）主要发源于黎曼在1854年的博士论文，这篇论文奠定了整个现代几何的基础，他把几何图像看成一个抽象但是能够自足的空间。这个空间后来成为了现代物理的基础，现在物理中研究引力波等都是从黎曼这里开始的，没有黎曼这个空间，爱因斯坦不可能研究出来广义相对论。同时假如我们细看黎曼的这篇论文的话，就会发现，黎曼还认为离散空间也是一个很重要的空间。这个离散的空间包括了我们现在研究的图论，也用来研究宇宙万物可能产生的一切。所以即使是150年以后的今天，我们依然能看到黎曼的这个观点很重要。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;二、对称的概念&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;几何学能够提供很多重要的想法，可以讲其影响是无所不在的。几何学的很多概念在高能物理和一般的物理学领域都产生重要的影响。其中一个重要的概念叫做“对称”。“对称”的概念是在1820年到1890年间由几个重要的数学家发展出来的。我们中国喜欢讲的阴阳，其实就是一个属于对称。在数学上有一个叫庞加莱对偶的概念，其实就是阴阳，但这个概念要比阴阳具体得多，同时也真正用在了数学的发展上。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;19世纪，Sophis Lee发展的李群，也是物理学界最重要的工具之一，在现代物理中几乎没有一个学科可以离开李群的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在几何学上，1870年的时候，伟大的数学家克莱因发表了《埃尔朗根纲领》，在这个纲领里克莱因提出用对称来统治几何的重要原理，随后产生了很多重要的几何学，包括仿射几何、保角几何和投影几何等。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这些几何对于图像处理都有密切的关系。我以及我的学生和朋友这十多年来就是用保角几何及种种几何来处理不同的图像。即使是当年看上去不重要的几何，现在实际上都有它重要的用处。这种种的计算都是从对称这个概念发展出来的。从大范围对称到小范围对称，这些在20世纪的基础研究中都有很成功的影响。&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;br/&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;三、平行移动&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;另外一个很重要的概念，我想是很多做工程的人都没有注意到的，就是平行移动的概念。这个概念影响了整个数学界两千年。平行移动的概念其实就是一点和另外一点要有一个很好的比较的方法；计算机也好，图形学也好，在某一点上看到的事情要和其他点进行比较，比较的方法就叫平行移动。这也是一个很广泛、很重要的概念。现在在计算数学里面还没有大量的引进，但是在物理学界已经被大量地使用上了。所以我期望这些基本的概念以后能在计算机里面大量地使用。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;四、几何学与计算机相互之间的影响&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;现在我们具体来讲一些的事情。现代几何为计算数学奠定了很多理论的基础，并且指导了计算机科学未来发展的方向。现代几何广泛应用到计算机的所有分支。举例来讲，计算机图形学、计算机视觉、计算机辅助几何设计、计算机网络等等都有广泛的应用。再例如，黎曼几何可以用来理解社交网络；现代几何理论也可以用来理解人工智能的特性。要记住，我们讲的几何并不是高中时代的几何，所有与图像或者网络有关的都是几何的一部分。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;从另一方面来看，计算机学科的发展为现代几何提供了需求和挑战，也推动了跨学科的发展方向。例如：&lt;/span&gt;&lt;/p&gt;

&lt;blockquote readability=&quot;14&quot;&gt;
&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;人工智能中的机械定理证明推动了计算代数的发展；&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;数据安全、比特币、区块链的发展推动了代数数论、椭圆曲线和模形式的发展；&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;社交网络、大数据的发展催生了持续同调理论（persistent homology）的发展；&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;span&gt;动漫、游戏的发展推动了计算共性几何学科的诞生和发展；&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;em&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;机器学习的发展推动了最优传输理论的发展等等。&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;&lt;br/&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;五、计算机&amp;amp;几何学研究案例&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们下面举几个具体的例子，分别是图论、计算机图形学、计算机视觉、人工智能、深度学习等。这几个和几何都有密切的联系。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;1、图论&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们先讲讲图论。图，就是一大堆顶点、一大堆边把它们连起来，这是最简单不过的事情。对于一个图，譬如交通图，我们要找出它们有着怎么样一个结构，什么地方比较拥挤。有时候我们也要研究怎么将这个图切成小部分，然后分解成简单的子图；如何衡量各个连通分支间的连接度；如何将图染色等。这些问题实际上都跟图上的特征函数有密切的关系。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;图上的特征函数跟光滑图形上的特征函数有很类似的地方。我在40年前跟几个朋友，郑绍远、李伟光，做了一个工作，将光滑黎曼流形的特征函数推广到图上，得到了很好的结果。这些结果可以用来决定图上的连结的生成，研究图上的边创造过程，尤其是有个量的估值来控制在图上发散的过程。约束发散的过程可以应用到许多实际的过程中。我们还研究了图上的薛定谔方程，定义了图上的量子隧道概念。这些概念都是从物理上来的，被借用到图上。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;假如我们在考虑有向图，就是每个点、每个边，给它一个方向，我们就可以将拓扑学整个引用到图上去，定义了图上的同调群。同调群可以用来研究图上密切的关系和它的内容。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;现在我们来讲讲我们做的关于博弈理论的一个事情。进化图论为表达种群结构提供了数学工具：顶点代表个体，边代表个体的交互作用。图可以用来代表各种具有空间结构的群，例如细菌、动植物、组织结构、多细胞器官和社交网络。在进化过程中，每个个体依据自身的适应程度，进行繁殖病侵占到邻近顶点。图的拓扑反映了基因的演化——变异和选择的平衡。类似的，互联网是一个大网，一个非常复杂的网络，我可以在上面研究它的变化。社交行为的进化可以用进化博弈论来研究。个体和邻居博弈，根据收益而繁殖。个体繁殖速率受到自身与其他个体的交互作用影响，从而产生博弈的动态演化。其中心的问题就在于对于给定的图如何决定哪种策略会取得成功。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们在今年年初的时候在nature上发了篇文章，我们得到一个结果，就是在任何给定的图上进行弱选择，自然选择从两种彼此竞争的策略中如何进行挑选，这个理论框架适用于人类决策，也适用于任何集群组织的生态演化。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们从弱选择极限得到的结果，解释了何种组织结构导致何种行为。我们发现，如果存在成对的强纽带结构，合作就会大规模出现。我们用数学证明了社会学方面的一个结论：稳定的伙伴或者伴侣，对于形成合作型的社会起到了骨干作用。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt; 2、计算机图形学：全局参数化 – 共形几何&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;下面我要讲的是“计算机图形学：全局参数化 – 共形几何”。这是我们发展了二十多年的一个学问。我和顾险峰从他还在哈佛念博士的时候（1999年）我们就开始做这个事情。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;当我们将图形整体光滑映射到参数区域，使几何变得很小，会破坏掉整个图形；一般来讲这个要用手工来做，否则的话它变化非常大。针对这个问题，我们使用了纹理贴图、法向量贴图等等的方法。共性几何是一个很重要的从很古典的黎曼几何中产生的几何。&lt;/span&gt;&lt;/p&gt;

&lt;section data-role=&quot;width&quot;&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6860068259385665&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkw0ZE91yg4ukVZIfiaDVnUdXHEle3v5hT9PicYSXRoAvJK1IUYBTiaD8Ny1TL8JkMl5n8Wc3KsXic0buA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;293&quot;/&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;p&gt;&lt;span&gt;举例来讲，这个大卫的雕像，我们将它保角地映射到平面上去。它表面上看好像变化很大，但实际上变化不大，因为它是保角不变的。这在图像处理中是一个很重要的事情。举个例子来讲，从图上要画格点，因为我们画到平面上去以后，我们就可以将平面上画的很好的格点映射到脸上，就可以变成很漂亮的四方形的格点。这对工程处理有很多好处，其好处就是它将图上很小的圆映射到对方图上还是一个很小的圆，不会有扭曲，不会有太大的变化。&lt;/span&gt;&lt;/p&gt;

&lt;section data-role=&quot;width&quot;&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.552901023890785&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkw0ZE91yg4ukVZIfiaDVnUdXcfuY6StFSYdSxZONPibPUokbpZaNsvVXvyPomPyUdS8doVMiaSzLr42A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;293&quot;/&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;p&gt;&lt;span&gt;前面这些应用到一个数学上很重的定理，叫做庞加莱单值化定理，这是一个从黎曼时候开始的定理。就是讲映射的图形只跟它的拓扑性有关，这上面有三种几何，分别为：球面几何、欧氏几何、双曲几何。所有二维的几何，不管是什么样子的，我们都可以用这三种几何来分类。因此我们就可以将很复杂的事情很简单地描述出来。&lt;/span&gt;&lt;/p&gt;

&lt;section data-role=&quot;width&quot;&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6216216216216216&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkw0ZE91yg4ukVZIfiaDVnUdXKjVnc9tvlowOV69anHEZP0yicekx416ma2OxFjlOIdTBcdShaMXF6Gg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;296&quot;/&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;p&gt;&lt;span&gt;上面这些我们得出了很好的结果。但是保角也有它的缺点，所以我们也发展了第二类映射，我们使得面元被保持，而角度不一定被保持。保角映射有时候可能将一个面拉的很远，左手边是保角映射，右手边是保面元映射。右面的图在不同的情形下会得出很好的结果。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;3、计算机视觉，表情追踪 – 拟共映射&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;共性映射也可以应用到表情识别和追踪当中。我们可以自动地找到球面上曲面间的光滑映射，使得特征点匹配，使映射带来的变化很小。这是我们得到的一个很重要的结果。 &lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;section data-role=&quot;width&quot;&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.5521885521885522&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkw0ZE91yg4ukVZIfiaDVnUdXB0AXIR7kv8JZ7x7FQicdtHIYaYukXtMVCBgHFgsFrVwThKVNC7DkF3A/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;297&quot;/&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;p&gt;&lt;span&gt;因此，我们可以用来追踪表情，表情捕捉。一个人他在笑、在哭、在种种不同的表现的时候，我们能够得到他的重要的面部特征，主要的方法就是我们将它映射到平面上，然后用共形映射或拟共形映射来研究它。这些都是很重要的数学工具，在计算上也有很重要的应用。&lt;/span&gt;&lt;/p&gt;

&lt;section data-role=&quot;width&quot;&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6033898305084746&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkw0ZE91yg4ukVZIfiaDVnUdXiaZz3VoWVez7ur3192ZwCdLRGicfAexnmp92xcwJVb5lia5nhd3MPb8bQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;295&quot;/&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;p&gt;&lt;span&gt;拟共形映射到目前来讲，纯数学家把它看得还是非常重要的，它不是一个正则方程，而是一个伪正则方程，也即Beltrami方程。这个方程在我们研究图像变形时在数学上是非常重要的，所以我们应用到图形处理里面去也得到很重要的结果。我们可在微分同胚的空间进行变化到最优的映射。它对医疗和动漫都有很重要的应用。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;4、计算力学 – 六面体网格生成，叶状结构理论&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们也可以用同样的变化（保角映射）来产生六面体网格的生成和叶状结构理论。&lt;/span&gt;&lt;/p&gt;

&lt;section data-role=&quot;width&quot;&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6292517006802721&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkw0ZE91yg4ukVZIfiaDVnUdXCdYK9CDuCleKlxnqkZvEoWDv0WLSmpKkvdTsicv2FicfKMwECYEn1Buw/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;294&quot;/&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;p&gt;&lt;span&gt;这是在一只兔子上找到的好的网格。但是这个网格会产生一些奇异点（拓扑学的缘故）。针对这些奇异点，我们就做了一些研究，得出了很好的结论。&lt;/span&gt;&lt;/p&gt;

&lt;section data-role=&quot;width&quot;&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6182432432432432&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkw0ZE91yg4ukVZIfiaDVnUdXnwDI2ysJuF4443H6E1O2Wd1n5SMhSINToIV6TBqbQvvWvDXUUyCicwA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;296&quot;/&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;p&gt;&lt;span&gt;再比如，我们看这个曲面，在这个曲面上我们画出一些叶状的结构，可是它也有一定的奇异点。我们将这些奇异点分类，得出了一些在计算机科学上有意义的结论。&lt;/span&gt;&lt;/p&gt;

&lt;section data-role=&quot;width&quot;&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6348122866894198&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkw0ZE91yg4ukVZIfiaDVnUdXhFOQkxQWFll3nicO2aczTOZydSRaIKdriaTUySjVEdxKHnrSKscNiapxA/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;293&quot;/&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;p&gt;&lt;span&gt;此外，全纯二次微分的网络中间有个六边形的变化。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;5、数字几何处理-几何压缩：蒙日-安培理论，几何逼近理论&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;下面我们来看计算机的几何压缩中的蒙日-安培理论以及几何逼近理论。如何压缩复杂几何数据，同时保证几误差最小，保证黎曼度量、曲率测度、微分算子的收敛性，这些都是很重要的问题。我们用了很多共形映射的方法将曲面映射到平面去；再用蒙日-安培方程，将高曲率区域放大；随后重采样，在共性参数域上计算Delaunay三角剖分。这样得到的简化多面体网格就能够保证黎曼度量、曲率测度、微分算子收敛。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;6、区块链：数字安全，椭圆曲线理论&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这方面很多人都知道，这部分我就跳过去不再讲了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;7、人工智能&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;目前机器学习算法需要大量的样本。虽然现在比从前进步得多了，但规模还是很庞大。所以我们的想法是，让理论来帮忙处理这种复杂的数据学习。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在机器学习中有很多统计的内容，但是很多内容我们都不是很了解它是如何产生的。所以我们需要用一些比较严格的数学的理论来从这些复杂的现象中抽取出它们的本质。我们今天介绍一下用几何的方法来研究对抗生成网络（GAN）的事情。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt;  生成对抗网络GAN（Generative Adversarial Networks）其实就是以己之矛克己之盾，在矛盾中发展，使得矛更加锋利，盾更加强韧。这里的盾就被称为判别器（Descriminator），矛被称为生成器（Generator）。生成器G一般是将一个随机变量（例如高斯分布或者均匀分布），通过参数化的概率生成模型（通常是用一个深度神经网进行参数化），进行概率分布的逆变换采样，从而得到一个生成的概率分布。判别器D也通常采用深度卷积神经网络。&lt;/span&gt;&lt;/p&gt;

&lt;section data-role=&quot;width&quot;&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6530612244897959&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkw0ZE91yg4ukVZIfiaDVnUdX7AQT0kp0XC6aHJPTiay4iavmdzWR0DKDRJJxX2OJZsicKQ1Grbus275QQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;294&quot;/&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;p&gt;&lt;span&gt;举个例子来讲，有个概率分布u，u是基本的白噪音，影射到右手边的图片，一个概率分布v。我们从映射里看到GAN的问题其实就是：在两个概率分布u和v之间，找到一个最优的传输映射，从一个空间到另外一个空间，使它的概率分布是保持的。&lt;/span&gt;&lt;/p&gt;

&lt;section data-role=&quot;width&quot;&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.711864406779661&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkw0ZE91yg4ukVZIfiaDVnUdXj9mOQ0g00wicjiaiatXlrIwB6V8T7c5YhSsc8svHE30vtbJ4vpNl2Kxdg/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;295&quot;/&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;p&gt;&lt;span&gt;u通过phi映射到v上去，同时我们要将它传输的代价变得最小。这样的变化是我们所需要的，因为这就不再需要像刚才所说的矛盾变化来达到最好的结果。我们知道，映射可以用一个方程来解决，所以我们其实就是要找一个凸函数U，它的梯度是我们的映射函数phi，它满足一个方程：蒙日-安培方程。&lt;/span&gt;&lt;/p&gt;

&lt;section data-role=&quot;width&quot;&gt;&lt;p&gt;&lt;img class=&quot;&quot; data-ratio=&quot;0.6952054794520548&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkw0ZE91yg4ukVZIfiaDVnUdXr9foXDzR1atcUF82ZwVKhZETv0YsyqvtjNfG8W3GKdWBwYKMdkjLbQ/640?wx_fmt=png&quot; data-type=&quot;png&quot; data-w=&quot;292&quot;/&gt;&lt;/p&gt;
&lt;/section&gt;
&lt;p&gt;&lt;span&gt;我们可以通过对这个方程进行求解的方式来找到最优传输映射，所以就节省很多生成对抗的时间。蒙日-安培方程本身其实是等价于微分几何中的亚历山大定理的。60年代就有人处理过这个方程，我自己也做过这个方程，前几年顾险峰跟他的学生也和我一起对它做了一个计算。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;对抗生成网络实质上就是用深度神经网络来计算概率测度之间的变换。虽然规模宏大，但是数学本质并不复杂。应用相对成熟的最优传输理论和蒙日-安培理论，我们可以为机器学习的黑箱给出透明的几何解释，这有助于设计出更为高效和可靠的计算方法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;六、总结&lt;/span&gt;&lt;/strong&gt;&lt;span&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;我们看到现代数学和计算机科学的发展紧密相关，共形几何的单值化定理、蒙日-安培理论、最优传输理论等现代几何中的定理应用到计算机科学中的很多领域。我希望我们能够将更多那些表面上看来很高深的数学应用到我们日常的计算机上去，不但是能够有效地提出计算机的算法，同时也能够给它一个理论的基础。人工智能需要一个坚实的理论基础，否则它的发展会有很大困难。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;∑编辑 | Gemini&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;来源 | 雷锋网&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.0437601296596435&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;617&quot; width=&quot;auto&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky7x6u1VxMVMia4MLibNzC2nrumY3zDflTsCeoM04M1BrkvPny8tsw6hYkIicUr42iarLmadL2x6JwV6A/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域，经采用我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Tue, 11 Sep 2018 09:04:15 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/gpubO7qazH</dc:identifier>
</item>
<item>
<title>如何优雅地从四个方面加深对深度学习的理解</title>
<link>http://www.jintiankansha.me/t/tG8EBnPeSk</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/tG8EBnPeSk</guid>
<description>&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; background-color:=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;在今年的 ICML 上，深度学习理论成为最大的主题之一。会议第一天，Sanjeev Arora 就展开了关于深度学习理论理解的教程，并从四个方面分析了关于该领域的研究：非凸优化、超参数和泛化、深度的意义以及生成模型。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; box-sizing:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;534&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp6Ybbr97W3HMw9iaR5RHE3hBsWgyzZlNz03qicp4EQeDuQIiacMT5DEbJA/0?wx_fmt=png&quot; data-ratio=&quot;0.64765625&quot; data-type=&quot;png&quot; data-w=&quot;1280&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp6Ybbr97W3HMw9iaR5RHE3hBsWgyzZlNz03qicp4EQeDuQIiacMT5DEbJA/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;2017 年 12 月 NIPS 的 Test-of-Time Award 颁奖典礼上，Ali Rahimi 这样呼吁人们加深对深度学习的理解：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;我希望生活在这样的一个世界，它的系统是建立在严谨可靠而且可证实的知识之上，而非炼金术。[……] 简单的实验和定理是帮助理解复杂大现象的基石。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Ali 的目标不是解散各个领域，而是「展开对话」。这个目标已经实现了，但对于目前的深度学习应被视为炼金术还是工程或科学，人们仍存在分歧。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;7 个月后，在斯德哥尔摩举行的国际机器学习会议 (ICML) 上，机器学习社区又聚焦了这个问题。此次大会与会者有 5000 多名，并累计发表论文 629 篇，这是基础机器学习研究的「年度大戏」。而深度学习理论已成为此次会议的最大主题之一。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;619&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp1dIWuDmhoB7L7gpic4EteZT5IqDf9zib9vhQPialJNYphHicLMTkRXkKlg/0?wx_fmt=png&quot; data-ratio=&quot;0.75&quot; data-type=&quot;png&quot; data-w=&quot;1200&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp1dIWuDmhoB7L7gpic4EteZT5IqDf9zib9vhQPialJNYphHicLMTkRXkKlg/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;会议第一天，最大的房间里就挤满了机器学习相关人员，他们准备聆听 Sanjeev Arora 关于深度学习理论理解的教程。这位普林斯顿大学计算机科学教授在演讲中总结了目前的深度学习理论研究领域，并将其分成四类：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;非凸优化：如何理解与深度神经网络相关的高度非凸损失函数？为什么随机梯度下降法会收敛？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;超参数和泛化：在经典统计理论中，为什么泛化依赖于参数的数量而非深度学习？存在其它较好的泛化方法吗？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;深度的意义：深度如何帮助神经网络收敛？深度和泛化之间的联系是什么？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;生成模型：为什么生成对抗网络（GAN）效果非常好？有什么理论特性能使模型稳定或者避免模式崩溃？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;在这一系列的文章中，我们将根据最新的论文（尤其是 ICML2018 的论文），帮助大家直观理解这四个方面。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;第一篇文章将重点讨论深度网络的非凸优化问题。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt; 非凸优化 &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;416&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpfuicL2hzO7alSeeyoSvrSoLF6RJTnuBicrFh1E1eibZae2dCDBXEJkE4w/0?wx_fmt=png&quot; data-ratio=&quot;0.5035405192761605&quot; data-type=&quot;png&quot; data-w=&quot;1271&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpfuicL2hzO7alSeeyoSvrSoLF6RJTnuBicrFh1E1eibZae2dCDBXEJkE4w/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;我敢打赌，你们很多人都曾尝试过训练自己的「深度网络」，结果却因为无法让它发挥作用而陷入自我怀疑。这不是你的错。我认为都是梯度下降的错。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Ali Rahimi 在 NIPS 演讲中曾说，随机梯度下降 (SGD) 的确是深度学习的基石，它应该解决高度非凸优化问题。理解它何时起作用，以及为什么起作用，是我们在深度学习的基本理论中一定会提出的最基本问题之一。具体来说，对于深度神经网络的非凸优化研究可以分为两个问题：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;损失函数是什么样的？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;SGD 为什么收敛？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt; 损失函数是什么样的？ &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;如果让你想象一个全局最小值，很可能你脑海中出现的第一幅图是这样的：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;274&quot; data-backw=&quot;436&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpjLVcBpDV1mVZQBBS933UEh2A6Za0Vn4HKPkiaMOtUroOjuP4psTvxHA/0?wx_fmt=png&quot; data-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpYosC1qjoB8iaobVth7kTklNOePv4I2T6cibO3kJ5zuldJ4uMKvzWAQyQ/0?wx_fmt=jpeg&quot; data-ratio=&quot;0.6284403669724771&quot; data-type=&quot;jpeg&quot; data-w=&quot;436&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpYosC1qjoB8iaobVth7kTklNOePv4I2T6cibO3kJ5zuldJ4uMKvzWAQyQ/640?wx_fmt=jpeg&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;二维世界中的全局最小值附近，函数是严格凸的（这意味着 hessian 矩阵的两个特征值都是正数）。但在一个有着数十亿参数的世界里，就像在深度学习中，全局最小值附近的方向都不平坦的可能性有多大？或者 hessian 中一个为零（或近似为零）的特征值都没有的概率有多大？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Sanjeev Arora 在教程中写的第一个评论是：损失函数的可能方向数量会随着维度的增长呈指数增长。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;218&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpGdVWAVt0fp4EicEZ7XLfL5dV5z385g3NqW1mP4OjjZGy3ZKKv7Qib9iag/0?wx_fmt=png&quot; data-ratio=&quot;0.26450344149459193&quot; data-type=&quot;png&quot; data-w=&quot;1017&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpGdVWAVt0fp4EicEZ7XLfL5dV5z385g3NqW1mP4OjjZGy3ZKKv7Qib9iag/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;直观上看，全局最小值似乎不是一个点而是一个连接管（connected manifold）。这意味着如果找到了全局最小值，你就能够穿过一条平坦的路径，在这条道路上，所有的点都是最小值。海德堡大学的一个研究团队在论文《Essentially No Barriers in Neural Network Energy Landscape》中证明了这一点。他们提出了一个更常规的说法，即任何两个全局最小值都可以通过一条平坦的路径连接。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;628&quot; data-backw=&quot;674&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpLL8vQpKoC4DluRJFwuQeLxTic5lm9YjpulFue2IUHy3Ly3GqaK3H0JA/0?wx_fmt=png&quot; data-ratio=&quot;0.9317507418397626&quot; data-type=&quot;png&quot; data-w=&quot;674&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpLL8vQpKoC4DluRJFwuQeLxTic5lm9YjpulFue2IUHy3Ly3GqaK3H0JA/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt;  在 MNIST 上的 CNN 或在 PTB 上的 RNN 已经是这样的情况，但是该项研究将这种认知扩展到了在更高级的数据集（CIFAR10 和 CIFAR100）上训练的更大网络（一些 DenseNet 和 ResNet）上。为了找到这条路径，他们使用了一种来自分子统计力学的启发式方法，叫做 AutoNEB。其思想是在两个极小值之间创建一个初始路径（例如线性），并在该路径上设置中心点。然后迭代地调整中心点的位置，以最小化每个中心点的损失，并确保中心点之间的距离保持不变（通过用弹簧建模中心点之间的空间）。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;虽然他们没有从理论上证明这个结果，但他们对为什么存在这样的路径给出了一些直观的解释：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;如果我们扰乱单个参数，比如添加一个小常数，然后让其它部分去自适应这种变化，仍然可以使损失最小化。因此可以认为，通过微调，无数其它参数可以「弥补」强加在一个参数上的改变。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;因此，本文的结果可以帮助我们通过超参数化和高维空间，以不同的方式看待极小值。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;通俗来说，当考虑神经网络的损失函数时，你应该牢记一个给定的点周围可能有非常多的方向。由此得出另一个结论，鞍点肯定比局部最小值多得多：在给定的关键点上，在数十亿个可能的方向中，很可能会找到一个向下的方向（如果不是在全局最小值上）。这种认知在 NIPS 2014 年发表的论文《Identifying and attacking the saddle point problem in high-dimensional non-convex optimization》中被严格规范化，并得到了实证证明。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;为什么 SGD 收敛（或不收敛）？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;深度神经网络优化的第二个重要问题与 SGD 的收敛性有关。虽然这种算法长期以来被看做是一种快速的近似版梯度下降，但我们现在可以证明 SGD 实际上收敛于更好、更一般的最小值。但我们能否将其规范化并定量地解释 SGD 脱离局部极小值或鞍点的能力？&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;SGD 修改了损失函数&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;论文《An Alternative View: When Does SGD Escape Local Minima?》表明，实施 SGD 相当于在卷积（所以平滑）的损失函数上进行常规梯度下降。根据这一观点并在某些假设下，他们证明了 SGD 将设法脱离局部最小值，并收敛到全局最小值附近的一个小区域。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt; SGD 由随机微分方程控制 &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;连续 SGD 彻底改变了我对这个算法的看法。在 ICML 2018 关于非凸优化的研讨会上，Yoshua Bengio 在他关于随机梯度下降、平滑和泛化的演讲中提出了这个想法。SGD 不是在损失函数上移动一个点，而是一片点云或者说一个分布。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;532&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpVp9Fh7rOaLEfLv0gnnQpxHacwrTOV7Xd2ibaAJAmczu0RmxC6yvmu3g/0?wx_fmt=png&quot; data-ratio=&quot;0.644747393744988&quot; data-type=&quot;png&quot; data-w=&quot;1247&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpVp9Fh7rOaLEfLv0gnnQpxHacwrTOV7Xd2ibaAJAmczu0RmxC6yvmu3g/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;幻灯片摘自 Y. Bengio 在 ICML 2018 发表的演讲。他提出用分布（或点云）代替点来看待 SGD。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;这个点云的大小（即相关分布的方差）与 learning_rate / batch_size 因子成正比。Pratik Chaudhari 和 Stefano Soatto 在论文《Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks》中证明了这一点。这个公式非常直观：较低的 batch size 意味着梯度非常混乱（因为要在数据集一个非常小的子集上计算），高学习率意味着步骤混乱。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;将 SGD 视为随时间变化的分布可以得出：控制下降的方程现在是随机偏微分方程。更准确地说，在某些假设下，论文表明控制方程实际上是一个 Fokker-Planck 方程。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;img class=&quot;&quot; data-backh=&quot;588&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpNKIctvKJ5QUYGIibribA3R1XCRBRK3JOlKWGa4KV3kZZt8FYb1I9OmBw/0?wx_fmt=png&quot; data-ratio=&quot;0.713&quot; data-type=&quot;png&quot; data-w=&quot;1000&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpNKIctvKJ5QUYGIibribA3R1XCRBRK3JOlKWGa4KV3kZZt8FYb1I9OmBw/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;幻灯片摘自 P. Chaudhari 和 S. Soatto 在 ICML 2018 发表的演讲——《High-dimensional Geometry and Dynamics of Stochastic Gradient Descent for Deep Networks》。他们展示了如何从离散系统过渡到 Fokker-Plank 方程所描述的连续系统。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;在统计物理学中，这种类型的方程描述了暴露在曳力 (使分布推移，即改变平均值) 和随机力 (使分布扩散，即增加方差) 下的粒子的演化。在 SGD 中，曳力由真实梯度建模，而随机力则对应算法的内在噪声。正如上面的幻灯片所示，扩散项与温度项 T = 1 /β= learning_rate /(2 * batch_size) 成正比，这再次显示了该比值的重要性！&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;__bg_gif&quot; data-ratio=&quot;0.65&quot; data-type=&quot;gif&quot; data-w=&quot;200&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_gif/ymzg67DoLHL0GAVyghIBeu2spTC1GJNplshR7jFFrYVEDRyqFgzPxth1ic3t5SlI8wx1x26CF7B0sgV3icyoJFLA/640?wx_fmt=gif&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Fokker-Planck 方程下分布的演化。它向左漂移，随时间扩散。图源：维基百科&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;通过这个框架，Chaudhari 和 Soatto 证明了我们的分布将单调地收敛于某个稳定的分布（从 KL 散度的意义来说）：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-backh=&quot;197&quot; data-backw=&quot;825&quot; data-before-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpkfAupiaScZjtY8P0KyUkcibmFr9S43kpPy4TXN6g91HwYPia6yFoIzZrg/0?wx_fmt=png&quot; data-oversubscription-url=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp77ic5V3v7dQJzqOWUGRpOic4htm7HNiaT0DHAZibLwkibyggphmBF6R0VDA/0?wx_fmt=jpeg&quot; data-ratio=&quot;0.23870220162224798&quot; data-type=&quot;jpeg&quot; data-w=&quot;863&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/ymzg67DoLHL0GAVyghIBeu2spTC1GJNp77ic5V3v7dQJzqOWUGRpOic4htm7HNiaT0DHAZibLwkibyggphmBF6R0VDA/640?wx_fmt=jpeg&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Pratik Chaudhari 和 Stefano Soatto 论文的一个主要定理，证明了分布的单调会收敛到稳定状态（在 KL 散度意义中）。第二个方程表明，使 F 最小化相当于最小化某个潜在的ϕ以及扩大熵的分布（温度 1 /β控制的权衡）。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;在上面的定理中有几个有趣的观点：&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;SGD 最小化的函数可以写成两项之和（Eq. 11）：潜在Φ和熵的分布。温度 1 /β控制这两项的权衡。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;潜在Φ只取决于数据和网络的架构（而非优化过程）。如果它等于损失函数，SGD 将收敛到全局最小值。然而, 本文表明这种情况比较少见。而如果知道Φ与损失函数的距离，你将可以知道 SGD 收敛的概率。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;最终分布的熵取决于 learning_rate/batch_size（温度）的比例。直观上看，熵与分布的大小有关，而高温会导致分布具有更大的方差，这意味着一个平坦的极小值。平坦极小值的泛化能力更好，这与高学习率和低 batch size 能得到更优最小值的经验是一致的。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;因此，将 SGD 看作是一个随时间变化的分布表明，在收敛性和泛化方面，learning_rate/batch_size 比每个独立的超参数更有意义。此外，它还引入了与收敛相关的网络潜力，为架构搜索提供了一个很好的度量。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;&lt;strong&gt; 结论 &lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;探索深度学习理论的过程可以分为两部分：首先，通过简单的模型和实验，建立起关于深度学习理论如何及其为什么起作用的认知，然后将这些理念以数学形式呈现，以帮助我们解释当前的结论并得到新的结果。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;在第一篇文章中，我们试图传达更多关于神经网络高维损失函数和 SGD 解说的直观认知，同时表明新的形式主义正在建立，目的是建立一个关于深层神经网络优化的真正数学理论。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;&gt;&lt;/ins&gt; &lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;然而，虽然非凸优化是深度学习的基石并且拥有大量的层数和参数，但它取得的成功大部分源于其优秀的泛化能力。这将是下一篇文章将分享的内容。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; normal=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-ratio=&quot;1.34&quot; data-type=&quot;png&quot; data-w=&quot;300&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/ymzg67DoLHL0GAVyghIBeu2spTC1GJNpU34THcyJPz4XtAkBib0gUCJG9eB5pnHm9fHK3KzANrXenNeBLNslY4w/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; justify=&quot;&quot; em=&quot;&quot; normal=&quot;&quot; auto=&quot;&quot; rgb=&quot;&quot; line-height:=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;Sanjeev Arora：印度裔美国理论计算机科学家，他以研究概率可检验证明，尤其是PCP定理而闻名。研究兴趣包括计算复杂度理论、计算随机性、概率可检验证明等。他于2018年2月被推选为美国国家科学院院士，目前是普林斯顿大学计算机科学系教授。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;∑编辑 | Gemini&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;来源 | 数盟&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;img class=&quot;&quot; data-copyright=&quot;0&quot; data-ratio=&quot;1.0437601296596435&quot; data-s=&quot;300,640&quot; data-type=&quot;png&quot; data-w=&quot;617&quot; width=&quot;auto&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabky7x6u1VxMVMia4MLibNzC2nrumY3zDflTsCeoM04M1BrkvPny8tsw6hYkIicUr42iarLmadL2x6JwV6A/640?wx_fmt=png&quot; /&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域，经采用我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p helvetica=&quot;&quot; neue=&quot;&quot; pingfang=&quot;&quot; sc=&quot;&quot; hiragino=&quot;&quot; sans=&quot;&quot; gb=&quot;&quot; microsoft=&quot;&quot; yahei=&quot;&quot; ui=&quot;&quot; arial=&quot;&quot; sans-serif=&quot;&quot; px=&quot;&quot; normal=&quot;&quot; center=&quot;&quot; border-box=&quot;&quot; important=&quot;&quot; break-word=&quot;&quot;&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 10 Sep 2018 04:49:06 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/tG8EBnPeSk</dc:identifier>
</item>
</channel>
</rss>