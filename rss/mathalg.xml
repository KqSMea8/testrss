<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=feed43.com%2Fmathalg-jtks.xml&amp;max=5&amp;links=preserve&amp;exc=1" />
<atom:link rel="alternate" title="Source URL" href="http://feed43.com/mathalg-jtks.xml" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dfeed43.com%252Fmathalg-jtks.xml%26max%3D5%26links%3Dpreserve%26exc%3D1" />
<title>算法与数学之美</title>
<link>http://www.jintiankansha.me/column/c9dZ5TM2aS</link>
<description>算法与数学之美 - 今天看啥</description>
<ttl>360</ttl>
<item>
<title>计算数学家石钟慈：于磅礴中上下求索</title>
<link>http://www.jintiankansha.me/t/9h8TYQeRcH</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/9h8TYQeRcH</guid>
<description>&lt;p&gt;&lt;span&gt;计算数学需要数学模型、算法，最后是计算机实现，而直到1958年，中国在苏联的帮助下真正有了自己开发的计算机。1960年石钟慈回国，并从事计算数学的研究及教学工作。&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;石钟慈从苏联留学回来后，就研究起了“有限元方法”——包括飞行器、火箭、宇宙飞船、造房子、汽车等领域，居然都能派上用场。此后很多年，石钟慈就一直跟“有限元方法”较上了劲。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;石钟慈希望自己能够解决一个真正的数学难题，而原先西方的有限元方法在理论方面还比较欠缺，用一般的物理、工程等方面虽然已经足够，但作为纯数学还不够严密。于是，他在“有限元方法”中找到了空间，并决心好好干一场。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;20世纪60年代，毛泽东大手一挥，发出指示：一定要把淮河治好。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;石钟慈当时正好赶上这事，就加入了由冯康先生领导的研究团体，搞大坝的数据研究。他们所掌握的方法，与西方20世纪50年代的有限元方法十分接近。通过冯康等数学家们的不懈努力，到20世纪70年代，西方科技界承认中国是有限元理论的第一个国家。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;1960年至1966年在中国科技大学任教期间，石钟慈被分至计算方法研究室三室二组，专攻水坝设计。当时的负责人是冯康先生。按照石钟慈的说法，如果说是华罗庚先生为他指明了计算数学这条未来的研究道路，那么冯康先生就在具体实践的层面上教会了自己如何搞计算数学。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;作为从苏联正式回来的计算数学专业的人，因计算数学人才紧缺，经冯康先生安排，石钟慈正式调入中国科技大学任教，负责这个新兴专业的建设。当时华罗庚先生任中国科技大学数学系主任，冯康、吴文俊任副主任，石钟慈任计算方法研究室副主任。石钟慈从此开始了他的教学生涯，从最初的编写教材、讲课、上机和带大学生的毕业论文，到指导硕士生、博士生。当时的同事有罗晓沛、李家楷等人。当时中国科技大学本科生是五年制，最后一年教学的内容相当于研究生水平。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这6年的时间，石钟慈在《数学学报》发表了四篇论文，被多次引用，即使在“文革”期间论文也被国外转载。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在“文化大革命”的十年，中国科技大学由“造反派”当家，一切围绕“阶级斗争”。华罗庚等人成了“反动学术权威”，石钟慈他们跟着一起挨批，不能回家。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;石钟慈痛心地说，在世界范围内，人家一直都在工作，我们却被迫停了下来。实际上还不止十年，从1960年开始到1980年，差不多20年时间，其中绝大部分时间都算是浪费掉了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;20世纪80年代，石钟慈在冯康先生的大力支持下才得以回到北京。之后，他对有限元方法进行了深入的回忆与思考。厚积而薄发，石钟慈凭其坚实的基础数学能力，在计算数学的理论和应用研究中取得多项创造性成果，取得了国际领先地位、既有深刻理论意义又紧密结合实际、对工程计算具有指导意义的独创性研究成果，促进了有限元方法的重大发展。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;1980年和1986年，石钟慈两度获得中国科学院自然科学奖，1987年获得国家自然科学奖，2000年又锦上添花地获得何梁何利科学技术进步奖，2003年获得华罗庚数学奖。1991年，他当选为中国科学院院士，那一年他58岁。&lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Wed, 11 Apr 2018 14:31:07 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/9h8TYQeRcH</dc:identifier>
</item>
<item>
<title>机器学习萌新必学的Top10算法</title>
<link>http://www.jintiankansha.me/t/5Wyl5sQMiu</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/5Wyl5sQMiu</guid>
<description>&lt;p&gt;&lt;span&gt; 在机器学习领域里，不存在一种万能的算法可以完美解决所有问题，尤其是像预测建模的监督学习里。&lt;/span&gt;&lt;/p&gt;
&lt;section data-id=&quot;89091&quot; data-tools=&quot;135&amp;#x7F16;&amp;#x8F91;&amp;#x5668;&quot;&gt;&lt;section&gt;&lt;section&gt;
&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;比方说，神经网络不见得比决策树好，同样反过来也不成立。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;最后的结果是有很多因素在起作用的，比方说数据集的大小以及组成。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;所以，针对你要解决的问题，最好是尝试多种不同的算法。并借一个测试集来评估不同算法之间的表现，最后选出一个结果最好的。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;当然，你要选适合解决你问题的算法来尝试。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;比方说，要打扫房子，你会用真空吸尘器，扫把，拖把；你绝对不会翻出一把铲子来开始挖坑，对吧。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;不过呢，对于所有预测建模的监督学习算法来说，还是有一些通用的底层原则的。&lt;/span&gt;&lt;/p&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;&lt;span&gt;机器学习算法，指的是要学习一个目标函数，能够尽可能地还原输入和输出之间的关系。&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span&gt;然后根据新的输入值X，来预测出输出值Y。精准地预测结果是机器学习建模的任务。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;So，Top10机器学习算法，了解一下。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;统计学与机器学习领域里研究最多的算法。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;做预测建模，最重要的是准确性（尽可能减小预测值和实际值的误差）。哪怕牺牲可解释性，也要尽可能提高准确性。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;为了达到这个目的，我们会从不同领域（包括统计学）参考或照搬算法。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;线性回归可用一条线表示输入值X和输出值Y之间的关系，这条线的斜率的值，也叫系数。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-ratio=&quot;0.5277078085642317&quot; data-type=&quot;png&quot; data-w=&quot;794&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkxAlHEYKL3uacmerV9IUIZficzqGbphfYHNUX31gqJaEJLKDibia9yBgRfianBfoh2GOhPo5o56Ibwiadw/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;比方说，y = B0 + B1*x&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;我们就可以根据X值来预测Y值。机器学习的任务就是找出系数B0和B1。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;从数据中建立线性回归的模型有不同的方法，比方说线性代数的最小二乘法、梯度下降优化。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;线性回归已经存在了200多年，相关研究已经很多了。用这个算法关键在于要尽可能地移除相似的变量以及清洗数据。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对算法萌新来说，是最简单的算法了。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这方法来自统计学领域，是一种可以用在二元分类问题上的方法。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;逻辑回归，和线性回归相似，都是要找出输入值的系数权重。不同的地方在于，对输出值的预测改成了逻辑函数。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;逻辑函数看起来像字母S，输出值的范围是0到1。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;把逻辑函数的输出值加一个处理规则，就能得到分类结果，非0即1。&lt;br/&gt;比方说，可以规定输入值小于0.5，那么输出值就是1。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-ratio=&quot;0.8237410071942446&quot; data-type=&quot;png&quot; data-w=&quot;556&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkxAlHEYKL3uacmerV9IUIZf5FVj703vyLhmJNmJ9MdKicXrjumScZGIGV6P5vlVdVlOmVJP4bg4VuA/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;
&lt;h6&gt;&lt;strong&gt;△&lt;/strong&gt; 逻辑回归&lt;/h6&gt;
&lt;p&gt;&lt;span&gt;这个算法还可以用来预测数据分布的概率，适用于需要更多数据论证支撑的预测。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;和线性回归相似，如果把和输出不相干的因子或者相近的因子剔除掉的话，逻辑回归算法的表现会更好。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对于二元分类问题，逻辑回归是个可快速上手又有效的算法。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;逻辑回归算法，只能用于二分问题。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;当输出的结果类别超过两类的时候，就要用线性判别分析算法了。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;这种算法的可视化结果还比较一目了然，能看出数据在统计学上的特征。这上面的结果都是分别计算得到的，单一的输入值可以是每一类的中位数，也可以是每一类值的跨度。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-ratio=&quot;0.7776096822995462&quot; data-type=&quot;png&quot; data-w=&quot;661&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkxAlHEYKL3uacmerV9IUIZfQw9Vib1ia8o84WXh4kZq1KVaY5wotN8u4QANNmXOq3dRhO7vj8OMyhKw/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;
&lt;h6&gt;&lt;strong&gt;△&lt;/strong&gt; 线性判别分析&lt;/h6&gt;
&lt;p&gt;&lt;span&gt;基于对每种类别计算之后所得到的判别值，取最大值做出预测。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这种方法是假定数据符合高斯分布。所以，最好在预测之前把异常值先踢掉。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对于分类预测问题来说，这种算法既简单又给力。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;预测模型里，决策树也是非常重要的一种算法。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;可以用分两叉的树来表示决策树的模型。每一个节点代表一个输入，每个分支代表一个变量（默认变量是数字类型）&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-ratio=&quot;0.5464824120603015&quot; data-type=&quot;png&quot; data-w=&quot;796&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkxAlHEYKL3uacmerV9IUIZfZ5Ae99z6yQJFjmWZHiaPcBPNVsicGaQZxOm1n68Wibtnn9kMfM6gVNFyA/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;
&lt;h6&gt;&lt;strong&gt;△&lt;/strong&gt; 决策树&lt;/h6&gt;
&lt;p&gt;&lt;span&gt;决策树的叶节点指的是输出变量。预测的过程会经过决策树的分岔口，直到最后停在了一个叶节点上，对应的就是输出值的分类结果。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;决策树很好学，也能很快地得到预测结果。对于大部分问题来说，得到的结果还挺准确，也不要求对数据进行预处理。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这种预测建模的算法强大到超乎想象。&lt;br/&gt;这种模型，可以直接从你的训练集中计算出来两种输出类别的概率。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;一个是每种输出种类的概率；另外一个，是根据给定的x值，得到的是有条件的种类概率。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;一旦计算之后，概率的模型可以用贝叶斯定理预测新的数据。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;当你的数据是实数值，那么按理说应该是符合高斯分布的，也就很容易估算出这个概率。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img width=&quot;776&quot; height=&quot;476&quot; data-ratio=&quot;0.61375&quot; data-type=&quot;png&quot; data-w=&quot;800&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/YicUhk5aAGtBu1KvjWeMvfdAobXEDFqGnOGng7fDJCAIToR6jficAexhYo13IBAwBGKeAGvoxCWOqib9Wpx8xRqibw/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;h6&gt;&lt;strong&gt;△&lt;/strong&gt; 贝叶斯定理&lt;/h6&gt;
&lt;p&gt;&lt;span&gt;朴素贝叶斯定理之所以名字里有个“朴素”，是因为这种算法假定每个输入的变量都是独立的。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;不过，真实的数据不可能满足这个隐藏前提。尽管如此，这个方法对很多复杂的问题还是很管用的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;最近K近邻的模型表示，就是整个训练集。很直截了当，对吧？&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;对新数据的预测，是搜索整个训练集的值，找到K个最近的例子（literally的邻居）。然后总结K个输出的变量。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这种算法难就难在，怎么定义两个数据的相似度（相距多近算相似）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;如果你的特征(attributes)属于同一个尺度的话，那最简单的方法是用欧几里得距离。这个数值，你可以基于每个输入变量之间的距离来计算得出。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-ratio=&quot;0.7503649635036497&quot; data-type=&quot;png&quot; data-w=&quot;685&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkxAlHEYKL3uacmerV9IUIZftQMMLQCsficrIZFlIIOJuBXAndvRrlaJnz7HMRyZruNIQRt65LsNPsg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;
&lt;h6&gt;&lt;strong&gt;△&lt;/strong&gt; 最近邻法&lt;/h6&gt;
&lt;p&gt;&lt;span&gt;最近邻法，需要占用大量的内存空间来放数据，这样在需要预测的时候就可以进行即时运算（或学习）。也可以不断更新训练集，使得预测更加准确。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;距离或亲密度这个思路遇到更高维度（大量的输入变量）就行不通了，会影响算法的表现。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这叫做维度的诅咒。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;当（数学）空间维度增加时，分析和组织高维空间（通常有成百上千维），因体积指数增加而遇到各种问题场景。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;所以最好只保留那些和输出值有关的输入变量。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;最近邻法的缺点是，你需要整个训练集。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;而学习矢量量化（后简称LVQ）这个神经网络算法，是自行选择训练样例。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-ratio=&quot;0.4434673366834171&quot; data-type=&quot;png&quot; data-w=&quot;796&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkxAlHEYKL3uacmerV9IUIZfBnE12n14UdQ3hygdUXYR4XiaFXWWwUkwCzVTHYVJ92wqJN519aQg9Rw/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;LVQ，是一组矢量，也叫码本。一开始，矢量是随机选的，经过几次学习算法迭代之后，慢慢选出最能代表训练集的矢量。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;学习完成后，码本就可以用来预测了，就像最近邻法那样。计算新输入样例和码本的距离，可以找出最相近的邻居，也就是最匹配的码本。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;如果你重新调整数据尺度，把数据归到同一个范围里，比如说0到1之间，那就可以获得最好的结果。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;如果用最近邻法就获得了不错的结果，那么可以再用LVQ优化一下，减轻训练集储存压力。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这可能是机器学习里最受欢迎的算法了。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;超平面是一条可以分割输入变量的空间的“线”。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;支持向量机的超平面，是能把输入变量空间尽可能理想地按种类切割，要么是0，要么是1。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;在二维空间里，你可以把超平面可以分割变量空间的那条“线”。这条线能把所有的输入值完美一分为二。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;SVM的学习目标就是要找出这个超平面。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-ratio=&quot;0.5784313725490197&quot; data-type=&quot;png&quot; data-w=&quot;612&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkxAlHEYKL3uacmerV9IUIZfJFR8Vibpuia1wvtVH2L8Ihh9ro8oW05eCU6eInbjHugoiclWxf1IQjGsw/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;
&lt;h6&gt;&lt;strong&gt;△&lt;/strong&gt; 支持矢量机&lt;/h6&gt;
&lt;p&gt;&lt;span&gt;超平面和挨得最近的数据点之间的距离，叫做边缘。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;最理想的超平面，是可以无误差地划分训练数据。 也就是说，每一类数据里距离超平面最近的向量与超平面之间的距离达到最大值。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;这些点就叫做支持向量，他们定义了超平面。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;从实际操作上，最理想的算法是能找到这些把最近矢量与超平面值距离最大化的点。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;支持向量可能是最强的拿来就用的分类器了。值得用数据集试试。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;随机森林，属于一种重复抽样算法，是最受欢迎也最强大的算法之一。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;在统计学里，bootstrap是个估算值大小很有效的方法。比方说估算平均值。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;span&gt;从数据库中取一些样本，计算平均值，重复几次这样的操作，获得多个平均值。然后平均这几个平均值，希望能得到最接近真实的平均值。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;而bagging算法，是每次取多个样本，然后基于这些样本建模。当要预测新数据的时候，之前建的这些模型都做次预测，最后取这些预测值的平均数，尽力接近真实的输出值。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-ratio=&quot;0.6073446327683616&quot; data-type=&quot;png&quot; data-w=&quot;708&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkxAlHEYKL3uacmerV9IUIZfvNb5cu9WrsC2GPOaSU9LYAnsZABYXXvs7XoS1wHGibYCm1xHyOAP1Xg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;随机森林在这个基础上稍微有点变化。&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;它包含多个决策树的分类器， 并且其输出的类别是由个别树输出的类别的众数而定。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;如果你的高方差算法取得了不错的结果（比方说决策树），那么用随机森林的话会进一步拿到更好的结果。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Boosting的核心是，对同一个训练集训练不同的分类器（弱分类器），然后把这些弱分类器集合起来，构成一个更强的最终分类器（强分类器）。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;所得到的第二个弱分类器会纠正第一个弱分类器的误差。弱分类器的不断叠加，直到预测结果完美为止。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Adaboost算法是首个成功用于二元分类问题的提升算法。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;现在有很多提升方法都是基于Adaboost。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img data-ratio=&quot;0.564945226917058&quot; data-type=&quot;png&quot; data-w=&quot;639&quot; data-s=&quot;300,640&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkxAlHEYKL3uacmerV9IUIZfbAx0rl0st61PVkGofO4yTQiaB1Jb7Ot4pE0sicOWV3xU8ZPJztZbqvKg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;AdaBoost适用于短的决策树。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;在第一个树建立出来之后，不同的样本训练之后的表现可以作参考，用不同的样本训练弱分类器，然后根据错误率给样本一个权重。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;很难预测的训练数据应该给更多的权重，反过来，好预测的就少一点权重。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;模型按顺序一个一个建，每个训练数据的权重都会受到上一个决策树表现的影响。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;当所有的决策树都建好之后，看新数据的预测表现，结果准不准。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;因为训练数据对于矫正算法非常重要，所以要确保数据清洗干净了，不要有奇奇怪怪的偏离值。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;面对海量的机器学习算法，萌新最爱问的是，“我该选什么算法？”&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;p&gt;&lt;span&gt;在回答这个问题之前，要先想清楚：&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;1）数据的数量、质量、本质；&lt;br/&gt;2）可供计算的时间；&lt;br/&gt;3）这个任务的紧急程度；&lt;br/&gt;4）你用这个数据想做什么。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;要知道，即使是老司机，也无法闭着眼睛说哪个算法能拿到最好的结果。还是得动手试。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;其实机器学习的算法很多的，以上只是介绍用得比较多的类型，比较适合萌新试试手找找感觉。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;最后，附编译来源，&lt;br/&gt;https://www.kdnuggets.com/2018/02/tour-top-10-algorithms-machine-learning-newbies.html&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;∑编辑 | Gemini&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;来源 | 量子位&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img width=&quot;auto&quot; data-ratio=&quot;0.7509529860228716&quot; data-type=&quot;jpeg&quot; data-w=&quot;787&quot; data-s=&quot;300,640&quot; data-copyright=&quot;0&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkz1SRWmm2kJgtV2NTQtdSgtyl7nJbJm8xS78Td2LBbJQKKqyE54oaOO9upMribZagMKYJVBaEDyKtA/640?wx_fmt=jpeg&quot;/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;算法数学之美微信公众号欢迎赐稿&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;稿件涉及数学、物理、算法、计算机、编程等相关领域&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;经采用我们将奉上稿酬。&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;投稿邮箱：math_alg@163.com&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Tue, 10 Apr 2018 19:37:23 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/5Wyl5sQMiu</dc:identifier>
</item>
<item>
<title>读书新方式，给生活找点乐子</title>
<link>http://www.jintiankansha.me/t/tezaksJbLg</link>
<guid isPermaLink="true" >http://www.jintiankansha.me/t/tezaksJbLg</guid>
<description>&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;-01-&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;小时候看蜡笔小新，觉得活成他爸爸那样，真可伶。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;三十二年的房贷、挨不完的暴揍、加不完的晚班……日子过得苦兮兮。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;然而，这位大叔成天就知傻笑，坦然得很。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;工作不顺，他就想想身边老婆儿子，想想今晚看场球赛。心情不爽，只要手边有杯冰啤酒，烦恼就咕咚咕咚灌下去。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img title=&quot;4dd84a9b446fe6ebea2639d141d12f9a.jpg&quot; data-ratio=&quot;0.546875&quot; data-type=&quot;jpeg&quot; data-w=&quot;512&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkwsgIiatzYRNQMa6xic921phrNFQDahPdbKG614sbtAzt1AVu72iahfUp3clS1o78fvZiaLcN931Fq1cA/640?wx_fmt=jpeg&quot; data-backw=&quot;512&quot; data-backh=&quot;280&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;现在回想，小新爸爸很厉害呀。那种 “生命要浪费在美好事物上”的人生哲学，他玩儿得很&lt;/span&gt;&lt;span&gt;溜。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;社会日益浮躁，功利主义渗透到每个角落，很多人过得不好，并不是因为没钱，而是因为他们没有精气神，没有恰当的审美，不讲究生活情趣，越来越追求实用化的背后，就是越来越平庸，越来越枯萎。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;梁文道说：“&lt;strong&gt;读一些无用的书，做一些无用的事，花一些无用的时间&lt;/strong&gt;，都是为了在一切已知之外，保留一个超越自己的机会，人生中一些很了不起的变化，就是来自这种时刻。”&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;span&gt;&lt;strong&gt;&lt;span&gt;-02-&lt;/span&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;看《花少》的时候，特别喜欢许晴，感觉她整个人都是鲜活的。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;这档节目主打穷游，所有人都在为钱困扰，每天都在那“钱钱钱”、“省省省”，过得很是粗糙。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在离开的前一天，许晴没有跟团队活动，在街上走了3个多小时去当地最有名的菜场买了食材。一路上又买了蜡烛，她还想买桌布，可是找了很多地方都没有买到，最后到一个头饰店买了各种颜色的丝巾。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;晚上，当花少团回来时，就看见天台上摆放着一桌美食，烛光在四周摇曳，餐座和椅背上铺就了各色丝巾。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;风尘仆仆的一群人，这一刻在五颜六色如梦一般的气氛中，就着异国他乡的夜景，摇晃起了红酒杯。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;那一晚，凯丽回忆，是她那段旅行中最惬意的时光。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;懂得生活的人就是这样，一碗稀粥也能喝出玫瑰的气息。&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 

&lt;p&gt;&lt;strong&gt;&lt;span&gt;-03-&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;美好的东西能抵抗生活中的沮丧和困顿，一个人专注于审美、讲究生活趣味的过程，就是纳悦自己、滋养身心、重获希望的过程。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;被自己的老师沈从文夸“可爱”的小老头汪曾祺，有段时间，被扔到了马铃薯研究站，远在沽源。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;一个文学大师画土豆，像个什么样子？但他倒好，埋头画花和薯块，画完了，就丢在牛粪火里烤熟吃掉。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;还一度很骄傲：&lt;strong&gt;“我敢说，像我一样吃过这么多品种马铃薯的，全国盖无第二人！”&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-ratio=&quot;0.685&quot; data-type=&quot;jpeg&quot; data-w=&quot;400&quot; data-s=&quot;300,640&quot; data-copyright=&quot;0&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkwsgIiatzYRNQMa6xic921phrIcoVVH5F7EjCFS3nnOIWvq2qmJgs9FPrh2icYFANsMIVyAYQ8E0Ddpw/640?wx_fmt=jpeg&quot; data-backw=&quot;400&quot; data-backh=&quot;274&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;《随遇而安》中，他更是写道， “烤土豆的这段经历，真是三生有幸。要不然我这一生就更加平淡了。”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;林语堂曾言：“我们最重要的不是去计较真与伪，得与失，名与利，贵与贱，富与贫，而是如何好好地快乐度日，并从中发现生活的诗意。”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;一个人，光活着是不够的，他还应该拥有诗意的世界。&lt;/span&gt;&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;&lt;span&gt;-04-&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;讲究生活情趣，将生活过得入木三分、活色生香是一种能力。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;在《欢乐颂》里，赵医生告诉曲筱绡他喜欢交往有趣的人，曲筱绡一直不明白他所说的有趣是什么意思。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;赵医生是医学博士，能欣赏王小波的爱情小说，也能看日本的原版黄暴漫画。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;曲筱绡要带他去吃法式大餐，他说宁愿在家看东野圭吾，沉浸在小说中未曾体验过的生活中。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img title=&quot;8a6a55796acb538367c7fc4bbf6c9db9.jpg&quot; data-ratio=&quot;0.6592592592592592&quot; data-type=&quot;jpeg&quot; data-w=&quot;540&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkwsgIiatzYRNQMa6xic921phrEEKtcrkVIZcGvQiaLiaorLooZ6CZFxtq0uT5tIk3wPssaCaxpIiavMeicg/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;说白了，精致、诗意的生活背后，其实就是一股子热爱，就像&lt;span&gt;汪曾祺说的：人活着，一定要爱点什么。&lt;span&gt;这种能力能&lt;/span&gt;&lt;/span&gt;&lt;strong&gt;把普通的日子折腾地会发光&lt;/strong&gt;。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;经历过人世复杂的汪曾祺，却一直天真得像个孩子。平日里，他酒一喝多，就吹大发：喂喂，你们对我客气点，我将来是要进文学史的。汪家人一个白眼扫过去。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;鹦鹉史航说：这世间可爱的老头儿很多，但可爱成汪曾祺这样的，却不常见。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;他写家乡的咸鸭蛋，“高邮咸蛋质细而油多，蛋白柔嫩，不似别处的发干、发粉，入口如嚼石灰。油多尤为别处所不及。筷子头一扎下去，吱——红油就冒出来了。”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;他写自创的塞馅回锅油条，“猪肉馅要肥瘦各半，馅中加盐、葱花、姜末，如加少量榨菜末或酱瓜末、川冬菜末，亦可。&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; 
&lt;p&gt;&lt;span&gt;拌好的肉馅塞入油条中，逐段下油锅炸至油条挺硬，此菜嚼之酥脆，油条中有矾，略有涩味，比炸春卷味道好。”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;img data-ratio=&quot;0.5626666666666666&quot; data-type=&quot;jpeg&quot; data-w=&quot;750&quot; data-s=&quot;300,640&quot; data-copyright=&quot;0&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkwsgIiatzYRNQMa6xic921phrdv0CMx7iaic63ic0wE4FDVibhoM3ygdiciaOguZRRlIePlqdRc65aNm3xrHQ/640?wx_fmt=jpeg&quot;/&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;最常见的栀子花在他笔下，都变得“张牙舞爪”：“栀子花粗粗大大，又香得掸都掸不开，于是为文雅人不取，以为品格不高。栀子花说：&lt;strong&gt;‘去你妈的，我就是要这样香，香得痛痛快快，你们他妈的管得着吗！’&lt;/strong&gt;”&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;什么是有趣？有趣就是在最普通寻常的日子里活出甜味、活出雅致、活出清欢。有趣才是一个人最高级的性感。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;对于懂得生活的人，生活中的这种小事才是最接近生活的本真。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;就像汪涵读书、研究方言，活成了“烟火神仙”；陈道明为女儿捏起糖人、面人，给爱人做手工包，总做“无用”之事；讲书人关熙潮繁忙生活之余拿起相机，捕捉生活的美好瞬间。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;img title=&quot;1808016702.jpg&quot; data-ratio=&quot;0.6555555555555556&quot; data-type=&quot;jpeg&quot; data-w=&quot;1080&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_jpg/951TjTgiabkwsgIiatzYRNQMa6xic921phrDOCKS68tIYLDKkM0oLIWoxTgIneoamXBxQFgaXuxHbWMlFGo6IibibHg/640?wx_fmt=jpeg&quot; data-backw=&quot;526&quot; data-backh=&quot;345&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;关熙潮/摄&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;很多人总是豪气冲天、幻想着策马扬鞭奔去远方，可是只有认真踏实地过好了眼前的小日子，才能像汪老头这样“活进文学史”里，活进自己的生活里。&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;span&gt;慢点走，品品茶，喝喝酒，听听曲，写写字。&lt;/span&gt;&lt;span&gt;——汪曾祺&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;hr/&gt;
&lt;p&gt;&lt;img title=&quot;&quot; data-ratio=&quot;1.744343891402715&quot; data-type=&quot;png&quot; data-w=&quot;442&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkwsgIiatzYRNQMa6xic921phrcvQREccP6vKOWRurfGYlPFTjMrPQLlSey2cuEeMcDvQc5l0rzadRiaw/640?wx_fmt=png&quot;/&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img title=&quot;&quot; data-ratio=&quot;0.48654708520179374&quot; data-type=&quot;png&quot; data-w=&quot;446&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkwsgIiatzYRNQMa6xic921phrHRrTMSyxkPS9KA3YtqTDNOror6LgwllHia1YYbjMNb5vLpHZHLnQQnQ/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img title=&quot;&quot; data-ratio=&quot;0.9040178571428571&quot; data-type=&quot;png&quot; data-w=&quot;448&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkwsgIiatzYRNQMa6xic921phrlUOySfAqico3ic9gC3k72FFts4J5WnGtNjZsrrnlprJtIKBTiblMHpTXg/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;读书新方式&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;影视片段+小动画&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;讲书类型覆盖14个大类&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;每天8分钟，&lt;/span&gt;&lt;span&gt;看大咖讲书&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;限时特惠 &lt;span&gt;39.9&lt;/span&gt;元/月&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;原价&lt;/span&gt;&lt;span&gt;69&lt;/span&gt;&lt;span&gt;元&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;一杯下午茶的消费，180本书畅享&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;读更多书，让自己的生活更有品质&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;长按下方二维码立即了解课程详情&lt;/span&gt;&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-1837452791782084&quot; data-ad-slot=&quot;7041996284&quot;/&gt; &lt;section class=&quot;RankEditor&quot; sans=&quot;sans&quot; pingfang=&quot;pingfang&quot; center=&quot;center&quot; rgb=&quot;rgb&quot; normal=&quot;normal&quot; px=&quot;px&quot; sans-serif=&quot;sans-serif&quot; e=&quot;e&quot; stheiti=&quot;stheiti&quot; fae=&quot;fae&quot; yahei=&quot;yahei&quot; microsoft=&quot;microsoft&quot; gb=&quot;gb&quot; hiragino=&quot;hiragino&quot; arial=&quot;arial&quot; helvetica=&quot;helvetica&quot; neue=&quot;neue&quot; sc=&quot;sc&quot;&gt;&lt;section&gt;&lt;section&gt;&lt;p&gt;&lt;img data-ratio=&quot;1&quot; data-type=&quot;png&quot; data-w=&quot;272&quot; data-src=&quot;http://img2.jintiankansha.me/get?src=http://mmbiz.qpic.cn/mmbiz_png/951TjTgiabkwsgIiatzYRNQMa6xic921phrqC7rbv2aRnp8MpnzZgCrLTBn0yb9wGWDvTH2voZT9ps6HFWic3Ficwnw/640?wx_fmt=png&quot;/&gt;&lt;/p&gt;

&lt;/section&gt;&lt;/section&gt;&lt;/section&gt;&lt;p&gt;&lt;span&gt;↓点击“阅读原文”，给生活找点乐子。&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
</description>
<pubDate>Tue, 10 Apr 2018 19:37:22 +0000</pubDate>
<dc:language>zh-CN</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.jintiankansha.me/t/tezaksJbLg</dc:identifier>
</item>
</channel>
</rss>