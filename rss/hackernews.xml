<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>The Matrix Calculus You Need for Deep Learning</title>
<link>http://explained.ai/matrix-calculus/index.html</link>
<guid isPermaLink="true" >http://explained.ai/matrix-calculus/index.html</guid>
<description>&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot; /&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400&quot; /&gt;&lt;title&gt;The matrix calculus you need for deep learning&lt;/title&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;1031.8839785493&quot;&gt;



&lt;p&gt;&lt;a href=&quot;http://parrt.cs.usfca.edu&quot;&gt;Terence Parr&lt;/a&gt; and &lt;a href=&quot;http://www.fast.ai/about/#jeremy&quot;&gt;Jeremy Howard&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(We teach in University of San Francisco's &lt;a href=&quot;https://www.usfca.edu/arts-sciences/graduate-programs/data-science&quot;&gt;MS in Data Science program&lt;/a&gt; and have other nefarious projects underway. You might know Terence as the creator of the &lt;a href=&quot;http://www.antlr.org&quot;&gt;ANTLR parser generator&lt;/a&gt;. For more material, see Jeremy's &lt;a href=&quot;http://course.fast.ai&quot;&gt;fast.ai courses&lt;/a&gt; and University of San Francisco's Data Institute &lt;a href=&quot;https://www.usfca.edu/data-institute/certificates/deep-learning-part-one&quot;&gt;in-person version of the deep learning course&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.01528&quot; onclick=&quot;gtag('event', 'download', { 'video_title': 'PDF version', 'non_interaction': true });&quot;&gt;Printable version&lt;/a&gt; (This HTML was generated from markup using &lt;a href=&quot;https://github.com/parrt/bookish&quot; onclick=&quot;gtag('event', 'bookish', { 'video_title': 'bookish', 'non_interaction': true });&quot;&gt;bookish&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do &lt;strong&gt;not&lt;/strong&gt; need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don't worry if you get stuck at some point along the way---just go back and reread the previous section, and try writing down and working through some examples. And if you're still stuck, we're happy to answer your questions in the &lt;a href=&quot;http://forums.fast.ai/c/theory&quot;&gt;Theory category at forums.fast.ai&lt;/a&gt;. &lt;strong&gt;Note&lt;/strong&gt;: There is a &lt;a href=&quot;http://explained.ai/matrix-calculus/index.html#reference&quot;&gt;reference section&lt;/a&gt; at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here.&lt;/p&gt;

&lt;h2 id=&quot;intro&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Most of us last saw calculus in school, but derivatives are a critical part of machine learning, particularly deep neural networks, which are trained by optimizing a loss function. Pick up a machine learning paper or the documentation of a library such as &lt;a href=&quot;http://pytorch.org&quot;&gt;PyTorch&lt;/a&gt; and calculus comes screeching back into your life like distant relatives around the holidays. And it's not just any old scalar calculus that pops up---you need differential &lt;em&gt;matrix calculus&lt;/em&gt;, the shotgun wedding of &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_algebra&quot;&gt;linear algebra&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariable_calculus&quot;&gt;multivariate calculus&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Well... maybe &lt;em&gt;need&lt;/em&gt; isn't the right word; Jeremy's courses show how to become a world-class deep learning practitioner with only a minimal level of scalar calculus, thanks to leveraging the automatic differentiation built in to modern deep learning libraries. But if you really want to really understand what's going on under the hood of these libraries, and grok academic papers discussing the latest advances in model training techniques, you'll need to understand certain bits of the field of matrix calculus.&lt;/p&gt;
&lt;p&gt;For example, the activation of a single computation unit in a neural network is typically calculated using the dot product (from linear algebra) of an edge weight vector &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; with an input vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; plus a scalar bias (threshold): &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-EEDCFA4252D0992243A283CE0EB777A6-depth003.31.svg&quot; /&gt;. Function &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-C599D931407509E0FA08F8686B205B6D-depth003.25.svg&quot; /&gt; is called the unit's &lt;em&gt;affine function&lt;/em&gt; and is followed by a &lt;a href=&quot;https://goo.gl/7BXceK&quot;&gt;rectified linear unit&lt;/a&gt;, which clips negative values to zero: &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-66258AA93A4746DA10D306190271DE4B-depth003.25.svg&quot; /&gt;. Such a computational unit is sometimes referred to as an “artificial neuron” and looks like:&lt;/p&gt;
&lt;center&gt;
&lt;p&gt;&lt;img src=&quot;http://explained.ai/matrix-calculus/images/neuron.png&quot; alt=&quot;neuron.png&quot; width=&quot;250&quot; /&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;p&gt;Neural networks consist of many of these units, organized into multiple collections of neurons called &lt;em&gt;layers&lt;/em&gt;. The activation of one layer's units become the input to the next layer's units. The activation of the unit or units in the final layer is called the network output.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Training&lt;/em&gt; this neuron means choosing weights &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; and bias &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt; so that we get the desired output for all &lt;span class=&quot;eqn&quot;&gt;N&lt;/span&gt; inputs &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;. To do that, we minimize a &lt;em&gt;loss function&lt;/em&gt; that compares the network's final &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-7971D42A6C6C6A28D6443F0645E4A036-depth003.25.svg&quot; /&gt; with the &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-E146B831A1E53B95E4C63775285D62CF-depth003.25.svg&quot; /&gt; (desired output of &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;) for all input &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; vectors. To minimize the loss, we use some variation on gradient descent, such as plain &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;stochastic gradient descent&lt;/a&gt; (SGD), SGD with momentum, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam&quot;&gt;Adam&lt;/a&gt;. All of those require the partial derivative (the gradient) of &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-7971D42A6C6C6A28D6443F0645E4A036-depth003.25.svg&quot; /&gt; with respect to the model parameters &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt;. Our goal is to gradually tweak &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt; so that the overall loss function keeps getting smaller across all &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; inputs.&lt;/p&gt;
&lt;p&gt;If we're careful, we can derive the gradient by differentiating the scalar version of a common loss function (mean squared error):&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-A129949CD1EF7BE2CA8BD424D34F9930.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;But this is just one neuron, and neural networks must train the weights and biases of all neurons in all layers simultaneously. Because there are multiple inputs and (potentially) multiple network outputs, we really need general rules for the derivative of a function with respect to a vector and even rules for the derivative of a vector-valued function with respect to a vector.&lt;/p&gt;
&lt;p&gt;This article walks through the derivation of some important rules for computing partial derivatives with respect to vectors, particularly those useful for training neural networks. This field is known as &lt;em&gt;matrix calculus&lt;/em&gt;, and the good news is, we only need a small subset of that field, which we introduce here. While there is a lot of online material on multivariate calculus and linear algebra, they are typically taught as two separate undergraduate courses so most material treats them in isolation. The pages that do discuss matrix calculus often are really just lists of rules with minimal explanation or are just pieces of the story. They also tend to be quite obscure to all but a narrow audience of mathematicians, thanks to their use of dense notation and minimal discussion of foundational concepts. (See the annotated list of resources at the end.)&lt;/p&gt;
&lt;p&gt;In contrast, we're going to rederive and rediscover some key matrix calculus rules in an effort to explain them. It turns out that matrix calculus is really not that hard! There aren't dozens of new rules to learn; just a couple of key concepts. Our hope is that this short paper will get you started quickly in the world of matrix calculus as it relates to training neural networks. We're assuming you're already familiar with the basics of neural network architecture and training. If you're not, head over to &lt;a href=&quot;http://course.fast.ai&quot;&gt;Jeremy's course&lt;/a&gt; and complete part 1 of that, then we'll see you back here when you're done. (Note that, unlike many more academic approaches, we strongly suggest &lt;em&gt;first&lt;/em&gt; learning to train and use neural networks in practice and &lt;em&gt;then&lt;/em&gt; study the underlying math. The math will be much more understandable with the context in place; besides, it's not necessary to grok all this calculus to become an effective practitioner.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A note on notation&lt;/em&gt;: Jeremy's course exclusively uses code, instead of math notation, to explain concepts since unfamiliar functions in code are easy to search for and experiment with. In this paper, we do the opposite: there is a lot of math notation because one of the goals of this paper is to help you understand the notation that you'll see in deep learning papers and books. At the &lt;a href=&quot;http://explained.ai/matrix-calculus/index.html#notation&quot;&gt;end of the paper&lt;/a&gt;, you'll find a brief table of the notation used, including a word or phrase you can use to search for more details.&lt;/p&gt;
&lt;h2 id=&quot;sec2&quot;&gt;Review: Scalar derivative rules&lt;/h2&gt;
&lt;p&gt;Hopefully you remember some of these main scalar derivative rules. If your memory is a bit fuzzy on this, have a look at &lt;a href=&quot;https://www.khanacademy.org/math/ap-calculus-ab/ab-derivative-rules&quot;&gt;Khan academy vid on scalar derivative rules&lt;/a&gt;.&lt;/p&gt;
&lt;center&gt;
&lt;/center&gt;
&lt;p&gt;There are other rules for trigonometry, exponentials, etc., which you can find at &lt;a href=&quot;https://www.khanacademy.org/math/differential-calculus&quot;&gt;Khan Academy differential calculus course&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When a function has a single parameter, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-50BBD36E1FD2333108437A2CA378BE62-depth003.25.svg&quot; /&gt;, you'll often see &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-6BDA8AF54C40BC23ED858E9E9F5C11D2-depth002.72.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-74CAF4D1EC90D3A36EA7C7BBFE65B516-depth003.25.svg&quot; /&gt; used as shorthands for &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-5BA9D16419154B1BDBECA39D99E8E809-depth004.58.svg&quot; /&gt;. We recommend against this notation as it does not make clear the variable we're taking the derivative with respect to.&lt;/p&gt;
&lt;p&gt;You can think of &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-624FEBB9A49A3FC96353C861D175C806-depth004.58.svg&quot; /&gt; as an operator that maps a function of one parameter to another function. That means that &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-EC0CEC5F9488EC510F8D688E7003222D-depth004.58.svg&quot; /&gt; maps &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-50BBD36E1FD2333108437A2CA378BE62-depth003.25.svg&quot; /&gt; to its derivative with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;, which is the same thing as &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-07A5EA519C4CEA1A3539E3A7FC289163-depth004.58.svg&quot; /&gt;. Also, if &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-FD91C508F91C2C84498680BD337C1D7A-depth003.25.svg&quot; /&gt;, then &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-B1ED3CF9BA4D6F25A5A4F481C45EC658-depth004.58.svg&quot; /&gt;. Thinking of the derivative as an operator helps to simplify complicated derivatives because the operator is distributive and lets us pull out constants. For example, in the following equation, we can pull out the constant 9 and distribute the derivative operator across the elements within the parentheses.&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-A1EC7F214318E08949CC8BFCED138D94.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;That procedure reduced the derivative of &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-FDFD125C741CD062B2CA779DDE0524BE-depth003.25.svg&quot; /&gt; to a bit of arithmetic and the derivatives of &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-32F5240D0DBF2CCBE75EF7F8EF2015E0-depth000.14.svg&quot; /&gt;, which are much easier to solve than the original derivative.&lt;/p&gt;
&lt;h2 id=&quot;sec3&quot;&gt;Introduction to vector calculus and partial derivatives&lt;/h2&gt;
&lt;p&gt;Neural network layers are not single functions of a single parameter, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-50BBD36E1FD2333108437A2CA378BE62-depth003.25.svg&quot; /&gt;. So, let's move on to functions of multiple parameters such as &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-3BAF1600AE50930A155F58AE172B51BD-depth003.25.svg&quot; /&gt;. For example, what is the derivative of &lt;span class=&quot;eqn&quot;&gt;xy&lt;/span&gt; (i.e., the multiplication of &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;)? In other words, how does the product &lt;span class=&quot;eqn&quot;&gt;xy&lt;/span&gt; change when we wiggle the variables? Well, it depends on whether we are changing &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; or &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;. We compute derivatives with respect to one variable (parameter) at a time, giving us two different &lt;em&gt;partial derivatives&lt;/em&gt; for this two-parameter function (one for &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; and one for &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;). Instead of using operator &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-624FEBB9A49A3FC96353C861D175C806-depth004.58.svg&quot; /&gt;, the partial derivative operator is &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-4D6C379F66675645B3FFE28A15306857-depth004.67.svg&quot; /&gt; (a stylized &lt;span class=&quot;eqn&quot;&gt;d&lt;/span&gt; and not the Greek letter &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-77A3B715842B45E440A5BEE15357AD29-depth000.22.svg&quot; /&gt;). So, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-08DCCFBE629A14FCCD9FB9A20F2E367C-depth004.67.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-1FDCF7A9F137AE48FA25EE34A69F8201-depth006.34.svg&quot; /&gt; are the partial derivatives of &lt;span class=&quot;eqn&quot;&gt;xy&lt;/span&gt;; often, these are just called the &lt;em&gt;partials&lt;/em&gt;. For functions of a single parameter, operator &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-4D6C379F66675645B3FFE28A15306857-depth004.67.svg&quot; /&gt; is equivalent to &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-624FEBB9A49A3FC96353C861D175C806-depth004.58.svg&quot; /&gt; (for sufficiently smooth functions). However, it's better to use &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-624FEBB9A49A3FC96353C861D175C806-depth004.58.svg&quot; /&gt; to make it clear you're referring to a scalar derivative.&lt;/p&gt;
&lt;p&gt;The partial derivative with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; is just the usual scalar derivative, simply treating any other variable in the equation as a constant. Consider function &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-D6DEAE7E403381C2C425D4B40CCA936E-depth003.25.svg&quot; /&gt;. The partial derivative with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; is written &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-F063B8EC812DF3D204F9327F5D094073-depth004.67.svg&quot; /&gt;. There are three constants from the perspective of &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-4D6C379F66675645B3FFE28A15306857-depth004.67.svg&quot; /&gt;: 3, 2, and &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;. Therefore, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-D981BA4BD14AC44C43A4E4E0EC750B4A-depth004.67.svg&quot; /&gt;. The partial derivative with respect to &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; treats &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; like a constant: &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-55A3A400FAD3326FEF1BB9DDD2658383-depth006.34.svg&quot; /&gt;. It's a good idea to derive these yourself before continuing otherwise the rest of the article won't make sense. Here's the &lt;a href=&quot;https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives&quot;&gt;Khan Academy video on partials&lt;/a&gt; if you need help.&lt;/p&gt;
&lt;p&gt;To make it clear we are doing vector calculus and not just multivariate calculus, let's consider what we do with the partial derivatives &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-84D799755A7F73945BD58B2E057121AB-depth004.67.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A2DE2EC029172B84A0A0E8A8D00F5A6F-depth006.34.svg&quot; /&gt; (another way to say &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-75645E70B6C95F7466C353E9C2306FE0-depth004.67.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-B20FF70C037320C2D0B710F4B592927E-depth006.34.svg&quot; /&gt;) that we computed for &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-D6DEAE7E403381C2C425D4B40CCA936E-depth003.25.svg&quot; /&gt;. Instead of having them just floating around and not organized in any way, let's organize them into a horizontal vector. We call this vector the &lt;em&gt;gradient&lt;/em&gt; of &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-3BAF1600AE50930A155F58AE172B51BD-depth003.25.svg&quot; /&gt; and write it as:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-0C95BB61B2BFFB0C2A95A9DC5D8AF44E.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;So the gradient of &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-3BAF1600AE50930A155F58AE172B51BD-depth003.25.svg&quot; /&gt; is simply a vector of its partials. Gradients are part of the vector calculus world, which deals with functions that map &lt;span class=&quot;eqn&quot;&gt;n&lt;/span&gt; scalar parameters to a single scalar. Now, let's get crazy and consider derivatives of multiple functions simultaneously.&lt;/p&gt;
&lt;h2 id=&quot;sec4&quot;&gt;Matrix calculus&lt;/h2&gt;
&lt;p&gt;When we move from derivatives of one function to derivatives of many functions, we move from the world of vector calculus to matrix calculus. Let's compute partial derivatives for two functions, both of which take two parameters. We can keep the same &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-D6DEAE7E403381C2C425D4B40CCA936E-depth003.25.svg&quot; /&gt; from the last section, but let's also bring in &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-D182AD2135D1E887AFFCA045F432B2CA-depth003.25.svg&quot; /&gt;. The gradient for &lt;span class=&quot;eqn&quot;&gt;g&lt;/span&gt; has two entries, a partial derivative for each parameter:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-FDB56AA8804E0D13E1555DB8E0E1AAEE.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-13AF8214DD5A2040D650C7B460C88129.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;giving us gradient &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-E2670D9705180E731C7455A4B46B7AF6-depth003.25.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;Gradient vectors organize all of the partial derivatives for a specific scalar function. If we have two functions, we can also organize their gradients into a matrix by stacking the gradients. When we do so, we get the &lt;em&gt;Jacobian matrix&lt;/em&gt; (or just the &lt;em&gt;Jacobian&lt;/em&gt;) where the gradients are rows:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-B45AD8AF1574CD63AE6980B44770D643.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Welcome to matrix calculus!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note that there are multiple ways to represent the Jacobian.&lt;/strong&gt; We are using the so-called &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions&quot;&gt;numerator layout&lt;/a&gt; but many papers and software will use the &lt;em&gt;denominator layout&lt;/em&gt;. This is just transpose of the numerator layout Jacobian (flip it around its diagonal):&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-B5113497453A60E25E3241A14CC582C3.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;h3 id=&quot;sec4.1&quot;&gt;Generalization of the Jacobian&lt;/h3&gt;
&lt;p&gt;So far, we've looked at a specific example of a Jacobian matrix. To define the Jacobian matrix more generally, let's combine multiple parameters into a single vector argument: &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-8C56090E55CDB76D1CD0E738EBA7F164-depth003.25.svg&quot; /&gt;. (You will sometimes see notation &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-B07B5ABD67EE0B72F4136C82C68A0C48-depth000.14.svg&quot; /&gt; for vectors in the literature as well.) Lowercase letters in bold font such as &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; are vectors and those in italics font like &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; are scalars. &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is the &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-97361F12A3555FC4FC4E2FFCE1799AC3-depth000.14.svg&quot; /&gt; element of vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; and is in italics because a single vector element is a scalar. We also have to define an orientation for vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;. We'll assume that all vectors are vertical by default of size &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-88512AB12706879FEC83C0C3AA79931F-depth001.08.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-D76C868C669197F65B05E96473454834.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;With multiple scalar-valued functions, we can combine them all into a vector just like we did with the parameters. Let &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A16203A31AD7C6CC63FD297D522170F1-depth003.25.svg&quot; /&gt; be a vector of &lt;span class=&quot;eqn&quot;&gt;m&lt;/span&gt; scalar-valued functions that each take a vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; of length &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-E3114C625CDDDC18ED29BA629242BD65-depth003.25.svg&quot; /&gt; where &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-DEA8E196A572D082201CD5ABF2FA82DE-depth003.25.svg&quot; /&gt; is the cardinality (count) of elements in &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;. Each &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; function within &lt;span class=&quot;eqnvec&quot;&gt;f&lt;/span&gt; returns a scalar just as in the previous section:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-CD6121D27CD89157BF272E5E50AE32FE.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;For instance, we'd represent &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-D6DEAE7E403381C2C425D4B40CCA936E-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-D182AD2135D1E887AFFCA045F432B2CA-depth003.25.svg&quot; /&gt; from the last section as&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-AB738ABA2B35F37C4A171037A396E5F5.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;It's very often the case that &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A193EBC083B4745370F6F1343383D9CC-depth000.14.svg&quot; /&gt; because we will have a scalar function result for each element of the &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; vector. For example, consider the identity function &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-23225C9E5521B6A9777579BE4B92245C-depth003.25.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-4BAF672444FD71616154DE2BE79A5DD6.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;So we have &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A193EBC083B4745370F6F1343383D9CC-depth000.14.svg&quot; /&gt; functions and parameters, in this case. Generally speaking, though, the Jacobian matrix is the collection of all &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-FBFEB9C8459FEE5A2BD529C07B881153-depth001.08.svg&quot; /&gt; possible partial derivatives (&lt;span class=&quot;eqn&quot;&gt;m&lt;/span&gt; rows and &lt;span class=&quot;eqn&quot;&gt;n&lt;/span&gt; columns), which is the stack of &lt;span class=&quot;eqn&quot;&gt;m&lt;/span&gt; gradients with respect to &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-C6F45926C0FEAD3BD359AA24A7FB23A2.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Each &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-98F52E62B85A836D750F1CEDF32E1D68-depth004.67.svg&quot; /&gt; is a horizontal &lt;span class=&quot;eqn&quot;&gt;n&lt;/span&gt;-vector because the partial derivative is with respect to a vector, &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;, whose length is &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A921357AFE67E47CC9D1DB575BCE1B77-depth003.25.svg&quot; /&gt;. The width of the Jacobian is &lt;span class=&quot;eqn&quot;&gt;n&lt;/span&gt; if we're taking the partial derivative with respect to &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; because there are &lt;span class=&quot;eqn&quot;&gt;n&lt;/span&gt; parameters we can wiggle, each potentially changing the function's value. Therefore, the Jacobian is always &lt;span class=&quot;eqn&quot;&gt;m&lt;/span&gt; rows for &lt;span class=&quot;eqn&quot;&gt;m&lt;/span&gt; equations. It helps to think about the possible Jacobian shapes visually:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/latex-6793E76E433509E38529D4B70EB4D956.svg&quot; alt=&quot; \begin{tabular}{c|ccl} &amp;amp; \begin{tabular}[t]{c} scalar\ \framebox(18,18){$x$}\ \end{tabular} &amp;amp; \begin{tabular}{c} vector\ \framebox(18,40){$\mathbf{x}$} \end{tabular}\ \hline \\[\dimexpr-\normalbaselineskip+5pt] \begin{tabular}[b]{c} scalar\ \framebox(18,18){$f$}\ \end{tabular} &amp;amp;\framebox(18,18){$\frac{\partial f}{\partial {x}}$} &amp;amp; \framebox(40,18){$\frac{\partial f}{\partial {\mathbf{x}}}$}&amp;amp;\ \begin{tabular}[b]{c} vector\ \framebox(18,40){$\mathbf{f}$}\ \end{tabular} &amp;amp; \framebox(18,40){$\frac{\partial \mathbf{f}}{\partial {x}}$} &amp;amp; \framebox(40,40){$\frac{\partial \mathbf{f}}{\partial \mathbf{x}}$}\ \end{tabular} &quot; /&gt;&lt;/div&gt;
&lt;p&gt;The Jacobian of the identity function &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-3ABE4A0471143ABFC180C9FA485E5F0A-depth003.25.svg&quot; /&gt;, with &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-E314355F5E2135483279531C62D7E8EC-depth003.25.svg&quot; /&gt;, has &lt;span class=&quot;eqn&quot;&gt;n&lt;/span&gt; functions and each function has &lt;span class=&quot;eqn&quot;&gt;n&lt;/span&gt; parameters held in a single vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;. The Jacobian is, therefore, a square matrix since &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A193EBC083B4745370F6F1343383D9CC-depth000.14.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/latex-229DDEF6A61228EE3F98CD129BBF9663.svg&quot; alt=&quot; \begin{eqnarray*} \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{bmatrix} \frac{\partial}{\partial {x}} f_1(\mathbf{x}) \ \frac{\partial}{\partial {x}} f_2(\mathbf{x})\ \ldots\ \frac{\partial}{\partial {x}} f_m(\mathbf{x}) \end{bmatrix} &amp;amp;=&amp;amp; \begin{bmatrix} \frac{\partial}{\partial {x_1}} f_1(\mathbf{x})~ \frac{\partial}{\partial {x_2}} f_1(\mathbf{x}) ~\ldots~ \frac{\partial}{\partial {x_n}} f_1(\mathbf{x}) \ \frac{\partial}{\partial {x_1}} f_2(\mathbf{x})~ \frac{\partial}{\partial {x_2}} f_2(\mathbf{x}) ~\ldots~ \frac{\partial}{\partial {x_n}} f_2(\mathbf{x}) \ \ldots\ ~\frac{\partial}{\partial {x_1}} f_m(\mathbf{x})~ \frac{\partial}{\partial {x_2}} f_m(\mathbf{x}) ~\ldots~ \frac{\partial}{\partial {x_n}} f_m(\mathbf{x}) \ \end{bmatrix}\\\ &amp;amp; = &amp;amp; \begin{bmatrix} \frac{\partial}{\partial {x_1}} x_1~ \frac{\partial}{\partial {x_2}} x_1 ~\ldots~ \frac{\partial}{\partial {x_n}} x_1 \ \frac{\partial}{\partial {x_1}} x_2~ \frac{\partial}{\partial {x_2}} x_2 ~\ldots~ \frac{\partial}{\partial {x_n}} x_2 \ \ldots\ ~\frac{\partial}{\partial {x_1}} x_n~ \frac{\partial}{\partial {x_2}} x_n ~\ldots~ \frac{\partial}{\partial {x_n}} x_n \ \end{bmatrix}\\\ &amp;amp; &amp;amp; (\text{and since } \frac{\partial}{\partial {x_j}} x_i = 0 \text{ for } j \neq i)\ &amp;amp; = &amp;amp; \begin{bmatrix} \frac{\partial}{\partial {x_1}} x_1 &amp;amp; 0 &amp;amp; \ldots&amp;amp; 0 \ 0 &amp;amp; \frac{\partial}{\partial {x_2}} x_2 &amp;amp;\ldots &amp;amp; 0 \ &amp;amp; &amp;amp; \ddots\ 0 &amp;amp; 0 &amp;amp;\ldots&amp;amp; \frac{\partial}{\partial {x_n}} x_n \ \end{bmatrix}\\\ &amp;amp; = &amp;amp; \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; \ldots&amp;amp; 0 \ 0 &amp;amp;1 &amp;amp;\ldots &amp;amp; 0 \ &amp;amp; &amp;amp; \ddots\ 0 &amp;amp; 0 &amp;amp; \ldots &amp;amp;1 \ \end{bmatrix}\\\ &amp;amp; = &amp;amp; I ~~~(I \text{ is the identity matrix with ones down the diagonal})\ \end{eqnarray*} &quot; /&gt;&lt;/div&gt;
&lt;p&gt;Make sure that you can derive each step above before moving on. If you get stuck, just consider each element of the matrix in isolation and apply the usual scalar derivative rules. That is a generally useful trick: Reduce vector expressions down to a set of scalar expressions and then take all of the partials, combining the results appropriately into vectors and matrices at the end.&lt;/p&gt;
&lt;p&gt;Also be careful to track whether a matrix is vertical, &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;, or horizontal, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-FAAEE783424BC0E27E9AA2F56A7B50B8-depth000.00.svg&quot; /&gt; where &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-FAAEE783424BC0E27E9AA2F56A7B50B8-depth000.00.svg&quot; /&gt; means &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; transpose. Also make sure you pay attention to whether something is a scalar-valued function, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-D3172B9A65679CB6EF09F17BE0918890-depth002.65.svg&quot; /&gt;, or a vector of functions (or a vector-valued function), &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-BCAEE673D10908E8197A79E9D4FB6249-depth002.33.svg&quot; /&gt;.&lt;/p&gt;
&lt;h3 id=&quot;sec4.2&quot;&gt;Derivatives of vector element-wise binary operators&lt;/h3&gt;
&lt;p&gt;Element-wise binary operations on vectors, such as vector addition &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-F26AC9FE4C8C9B953164428DCE00BE9C-depth001.06.svg&quot; /&gt;, are important because we can express many common vector operations, such as the multiplication of a vector by a scalar, as element-wise binary operations. By “element-wise binary operations” we simply mean applying an operator to the first item of each vector to get the first item of the output, then to the second items of the inputs for the second item of the output, and so forth. This is how all the basic math operators are applied by default in numpy or tensorflow, for example. Examples that often crop up in deep learning are &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-E50C0CB908AF7566CCC6D4585634EDC2-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-C65762AB635D8E08F759F1203D223C29-depth000.51.svg&quot; /&gt; (returns a vector of ones and zeros).&lt;/p&gt;
&lt;p&gt;We can generalize the element-wise binary operations with notation &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-E2E59AE84EE7A5B1C905E50FA7753A31-depth003.25.svg&quot; /&gt; where &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-5D19E7D8CDFE53DEB40F29D8936E6C89-depth003.25.svg&quot; /&gt;. (Reminder: &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-CF513DECF6E4ACE0E25CB1C932AAA049-depth003.25.svg&quot; /&gt; is the number of items in &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;.) The &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-1A74909B6CBAA4532A76D83B72C12DE0-depth002.52.svg&quot; /&gt; symbol represents any element-wise operator (such as &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-26B17225B626FB9238849FD60EABDF60-depth001.06.svg&quot; /&gt;) and not the &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-1B3C1A40F9CB094D47E8C6F9B0DF773F-depth000.00.svg&quot; /&gt; function composition operator. Here's what equation &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-E2E59AE84EE7A5B1C905E50FA7753A31-depth003.25.svg&quot; /&gt; looks like when we zoom in to examine the scalar equations:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-ADD1230AE3E64A1B7FA77851BB1F07A1.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;where we write &lt;span class=&quot;eqn&quot;&gt;n&lt;/span&gt; (not &lt;span class=&quot;eqn&quot;&gt;m&lt;/span&gt;) equations vertically to emphasize the fact that the result of element-wise operators give &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A193EBC083B4745370F6F1343383D9CC-depth000.14.svg&quot; /&gt; sized vector results.&lt;/p&gt;
&lt;p&gt;Using the ideas from the last section, we can see that the general case for the Jacobian with respect to &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; is the square matrix:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-54F95B3CFFD404740FAD218B308DEF70.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;and the Jacobian with respect to &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; is:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-2693C112F589CD0E26853EAD5ED36CFD.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;That's quite a furball, but fortunately the Jacobian is very often a diagonal matrix, a matrix that is zero everywhere but the diagonal. Because this greatly simplifies the Jacobian, let's examine in detail when the Jacobian reduces to a diagonal matrix for element-wise operations.&lt;/p&gt;
&lt;p&gt;In a diagonal Jacobian, all elements off the diagonal are zero, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-8B14469E98630C19F16578F90C45F62E-depth007.21.svg&quot; /&gt; where &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-B064F8555EC660F2F8BDC927D9636A06-depth002.72.svg&quot; /&gt;. (Notice that we are taking the partial derivative with respect to &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;j&lt;/sub&gt;&lt;/span&gt; not &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;.) Under what conditions are those off-diagonal elements zero? Precisely when &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;g&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; are contants with respect to &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;j&lt;/sub&gt;&lt;/span&gt;, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-44C0281EC00A2C0E93E4E3863EE9083D-depth007.21.svg&quot; /&gt;. Regardless of the operator, if those partial derivatives go to zero, the operation goes to zero, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A180A4C31CAC188626034423680B71E0-depth002.52.svg&quot; /&gt; no matter what, and the partial derivative of a constant is zero.&lt;/p&gt;
&lt;p&gt;Those partials go to zero when &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;g&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; are not functions of &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;j&lt;/sub&gt;&lt;/span&gt;. We know that element-wise operations imply that &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is purely a function of &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;g&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is purely a function of &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;. For example, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-2361C23CB78AC04537D7D642DF065EF5-depth001.06.svg&quot; /&gt; sums &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-4AF62C4A6B74D712D3FFD3FA4A0062BD-depth002.05.svg&quot; /&gt;. Consequently, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-45553270D20A27EBD4AAE84292606CDD-depth003.25.svg&quot; /&gt; reduces to &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-5F2A3B3A730ABED47918785C5EBF5039-depth003.25.svg&quot; /&gt; and the goal becomes &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-767EBC7C8A1785557E38FD32A10FB123-depth007.21.svg&quot; /&gt;. &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-DB590A5BF8B0ACC05B1FEEDC07929CD7-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-1BF8B4ED9F2C1D993DC0A1E547BDF5CB-depth003.25.svg&quot; /&gt; look like constants to the partial differentiation operator with respect to &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;j&lt;/sub&gt;&lt;/span&gt; when &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-B064F8555EC660F2F8BDC927D9636A06-depth002.72.svg&quot; /&gt; so the partials are zero off the diagonal. (Notation &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-DB590A5BF8B0ACC05B1FEEDC07929CD7-depth003.25.svg&quot; /&gt; is technically an abuse of our notation because &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;g&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; are functions of vectors not individual elements. We should really write something like &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-616C179024B43E6C340B1EE24D414DE8-depth003.25.svg&quot; /&gt;, but that would muddy the equations further, and programmers are comfortable overloading functions, so we'll proceed with the notation anyway.)&lt;/p&gt;
&lt;p&gt;We'll take advantage of this simplification later and refer to the constraint that &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-BD5A63074F44F11CB2ED06325816582A-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-5E597DAA4E5D9263DCBFB6AB02BDB67F-depth003.25.svg&quot; /&gt; access at most &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;, respectively, as the &lt;em&gt;element-wise diagonal condition&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Under this condition, the elements along the diagonal of the Jacobian are &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-C196BF268027E86D3D2420C2A205AF28-depth005.92.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-4CFC6C644E4A95B5760435C5094BE095.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;(The large “0”s are a shorthand indicating all of the off-diagonal are 0.)&lt;/p&gt;
&lt;p&gt;More succinctly, we can write:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-3D114C6873F46EE41AF91BF8B1BB37CD.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-78E1B2628221D9FD588A011D54670619.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;where &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-4935E4C6B875FD6C7C181871B566AB1A-depth003.25.svg&quot; /&gt; constructs a matrix whose diagonal elements are taken from vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Because we do lots of simple vector arithmetic, the general function &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-5B39FE68B4FDD0AF04290BA579A993CB-depth003.25.svg&quot; /&gt; in the binary element-wise operation is often just the vector &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt;. Any time the general function is a vector, we know that &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-BD5A63074F44F11CB2ED06325816582A-depth003.25.svg&quot; /&gt; reduces to &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-F5BDA09DDE6E00806B01094F5BED3026-depth003.25.svg&quot; /&gt;. For example, vector addition &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-DF3F5FDED9142A243031D03CF82121AE-depth001.06.svg&quot; /&gt; fits our element-wise diagonal condition because &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-1ED79D2E2BBE9EE796433D13773157A7-depth003.25.svg&quot; /&gt; has scalar equations &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-F7C644AAA7A70D588A0E003C7C9E439E-depth003.25.svg&quot; /&gt; that reduce to just &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-85550E18F87E4AF75645A38273B97A80-depth003.25.svg&quot; /&gt; with partial derivatives:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-A5DC35FE0CEC748B35BB5991933C4698.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-C2B0BF832F19994D832F90C18B1F04AF.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;That gives us &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-2935E504F134B53B2C03072175BCCD1F-depth004.67.svg&quot; /&gt;, the identity matrix, because every element along the diagonal is 1. &lt;span class=&quot;eqn&quot;&gt;I&lt;/span&gt; represents the square identity matrix of appropriate dimensions that is zero everywhere but the diagonal, which contains all ones.&lt;/p&gt;
&lt;p&gt;Given the simplicity of this special case, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-BD5A63074F44F11CB2ED06325816582A-depth003.25.svg&quot; /&gt; reducing to &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-DB590A5BF8B0ACC05B1FEEDC07929CD7-depth003.25.svg&quot; /&gt;, you should be able to derive the Jacobians for the common element-wise binary operations on vectors:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-0C9CE28C888576E0D4873BDD69BC74EA.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-10B3C04502114250E4A74A1EB5F27F05.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-790C76CEB13E928D08EDC53D7AC4BB5C-depth001.08.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-98FF0549EBE322C195C2B36FD5EEAD33-depth001.08.svg&quot; /&gt; operators are element-wise multiplication and division; &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-790C76CEB13E928D08EDC53D7AC4BB5C-depth001.08.svg&quot; /&gt; is sometimes called the &lt;em&gt;Hadamard product&lt;/em&gt;. There isn't a standard notation for element-wise multiplication and division so we're using an approach consistent with our general binary operation notation.&lt;/p&gt;
&lt;h3 id=&quot;sec4.3&quot;&gt;Derivatives involving scalar expansion&lt;/h3&gt;
&lt;p&gt;When we multiply or add scalars to vectors, we're implicitly expanding the scalar to a vector and then performing an element-wise binary operation. For example, adding scalar &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; to vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-6307CEA088D2D4E98E5B163B9CE8F510-depth002.33.svg&quot; /&gt;, is really &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-8D9C0B9B15490F45C353D9DE64565A4F-depth003.25.svg&quot; /&gt; where &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-3ABE4A0471143ABFC180C9FA485E5F0A-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-BD2335FC4BBF16BE9590D2501CE8C030-depth003.25.svg&quot; /&gt;. (The notation &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-C2C0146718E407005D0C74774C5C5FFC-depth000.00.svg&quot; /&gt; represents a vector of ones of appropriate length.) &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; is any scalar that doesn't depend on &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;, which is useful because then &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-EBCC35EEAE3B420D59689973D8B6BD2E-depth005.92.svg&quot; /&gt; for any &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and that will simplify our partial derivative computations. (It's okay to think of variable &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; as a constant for our discussion here.) Similarly, multiplying by a scalar, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-C9A7B878F49F3D964AFEC9C1F78061CF-depth002.33.svg&quot; /&gt;, is really &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-93A461AB49FD151E602D9344358732CD-depth003.25.svg&quot; /&gt; where &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-790C76CEB13E928D08EDC53D7AC4BB5C-depth001.08.svg&quot; /&gt; is the element-wise multiplication (Hadamard product) of the two vectors.&lt;/p&gt;
&lt;p&gt;The partial derivatives of vector-scalar addition and multiplication with respect to vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; use our element-wise rule:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-F8F0A8F213DE2D87CB4F0C88B2CE8F4C.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;This follows because functions &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-3ABE4A0471143ABFC180C9FA485E5F0A-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-BD2335FC4BBF16BE9590D2501CE8C030-depth003.25.svg&quot; /&gt; clearly satisfy our element-wise diagonal condition for the Jacobian (that &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-DAE17AB5EE9C0A7FFA3E9B1774E80201-depth003.25.svg&quot; /&gt; refer at most to &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-9F8AE8514327A98189F8F05E2ECD6496-depth003.25.svg&quot; /&gt; refers to the &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-97361F12A3555FC4FC4E2FFCE1799AC3-depth000.14.svg&quot; /&gt; value of the &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-49508A178E36D0863E041575526BEA1A-depth001.05.svg&quot; /&gt; vector).&lt;/p&gt;
&lt;p&gt;Using the usual rules for scalar partial derivatives, we arrive at the following diagonal elements of the Jacobian for vector-scalar addition:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-6B4EFB58ED5F9EBD623321FE1975FA4E.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;So, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-5107D6819CDC50A8988D3EA0FB9B94CE-depth004.67.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;Computing the partial derivative with respect to the scalar parameter &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt;, however, results in a vertical vector, not a diagonal matrix. The elements of the vector are:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-5B07C016CBBA27F3E3650DA92BF06A24.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Therefore, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-F05F04525E4A1B8959DE54DC7C692060-depth005.22.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;The diagonal elements of the Jacobian for vector-scalar multiplication involve the product rule for scalar derivatives:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-3E6008F6437DA818B481A79FD47D38E4.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;So, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-3F231096152DFB321FAC62F57A808C35-depth004.67.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;The partial derivative with respect to scalar parameter &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; is a vertical vector whose elements are:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-C1C1AD1A9E7A6ECCAAACE952B8355BFB.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;This gives us &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-82B897A0A23D9C6BB66EDF17D1D3CB02-depth005.22.svg&quot; /&gt;.&lt;/p&gt;
&lt;h3 id=&quot;sec4.4&quot;&gt;Vector sum reduction&lt;/h3&gt;
&lt;p&gt;Summing up the elements of a vector is an important operation in deep learning, such as the network loss function, but we can also use it as a way to simplify computing the derivative of vector dot product and other operations that reduce vectors to scalars.&lt;/p&gt;
&lt;p&gt;Let &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-DC5FB5DC7AEB54D8C206744EED4AD748-depth003.31.svg&quot; /&gt;. Notice we were careful here to leave the parameter as a vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; because each function &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; could use all values in the vector, not just &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;. The sum is over the &lt;strong&gt;results&lt;/strong&gt; of the function and not the parameter. The gradient (&lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-4C8971A9939B0BB2D8AF44195C5BD833-depth001.08.svg&quot; /&gt; Jacobian) of vector summation is:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-40C0C67E5948039B40D9718ECC2858AE.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;(The summation inside the gradient elements can be tricky so make sure to keep your notation consistent.)&lt;/p&gt;
&lt;p&gt;Let's look at the gradient of the simple &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-5FAFFB6A723E437AC6433DCA0B269846-depth003.25.svg&quot; /&gt;. The function inside the summation is just &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-E314355F5E2135483279531C62D7E8EC-depth003.25.svg&quot; /&gt; and the gradient is then:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-B6DF766D85C48FF7434B8FFF7BEC9410.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Because &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-9B32CEB09BA5F0B84935A24BF81D3C9C-depth007.21.svg&quot; /&gt; for &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-B064F8555EC660F2F8BDC927D9636A06-depth002.72.svg&quot; /&gt;, we can simplify to:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-9D07CB9DFE389978D329A8CFE7568825.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Notice that the result is a horizontal vector full of 1s, not a vertical vector, and so the gradient is &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-DFB70C45B6CCB149DFDA0E3690715F92-depth000.00.svg&quot; /&gt;. (The &lt;span class=&quot;eqn&quot;&gt;T&lt;/span&gt; exponent of &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-DFB70C45B6CCB149DFDA0E3690715F92-depth000.00.svg&quot; /&gt; represents the transpose of the indicated vector. In this case, it flips a vertical vector to a horizontal vector.) It's very important to keep the shape of all of your vectors and matrices in order otherwise it's impossible to compute the derivatives of complex functions.&lt;/p&gt;
&lt;p&gt;As another example, let's sum the result of multiplying a vector by a constant scalar. If &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-729968724651D87C8269B6FFEAD6EA90-depth003.25.svg&quot; /&gt; then &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A5D03DE59DCDBA948F463FAABD04791D-depth003.25.svg&quot; /&gt;. The gradient is:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-4C28E3734FC6110AF58C567604ED3462.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The derivative with respect to scalar variable &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; is &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-5ED2D4C114D036610B8E20271C5026EF-depth001.08.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-709809A15FF63948512A3F83DF9F04EA.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;h3 id=&quot;sec4.5&quot;&gt;The Chain Rules&lt;/h3&gt;
&lt;p&gt;We can't compute partial derivatives of very complicated functions using just the basic matrix calculus rules we've seen so far. For example, we can't take the derivative of nested expressions like &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-DCA4F9F0CE7F7CA365E8B26987ED972A-depth003.25.svg&quot; /&gt; directly without reducing it to its scalar equivalent. We need to be able to combine our basic vector rules using what we can call the &lt;em&gt;vector chain rule&lt;/em&gt;. Unfortunately, there are a number of rules for differentiation that fall under the name “chain rule” so we have to be careful which chain rule we're talking about. Part of our goal here is to clearly define and name three different chain rules and indicate in which situation they are appropriate. To get warmed up, we'll start with what we'll call the &lt;em&gt;single-variable chain rule&lt;/em&gt;, where we want the derivative of a scalar function with respect to a scalar. Then we'll move on to an important concept called the &lt;em&gt;total derivative&lt;/em&gt; and use it to define what we'll pedantically call the &lt;em&gt;single-variable total-derivative chain rule&lt;/em&gt;. Then, we'll be ready for the vector chain rule in its full glory as needed for neural networks.&lt;/p&gt;
&lt;p&gt;The chain rule is conceptually a divide and conquer strategy (like Quicksort) that breaks complicated expressions into subexpressions whose derivatives are easier to compute. Its power derives from the fact that we can process each simple subexpression in isolation yet still combine the intermediate results to get the correct overall result.&lt;/p&gt;
&lt;p&gt;The chain rule comes into play when we need the derivative of an expression composed of nested subexpressions. For example, we need the chain rule when confronted with expressions like &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-06D67DC7FE74C1895AEF564F8295E918-depth004.58.svg&quot; /&gt;. The outermost expression takes the &lt;span class=&quot;eqn&quot;&gt;sin&lt;/span&gt; of an intermediate result, a nested subexpression that squares &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;. Specifically, we need the single-variable chain rule, so let's start by digging into that in more detail.&lt;/p&gt;
&lt;h4 id=&quot;sec4.5.1&quot;&gt;Single-variable chain rule&lt;/h4&gt;
&lt;p&gt;Let's start with the solution to the derivative of our nested expression: &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-AB5ECA885C5685990CD778580665B3A4-depth004.58.svg&quot; /&gt;. It doesn't take a mathematical genius to recognize components of the solution that smack of scalar differentiation rules, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-1A32AA532898DEBB80C0C7A818C5C70B-depth004.58.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-DC7C2E5FB11394451EA6A2010904F0B2-depth004.58.svg&quot; /&gt;. It looks like the solution is to multiply the derivative of the outer expression by the derivative of the inner expression or “chain the pieces together,” which is exactly right. In this section, we'll explore the general principle at work and provide a process that works for highly-nested expressions of a single variable.&lt;/p&gt;
&lt;p&gt;Chain rules are typically defined in terms of nested functions, such as &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-281717E562B6C04AA861AC9F2801D016-depth003.25.svg&quot; /&gt; for single-variable chain rules. (You will also see the chain rule defined using function composition &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-88806005B64072FE5A06E4E609A9E251-depth003.25.svg&quot; /&gt;, which is the same thing.) Some sources write the derivative using shorthand notation &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-3D59985B51738C2BF54BA3D955AB8588-depth003.25.svg&quot; /&gt;, but that hides the fact that we are introducing an intermediate variable: &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-8C36385E70550C1C0EA86E14320174DF-depth003.25.svg&quot; /&gt;, which we'll see shortly. It's better to define the &lt;a href=&quot;http://m.wolframalpha.com/input/?i=chain+rule&quot;&gt;single-variable chain rule&lt;/a&gt; of &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-8BC3A7E80988236E8F017205F413461C-depth003.25.svg&quot; /&gt; explicitly so we never take the derivative with respect to the wrong variable. Here is the formulation of the single-variable chain rule we recommend:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-9D3919C42833D1FF1456DEA11D8CC927.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;To deploy the single-variable chain rule, follow these steps:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Introduce intermediate variables for nested subexpressions and subexpressions for both binary and unary operators; e.g., &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-60C13E05D3EC8C10B8564EAE7023D9DB-depth001.08.svg&quot; /&gt; is binary, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-6FFE53F1F614CC1D470F85B6C56A3AFB-depth003.25.svg&quot; /&gt; and other trigonometric functions are usually unary because there is a single operand. This step normalizes all equations to single operators or function applications.&lt;/li&gt;
&lt;li&gt;Compute derivatives of the intermediate variables with respect to their parameters.&lt;/li&gt;
&lt;li&gt;Combine all derivatives of intermediate variables by multiplying them together to get the overall result.&lt;/li&gt;
&lt;li&gt;Substitute intermediate variables back in if any are referenced in the derivative equation.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;The third step puts the “chain” in “chain rule” because it chains together intermediate results. Multiplying the intermediate derivatives together is the common theme among all variations of the chain rule.&lt;/p&gt;
&lt;p&gt;Let's try this process on &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-872F24FC57CA661E3704C0A10869C6B5-depth003.25.svg&quot; /&gt;:&lt;/p&gt;
&lt;ol readability=&quot;0&quot;&gt;&lt;li readability=&quot;3&quot;&gt;Introduce intermediate variables. Let &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-FA510039FEDBE5A935A70EF6E3B46394-depth000.14.svg&quot; /&gt; represent subexpression &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-32F5240D0DBF2CCBE75EF7F8EF2015E0-depth000.14.svg&quot; /&gt; (shorthand for &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-879B57F3F479F58707D2477B46B060CF-depth003.25.svg&quot; /&gt;). This gives us:
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-165CA85C4E868C4589FDC97854EF5AFE.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The order of these subexpressions does not affect the answer, but we recommend working in the reverse order of operations dictated by the nesting (innermost to outermost). That way, expressions and derivatives are always functions of previously-computed elements.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Compute derivatives.
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-33DB49B9B2BFE622EF83565332547027.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Combine.
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-9D1B1984635759F4C2D23464EBBAA995.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Substitute.
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-6E1052BD462233E4BEA04D695437A984.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Notice how easy it is to compute the derivatives of the intermediate variables in isolation! The chain rule says it's legal to do that and tells us how to combine the intermediate results to get &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-E4D3AF86AC3E4315148DA23A886A72EA-depth003.25.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;You can think of the combining step of the chain rule in terms of units canceling. If we let &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; be miles, &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; be the gallons in a gas tank, and &lt;span class=&quot;eqn&quot;&gt;u&lt;/span&gt; as gallons we can interpret &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-70047C98E4163674E78BB42D0CF4AEA8-depth004.58.svg&quot; /&gt; as &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-F98E0EBA888E860C7E51565C7225EBFA-depth006.34.svg&quot; /&gt;. The &lt;span class=&quot;eqn&quot;&gt;gallon&lt;/span&gt; denominator and numerator cancel.&lt;/p&gt;
&lt;p&gt;Another way to to think about the single-variable chain rule is to visualize the overall expression as a dataflow diagram or chain of operations (or &lt;a href=&quot;https://en.wikipedia.org/wiki/Abstract_syntax_tree&quot;&gt;abstract syntax tree&lt;/a&gt; for compiler people):&lt;/p&gt;
&lt;center&gt;
&lt;p&gt;&lt;img src=&quot;http://explained.ai/matrix-calculus/images/sin-square.png&quot; alt=&quot;sin-square.png&quot; width=&quot;130&quot; /&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;p&gt;Changes to function parameter &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; bubble up through a squaring operation then through a &lt;span class=&quot;eqn&quot;&gt;sin&lt;/span&gt; operation to change result &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;. You can think of &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-B13DB657CBF4B8318DBF2799E687D1A1-depth004.58.svg&quot; /&gt; as “getting changes from &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;u&lt;/span&gt;” and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-6E939E305B65D08F88FF95E7E028796B-depth004.58.svg&quot; /&gt; as “getting changes from &lt;span class=&quot;eqn&quot;&gt;u&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;.” Getting from &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; requires an intermediate hop. The chain rule is, by convention, usually written from the output variable down to the parameter(s), &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-70047C98E4163674E78BB42D0CF4AEA8-depth004.58.svg&quot; /&gt;. But, the &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;-to-&lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; perspective would be more clear if we reversed the flow and used the equivalent &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-256F5475DE15D99D8D55FE6F3A15CEA4-depth004.58.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conditions under which the single-variable chain rule applies&lt;/strong&gt;. Notice that there is a single dataflow path from &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to the root &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;. Changes in &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; can influence output &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; in only one way. That is the condition under which we can apply the single-variable chain rule. An easier condition to remember, though one that's a bit looser, is that none of the intermediate subexpression functions, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-3BCB9E96DA63C9CDC1E56647C2071688-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-C6E9D6FA0C33AA632E25E953C6E5C35D-depth003.25.svg&quot; /&gt;, have more than one parameter. Consider &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-026E2EF5E8E6906B6CE75FF6CDB0F14E-depth003.25.svg&quot; /&gt;, which would become &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A1829C9E1CDCFF4FE0BA9C0E7A70E635-depth003.25.svg&quot; /&gt; after introducing intermediate variable &lt;span class=&quot;eqn&quot;&gt;u&lt;/span&gt;. As we'll see in the next section, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-2C284C56D6E912E6D71990A11005902E-depth003.25.svg&quot; /&gt; has multiple paths from &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;. To handle that situation, we'll deploy the single-variable total-derivative chain rule.&lt;/p&gt;
&lt;div readability=&quot;34&quot;&gt;As an aside for those interested in automatic differentiation, papers and library documentation use terminology &lt;em&gt;forward differentiation&lt;/em&gt; and &lt;em&gt;backward differentiation&lt;/em&gt; (for use in the back-propagation algorithm). From a dataflow perspective, we are computing a forward differentiation because it follows the normal data flow direction. Backward differentiation, naturally, goes the other direction and we're asking how a change in the output would affect function parameter &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;. Because backward differentiation can determine changes in all function parameters at once, it turns out to be much more efficient for computing the derivative of functions with lots of parameters. Forward differentiation, on the other hand, must consider how a change in each parameter, in turn, affects the function output &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;. The following table emphasizes the order in which partial derivatives are computed for the two techniques.
&lt;center&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Forward differentiation from &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;&lt;/th&gt;
&lt;th align=&quot;left&quot;&gt;Backward differentiation from &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-256F5475DE15D99D8D55FE6F3A15CEA4-depth004.58.svg&quot; /&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-70047C98E4163674E78BB42D0CF4AEA8-depth004.58.svg&quot; /&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;p&gt;Automatic differentiation is beyond the scope of this article, but we're setting the stage for a future article.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Many readers can solve &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-59B8E919C92D6DBE1DC50C5BE2CD8C1C-depth004.58.svg&quot; /&gt; in their heads, but our goal is a process that will work even for very complicated expressions. This process is also how &lt;a href=&quot;https://en.wikipedia.org/wiki/Automatic_differentiation&quot;&gt;automatic differentiation&lt;/a&gt; works in libraries like PyTorch. So, by solving derivatives manually in this way, you're also learning how to define functions for custom neural networks in PyTorch.&lt;/p&gt;
&lt;p&gt;With deeply nested expressions, it helps to think about deploying the chain rule the way a compiler unravels nested function calls like &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-7559A5FC5EC5CA0B3E50011742D0A87B-depth003.25.svg&quot; /&gt; into a sequence (chain) of calls. The result of calling function &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is saved to a temporary variable called a register, which is then passed as a parameter to &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-36930D135F4763C21D1191803AC41B85-depth002.72.svg&quot; /&gt;. Let's see how that looks in practice by using our process on a highly-nested equation like &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-642FD3E590A2B2D21AFE5254BE8E832F-depth003.25.svg&quot; /&gt;:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Introduce intermediate variables.
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-A153933499426CFC383D252C30A87953.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Compute derivatives.
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-4561212E91367D4B2DCC40262E36921D.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Combine four intermediate values.
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-2CF824877C0FB75B7648CE66E56FB509.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Substitute.
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-9BC49A78C13740AC58294EAA333AF3CC.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Here is a visualization of the data flow through the chain of operations from &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;:&lt;/p&gt;
&lt;center&gt;
&lt;p&gt;&lt;img src=&quot;http://explained.ai/matrix-calculus/images/chain-tree.png&quot; alt=&quot;chain-tree.png&quot; width=&quot;150&quot; /&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;p&gt;At this point, we can handle derivatives of nested expressions of a single variable, &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;, using the chain rule but only if &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; can affect &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; through a single data flow path. To handle more complicated expressions, we need to extend our technique, which we'll do next.&lt;/p&gt;
&lt;h4 id=&quot;sec4.5.2&quot;&gt;Single-variable total-derivative chain rule&lt;/h4&gt;
&lt;p&gt;Our single-variable chain rule has limited applicability because all intermediate variables must be functions of single variables. But, it demonstrates the core mechanism of the chain rule, that of multiplying out all derivatives of intermediate subexpressions. To handle more general expressions such as &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-4E2EFF28C6823738FA61BFC9A3DD6D0F-depth003.25.svg&quot; /&gt;, however, we need to augment that basic chain rule.&lt;/p&gt;
&lt;p&gt;Of course, we immediately see &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-18EF15EC4DDA421BE5BC86F0295D36CD-depth004.58.svg&quot; /&gt;, but that is using the scalar addition derivative rule, not the chain rule. If we tried to apply the single-variable chain rule, we'd get the wrong answer. In fact, the previous chain rule is meaningless in this case because derivative operator &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-624FEBB9A49A3FC96353C861D175C806-depth004.58.svg&quot; /&gt; does not apply to multivariate functions, such as &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-89AE78BE880A004AA5404AC874A01BFF-depth001.95.svg&quot; /&gt; among our intermediate variables:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-5CB23F92FE51ABF1B1885A985EA61BC6.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Let's try it anyway to see what happens. If we pretend that &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-400C939294CFCAC13949F5A92DD9537A-depth005.85.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-8B1AAFE58A962E6F06775EBD2808D5FE-depth004.58.svg&quot; /&gt;, then &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-69A0BF0F0F217A5C8CDB490B4C60ABEE-depth005.85.svg&quot; /&gt; instead of the right answer &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-632334BF7A82AE1CEB6BF98756648B4E-depth001.06.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;Because &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-3DD76E5D04C44A4E7170558B8BFE3219-depth003.25.svg&quot; /&gt; has multiple parameters, partial derivatives come into play. Let's blindly apply the partial derivative operator to all of our equations and see what we get:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-7A7B19296641D8B6B96136527F381589.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Ooops! The partial &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-9A001AB7445AD6C36176920C0E5D253F-depth004.67.svg&quot; /&gt; is wrong because it violates a key assumption for partial derivatives. When taking the partial derivative with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;, the other variables must not vary as &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; varies. Otherwise, we could not act as if the other variables were constants. Clearly, though, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-0F0BF7F8711E6437357749F43EF529D8-depth003.25.svg&quot; /&gt; is a function of &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; and therefore varies with &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;. &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-78D6E87F2E36D9F4C482E9236F997C4C-depth004.67.svg&quot; /&gt; because &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-C9E3A40CD8239D58296F30B000154F1A-depth004.67.svg&quot; /&gt;. A quick look at the data flow diagram for &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-AC2672C7A79B0E068FB3AC3D7FBF94C9-depth003.25.svg&quot; /&gt; shows multiple paths from &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;, thus, making it clear we need to consider direct and indirect (through &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-D8F3471EF0522DC14012F0DC5D01D570-depth003.25.svg&quot; /&gt;) dependencies on &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;:&lt;/p&gt;
&lt;center&gt;
&lt;p&gt;&lt;img src=&quot;http://explained.ai/matrix-calculus/images/plus-square.png&quot; alt=&quot;plus-square.png&quot; width=&quot;150&quot; /&gt;&lt;/p&gt;
&lt;/center&gt;
&lt;p&gt;A change in &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; affects &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; both as an operand of the addition and as the operand of the square operator. Here's an equation that describes how tweaks to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; affect the output:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-69B14AB41D9A7514E8C105FDDF9649C5.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Then, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-461DF6ABE0C0386D728B786B3116A5B1-depth002.65.svg&quot; /&gt;, which we can read as “the change in &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; is the difference between the original &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; at a tweaked &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;.”&lt;/p&gt;
&lt;p&gt;If we let &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A255512F9D61A6777BD5A304235BD26D-depth000.14.svg&quot; /&gt;, then &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-433AD860B59D47738D7AECAB6367A8AD-depth002.65.svg&quot; /&gt;. If we bump &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; by 1, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-60BD3A416346416B27D420F8EFEE9C9E-depth000.14.svg&quot; /&gt;, then &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-4DCF3FC9093036B469E832AEDCFDA608-depth003.25.svg&quot; /&gt;. The change in &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; is not &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-C4CA4238A0B923820DCC509A6F75849B-depth000.00.svg&quot; /&gt;, as &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-BE27DFA2B74F7608759BD413AF458EB2-depth003.25.svg&quot; /&gt; would lead us to believe, but &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-F0B6923B35563AE91BDFC8B06222E495-depth001.08.svg&quot; /&gt;!&lt;/p&gt;
&lt;p&gt;Enter the “law” of &lt;a href=&quot;https://en.wikipedia.org/wiki/Total_derivative&quot;&gt;total derivatives&lt;/a&gt;, which basically says that to compute &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-3BAFFD623D24688B6229E8808F4DD24A-depth004.58.svg&quot; /&gt;, we need to sum up all possible contributions from changes in &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to the change in &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;. The total derivative with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; assumes all variables, such as &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-D33DEF0EB4933F91B88EB4E784ADAF05-depth001.95.svg&quot; /&gt; in this case, are functions of &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; and potentially vary as &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; varies. The total derivative of &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-20596CA2E264644A086ABF3ABCA89367-depth003.25.svg&quot; /&gt; that depends on &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; directly and indirectly via intermediate variable &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-D8F3471EF0522DC14012F0DC5D01D570-depth003.25.svg&quot; /&gt; is given by:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-09EB861D79D7E60D9B37567CE097631B.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Using this formula, we get the proper answer:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-217390CDB48372744AC16E8277C9D0CC.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;That is an application of what we can call the &lt;em&gt;single-variable total-derivative chain rule&lt;/em&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-4586E8ADC3AA440DD41501217E7B6E67.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The total derivative assumes all variables are potentially codependent whereas the partial derivative assumes all variables but &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; are constants.&lt;/p&gt;
&lt;p&gt;There is something subtle going on here with the notation. All of the derivatives are shown as partial derivatives because &lt;span class=&quot;eqn&quot;&gt;f&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;u&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; are functions of multiple variables. This notation mirrors that of &lt;a href=&quot;http://mathworld.wolfram.com/TotalDerivative.html&quot;&gt;MathWorld's notation&lt;/a&gt; but differs from &lt;a href=&quot;https://en.wikipedia.org/wiki/Total_derivative&quot;&gt;Wikipedia&lt;/a&gt;, which uses &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-8A4956B34F845398E8CB25E9789E1477-depth003.25.svg&quot; /&gt; instead (possibly to emphasize the total derivative nature of the equation). We'll stick with the partial derivative notation so that it's consistent with our discussion of the vector chain rule in the next section.&lt;/p&gt;
&lt;p&gt;In practice, just keep in mind that when you take the total derivative with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;, other variables might also be functions of &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; so add in their contributions as well. The left side of the equation looks like a typical partial derivative but the right-hand side is actually the total derivative. It's common, however, that many temporary variables are functions of a single parameter, which means that the single-variable total-derivative chain rule degenerates to the single-variable chain rule.&lt;/p&gt;
&lt;p&gt;Let's look at a nested subexpression, such as &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-FD3AC7E953C3B294864C747188F6F370-depth003.25.svg&quot; /&gt;. We introduce three intermediate variables:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-B5215A178D5EDFCCA280ED63D64A8025.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;and partials:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-C3A02B93F5C0F2C8979E524BB28D35AC.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;where both &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A5D73F881FB427D3CD136DA4815CACA4-depth004.67.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-110F22244D49C5AD607D2BEFE91944A5-depth004.67.svg&quot; /&gt; have &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-4E8A57E5001AA58AEAA927CF98746B9C-depth004.67.svg&quot; /&gt; terms that take into account the total derivative.&lt;/p&gt;
&lt;p&gt;Also notice that the total derivative formula always &lt;strong&gt;sums&lt;/strong&gt; versus, say, multiplies terms &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-935DA2BFCBE67A179A8DAAB35E19A1DA-depth005.92.svg&quot; /&gt;. It's tempting to think that summing up terms in the derivative makes sense because, for example, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-FCCD52402617C355E7BB85B3336D0142-depth002.65.svg&quot; /&gt; adds two terms. Nope. The total derivative is adding terms because it represents a weighted sum of all &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; contributions to the change in &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;. For example, given &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-5C5A09876AA823C381141FDAC1D28BA6-depth002.65.svg&quot; /&gt; instead of &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-C26DA96086B50304FCE872846D4BE19F-depth002.65.svg&quot; /&gt;, the total-derivative chain rule formula still adds partial derivative terms. (&lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-94E561C972A5FE1DDE82761012FB6DB1-depth001.08.svg&quot; /&gt; simplifies to &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-8C0FB3B076D9AEA142467B34F0F794EB-depth000.14.svg&quot; /&gt; but for this demonstration, let's not combine the terms.) Here are the intermediate variables and partial derivatives:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-A1BE672D8676E7C9DE634935C1CDEBBA.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The form of the total derivative remains the same, however:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-50D1EABA22B46536559D83F8C21F749D.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;It's the partials (weights) that change, not the formula, when the intermediate variable operators change.&lt;/p&gt;
&lt;p&gt;Those readers with a strong calculus background might wonder why we aggressively introduce intermediate variables even for the non-nested subexpressions such as &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-32F5240D0DBF2CCBE75EF7F8EF2015E0-depth000.14.svg&quot; /&gt; in &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-06671AD4FF442E623460886DA749C797-depth001.06.svg&quot; /&gt;. We use this process for three reasons: (i) computing the derivatives for the simplified subexpressions is usually trivial, (ii) we can simplify the chain rule, and (iii) the process mirrors how automatic differentiation works in neural network libraries.&lt;/p&gt;
&lt;p&gt;Using the intermediate variables even more aggressively, let's see how we can simplify our single-variable total-derivative chain rule to its final form. The goal is to get rid of the &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-F6E0346D1D3410B0FBE32B41B85999AA-depth004.67.svg&quot; /&gt; sticking out on the front like a sore thumb:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-D8F0438A2B5867000190B7AB280DDD9A.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;We can achieve that by simply introducing a new temporary variable as an alias for &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;: &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-80A59ADB26E30571A3056E2EB3E8DDCB-depth002.69.svg&quot; /&gt;. Then, the formula reduces to our final form:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-DE79CDE41F7B81D0C8E5D49F3D9766BB.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;This chain rule that takes into consideration the total derivative degenerates to the single-variable chain rule when all intermediate variables are functions of a single variable. Consequently, you can remember this more general formula to cover both cases. As a bit of dramatic foreshadowing, notice that the summation sure looks like a vector dot product, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-3F95AED50A8E6A5C514952D767B771CE-depth004.67.svg&quot; /&gt;, or a vector multiply &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-51D7B0B0455FB229AB4920E9CA4AB032-depth004.67.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;Before we move on, a word of caution about terminology on the web. Unfortunately, the chain rule given in this section, based upon the total derivative, is universally called “multivariable chain rule” in calculus discussions, which is highly misleading! Only the intermediate variables are multivariate functions. The overall function, say, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A5A7963264669BCFA0CCFA897853A1E0-depth003.25.svg&quot; /&gt;, is a scalar function that accepts a single parameter &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;. The derivative and parameter are scalars, not vectors, as one would expect with a so-called multivariate chain rule. (Within the context of a non-matrix calculus class, “multivariate chain rule” is likely unambiguous.) To reduce confusion, we use “single-variable total-derivative chain rule” to spell out the distinguishing feature between the simple single-variable chain rule, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-9D3919C42833D1FF1456DEA11D8CC927-depth004.58.svg&quot; /&gt;, and this one.&lt;/p&gt;
&lt;h4 id=&quot;sec4.5.3&quot;&gt;Vector chain rule&lt;/h4&gt;
&lt;p&gt;Now that we've got a good handle on the total-derivative chain rule, we're ready to tackle the chain rule for vectors of functions and vector variables. Surprisingly, this more general chain rule is just as simple looking as the single-variable chain rule for scalars. Rather than just presenting the vector chain rule, let's rediscover it ourselves so we get a firm grip on it. We can start by computing the derivative of a sample vector function with respect to a scalar, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-82C28F66A012D28717CA0CFC8ED7F09B-depth003.25.svg&quot; /&gt;, to see if we can abstract a general formula.&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-428F0EFA4C7B2EEC64829258E8DAFF86.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Let's introduce two intermediate variables, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-E11BA37D5D784AF689E175BEC8A2F284-depth002.65.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-D7F116997176D81A3BBD4E6DFC6FE6B0-depth002.65.svg&quot; /&gt;, one for each &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; so that &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; looks more like &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-C5AF10997DA85D7540CDF87F1F10016C-depth003.25.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-5CE597B34A604EDC0DD0B1AD97CFD690.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-E04563108617169E3793740388785DAB.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The derivative of vector &lt;span class=&quot;eqnvec&quot;&gt;y&lt;/span&gt; with respect to scalar &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; is a vertical vector with elements computed using the single-variable total-derivative chain rule:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-63BFCE8E2B4E8F603E209ECA0C1DADCE.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Ok, so now we have the answer using just the scalar rules, albeit with the derivatives grouped into a vector. Let's try to abstract from that result what it looks like in vector form. The goal is to convert the following vector of scalar operations to a vector operation.&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-0EA3C72DBB9F820121EE6A27D76EC7CC.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;If we split the &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-97D1ACA90CB316ED8AE1EDFFAED02C77-depth007.21.svg&quot; /&gt; terms, isolating the &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-D513134B704605A9687A47F8841D7D29-depth004.67.svg&quot; /&gt; terms into a vector, we get a matrix by vector multiplication:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-692581F3416029FED8E1CE09890F4A5E.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;That means that the Jacobian is the multiplication of two other Jacobians, which is kinda cool. Let's check our results:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-4A8689EA58BF9FA2AF675AAE0C093010.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Whew! We get the same answer as the scalar approach. This vector chain rule for vectors of functions and a single parameter appears to be correct and, indeed, mirrors the single-variable chain rule. Compare the vector rule:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-B0D7932C93DA81FD62418DA5DF3CBE14.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;with the single-variable chain rule:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-8617320E088DCA9EC6795865C614324A.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;To make this formula work for multiple parameters or vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;, we just have to change &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; in the equation. The effect is that &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A24633DDE0B5346B4AE6B49395AC8B6D-depth004.67.svg&quot; /&gt; and the resulting Jacobian, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A9A13E2AE753365278B4F2CD198BBF92-depth004.67.svg&quot; /&gt;, are now matrices instead of vertical vectors. Our complete &lt;em&gt;vector chain rule&lt;/em&gt; is:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-63A95E8A883CCB871C2C68B2D8B6EAA4.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The beauty of the vector formula over the single-variable chain rule is that it automatically takes into consideration the total derivative while maintaining the same notational simplicity. The Jacobian contains all possible combinations of &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; with respect to &lt;span class=&quot;eqn&quot;&gt;g&lt;sub&gt;j&lt;/sub&gt;&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;g&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;j&lt;/sub&gt;&lt;/span&gt;. For completeness, here are the two Jacobian components in their full glory:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-875D6B48E0F3610A491D91FA12067AED.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;where &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-99ADCB62C02EE5D10CC7B5F211BFA15B-depth003.25.svg&quot; /&gt;, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-6E37471488C004C0ABACDF4148B8F3D6-depth003.25.svg&quot; /&gt;, and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-26312045D51B3E69C8357FF7FAF3BB3F-depth003.25.svg&quot; /&gt;. The resulting Jacobian is &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-FBFEB9C8459FEE5A2BD529C07B881153-depth001.08.svg&quot; /&gt; (an &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-2DC13003CC069B1027F12896B1A00631-depth001.08.svg&quot; /&gt; matrix multiplied by a &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-6D999BBC8A6ECBB820C04121088529A1-depth001.08.svg&quot; /&gt; matrix).&lt;/p&gt;
&lt;p&gt;Even within this &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A7896E2D919F33CF607DC9E972C70458-depth006.23.svg&quot; /&gt; formula, we can simplify further because, for many applications, the Jacobians are square (&lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A193EBC083B4745370F6F1343383D9CC-depth000.14.svg&quot; /&gt;) and the off-diagonal entries are zero. It is the nature of neural networks that the associated mathematics deals with functions of vectors not vectors of functions. For example, the neuron affine function has term &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-DE94554BD6F158BD8A829624C65169F4-depth003.25.svg&quot; /&gt; and the activation function is &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-EA2DFCC1C759A6D2EE675EBD05B2C593-depth003.25.svg&quot; /&gt;; we'll consider derivatives of these functions in the next section.&lt;/p&gt;
&lt;p&gt;As we saw in a previous section, element-wise operations on vectors &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; and &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; yield diagonal matrices with elements &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-D840B92230E9F1C5F2545BCA90B34038-depth005.92.svg&quot; /&gt; because &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is a function purely of &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; but not &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;j&lt;/sub&gt;&lt;/span&gt; for &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-B064F8555EC660F2F8BDC927D9636A06-depth002.72.svg&quot; /&gt;. The same thing happens here when &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is purely a function of &lt;span class=&quot;eqn&quot;&gt;g&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;g&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is purely a function of &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-4869BA6DCF2C9A404EECD993808A74B7.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-8866C26258F279CD69740D3A26C0CD90.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;In this situation, the vector chain rule simplifies to:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-2D82A2CFEA0A49E9B7D3C8F986DAF14A.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Therefore, the Jacobian reduces to a diagonal matrix whose elements are the single-variable chain rule values.&lt;/p&gt;
&lt;p&gt;After slogging through all of that mathematics, here's the payoff. All you need is the vector chain rule because the single-variable formulas are special cases of the vector chain rule. The following table summarizes the appropriate components to multiply in order to get the Jacobian.&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/latex-17BE59DB8766A07658ADAA8522995C53.svg&quot; alt=&quot; \begin{tabular}[t]{c|cccc} &amp;amp; \multicolumn{2}{c}{ \begin{tabular}[t]{c} scalar\ \framebox(18,18){$x$}\ \end{tabular}} &amp;amp; &amp;amp;\begin{tabular}{c} vector\ \framebox(18,40){$\mathbf{x}$}\ \end{tabular} \ \begin{tabular}{c}$\frac{\partial}{\partial \mathbf{x}} \mathbf{f}(\mathbf{g}(\mathbf{x}))$ = $\frac{\partial \mathbf{f}}{\partial \mathbf{g}}\frac{\partial\mathbf{g}}{\partial \mathbf{x}}$ \ \end{tabular} &amp;amp; \begin{tabular}[t]{c} scalar\ \framebox(18,18){$u$}\ \end{tabular} &amp;amp; \begin{tabular}{c} vector\ \framebox(18,40){$\mathbf{u}$} \end{tabular}&amp;amp; &amp;amp; \begin{tabular}{c} vector\ \framebox(18,40){$\mathbf{u}$}\ \end{tabular} \ \hline \\[\dimexpr-\normalbaselineskip+5pt] \begin{tabular}[b]{c} scalar\ \framebox(18,18){$f$}\ \end{tabular} &amp;amp;\framebox(18,18){$\frac{\partial f}{\partial {u}}$} \framebox(18,18){$\frac{\partial u}{\partial {x}}$} ~~~&amp;amp; \raisebox{22pt}{\framebox(40,18){$\frac{\partial f}{\partial {\mathbf{u}}}$}} \framebox(18,40){$\frac{\partial \mathbf{u}}{\partial x}$} &amp;amp; ~~~&amp;amp; \raisebox{22pt}{\framebox(40,18){$\frac{\partial f}{\partial {\mathbf{u}}}$}} \framebox(40,40){$\frac{\partial \mathbf{u}}{\partial \mathbf{x}}$} \ \begin{tabular}[b]{c} vector\ \framebox(18,40){$\mathbf{f}$}\ \end{tabular} &amp;amp; \framebox(18,40){$\frac{\partial \mathbf{f}}{\partial {u}}$} \raisebox{22pt}{\framebox(18,18){$\frac{\partial u}{\partial {x}}$}} &amp;amp; \framebox(40,40){$\frac{\partial \mathbf{f}}{\partial \mathbf{u}}$} \framebox(18,40){$\frac{\partial \mathbf{u}}{\partial x}$} &amp;amp; &amp;amp; \framebox(40,40){$\frac{\partial \mathbf{f}}{\partial \mathbf{u}}$} \framebox(40,40){$\frac{\partial \mathbf{u}}{\partial \mathbf{x}}$}\ \end{tabular} &quot; /&gt;&lt;/div&gt;
&lt;h2 id=&quot;sec5&quot;&gt;The gradient of neuron activation&lt;/h2&gt;
&lt;p&gt;We now have all of the pieces needed to compute the derivative of a typical neuron activation for a single neural network computation unit with respect to the model parameters, &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-F3041D1B0AB2DA26CFE6581CCE10BF0F.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;(This represents a neuron with fully connected weights and rectified linear unit activation. There are, however, other affine functions such as convolution and other activation functions, such as exponential linear units, that follow similar logic.)&lt;/p&gt;
&lt;p&gt;Let's worry about &lt;span class=&quot;eqn&quot;&gt;max&lt;/span&gt; later and focus on computing &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-37826EBEE16CD487A60FA876F5038265-depth004.67.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-45DA2FE78D565E2361F35FF898D806A8-depth004.67.svg&quot; /&gt;. (Recall that neural networks learn through optimization of their weights and biases.) We haven't discussed the derivative of the dot product yet, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-31E193E45068EDA5F2E229B246720968-depth003.25.svg&quot; /&gt;, but we can use the chain rule to avoid having to memorize yet another rule. (Note notation &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; not &lt;span class=&quot;eqnvec&quot;&gt;y&lt;/span&gt; as the result is a scalar not a vector.)&lt;/p&gt;
&lt;p&gt;The dot product &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-9CA8C3CD16894AF7620468A20C53D6FA-depth000.00.svg&quot; /&gt; is just the summation of the element-wise multiplication of the elements: &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-4B722CF0EFB8F8ABCBB968086BB587E8-depth003.31.svg&quot; /&gt;. (You might also find it useful to remember the linear algebra notation &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-DE393E92B808CB595831DB7AF0D46F39-depth000.00.svg&quot; /&gt;.) We know how to compute the partial derivatives of &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-1B8AC0FD13AAD81B7EFBE58CDD162D02-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-68EE6C0B6C39602AC2A620854B5785B2-depth001.08.svg&quot; /&gt; but haven't looked at partial derivatives for &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-307751654E2CF1B689CE1D06776B3934-depth003.25.svg&quot; /&gt;. We need the chain rule for that and so we can introduce an intermediate vector variable &lt;span class=&quot;eqnvec&quot;&gt;u&lt;/span&gt; just as we did using the single-variable chain rule:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-AA40E45F705402308665F4778260405C.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Once we've rephrased &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;, we recognize two subexpressions for which we already know the partial derivatives:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-CB86F7761DC5757FCE7D9B440DEB6630.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The vector chain rule says to multiply the partials:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-735EE304812513469D0BAE8D1D32E578.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;To check our results, we can grind the dot product down into a pure scalar function:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-8E233C707CFA165FFECCE145E88AEB24.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Then:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-44FAB0E50B6FA0C7012FF79FB03EBD14.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Hooray! Our scalar results match the vector chain rule results.&lt;/p&gt;
&lt;p&gt;Now, let &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-EABE2A2F7035C793F48B3885A2EA0009-depth002.65.svg&quot; /&gt;, the full expression within the &lt;span class=&quot;eqn&quot;&gt;max&lt;/span&gt; activation function call. We have two different partials to compute, but we don't need the chain rule:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-0F1D53EBF96D7DC463E2226B77812776.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Let's tackle the partials of the neuron activation, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-602D78206C69DAFE86EDC0775CF04CDB-depth003.25.svg&quot; /&gt;. The use of the &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-2E8B663420C79458FF788F7C8F198AA2-depth003.25.svg&quot; /&gt; function call on scalar &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; just says to treat all negative &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; values as 0. The derivative of the max function is a piecewise function. When &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-B960DEAE3DC80FA85FEA3304A9474DB5-depth001.72.svg&quot; /&gt;, the derivative is 0 because &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; is a constant. When &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-E929E61EB1C05B5B7AC234027C595BE2-depth001.05.svg&quot; /&gt;, the derivative of the max function is just the derivative of &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt;, which is &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-C4CA4238A0B923820DCC509A6F75849B-depth000.00.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-61BE19B395EB3114577B2100997DFB6D.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div readability=&quot;18&quot;&gt;An aside on broadcasting functions across scalars. When one or both of the &lt;span class=&quot;eqn&quot;&gt;max&lt;/span&gt; arguments are vectors, such as &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-EA2DFCC1C759A6D2EE675EBD05B2C593-depth003.25.svg&quot; /&gt;, we broadcast the single-variable function &lt;span class=&quot;eqn&quot;&gt;max&lt;/span&gt; across the elements. This is an example of an element-wise unary operator. Just to be clear:
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-94240FE4B77DEE9DA38F596CD4149F9D.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;For the derivative of the broadcast version then, we get a vector of zeros and ones where:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-5B3555A4691C5DA19688E4F76BA1C3AD.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-4EA88847E78682CFDBCA0019C1623945.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;To get the derivative of the &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-FD043EAC3999F8DEAF1FF1E131B3346C-depth003.25.svg&quot; /&gt; function, we need the chain rule because of the nested subexpression, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-6C156670CEC09096976A6722592523F3-depth001.06.svg&quot; /&gt;. Following our process, let's introduce intermediate scalar variable &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; to represent the affine function giving:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-261BC49758F84DF99117345CD8D22CFE.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-9E429AF61D15BF6942A3132FABAC77A2.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The vector chain rule tells us:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-B0C84C12426A7A698FBBCB890502411F.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;which we can rewrite as follows:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-AE892FF5E074073E025BB3BBE586B9F5.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;and then substitute &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-F394C21EEE331911707D5EDBD9BCAE20-depth001.06.svg&quot; /&gt; back in:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-A123AAABD8432822C27BEE74393F78AD.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;That equation matches our intuition. When the activation function clips affine function output &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; to 0, the derivative is zero with respect to any weight &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;. When &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-E929E61EB1C05B5B7AC234027C595BE2-depth001.05.svg&quot; /&gt;, it's as if the &lt;span class=&quot;eqn&quot;&gt;max&lt;/span&gt; function disappears and we get just the derivative of &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; with respect to the weights.&lt;/p&gt;
&lt;p&gt;Turning now to the derivative of the neuron activation with respect to &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt;, we get:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-9CE42F7BD715F354A87DF9043310E3BB.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Let's use these partial derivatives now to handle the entire loss function.&lt;/p&gt;
&lt;h2 id=&quot;sec6&quot;&gt;The gradient of the neural network loss function&lt;/h2&gt;
&lt;p&gt;Training a neuron requires that we take the derivative of our loss or “cost” function with respect to the parameters of our model, &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt;. Because we train with multiple vector inputs (e.g., multiple images) and scalar targets (e.g., one classification per image), we need some more notation. Let&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-D9D9E4DA80EA78BCCCDDC0BE89A198CD.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;where &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-BEDD04B41054D234623B0BB3759ABF73-depth003.25.svg&quot; /&gt;, and then let&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-F924EBF36B5E655648826C8AE83DE16D.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;where &lt;span class=&quot;eqn&quot;&gt;y&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is a scalar. Then the cost equation becomes:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-3D04A32BFDCD990F21451D8230C46FB1.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Following our chain rule process introduces these intermediate variables:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-ED01A1463E7C7657E0DD1546F6C48BFB.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Let's compute the gradient with respect to &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; first.&lt;/p&gt;
&lt;h3 id=&quot;sec6.1&quot;&gt;The gradient with respect to the weights&lt;/h3&gt;
&lt;p&gt;From before, we know:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-577C100C01A97DEA7FB361169DA383B5.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-8E94E788919EA6D0BE11C2615A11C009.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Then, for the overall gradient, we get:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/latex-9F8112A77C51E95057A9E56D29FFB669.svg&quot; alt=&quot; \begin{eqnarray*} \frac{\partial C(v)}{\partial \mathbf{w}} &amp;amp; = &amp;amp; \frac{\partial }{\partial \mathbf{w}}\frac{1}{N} \sum_{i=1}^N v^2\\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \frac{\partial}{\partial \mathbf{w}} v^2\\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \frac{\partial v^2}{\partial v} \frac{\partial v}{\partial \mathbf{w}} \\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N 2v \frac{\partial v}{\partial \mathbf{w}} \\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \begin{cases} 2v\vec{0}^T = \vec{0}^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b \leq 0\ -2v\mathbf{x}^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b &amp;gt; 0\ \end{cases}\\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \begin{cases} \vec{0}^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b \leq 0\ -2(y_i-u)\mathbf{x}_i^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b &amp;gt; 0\ \end{cases}\\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \begin{cases} \vec{0}^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b \leq 0\ -2(y_i-max(0, \mathbf{w}\cdot\mathbf{x}_i+b))\mathbf{x}_i^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b &amp;gt; 0\ \end{cases}\ \phantom{\frac{\partial C(v)}{\partial \mathbf{w}}} &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \begin{cases} \vec{0}^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b \leq 0\ -2(y_i-(\mathbf{w}\cdot\mathbf{x}_i+b))\mathbf{x}_i^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b &amp;gt; 0\ \end{cases}\\\ &amp;amp; = &amp;amp; \begin{cases} \vec{0}^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b \leq 0\ \frac{-2}{N} \sum_{i=1}^N (y_i-(\mathbf{w}\cdot\mathbf{x}_i+b))\mathbf{x}_i^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b &amp;gt; 0\ \end{cases}\\\ &amp;amp; = &amp;amp; \begin{cases} \vec{0}^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b \leq 0\ \frac{2}{N} \sum_{i=1}^N (\mathbf{w}\cdot\mathbf{x}_i+b-y_i)\mathbf{x}_i^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b &amp;gt; 0\ \end{cases} \end{eqnarray*} &quot; /&gt;&lt;/div&gt;
&lt;p&gt;To interpret that equation, we can substitute an error term &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-A90A948C72E3B6E10EF49E9CA3323248-depth002.65.svg&quot; /&gt; yielding:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-6F2CDC50A69419550C3127B318DB71CD.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;From there, notice that this computation is a weighted average across all &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;&lt;sub class=&quot;eqn&quot;&gt;i&lt;/sub&gt; in &lt;span class=&quot;eqn&quot;&gt;X&lt;/span&gt;. The weights are the error terms, the difference between the target output and the actual neuron output for each &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;&lt;sub class=&quot;eqn&quot;&gt;i&lt;/sub&gt; input. The resulting gradient will, on average, point in the direction of higher cost or loss because large &lt;span class=&quot;eqn&quot;&gt;e&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; emphasize their associated &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;&lt;sub class=&quot;eqn&quot;&gt;i&lt;/sub&gt;. Imagine we only had one input vector, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-75147DC2E59AB7AF04E48C0E3C2D71EA-depth003.25.svg&quot; /&gt;, then the gradient is just &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-26DDD7C22F3D8F5B07FC7EE421EEE6ED-depth003.45.svg&quot; /&gt;. If the error is 0, then the gradient is zero and we have arrived at the minimum loss. If &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-3104BCC503B3B8C16FFA2940B56AAF1C-depth001.95.svg&quot; /&gt; is some small positive difference, the gradient is a small step in the direction of &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-262A3BD0318D5034272A8F904D6FAD24-depth001.95.svg&quot; /&gt;. If &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-3104BCC503B3B8C16FFA2940B56AAF1C-depth001.95.svg&quot; /&gt; is large, the gradient is a large step in that direction. If &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-3104BCC503B3B8C16FFA2940B56AAF1C-depth001.95.svg&quot; /&gt; is negative, the gradient is reversed, meaning the highest cost is in the negative direction.&lt;/p&gt;
&lt;p&gt;Of course, we want to reduce, not increase, the loss, which is why the &lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;&gt;gradient descent&lt;/a&gt; recurrence relation takes the negative of the gradient to update the current position (for scalar learning rate &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-FFE9F913124F345732E9F00FA258552E-depth002.65.svg&quot; /&gt;):&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-C50B63069D6F8CA47A43A8116F4AD21B.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Because the gradient indicates the direction of higher cost, we want to update &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; in the opposite direction.&lt;/p&gt;
&lt;h3 id=&quot;sec6.2&quot;&gt;The derivative with respect to the bias&lt;/h3&gt;
&lt;p&gt;To optimize the bias, &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt;, we also need the partial with respect to &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt;. Here are the intermediate variables again:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-18067E5F73988B179A304788A7BC5786.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;We computed the partial with respect to the bias for equation &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-FBD5708C00E92A391A69A42591580BE0-depth003.25.svg&quot; /&gt; previously:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-97F6B2C0D2E31579875BAE3E458BF333.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;For &lt;span class=&quot;eqn&quot;&gt;v&lt;/span&gt;, the partial is:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-8C418D6F20C9CD5F0C56184F94005AF3.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;And for the partial of the cost function itself we get:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/latex-D2EB5E709D4D4474EDB3DE6699F91F9A.svg&quot; alt=&quot; \begin{eqnarray*} \frac{\partial C(v)}{\partial b} &amp;amp; = &amp;amp; \frac{\partial }{\partial b}\frac{1}{N} \sum_{i=1}^N v^2\\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \frac{\partial}{\partial b} v^2\\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \frac{\partial v^2}{\partial v} \frac{\partial v}{\partial b} \\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N 2v \frac{\partial v}{\partial b} \\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \begin{cases} 0 &amp;amp; \mathbf{w} \cdot \mathbf{x} + b \leq 0\ -2v &amp;amp; \mathbf{w} \cdot \mathbf{x} + b &amp;gt; 0\ \end{cases}\\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \begin{cases} 0 &amp;amp; \mathbf{w} \cdot \mathbf{x} + b \leq 0\ -2(y_i-max(0, \mathbf{w}\cdot\mathbf{x}_i+b)) &amp;amp; \mathbf{w} \cdot \mathbf{x} + b &amp;gt; 0\ \end{cases}\\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \begin{cases} 0 &amp;amp; \mathbf{w} \cdot \mathbf{x} + b \leq 0\ 2(\mathbf{w}\cdot\mathbf{x}_i+b-y_i) &amp;amp; \mathbf{w} \cdot \mathbf{x} + b &amp;gt; 0\ \end{cases}\\\ &amp;amp; = &amp;amp; \begin{cases} 0 &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b \leq 0\ \frac{2}{N} \sum_{i=1}^N (\mathbf{w}\cdot\mathbf{x}_i+b-y_i) &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b &amp;gt; 0\ \end{cases} \end{eqnarray*} &quot; /&gt;&lt;/div&gt;
&lt;p&gt;As before, we can substitute an error term:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-2D57432B77DCFDC3D65FC04C9F6621A7.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The partial derivative is then just the average error or zero, according to the activation level. To update the neuron bias, we nudge it in the opposite direction of increased cost:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-301542C82A1BF05D145392056ADC0AC2.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;In practice, it is convenient to combine &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt; into a single vector parameter rather than having to deal with two different partials: &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-1C5C3BA710F818B84DF992E699DB50C3-depth003.25.svg&quot; /&gt;. This requires a tweak to the input vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; as well but simplifies the activation function. By tacking a 1 onto the end of &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-CDA92F9769DA156F5D82B4BF0D40A8B4-depth003.25.svg&quot; /&gt;, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-6C156670CEC09096976A6722592523F3-depth001.06.svg&quot; /&gt; becomes &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-03741D422AA7DF7FF34288D8E4395143-depth000.00.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;This finishes off the optimization of the neural network loss function because we have the two partials necessary to perform a gradient descent.&lt;/p&gt;
&lt;h2 id=&quot;sec7&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Hopefully you've made it all the way through to this point. You're well on your way to understanding matrix calculus! We've included a reference that summarizes all of the rules from this article in the next section. Also check out the annotated resource link below.&lt;/p&gt;
&lt;p&gt;Your next step would be to learn about the partial derivatives of matrices not just vectors. For example, you can take a look at the matrix differentiation section of &lt;a href=&quot;https://atmos.washington.edu/~dennis/MatrixCalculus.pdf&quot;&gt;Matrix calculus&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Acknowledgements&lt;/strong&gt;. We thank &lt;a href=&quot;https://www.usfca.edu/faculty/yannet-interian&quot;&gt;Yannet Interian&lt;/a&gt; (Faculty in MS data science program at University of San Francisco) and &lt;a href=&quot;http://www.cs.usfca.edu/~duminsky/&quot;&gt;David Uminsky&lt;/a&gt; (Faculty/director of MS data science) for their help with the notation presented here.&lt;/p&gt;
&lt;h2 id=&quot;reference&quot;&gt;Matrix Calculus Reference&lt;/h2&gt;
&lt;h3 id=&quot;sec8.1&quot;&gt;Gradients and Jacobians&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;gradient&lt;/em&gt; of a function of two variables is a horizontal 2-vector:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-D72132A48C466D3BFB703D0F1E183152.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The &lt;em&gt;Jacobian&lt;/em&gt; of a vector-valued function that is a function of a vector is an &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-FBFEB9C8459FEE5A2BD529C07B881153-depth001.08.svg&quot; /&gt; (&lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-4B7BE5D4BAEFA7643CD9638A527AC10F-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-E3114C625CDDDC18ED29BA629242BD65-depth003.25.svg&quot; /&gt;) matrix containing all possible scalar partial derivatives:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-137DC03E772BD8D2A21C78E3A744132A.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The Jacobian of the identity function &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-3ABE4A0471143ABFC180C9FA485E5F0A-depth003.25.svg&quot; /&gt; is &lt;span class=&quot;eqn&quot;&gt;I&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&quot;sec8.2&quot;&gt;Element-wise operations on vectors&lt;/h3&gt;
&lt;p&gt;Define generic &lt;em&gt;element-wise operations&lt;/em&gt; on vectors &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; and &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; using operator &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-1A74909B6CBAA4532A76D83B72C12DE0-depth002.52.svg&quot; /&gt; such as &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-26B17225B626FB9238849FD60EABDF60-depth001.06.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-70BD11FE09064F041D0EBEC6D8E84FBA.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The Jacobian with respect to &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; (similar for &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;) is:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-A61DCB134D4B8F779EA6856022B98B45.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Given the constraint (&lt;em&gt;element-wise diagonal condition&lt;/em&gt;) that &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-BD5A63074F44F11CB2ED06325816582A-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-5E597DAA4E5D9263DCBFB6AB02BDB67F-depth003.25.svg&quot; /&gt; access at most &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;, respectively, the Jacobian simplifies to a diagonal matrix:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-80D7EB6F16ABCCCAA6F1CFB0D7CA05D2.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Here are some sample element-wise operators:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-0D9C6372E2681B466B6E1AF1373C07F4.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;h3 id=&quot;sec8.3&quot;&gt;Scalar expansion&lt;/h3&gt;
&lt;p&gt;Adding scalar &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; to vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-6307CEA088D2D4E98E5B163B9CE8F510-depth002.33.svg&quot; /&gt;, is really &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-8D9C0B9B15490F45C353D9DE64565A4F-depth003.25.svg&quot; /&gt; where &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-3ABE4A0471143ABFC180C9FA485E5F0A-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-BD2335FC4BBF16BE9590D2501CE8C030-depth003.25.svg&quot; /&gt;.&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-32264352DB9E0540766087FB1B70A249.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-A328432124854CC510FE59FEC916AC6C.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Scalar multiplication yields:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-2A0AFB09BE7BE042E565C9C7FCC8B136.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-82B897A0A23D9C6BB66EDF17D1D3CB02.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;h3 id=&quot;sec8.4&quot;&gt;Vector reductions&lt;/h3&gt;
&lt;p&gt;The partial derivative of a vector sum with respect to one of the vectors is:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-77BD17F51D7E67D76D508948DB571A81.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;For &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-5FAFFB6A723E437AC6433DCA0B269846-depth003.25.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-7D69817EEF004855C22D1BB441F8C8BF.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;For &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-9D785998BBDB763E1D5EE5546D47E47E-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-B00911788A7536593130B4C89B6653A2-depth003.25.svg&quot; /&gt;, we get:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-C49A553A6A2A5CFFDF4A4EE2C82A7C06.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-2FD6163B4277742378701A51750A9AB5.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Vector dot product &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-CF556A1D1CDF0863FDD547594536501A-depth003.31.svg&quot; /&gt;. Substituting &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-02A67275B737296ACC7D3FACA124192C-depth001.08.svg&quot; /&gt; and using the vector chain rule, we get:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-8FF1F0A13C3FF271E22ADEC15D7F10DC.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Similarly, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-D0E108FD2A63FFC84018DF9BCBE4C91B-depth004.58.svg&quot; /&gt;.&lt;/p&gt;
&lt;h3 id=&quot;sec8.5&quot;&gt;Chain rules&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;vector chain rule&lt;/em&gt; is the general form as it degenerates to the others. When &lt;span class=&quot;eqn&quot;&gt;f&lt;/span&gt; is a function of a single variable &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; and all intermediate variables &lt;span class=&quot;eqn&quot;&gt;u&lt;/span&gt; are functions of a single variable, the single-variable chain rule applies. When some or all of the intermediate variables are functions of multiple variables, the single-variable total-derivative chain rule applies. In all other cases, the vector chain rule applies.&lt;/p&gt;
&lt;center&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Single-variable rule&lt;/th&gt;
&lt;th align=&quot;left&quot;&gt;Single-variable total-derivative rule&lt;/th&gt;
&lt;th align=&quot;left&quot;&gt;Vector rule&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-32235B531450ABE9E39C9C91D083A8E2-depth004.58.svg&quot; /&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-ED4CD5FBA6B6EC8A51FA203E2AAFF531-depth004.67.svg&quot; /&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-6396586BD585A1CFB33959EB6FA8BFA0-depth006.23.svg&quot; /&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;h2 id=&quot;notation&quot;&gt;Notation&lt;/h2&gt;
&lt;p&gt;Lowercase letters in bold font such as &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; are vectors and those in italics font like &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; are scalars. &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is the &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-97361F12A3555FC4FC4E2FFCE1799AC3-depth000.14.svg&quot; /&gt; element of vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; and is in italics because a single vector element is a scalar. &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-DEA8E196A572D082201CD5ABF2FA82DE-depth003.25.svg&quot; /&gt; means “length of vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;.”&lt;/p&gt;
&lt;p&gt;The &lt;span class=&quot;eqn&quot;&gt;T&lt;/span&gt; exponent of &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-FAAEE783424BC0E27E9AA2F56A7B50B8-depth000.00.svg&quot; /&gt; represents the transpose of the indicated vector.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-60D8CDDDBF54CF43BC22AF322D2BB8E3-depth003.31.svg&quot; /&gt; is just a for-loop that iterates &lt;span class=&quot;eqn&quot;&gt;i&lt;/span&gt; from &lt;span class=&quot;eqn&quot;&gt;a&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt;, summing all the &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Notation &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-50BBD36E1FD2333108437A2CA378BE62-depth003.25.svg&quot; /&gt; refers to a function called &lt;span class=&quot;eqn&quot;&gt;f&lt;/span&gt; with an argument of &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;eqn&quot;&gt;I&lt;/span&gt; represents the square “identity matrix” of appropriate dimensions that is zero everywhere but the diagonal, which contains all ones.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-4935E4C6B875FD6C7C181871B566AB1A-depth003.25.svg&quot; /&gt; constructs a matrix whose diagonal elements are taken from vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The dot product &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-9CA8C3CD16894AF7620468A20C53D6FA-depth000.00.svg&quot; /&gt; is the summation of the element-wise multiplication of the elements: &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-4B722CF0EFB8F8ABCBB968086BB587E8-depth003.31.svg&quot; /&gt;. Or, you can look at it as &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-CF0A6D64FC3321DB0EC98B7683024367-depth000.22.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;Differentiation &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-624FEBB9A49A3FC96353C861D175C806-depth004.58.svg&quot; /&gt; is an operator that maps a function of one parameter to another function. That means that &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-EC0CEC5F9488EC510F8D688E7003222D-depth004.58.svg&quot; /&gt; maps &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-50BBD36E1FD2333108437A2CA378BE62-depth003.25.svg&quot; /&gt; to its derivative with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;, which is the same thing as &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-07A5EA519C4CEA1A3539E3A7FC289163-depth004.58.svg&quot; /&gt;. Also, if &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-FD91C508F91C2C84498680BD337C1D7A-depth003.25.svg&quot; /&gt;, then &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-B1ED3CF9BA4D6F25A5A4F481C45EC658-depth004.58.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;The partial derivative of the function with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-2E335C72CE43928A54BED52C5A6FCC87-depth004.67.svg&quot; /&gt;, performs the usual scalar derivative holding all other variables constant.&lt;/p&gt;
&lt;p&gt;The gradient of &lt;span class=&quot;eqn&quot;&gt;f&lt;/span&gt; with respect to vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;, &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-69AA2CF3DFE9D41CB1DB567D1B0AD275-depth003.25.svg&quot; /&gt;, organizes all of the partial derivatives for a specific scalar function.&lt;/p&gt;
&lt;p&gt;The Jacobian organizes the gradients of multiple functions into a matrix by stacking them:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-620A2D5A110082A77BCB7A2BA1E00590.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The following notation means that &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; has the value &lt;span class=&quot;eqn&quot;&gt;a&lt;/span&gt; upon &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-585B40029A25F6E19FF42DBC26AE5702-depth001.95.svg&quot; /&gt; and value &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt; upon &lt;img src=&quot;http://explained.ai/matrix-calculus/images/eqn-CBECF4275AFD44DAD4B312042088DA7E-depth001.95.svg&quot; /&gt;.&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://explained.ai/matrix-calculus/images/blkeqn-AE0AEA302ADBC6C30F0A32446C7912AA.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;h2 id=&quot;sec10&quot;&gt;Resources&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://www.wolframalpha.com/input/?i=D%5B%7Bx%5E2,+x%5E3%7D.%7B%7B1,2%7D,%7B3,4%7D%7D.%7Bx%5E2,+x%5E3%7D,+x%5D&quot;&gt;Wolfram Alpha&lt;/a&gt; can do symbolic matrix algebra and there is also a cool dedicated &lt;a href=&quot;http://www.matrixcalculus.org/&quot;&gt;matrix calculus differentiator&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When looking for resources on the web, search for “matrix calculus” not “vector calculus.” Here are some comments on the top links that come up from a &lt;a href=&quot;https://www.google.com/search?q=matrix+calculus&amp;amp;oq=matrix+calculus&quot;&gt;Google search&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;To learn more about neural networks and the mathematics behind optimization and back propagation, we highly recommend &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap1.html&quot;&gt;Michael Nielsen's book&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For those interested specifically in convolutional neural networks, check out &lt;a href=&quot;https://arxiv.org/pdf/1603.07285.pdf&quot;&gt;A guide to convolution arithmetic for deep learning&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We reference the law of &lt;a href=&quot;https://en.wikipedia.org/wiki/Total_derivative&quot;&gt;total derivative&lt;/a&gt;, which is an important concept that just means derivatives with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; must take into consideration the derivative with respect &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; of all variables that are a function of &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;.&lt;/p&gt;
&lt;/body&gt;</description>
<pubDate>Fri, 29 Jun 2018 06:23:11 +0000</pubDate>
<dc:creator>prostoalex</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>http://explained.ai/matrix-calculus/index.html</dc:identifier>
</item>
<item>
<title>Exactis Data Breach May Have Leaked Personal Data of Almost Every American Adult</title>
<link>https://www.marketwatch.com/story/a-new-data-breach-may-have-exposed-personal-information-of-almost-every-american-adult-2018-06-27</link>
<guid isPermaLink="true" >https://www.marketwatch.com/story/a-new-data-breach-may-have-exposed-personal-information-of-almost-every-american-adult-2018-06-27</guid>
<description>&lt;p&gt;A little-known Florida company may have exposed the personal data of nearly every American adult, according to a new report.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.wired.com/story/exactis-database-leak-340-million-records/&quot; target=&quot;_new&quot; class=&quot;icon&quot;&gt;Wired reported Wednesday&lt;/a&gt; that Exactis, a Palm Coast, Fla.-based marketing and data-aggregation company, had exposed a database containing almost 2 terabytes of data, containing nearly 340 million individual records, on a public server. That included records of 230 million consumers and 110 million businesses.&lt;/p&gt;
&lt;p&gt;“It seems like this is a database with pretty much every U.S. citizen in it,” security researcher Vinny Troia, who discovered the breach earlier this month, told Wired. “I don’t know where the data is coming from, but it’s one of the most comprehensive collections I’ve ever seen,” he said.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Read:&lt;/strong&gt; &lt;a href=&quot;https://www.marketwatch.com/story/how-the-number-of-data-breaches-is-soaring-in-one-chart-2018-02-26&quot; class=&quot;icon none&quot;&gt;How the number of data breaches is soaring — in one chart&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;While the database apparently does not include credit-card numbers or Social Security numbers, it does include phone numbers, email and postal addresses as well as more than 400 personal characteristics, such as whether a person is a smoker, if they own a dog or cat, their religion and a multitude of personal interests. Even though no financial information was included, the breadth of personal data could make it possible to profile individuals or help scammers steal identities.&lt;/p&gt;
&lt;p&gt;Troia told Wired that he was easily able to access the database on the internet, and in theory, plenty of other people could have too. He said he warned Exactis and the FBI about the vulnerability, and the data is no longer publicly accessible.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Also see:&lt;/strong&gt; &lt;a href=&quot;https://www.marketwatch.com/story/at-what-point-should-you-be-concerned-about-a-data-breach-2018-04-03&quot; class=&quot;icon none&quot;&gt;Everything you wanted to know about data breaches, privacy violations and hacks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.exactis.com/&quot; target=&quot;_new&quot; class=&quot;icon&quot;&gt;On its website&lt;/a&gt;, Exactis said it maintained 3.5 billion consumer, business and digital records, including “demographic, geographic, firmographic, lifestyle, interests, CPG, automotive, and behavioral data.” The company said it has data on 218 million individuals and 110 million U.S. households.&lt;/p&gt;
&lt;p&gt;There are about 325 million residents in the U.S., with about 244 million adults and 126 million households, according to the U.S. Census Bureau.&lt;/p&gt;
&lt;p&gt;Exactis did not immediately respond when asked to confirm the breach.&lt;/p&gt;
&lt;p&gt;If confirmed, the data leak would be one of the largest in history, and far bigger than the &lt;a href=&quot;https://www.marketwatch.com/story/regulators-in-8-states-order-equifax-to-improve-its-cybersecurity-2018-06-27&quot; class=&quot;icon none&quot;&gt;Equifax data breach last year&lt;/a&gt; that exposed the personal information of about 148 million consumers.&lt;/p&gt;
&lt;p&gt;A 2016 breach of AdultFriendFinder exposed the data of &lt;a href=&quot;https://www.marketwatch.com/story/friendfinder-breach-may-have-exposed-412-million-users-data-2016-11-14&quot; class=&quot;icon none&quot;&gt;more than 412 million accounts&lt;/a&gt;, while Yahoo’s 2013 hack exposed the personal data of &lt;a href=&quot;https://www.wsj.com/articles/yahoo-triples-estimate-of-breached-accounts-to-3-billion-1507062804&quot; target=&quot;_new&quot; class=&quot;icon&quot;&gt;about 3 billion accounts&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;While technically not a breach, Facebook Inc. &lt;span class=&quot;quote down bgQuote&quot; data-channel=&quot;/quotes/zigman/9962609/composite&quot; data-bgformat=&quot;&quot;&gt;&lt;a class=&quot;qt-chip trackable&quot; data-fancyid=&quot;XNASStockFB&quot; href=&quot;https://www.marketwatch.com/investing/stock/fb&quot; data-track-mod=&quot;MW_story_quote&quot;&gt;FB, &lt;span class=&quot;bgPercentChange&quot;&gt;-0.18%&lt;/span&gt;&lt;/a&gt;&lt;/span&gt;   said in March that &lt;a href=&quot;https://www.marketwatch.com/story/the-cult-of-facebook-2018-03-23&quot; class=&quot;icon none&quot;&gt;most of its 2 billion users&lt;/a&gt; had their personal data “improperly shared” without their permission, including about 87 million profiles that were scraped by Cambridge Analytica.&lt;/p&gt;



</description>
<pubDate>Fri, 29 Jun 2018 02:09:51 +0000</pubDate>
<dc:creator>jaytaylor</dc:creator>
<og:description>A Florida-based company that few have heard of may have exposed the personal data of nearly every American adult, according to a new report.</og:description>
<og:type>article</og:type>
<og:url>https://www.marketwatch.com/story/a-new-data-breach-may-have-exposed-personal-information-of-almost-every-american-adult-2018-06-27</og:url>
<og:title>A new data breach may have exposed personal information of almost every American adult</og:title>
<og:image>http://s.marketwatch.com/public/resources/MWimages/MW-GL656_hack06_ZG_20180627211548.jpg</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.marketwatch.com/story/a-new-data-breach-may-have-exposed-personal-information-of-almost-every-american-adult-2018-06-27</dc:identifier>
</item>
<item>
<title>Marketing Firm Exactis Leaked a Personal Info Database with 340M Records</title>
<link>https://www.wired.com/story/exactis-database-leak-340-million-records/</link>
<guid isPermaLink="true" >https://www.wired.com/story/exactis-database-leak-340-million-records/</guid>
<description>&lt;p&gt;&lt;span class=&quot;lede&quot;&gt;You've probably never&lt;/span&gt; heard of the marketing and data aggregation firm Exactis. But it may well have heard of you. And now there's also a good chance that whatever information the company has about you, it recently leaked onto the public internet, available to any hacker who simply knew where to look.&lt;/p&gt;
&lt;p&gt;Earlier this month, security researcher Vinny Troia discovered that Exactis, a data broker based in Palm Coast, Florida, had exposed a database that contained close to 340 million individual records on a publicly accessible server. The haul comprises close to 2 terabytes of data that appears to include personal information on hundreds of millions of American adults, as well as millions of businesses. While the precise number of individuals included in the data isn't clear—and the leak doesn't seem to contain credit card information or Social Security numbers—it does go into minute detail for each individual listed, including phone numbers, home addresses, email addresses, and other highly personal characteristics for every name. The categories range from interests and habits to the number, age, and gender of the person's children.&lt;/p&gt;
&lt;p class=&quot;paywall&quot;&gt;&quot;It seems like this is a database with pretty much every US citizen in it,&quot; says Troia, who is the founder of his own New York-based security company, Night Lion Security. Troia notes that almost every person he's searched for in the database, he's found. And when WIRED asked him to find records for a list of 10 specific people in the database, he very quickly found six of them. &quot;I don’t know where the data is coming from, but it’s one of the most comprehensive collections I’ve ever seen,&quot; he says.&lt;/p&gt;
&lt;h3 class=&quot;paywall&quot;&gt;In the Open&lt;/h3&gt;
&lt;p class=&quot;paywall&quot;&gt;While it's far from clear if any criminal or malicious hackers have accessed the database, Troia says it would have been easy enough for them to find. Troia himself spotted the database while using the search tool Shodan, which allows researchers to scan for all manner of internet-connected devices. He says he'd been curious about the security of ElasticSearch, a popular type of database that's designed to be easily queried over the internet using just the command line. So he simply used Shodan to search for all ElasticSearch databases visible on publicly accessible servers with American IP addresses. That returned about 7,000 results. As Troia combed through them, he quickly found the Exactis database, unprotected by any firewall.&lt;/p&gt;

&lt;p class=&quot;paywall&quot;&gt;&quot;I’m not the first person to think of scraping ElasticSearch servers,&quot; he says. &quot;I’d be surprised if someone else didn't already have this.&quot;&lt;/p&gt;
&lt;p class=&quot;paywall&quot;&gt;Troia contacted both Exactis and the FBI about his discovery last week, and he says the company has since protected the data so that it's no longer accessible. Exactis did not respond to multiple calls and emails from WIRED asking for comment on its data leak.&lt;/p&gt;
&lt;p class=&quot;paywall&quot;&gt;Aside from the sheer breadth of the Exactis leak, it may be even more remarkable for its depth: Each record contains entries that go far beyond contact information and public records to include more than 400 variables on a vast range of specific characteristics: whether the person smokes, their religion, whether they have dogs or cats, and interests as varied as scuba diving and plus-size apparel. WIRED independently analyzed a sample of the data Troia shared and confirmed its authenticity, though in some cases the information is outdated or inaccurate.&lt;/p&gt;
&lt;div class=&quot;inset-left-component paywall inset-left-component--pullquote&quot; readability=&quot;10&quot;&gt;
&lt;blockquote name=&quot;inset-left&quot; class=&quot;inset-left-component__el&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;&quot;I don’t know where the data is coming from, but it’s one of the most comprehensive collections I’ve ever seen.&quot;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;inset-left&quot; class=&quot;inset-left-component__el&quot;&gt;Vinny Troia, Night Lion Security&lt;/p&gt;
&lt;/div&gt;
&lt;p class=&quot;paywall&quot;&gt;While the lack of financial information or Social Security numbers means the database isn't a straightforward tool for identity theft, the depth of personal info nonetheless could help scammers with other forms of social engineering, says Marc Rotenberg, executive director of the nonprofit Electronic Privacy Information Center. &quot;The likelihood of financial fraud is not that great, but the possibility of impersonation or profiling is certainly there,&quot; Rotenberg says. He notes that while some of the data is available in public records, much of it appears to be the sort of nonpublic information that data brokers aggregate from sources like magazine subscriptions, credit card transaction data sold by banks, and credit reports. &quot;A lot of this information is now routinely gathered on American consumers,&quot; Rotenberg adds.&lt;/p&gt;

&lt;p class=&quot;paywall&quot;&gt;Without confirmation from Exactis, the precise number of people affected by the data leak remains tough to count. Troia found two versions of Exactis' database, one of which appears to have been newly added during the period he was observing its server. Both contained roughly 340 million records, split into about 230 million records on consumers and 110 million on business contacts. On its website, Exactis boasts that it possesses data on 218 million individuals, including 110 million US households, as well a total of 3.5 billion &quot;consumer, business, and digital records.&quot;&lt;/p&gt;
&lt;p class=&quot;paywall&quot;&gt;&quot;Data is the fuel that powers Exactis,&quot; the site reads. &quot;Layer on hundreds of selects including demographic, geographic, lifestyle, interests, and behavioral data to target highly specific audiences with laser-like precision.&quot;&lt;/p&gt;
&lt;h3 class=&quot;paywall&quot;&gt;A Database Dilemma&lt;/h3&gt;
&lt;p class=&quot;paywall&quot;&gt;&lt;a href=&quot;https://www.wired.com/2017/03/want-stop-big-data-breaches-start-databases/&quot;&gt;Massive leaks of user databases that are accidentally left accessible on the public internet&lt;/a&gt; have nearly reached epidemic status, affecting everything from health information to password caches stored by software firms. One particularly prolific researcher, security firm UpGuard's Chris Vickery, &lt;a href=&quot;https://www.zdnet.com/article/chris-vickery-data-breach-hunter/&quot; target=&quot;_blank&quot;&gt;has discovered those database leaks again and again&lt;/a&gt;, from 93 million Mexican citizens' voter registration records to a list of 2.2 million &quot;high-risk&quot; people suspected of crime or terrorism, known as the World Check Risk Screening database.&lt;/p&gt;

&lt;p class=&quot;paywall&quot;&gt;But if the Exactis leak does in fact include 230 million people's information, that would make it one of the largest in years, bigger even than 2017's &lt;a href=&quot;https://www.wired.com/story/equifax-found-millions-more-people-affected-2017-breach/&quot;&gt;Equifax breach of 145.5 million people's data&lt;/a&gt;, though smaller than the &lt;a href=&quot;https://www.wired.com/story/yahoo-breach-three-billion-accounts/&quot;&gt;Yahoo hack that affected 3 billion accounts&lt;/a&gt;, revealed last October. (It's worth emphasizing in the case of the Exactis leak, unlike in those earlier data breaches, the data wasn't necessarily stolen by malicious hackers, only publicly exposed on the internet.) But like the Equifax breach, the vast majority of people included in the Exactis leak likely have no idea they're in the database.&lt;/p&gt;
&lt;p class=&quot;paywall&quot;&gt;EPIC's Marc Rotenberg argues that the timing of the breach, just after the implementation of Europe's General Data Protection Regulation, highlights the persistent lack of regulation around privacy and data collection in the US. A GDPR-like law in the US, he notes, might not have prevented Exactis from collecting the data it later leaked, but it might have required the company to at least disclose to individuals what sort of data it collects about them and allow them to limit how that data is stored or used.&lt;/p&gt;
&lt;p class=&quot;paywall&quot;&gt;&quot;If you have a profile on someone, that person should be able to see their profile and limit its use,&quot; Rotenberg says. &quot;It’s one thing to subscribe to a magazine. It’s another for a single company to have such a detailed profile of your entire life.&quot;&lt;/p&gt;
&lt;hr class=&quot;paywall&quot;/&gt;&lt;h3 class=&quot;paywall&quot;&gt;More Great WIRED Stories&lt;/h3&gt;
</description>
<pubDate>Thu, 28 Jun 2018 22:56:22 +0000</pubDate>
<dc:creator>georgecmu</dc:creator>
<og:type>article</og:type>
<og:title>Marketing Firm Leaked Database With 340 Million Records</og:title>
<og:description>The leak may include data on hundreds of millions of Americans, with hundreds of details for each, from demographics to personal interests.</og:description>
<og:image>https://media.wired.com/photos/5b32a4f11027fe1d7ddd121e/191:100/pass/PersonalData-Security-945420192.jpg</og:image>
<og:url>https://www.wired.com/story/exactis-database-leak-340-million-records/</og:url>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.wired.com/story/exactis-database-leak-340-million-records/</dc:identifier>
</item>
<item>
<title>The Strange Brain of the World’s Greatest Solo Climber (2016)</title>
<link>http://nautil.us/issue/61/coordinates/the-strange-brain-of-the-worlds-greatest-solo-climber-rp</link>
<guid isPermaLink="true" >http://nautil.us/issue/61/coordinates/the-strange-brain-of-the-worlds-greatest-solo-climber-rp</guid>
<description>&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;A&lt;/span&gt;lex Honnold has his own verb. “To honnold”—usually written as “honnolding”—is to stand in some high, precarious place with your back to the wall, looking straight into the abyss. To face fear, literally.&lt;/p&gt;
&lt;p&gt;The verb was inspired by photographs of Honnold in precisely that position on Thank God Ledge, located 1,800 feet off the deck in Yosemite National Park. Honnold side-shuffled across this narrow sill of stone, heels to the wall, toes touching the void, when, in 2008, he became the first rock climber ever to scale the sheer granite face of Half Dome alone and without a rope. Had he lost his balance, he would have fallen for 10 long seconds to his death on the ground far below. One. Two. Three. Four. Five. Six. Seven. Eight. Nine. Ten.&lt;/p&gt;
&lt;img src=&quot;http://static.nautil.us/10085_5e62c1998206e0110459a6143546fe2e.png&quot; width=&quot;733&quot; alt=&quot;&quot;/&gt;&lt;p&gt;Honnold is history’s greatest ever climber in the free solo style, meaning he ascends without a rope or protective equipment of any kind. Above about 50 feet, any fall would likely be lethal, which means that, on epic days of soloing, he might spend 12 or more hours in the Death Zone. On the hardest parts of some climbing routes, his fingers will have no more contact with the rock than most people have with the touchscreens of their phones, while his toes press down on edges as thin as sticks of gum. Just watching a video of Honnold climbing will trigger some degree of vertigo, heart palpitations, or nausea in most people, and that’s if they can watch them at all. Even Honnold has said that his palms sweat when he watches himself on film.&lt;/p&gt;
&lt;p&gt;All of this has made Honnold the most famous climber in the world. He has appeared on the cover of &lt;em&gt;National Geographic&lt;/em&gt;, on &lt;em&gt;60 Minutes&lt;/em&gt;, in commercials for Citibank and BMW, and in a trove of viral videos. He might insist that he feels fear (he describes standing on Thank God Ledge as “surprisingly scary”), but he has become a paramount symbol of fearlessness.&lt;/p&gt;
&lt;p&gt;He also inspires no shortage of peanut-gallery commentary that something is wrong with his wiring. In 2014, he gave a presentation at Explorers Hall, at the National Geographic Society headquarters in Washington, D.C. The audience was there to hear from climbing photographer Jimmy Chin and veteran explorer Mark Synnott, but above all they had gathered to gasp at tales about Honnold.&lt;/p&gt;
&lt;div class=&quot;reco&quot;&gt;
&lt;article class=&quot;issue-article&quot;&gt;&lt;div&gt;&lt;a href=&quot;http://nautil.us/issue/61/Coordinates/what-time-feels-like-when-youre-improvising&quot; class=&quot;obnd_lnk&quot; data-trval=&quot;what-time-feels-like-when-youre-improvising&quot; data-trlbl=&quot;foc_rec&quot; data-tract=&quot;internal_art&quot;&gt;&lt;img src=&quot;http://static.nautil.us/14801_209dc878267a1292ff2a1b0bdfbbc52e.png&quot; alt=&quot;Sapolsky_TH-F1&quot; width=&quot;314&quot; height=&quot;177&quot;/&gt;&lt;/a&gt;&lt;/div&gt;


&lt;/article&gt;&lt;/div&gt;
&lt;p&gt;Synnott got the biggest response from a story set in Oman, where the team had traveled by sailboat to visit the remote mountains of the Musandam Peninsula, which reaches like a skeletal hand into the mouth of the Persian Gulf. Coming upon an isolated village, they went ashore to mix with the locals. “At a certain point,” Synnott said, “these guys start yelling and they’re pointing up at the cliff. And we’re like, ‘What’s going on?’ And of course I’m thinking, ‘Well, I’m pretty sure I know.’ ”&lt;/p&gt;
&lt;p&gt;Up came the photograph for the gasp from the crowd. There was Honnold, the same casual dude who was sitting on stage in a grey hoodie and khakis, now looking like a toy as he scaled a huge, bone-colored wall behind the town. (“The rock quality wasn’t the best,” Honnold said later.) He was alone and without a rope. Synnott summed up the villagers’ reaction: “Basically, they think Alex is a witch.”&lt;/p&gt;
&lt;p&gt;When the Explorers Hall presentation concluded, the adventurers sat down to autograph posters. Three lines formed. In one of them, a neurobiologist waited to share a few words with Synnott about the part of the brain that triggers fear. The concerned scientist leaned in close, shot a glance toward Honnold, and said, “That kid’s amygdala isn’t firing.”&lt;/p&gt;
&lt;img src=&quot;http://static.nautil.us/10084_c1aff6753244c6ee93d489992b51f012.png&quot; width=&quot;733&quot; alt=&quot;&quot;/&gt;&lt;span class=&quot;caption&quot;&gt;&lt;strong&gt;NO BIG DEAL:&lt;/strong&gt; Technician James Purl and neuroscientist Jane E. Joseph put Honnold into an MRI tube to measure his brain’s fear levels. After looking at gruesome and arousing images inside, he commented, “I was like, whatever.”&lt;/span&gt;&lt;span class=&quot;credit&quot;&gt;©2016 NGC Network International, LLC and NGC Network US, LLC&lt;/span&gt;

&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;O&lt;/span&gt;nce upon a time, Honnold tells me, he would have been afraid—his word, not mine—to have psychologists and scientists looking at his brain, probing his behavior, surveying his personality. “I’ve always preferred not to look inside the sausage,” he says. “Like, if it works, it works. Why ask questions about it? But now I feel like I’ve sort of stepped past that.”&lt;/p&gt;
&lt;p&gt;And so, on this morning in March, 2016, he is laid out, sausage-roll style, inside a large, white tube at the Medical University of South Carolina, in Charleston. The tube is a functional magnetic resonance imaging (fMRI) brain scanner, essentially a huge magnet, which detects activity in the brain’s different regions by tracing blood flows.&lt;/p&gt;
&lt;p&gt;Months earlier, I had approached Honnold about taking a look at his much admired, much maligned brain. “I feel totally normal, whatever that means,” he said. “It’d be interesting to see what the science says.”&lt;/p&gt;
&lt;blockquote class=&quot;pull-quote&quot;&gt;
&lt;p&gt;“Why does he do this?”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The cognitive neuroscientist who volunteered to carry out the scan is Jane Joseph, who in 2005 was one of the first people to perform fMRIs on high sensation seekers—people who are drawn to intense experiences and are willing to take risks to have them. Psychologists have studied sensation seeking for decades because it often leads to out-of-control behaviors such as drug and alcohol addiction, unsafe sex, and problem gambling. In Honnold, Joseph saw the possibility of a more remarkable typology: the super sensation seeker, who pursues experiences at the outer limits of danger, yet is able to tightly regulate the mind and body’s responses to them. She is also simply in awe of what Honnold can do. She had tried to watch videos of him climbing ropeless, but being a low sensation seeker herself, found them overwhelming.&lt;/p&gt;
&lt;p&gt;“I’m excited to see what his brain looks like,” she says, sitting in the control room behind leaded glass as the scan begins. “Then we’ll just check what his amygdala is doing, to see: Does he really have no fear?”&lt;/p&gt;
&lt;p&gt;Often referred to as the brain’s fear center, the amygdala is more precisely the center of a threat response and interpretation system. It receives information on a straight pathway from our senses, which allows us to, for example, step back from an unexpected precipice without a moment’s conscious thought, and triggers a roster of other bodily responses familiar to almost everyone: racing heartbeat, sweaty palms, tunnel vision, loss of appetite. Meanwhile, the amygdala sends information up the line for higher processing in the cortical structures of the brain, where it may be translated into the conscious emotion we call fear.&lt;/p&gt;
&lt;p&gt;An initial anatomical scan of Honnold’s brain appears on MRI technician James Purl’s computer. “Can you go down to his amygdala? We have to know,” says Joseph. Medical literature includes cases of people with rare congenital conditions, such as Urbach-Wiethe disease, which damage and degrade the amygdala. While these people generally don’t experience fear, they also tend to show other bizarre symptoms, such as a total lack of concern for personal space. One individual was comfortable standing nose-to-nose with others while making direct eye contact.&lt;/p&gt;
&lt;p&gt;Purl scrolls down, down, through the Rorschach topography of Honnold’s brain, until, with the suddenness of a photo bomb, a pair of almond-shaped nodes materialize out of the morass. “He has one!” says Joseph, and Purl laughs. Whatever else explains how Honnold can climb ropeless into the Death Zone, it isn’t because there’s an empty space where his amygdala should be. At a glance, Joseph says, the apparatus seems perfectly healthy.&lt;/p&gt;
&lt;p&gt;Inside the tube, Honnold is looking at a series of about 200 images that flick past at the speed of channel surfing. The photographs are meant to disturb or excite. “At least in non-Alex people, these would evoke a strong response in the amygdala,” says Joseph. “I can’t bear to look at some of them, to be honest.” The selection includes corpses with their facial features bloodily reorganized; a toilet choked with feces; a woman shaving herself, Brazilian style; and two invigorating mountain-climbing scenes.&lt;/p&gt;
&lt;p&gt;“Maybe his amygdala is not firing—he’s having no internal reactions to these stimuli,” says Joseph. “But it could be the case that he has such a well-honed regulatory system that he can say, ‘OK, I’m feeling all this stuff, my amygdala is going off,’ but his frontal cortex is just so powerful that it can calm him down.”&lt;/p&gt;
&lt;img src=&quot;http://static.nautil.us/10086_6412121cbb2dc2cb9e460cfee7046be2.png&quot; width=&quot;733&quot; alt=&quot;&quot;/&gt;&lt;span class=&quot;caption&quot;&gt;&lt;strong&gt;ABSENCE OF FEAR:&lt;/strong&gt; Scans compare Honnold’s brain (left) with a control subject’s (right), a rock climber of a similar age. Crosshairs mark the amygdala, a group of nuclei involved in generating fear. As both climbers look at the same arousing images, the control subject’s amygdala glows, while Honnold’s remains inert, showing no activity whatsoever.&lt;/span&gt;&lt;span class=&quot;credit&quot;&gt;Jane Joseph&lt;/span&gt;
&lt;p&gt;There is also a more existential question. “Why does he do this?” she says. “He knows it’s life-threatening—I’m sure people tell him every day. So there may be some kind of really strong reward, like the thrill of it is very rewarding.”&lt;/p&gt;
&lt;p&gt;To find out, Honnold is now running through a second experiment, the “reward task,” in the scanner. He can win or lose small amounts of money (the most he can win is $22) depending on how quickly he clicks a button when signaled. “It’s a task that we know activates the reward circuitry very strongly in the rest of us,” Joseph says.&lt;/p&gt;
&lt;p&gt;In this case, she’s looking most closely at another brain apparatus, the nucleus accumbens, located not far from the amygdala (which is also at play in the reward circuitry) near the top of the brainstem. It is one of the principal processors of dopamine, a neurotransmitter that arouses desire and pleasure. High sensation seekers, Joseph explains, may require more stimulation than other people to get a dopamine hit.&lt;/p&gt;
&lt;p&gt;After about half an hour, Honnold emerges from the scanner looking sleepily doe-eyed. Raised in Sacramento, California, he has a refreshingly frank manner of speaking, and an oddly contradictory demeanor that might be described as intensely laid back—his nickname is No Big Deal, which is his assessment of almost every experience he undergoes. Like most expert climbers, he is leanly muscled, more like a fitness buff than a body builder. The exceptions are his fingers, which permanently look as though they’ve just been slammed in a car door, and his forearms, which bring to mind Popeye.&lt;/p&gt;
&lt;p&gt;“Looking at all those images—does that count as being under stress?” he asks Joseph.&lt;/p&gt;
&lt;p&gt;“Those images that you saw are used pretty widely in the field for inducing fairly strong arousal responses,” Joseph replies.&lt;/p&gt;
&lt;p&gt;“Because, I can’t say for sure, but I was like, &lt;em&gt;whatever&lt;/em&gt;,” he says. The photographs, even the “gruesome burning children and stuff” struck him as dated and jaded. “It’s like looking through a curio museum.”&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;A&lt;/span&gt; month later, having studied Honnold’s scans, Joseph is on a patchy conference call to Shanghai, China, where Honnold is en route to climb, with ropes, the underbelly of the stalactite-spangled Great Arch of Getu. Unusually for Honnold, his voice betrays tiredness and even stress. A few days earlier, near Index, Washington, he had climbed an easy route to set up ropes for his girlfriend’s parents. As his girlfriend, Sanni McCandless, lowered him back to the ground, he suddenly dropped the final 10 feet to land in a jumble on the rocks below—the rope wasn’t long enough to get him to the ground, and the end had slipped through McCandless’ hands. “It was kind of just a botch,” he says. He suffered compression fractures in two vertebrae. It was the most serious accident of his rock climbing life, and it came while he was tied into a rope.&lt;/p&gt;
&lt;p&gt;“What do all the brain pictures mean?” Honnold asks, looking at the brightly colored fMRI images that Joseph has sent him. “Is my brain intact?”&lt;/p&gt;
&lt;p&gt;“Your brain’s intact,” says Joseph. “And it’s quite interesting.”&lt;/p&gt;
&lt;p&gt;Even to the untrained eye, the reason for her interest is clear. Joseph had used a control subject—a high-sensation-seeking male rock climber of similar age to Honnold—for comparison. Like Honnold, the control subject had described the scanner tasks as utterly unstimulating. Yet in the fMRI images of the two men’s responses to the high-arousal photographs, with brain activity indicated in electric purple, the control subject’s amygdala might as well be a neon sign. Honnold’s is gray. He shows zero activation.&lt;/p&gt;
&lt;p&gt;Flip to the scans for the monetary reward task: Once again, the control subject’s amygdala and several other brain structures “look like a Christmas tree lit up,” Joseph says. In Honnold’s brain, the only activity is in the regions that process visual input, confirming only that he had been awake and looking at the screen. The rest of his brain is in lifeless black and white.&lt;/p&gt;
&lt;p&gt;“There’s just not much going on in my brain,” Honnold muses. “It just doesn’t do anything.”&lt;/p&gt;
&lt;p&gt;To see if she was somehow missing something, Joseph had tried dialing down the statistical threshold. She finally found a single voxel—the smallest volume of brain matter sampled by the scanner—that had lit up in the amygdala. By that point, though, real data was indistinguishable from error. “Nowhere, at a decent threshold, was there amygdala activation,” she says.&lt;/p&gt;
&lt;p&gt;Could the same be happening as Honnold climbs ropeless into situations that would cause almost any other person to melt down in terror? Yes, says Joseph—in fact, that’s exactly what she thinks is going on. Where there is no activation, she says, there probably is no threat response. Honnold really does have an extraordinary brain, and he really could be feeling no fear up there. None at all. None whatsoever.&lt;/p&gt;
&lt;img src=&quot;http://static.nautil.us/10088_42edd1ec1dc5f5c1f11fd74a959e96c9.png&quot; width=&quot;733&quot; alt=&quot;&quot;/&gt;&lt;strong&gt;A WARNING SIGN:&lt;/strong&gt; Joseph was surprised by some of the results of Honnold’s personality survey. Despite his extraordinary calm and focus while climbing, he showed a higher level of urgency and disinhibition than the average sensation seeker, suggesting he has a risky impulsivity.

&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;onnold has always rejected the idea that he is fearless. To the wider world, he is known as a figure of preternatural calm as he hangs by his fingertips on the fine line between life and death. No one was watching, though, more than a decade ago, when he was 19 years old, standing at the base of his first major ropeless rock climb: Corrugation Corner, near Lake Tahoe, California. On the arcane grade scale climbers use to describe a route’s difficulty, Corrugation Corner is a 5.7—more than 15 points easier than Honnold’s maximum skill level at the time. Still, the line is 300 feet high. “You’d fall and die,” Honnold says.&lt;/p&gt;

&lt;p&gt;In order to free solo the route, he first had to have the desire to do so. “I think that the unique thing isn’t my ability to solo, I think the unique thing is really wanting to,” Honnold says. His heroes were ropeless climbers like Peter Croft and John Bachar, who had set new standards in the style in the 1980s and ’90s. (Honnold was also intensely shy, which made it difficult for him to find partners for roped climbing.) He saw their photographs in climbing magazines and knew—he just &lt;em&gt;knew&lt;/em&gt;—that he wanted to put himself in those same kinds of positions: wildly exposed, potentially deadly, totally under control.&lt;/p&gt;
&lt;p&gt;He is, in other words, the classic high sensation seeker. On the same day he climbed into the MRI tube, Honnold also answered several surveys used by psychologists to measure the degree of a person’s sensation seeking. He was asked to agree or disagree with statements such as, &lt;em&gt;I would enjoy the sensation of skiing very fast down a high mountain slope&lt;/em&gt; (“I frickin’ love skiing fast downhill,” he says); &lt;em&gt;I would enjoy parachute jumping&lt;/em&gt; (“I learned how to skydive”); and &lt;em&gt;I like to explore a strange city or section of town myself, even if it means getting lost&lt;/em&gt; (“That’s everyday life for me”). He once filled out a similar questionnaire at an outdoors industry show, in which the question about whether he would ever consider rock climbing was illustrated by a photo of: Alex Honnold.&lt;/p&gt;
&lt;blockquote class=&quot;pull-quote&quot;&gt;
&lt;p&gt;Nowhere in the fear center of Honnold’s brain could the neuroscientist spot activity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yet Honnold ended up scared, really scared, on Corrugation Corner. He clung to the big, friendly holds. “I overgripped the shit out of it,” he says. Obviously, though, he didn’t give up after that first experience. Instead, Honnold donned what he called “mental armor” and crossed the threshold of fear again and again. “For every hard pitch I’ve soloed I’ve probably soloed a hundred easy pitches,” he says.&lt;/p&gt;
&lt;p&gt;One by one, acts that had seemed outrageous to him began to seem not so crazy: soloing moves in which he hangs only by his fingers, for example, with his feet swinging in the open air, or, as he did in June on a notorious route called The Complete Scream, climbing ropeless up a pitch that he had never ascended before. In 12 years of free solos, Honnold has broken holds, had his feet slip, gotten off-route into unknown terrain, been surprised by animals like birds and ants, or just suffered “that fraying at the edges, you know, where you’ve just been up in the void too long.” But because he managed to deal with these problems, he gradually dampened his anxieties about them.&lt;/p&gt;
&lt;p&gt;To Marie Monfils, who heads the Monfils Fear Memory Lab at the University of Texas at Austin, Honnold’s process sounds like an almost textbook, if obviously extreme, approach to dealing with fear. Until recently, Monfils says, most psychologists believed that memories—including fear memories—became “consolidated,” or unchangeable, soon after they were acquired. In just the past 16 years, that understanding has shifted. Research has shown that every time we recall a memory, it undergoes reconsolidation, meaning we are able to add new information or a different interpretation to our remembrance, even turning fearful memories into fearless ones.&lt;/p&gt;
&lt;p&gt;Honnold keeps a detailed climbing journal, in which he revisits his climbs and makes note of what he can do better. For his most challenging solos, he also puts a lot of time into preparation: rehearsing the moves and, later, picturing each movement in perfect execution. To get ready for one 1,200-foot-high ascent at the cutting edge of free soloing, he even visualized everything that could possibly go wrong—including “losing it,” falling off, and bleeding out on the rock below—to come to terms with those possibilities before he left the ground. Honnold completed that climb, known as Moonlight Buttress, in Utah’s Zion National Park, about 13 years after he started climbing, and four years after he started soloing.&lt;/p&gt;
&lt;p&gt;Revisiting memories to cast them in a new light, Monfils says, is almost certainly something that we do all the time without being aware of it. But doing so actively, as Honnold did, is better—“a beautiful example of reconsolidation.”&lt;/p&gt;
&lt;p&gt;Visualization—which we might think of as &lt;em&gt;pre&lt;/em&gt;-consolidation, whereby a person pictures a future event rather than a past one—functions in much the same way. “To review move after move, you’d expect that he did consolidate his motor memory and as a result probably had an increased sense of competence,” Monfils says. Feelings of competence, in turn, have been shown to reduce anxiety, which helps to explain why, for example, people who are fearful of public speaking (as Honnold used to be, by the way) feel less anxious about it as they do it more often and develop their skills.&lt;/p&gt;
&lt;p&gt;“It’s better over time if you can put yourself in a situation where you experience some fear, but you overcome it, and you do it again and again and again,” Monfils says. “It’s hard, and it’s a big investment, but it becomes easier.”&lt;/p&gt;
&lt;p&gt;The amygdala, again, plays a key role. Monfils offers an example from her own life. She has a genuine phobia of snakes. One day, canoeing with friends at the edge of a lake, she spotted a water moccasin, which is a venomous species, hanging on a branch. Monfils started screaming, paddled frantically to the middle of the lake, and avoided outdoor adventures for a year afterward. Then, on a hike, she ran into another snake and freaked out again. This time, she applied her expertise to the problem. She made efforts to lie down and recall the episode in calm and logical terms. She reconsolidated her scary memory into something more useful. Just one week later, she suppressed her fear, mustered her courage, and got out on the trails again.&lt;/p&gt;
&lt;p&gt;“The amygdala probably activates a split second before you explicitly remember, ‘ah, this is where I saw the snake,’ ” she says. “So you feel your hands being sweaty and you feel this flood of emotions. And it requires this explicit engagement on your part to involve your prefrontal cortex and say, ‘the snake is not here now, in fact the snake didn’t do anything when it was there, it just happened to be there.’ And then progressively what this does is that your prefrontal cortex quenches this amygdala-on-fire. It puts the information in its appropriate context to say, ‘there’s no need to be afraid here, you can just walk on the path.’ ”&lt;/p&gt;
&lt;img src=&quot;http://static.nautil.us/10089_69f8ea31de0c00502b2ae571fbab1f95.png&quot; width=&quot;733&quot; alt=&quot;&quot;/&gt;&lt;strong&gt;ON EDGE:&lt;/strong&gt; In 2008, “as a matter of pride,” Honnold walked across Thank God Ledge, while free-climbing Half Dome in Yosemite. Later, he wrote, “Walking face-out across Thank God Ledge is surprisingly scary.”

&lt;p&gt;&lt;span class=&quot;dropcap mqw&quot;&gt;W&lt;/span&gt;ithout going back in time to scan Honnold’s brain before he started down his own path as a free soloist, there is no way to know how much nature and how much nurture went into his fearlessness. But a few possibilities seem safe to rule out.&lt;/p&gt;
&lt;p&gt;Joseph LeDoux, a neuroscientist at New York University who has been studying the brain’s response to threats since the 1980s, tells me he has never heard of any person being born with a normal amygdala—as Honnold’s appears to be—that shows no sign of activation. Addressing a possibility raised by Honnold that a person could burn out his amygdala from overstimulation, LeDoux says, “I don’t think that could happen.” Still, when I describe Honnold’s total absence of amygdala activation during the scan tasks, LeDoux’s response is, “That sounds pretty impressive.”&lt;/p&gt;
&lt;p&gt;There is genetic variability between individuals in all parts of the brain, LeDoux says, so it’s a fair bet that Honnold’s threat-response circuitry started out on the cool end of the spectrum—which would explain why his younger self saw a powerful appeal, rather than lethal danger, in the photographs of his ropeless climbing heroes. At least as important as the brain that Honnold was born with, however, is the one that he has wired for himself through thousands of hours of risk-taking. “His brain is probably predisposed to be less reactive to threats that other people would be naturally responsive to, simply because of the choices he’s made,” LeDoux says. “On top of that, these self-imposed strategies that he’s using make that even better, or stronger.”&lt;/p&gt;
&lt;p&gt;Genetics has a clearer role in the personality traits that have helped motivate Honnold’s ropeless climbing. Sensation seeking is thought to be partly heritable, and can be passed down from parents to their children. The trait is associated with lower anxiety and a blunted response to potentially dangerous situations. One result can be a tendency to underestimate risks, which a recent study linked to an imbalance caused by low amygdala reactivity and less effective inhibition of sensation seeking by the prefrontal cortex.&lt;/p&gt;
&lt;blockquote class=&quot;pull-quote&quot;&gt;
&lt;p&gt;Has Honnold’s new awareness of his atypical brain affected his sense of self?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Joseph’s own research doesn’t look at individual cases (she considers her scan of Honnold an “observation”), but she has noted “greatly diminished” amygdala responsiveness among some cohorts of high sensation seekers—and Honnold is a &lt;em&gt;very&lt;/em&gt; high sensation seeker. Compared against the data pool collected by Joseph’s lab, Honnold is twice as sensation-seeking as the average person, and fully 20 percent higher than the average high sensation seeker. The most likely explanation for his flatline amygdala activation in the scanner, Joseph says, is that the tasks she set for him simply were not strong enough tea.&lt;/p&gt;
&lt;p&gt;Honnold also scores as exceedingly conscientious, associated with the ability to concentrate, remain focused on a task, and see things through. He also surveyed high in premeditation, his typical &lt;em&gt;modus operandi&lt;/em&gt;, and very low in neuroticism, making him unlikely to ruminate over unlikely outcomes or risks that are impossible to manage. “If you don’t have any fear to begin with,” Honnold says, “there’s a lot less to control.”&lt;/p&gt;
&lt;p&gt;“He has the traits that enable him to be incredibly focused, and incredibly patient, but at the same time totally sensation seeking,” Joseph says. A single example is a long way from proving a theory, but a guy who free solos into the Death Zone, and yet goes by the nickname No Big Deal, is compelling evidence of Joseph’s super-sensation-seeker hypothesis when it comes to Honnold.&lt;/p&gt;
&lt;p&gt;“The idea of the super sensation seeker—who is defined by having this really strong motivation to pursue these kinds of positive and thrilling experiences, but at the same time having the control and the regulation—is important. I think it could teach us a lot about potentially treating substance-abuse disorder, anxiety disorders, and coming up with strategies that people can use,” she says. “Potentially just talking with Alex, you could envision a new kind of intervention.”&lt;/p&gt;
&lt;p&gt;For example, many high sensation seekers’ problematic behaviors involve intense experiences that can be pursued impulsively and without obvious immediate consequences, such as binge drinking or drug use. (Honnold has always avoided alcohol and drugs, and doesn’t drink coffee.) Joseph wonders if that energy could be redirected into high-arousal activities—such as rock climbing, but with protective gear—that by their nature involve constraint, premeditation, and specific goals, reinforcing different life patterns.&lt;/p&gt;
&lt;p&gt;At the very least, it might be possible for any one of us to work a little bit of Honnold’s magic. You may not have the traits of a super sensation seeker, or be able to quench your amygdala on command, but with conscious effort and gradual, repeated exposure to what you fear, any one of us might muster courage that we didn’t know we had.&lt;/p&gt;
&lt;p&gt;Honnold’s personal challenge is different, with higher stakes. As remarkably well wired as he is—or has made himself to be—there are risk factors in the mix.&lt;/p&gt;
&lt;p&gt;When I ask Honnold to describe the ideal free-solo psychological experience, he says, “You get into positions where you’re like, this is so outrageous, you know? Like, this is so amazing. That’s the whole point, really—to be up in some position that makes you feel like a total hero.”&lt;/p&gt;
&lt;p&gt;Yet he also tells me that easier, day-to-day soloing (the kind that most rock climbers would still consider to be an extreme activity) has lost some of its novelty, and even life-list solos sometimes leave him underwhelmed. “I didn’t find it as fulfilling as I’d hoped,” Honnold has written about an all-day solo link-up of three difficult routes. “People might expect these kinds of climbing achievements to generate euphoria, but in fact I seem to experience the opposite.”&lt;/p&gt;
&lt;p&gt;The total lack of activation throughout most of Honnold’s brain during the reward task is a good fit with the hypothesis that sensation seekers need powerful stimuli in order to ramp up the dopamine circuitry that makes an experience feel rewarding, Joseph says. One result can be the endless pursuit of strong sensations, which in the case of substance abuse and gambling, contributes to addiction and dependency.&lt;/p&gt;
&lt;p&gt;Honnold could, in that sense, be “addicted to climbing,” Joseph says, and the hunger for sensation could push him ever closer to his limits as a free soloist. At the same time, a defining quality of his ropeless climbing has been the conscientiousness and premeditation that he brings to it. The greatest risk for Honnold, Joseph says, may lie in the tension between those opposing compulsions.&lt;/p&gt;
&lt;p&gt;Joseph had expected Honnold to survey low in impulsivity traits, such as urgency and disinhibition, associated with rash decisions and actions taken without much thought to the consequences, particularly when a person is feeling down. In fact, he scored on the high end. This helps explain what might be called, using Honnold’s own terminology, his “fuck it” ascents, in which composure gives way to depression and angst, and planning to, well, impulsivity.&lt;/p&gt;
&lt;p&gt;Here’s one: While “emotionally unhinged,” as he put it, by a faltering relationship in 2010, he soloed a 1,000-foot wall in the Nevada desert that he had climbed with a rope only once before, several years earlier. Honnold considers that climb an example of how he has learned to harness both positive and negative moods to achieve his goals. Obviously, it worked out fine—he is still around to tell the tale. But when I ask Joseph if she has any warning to offer Honnold based on the scan and survey results, she replies, “Don’t let the impulsivity win out over the conscientiousness.”&lt;/p&gt;
&lt;img src=&quot;http://static.nautil.us/10087_afc2637129ad904485e07d2c0e6b0688.png&quot; width=&quot;733&quot; alt=&quot;&quot;/&gt;&lt;strong&gt;SOLO:&lt;/strong&gt; Honnold says he began climbing solo because he was “too shy to go up to strangers at a crag and ask if they’d like to rope up with me.” Here in Oman, on the Arabian Peninsula, he “deep-water solos,” climbing that usually ends with a drop into the water below.

&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;T&lt;/span&gt;he next time I catch up with Honnold, he’s climbing with his girlfriend in Europe. I want to know if his new awareness of his atypical brain has affected his sense of self. No, he says, the discovery that his amygdala sleeps in his brain like an old dog in an Irish pub has not changed the way he climbs, nor shaken his sense of identity. That isn’t to say that it hasn’t given him pause for reflection.&lt;/p&gt;
&lt;p&gt;On a recent rest day from climbing, he says, he and McCandless decided to try a “&lt;em&gt;via ferrata&lt;/em&gt;” near Lauterbrunnen, Switzerland. A &lt;em&gt;via ferrata&lt;/em&gt; is a kind of climbing route with artificial holds: rungs, pegs, ladders, and bridges are attached to the rock, while the climber is protected by a harness connected to a fixed cable. Honnold, of course, didn’t bother with the harness.&lt;/p&gt;
&lt;p&gt;“But then at a certain point, I was like, actually, this is kind of hardcore. Like I actually needed to pay attention,” he says. The &lt;em&gt;via ferrata&lt;/em&gt;, it turned out, climbed across a sheer rock wall on a series of rebar rungs set 3,000 feet off the valley floor. They were high in the mountains, the weather threatened, McCandless was near tears, and after recent rains, water was streaking down the limestone face and dripping on the hand holds, the foot holds, and their heads.&lt;/p&gt;
&lt;p&gt;“I definitely thought about how I process fear,” Honnold says. What he realized was that, in this case at least, he didn’t. He had been in similar situations so many times that it had become normal. There was nothing to process; there was only who he had become. “This is not scary,” he said to himself, “because this is what I do.”&lt;/p&gt;

&lt;p&gt;&lt;em&gt;J.B. MacKinnon writes on the environment, outdoors, consumerism, and other topics. His most recent book is&lt;/em&gt; The Once and Future World: Nature As It Was, As It Is, As It Could Be.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This article was originally published in our “Sport” issue in August, 2016.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&quot;inpagesub&quot;&gt;
&lt;div class=&quot;secthdr&quot;&gt;Get the &lt;span class=&quot;plain&quot;&gt;Nautilus&lt;/span&gt; newsletter&lt;/div&gt;
&lt;p&gt;The newest and most popular articles delivered right to your inbox!&lt;/p&gt;

&lt;/div&gt;
&lt;section class=&quot;leaderboard-ad-belt&quot;&gt;&lt;div class=&quot;leaderboard-ad-belt-inner adarticle&quot;&gt;&lt;div id=&quot;div-gpt-ad-1380044019755-0&quot; class=&quot;leaderboard-ad&quot;/&gt;
&lt;/div&gt;
&lt;/section&gt;</description>
<pubDate>Thu, 28 Jun 2018 22:41:37 +0000</pubDate>
<dc:creator>dnetesn</dc:creator>
<og:type>website</og:type>
<og:url>http://nautil.us/issue/61/coordinates/the-strange-brain-of-the-worlds-greatest-solo-climber-rp</og:url>
<og:title>The Strange Brain of the World’s Greatest Solo Climber - Issue 61: Coordinates - Nautilus</og:title>
<og:description>Alex Honnold has his own verb. “To honnold”—usually written as “honnolding”—is to stand in some high, precarious place&amp;#8230;</og:description>
<og:image>http://static.nautil.us/10082_9b22a40256b079f338827b0ff1f4792b.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>http://nautil.us/issue/61/coordinates/the-strange-brain-of-the-worlds-greatest-solo-climber-rp</dc:identifier>
</item>
<item>
<title>Twitter Will Show Who Pays for Ads and How Much They Spend</title>
<link>https://www.bloomberg.com/news/articles/2018-06-28/twitter-will-show-who-pays-for-ads-and-how-much-they-spend</link>
<guid isPermaLink="true" >https://www.bloomberg.com/news/articles/2018-06-28/twitter-will-show-who-pays-for-ads-and-how-much-they-spend</guid>
<description>[unable to retrieve full-text content]&lt;p&gt;Article URL: &lt;a href=&quot;https://www.bloomberg.com/news/articles/2018-06-28/twitter-will-show-who-pays-for-ads-and-how-much-they-spend&quot;&gt;https://www.bloomberg.com/news/articles/2018-06-28/twitter-will-show-who-pays-for-ads-and-how-much-they-spend&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=17420681&quot;&gt;https://news.ycombinator.com/item?id=17420681&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Points: 357&lt;/p&gt;&lt;p&gt;# Comments: 79&lt;/p&gt;</description>
<pubDate>Thu, 28 Jun 2018 21:36:34 +0000</pubDate>
<dc:creator>champagnepapi</dc:creator>
<og:type>website</og:type>
<og:url>http://nautil.us/issue/61/coordinates/the-strange-brain-of-the-worlds-greatest-solo-climber-rp</og:url>
<og:title>The Strange Brain of the World’s Greatest Solo Climber - Issue 61: Coordinates - Nautilus</og:title>
<og:description>Alex Honnold has his own verb. “To honnold”—usually written as “honnolding”—is to stand in some high, precarious place&amp;#8230;</og:description>
<og:image>http://static.nautil.us/10082_9b22a40256b079f338827b0ff1f4792b.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>http://nautil.us/issue/61/coordinates/the-strange-brain-of-the-worlds-greatest-solo-climber-rp</dc:identifier>
</item>
<item>
<title>Southern Europe Has Not Seen Net Job Creation in over a Decade</title>
<link>http://thesoundingline.com/southern-europe-has-not-seen-net-job-creation-in-over-a-decade/</link>
<guid isPermaLink="true" >http://thesoundingline.com/southern-europe-has-not-seen-net-job-creation-in-over-a-decade/</guid>
<description>&lt;p&gt;&lt;em&gt;Submitted by Taps Coogan on the 4th of April 2018 to The Sounding Line.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It is well known that Southern Europe has endured a particularly weak economic performance over the last decade, yet the magnitude and impact of the economic under-performance is often under-appreciated. Combined Italy, Spain, Portugal, and Greece have a population of over 126 million people and a GDP of roughly $3.7 trillion (2.99 trillion Euros). If the region were a country it would be the 10th most populous in the world and the fourth largest economy after the US, China, and Japan. Yet Southern Europe’s economy has yet to recover from a financial crisis that struck a decade ago. To this day, there are three million fewer employed people in the four countries than in 2008 and that counts anyone who works at least one hour a week as employed. In fact, the number of employed people is still lower than in 2005 despite the population of the region having grown by over three million people since then.&lt;/p&gt;
&lt;a href=&quot;http://thesoundingline.com/wp-content/uploads/2018/04/Total-Employment-in-Italy-Spain-Protugal-and-Greece-Since-1999.jpg&quot;&gt;&lt;img class=&quot;size-full wp-image-18461&quot; src=&quot;http://thesoundingline.com/wp-content/uploads/2018/04/Total-Employment-in-Italy-Spain-Protugal-and-Greece-Since-1999.jpg&quot; alt=&quot;Total Employment in Italy Spain Protugal and Greece Since 1999&quot; width=&quot;677&quot; height=&quot;416&quot; srcset=&quot;http://thesoundingline.com/wp-content/uploads/2018/04/Total-Employment-in-Italy-Spain-Protugal-and-Greece-Since-1999.jpg 677w, http://thesoundingline.com/wp-content/uploads/2018/04/Total-Employment-in-Italy-Spain-Protugal-and-Greece-Since-1999-300x184.jpg 300w, http://thesoundingline.com/wp-content/uploads/2018/04/Total-Employment-in-Italy-Spain-Protugal-and-Greece-Since-1999-400x246.jpg 400w&quot; sizes=&quot;(max-width: 677px) 100vw, 677px&quot;/&gt;&lt;/a&gt;Data Source: OECD
&lt;p&gt;As we discussed &lt;a href=&quot;http://thesoundingline.com/ranking-worlds-economic-growth-21st-century-greece-italy-come-near-last/&quot;&gt;here&lt;/a&gt;, Italy, Portugal, and Greece have seen some of the slowest rates of economic growth in the entire world in the 21st century. The Eurozone’s failure to implement meaningful structural pro-growth reforms, choosing instead to &lt;a href=&quot;https://ec.europa.eu/taxation_customs/sites/taxation/files/taxation_trends_report_2017.pdf&quot;&gt;continue raising taxes&lt;/a&gt; and increasing regulation, has left the entire burden of reviving the region’s economy to the European Central Bank (ECB). To little avail, the ECB has kept interest rates at record negative lows and continues to perform QE and expand its balance sheet.&lt;/p&gt;
&lt;div class=&quot;quads-location quads-ad2&quot; id=&quot;quads-ad2&quot;&gt;&lt;ins class=&quot;adsbygoogle&quot; data-ad-client=&quot;ca-pub-4401495442679385&quot; data-ad-slot=&quot;2384793605&quot; data-ad-format=&quot;auto&quot;/&gt; &lt;/div&gt;
&lt;p&gt;Meanwhile, the much of the rest of the world has experienced one of the longest periods in modern history without a formal recession or financial crisis. Every other major developed economy has seen absolute employment exceed 2008 levels.&lt;/p&gt;
&lt;table width=&quot;339&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td width=&quot;125&quot;&gt;Country&lt;/td&gt;
&lt;td width=&quot;214&quot;&gt;Change in Employment Q1 2008- Q4 2016 (Thousands of Workers)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;125&quot;&gt;USA&lt;/td&gt;
&lt;td width=&quot;214&quot;&gt;5803&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;125&quot;&gt;South Korea&lt;/td&gt;
&lt;td width=&quot;214&quot;&gt;2788&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;125&quot;&gt;UK&lt;/td&gt;
&lt;td width=&quot;214&quot;&gt;2171&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;125&quot;&gt;Australia&lt;/td&gt;
&lt;td width=&quot;214&quot;&gt;1328&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;125&quot;&gt;Canada&lt;/td&gt;
&lt;td width=&quot;214&quot;&gt;1217&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;125&quot;&gt;Japan&lt;/td&gt;
&lt;td width=&quot;214&quot;&gt;727&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;125&quot;&gt;Switzerland&lt;/td&gt;
&lt;td width=&quot;214&quot;&gt;423&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;125&quot;&gt;New Zealand&lt;/td&gt;
&lt;td width=&quot;214&quot;&gt;328&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;When the next recession inevitably arrives, Southern Europe’s economy risks slipping even further into pre-2008 levels and the ECB is horribly positioned to prevent it by applying additional meaningful stimulus.&lt;/p&gt;
&lt;p&gt;P.S. If you would like to be updated via email when we post a new article, please click &lt;a href=&quot;http://thesoundingline.com/taps-coogan-email-subscription-now-available/&quot;&gt;here&lt;/a&gt;. It’s free and we won’t send any promotional materials. &lt;/p&gt;
&lt;div class=&quot;quads-location quads-ad4&quot; id=&quot;quads-ad4&quot;&gt;

&lt;/div&gt;

</description>
<pubDate>Thu, 28 Jun 2018 18:22:43 +0000</pubDate>
<dc:creator>Four_Star</dc:creator>
<og:type>article</og:type>
<og:title>Southern Europe Has Not Seen Net Job Creation in over a Decade - The Sounding Line</og:title>
<og:description>The number of employed people is still lower than in 2005 despite the population of the region having grown by over three million people since then</og:description>
<og:url>http://thesoundingline.com/southern-europe-has-not-seen-net-job-creation-in-over-a-decade/</og:url>
<og:image>http://thesoundingline.com/wp-content/uploads/2018/04/Total-Employment-in-Italy-Spain-Protugal-and-Greece-Since-1999.jpg</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://thesoundingline.com/southern-europe-has-not-seen-net-job-creation-in-over-a-decade/</dc:identifier>
</item>
<item>
<title>How to Make Everything Ourselves: Open Modular Hardware</title>
<link>http://www.lowtechmagazine.com/2012/12/how-to-make-everything-ourselves-open-modular-hardware.html</link>
<guid isPermaLink="true" >http://www.lowtechmagazine.com/2012/12/how-to-make-everything-ourselves-open-modular-hardware.html</guid>
<description>&lt;p&gt;----------------------------------------------------------------------------------------------------------------------------------------------&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------------------------------------------------------------------------------------&lt;/p&gt;
&lt;p&gt;A modular system unites the advantages of standardisation (as parts can be produced cheaply in large amounts) with the advantages of customisation (since a large diversity of unique objects can be made with relatively few parts). Modularity can be found to a greater or lesser extent in many products (like bicycles and computers) and systems (like trains and logistics), but the best examples of modular systems are toys: &lt;a href=&quot;http://en.wikipedia.org/wiki/Lego&quot; target=&quot;_blank&quot;&gt;LEGO&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Meccano&quot; target=&quot;_blank&quot;&gt;Meccano&lt;/a&gt;, and &lt;a href=&quot;http://en.wikipedia.org/wiki/Erector_Set&quot; target=&quot;_blank&quot;&gt;Erector&lt;/a&gt; (which is now the brand name of Meccano in the US).&lt;/p&gt;
&lt;p&gt;LEGO, Meccano and Erector are composed of relatively few elementary building blocks, which can be used to build various objects. The parts can then be disassembled and re-used to build something completely different. Apart from the elementary buildings blocks, these manufacturers have produced many more specific building blocks, which are less versatile, but further increase customisation possibilities.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;asset-img-link&quot; href=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017ee6156f1c970d-pi&quot;&gt;&lt;img class=&quot;asset asset-image at-xid-6a00e0099229e88833017ee6156f1c970d&quot; title=&quot;Afmetingen lego bouwstenen&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017ee6156f1c970d-500wi&quot; alt=&quot;Afmetingen lego bouwstenen&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All the building blocks in a set of LEGO, Meccano or Erector fit together because they are designed according to a set of specific rules. The holes (Meccano and Erector) or studs (LEGO) have a precise diameter and are spaced apart at specific distances. In addition, the dimensions of the building blocks are precisely matched to each other. The long lasting success of LEGO, Meccano and Erector (which appeared on the market in 1947, 1902 and 1911 respectively) is based on the fact that those rules have never changed. All new buildings blocks that were added in the course of the years are compatible with the existing ones. Today, kids can expand their collection of these toys with that of their parents or grandparents, and they are worth as much on the second hand market as they are worth new.&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;Grid Beam, Bit Beam, Open Beam, Maker Beam and Contraptor&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The same principle could be applied to everyday objects, from coffeemakers to furniture, gadgets, cars and renewable energy systems. All you need is a standardisation in design. The design rules can be very simple, as is the case with &lt;a href=&quot;http://www.gridbeam.com/&quot; target=&quot;_blank&quot;&gt;Grid Beam&lt;/a&gt;. This modular construction system, which was developed in 1976, is based on beams with a simple geometry and a repetitive hole-pattern. The beams can be made of wood, aluminium, steel, or any other material.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;asset-img-link&quot; href=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017c347227d0970b-pi&quot;&gt;&lt;img class=&quot;asset asset-image at-xid-6a00e0099229e88833017c347227d0970b&quot; title=&quot;Grid beam high sleeper&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017c347227d0970b-500wi&quot; alt=&quot;Grid beam high sleeper&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In spite of the simplicity of the design, a great variety of objects can be constructed. Grid Beam has been used to make all kinds of furniture, greenhouses, constructions for workshops and industrial processes, windmills, wheelbarrows, agricultural machinery, vehicles, sheds and buildings (a book about the system was published in 2009, and &lt;a href=&quot;http://www.gridbeam.com/a-revolution-in-diy-engineering/&quot; target=&quot;_blank&quot;&gt;can be found online&lt;/a&gt;). Grid Beam was inspired by a system envisioned by Ken Isaacs in the 1950s, &lt;a href=&quot;http://www.publiccollectors.org/CompletePublications.htm&quot; target=&quot;_blank&quot;&gt;Living Structures&lt;/a&gt;, which used similar beams but contained only a few holes.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;asset-img-link&quot; href=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017ee6159c81970d-pi&quot;&gt;&lt;img class=&quot;asset asset-image at-xid-6a00e0099229e88833017ee6159c81970d&quot; title=&quot;Grid beam wheelbarrow&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017ee6159c81970d-500wi&quot; alt=&quot;Grid beam wheelbarrow&quot; /&gt;&lt;/a&gt;&lt;br /&gt;In recent years, several systems have appeared that use a very similar set of rules, based on a repetitive hole pattern. &lt;a href=&quot;http://bitbeam.org/&quot; target=&quot;_blank&quot;&gt;Bit Beam&lt;/a&gt; is basically a scaled-down version of Grid Beam, aimed at building smaller structures in balsa-wood, like a laptop stand or a prototype device. &lt;a href=&quot;http://www.contraptor.org/&quot; target=&quot;_blank&quot;&gt;Contraptor&lt;/a&gt; uses a similar approach, but is aimed at providing structural metal frames for DIY 3D-printers, milling machines, or robotics. &lt;a href=&quot;http://openbeamusa.com/&quot; target=&quot;_blank&quot;&gt;OpenBeam&lt;/a&gt; and &lt;a href=&quot;https://www.sparkfun.com/products/10540?&quot; target=&quot;_blank&quot;&gt;MakerBeam&lt;/a&gt; are also modular construction systems based on very simple rules. These are not based on a hole-pattern, but use T-slot aluminium profiles. &lt;a href=&quot;http://www.kickstarter.com/projects/1397854503/makeblock-next-generation-of-construct-platform&quot; target=&quot;_blank&quot;&gt;Makeblock&lt;/a&gt; combines both approaches and includes electronic modules.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;asset-img-link&quot; href=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017d3ea10e98970c-pi&quot;&gt;&lt;img class=&quot;asset asset-image at-xid-6a00e0099229e88833017d3ea10e98970c&quot; title=&quot;Bitbeam&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017d3ea10e98970c-500wi&quot; alt=&quot;Bitbeam&quot; /&gt;&lt;/a&gt;&lt;br /&gt;Most of these construction systems are limited to the design of frameworks. There is one system, however, that offers much more possibilities, because it is based on a more sophisticated set of rules: &lt;a href=&quot;http://www.openstructures.net/&quot; target=&quot;_blank&quot;&gt;OpenStructures&lt;/a&gt;. The project was kicked off in Brussels in 2007. Unlike all the projects above, OpenStructures is still in an experimental phase. However, it is interesting enough to look at in more detail, because it best shows where modular construction systems may be headed in the future.&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;OpenStructures&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The first basic rule of OpenStructures is shared with Grid Beam and similar systems: all parts are connected to each other in such a way that they can be easily disassembled, using bolts and screws rather than nails or glue. However, the OpenStructures design &quot;language&quot; is different: it is based on the &lt;a href=&quot;http://openstructures.net/pages/9&quot; target=&quot;_blank&quot;&gt;OS Grid&lt;/a&gt;, which is built around a square of 4x4 cm and is scalable. The squares can be further subdivided or put together to form larger squares, without losing inter-compatibility. The illustration below shows nine complete squares of each 4x4 cm put together.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;asset-img-link&quot; href=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017ee615d612970d-pi&quot;&gt;&lt;img class=&quot;asset asset-image at-xid-6a00e0099229e88833017ee615d612970d&quot; title=&quot;OS grid&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017ee615d612970d-700wi&quot; alt=&quot;OS grid&quot; /&gt;&lt;/a&gt;&lt;br /&gt;The borders of the squares mark the cutting lines (which define the dimensions of square parts),  the diagonals determine the assembly points, and the circles define the common diameters. As is the case with LEGO, any modular part has to comply with at least one of these conditions in order to be compatible with other parts. Either the dimensions have to correspond with the horizontal and vertical lines, or the assembly points should be spaced according to the grid, or the diameters should be similar. Below is a part that fulfills two of three conditions.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;asset-img-link&quot; href=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017c347271cd970b-pi&quot;&gt;&lt;img class=&quot;asset asset-image at-xid-6a00e0099229e88833017c347271cd970b&quot; title=&quot;Compatibel onderdeel&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017c347271cd970b-500wi&quot; alt=&quot;Compatibel onderdeel&quot; /&gt;&lt;/a&gt;&lt;br /&gt;While this set of rules is more sophisticated than that of the Grid Beam system, complicated it is not. Nevertheless, it allows for the design of a much larger variety of objects, not just square or rectangular frames. Over the course of five years, OpenStructures has yielded objects ranging from household devices to cargo bicycles, suitcases and furniture.&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;Open versus Closed Modular Systems&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In spite of the similarities, there is one fundamental difference between modular construction systems such as OpenStructures, Grid Beam and Contraptor, and modular toys such as LEGO, Meccano and Erector. The first group consists of &quot;open&quot; modular systems, where everyone is free to design and produce parts, while the second consists of &quot;closed&quot; modular systems, where all parts are designed and produced by one manufacturer. Closed modular systems produce uniform parts. For instance, all LEGO building blocks are made of plastic. LEGO does not produce building blocks made of wood, aluminium, glass or ceramics. There is a limited range of colours. And because LEGO is a closed system, nobody else is allowed to produce LEGO pieces.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;asset-img-link&quot; href=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017ee618926c970d-pi&quot;&gt;&lt;img class=&quot;asset asset-image at-xid-6a00e0099229e88833017ee618926c970d&quot; title=&quot;Closed modular system&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017ee618926c970d-350wi&quot; alt=&quot;Closed modular system&quot; /&gt;&lt;/a&gt; &lt;a class=&quot;asset-img-link&quot; href=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017ee618935a970d-pi&quot;&gt;&lt;img class=&quot;asset asset-image at-xid-6a00e0099229e88833017ee618935a970d&quot; title=&quot;Open modular system&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017ee618935a970d-350wi&quot; alt=&quot;Open modular system&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There exist modular construction systems that operate according to the same principles, like the &lt;a href=&quot;http://www.8020.net/&quot; target=&quot;_blank&quot;&gt;T-profiles&lt;/a&gt; made by 80/20 inc. However, in the modular construction systems that we have introduced above, everyone is allowed to design and produce parts, as long as these parts are compatible with the basic set of rules. We find the same approach with open software, like Linux (an operating system), OpenOffice (office software) or WordPress (a blogging platform). The computer code for these systems is being written by a large amount of people, who all build a part of something larger. Because all participants stick to a basic set of rules, a great amount of people can, independently of one another,  add parts that are inter-compatible.&lt;/p&gt;
&lt;p&gt;----------------------------------------------------------------------------------------------------------------------------------------------&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Consumer products based on an open modular system can foster rapid innovation, without the drawback of wasting energy and materials&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;----------------------------------------------------------------------------------------------------------------------------------------------&lt;/p&gt;
&lt;p&gt;An open modular system has many advantages over a closed modular system. Since anyone can design parts in an open system, it generates a much larger diversity of parts: they can be made in different colours and materials, and none of the producers can set a fixed price for all consumers. And because many designers constantly review, adapt and improve each others' work, innovation is accelerated. All open software systems described above are arguably better than their closed counterparts, and some of them have become more successful. A closed modular system only has one advantage: the one who holds the copyright makes a lot of money.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;Sustainable Consumer Goods&lt;/span&gt;&lt;br /&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Modular construction systems encourage the re-use of physical parts, and thus form a sustainable alternative to our present-day system of producing consumer items. Most products that we buy end up in landfills or incinerators within a couple of years, at most. This is because the majority of manufacturers encourages consumers to replace their products as quickly as possible, either by designing objects that break down easily, or by introducing new generations of products which make the former generation of products obsolete. This approach not only generates a massive pile of waste, it squanders an equally massive amount of energy and raw materials.&lt;/p&gt;
&lt;p&gt;&lt;a&gt;&lt;img title=&quot;Part of OS grid&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017ee616b68d970d-500wi&quot; alt=&quot;Part of OS grid&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Consumer products based on an open modular system can foster rapid innovation, without the drawback of wasting energy and materials. The parts of an obsolete generation of products can be used to design the next generation, or something completely different. Furthermore, modular objects have built-in repairability.&lt;/p&gt;
&lt;p&gt;Open modular construction systems could greatly speed up the diffusion of low-technologies, such as &lt;a href=&quot;http://www.lowtechmagazine.com/2011/05/pedal-powered-farms-and-factories.html&quot; target=&quot;_blank&quot;&gt;pedal-powered machines&lt;/a&gt;, &lt;a href=&quot;http://www.lowtechmagazine.com/2011/07/solar-powered-factories.html&quot; target=&quot;_blank&quot;&gt;solar thermal collectors&lt;/a&gt;, &lt;a href=&quot;http://www.lowtechmagazine.com/velomobiles/&quot; target=&quot;_blank&quot;&gt;velomobiles&lt;/a&gt; or &lt;a href=&quot;http://www.lowtechmagazine.com/2012/09/jobs-of-the-future-cargo-cyclist.html&quot; target=&quot;_blank&quot;&gt;cargo cycles&lt;/a&gt;. Building a windmill or a cargo bike goes much faster when using modular parts than when using carpentry or welding, and there is no need for expensive tools or special skills. Mistakes can be easily corrected -- just unscrew the bolts and start again. It would also be interesting to see modular parts combined with an open hardware project such as the &lt;a href=&quot;http://www.notechmagazine.com/2011/05/how-to-build-your-own-industrial-civilization.html&quot; target=&quot;_blank&quot;&gt;Global Village Construction Set&lt;/a&gt;, which generates many interesting designs but makes limited use of modularity.&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;Circulation of Parts&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&quot;While eBay provides a circulation of objects, and cradle-to-cradle provides a circulation of materials, modular construction systems provide a circulation of parts and components&quot;, says Thomas Lommée, the creator of OpenStructures. &quot;Our ambition is to create puzzles instead of static objects. The system should generate objects of which it is not entirely clear anymore who designed them. An object evolves as it is taken in hands by more designers.&quot;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017ee616d57b970d-pi&quot;&gt;&lt;img title=&quot;Kitchen appliances openstructures&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017ee616d57b970d-700wi&quot; alt=&quot;Kitchen appliances openstructures&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The kitchen appliances that were designed in the context of the project are good examples. A couple of parts were initially made for a coffee grinder, were then used, together with new parts, by another designer to build a coffeemaker. This appliance was then further developed into a water purification device by a third designer. The plastic bottle that served as a water container was replaced by a cut through glass bottle containing a clay filter. Thomas Lommée: &quot;By adding or removing components, or by using them in a different manner, what you get is a family of objects&quot;.&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;Cargo Cycle&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Another prototype that originated from the project, is a cargo cycle. The rear is a sawed through frame of a standard bicycle, the end of which is compatible with the OS Grid. This means that the front of the cycle can be built up in a modular way. Designer Jo Van Bostraeten used this opportunity to design both a cargo bicycle and a cargo tricycle (the latter is carrying a 3D-printer), and it doesn't end there. Together with Lommée, he also constructed a modular motor block. The unit consists of an electric motor and wheels, on top of which a similar unit can be placed that holds a battery. Since the units are compatible with the OS Grid, they can be coupled to the front of the cargo cycle, resulting in a completely modular motorised cargo vehicle.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017c3473a963970b-pi&quot;&gt;&lt;img title=&quot;Openstructures cargo vehicles&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017c3473a963970b-700wi&quot; alt=&quot;Openstructures cargo vehicles&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The latest &quot;family&quot; of objects to come out of the project is aimed at children. It is noteworthy that this collection arose from one component of the cargo cycle -- the container.  It is built up from modular parts that can be bolted together, and can thus be combined in different ways. A couple of designers got started with those parts, resulting in (among other things) a sled, a seat, a toy excavator, and a swing. When the child becomes an adolescent, the parts can be used to make a suitcase or a tool box, or become part of a cargo cycle that could &lt;a href=&quot;http://www.lowtechmagazine.com/2012/09/jobs-of-the-future-cargo-cyclist.html&quot; target=&quot;_blank&quot;&gt;make him or her some pocket money&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a&gt;&lt;img class=&quot;asset asset-image at-xid-6a00e0099229e88833017d3ed22c5c970c&quot; title=&quot;Open source objects&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017d3ed22c5c970c-700wi&quot; alt=&quot;Open source objects&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;More interesting than the objects themselves, is their user support system. Grid Beam is obviously a product from the pre-internet age. Those who want to copy a design are encouraged to look at a picture of someone else's creation and &quot;count the holes&quot;. OpenStructures, on the other hand, leans heavily on online user support. The re-use of parts is being facilitated by an online database that can be used in three ways.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;A Modular Database&lt;/span&gt;&lt;br /&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, you can request an overview of &lt;a href=&quot;http://openstructures.net/pages/14/structures&quot; target=&quot;_blank&quot;&gt;all objects&lt;/a&gt; that were designed based on the OS grid. The webpage for each object then shows you the parts and components from which it is made. Second, you can request an overview of &lt;a href=&quot;http://openstructures.net/pages/44/parts&quot; target=&quot;_blank&quot;&gt;all parts&lt;/a&gt; that were designed based on the OS grid. The webpage for each of these shows you which components and objects they could serve. Third, you can request an overview of &lt;a href=&quot;http://openstructures.net/pages/11/components&quot; target=&quot;_blank&quot;&gt;all components&lt;/a&gt;. The webpage for each component shows you their parts and the objects they can be used for.&lt;/p&gt;
&lt;p&gt;----------------------------------------------------------------------------------------------------------------------------------------------&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Open modular construction does not mean that everyone should make their own consumer products&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;----------------------------------------------------------------------------------------------------------------------------------------------&lt;/p&gt;
&lt;p&gt;The webpage for each part, component and object also gives additional information: the dimensions, the materials, the designer's name, the licence and the order information. To add to this, all parts and components receive a serial number. This means that after a modular object is taken apart, the serial number of each part and component can be entered into the database to see what else can be made with it. Missing parts can be obtained via the database: either by ordering them online, by finding the address of a shop where they sell them, or by downloading the digital design and making them.&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;Not Everyone is a Designer&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Open modular construction does not mean that everyone should make their own consumer products. An object like a coffee maker or a workbench could be obtained in at least three ways. Firstly, the consumer can download the digital design and then assemble the object with parts that he or she buys, re-uses, or makes using a 3D-printer or laser cutter, whether at home or at a fab lab or tech shop. It can also happen in a more low-tech fashion, as is the case with Grid Beam: the consumer buys wood or metal beams, and &lt;a href=&quot;http://www.lowtechmagazine.com/2010/12/hand-powered-drilling-tools-and-machines.html&quot; target=&quot;_blank&quot;&gt;drills the holes himself&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;asset-img-link&quot; href=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017d3eaa499a970c-pi&quot;&gt;&lt;img class=&quot;asset asset-image at-xid-6a00e0099229e88833017d3eaa499a970c&quot; title=&quot;Modular parts water boiler&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017d3eaa499a970c-500wi&quot; alt=&quot;Modular parts water boiler&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A second option is that a company buys the license of the design (if it is not free) and converts it into a building kit, comparable to a kit from LEGO, Meccano or Erector. In this case, the consumer would not have to search for the parts himself, but he still assembles the product himself, just like he would assemble a piece of furniture by IKEA. Similarly, a company could offer a more general building kit, which can be used to make whatever one would like, similar to a box of basic LEGO bricks. Bit Beam, Contraptor, Open Beam, Maker Beam and, &lt;a href=&quot;http://news.cnet.com/8301-32973_3-57440288-296/after-more-than-30-years-grid-beam-modular-construction-system-comes-to-market/&quot; target=&quot;_blank&quot;&gt;recently&lt;/a&gt;, Grid Beam offer one or both of these options.&lt;/p&gt;
&lt;p&gt;The third possibility is that a manufacturer places the object on the market as a finished, assembled product. The coffee maker or the workbench would then be sold and bought just as any other product today, but it can be disassembled after use, and its parts can be re-used for other objects.&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;Economic Model: who Produces the Parts?&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;While the design process behind OpenStructures and other open modular construction systems is identical to that of digital products such as Wikipedia, Linux or WordPress, there is also a fundamental difference. Computer code and digital text accumulate without any material costs. This is not the case with objects. This makes open modular hardware less easy, but it also creates  economic opportunities. It's hard to make money with open software or online writing. However, in the case of an open modular system for objects, someone has to provide the materials.&lt;/p&gt;
&lt;p&gt;It is also important that the parts are produced by as many manufacturers as possible, so that they are available worldwide. Otherwhise, the shipping costs can be so high that a modular object becomes too expensive.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;asset-img-link&quot; href=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017c347b73e6970b-pi&quot;&gt;&lt;img class=&quot;asset asset-image at-xid-6a00e0099229e88833017c347b73e6970b&quot; title=&quot;Modular toaster&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017c347b73e6970b-500wi&quot; alt=&quot;Modular toaster&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There are many opportunities to make money with an open modular construction model. A manufacturer can choose to produce a part in which they sees economic potential. Another manufacturer can choose to sell a building kit or a finished product of a design they think will sell. A designer can make money by uploading a design that might be free to download for personal use, but not for commercial use. A manufacturer that wants to commercialise this design, can then buy the licence from the designer.&lt;/p&gt;
&lt;p&gt;Craftsmen can focus on the design of exclusive, handmade parts in special materials, which are compatible with popular mass produced items. Others can start a fab lab or a tech shop where people can build their own modular objects for a monthly fee. In short, an open modular construction system offers economic opportunities for everybody.&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;Collaborative Economy&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&quot;It is not our ambition to build a gigantic factory that produces all possible parts&quot;, Lommée notes. &quot;OpenStructures should not become a modular IKEA. Our ambition is the creation of a collective economic system, where one producer benefits from the production of another producer. Because parts which are made by one, can be used by another. What we would like to see, are streets full of little shops where everybody generates their own little part of a larger system, a collaborative economy where small, self-employed producers have their place. Not one big player that makes everything. The social dimension is very important.&quot;&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;asset-img-link&quot; href=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017d3eaa4d36970c-pi&quot;&gt;&lt;img class=&quot;asset asset-image at-xid-6a00e0099229e88833017d3eaa4d36970c&quot; title=&quot;Contraptor parts&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017d3eaa4d36970c-500wi&quot; alt=&quot;Contraptor parts&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&quot;If IKEA wants to sell a product that is compatible with our system, then that's fine with me. But the system can only work if it remains open. The larger it becomes, the easier it is for a small company or a craftsman to be a part of it. The ambition is to start a universal, collaborative puzzle that allows the widest possible range of people -- from craftsmen to multinationals -- to design, build and exchange the widest possible range of modular parts and components.&quot;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;Organising Re-use&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Apart from a design language (the OS grid) and an online database, OpenStructures also has set up a prototype of a warehouse in Brussels. This kind of place should become the hub for the organisation of the re-use of parts and components. Think a fab lab or tech shop, but then combined with the storage of modular parts. If a modular product is no longer needed, and the owner does not feel like using the parts to build something new, he or she brings it to one of these places, where it is taken apart, and its parts are stored.&lt;/p&gt;
&lt;p&gt;----------------------------------------------------------------------------------------------------------------------------------------------&lt;/p&gt;
&lt;p&gt;&lt;span&gt;An open modular construction system offers economic opportunities for everybody&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;----------------------------------------------------------------------------------------------------------------------------------------------&lt;/p&gt;
&lt;p&gt;Other people could come to this place to buy parts or to use them on site to build something new. As Lommée says: &quot;Not everyone has to make their own products, but after its useful life, a modular product always comes into the hands of a group of people who like to make things.&quot;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;Compatibility between Open Modular Systems&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;While it is still in an experimental phase, OpenStructures is by far the most ambitious and complete open modular system designed to date. However, being a European project, it follows the international metric system, while the much older Grid Beam follows the imperial system. The systems are not compatible. With more and more open modular systems appearing, would it not be important to provide inter-compatibility between them?&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;asset-img-link&quot; href=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017d3eda36a7970c-pi&quot;&gt;&lt;img class=&quot;asset asset-image at-xid-6a00e0099229e88833017d3eda36a7970c&quot; title=&quot;Makeblock&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017d3eda36a7970c-500wi&quot; alt=&quot;Makeblock&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Lommée doesn't think so: &quot;Most of these systems are designed for different applications. For instance, Contraptor aims at precision, because the parts are used to build robots and other sophisticated machines. Esthetics are clearly not important. I am a designer, so what interests me especially is whether or not a modular system can generate beautiful objects, things you would want to put in your interior. There is also &lt;a href=&quot;http://www.wikispeed.com/&quot; target=&quot;_blank&quot;&gt;Wikispeed&lt;/a&gt;, for instance, which concentrates on the development of a modular car. &lt;a href=&quot;http://www.arduino.cc/&quot; target=&quot;_blank&quot;&gt;Arduino&lt;/a&gt; is aimed at electronics. I don't think that all of these modular systems have to be compatible with each other because the applications are very different.&quot;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017c347b782a970b-pi&quot;&gt;&lt;img title=&quot;Open beam&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e88833017c347b782a970b-500wi&quot; alt=&quot;Open beam&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;He goes on to explain why he chose the metric system. &quot;I have been doubting a lot about this. But in the end I decided that the metric system is easier to work with. And I think the world is big enough for two systems -- just look at the variety of energy standards which are in use. Somebody has developed a &lt;a href=&quot;http://www.contraptor.org/forum/t-273404/contraptor-openstructures&quot; target=&quot;_blank&quot;&gt;European version of Contraptor&lt;/a&gt;, based on the metric system and compatible with the OS grid. And it is always possible to design a coupling between two systems, so that they can be used together. On the other hand, we live in a networked world where everything is connected and copied. This often means that when standards compete, only one survives. And this is not necessarily the best one. I'll keep my fingers crossed.&quot;&lt;/p&gt;
&lt;p&gt;Kris De Decker (edited by &lt;a href=&quot;http://theculturemuncher.com/&quot; target=&quot;_blank&quot;&gt;Deva Lee&lt;/a&gt;). This article is also available in &lt;a href=&quot;http://www.es.lowtechmagazine.com/2013/08/hardware-modular-abierto.html&quot; target=&quot;_blank&quot;&gt;Spanish&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;----------------------------------------------------------------------------------------------------------------------------------------------&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a class=&quot;asset-img-link&quot; href=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e8883301a3fcdec9e1970b-pi&quot;&gt;&lt;img class=&quot;asset asset-image at-xid-6a00e0099229e8883301a3fcdec9e1970b img-responsive&quot; title=&quot;Basket&quot; src=&quot;http://krisdedecker.typepad.com/.a/6a00e0099229e8883301a3fcdec9e1970b-320wi&quot; alt=&quot;Basket&quot; /&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span&gt;Related Articles:&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.lowtechmagazine.com/&quot; target=&quot;_self&quot;&gt;Main page&lt;/a&gt;.&lt;/p&gt;

----------------------------------------------------------------------------------------------------------------------------------------------</description>
<pubDate>Thu, 28 Jun 2018 17:52:52 +0000</pubDate>
<dc:creator>dvfjsdhgfv</dc:creator>
<og:title>How to Make Everything Ourselves: Open Modular Hardware</og:title>
<og:type>blog</og:type>
<og:url>http://www.lowtechmagazine.com/2012/12/how-to-make-everything-ourselves-open-modular-hardware.html</og:url>
<og:description>Reverting to traditional handicrafts is one way to sabotage the throwaway society. In this article, we discuss another possibility: the design of modular consumer products, whose parts and components could be re-used for the design of other products. Initiatives like OpenStructures, Grid Beam, and Contraptor combine the modularity of systems like LEGO, Meccano and Erector with the collaborative power of digital success stories like Wikipedia, Linux or WordPress. An economy based on the concept of re-use would not only bring important advantages in terms of sustainability, but would also save consumers money, speed up innovation, and take manufacturing out of the hands of multinationals. ---------------------------------------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------------------------------------- A modular system unites the advantages of standardisation (as parts can be produced cheaply...</og:description>
<og:image>http://krisdedecker.typepad.com/.a/6a00e0099229e88833017ee61563a5970d-600wi</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.lowtechmagazine.com/2012/12/how-to-make-everything-ourselves-open-modular-hardware.html</dc:identifier>
</item>
<item>
<title>Former Equifax Manager Charged with Insider Trading</title>
<link>https://www.sec.gov/news/press-release/2018-115</link>
<guid isPermaLink="true" >https://www.sec.gov/news/press-release/2018-115</guid>
<description>&lt;p&gt;The Securities and Exchange Commission today charged a former Equifax manager with insider trading in advance of the company’s September 2017 announcement of a massive data breach that exposed Social Security numbers and other personal information of approximately 148 million U.S. customers.   This is the second case the SEC has filed arising from the Equifax data breach.  &lt;a href=&quot;https://www.sec.gov/news/press-release/2018-40&quot;&gt;In March, the former chief information officer of Equifax’s U.S. business unit&lt;/a&gt; was charged with insider trading. &lt;/p&gt;
&lt;p&gt;In a complaint filed in federal court in Atlanta today, the SEC charged that Equifax software engineering manager Sudhakar Reddy Bonthu traded on confidential information he received while creating a website for consumers impacted by a data breach.&lt;/p&gt;
&lt;p&gt;According to the complaint, Bonthu was told the work was being done for an unnamed potential client, but based on information he received, he concluded that Equifax itself was the victim of the breach.  The SEC alleges that Bonthu violated company policy when he traded on the non-public information by purchasing Equifax put options.  Less than a week later, after Equifax publicly announced the data breach and its stock declined nearly 14 percent, Bonthu sold the put options and netted more than $75,000, a return of more than 3,500 percent on his initial investment.&lt;/p&gt;
&lt;p&gt;Bonthu, 44, was terminated from Equifax in March after refusing to cooperate with an internal investigation into whether he had violated the company’s insider trading policy.&lt;/p&gt;
&lt;p&gt;“As we allege, Bonthu, who was entrusted with confidential information by his employer, misused that information to conclude that his company had suffered a massive data breach and then sought to illegally profit,” said Richard R. Best, Director of the SEC’s Atlanta Regional Office.  “Corporate insiders simply cannot abuse their access to sensitive information and illegally enrich themselves.”&lt;/p&gt;
&lt;p&gt;In a parallel proceeding, the U.S. Attorney’s Office for the Northern District of Georgia filed criminal charges against Bonthu.&lt;/p&gt;
&lt;p&gt;To settle the SEC’s civil charges, Bonthu has agreed to a permanent injunction and to return his allegedly ill-gotten gains plus interest.  The settlement is subject to court approval.&lt;/p&gt;
&lt;p&gt;The SEC’s investigation, which is continuing, has been conducted by Elizabeth Skola and Justin Jeffries.  The litigation is being led by Shawn Murnahan and Graham Loomis.  The SEC appreciates the assistance of the U.S. Attorney’s Office for the Northern District of Georgia, Federal Bureau of Investigation, and Financial Industry Regulatory Authority.&lt;/p&gt;
</description>
<pubDate>Thu, 28 Jun 2018 17:05:33 +0000</pubDate>
<dc:creator>moonka</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.sec.gov/news/press-release/2018-115</dc:identifier>
</item>
<item>
<title>We Should Be Building Cities for People, Not Cars (2016)</title>
<link>http://devonzuegel.com/post/we-should-be-building-cities-for-people-not-cars</link>
<guid isPermaLink="true" >http://devonzuegel.com/post/we-should-be-building-cities-for-people-not-cars</guid>
<description>&lt;p&gt;&lt;span&gt;&lt;span&gt;The way we live is shaped by our infrastructure—the public spaces, building codes, and utilities that serve a city or region. It can act as the foundation for thriving communities, but it can also establish unhealthy patterns when designed poorly.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;&lt;colgroup&gt;&lt;col/&gt;&lt;col/&gt;&lt;/colgroup&gt;&lt;tbody readability=&quot;6.9778305621536&quot;&gt;&lt;tr readability=&quot;21.930324623911&quot;&gt;&lt;td readability=&quot;23.923990498812&quot;&gt;
&lt;div readability=&quot;14.851485148515&quot;&gt;For decades, San Francisco’s waterfront was dominated by the massive Embarcadero Freeway. The Ferry Building was hidden in the shadow of a grungy overpass, and the double decker highway blocked residents’ access to the bay all along the eastern edge of the peninsula. One local writer &lt;a href=&quot;https://books.google.com/books?id=-ywDBAAAQBAJ&amp;amp;lpg=PA144&amp;amp;ots=uB1n9Xv9K3&amp;amp;dq=William%20Thompson%20%20%22shunted%20pedestrians%20through%20a%20dark%2C%20sooty%20gauntlet%20between%20downtown%20and%20the%20San%20Francisco%20Bay%22&amp;amp;pg=PA145#v=onepage&amp;amp;q=William%20Thompson%20%20%22shunted%20pedestrians%20through%20a%20dark%2C%20sooty%20gauntlet%20between%20downtown%20and%20the%20San%20Francisco%20Bay%22&amp;amp;f=false&quot;&gt;said&lt;/a&gt; the freeway “shunted pedestrians through a dark, sooty gauntlet between downtown and the San Francisco Bay&quot;.&lt;/div&gt;



&lt;p&gt;Then, in 2002, the freeway was replaced with a palm-lined boulevard and a streetcar line down the middle. The Embarcadero was transformed from a fading industrial zone into a center of tourism and leisure. San Francisco’s stunning waterfront became a destination rather than just a place to pass through.&lt;/p&gt;



&lt;p&gt;The turning point for the Embarcadero was the 1989 Loma Prieta earthquake that destroyed the freeway. Demolition proposals had always been shot down due to fears that it would create unbearable traffic. The quake forced San Franciscans to live without the freeway, and they quickly discovered that the feared congestion never happened. Instead, the neighborhood saw a massive economic boost, and the new boulevard is one of the best parts of the city. By damaging the highway beyond repair, the quake was able to do what political pressure couldn’t.&lt;/p&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div&gt;&lt;img src=&quot;https://postachio-images.s3.amazonaws.com/0bd25fcc-8ab1-40fe-8eef-bcafaae885c1/c6aa6c33-807b-47a8-b315-0530cf601a41/b03d431e-55b7-42b3-a96d-df571c48fbb6.png&quot;/&gt;&lt;br/&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;div&gt;&lt;img src=&quot;https://postachio-images.s3.amazonaws.com/0bd25fcc-8ab1-40fe-8eef-bcafaae885c1/c6aa6c33-807b-47a8-b315-0530cf601a41/42f72a9b-f323-4523-b1ad-1358fda1794b.png&quot;/&gt;&lt;br/&gt;&lt;/div&gt;
&lt;p&gt;&lt;span&gt;San Francisco’s Ferry Building, before and after the Embarcadero Freeway was torn down&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Unfortunately, America’s inherited infrastructure is more like the old Embarcadero Highway than the boulevard that replaced it. Urban planners spent the 20th century building cities for cars, not people, and alternatives to driving have been systemically undervalued. This legacy has resulted in substandard health outcomes, missed economic opportunities, and a shortage of affordable housing.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;We can’t wait around for another earthquake to reverse generations of bad policy. Luckily, it doesn’t require a natural disaster to begin reshaping our infrastructure. Small changes can have an outsized impact in expanding alternatives for how people move around. Rebuilding our infrastructure to enable walking, cycling, and mass transit would bring health and economic benefits that far outweigh its price tag.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Our infrastructure is making us sick&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The American health care crisis is at its heart an urban design crisis. To get a sense for the toll our car-centric infrastructure has taken on our physical and mental well-being:&lt;/span&gt;&lt;/p&gt;

&lt;div&gt;&lt;span&gt;Despite its costs, driving remains the most common mode of travel in the US, at around&lt;/span&gt; &lt;a href=&quot;https://www.census.gov/hhes/commuting/files/2014/acs-32.pdf&quot;&gt;85 percent of commuting trips&lt;/a&gt;&lt;span&gt;.&lt;/span&gt; &lt;a href=&quot;https://books.google.com/books?id=0726UXcSEEsC&amp;amp;pg=PA46&amp;amp;lpg=PA46&amp;amp;dq=Roughly+one+out+of+every+six+American+workers+commutes+more+than+forty-five+minutes,+each+way&amp;amp;source=bl&amp;amp;ots=6GkbbrpgZ9&amp;amp;sig=lT2Vy_433DG8K7i7OHlBrnO5aKc&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ved=0ahUKEwiLvu-Rg7jOAhUH8mMKHcgEBdsQ6AEIHjAA#v=onepage&amp;amp;q=Roughly%20one%20out%20of%20every%20six%20American%20workers%20commutes%20more%20than%20forty-five%20minutes,%20each%20way&amp;amp;f=false&quot;&gt;One out of six workers&lt;/a&gt; &lt;span&gt;commutes more than 45 minutes each way, and private vehicles account for&lt;/span&gt; &lt;a href=&quot;http://bikeleague.org/content/national-household-travel-survey-short-trips-analysis&quot;&gt;60% of trips a mile or shorter&lt;/a&gt;&lt;span&gt;. But it’s not drivers’ fault; rather, our infrastructure encourages — and in many cases forces — us to depend on cars. Our communities are built at the scale of highways rather than humans. Expressways slice through neighborhoods, and widened streets prioritize the flow of cars over making it worth traveling to their destinations. In the 1950s and 60s,&lt;/span&gt; &lt;a href=&quot;http://urbanomnibus.net/2015/06/the-scythe-of-progress-must-move-northward-urban-renewal-on-the-upper-west-side/&quot;&gt;large scale demolitions&lt;/a&gt; &lt;span&gt;were commonplace in order to make way for fast moving motorized traffic, and communities from&lt;/span&gt; &lt;a href=&quot;http://untappedcities.com/2013/12/18/5-things-in-nyc-we-can-blame-on-robert-moses/&quot;&gt;New York&lt;/a&gt; &lt;span&gt;to&lt;/span&gt; &lt;a href=&quot;http://www.slate.com/blogs/the_vault/2014/07/25/transit_history_in_los_angeles_a_1906_map_of_the_city_s_streetcar_system.html&quot;&gt;Los Angeles&lt;/a&gt; &lt;span&gt;have never fully recovered.&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;&lt;span&gt;People choose to drive despite its costs because they lack reasonable alternatives. Unfortunately, this isn’t an accident of history. Our transportation system has been overly focused on automobile traffic flow as its metric of success. This single-minded focus has come at the cost of infrastructure that supports alternative ways to travel. Traffic flow should instead be one goal out of many. Communities would be far healthier if our infrastructure actively encouraged walking, cycling, and other forms of transportation rather than subsidizing driving and ignoring alternatives.&lt;/span&gt;&lt;/p&gt;
&lt;div&gt;&lt;img src=&quot;https://postachio-images.s3.amazonaws.com/0bd25fcc-8ab1-40fe-8eef-bcafaae885c1/c6aa6c33-807b-47a8-b315-0530cf601a41/160edf3b-a301-4027-96da-0d8fc741f6f0.png&quot;/&gt;&lt;br/&gt;&lt;/div&gt;

&lt;p&gt;&lt;span&gt;Walkability has major economic benefits&lt;/span&gt;&lt;/p&gt;
&lt;div&gt;&lt;span&gt;It makes financial sense for cities to invest in walkability. There is a&lt;/span&gt; &lt;a href=&quot;http://jpe.sagepub.com/content/24/3/317.abstract&quot;&gt;scarcity of walkable neighborhoods&lt;/a&gt; &lt;span&gt;in the US housing market, and this unmet demand is reflected in the prices people are willing to pay for housing in those cities that embrace a pedestrian-friendly model. Cities that remove barriers to pedestrian-centric development have an economic edge on those that do not.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;Having a critical mass of pedestrians signals that a neighborhood is safe and interesting, which in turn draws more people into that area. This creates a positive feedback loop — with people and opportunities nearby, it becomes increasingly attractive for businesses and residents to move in. Walkability&lt;/span&gt; &lt;a href=&quot;https://www.redfin.com/blog/2009/08/new-study-shows-one-point-of-walk-score-worth-up-to-3000.html&quot;&gt;correlates&lt;/a&gt; &lt;span&gt;with property values so closely that it has become standard practice in real estate to advertise a home’s “walk score&quot;, because a high grade earns a premium. Websites like Zillow and&lt;/span&gt; &lt;a href=&quot;http://Apartments.com&quot;&gt;Apartments.com&lt;/a&gt;&lt;a href=&quot;https://www.walkscore.com/professional/case-studies.php&quot;&gt;offer&lt;/a&gt; &lt;span&gt;calculations for commute length and walkability to everyday destinations like grocery stores, and it is common for real estate agents to advertise the walk score as a major selling point.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;The Embarcadero in San Francisco is a perfect illustration of how transitioning away from car-centric infrastructure can revitalize a neighborhood. When the highway was removed in 2002,&lt;/span&gt; &lt;a href=&quot;https://reclaimingoldwestbroad.files.wordpress.com/2014/04/chapter-9-appendix-1-related-articles-and-case-studies.pdf&quot;&gt;the area experienced a massive economic boost&lt;/a&gt;&lt;span&gt;. Within four years, housing in the area increased by 51% and jobs increased by 23%, and by freeing up land for development, the highway removal has also increased the city’s tax base.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;Portland offers another helpful model. In contrast to most American cities, Portland has invested in multiple modes of transportation for decades. As a result, its residents drive 20% less than other major metropolitan areas, and the city has experienced economic benefits beyond just a rise in property values. “Portlanders save a bundle on cars and gas, [so] residents have more money to spend on other things they value, which in turn stimulates the local economy,&quot; said economist Joe Cortright in&lt;/span&gt; &lt;a href=&quot;http://blog.oregonlive.com/commuting/2009/09/pdxgreendividend.pdf&quot;&gt;Portland’s Green Dividend&lt;/a&gt;&lt;span&gt;. Less driving leads to greater savings, and those savings flow to the local economy rather than distant oil producers.&lt;/span&gt;&lt;/div&gt;

&lt;p&gt;&lt;span&gt;Walkability makes communities more affordable&lt;/span&gt;&lt;/p&gt;
&lt;div&gt;&lt;span&gt;Walkable, transit-friendly cities like New York, Chicago, and San Francisco are infamous for their high housing prices. However, when housing and transportation costs are considered together,&lt;/span&gt; &lt;a href=&quot;http://www.huffingtonpost.com/f-kaid-benfield/how-transit-walkability-h_b_5704997.html&quot;&gt;these places are far more affordable than sprawling cities&lt;/a&gt;&lt;span&gt;. It is true that rents and property values decrease as you move farther from the city center, but the added costs of owning and operating a car quickly burns through whatever savings came from moving away.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;This calculation is even more clear when you also consider the opportunity cost of driving. Time spent in traffic means you have less time to do other things that you care about. “There’s a simple rule of thumb: Every 10 minutes of commuting results in 10% fewer social connections,&quot;&lt;/span&gt; &lt;a href=&quot;http://www.newyorker.com/reporting/2007/04/16/070416fa_fact_paumgarten&quot;&gt;said&lt;/a&gt; &lt;span&gt;Harvard political scientist Robert Putnam. “Commuting is connected to social isolation, which causes unhappiness.&quot; Workers with long commutes are also less productive. Employee absenteeism would be about&lt;/span&gt; &lt;a href=&quot;http://www.academia.edu/277760/Are_workers_with_a_long_commute_less_productive_An_empirical_analysis_of_absenteeism&quot;&gt;15–20%&lt;/a&gt; &lt;span&gt;less if all workers had a negligible commute, and workers with a long commute are more likely to leave early or arrive late.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;img src=&quot;https://postachio-images.s3.amazonaws.com/0bd25fcc-8ab1-40fe-8eef-bcafaae885c1/c6aa6c33-807b-47a8-b315-0530cf601a41/565a9b2b-fec5-42fc-bedc-25b31dded3f5.png&quot;/&gt;&lt;br/&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;Affordable housing would be far more attainable if our infrastructure encouraged walking and alternative forms of transportation. Parking minimums are one example of how car-centric infrastructure works against this goal. A standard parking spot occupies as much square footage as a small studio apartment, and that’s without factoring in the driveway space required for maneuvering into the spot. All in all, shops in California are required to provide 288 square feet of parking space for every 250 square feet of retail space — 115% more space is required to accommodate the parking for any new retail development. In Los Angeles,&lt;/span&gt; &lt;a href=&quot;http://www.citylab.com/commute/2015/12/parking-los-angeles-maps-study/418593/&quot;&gt;3.3 spaces exist for every car in the city&lt;/a&gt;&lt;span&gt;. Other states have similar or worse requirements. As a result, acreage devoted to parking hovers around&lt;/span&gt; &lt;a href=&quot;http://www.autolife.umd.umich.edu/Environment/E_Casestudy/E_casestudy2.htm&quot;&gt;30% of the ground area in the core of the average sun belt city&lt;/a&gt;&lt;span&gt;. This is area that could otherwise be used for some other productive use. If we could make even a tiny dent in the amount of space consumed by parking, we could greatly improve the affordability of cities.&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;&lt;span&gt;These parking minimums not only consume valuable space — they also induce more driving. With so much area devoted to parking, the distance between destinations increases, which in turn increases the necessity of driving. With more drivers doing more traveling, more parking spots are needed to accommodate their cars, continuing this vicious cycle. On the bright side, this also means that the benefits of changing these requirements are compounding. If we loosen requirements, we would also see a reduction in demand for parking spots, because the demand for driving itself would decrease as destinations cluster closer together.&lt;/span&gt;&lt;/p&gt;
&lt;div&gt;&lt;span&gt;Fun fact: SimCity was forced to pretend that all parking lots were underground, because the game would be “really boring if it was proportional in terms of parking lots&quot;. SimCity’s lead designer&lt;/span&gt; &lt;a href=&quot;http://www.theatlantic.com/technology/archive/2013/05/the-philosophy-of-simcity-an-interview-with-the-games-lead-designer/275724/&quot;&gt;explained&lt;/a&gt;&lt;span&gt;, “I was blown away by how much more space was parking lot rather than actual store. That was kind of a problem, because we were originally just going to model real cities … We had to do the best we could do and still make the game look attractive.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;img src=&quot;https://postachio-images.s3.amazonaws.com/0bd25fcc-8ab1-40fe-8eef-bcafaae885c1/c6aa6c33-807b-47a8-b315-0530cf601a41/eee8af00-0f30-4896-af41-d65fe7c13a85.png&quot;/&gt;&lt;br/&gt;&lt;/div&gt;
&lt;div&gt;&lt;img src=&quot;https://postachio-images.s3.amazonaws.com/0bd25fcc-8ab1-40fe-8eef-bcafaae885c1/c6aa6c33-807b-47a8-b315-0530cf601a41/964cd6c9-b945-480d-8e1b-47576003a23e.png&quot;/&gt;&lt;/div&gt;

&lt;p&gt;&lt;span&gt;Tactical, concentrated improvements have an outsized impact&lt;/span&gt;&lt;/p&gt;
&lt;div&gt;&lt;span&gt;It is feasible — both financially and politically — to make cities more walkable. Simple, low-cost improvements to features like&lt;/span&gt; &lt;a href=&quot;http://usa.streetsblog.org/2015/04/20/which-matters-more-a-bike-networks-connectivity-or-its-density/&quot;&gt;painted bike lanes&lt;/a&gt;&lt;span&gt;,&lt;/span&gt; &lt;a href=&quot;http://onlinepubs.trb.org/onlinepubs/conferences/2015/ActiveTransportation/Presentations/Janet%20Fulton.pdf&quot;&gt;wayfinding signs&lt;/a&gt;&lt;span&gt;,&lt;/span&gt; &lt;a href=&quot;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=11&amp;amp;ved=0ahUKEwjo2cnFj7rOAhXI7xQKHeZhDtgQFghUMAo&amp;amp;url=http%3A%2F%2Famericanhistory.si.edu%2Fblog%2Fsmashing-barriers-access-disability-activism-and-curb-cuts&amp;amp;usg=AFQjCNHKLn48DZJb37DQR_F8yjDJT9bGdQ&amp;amp;sig2=uriDBv2RDJNjBOTcmsBdUA&amp;amp;cad=rja&quot;&gt;curb cuts&lt;/a&gt;&lt;span&gt;, and&lt;/span&gt; &lt;a href=&quot;https://catalog.data.gov/dataset/enviroatlas-des-moines-ia-estimated-percent-tree-cover-along-walkable-roads&quot;&gt;tree coverage&lt;/a&gt; &lt;span&gt;have an disproportionate impact in transforming a car-dependent metropolis into human-scale, walkable neighborhoods. It is rare that cities find a goal that is both worthwhile and attainable, so urban planners should jump on this opportunity.&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;&lt;span&gt;Concentrated efforts to revitalize a small area can have a massive impact on an entire city. Mountain View, where I grew up, is a great example of this. When I was a kid, the town’s suburban sprawl required a car ride to get anywhere. However, in the past few years, resources have been directed towards making specific areas in and around downtown more walkable, and residents all over town have seen the benefits. In the summer, the main street is closed for weekly street festivals. Downtown parking spots have been replaced with cafe seating, separated from the road by planter beds. The Stevens Creek Trail connects otherwise isolated neighborhoods to these walkable zones, providing a corridor extending from the marshlands at the edge of the city into the heart of Mountain View and through three other neighboring cities. This corridor lets residents all over town benefit from efforts concentrated in a few small areas of the city. Most streets are still oriented towards cars, but few neighborhoods are more than a short stroll away from a connection to pedestrian-friendly zone. When used strategically, limited resources can revitalize the walkability of an entire community.&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;&lt;colgroup&gt;&lt;col/&gt;&lt;col/&gt;&lt;/colgroup&gt;&lt;tbody readability=&quot;3&quot;&gt;&lt;tr readability=&quot;9&quot;&gt;&lt;td readability=&quot;7&quot;&gt;
&lt;div&gt;&lt;img src=&quot;https://postachio-images.s3.amazonaws.com/0bd25fcc-8ab1-40fe-8eef-bcafaae885c1/c6aa6c33-807b-47a8-b315-0530cf601a41/c4b6ca13-c581-4b5b-a4ba-d784a8215fe5.png&quot;/&gt;&lt;br/&gt;&lt;/div&gt;
&lt;p&gt;&lt;span&gt;Every Sunday, Mountain View’s parking lot at the Caltrain station transforms into a farmers market&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td readability=&quot;5&quot;&gt;
&lt;div&gt;&lt;img src=&quot;https://postachio-images.s3.amazonaws.com/0bd25fcc-8ab1-40fe-8eef-bcafaae885c1/c6aa6c33-807b-47a8-b315-0530cf601a41/46643c9b-37ed-47d8-8371-e178eb90f4c9.png&quot;/&gt;&lt;br/&gt;&lt;/div&gt;
&lt;p&gt;&lt;span&gt;Brightly painted bike lanes on Market St have made San Francisco far more bike-friendly&lt;/span&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div&gt;&lt;span&gt;Communities all around the country have begun to to experiment with&lt;/span&gt; &lt;a href=&quot;http://www.tacticalurbanismguide.com/about-the-project/&quot;&gt;retrofitting their built environments&lt;/a&gt; &lt;span&gt;in other small ways. The changes don’t have to be radical — they can be as small as repainting crosswalks or planting more trees to provide shade from the sun. California&lt;/span&gt; &lt;a href=&quot;http://www.arb.ca.gov/planning/tsaq/cashout/cashout.htm&quot;&gt;requires&lt;/a&gt; &lt;span&gt;employers who provide parking for employees to offer a cash allowance in lieu of a parking space.&lt;/span&gt; &lt;a href=&quot;https://www.ams.usda.gov/press-release/national-farmers-market-manager-survey-shows-farmers-markets-continue-grow&quot;&gt;It has become common&lt;/a&gt; &lt;span&gt;for downtowns to close Main Street for farmers markets and street fairs, and food trucks are a popular way to attract people to public spaces without the risk and hassle of setting up permanent shops. Improvements can range from grassroots efforts to add seating in public parks to city projects to reshape road networks and everything in between. Each community is different in its needs, resources, and existing infrastructure, so I find it particularly exciting to see how different towns come up with creative solutions that fit their particular situations.&lt;/span&gt;&lt;/div&gt;
&lt;p&gt;&lt;span&gt;Walkability leads to better health outcomes, more affordable housing, and greater economic vitality. The best part is that easy, low-cost changes can go a long way in improving the options for how people move around their communities. Cities all around the country have begun to appreciate and capitalize on alternative modes of transportation, especially walking. We still have decades of auto-dependent land use trends to offset, but small shifts in the way our received infrastructure is designed can go a long way to making our neighborhoods more walkable — and in turn healthier, more affordable, and more vibrant.&lt;/span&gt;&lt;/p&gt;

&lt;hr/&gt;
&lt;p&gt;&lt;span&gt;Suggested reading&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;If you’d like to learn more about the benefits, history, and challenges of alternative transportation, here are a few resources to get you started:&lt;/span&gt;&lt;/p&gt;
&lt;div&gt;&lt;span&gt;Originally published on&lt;/span&gt; &lt;a href=&quot;https://medium.com/by-the-bay/walkable-cities-5b2d8766d0e2&quot;&gt;Medium&lt;/a&gt;&lt;/div&gt;
</description>
<pubDate>Thu, 28 Jun 2018 17:05:15 +0000</pubDate>
<dc:creator>mrzool</dc:creator>
<og:image>https://postachio-images.s3.amazonaws.com/0bd25fcc-8ab1-40fe-8eef-bcafaae885c1/c6aa6c33-807b-47a8-b315-0530cf601a41/b03d431e-55b7-42b3-a96d-df571c48fbb6.png</og:image>
<og:type>article</og:type>
<og:title>We Should Be Building Cities for People, Not Cars</og:title>
<og:url>http://devonzuegel.com/post/we-should-be-building-cities-for-people-not-cars</og:url>
<og:description>The way we live is shaped by our infrastructure—the public spaces, building codes, and utilities that serve a city or region. It can act as the foundation for thriving communities, but it can also establish unhealthy patterns when designed poorly.For...</og:description>
<dc:format>text/html</dc:format>
<dc:identifier>http://devonzuegel.com/post/we-should-be-building-cities-for-people-not-cars</dc:identifier>
</item>
<item>
<title>A Bank for Student Hackers</title>
<link>https://medium.com/hackclub/a-bank-for-student-hackers-e5d894ea5375</link>
<guid isPermaLink="true" >https://medium.com/hackclub/a-bank-for-student-hackers-e5d894ea5375</guid>
<description>&lt;div class=&quot;section-inner sectionLayout--insetColumn&quot; readability=&quot;28.318385650224&quot;&gt;
&lt;p name=&quot;b604&quot; id=&quot;b604&quot; class=&quot;graf graf--p graf--leading&quot;&gt;Kapaa High School isn’t an isolated case. In fact, high school students in general have no way to safely store money for clubs and non-profit activities. Schools, banks and local organizations lack the infrastructure to give students access to safe, tax-free and fee-exempt bank accounts. To do it yourself, you must incorporate and seek non-profit status which can take over a year and thousands of dollars in the United States.&lt;/p&gt;
&lt;p name=&quot;d0d3&quot; id=&quot;d0d3&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;This puts students in a tough spot. Sure, you can secure the money to run a hackathon or an event with your club, but where do you put it?&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*5io9DTGiK7rV5vQsbcNTBw.jpeg&quot; data-width=&quot;2048&quot; data-height=&quot;1365&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*5io9DTGiK7rV5vQsbcNTBw.jpeg&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*5io9DTGiK7rV5vQsbcNTBw.jpeg&quot;/&gt;&lt;/div&gt;
From &lt;a href=&quot;https://hackhappyvalley.com/&quot; data-href=&quot;https://hackhappyvalley.com/&quot; class=&quot;markup--anchor markup--figure-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Hack Happy Valley&lt;/a&gt; ran by the &lt;a href=&quot;http://club.hackhappyvalley.com/&quot; data-href=&quot;http://club.hackhappyvalley.com/&quot; class=&quot;markup--anchor markup--figure-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Hack Club at State College Area High School&lt;/a&gt;.
&lt;p name=&quot;50eb&quot; id=&quot;50eb&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Today, I’m proud to share the launch of the&lt;/strong&gt; &lt;a href=&quot;https://hackclub.com/bank&quot; data-href=&quot;https://hackclub.com/bank&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Hack Club Bank&lt;/strong&gt;&lt;/a&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;. Students running events can now choose to store their money with Hack Club and get the full benefits of the backing of a 501(c)(3) non-profit.&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;3463&quot; id=&quot;3463&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Here’s how it works. Students who choose to store their money with Hack Club get access to a dashboard where they can see a financial overview of their event.&lt;/p&gt;
&lt;p name=&quot;ab86&quot; id=&quot;ab86&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;From the dashboard, event organizers can do three crucial things:&lt;/p&gt;
&lt;ol class=&quot;postList&quot;&gt;&lt;li name=&quot;8f82&quot; id=&quot;8f82&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Account balance:&lt;/strong&gt; See the event’s real-time account balance and transaction history&lt;/li&gt;
&lt;li name=&quot;4a4b&quot; id=&quot;4a4b&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Invoice:&lt;/strong&gt; Collect payments from sponsors via debit card, credit card, domestic wire transfer, and even ACH&lt;/li&gt;
&lt;li name=&quot;93b3&quot; id=&quot;93b3&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Debit cards:&lt;/strong&gt; Issue pre-loaded debit cards to fellow team members&lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;
&lt;div class=&quot;section-inner sectionLayout--outsetColumn&quot;&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*ZXfxfzOd2SFO3Wn-Yqklng.png&quot; data-width=&quot;2164&quot; data-height=&quot;1500&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*ZXfxfzOd2SFO3Wn-Yqklng.png&quot; src=&quot;https://cdn-images-1.medium.com/max/2000/1*ZXfxfzOd2SFO3Wn-Yqklng.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;section-inner sectionLayout--insetColumn&quot; readability=&quot;32.404895461499&quot;&gt;
&lt;p name=&quot;df00&quot; id=&quot;df00&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;When designing Hack Club Bank, one of the things most important to us was automating as much as possible&lt;em class=&quot;markup--em markup--p-em&quot;&gt;.&lt;/em&gt; You should never have to wait for someone from Hack Club to find out your account balance. You should never have to wait for someone to issue an invoice for you. You should never have to go through a lengthy human-managed expense reimbursement process.&lt;/p&gt;
&lt;p name=&quot;15ed&quot; id=&quot;15ed&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;That said, some things are inevitable. There’s nothing worse than needing something last-minute and having the person managing your financials go dark.&lt;/p&gt;
&lt;p name=&quot;75c8&quot; id=&quot;75c8&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;As part of Hack Club Bank, every event gets a dedicated point-of-contact and a best-effort 12 hour response window with a pre-defined escalation policy. &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Hack Club will never go dark on events.&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*v90EpimUWPlVPQJ5rTxluw.jpeg&quot; data-width=&quot;4928&quot; data-height=&quot;3264&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*v90EpimUWPlVPQJ5rTxluw.jpeg&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*v90EpimUWPlVPQJ5rTxluw.jpeg&quot;/&gt;&lt;/div&gt;
From Hack the Fog hosted by Lowell High School’s Hack Club. &lt;a href=&quot;https://www.sfchronicle.com/bayarea/article/Hack-the-Fog-makes-history-as-San-12729895.php&quot; data-href=&quot;https://www.sfchronicle.com/bayarea/article/Hack-the-Fog-makes-history-as-San-12729895.php&quot; class=&quot;markup--anchor markup--figure-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;See the SF Chronicle’s article on the event.&lt;/a&gt;
&lt;p name=&quot;f8b6&quot; id=&quot;f8b6&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Here’s a full list of everything provided as part of Hack Club Bank:&lt;/p&gt;
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;af55&quot; id=&quot;af55&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;An underlying bank account with 501(c)(3) status, so donations to events are tax deductible&lt;/li&gt;
&lt;li name=&quot;f3c2&quot; id=&quot;f3c2&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Invoicing software that allows sponsors to pay via debit card, credit card, wire transfer, and even ACH&lt;/li&gt;
&lt;li name=&quot;0c58&quot; id=&quot;0c58&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Pre-loaded debit cards that can be issued to team members. No more waiting for lengthy reimbursement processes.&lt;/li&gt;
&lt;li name=&quot;8b4d&quot; id=&quot;8b4d&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;G Suite accounts for each team member, so everyone gets a custom email address like &lt;a href=&quot;mailto:megan@hackchicago.io&quot; data-href=&quot;mailto:megan@hackchicago.io&quot; class=&quot;markup--anchor markup--li-anchor&quot; target=&quot;_blank&quot;&gt;megan@hackchicago.io&lt;/a&gt;&lt;/li&gt;
&lt;li name=&quot;42e9&quot; id=&quot;42e9&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Pre-written liability and photo releases for attendees&lt;/li&gt;
&lt;li name=&quot;ee0b&quot; id=&quot;ee0b&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Best-effort 12 hour response window and a dedicated point of contact&lt;/li&gt;
&lt;li name=&quot;d866&quot; id=&quot;d866&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;End of year tax filings, so events can focus on running a great event and we can take care of the backend&lt;/li&gt;
&lt;/ul&gt;&lt;p name=&quot;388a&quot; id=&quot;388a&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;While we’re still in the early days, we’ve already signed on four upcoming high school hackathons with combined budgets of over $50,000.&lt;/p&gt;
&lt;p name=&quot;0f7b&quot; id=&quot;0f7b&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;To start, we’re allowing students already working with Hack Club to invite new events to Hack Club Bank. Down the road, we’ll be offering access to Hack Club Bank to the public through an open application process.&lt;/p&gt;
&lt;h3 name=&quot;8d52&quot; id=&quot;8d52&quot; class=&quot;graf graf--h3 graf-after--p graf--trailing&quot;&gt;Go to &lt;a href=&quot;https://hackclub.com/bank&quot; data-href=&quot;https://hackclub.com/bank&quot; class=&quot;markup--anchor markup--h3-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;https://hackclub.com/bank&lt;/a&gt; to learn more about how to get started.&lt;/h3&gt;
&lt;/div&gt;
</description>
<pubDate>Thu, 28 Jun 2018 15:20:42 +0000</pubDate>
<dc:creator>zachlatta</dc:creator>
<og:title>Hack Club Bank: A Bank For Student Hackers – Hack Club – Medium</og:title>
<og:url>https://medium.com/hackclub/hack-club-bank-a-bank-for-student-hackers-e5d894ea5375</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*K3dUuu0bnxMZQgV4m2agvw.jpeg</og:image>
<og:description>It was February 2nd, 2018 at 2 PM when I got a call from Jake Reilly, the founder of Kapaa High School's Hack Club, our first chapter in…</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://medium.com/hackclub/hack-club-bank-a-bank-for-student-hackers-e5d894ea5375</dc:identifier>
</item>
</channel>
</rss>