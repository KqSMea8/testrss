<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Xray – An experimental next-generation Electron-based text editor</title>
<link>https://github.com/atom/xray</link>
<guid isPermaLink="true" >https://github.com/atom/xray</guid>
<description>&lt;h3&gt;README.md&lt;/h3&gt;
&lt;article class=&quot;markdown-body entry-content&quot; itemprop=&quot;text&quot;&gt;
&lt;p&gt;Xray is an experimental Electron-based text editor informed by what we've learned in the four years since the launch of Atom. In the short term, this project is a testbed for rapidly iterating on several radical ideas without risking the stability of Atom. The longer term future of the code in this repository will become clearer after a few months of progress. For now, our primary goal is to iterate rapidly and learn as much as possible.&lt;/p&gt;
&lt;h2&gt;Updates&lt;/h2&gt;
&lt;h2&gt;Foundational priorities&lt;/h2&gt;
&lt;p&gt;Our goal is to build a cross-platform text editor that is designed from the beginning around the following foundational priorities:&lt;/p&gt;
&lt;h3&gt;High performance&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Xray feels lightweight and responsive.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We design our features to be responsive from the beginning. We reliably provide visual feedback within the latency windows suggested by the &lt;a href=&quot;https://developers.google.com/web/fundamentals/performance/rail&quot; rel=&quot;nofollow&quot;&gt;RAIL performance model&lt;/a&gt;. For all interactions, we shoot for the following targets on the hardware of our median user:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Duration&lt;/th&gt;
&lt;th&gt;Action&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;8ms&lt;/td&gt;
&lt;td&gt;Scrolling, animations, and fine-grained interactions such as typing or cursor movement.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;50ms&lt;/td&gt;
&lt;td&gt;Coarse-grained interactions such as opening a file or initiating a search. If we can't complete the action within this window, we should show a progress bar.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;150ms&lt;/td&gt;
&lt;td&gt;Opening an application window.&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;We are careful to maximize throughput of batch operations such as project-wide search. Memory consumption is kept within a low constant factor of the size of the project and open buffer set, but we trade memory for speed and extensibility so long as memory requirements are reasonable.&lt;/p&gt;
&lt;h3&gt;Collaboration&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Xray makes it as easy to code together as it is to code alone.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We design features for collaborative use from the beginning. Editors and other relevant UI elements are designed to be occupied by multiple users. Interactions with the file system and other resources such as subprocesses are abstracted to work over network connections.&lt;/p&gt;
&lt;h3&gt;Extensibility&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Xray gives developers control over their own tools.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We expose convenient and powerful APIs to enable users to add non-trivial functionality to the application. We balance the power of our APIs with the ability to ensure the responsiveness, stability, and security of the application as a whole. We avoid leaking implementation details and use versioning where possible to enable a sustained rapid development without destabilizing the package ecosystem.&lt;/p&gt;
&lt;h3&gt;Web compatibility&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Editing on GitHub feels like editing in Xray.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We provide a feature-rich editor component that can be used on the web and within other Electron applications. This will ultimately help us provide a more unified experience between GitHub.com and this editor and give us a stronger base of stakeholders in the core editing technology. If this forces serious performance compromises we may potentially drop this objective, but we don't think that it will.&lt;/p&gt;
&lt;h2&gt;Architecture&lt;/h2&gt;
&lt;p&gt;Martin Fowler defines software architecture those decisions which are both important and hard to change. Since these decisions are hard to change, we need to be sure that our foundational priorities are well-served by these decisions.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/atom/xray/blob/master/docs/images/architecture.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://github.com/atom/xray/raw/master/docs/images/architecture.png&quot; alt=&quot;Architecture&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;The UI is built with Electron&lt;/h3&gt;
&lt;p&gt;Electron adds a lot of overhead, which detracts from our top priority of high-performance. However, Electron is also the best approach that we know of to deliver a cross-platform, extensible user interface. Atom proved that developers want to add non-trivial UI elements to their editor, and we still see web technologies as the most viable way to offer them that ability. We also want to provide extension authors with a scripting API, and the JavaScript VM that ships with Electron is well suited to that task.&lt;/p&gt;
&lt;p&gt;The fundamental question is whether we can gain Electron's benefits for extensibility while still meeting our desired performance goals. Our hypothesis is that it's possible–with the right architecture.&lt;/p&gt;
&lt;h3&gt;Core application logic is written in Rust&lt;/h3&gt;
&lt;p&gt;The core of the application is implemented in a pure Rust crate (&lt;code&gt;/xray_core&lt;/code&gt;) and made accessible to JavaScript through N-API bindings (&lt;code&gt;/xray_node&lt;/code&gt;). This module is loaded into the Electron application (&lt;code&gt;/xray_electron&lt;/code&gt;) via Node's native add-on system. The binding layer will be responsible for exposing a thread-safe API to JS so that the same native module can be used in the render thread and worker threads.&lt;/p&gt;
&lt;p&gt;All of the core application code other than the view logic should be written in Rust. This will ensure that it has a minimal footprint to load and execute, and Rust's robust type system will help us maintain it more efficiently than dynamically typed code. A language that is fundamentally designed for multi-threading will also make it easier to exploit parallelism whenever the need arises, whereas JavaScript's single-threaded nature makes parallelism awkward and challenging.&lt;/p&gt;
&lt;p&gt;Fundamentally, we want to spend our time writing in a language that is fast by default. It's true that it's possible to write slow Rust also possible to write fast JavaScript. It's &lt;em&gt;also&lt;/em&gt; true that it's much harder to write slow Rust than it is to write slow JavaScript. By spending fewer resources on the implementation of the platform itself, we'll make more resources available to run package code.&lt;/p&gt;
&lt;h3&gt;Packages will run primarily in worker threads&lt;/h3&gt;
&lt;p&gt;A misbehaving package should not be able to impact the responsiveness of the application. The best way to guarantee this while preserving ease of development is to activate packages on worker threads. We can do a worker thread per package or run packages in their own contexts across a pool of threads.&lt;/p&gt;
&lt;p&gt;Packages &lt;em&gt;can&lt;/em&gt; run code on the render thread by specifying versioned components in their &lt;code&gt;package.json&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-json&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;components&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: {
  &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;TodoList&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;./components/todo-list.js&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If a package called &lt;code&gt;my-todos&lt;/code&gt; had the above entry in its &lt;code&gt;package.json&lt;/code&gt;, it could request that the workspace attach that component by referring to &lt;code&gt;myTodos.TodoList&lt;/code&gt; when adding an item. On package installation, we can automatically update the V8 snapshot to include the components of every installed package. Components will only be dynamically loaded from the provided paths in development mode.&lt;/p&gt;
&lt;p&gt;APIs for interacting with the core application state and the underlying operating system will only be available within worker threads, discouraging package authors from putting too much logic into their views. We'll use a combination of asynchronous channels and CRDTs to present convenient APIs to package authors within worker threads.&lt;/p&gt;
&lt;h3&gt;Text is stored in a copy-on-write CRDT&lt;/h3&gt;
&lt;p&gt;To fully exploit Rust's unique advantage of parallelism, we need to store text in a concurrency-friendly way. We use a variant of RGA called RGASplit, which is described in &lt;a href=&quot;https://pages.lip6.fr/Marc.Shapiro/papers/rgasplit-group2016-11.pdf&quot; rel=&quot;nofollow&quot;&gt;this research paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/atom/xray/blob/master/docs/images/crdt.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://github.com/atom/xray/raw/master/docs/images/crdt.png&quot; alt=&quot;CRDT diagram&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In RGA split, the document is stored as a sequence of insertion fragments. In the example above, the document starts as just a single insertion containing &lt;code&gt;hello world&lt;/code&gt;. We then introduce &lt;code&gt;, there&lt;/code&gt; and &lt;code&gt;!&lt;/code&gt; as additional insertions, splitting the original insertion into two fragments. To delete the &lt;code&gt;ld&lt;/code&gt; at the end of &lt;code&gt;world&lt;/code&gt; in the third step, we create another fragment containing just the &lt;code&gt;ld&lt;/code&gt; and mark it as deleted with a tombstone.&lt;/p&gt;
&lt;p&gt;Structuring the document in this way has a number of advantages.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Real-time collaboration works out of the box&lt;/li&gt;
&lt;li&gt;Concurrent edits: Any thread can read or write its own replica of the document without diverging in the presence of concurrent edits.&lt;/li&gt;
&lt;li&gt;Integrated non-linear history: To undo any group of operations, we increment an undo counter associated with any insertions and deletions that controls their visibility. This means we only need to store operation ids in the history rather than operations themselves, and we can undo any operation at any time rather than adhering to historical order.&lt;/li&gt;
&lt;li&gt;Stable logical positions: Instead of tracking the location of markers on every edit, we can refer to stable positions that are guaranteed to be valid for any future buffer state. For example, we can mark the positions of all search results in a background thread and continue to interpret them in a foreground thread if edits are performed in the meantime.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Our use of a CRDT is similar to the Xi editor, but the approach we're exploring is somewhat different. Our current understanding is that in Xi, the buffer is stored in a rope data structure, then a secondary layer is used to incorporate edits. In Xray, the fundamental storage structure of all text is itself a CRDT. It's similar to Xi's rope in that it uses a copy-on-write B-tree to index all inserted fragments, but it does not require any secondary system for incorporating edits.&lt;/p&gt;
&lt;h3&gt;Derived state will be computed asynchronously&lt;/h3&gt;
&lt;p&gt;We should avoid implementing synchronous APIs that depend on open-ended computations of derived state. For example, when soft wrapping is enabled in Atom, we synchronously update a display index that maps display coordinates to buffer coordinates, which can block the UI.&lt;/p&gt;
&lt;p&gt;In Xray, we want avoid making these kinds of promises in our API. For example, we will allow the display index to be accessed synchronously after a buffer edit, but only provide an interpolated version of its state that can be produced in logarithmic time. These means it will be spatially consistent with the underlying buffer, but may contain lines that have not yet been soft-wrapped.&lt;/p&gt;
&lt;p&gt;We can expose an asynchronous API that allows a package author to wait until the display layer is up to date with a specific version of the buffer. In the user interface, we can display a progress bar for any derived state updates that exceed 50ms, which may occur when the user pastes multiple megabytes of text into the editor.&lt;/p&gt;
&lt;h3&gt;System interaction will be centralized in a &quot;workspace server&quot;&lt;/h3&gt;
&lt;p&gt;File system interactions will be routed through a central server called the &lt;em&gt;workspace server&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The server will serialize buffer loads and saves on a per-path basis, and maintains a persistent database of CRDT operations for each file. As edits are performed in windows, they will be streamed to the host process to be stored and echoed out to any other windows with the same open buffer. This will enable unsaved changes to always be incrementally preserved in case of a crash or power failure and preserves the history associated with a file indefinitely.&lt;/p&gt;
&lt;p&gt;Early on, we should design the application process to be capable of connecting to multiple workspace servers to facilitate real-time collaboration or editing files on a remote server by running a headless host process. To support these use cases, we should prefer implementing most code paths that touch the file system or spawn subprocesses in the host process and interacting with them via RPC.&lt;/p&gt;
&lt;h3&gt;React will be used for presentation&lt;/h3&gt;
&lt;p&gt;By using React, we completely eliminate the view framework as a concern that we need to deal with and give package authors access to a tool they're likely to be familiar with. We also raise the level of abstraction above basic DOM APIs. The risk of using React is of course that it is not standardized and could have breaking API changes. To mitigate this risk, we will require packages to declare which version of React they depend on. We will attempt use this version information to provide shims to older versions of React when we upgrade the bundled version. When it's not possible to shim breaking changes, we'll use the version information to present a warning.&lt;/p&gt;
&lt;h3&gt;Styling will be specified in JS&lt;/h3&gt;
&lt;p&gt;CSS is a widely-known and well-supported tool for styling user interfaces, which is why we embraced it in Atom. Unfortunately, the performance and maintainability of CSS degrade as the number of selectors increases. CSS also lacks good tools for exposing a versioned theming APIs and applying programmatic logic such as altering colors. Finally, the browser does not expose APIs for being notified when computed styles change, making it difficult to use CSS as a source of truth for complex components. For a theming system that performs well and scales, we need more direct control. We plan to use a CSS-in-JS approach that automatically generates atomic selectors so as to keep our total number of selectors minimal.&lt;/p&gt;
&lt;h3&gt;Text is rendered via WebGL&lt;/h3&gt;
&lt;p&gt;In Atom, the vast majority of computation of any given frame is spent manipulating the DOM, recalculating styles, and performing layout. To achieve good text rendering performance, it is critical that we bypass this overhead and take direct control over rendering. Like Alacritty and Xi, we plan to employ OpenGL to position quads that are mapped to glyph bitmaps in a texture atlas.&lt;/p&gt;
&lt;p&gt;We plan to use HarfBuzz to determine an accurate mapping between character sequences and glyphs, since one character does not always correspond to one glyph. Once we identify clusters of characters corresponding to glyphs, we'll rasterize glyphs via an HTML 5 canvas to ensure we use the appropriate text rasterization method for the current platform, then upload them to the texture atlas on the GPU to be referenced in shaders.&lt;/p&gt;
&lt;p&gt;Bypassing the DOM means that we'll need to implement styling and text layout ourselves. That is a high price to pay, but we think it will be worth it to bypass the performance overhead imposed by the DOM.&lt;/p&gt;
&lt;h2&gt;Development process&lt;/h2&gt;
&lt;h3&gt;Experiment&lt;/h3&gt;
&lt;p&gt;At this phase, this code is focused on learning. Whatever code we write should be production-quality, but we don't need to support everything at this phase. We can defer features that don't contribute substantially to learning.&lt;/p&gt;
&lt;h3&gt;Documentation-driven development&lt;/h3&gt;
&lt;p&gt;Before coding, we ask ourselves whether the code we're writing can be motivated by something that's written in the guide. The right approach here will always be a judgment call, but lets err on the side of transparency and see what happens. See &lt;a href=&quot;https://github.com/atom/xray/blob/master&quot;&gt;About this guide&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h3&gt;Disciplined monorepo&lt;/h3&gt;
&lt;p&gt;All code related to Xray should live in this repository, but intra-repository dependencies should be expressed in a disciplined way to ensure that a one-line docs change doesn't require us to rebuild the world. Builds should be finger-printed on a per-component basis and we should aim to keep components granular.&lt;/p&gt;
&lt;h3&gt;Community SLA&lt;/h3&gt;
&lt;p&gt;Well-formulated PRs and issues will receive some form of response by the end of the next business day. If this interferes with our ability to learn, we revisit.&lt;/p&gt;
&lt;h2&gt;Q1 2018 Roadmap&lt;/h2&gt;
&lt;p&gt;The primary goal of the next three months is to validate the key ideas presented in this document and to get a sense for how long the envisioned system might take to develop. That's a pretty abstract goal, however.&lt;/p&gt;
&lt;p&gt;More concretely, our goal is to ship a high-performance standalone editor component suitable for use in any web application, something we could eventually use on GitHub.com. This standalone editor will give us a chance to test a limited set of critical features in production scenarios without building out an entire desktop-based editor. We plan to develop this new editor in the context of a prototype Electron application, but we'll offer the standalone component as a separate build artifact from the main app.&lt;/p&gt;
&lt;/article&gt;</description>
<pubDate>Mon, 05 Mar 2018 23:59:22 +0000</pubDate>
<dc:creator>seanwilson</dc:creator>
<og:image>https://avatars0.githubusercontent.com/u/1089146?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>atom/xray</og:title>
<og:url>https://github.com/atom/xray</og:url>
<og:description>xray - An experimental next-generation Electron-based text editor</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/atom/xray</dc:identifier>
</item>
<item>
<title>History of the browser user-agent string (2008)</title>
<link>http://webaim.org/blog/user-agent-string-history/</link>
<guid isPermaLink="true" >http://webaim.org/blog/user-agent-string-history/</guid>
<description>&lt;p&gt;&lt;img src=&quot;https://webaim.org/blog/media/useragents/mosaic.jpg&quot; alt=&quot;&quot;/&gt;In the beginning there was &lt;a href=&quot;http://www.ncsa.illinois.edu/Projects/mosaic.html&quot;&gt;NCSA Mosaic&lt;/a&gt;, and Mosaic called itself &lt;em&gt;NCSA_Mosaic/2.0 (Windows 3.1)&lt;/em&gt;, and Mosaic displayed pictures along with text, and there was much rejoicing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://webaim.org/blog/media/useragents/netscape.jpg&quot; alt=&quot;&quot;/&gt;And behold, then came a new web browser known as &lt;a href=&quot;http://en.wikipedia.org/wiki/Mozilla&quot;&gt;“Mozilla”&lt;/a&gt;, being short for “Mosaic Killer,” but Mosaic was not amused, so the public name was changed to Netscape, and Netscape called itself &lt;em&gt;Mozilla/1.0 (Win3.1)&lt;/em&gt;, and there was more rejoicing. And Netscape supported frames, and frames became popular among the people, but Mosaic did not support frames, and so came “user agent sniffing” and to “Mozilla” webmasters sent frames, but to other browsers they sent not frames.&lt;/p&gt;
&lt;p&gt;And Netscape said, let us make fun of Microsoft and refer to Windows as “poorly debugged device drivers,” and Microsoft was angry. And so Microsoft made their own web browser, which they called Internet Explorer, hoping for it to be a “Netscape Killer”. &lt;img src=&quot;https://webaim.org/blog/media/useragents/ie.png&quot; alt=&quot;&quot;/&gt;And Internet Explorer supported frames, and yet was not Mozilla, and so was not given frames. And Microsoft grew impatient, and did not wish to wait for webmasters to learn of IE and begin to send it frames, and so Internet Explorer declared that it was “Mozilla compatible” and began to impersonate Netscape, and called itself &lt;em&gt;Mozilla/1.22 (compatible; MSIE 2.0; Windows 95)&lt;/em&gt;, and Internet Explorer received frames, and all of Microsoft was happy, but webmasters were confused.&lt;/p&gt;
&lt;p&gt;And Microsoft sold IE with Windows, and made it better than Netscape, and the first browser war raged upon the face of the land. &lt;img src=&quot;https://webaim.org/blog/media/useragents/mozilla.png&quot; alt=&quot;&quot;/&gt;And behold, Netscape was killed, and there was much rejoicing at Microsoft. But Netscape was reborn as Mozilla, and Mozilla built Gecko, and called itself &lt;em&gt;Mozilla/5.0 (Windows; U; Windows NT 5.0; en-US; rv:1.1) Gecko/20020826&lt;/em&gt;, and Gecko was the rendering engine, and Gecko was good. &lt;img src=&quot;https://webaim.org/blog/media/useragents/firefox.jpg&quot; alt=&quot;&quot;/&gt; And Mozilla became Firefox, and called itself &lt;em&gt;Mozilla/5.0 (Windows; U; Windows NT 5.1; sv-SE; rv:1.7.5) Gecko/20041108 Firefox/1.0&lt;/em&gt;, and Firefox was very good. And Gecko began to multiply, and other browsers were born that used its code, and they called themselves &lt;em&gt;Mozilla/5.0 (Macintosh; U; PPC Mac OS X Mach-O; en-US; rv:1.7.2) Gecko/20040825 Camino/0.8.1&lt;/em&gt; the one, and &lt;em&gt;Mozilla/5.0 (Windows; U; Windows NT 5.1; de; rv:1.8.1.8) Gecko/20071008 SeaMonkey/1.0&lt;/em&gt; another, each pretending to be Mozilla, and all of them powered by Gecko.&lt;/p&gt;
&lt;p&gt;And Gecko was good, and IE was not, and sniffing was reborn, and Gecko was given good web code, and other browsers were not. &lt;img src=&quot;https://webaim.org/blog/media/useragents/konqueror.jpg&quot; alt=&quot;&quot;/&gt;And the followers of Linux were much sorrowed, because they had built Konqueror, whose engine was KHTML, which they thought was as good as Gecko, but it was not Gecko, and so was not given the good pages, and so Konquerer began to pretend to be “like Gecko” to get the good pages, and called itself &lt;em&gt;Mozilla/5.0 (compatible; Konqueror/3.2; FreeBSD) (KHTML, like Gecko)&lt;/em&gt; and there was much confusion.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://webaim.org/blog/media/useragents/opera.jpg&quot; alt=&quot;&quot;/&gt;Then cometh Opera and said, “surely we should allow our users to decide which browser we should impersonate,” and so Opera created a menu item, and Opera called itself &lt;em&gt;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.51&lt;/em&gt;, or &lt;em&gt;Mozilla/5.0 (Windows NT 6.0; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.51&lt;/em&gt;, or &lt;em&gt;Opera/9.51 (Windows NT 5.1; U; en)&lt;/em&gt; depending on which option the user selected.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://webaim.org/blog/media/useragents/safari.jpg&quot; alt=&quot;&quot;/&gt;And Apple built Safari, and used KHTML, but added many features, and forked the project, and called it WebKit, but wanted pages written for KHTML, and so Safari called itself &lt;em&gt;Mozilla/5.0 (Macintosh; U; PPC Mac OS X; de-de) AppleWebKit/85.7 (KHTML, like Gecko) Safari/85.5&lt;/em&gt;, and it got worse.&lt;/p&gt;
&lt;p&gt;And Microsoft feared Firefox greatly, and Internet Explorer returned, and called itself &lt;em&gt;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0)&lt;/em&gt; and it rendered good code, but only if webmasters commanded it to do so.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://webaim.org/blog/media/useragents/chrome.jpg&quot; alt=&quot;&quot;/&gt;And then Google built &lt;a href=&quot;http://www.google.com/chrome&quot;&gt;Chrome&lt;/a&gt;, and Chrome used Webkit, and it was like Safari, and wanted pages built for Safari, and so pretended to be Safari. And thus Chrome used WebKit, and pretended to be Safari, and WebKit pretended to be KHTML, and KHTML pretended to be Gecko, and all browsers pretended to be Mozilla, and Chrome called itself &lt;em&gt;Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/525.13 (KHTML, like Gecko) Chrome/0.2.149.27 Safari/525.13&lt;/em&gt;, and the user agent string was a complete mess, and near useless, and everyone pretended to be everyone else, and confusion abounded.&lt;/p&gt;
</description>
<pubDate>Mon, 05 Mar 2018 23:30:44 +0000</pubDate>
<dc:creator>jesperht</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://webaim.org/blog/user-agent-string-history/</dc:identifier>
</item>
<item>
<title>The Mystery of the Slow Downloads</title>
<link>https://panic.com/blog/mystery-of-the-slow-downloads/</link>
<guid isPermaLink="true" >https://panic.com/blog/mystery-of-the-slow-downloads/</guid>
<description>&lt;div class=&quot;fullWidth&quot;&gt;&lt;img class=&quot;aligncenter size-full wp-image-7532&quot; src=&quot;https://panic.com/blog/wp-content/uploads/2017/11/turtle.jpg&quot; alt=&quot;&quot; width=&quot;2000&quot; height=&quot;601&quot;/&gt;&lt;/div&gt;
&lt;p&gt;A few months ago, a complaint started popping up from users downloading or updating our apps: &lt;strong&gt;“Geez, your downloads are really slow!”&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you work in support, you probably have a reflexive reaction to a complaint like this. It’s vague. There’s a million possible factors. It’ll probably resolve itself by tomorrow. You hope. Boy do you hope.&lt;/p&gt;
&lt;p&gt;Except… we also started noticing it ourselves when we were working from home. When we’d come in to the office, transfers were lightning fast. But at home, it was really, seriously getting hard to get any work done remotely at all.&lt;/p&gt;
&lt;p&gt;So, maybe there &lt;em&gt;was&lt;/em&gt; something screwy here?&lt;/p&gt;
&lt;h2&gt;The Video&lt;/h2&gt;
&lt;p&gt;Before digging in, &lt;strong&gt;here’s this story in convenient summarized video form&lt;/strong&gt;, if you’d prefer!&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;embed-youtube&quot;&gt;&lt;iframe class=&quot;youtube-player&quot; type=&quot;text/html&quot; width=&quot;750&quot; height=&quot;422&quot; src=&quot;https://www.youtube.com/embed/yh3touL9eqg?version=3&amp;amp;rel=0&amp;amp;fs=1&amp;amp;autohide=2&amp;amp;showsearch=0&amp;amp;showinfo=1&amp;amp;iv_load_policy=1&amp;amp;wmode=transparent&quot; allowfullscreen=&quot;true&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now on to the details.&lt;/p&gt;
&lt;h2&gt;The Test&lt;/h2&gt;
&lt;p&gt;The Panic “network topology” is actually very simple. &lt;strong&gt;The Panic web servers have a single connection to the internet via Cogent.&lt;/strong&gt; We colocate our own servers, rather than using AWS or any other PaaS, and we also don’t currently use a CDN or any other cloud distribution platform.&lt;/p&gt;
&lt;p&gt;So, if something is making our downloads slow, it ought to be pretty easy to do some analysis and figure out why, or at least where.&lt;/p&gt;
&lt;p&gt;We wanted to know three things:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;How fast can people download from our website?&lt;/li&gt;
&lt;li&gt;How fast can people download from a “control” website that’s &lt;em&gt;not&lt;/em&gt; on our network?&lt;/li&gt;
&lt;li&gt;What are people using for their internet provider?&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;We made an &lt;a href=&quot;https://panic.com/bin/panic_1.php&quot;&gt;extremely simple test page&lt;/a&gt; that transfers 20MB of data from our server to the browser, then sends the user to run the same script on the control server, which we chose to host with Linode. (The Linode server is located in Fremont, CA, the closest we could find to us here in Portland.)&lt;/p&gt;
&lt;p&gt;We tweeted the link out, and data started pouring in…&lt;/p&gt;
&lt;h2&gt;The Results&lt;/h2&gt;
&lt;p&gt;Here’s what we got back, comparing how fast our users could download from our control server through Linode, and from our own servers through Cogent:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-full wp-image-7665&quot; src=&quot;https://panic.com/blog/wp-content/uploads/2018/02/all-providers-graph-1.png&quot; alt=&quot;Graph comparing transfer speeds between servers&quot; width=&quot;1670&quot; height=&quot;1970&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;em&gt;(There are 1,645 samples in our target range, after filtering out TLDs with fewer than 10 occurrences, and we’ve done a &lt;a href=&quot;https://www.khanacademy.org/math/probability/data-distributions-a1/box--whisker-plots-a1/v/reading-box-and-whisker-plots&quot;&gt;box plot&lt;/a&gt;, which shows a spread of all the data points.)&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Well, well, well. &lt;strong&gt;It doesn’t take statistical genius to see one glaring outlier — and that was Comcast, with download speeds often being as low as 300 kilobytes/second.&lt;/strong&gt; And you’ll never guess what provider is used by virtually every Panic employee when they work from home? Yeah, Comcast. There is, in fact, no other cable ISP available to Portland residents.&lt;/p&gt;
&lt;p&gt;But, before jumping to conclusions, there was something else that was weird with the Comcast data: a huge number of outliers, way more outliers than any other provider. See all those red dots on the graph, ranging from very slow to very fast?&lt;/p&gt;
&lt;p&gt;The answer to &lt;em&gt;that&lt;/em&gt; mystery was solved when we plotted out Comcast data across different times of the day…&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-full wp-image-7666&quot; src=&quot;https://panic.com/blog/wp-content/uploads/2018/02/graph-comparison-1.png&quot; alt=&quot;Graph of Comcast transfer speeds in the morning versus the evening&quot; width=&quot;1669&quot; height=&quot;1298&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Nuts. The problem reports we’d been hearing were indeed a real thing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Our downloads really &lt;em&gt;were&lt;/em&gt; slow — but seemingly &lt;em&gt;only&lt;/em&gt; to Comcast users, and &lt;em&gt;only&lt;/em&gt; during peak internet usage times. Something was up.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At first we thought, maybe Comcast bandwidth is just naturally more congested in the evening as people come home from work and begin streaming Netflix, etc. But that didn’t explain why the connections to our Linode control server from Comcast, during the exact same time windows for each tester, were downloading with good speeds.&lt;/p&gt;
&lt;p&gt;We wondered, is Comcast intentionally “throttling” Cogent customers? And if so, why?&lt;/p&gt;
&lt;h2&gt;The Why&lt;/h2&gt;
&lt;p&gt;Peering.&lt;/p&gt;
&lt;p&gt;Major internet pipes, like Cogent, have peering agreements with network providers, like Comcast. These companies need each other — Cogent can’t exist if their network doesn’t go all the way to the end user, and Comcast can’t exist if they can’t send their customer’s data all over the world. One core tenet of peering is that it is “settlement-free” — neither party pays the other party to exchange their traffic. Instead, each party generates revenue from their customers. Cogent generates revenue from us. Comcast generates revenue from us at home. Everyone wins, right?&lt;/p&gt;
&lt;p&gt;After a quick Google session, I learned that Cogent and Comcast have quite a storied history. This history started when Cogent started delivering a great deal of video content to Comcast customers… content from Netflix. and suddenly, the “peering pipe” that connects Cogent and Comcast filled up and slowed dramatically down.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Normally when these peering pipes “fill up”, more capacity is added between the two companies.&lt;/strong&gt; But, if you believe Cogent’s side of the story, Comcast simply decided not to play ball — and refused to add any additional bandwidth unless Cogent paid them. In other words, Comcast didn’t like being paid nothing to deliver Netflix traffic, which competes with its own TV and streaming offerings. &lt;a href=&quot;https://arstechnica.com/tech-policy/2014/05/comcast-is-the-one-who-should-pay-for-network-connections-cogent-claims/&quot;&gt;This Ars Technica article covers it well.&lt;/a&gt; (How did Netflix solve this problem in 2014? &lt;a href=&quot;https://arstechnica.com/information-technology/2014/02/comcast-gets-paid-by-netflix-and-might-still-want-money-from-cogent/&quot;&gt;Netflix entered into a business agreement to pay Comcast directly.&lt;/a&gt; And suddenly, more peering bandwidth opened up between Comcast and Cogent, like magic.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We felt certain history was repeating itself: the peering connection between Comcast and Cogent was once again saturated.&lt;/strong&gt; Cogent said their hands were tied. What now?&lt;/p&gt;
&lt;h2&gt;The Fix&lt;/h2&gt;
&lt;p&gt;There was only one last hope: get Comcast to fix it. I know, like we were somehow going to convince this 200 billion dollar corporation to add more capacity to their interconnection with Cogent. If I asked you to rate the possibility of that actually happening on a scale of “no” to “never”, you’d probably pick “come on man are you serious”, right?&lt;/p&gt;
&lt;p&gt;But after a lifetime of being a “hey, it’s worth a shot” guy, I had to try. I did a real quick Google search for Comcast corporate contacts and found a person who seemed like they were involved in network operations PR, and &lt;strong&gt;I fired off a quick e-mail explaining the situation to Comcast.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;And then, the craziest thing happened…&lt;/p&gt;
&lt;p&gt;They wrote back quickly. Not only that, but they were &lt;em&gt;on it&lt;/em&gt;. We set up a phone call. They took us seriously, they wanted to know the backstory, they wanted to know what our customers were seeing, and they were going to talk to the right people — they even e-mailed Cogent to connect with the right person in peering over there.&lt;/p&gt;
&lt;p&gt;And pretty soon a call came back with a definitive-sounding statement: “Give us 1 to 2 weeks, and if you re-run your test I think you’ll be happy with the results.”&lt;/p&gt;
&lt;p&gt;Sure enough, we waited two weeks, had our users re-run the speed test, and wouldn’t you know it…&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-full wp-image-7667&quot; src=&quot;https://panic.com/blog/wp-content/uploads/2018/02/all-providers-graph-2.png&quot; alt=&quot;Graph comparing transfer speeds of different ISPs&quot; width=&quot;1670&quot; height=&quot;1970&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-full wp-image-7668&quot; src=&quot;https://panic.com/blog/wp-content/uploads/2018/02/graph-comparison-2.png&quot; alt=&quot;Graph comparing Comcast transfer speeds at different times of day&quot; width=&quot;1669&quot; height=&quot;1298&quot;/&gt;&lt;/p&gt;
&lt;p&gt;…the problem was essentially gone. &lt;strong&gt;Comcast really did fix it.&lt;/strong&gt; We were now able to measure our Comcast download speeds in megabytes/second instead of kilobytes.&lt;/p&gt;
&lt;p&gt;According to Comcast, two primary changes were made:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Comcast added more capacity for Cogent traffic. (Exactly as we suspected, the pipe was full.)&lt;/li&gt;
&lt;li&gt;Cogent made some unspecified changes to their traffic engineering.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;Here’s where I have to give Comcast credit where credit is due: they really did care about this problem, and they really did work quickly to make it go away.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;(One weird thing, though: I was so prepared for a total Comcast dead-end, so sure that Comcast would never even reply, let alone help, that this incredibly positive outcome made me feel &lt;em&gt;suspicious&lt;/em&gt;: why me? Why was I able to get this corrected with an e-mail when Cogent couldn’t?&lt;/p&gt;
&lt;p&gt;It felt like there was no way this should have worked. If I had to guess, I’d say it’s simple: in the middle of a serious ongoing debate over net neutrality, the last thing Comcast wanted to look like was a network-throttling bad guy in this blog post. But then again, maybe I’m still being too cynical — maybe they just saw a problem they hadn’t noticed and fixed it. (But really, did they &lt;em&gt;really&lt;/em&gt; not notice that pipe was full until I asked? Surely there are network monitoring tools?) Frankly, I have to stop thinking about it, because I’ll never know. But no matter the reason, I’m very grateful: thanks for listening to us, Comcast.)&lt;/p&gt;
&lt;h2&gt;What Does This All Mean&lt;/h2&gt;
&lt;p&gt;I’d summarize it as follows:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;The internet is fragile — and that’s pretty scary.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;And while this story amazingly had a happy ending, I’m not looking forward to the next time we’re stuck in the middle of a peering dispute between two companies. It feels absolutely inevitable, all the more so now that net neutrality is gone. Here’s hoping the next time it happens, the responsible party is as responsive as Comcast was this time.&lt;/p&gt;
&lt;h2&gt;Check Our Work&lt;/h2&gt;
&lt;p&gt;All of our data, our data analysis scripts, and more, is &lt;a href=&quot;https://github.com/panicinc/speed_mystery&quot;&gt;available at this GitHub repository&lt;/a&gt;. You can even click the button in the readme and it will take you to a running JupyterHub notebook where you can play with the data yourself, live in your browser. If you find any insights, or mistakes, please let us know.&lt;/p&gt;
</description>
<pubDate>Mon, 05 Mar 2018 22:55:38 +0000</pubDate>
<dc:creator>Doubleguitars</dc:creator>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://panic.com/blog/mystery-of-the-slow-downloads/</dc:identifier>
</item>
<item>
<title>MoviePass CEO says the app tracks your location before and after movies</title>
<link>https://techcrunch.com/2018/03/05/moviepass-ceo-proudly-says-the-app-tracks-your-location-before-and-after-movies/</link>
<guid isPermaLink="true" >https://techcrunch.com/2018/03/05/moviepass-ceo-proudly-says-the-app-tracks-your-location-before-and-after-movies/</guid>
<description>&lt;img src=&quot;https://tctechcrunch2011.files.wordpress.com/2018/03/phone-in-hand.jpg?w=738&quot; class=&quot;&quot;/&gt;&lt;p id=&quot;speakable-summary&quot;&gt;Everyone knew the MoviePass deal is too good to be true — and as is so often the case these days, it turns out you’re not the customer, you’re the product. And in this case they’re not even attempting to camouflage that. Mitch Lowe, the company’s CEO, told an audience at a Hollywood event that “we know all about you.”&lt;/p&gt;&lt;p&gt;Lowe was giving the keynote at the Entertainment Finance Forum; his talk was entitled “Data is the New Oil: How will MoviePass Monetize It?” &lt;a target=&quot;_blank&quot; href=&quot;https://www.mediaplaynews.com/ceo-mitch-lowe-says-moviepass-will-reach-5-million-subs-by-end-of-year/&quot; rel=&quot;noopener&quot;&gt;Media Play News&lt;/a&gt; first reported his remarks.&lt;/p&gt;
&lt;p&gt;“We get an enormous amount of information,” Lowe continued. “We watch how you drive from home to the movies. We watch where you go afterwards.”&lt;/p&gt;
&lt;p&gt;It’s no secret that MoviePass is planning on making hay out of the data collected through its service. But what I imagined, and what I think most people imagined, was that it would be interesting next-generation data about ticket sales, movie browsing, A/B testing on promotions in the app and so on.&lt;/p&gt;
&lt;p&gt;I &lt;em&gt;didn’t&lt;/em&gt; imagine that the app would be tracking your location before you even left your home, and then follow you while you drive back or head out for a drink afterwards. Did you?&lt;/p&gt;
&lt;p&gt;It sure isn’t in the company’s &lt;a target=&quot;_blank&quot; href=&quot;https://www.moviepass.com/privacy/&quot; rel=&quot;noopener&quot;&gt;privacy policy&lt;/a&gt;, which in relation to location tracking discloses only a “single request” when selecting a theater, which will “only be used as a means to develop, improve, and personalize the service.” Which part of development requires them to track you before and after you see the movie?&lt;/p&gt;
&lt;p&gt;Naturally I contacted MoviePass for comment and will update if I hear back. But it’s pretty hard to misinterpret Lowe’s words.&lt;/p&gt;

&lt;p&gt;The startup’s plan is to “build a night at the movies,” perhaps complete with setting up parking or ordering you a car, giving you a deal on dinner before or after, connecting you with like-minded moviegoers, etc. Of course they need data to do that, but one would hope that the collection would be a bit more nuanced than this.&lt;/p&gt;
&lt;p&gt;People clearly value the service, because it essentially lets them use someone else’s credit card instead of their own at the movies (and one belonging to a bunch of venture capitalists at that). Who would say no? Some people sure might, if they knew their activities were being tracked at this granularity (and, it has to be said, with such a cavalier attitude) to be packaged up and sold. (&lt;a target=&quot;_blank&quot; href=&quot;https://techcrunch.com/2018/01/20/wtf-is-gdpr/&quot; rel=&quot;noopener&quot;&gt;Good luck with the GDPR&lt;/a&gt;, by the way.)&lt;/p&gt;
&lt;p&gt;Hopefully MoviePass can explain exactly what data it collects and what it does with it, so everyone can make an informed choice.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: In a statement, a MoviePass representative says:&lt;/p&gt;
&lt;blockquote readability=&quot;11&quot;&gt;
&lt;p&gt;We are exploring utilizing location-based marketing as a way to help enhance the overall experience by creating more opportunities for our subscribers to enjoy all the various elements of a good movie night. We will not be selling the data that we gather. Rather, we will use it to better inform how to market potential customer benefits including discounts on transportation, coupons for nearby restaurants, and other similar opportunities.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I’ve also asked for information on what location data specifically is collected, for how long before and after a movie users are tracked, and where these policies are disclosed to users.&lt;/p&gt;
&lt;small&gt;Featured Image: iStock / Getty Images Plus/Getty Images UNDER A Royalty free LICENSE&lt;/small&gt;</description>
<pubDate>Mon, 05 Mar 2018 21:51:29 +0000</pubDate>
<dc:creator>vorpalhex</dc:creator>
<og:title>MoviePass CEO proudly says the app tracks your location before and after movies</og:title>
<og:description>Everyone knew the MoviePass deal is too good to be true — and as is so often the case these days, it turns out you're not the customer, you're the product...</og:description>
<og:image>https://tctechcrunch2011.files.wordpress.com/2018/03/phone-in-hand.jpg</og:image>
<og:url>http://social.techcrunch.com/2018/03/05/moviepass-ceo-proudly-says-the-app-tracks-your-location-before-and-after-movies/</og:url>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://techcrunch.com/2018/03/05/moviepass-ceo-proudly-says-the-app-tracks-your-location-before-and-after-movies/</dc:identifier>
</item>
<item>
<title>Clang Is Now Used to Build Chrome for Windows</title>
<link>http://blog.llvm.org/2018/03/clang-is-now-used-to-build-chrome-for.html</link>
<guid isPermaLink="true" >http://blog.llvm.org/2018/03/clang-is-now-used-to-build-chrome-for.html</guid>
<description>&lt;div dir=&quot;ltr&quot; trbidi=&quot;on&quot;&gt;
&lt;div&gt;As of Chrome 64, Chrome for Windows is compiled with Clang. We now use Clang to build Chrome for all platforms it runs on: macOS, iOS, Linux, Chrome OS, Android, and Windows. Windows is the platform with the second most Chrome users after Android &lt;a href=&quot;http://gs.statcounter.com/browser-version-market-share&quot;&gt;according to statcounter&lt;/a&gt;, which made this switch particularly exciting.&lt;/div&gt;
&lt;br/&gt;Clang is the first-ever open-source C++ compiler that’s ABI-compatible with Microsoft Visual C++ (MSVC) – meaning you can build some parts of your program (for example, system libraries) with the MSVC compiler (“cl.exe”), other parts with Clang, and when linked together (either by MSVC’s linker, “link.exe”, or LLD, the LLVM project’s linker – see below) the parts will form a working program.&lt;p&gt;Note that Clang is not a replacement for Visual Studio, but an addition to it. We still use Microsoft’s headers and libraries to build Chrome, we still use some SDK binaries like midl.exe and mc.exe, and many Chrome/Win developers still use the Visual Studio IDE (for both development and for debugging).&lt;/p&gt;&lt;p&gt;This post discusses numbers, motivation, benefits and drawbacks of using Clang instead of MSVC, how to try out Clang for Windows yourself, project history, and next steps. For more information on the technical side you can look at the &lt;a href=&quot;https://docs.google.com/presentation/d/1oxNHaVjA9Gn_rTzX6HIpJHP7nXRua_0URXxxJ3oYRq0/edit#slide=id.p&quot;&gt;slides of our 2015 LLVM conference talk&lt;/a&gt;, and the slides linked from there.&lt;br/&gt;&lt;/p&gt;&lt;h3&gt;Numbers&lt;/h3&gt;
This is what most people ask about first, so let’s talk about it first. We think the other sections are more interesting though.&lt;br/&gt;&lt;h4&gt;Build time&lt;/h4&gt;
Building Chrome locally with Clang is about 15% slower than with MSVC. (We’ve heard that Windows Defender can make Clang builds a lot slower on some machines, so if you’re seeing larger slowdowns, make sure to whitelist Clang in Windows Defender.) However, the way Clang emits debug info is more parallelizable and builds with a distributed build service (e.g. &lt;a href=&quot;https://chromium.googlesource.com/infra/goma/client/&quot;&gt;Goma&lt;/a&gt;) are hence faster.&lt;br/&gt;&lt;h4&gt;Binary size&lt;/h4&gt;
Chrome installer size gets smaller for 64-bit builds and slightly larger for 32-bit builds using Clang. The same difference shows in uncompressed code size for regular builds as well (see the &lt;a href=&quot;https://crbug.com/457078&quot;&gt;tracking bug for Clang binary size&lt;/a&gt; for many numbers). However, compared to MSVC builds using link-time code generation (LTCG) and &lt;a href=&quot;https://blog.chromium.org/2016/10/making-chrome-on-windows-faster-with-pgo.html&quot;&gt;profile-guided optimization&lt;/a&gt; (PGO) Clang generates larger code in 64-bit for targets that use /O2 but smaller code for targets that use /Os. The installer size comparison suggests Clang's output compresses better.&lt;p&gt;Some raw numbers for versions 64.0.3278.2 (MSVC PGO) and 64.0.3278.0 (Clang). mini_installer.exe is Chrome’s installer that users download, containing the LZMA-compressed code. chrome_child.dll is one of the two main dlls; it contains Blink and V8, and generally has many targets that are built with /O2. chrome.dll is the other main dll, containing the browser process code, mostly built with /Os.&lt;br/&gt;&lt;span id=&quot;docs-internal-guid-891146fc-f7de-b09a-1a30-b3298ec281f3&quot;&gt;&lt;br/&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;&lt;div dir=&quot;ltr&quot;&gt;
&lt;table&gt;&lt;colgroup&gt;&lt;col width=&quot;117&quot;/&gt;&lt;col width=&quot;142&quot;/&gt;&lt;col width=&quot;106&quot;/&gt;&lt;col width=&quot;137&quot;/&gt;&lt;col width=&quot;121&quot;/&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;br/&gt;&lt;/td&gt;
&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;mini_installer.exe&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;chrome.dll&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;chrome_child.dll&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;chrome.exe&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;32-bit win-pgo&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;32-bit win-clang&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;


&lt;/td&gt;
&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;42.56 MB (+16.7%)&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;


&lt;/td&gt;
&lt;td&gt;


&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;64-bit win-pgo&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;64-bit win-clang&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;


&lt;/td&gt;
&lt;td&gt;


&lt;/td&gt;
&lt;td&gt;


&lt;/td&gt;
&lt;td&gt;


&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;h4&gt;Performance&lt;/h4&gt;
We conducted extensive A/B testing of performance. Performance telemetry numbers are about the same for MSVC-built and clang-built Chrome – some metrics get better, some get worse, but all of them are within 5% of each other. The official MSVC builds used LTCG and PGO, while the Clang builds currently use neither of these. This is potential for improvement that we look forward to exploring. The PGO builds took a very long time to build due to the need for collecting profiles and then building again, and as a result, the configuration was not enabled on our performance-measurement buildbots. Now that we use Clang, the perf bots again track the configuration that we ship.&lt;p&gt;Startup performance was worse in Clang-built Chrome until we started using a &lt;a href=&quot;https://chromium.googlesource.com/chromium/src/+/master/docs/win_order_files.md&quot;&gt;link-order file&lt;/a&gt; – a form of “PGO light” .&lt;br/&gt;&lt;/p&gt;&lt;h4&gt;Stability&lt;/h4&gt;
We A/B-tested stability as well and found no difference between the two build configurations.&lt;br/&gt;&lt;h3&gt;Motivation&lt;/h3&gt;
There were many motivating reasons for this project, the overarching theme being the benefits of using the same compiler across all of Chrome’s platforms, as well as the ability to change the compiler and deploy those changes to all our developers and buildbots quickly. Here’s a non-exhaustive list of examples.&lt;br/&gt;&lt;ul&gt;&lt;li&gt;Chrome is heavily using technology that’s based on compiler instrumentation (ASan, CFI, &lt;a href=&quot;https://blog.chromium.org/2012/04/fuzzing-for-security.html&quot;&gt;ClusterFuzz&lt;/a&gt;—uses ASan). Clang supports this instrumentation already, but we can’t add it to MSVC. We previously used &lt;a href=&quot;https://github.com/google/syzygy&quot;&gt;after-the-fact binary instrumentation&lt;/a&gt; to mitigate this a bit, but having the toolchain write the right bits in the first place is cleaner and faster.&lt;/li&gt;
&lt;li&gt;Clang enables us to write compiler &lt;a href=&quot;https://www.chromium.org/developers/coding-style/chromium-style-checker-errors&quot;&gt;plugins&lt;/a&gt; that add Chromium-specific warnings and to write tooling for &lt;a href=&quot;https://chromium.googlesource.com/chromium/src/+/lkcr/docs/clang_tool_refactoring.md&quot;&gt;large-scale refactoring&lt;/a&gt;. &lt;a href=&quot;https://cs.chromium.org/&quot;&gt;Chromium’s code search&lt;/a&gt; can now learn to index Windows code.&lt;/li&gt;
&lt;li&gt;Chromium is open-source, so it’s nice if it’s built with an open-source toolchain.&lt;/li&gt;
&lt;li&gt;Chrome runs on 6+ platforms, and most developers are only familiar with 1-3 platforms. If your patch doesn’t compile on a platform you’re unfamiliar with, due to a compiler error that you can’t locally reproduce on your local development machine, it’ll take you a while to fix. On the other hand, if all platforms use the same compiler, if it builds on your machine then it’s probably going to build on all platforms.&lt;/li&gt;
&lt;li&gt;Using the same compiler also means that compiler-specific micro-optimizations will help on all platforms (assuming that the same -O flags are used on all platforms – not yet the case in Chrome, and only on the same ISAs – x86 vs ARM will stay different).&lt;/li&gt;
&lt;li&gt;Using the same compiler enables &lt;a href=&quot;https://cs.chromium.org/chromium/src/docs/win_cross.md&quot;&gt;cross-compiling&lt;/a&gt; – developers who feel most at home on a Linux box can now work on Windows-specific code, from their Linux box (without needing to run Wine).&lt;/li&gt;
&lt;li&gt;We can &lt;a href=&quot;https://ci.chromium.org/p/chromium/g/chromium.clang/console&quot;&gt;continuously build Chrome trunk with Clang trunk&lt;/a&gt; to find compiler regressions quickly. This allows us to update Clang every week or two. Landing a major MSVC update in Chrome usually took a year or more, with several rounds of reporting internal compiler bugs and miscompiles. The issue here isn’t that MSVC is more buggy than Clang – it isn’t, all software is buggy – but that we can continuously improve Clang due to Clang being open-source.&lt;/li&gt;
&lt;li&gt;C++ receives major new revisions every few years. When C++11 was released, we were still using six different compilers, and &lt;a href=&quot;http://chromium-cpp.appspot.com/&quot;&gt;enabling C++11&lt;/a&gt; was difficult. With fewer compilers, this gets much easier.&lt;/li&gt;
&lt;li&gt;We can prioritize compiler features that are important to us. For example:&lt;/li&gt;
&lt;li&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;Of course, not all – or even most – of these reasons will apply to other projects.&lt;br/&gt;&lt;h3&gt;Benefits and drawbacks of using Clang instead of Visual C++&lt;/h3&gt;
&lt;div&gt;Benefits of using Clang, if you want to try for your project:&lt;/div&gt;

&lt;ul&gt;&lt;li&gt;Clang supports 64-bit inline assembly. For example, in Chrome we built libyuv (a video format conversion library) with Clang long before we built all of Chrome with it. libyuv had highly-tuned 64-bit inline assembly with performance not reachable with intrinsics, and we could just use that code on Windows.&lt;/li&gt;
&lt;li&gt;If your project runs on multiple platforms, you can use one compiler everywhere. Building your project with several compilers is generally considered good for code health, but in Chrome we found that Clang’s diagnostics found most problems and we were mostly battling compiler bugs (and if another compiler has a great new diagnostic, we can add that to Clang).&lt;/li&gt;
&lt;li&gt;Likewise, if your project is Windows-only, you can get a second compiler’s opinion on your code, and Clang’s warnings might find bugs.&lt;/li&gt;
&lt;li&gt;You can use &lt;a href=&quot;https://clang.llvm.org/docs/AddressSanitizer.html&quot;&gt;Address Sanitizer&lt;/a&gt; to find memory bugs.&lt;/li&gt;
&lt;li&gt;If you don’t use LTCG and PGO, it’s possible that Clang might create faster code.&lt;/li&gt;
&lt;li&gt;Clang’s &lt;a href=&quot;https://clang.llvm.org/diagnostics.html&quot;&gt;diagnostics and fix-it hints&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
There are also drawbacks:&lt;br/&gt;&lt;ul&gt;&lt;li&gt;Clang doesn’t support C++/CX or #import “foo.dll”.&lt;/li&gt;
&lt;li&gt;MSVC offers paid support, Clang only gives you the code and the ability to write patches yourself (although the community is very active and helpful!).&lt;/li&gt;
&lt;li&gt;MSVC has better documentation.&lt;/li&gt;
&lt;li&gt;Advanced debugging features such as Edit &amp;amp; Continue don’t work when using Clang.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;How to use&lt;/h3&gt;
If you want to give Clang for Windows a try, there are two approaches:&lt;br/&gt;&lt;ol&gt;&lt;li&gt;You could use clang-cl, a compiler driver that tries to be command-line flag compatible with cl.exe (just like Clang tries to be command-line flag compatible with gcc). The &lt;a href=&quot;https://clang.llvm.org/docs/UsersManual.html#clang-cl&quot;&gt;Clang user manual&lt;/a&gt; describes how you can tell popular Windows build systems how to call clang-cl instead of cl.exe. We used this approach in Chrome to keep the Clang/Win build working alongside the MSVC build for years, with minimal maintenance cost. You can keep using link.exe, all your current compile flags, the MSVC debugger or windbg, ETW, etc. clang-cl even writes warning messages in a format that’s compatible with cl.exe so that you can click on build error messages in Visual Studio to jump to the right file and line. Everything should just work.&lt;/li&gt;
&lt;li&gt;Alternatively, if you have a cross-platform project and want to use gcc-style flags for your Windows build, you can pass a Windows triple (e.g. --target=x86_64-windows-msvc) to regular Clang, and it will produce MSVC-ABI-compatible output. Starting in Clang 7.0.0, due Fall 2018, Clang will also default to CodeView debug info with this triple.&lt;/li&gt;
&lt;/ol&gt;
Since Clang’s output is ABI-compatible with MSVC, you can build parts of your project with clang and other parts with MSVC. You can also pass &lt;a href=&quot;https://clang.llvm.org/docs/UsersManual.html#the-fallback-option&quot;&gt;/fallback&lt;/a&gt; to clang-cl to make it call cl.exe on files it can’t yet compile (this should be rare; it never happens in the Chrome build).&lt;p&gt;clang-cl accepts Microsoft language extensions needed to parse system headers but tries to emit -Wmicrosoft-foo warnings when it does so (warnings are ignored for system headers). You can choose to fix your code, or pass -Wno-microsoft-foo to Clang.&lt;/p&gt;&lt;p&gt;link.exe can produce regular PDB files from the CodeView information that Clang writes.&lt;br/&gt;&lt;/p&gt;&lt;h3&gt;Project History&lt;/h3&gt;
We switched chrome/mac and &lt;a href=&quot;http://blog.llvm.org/2015/01/using-clang-for-chrome-production.html&quot;&gt;chrome/linux&lt;/a&gt; to Clang a while ago. But on Windows, Clang was still missing support for parsing many Microsoft language extensions, and it didn’t have any Microsoft C++ ABI-compatible codegen at all. In 2013, we &lt;a href=&quot;http://blog.llvm.org/2013/09/a-path-forward-for-llvm-toolchain-on.html&quot;&gt;spun up a team&lt;/a&gt; to improve Clang’s Windows support, consisting half of Chrome engineers with a compiler background and half of other toolchain people. In mid-2014, Clang could &lt;a href=&quot;http://blog.llvm.org/2014/07/clangllvm-on-windows-update.html&quot;&gt;self-host on Windows&lt;/a&gt;. In February 2015, we had the first fallback-free build of 64-bit Chrome, in July 2015 the first fallback-free build of 32-bit Chrome (32-bit SEH was difficult). In Oct 2015, we shipped a first clang-built Chrome to the Canary channel. Since then, we’ve worked on &lt;a href=&quot;http://https//crbug.com/457078&quot;&gt;improving the size of Clang’s output&lt;/a&gt;, &lt;a href=&quot;https://crbug.com/636111&quot;&gt;improved Clang’s debug information&lt;/a&gt; (some of it behind -instcombine-lower-dbg-declare=0 for now), and A/B-tested stability and telemetry performance metrics.&lt;p&gt;We use versions of Clang that are pinned to a recent upstream revision that we update every one to three weeks, without any local patches. All our work is done in upstream LLVM.&lt;/p&gt;&lt;p&gt;Mid-2015, Microsoft announced that they were building on top of our work of making Clang able to parse all the Microsoft SDK headers with &lt;a href=&quot;https://blogs.msdn.microsoft.com/vcblog/2015/05/01/bringing-clang-to-windows/&quot;&gt;clang/c2&lt;/a&gt;, which used the Clang frontend for parsing code, but cl.exe’s codegen to generate code. &lt;a href=&quot;https://twitter.com/stephantlavavej/status/871861920315211776?lang=en&quot;&gt;Development on clang/c2 was halted again&lt;/a&gt; in mid-2017; it is conceivable that this was related to our improvements to MSVC-ABI-compatible Clang codegen quality. We’re thankful to Microsoft for publishing documentation on the PDB file format, answering many of our questions, fixing Clang compatibility issues in their SDKs, and for giving us publicity on their blog! Again, Clang is not a replacement for MSVC, but a complement to it.&lt;/p&gt;&lt;p&gt;Opera for Windows is also &lt;a href=&quot;http://blogs.opera.com/desktop/2018/02/opera-51/&quot;&gt;compiled with Clang&lt;/a&gt; starting in version 51.&lt;/p&gt;&lt;p&gt;Firefox is also looking at &lt;a href=&quot;https://ehsanakhgari.org/blog/2016-01-29/building-firefox-with-clang-cl-a-status-update&quot;&gt;using clang-cl for building Firefox for Windows&lt;/a&gt;.&lt;br/&gt;&lt;/p&gt;&lt;h3&gt;Next Steps&lt;/h3&gt;
Just as clang-cl is a cl.exe-compatible interface for Clang, lld-link is a link.exe-compatible interface for lld, the LLVM linker. Our next step is to use lld-link as an alternative to link.exe for linking Chrome for Windows. This has many of the same advantages as clang-cl (open-source, easy to update, …). Also, using clang-cl together with lld-link allows using &lt;a href=&quot;https://llvm.org/docs/LinkTimeOptimization.html&quot;&gt;LLVM-bitcode-based LTO&lt;/a&gt; (which in turn enables using &lt;a href=&quot;https://clang.llvm.org/docs/ControlFlowIntegrity.html&quot;&gt;CFI&lt;/a&gt;) and &lt;a href=&quot;http://blog.llvm.org/2018/01/improving-link-time-on-windows-with.html&quot;&gt;using PE/COFF extensions to speed up linking&lt;/a&gt;. A prerequisite for using lld-link was &lt;a href=&quot;http://blog.llvm.org/2017/08/llvm-on-windows-now-supports-pdb-debug.html&quot;&gt;its ability to write PDB files&lt;/a&gt;.&lt;br/&gt;We’re also considering using libc++ instead of the MSVC STL – this allows us to instrument the standard library, which is again useful for CFI and Address Sanitizer.&lt;br/&gt;&lt;h3&gt;In Closing&lt;/h3&gt;
Thanks to the whole LLVM community for helping to create the first new production C++ compiler for Windows in over a decade, and the first-ever open-source open-source C++ compiler that’s ABI-compatible with MSVC!&lt;br/&gt;
&lt;/div&gt;

</description>
<pubDate>Mon, 05 Mar 2018 20:58:10 +0000</pubDate>
<dc:creator>janvdberg</dc:creator>
<og:url>http://blog.llvm.org/2018/03/clang-is-now-used-to-build-chrome-for.html</og:url>
<og:title>Clang is now used to build Chrome for Windows</og:title>
<og:description> As of Chrome 64, Chrome for Windows is compiled with Clang. We now use Clang to build Chrome for all platforms it runs on: macOS, iOS, Lin...</og:description>
<dc:format>text/html</dc:format>
<dc:identifier>http://blog.llvm.org/2018/03/clang-is-now-used-to-build-chrome-for.html</dc:identifier>
</item>
<item>
<title>Counterintuitive Properties of High Dimensional Space</title>
<link>https://marckhoury.github.io/counterintuitive-properties-of-high-dimensional-space/</link>
<guid isPermaLink="true" >https://marckhoury.github.io/counterintuitive-properties-of-high-dimensional-space/</guid>
<description>&lt;p&gt;Our geometric intuition developed in our three-dimensional world often fails us in higher dimensions. Many properties of even simple objects, such as higher dimensional analogs of cubes and spheres, are very counterintuitive. Below we discuss just a few of these properties in an attempt to convey some of the weirdness of high dimensional space.&lt;/p&gt;&lt;p&gt;You may be used to using the word “circle” in two dimensions and “sphere” in three dimensions. However, in higher dimensions we generally just use the word sphere, or -sphere when the dimension of the sphere is not clear from context. With this terminology, a circle is also called a 1-sphere, for a 1-dimensional sphere. A standard sphere in three dimensions is called a 2-sphere, and so on. This sometimes causes confusion, because a -sphere is usually thought of as existing in -dimensional space. When we say -sphere, the value of  refers to the dimension of the sphere locally on the object, not the dimension in which it lives. Similarly we’ll often use the word cube for a square, a standard cube, and its higher dimensional analogues.&lt;/p&gt;
&lt;h3 id=&quot;escaping-spheres&quot;&gt;Escaping Spheres&lt;/h3&gt;
&lt;p&gt;Consider a square with side length 1. At each corner of the square place a circle of radius 1/2, so that the circles cover the edges of the square. Then consider the circle centered at the center of the square that is just large enough to touch the circles at the corners of the square. In two dimensions it’s clear that the inner circle is entirely contained in the square.&lt;/p&gt;
&lt;div align=&quot;middle&quot; readability=&quot;8&quot;&gt;&lt;img src=&quot;https://marckhoury.github.io/assets/images/Figure1.png&quot; width=&quot;300&quot;/&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; At each corner of the square we place a circle of radius 1/2. The inner circle is just large enough to touch the circles at the corners.&lt;/div&gt;
&lt;p&gt;We can do the same thing in three dimensions. At each corner of the unit cube place a sphere of radius 1/2, again covering the edges of the cube. The sphere centered at the center of the cube and tangent to spheres at the corners of the cube is shown in red in Figure 2. Again we see that, in three dimensions, the inner sphere is entirely contained in the cube.&lt;/p&gt;
&lt;img src=&quot;https://marckhoury.github.io/assets/images/Figure2.png&quot;/&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; In three dimensions we place a sphere at the each of the eight corners of a cube.
&lt;p&gt;To understand what happens in higher dimensions we need to compute the radius of the inner sphere in terms of the dimension. The radius of the inner sphere is equal to the length of the diagonal of the cube minus the radius of the spheres at the corners. The latter value is always 1/2, regardless of the dimension. We can compute the length of the diagonal as&lt;/p&gt;
&lt;p&gt;$$ \begin{align*} d((\frac{1}{2}, \frac{1}{2}, \ldots, \frac{1}{2}), (1,1, \ldots, 1)) &amp;amp;= \sqrt{\sum_{i = 1}^{d} (1 - 1/2)^2}\\ &amp;amp;= \sqrt{d}/2 \end{align*} $$&lt;/p&gt;
&lt;p&gt;Thus the radius of the inner sphere is . Notice that the radius of the inner sphere is increasing with the dimension!&lt;/p&gt;
&lt;img src=&quot;https://marckhoury.github.io/assets/images/Figure3.png&quot;/&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; The size of the radius of the inner sphere is growing as the dimension increases because the distance to the corner increases while the radius of the corner sphere remains constant.
&lt;p&gt;In dimensions two and three, the sphere is strictly inside the cube, as we’ve seen in the figures above. However in four dimensions something very interesting happens. The radius of the inner sphere is exactly 1/2, which is just large enough for the inner sphere to touch the sides of the cube! In five dimensions, the radius of the inner sphere is , and the sphere starts poking outside of the cube! By ten dimensions, the radius is  and the sphere is poking very far outside of the cube!&lt;/p&gt;
&lt;h3 id=&quot;volume-in-high-dimensions&quot;&gt;Volume in High Dimensions&lt;/h3&gt;
&lt;p&gt;The area of a circle , where  is the radius. Given the equation for the area of a circle, we can compute the volume of a sphere by considering cross sections of the sphere. That is, we intersect the sphere with a plane at some height  above the center of the sphere.&lt;/p&gt;
&lt;div align=&quot;middle&quot; readability=&quot;7&quot;&gt;&lt;img src=&quot;https://marckhoury.github.io/assets/images/Figure4.png&quot; width=&quot;300&quot;/&gt;&lt;strong&gt;Figure 4:&lt;/strong&gt; Intersecting the sphere with a plane gives a circle.&lt;/div&gt;
&lt;p&gt;The intersection between a sphere and a plane is a circle. If we look at the sphere from a side view, as shown in Figure 5, we see that the radius can be computed using the Pythagorean theorem (). The radius of the circle is .&lt;/p&gt;
&lt;div align=&quot;middle&quot; readability=&quot;8&quot;&gt;&lt;img src=&quot;https://marckhoury.github.io/assets/images/Figure5.png&quot; width=&quot;300&quot;/&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; A side view of Figure 4. The radius of the circle defined by the intersection can be found using the Pythagorean theorem.&lt;/div&gt;
&lt;p&gt;Summing up the area of each cross section from the bottom of the sphere to the top of the sphere gives the volume&lt;/p&gt;
&lt;p&gt;$$ \begin{align*} V &amp;amp;= \int_{-r}^{r} A(\sqrt{r^2 - h^2})\; dh\\ &amp;amp;= \int_{-r}^{r} \pi \sqrt{r^2 - h^2}^2 \; dh\\ &amp;amp;= \frac{4}{3}\pi r^3. \end{align*} $$&lt;/p&gt;
&lt;p&gt;Now that we know the volume of the -sphere, we can compute the volume of the -sphere in a similar way. The only difference is where before we used the equation for the area of a circle, we instead use the equation for the volume of the -sphere. The general formula for the volume of a -sphere is approximately&lt;/p&gt;
&lt;p&gt;$$ \begin{equation*} \frac{\pi^{d/2}}{(d/2+1)!}r^d. \end{equation*} $$&lt;/p&gt;
&lt;p&gt;(Approximately because the denominator should be the &lt;a href=&quot;https://en.wikipedia.org/wiki/Gamma_function&quot;&gt;Gamma function&lt;/a&gt;, but that’s not important for understanding the intuition.)&lt;/p&gt;
&lt;p&gt;Set  and consider the volume of the unit -sphere as  increases. The plot of the volume is shown in Figure 6.&lt;/p&gt;
&lt;img src=&quot;https://marckhoury.github.io/assets/images/Figure6.png&quot;/&gt;&lt;strong&gt;Figure 6:&lt;/strong&gt; The volume of the unit d-sphere goes to 0 as d increases!
&lt;p&gt;The volume of the unit -sphere goes to 0 as  grows! A high dimensional unit sphere encloses almost no volume! The volume increases from dimensions one to five, but begins decreasing rapidly toward 0 after dimension six.&lt;/p&gt;
&lt;h3 id=&quot;more-accurate-pictures&quot;&gt;More Accurate Pictures&lt;/h3&gt;
&lt;p&gt;Given the rather unexpected properties of high dimensional cubes and spheres, I hope that you’ll agree that the following are somewhat more accurate pictorial representations.&lt;/p&gt;
&lt;div align=&quot;middle&quot; readability=&quot;7&quot;&gt;&lt;img src=&quot;https://marckhoury.github.io/assets/images/Figure7.png&quot; height=&quot;200&quot;/&gt;&lt;strong&gt;Figure 7:&lt;/strong&gt; More accurate pictorial representations of high dimensional cubes (left) and spheres (right).&lt;/div&gt;
&lt;p&gt;Notice that the corners of the cube are much further away from the center than are the sides. The -sphere is drawn so that it contains almost no volume but still has radius 1. This image also suggests the next counterintuitive property of high dimensional spheres.&lt;/p&gt;
&lt;h3 id=&quot;concentration-of-measure&quot;&gt;Concentration of Measure&lt;/h3&gt;
&lt;p&gt;Suppose that you wanted to place a band around the equator of the sphere so that, say, 99% of the surface area of the sphere falls within that band. See Figure 8. How large do you think that band would have to be?&lt;/p&gt;
&lt;div align=&quot;middle&quot; readability=&quot;8&quot;&gt;&lt;img src=&quot;https://marckhoury.github.io/assets/images/Figure8.png&quot; height=&quot;250&quot;/&gt;&lt;strong&gt;Figure 8:&lt;/strong&gt; In two dimensions a the width of a band around the equator must be very large to contain 99% of the perimeter.&lt;/div&gt;
&lt;p&gt;In two dimensions the width of the band needs to be pretty large, indeed nearly 2, to capture 99% of the perimeter of the circle. However as the dimension increases the width of the band needed to capture 99% of the surface area gets smaller. In very high dimensional space nearly all of the surface area of the sphere lies a very small distance away from the equator!&lt;/p&gt;
&lt;img src=&quot;https://marckhoury.github.io/assets/images/Figure9.png&quot;/&gt;&lt;strong&gt;Figure 9:&lt;/strong&gt; As the dimension increases the width of the band necessary to capture 99% of the surface area decreases rapidly. Nearly all of the surface area of a high dimensional sphere lies near the equator.
&lt;p&gt;To provide some intuition consider the situation in two dimensions, as shown in Figure 10. For a point on the circle to be close to the equator, its -coordinate must be small.&lt;/p&gt;
&lt;img src=&quot;https://marckhoury.github.io/assets/images/Figure10.png&quot;/&gt;&lt;strong&gt;Figure 10:&lt;/strong&gt; Points near the equator have small y coordinate.
&lt;p&gt;What happens to the values of the coordinates as the dimensions increases? Figure 11 is a plot of 20000 random points sampled uniformly from a -sphere. As  increases the values become more and more concentrated around 0.&lt;/p&gt;
&lt;img src=&quot;https://marckhoury.github.io/assets/images/Figure11.gif&quot;/&gt;&lt;strong&gt;Figure 11:&lt;/strong&gt; As the dimension increases the coordinates become increasingly concentrated around 0.
&lt;p&gt;Recall that every point on a -sphere must satisfy the equation . Intuitively as  increases the number of terms in the sum increases, and each coordinate gets a smaller share of the single unit, on the average.&lt;/p&gt;
&lt;p&gt;The really weird thing is that any choice of “equator” works! It must, since the sphere is, well, spherically symmetrical. We could have just as easily have chosen any of the options shown in Figure 12.&lt;/p&gt;
&lt;img src=&quot;https://marckhoury.github.io/assets/images/Figure12.png&quot;/&gt;&lt;strong&gt;Figure 12:&lt;/strong&gt; Any choice of equator works equally well!
&lt;h3 id=&quot;kissing-numbers&quot;&gt;Kissing Numbers&lt;/h3&gt;
&lt;p&gt;Consider a unit circle in the plane, shown in Figure 13 in red. The blue circle is said to &lt;em&gt;kiss&lt;/em&gt; the red circle if it just barely touches the red circle. (Leave it to mathematicians to think that barely touching is a desirable property of a kiss.) The &lt;em&gt;kissing number&lt;/em&gt; is the maximum number of non-overlapping blue circles that can simultaneously kiss the red circle.&lt;/p&gt;
&lt;div align=&quot;middle&quot; readability=&quot;7&quot;&gt;&lt;img src=&quot;https://marckhoury.github.io/assets/images/Figure13.png&quot; width=&quot;300&quot;/&gt;&lt;strong&gt;Figure 13:&lt;/strong&gt; The kissing number is six in two dimensions.&lt;/div&gt;
&lt;p&gt;In two dimensions it’s easy to see that the kissing number is 6. The entire proof is shown in Figure 14. The proof is by contradiction. Assume that more than six non-overlapping blue circles can simultaneously kiss the red circle. We draw the edges from the center of the red circle to the centers of the blue circles, as shown in Figure 14. The angles between these edges must sum to exactly . Since there are more than six angles, at least one must be less than . The resulting triangle, shown in Figure 14, is an isosceles triangle. The side opposite the angle that is less than  must be strictly shorter than the other two sides, which are  in length. Thus the centers of the two circles must be closer than  and the circles must overlap, which is a contradiction.&lt;/p&gt;
&lt;div align=&quot;middle&quot; readability=&quot;11&quot;&gt;&lt;img src=&quot;https://marckhoury.github.io/assets/images/Figure14.png&quot;/&gt;&lt;strong&gt;Figure 14:&lt;/strong&gt; A proof that the kissing number is six in two dimensions. If more than six blue circles can kiss the red, then one of the angles must be less than 60 degrees. It follows that the two blue circles that form that angle must overlap, which is a contradiction.&lt;/div&gt;
&lt;p&gt;It is more difficult to see that in three dimensions the kissing number is 12. Indeed this was famously disputed between Isaac Newton, who correctly thought the kissing number was 12, and David Gregory, who thought it was 13. (Never bet against Newton.) Looking at the optimal configuration, it’s easy to see why Gregory thought it might be possible to fit a 13th sphere in the space between the other 12. As the dimension increases there is suddenly even more space between neighboring spheres and the problem becomes even more difficult.&lt;/p&gt;
&lt;div align=&quot;middle&quot; readability=&quot;7&quot;&gt;&lt;img src=&quot;https://marckhoury.github.io/assets/images/Figure15.png&quot; width=&quot;300&quot;/&gt;&lt;strong&gt;Figure 15:&lt;/strong&gt; The kissing number is 12 in three dimensions.&lt;/div&gt;
&lt;p&gt;In fact, there are very few dimensions where we know the kissing number exactly. In most dimensions we only have an upper and lower bound on the kissing number, and these bounds can vary by as much as several thousand spheres!&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Dimension&lt;/th&gt;
&lt;th&gt;Lower Bound&lt;/th&gt;
&lt;th&gt;Upper Bound&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;6&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;3&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;12&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;12&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;24&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;24&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;44&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;72&lt;/td&gt;
&lt;td&gt;78&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;126&lt;/td&gt;
&lt;td&gt;134&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;8&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;240&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;240&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;306&lt;/td&gt;
&lt;td&gt;364&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;500&lt;/td&gt;
&lt;td&gt;554&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;582&lt;/td&gt;
&lt;td&gt;870&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;840&lt;/td&gt;
&lt;td&gt;1357&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;1154&lt;/td&gt;
&lt;td&gt;2069&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;1606&lt;/td&gt;
&lt;td&gt;3183&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;2564&lt;/td&gt;
&lt;td&gt;4866&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;4320&lt;/td&gt;
&lt;td&gt;7355&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;5346&lt;/td&gt;
&lt;td&gt;11072&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;18&lt;/td&gt;
&lt;td&gt;7398&lt;/td&gt;
&lt;td&gt;16572&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;19&lt;/td&gt;
&lt;td&gt;10668&lt;/td&gt;
&lt;td&gt;24812&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;17400&lt;/td&gt;
&lt;td&gt;36764&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;27720&lt;/td&gt;
&lt;td&gt;54584&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;49896&lt;/td&gt;
&lt;td&gt;82340&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;23&lt;/td&gt;
&lt;td&gt;93150&lt;/td&gt;
&lt;td&gt;124416&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;24&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;196560&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;196560&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;As shown in the table, we only know the kissing number exactly in dimensions one through four, eight, and twenty-four. The eight and twenty-four dimensional cases follow from special lattice structures that are known to give optimal packings. In eight dimensions the kissing number is 240, given by the &lt;a href=&quot;https://en.wikipedia.org/wiki/E8_lattice&quot;&gt; lattice&lt;/a&gt;. In twenty-four dimensions the kissing number is 196560, given by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Leech_lattice&quot;&gt;Leech lattice&lt;/a&gt;. And not a single sphere more.&lt;/p&gt;
&lt;aside class=&quot;notice&quot;&gt;This post accompanies a talk given to high school students through Berkeley Splash. Thus intuition is prioritized over mathematical rigor, language is abused, and details are laborious spelled out. If you're interested in more rigorous treatments of the presented material, please feel free to contact me. Slides from the talk are available &lt;a href=&quot;https://people.eecs.berkeley.edu/~khoury/talks/BerkeleySplash.pdf&quot;&gt;here&lt;/a&gt;.&lt;/aside&gt;</description>
<pubDate>Mon, 05 Mar 2018 19:48:25 +0000</pubDate>
<dc:creator>marckhoury</dc:creator>
<og:type>article</og:type>
<og:title>Counterintuitive Properties of High Dimensional Space</og:title>
<og:url>https://marckhoury.github.io/counterintuitive-properties-of-high-dimensional-space/</og:url>
<og:description></og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://marckhoury.github.io/counterintuitive-properties-of-high-dimensional-space/</dc:identifier>
</item>
<item>
<title>Open-sourcing a 10x reduction in Apache Cassandra tail latency</title>
<link>https://engineering.instagram.com/open-sourcing-a-10x-reduction-in-apache-cassandra-tail-latency-d64f86b43589</link>
<guid isPermaLink="true" >https://engineering.instagram.com/open-sourcing-a-10x-reduction-in-apache-cassandra-tail-latency-d64f86b43589</guid>
<description>&lt;p name=&quot;1d80&quot; id=&quot;1d80&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;At Instagram, we have one of the world’s largest deployments of the Apache Cassandra database. We began using Cassandra in 2012 to replace Redis and support product use cases like fraud detection, Feed, and the Direct inbox. At first we ran Cassandra clusters in an AWS environment, but migrated them over to Facebook’s infrastructure when the rest of Instagram moved. We’ve had a really good experience with the reliability and availability of Cassandra, but saw room for improvement in read latency.&lt;br/&gt; &lt;br/&gt;Last year Instagram’s Cassandra team started working on a project to reduce Cassandra’s read latency significantly, which we call Rocksandra. In this post, I will describe the motivation for this project, the challenges we overcame, and performance metrics in both internal and public cloud environments.&lt;/p&gt;
&lt;h3 name=&quot;d01d&quot; id=&quot;d01d&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Motivation&lt;/h3&gt;
&lt;p name=&quot;c5aa&quot; id=&quot;c5aa&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;At Instagram, we use Apache Cassandra heavily as a general key value storage service. The majority of Instagram’s Cassandra requests are online, so in order to provide a reliable and responsive user experience for hundreds of millions of Instagram users, we have very tight SLA on the metrics. &lt;br/&gt; &lt;br/&gt;Instagram maintains a 5–9s reliability SLA, which means at any given time, the request failure rate should be less than 0.001%. For performance, we actively monitor the throughput and latency of different Cassandra clusters, especially the P99 read latency. &lt;br/&gt; &lt;br/&gt; Here’s a graph that shows the client-side latency of one production Cassandra cluster. The blue line is the average read latency (5ms) and the orange line is the P99 read latency (in the range of 25ms to 60ms and changing a lot based on client traffic).&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*Scn1Nm33oukOJpUd4Ukszw.png&quot; data-width=&quot;1052&quot; data-height=&quot;668&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*Scn1Nm33oukOJpUd4Ukszw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*Scn1Nm33oukOJpUd4Ukszw.png&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*ItBORNwCXce82ZNX6qf6Vg.png&quot; data-width=&quot;1052&quot; data-height=&quot;668&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*ItBORNwCXce82ZNX6qf6Vg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*ItBORNwCXce82ZNX6qf6Vg.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;0e33&quot; id=&quot;0e33&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;After investigation, we found the JVM garbage collector (GC) contributed a lot to the latency spikes. We defined a metric called GC stall percentage to measure the percentage of time a Cassandra server was doing stop-the-world GC (Young Gen GC) and could not serve client requests. Here’s another graph that shows the GC stall percentage on our production Cassandra servers. It was 1.25% during the lowest traffic time windows, and could be as high as 2.5% during peak hours.&lt;/p&gt;
&lt;p name=&quot;14d6&quot; id=&quot;14d6&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The graph shows that a Cassandra server instance could spend 2.5% of runtime on garbage collections instead of serving client requests. The GC overhead obviously had a big impact on our P99 latency, so if we could lower the GC stall percentage, we would be able to reduce our P99 latency significantly.&lt;/p&gt;
&lt;h3 name=&quot;87af&quot; id=&quot;87af&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Solution&lt;/h3&gt;
&lt;p name=&quot;5830&quot; id=&quot;5830&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Apache Cassandra is a distributed database with it’s own LSM tree-based storage engine written in Java. We found that the components in the storage engine, like memtable, compaction, read/write path, etc., created a lot of objects in the Java heap and generated a lot of overhead to JVM. To reduce the GC impact from the storage engine, we considered different approaches and ultimately decided to develop a C++ storage engine to replace existing ones. &lt;br/&gt; &lt;br/&gt;We did not want to build a new storage engine from scratch, so we decided to build the new storage engine on top of RocksDB. &lt;br/&gt; &lt;br/&gt;RocksDB is an open source, high-performance embedded database for key-value data. It’s written in C++, and provides official API language bindings for C++, C, and Java. RocksDB is optimized for performance, especially on fast storage like SSD. It’s widely used in the industry as the storage engine for MySQL, mongoDB, and other popular databases.&lt;/p&gt;
&lt;h3 name=&quot;cb1c&quot; id=&quot;cb1c&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Challenges&lt;/h3&gt;
&lt;p name=&quot;4c10&quot; id=&quot;4c10&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;We overcame three main challenges when implementing the new storage engine on RocksDB.&lt;br/&gt; &lt;br/&gt;The first challenge was that Cassandra does not have a pluggable storage engine architecture yet, which means the existing storage engine is coupled together with other components in the database. To find a balance between massive refactoring and quick iterations, we defined a new storage engine API, including the most common read/write and streaming interfaces. This way we could implement the new storage engine behind the API and inject it into the related code paths inside Cassandra.&lt;br/&gt; &lt;br/&gt;Secondly, Cassandra supports rich data types and table schema, while RocksDB provides purely key-value interfaces. We carefully defined the encoding/decoding algorithms to support Cassandra’s data model within RocksDB’s data structure and supported same-query semantics as original Cassandra. &lt;br/&gt; &lt;br/&gt;The third challenge was about streaming. Streaming is an important component for a distributed database like Cassandra. Whenever we join or remove a node from a Cassandra cluster, Cassandra needs to stream data among different nodes to balance the load across the cluster. The existing streaming implementation was based on the details in the current storage engine. Accordingly, we had to decouple them from each other, make an abstraction layer, and re-implement the streaming using RocksDB APIs. For high streaming throughput, we now stream data into temp sst files first, and then use the RocksDB ingest file API to bulk load them into the RocksDB instance at once.&lt;/p&gt;
&lt;h3 name=&quot;a1f4&quot; id=&quot;a1f4&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Performance metrics&lt;/h3&gt;
&lt;p name=&quot;7a2f&quot; id=&quot;7a2f&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;After about a year of development and testing, we have finished a first version of the implementation and successfully rolled it into several production Cassandra clusters in Instagram. In one of our production clusters, the P99 read latency dropped from 60ms to 20ms. We also observed that the GC stalls on that cluster dropped from 2.5% to 0.3%, which was a 10X reduction!&lt;br/&gt; &lt;br/&gt;We also wanted to verify whether Rocksandra would perform well in a public cloud environment. We setup a Cassandra cluster in an AWS environment using three i3.8 xlarge EC2 instances, each with 32 cores CPU, 244GB memory, and raid0 with 4 nvme flash disks. &lt;br/&gt; &lt;br/&gt;We used &lt;a href=&quot;https://github.com/Netflix/ndbench&quot; data-href=&quot;https://github.com/Netflix/ndbench&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;NDBench&lt;/a&gt; for the benchmark, and the default table schema in the framework:&lt;/p&gt;
&lt;blockquote name=&quot;7425&quot; id=&quot;7425&quot; class=&quot;graf graf--blockquote graf-after--p&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;&lt;code class=&quot;markup--code markup--blockquote-code&quot;&gt;TABLE emp (&lt;/code&gt;&lt;br/&gt; &lt;code class=&quot;markup--code markup--blockquote-code&quot;&gt;emp_uname text PRIMARY KEY,&lt;br/&gt;emp_dept text,&lt;br/&gt;emp_first text,&lt;br/&gt;emp_last text&lt;/code&gt;&lt;br/&gt; &lt;code class=&quot;markup--code markup--blockquote-code&quot;&gt;)&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;e302&quot; id=&quot;e302&quot; class=&quot;graf graf--p graf-after--blockquote&quot;&gt;We pre-loaded 250M 6KB rows into the database (each server stores about 500GB data on disk). We configured 128 readers and 128 writers in NDBench.&lt;br/&gt; &lt;br/&gt;We tested different workloads and measured the avg/P99/P999 read/write latencies. As you can see, Rocksandra provided much lower and consistent tail read/write latency.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*Mpvc-jd61xmcrE4aEth4NA.png&quot; data-width=&quot;1132&quot; data-height=&quot;725&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*Mpvc-jd61xmcrE4aEth4NA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*Mpvc-jd61xmcrE4aEth4NA.png&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*zZO7xeU8fsWosWbkev873g.png&quot; data-width=&quot;1131&quot; data-height=&quot;724&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*zZO7xeU8fsWosWbkev873g.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*zZO7xeU8fsWosWbkev873g.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;7f48&quot; id=&quot;7f48&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;We also tested a read-only workload and observed that, at similar P99 read latency (2ms), Rocksandra could provide 10X higher read throughput (300K/s for Rocksandra vs. 30K/s for C* 3.0).&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*E-2efj-mMo0dQWEvZyxn1g.png&quot; data-width=&quot;1483&quot; data-height=&quot;746&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*E-2efj-mMo0dQWEvZyxn1g.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*E-2efj-mMo0dQWEvZyxn1g.png&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*d5gs5SJzq6laocevBqA1Bg.png&quot; data-width=&quot;1359&quot; data-height=&quot;731&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*d5gs5SJzq6laocevBqA1Bg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*d5gs5SJzq6laocevBqA1Bg.png&quot;/&gt;&lt;/div&gt;
&lt;h3 name=&quot;d343&quot; id=&quot;d343&quot; class=&quot;graf graf--h3 graf-after--figure&quot;&gt;Future work&lt;/h3&gt;
&lt;p name=&quot;5bac&quot; id=&quot;5bac&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;We have open sourced our &lt;a href=&quot;https://github.com/Instagram/cassandra/tree/rocks_3.0&quot; data-href=&quot;https://github.com/Instagram/cassandra/tree/rocks_3.0&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Rocksandra code base&lt;/a&gt; and &lt;a href=&quot;https://github.com/Instagram/cassandra-aws-benchmark&quot; data-href=&quot;https://github.com/Instagram/cassandra-aws-benchmark&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;benchmark framework&lt;/a&gt;, which you can download from Github to try out in your own environment! Please let us know how it performs.&lt;br/&gt; &lt;br/&gt;As our next step, we are actively working on the development of more C* features support, like secondary indexes, repair, etc. We are also working on a &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13474&quot; data-href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13474&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;C* pluggable storage engine architecture&lt;/a&gt; to contribute our work back to the Apache Cassandra community. &lt;br/&gt; &lt;br/&gt;If you are in the Bay Area and are interested in learning more about our Cassandra developments, join us at our next meetup event &lt;a href=&quot;https://www.meetup.com/Apache-Cassandra-Bay-Area/events/248376266/&quot; data-href=&quot;https://www.meetup.com/Apache-Cassandra-Bay-Area/events/248376266/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p name=&quot;3171&quot; id=&quot;3171&quot; class=&quot;graf graf--p graf-after--p graf--trailing&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;Dikang Gu is an infrastructure engineer at Instagram.&lt;/em&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 05 Mar 2018 18:20:37 +0000</pubDate>
<dc:creator>mikeyk</dc:creator>
<og:title>Open-sourcing a 10x reduction in Apache Cassandra tail latency</og:title>
<og:url>https://engineering.instagram.com/open-sourcing-a-10x-reduction-in-apache-cassandra-tail-latency-d64f86b43589</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*Scn1Nm33oukOJpUd4Ukszw.png</og:image>
<og:description>At Instagram, we have one of the world’s largest deployments of the Apache Cassandra database. We began using Cassandra in 2012 to replace…</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://engineering.instagram.com/open-sourcing-a-10x-reduction-in-apache-cassandra-tail-latency-d64f86b43589?gi=86845ea86aa3</dc:identifier>
</item>
<item>
<title>Notes for new Make users</title>
<link>http://gromnitsky.users.sourceforge.net/articles/notes-for-new-make-users/</link>
<guid isPermaLink="true" >http://gromnitsky.users.sourceforge.net/articles/notes-for-new-make-users/</guid>
<description>&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;/&gt;&lt;meta name=&quot;generator&quot; content=&quot;pandoc&quot;/&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0, user-scalable=yes&quot;/&gt;&lt;meta name=&quot;author&quot; content=&quot;Alexander Gromnitsky&quot;/&gt;&lt;title&gt;Notes for new Make users&lt;/title&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;style.css&quot; type=&quot;text/css&quot;/&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;601.18816682832&quot;&gt;
&lt;header readability=&quot;1.5454545454545&quot;&gt;
&lt;p class=&quot;author&quot;&gt;&lt;a href=&quot;http://gromnitsky.blogspot.com&quot;&gt;Alexander Gromnitsky&lt;/a&gt;&lt;/p&gt;
&lt;p class=&quot;date&quot;&gt;Last update: Tue 6 Mar 2018 03:44:54 +0200&lt;/p&gt;
&lt;/header&gt;&lt;nav id=&quot;4b6d995-TOC&quot;/&gt;&lt;p&gt;&lt;img src=&quot;http://gromnitsky.users.sourceforge.net/articles/notes-for-new-make-users/man-on-horse.svg&quot; class=&quot;man-on-horse&quot;/&gt;&lt;/p&gt;
&lt;p&gt;John Constable would have said that ‘There is nothing ugly in Make; I never saw an ugly build system in my life: for let the number of targets be what it may,–dependencies, recipes, and parallel jobs will always make it manageable.’&lt;/p&gt;
&lt;p&gt;It’s funny to see a sudden spike of interest in Make-like tools on &lt;a href=&quot;https://news.ycombinator.com/item?id=16483889&quot;&gt;HN&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://www.reddit.com/r/programming/comments/80x5z6/&quot;&gt;reddit&lt;/a&gt;, when many start joyfully sharing their favourite tricks. I predict some ppl will get overexcited &amp;amp; start rewriting their existing build infrastructure to be more “Make friendly” only to realize it’s not that straightforward as it sounds.&lt;/p&gt;
&lt;p&gt;The discussion about Make ordinaly ends up w/ ideas how Make can be improved. Nevertheless, instead of &lt;em&gt;improving&lt;/em&gt; Make, avid &amp;amp; enthusiastic chaps invariably decide to start from ground zero, producing something unequivocally awesome, but incompatible w/ everything that was written before.&lt;/p&gt;
&lt;p&gt;If you say Make is suboptimal, no one sane will disagree w/ you. However, it’s a solid, well known tool that (if used correctly, w/o wonky religious zeal) works like the &lt;a href=&quot;https://en.wikipedia.org/wiki/Beverly_Clock&quot;&gt;Beverly Clock&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you really curious about (GNU) Make, read on. I’m not going to evangelize Make, I assume you’ve already decided to try it.&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-dont-try-to-be-clever&quot;&gt;Don’t try to be clever&lt;/h2&gt;
&lt;p&gt;Regardless of how you like your makefiles now, you’ll cringe at them in 6 months &amp;amp; laugh at them in 1 year. Your style will evolve.&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-the-official-docs&quot;&gt;The official docs&lt;/h2&gt;
&lt;p&gt;When in doubt, don’t google until you read the &lt;a href=&quot;https://www.gnu.org/software/make/manual/&quot;&gt;official manual&lt;/a&gt; &lt;del&gt;from cover to cover&lt;/del&gt;. It’s a document of an astonishing quality. Go print the pdf.&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-adhere-to-the-common-terminology&quot;&gt;Adhere to the common terminology&lt;/h2&gt;
&lt;pre&gt;
&lt;code&gt;target: prerequisite(s)
        recipe&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;so in&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;out/bundle.js: main.js foo.js bar.js
        mkdir $(dir $@)
        browserify main.js -o $@&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;out/bundle.js&lt;/code&gt; is a &lt;em&gt;target&lt;/em&gt; that has 3 &lt;em&gt;prerequisites&lt;/em&gt; (dependencies): &lt;code&gt;main.js&lt;/code&gt;, &lt;code&gt;foo.js&lt;/code&gt;, &lt;code&gt;bar.js&lt;/code&gt;. The set of lines prefixed w/ the TAB char is a &lt;em&gt;recipe&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Rules may have empty recipes:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;deploy: rsync
rsync: compile
        rsync out/ user@host:/somewhere/
compile: out/bundle.js&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The 1st rule that Make stumbles upon is called a &lt;em&gt;default goal&lt;/em&gt;. Here, the default goal is &lt;code&gt;deploy&lt;/code&gt;. You can override the default goal by passing the name of a desired target as an argument to the Make cmd:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ make compile&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Macros&lt;/em&gt; serve a 2fold purpose: as variables whose values you can override from the command line, &amp;amp; as a mechanism for writing custom functions. I won’t talk about the latter much, but here’s an example of the former, which you may find moderately amusing to play w/:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;date = $(shell date -d $(today) +%Y-%m-%d)
today = now

comic: $(date).gif
        xv $&amp;lt;

%.html:
        wget -q http://dilbert.com/strip/$* -O $@

%.gif: %.html
        nokogiri -e 'p $$_.css(&quot;.comic-item-container&quot;).first[&quot;data-image&quot;]' $&amp;lt; | xargs wget -q -O $@&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;If you save it in the file named &lt;code&gt;Makefile&lt;/code&gt;, then type &lt;code&gt;make&lt;/code&gt; in the same directory, Make downloads the current Dilbert strip page, parses the html, downloads the .gif image &amp;amp; displays it. If you run &lt;code&gt;make&lt;/code&gt; again, it won’t re-download or re-parse anything, but will display the .gif image right away.&lt;/p&gt;
&lt;p&gt;If you run&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ make today=2002-04-10
wget -q http://dilbert.com/strip/2002-04-10 -O 2002-04-10.html
nokogiri -e 'p $_.css(&quot;.comic-item-container&quot;).first[&quot;data-image&quot;]' 2002-04-10.html | xargs wget -q -O 2002-04-10.gif
xv 2002-04-10.gif&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;the &lt;code&gt;today&lt;/code&gt; macro gets overriden from its default &lt;code&gt;now&lt;/code&gt; string to the supplied &lt;code&gt;2002-04-10&lt;/code&gt; value. Notice how &lt;code&gt;date&lt;/code&gt; macro recursively expands to get the properly formatted date string. It uses the internal &lt;code&gt;shell()&lt;/code&gt; fn to get the output of the external &lt;code&gt;date&lt;/code&gt; command.&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-most-important-concept-1-dag&quot;&gt;Most important concept #1: DAG&lt;/h2&gt;
&lt;blockquote readability=&quot;4.5&quot;&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;‘The &lt;em&gt;make&lt;/em&gt; utility shall update files that are derived from other files.’&lt;br/&gt;— The Open Group Base Specifications Issue 7&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;The 1st thing you need to understand about Make is that all you do is contruct &lt;a href=&quot;https://en.wikipedia.org/wiki/Directed_acyclic_graph&quot;&gt;DAG&lt;/a&gt;. You either do it by hand using simple explicit rules (I’m skipping the recipes for brevity):&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;foo.js.min: foo.es5
foo.es5: foo.js&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;which means a straightforward &lt;code&gt;foo.min.js&lt;/code&gt; → &lt;code&gt;foo.es5&lt;/code&gt; → &lt;code&gt;foo.js&lt;/code&gt; graph (where an arrow means &lt;em&gt;depends on&lt;/em&gt;) or you use special &lt;em&gt;pattern rules&lt;/em&gt; (smtms also called &lt;em&gt;metarules&lt;/em&gt;) w/ which Make can &lt;em&gt;generate&lt;/em&gt; DAG vertices (nodes) for you automatically:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;%.js.min: %.es5
%.es5: %.js&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In both cases, when you type &lt;code&gt;make foo.min.js&lt;/code&gt; &amp;amp; you indeed have &lt;code&gt;foo.js&lt;/code&gt; file–Make creates a proper dependency graph, checks that it doesn’t contain cycles, takes &lt;code&gt;foo.min.js&lt;/code&gt; as it’s goal, sees that such a vertex has an out-degree number of 1 (i.e., it has exactly 1 &lt;em&gt;prerequisite&lt;/em&gt;–&lt;code&gt;foo.es5&lt;/code&gt;), recursively jumps from vertex to vertex until it hits a vertex w/ 0 prereqs (in compsci speak: w/ an out-degree number of 0). Then it creates the lonely leaf &amp;amp; unwinds itself until it reaches the orig goal. If anything goes wrong along the way, Make stops.&lt;/p&gt;
&lt;p&gt;In the case w/ the pattern rules, Make cannot create the full DAG until it has all the vertices, hence it automatically searches for a so called &lt;em&gt;implicit rule&lt;/em&gt; for file &lt;code&gt;foo.js&lt;/code&gt;, finds a perfect match in &lt;code&gt;%.es5: %.js&lt;/code&gt;, then looks for a rule for &lt;code&gt;foo.es5&lt;/code&gt; &amp;amp; so on.&lt;/p&gt;
&lt;p&gt;This is all you need to know. When Make looks at a single graph endpoint, it runs a recipe for a target (&lt;em&gt;remakes&lt;/em&gt; it) iff its prereqs are “newer”. This is how it avoids unnecessary work &amp;amp; as any young lad should know, the only way to improve performance is not to do something faster but not do it at all.&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-most-important-concept-2-2-phases&quot;&gt;Most important concept #2: 2 phases&lt;/h2&gt;
&lt;p&gt;The 2nd thing you need to understand is how Make reads makefiles. It does it in 2 phases:&lt;/p&gt;
&lt;ol type=&quot;I&quot;&gt;&lt;li&gt;Creates a DAG.&lt;/li&gt;
&lt;li&gt;Invokes the recipes during which it resolves the required macros.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;E.g., after the phase I, a makefile&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;opts = --tree-shake
bundle = foo.js

$(bundle).es5: $(bundle).js
        babel $(opts) $&amp;lt; -o $@&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;to Make looks like&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;opts = ?
bundle = ?

foo.es5: foo.js
        ?&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Make was forced to expand &lt;code&gt;bundle&lt;/code&gt; macro in the rule def line, but it did nothing for &lt;code&gt;opts&lt;/code&gt; in the recipe.&lt;/p&gt;
&lt;p&gt;If you forget about the 2 phases, it could lead to a misunderstanding when a rule creates some file that you expect Make to pick up later on.&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-the-order-of-the-garter&quot;&gt;The order &lt;s&gt;of the garter&lt;/s&gt;&lt;/h2&gt;
&lt;p&gt;All macros &amp;amp; rules are hoisted so the order it which they appear in makefiles is irrelevant, but Make has also ordinal variables (called &lt;em&gt;simply expanded variables&lt;/em&gt;, SEVs or plainly &lt;em&gt;variables&lt;/em&gt;):&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt; src := foo.js bar.js&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;(note &lt;code&gt;:=&lt;/code&gt; instead of &lt;code&gt;=&lt;/code&gt;) that get expanded exactly 1 time during the phase I only. Hence this will work as expected:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;bundle.deps = $(js) vendor/baz.js
js = foo.js bar.js&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;but this won’t:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;bundle.deps := $(js) vendor/baz.js
js := foo.js bar.js&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;here, &lt;code&gt;bundle.deps&lt;/code&gt; will contain only &lt;code&gt;vendor/baz.js&lt;/code&gt; string. Remember that if you use SEVs, their order does matter as well as the order of rules/macros that employ variables.&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-automatic-variables&quot;&gt;Automatic variables&lt;/h2&gt;
&lt;p&gt;If you write metarules you can’t do it w/ the &lt;em&gt;automatic variables&lt;/em&gt;, for in a rule&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;out/.cache/%.js: %.mjs
        babel $&amp;lt; -o $@&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;it’s impossible to know beforehand the actual names of the files. The popular clamour is that the autovars are too short &amp;amp; confusing. Unless you like to write magic makefiles (e.g., w/ &lt;code&gt;.SECONDEXPANSION&lt;/code&gt; nifty tricks), you’ll need to remember only 3 autovar types:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;code&gt;$@&lt;/code&gt; == target&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$&amp;lt;&lt;/code&gt; == the 1st prereq&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$^&lt;/code&gt; == all prereqs&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;img src=&quot;http://gromnitsky.users.sourceforge.net/articles/notes-for-new-make-users/autovars.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Are they really so hard to grasp?&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-functions&quot;&gt;Functions&lt;/h2&gt;
&lt;p&gt;Make comes w/ a set of internal functions for text processing. Most of the are pure, idempotent &amp;amp; don’t do any IO. E.g., &lt;code&gt;dir&lt;/code&gt; fn&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$(dir lib/foo.js vendor/bar.js)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;treats its argument as a string, returning &lt;code&gt;lib/ vendor/&lt;/code&gt; regardless of whether &lt;code&gt;lib/foo.js&lt;/code&gt; or &lt;code&gt;vendor/bar.js&lt;/code&gt; exist.&lt;/p&gt;
&lt;p&gt;Some functions (&lt;code&gt;patsubst()&lt;/code&gt;, &lt;code&gt;filter()&lt;/code&gt;, &lt;code&gt;filter-out()&lt;/code&gt;) support a tiny DSL: strings that use &lt;code&gt;%&lt;/code&gt; char as a wildcard. If you don’t fully get &lt;a href=&quot;http://gromnitsky.blogspot.com/2016/10/how-gnu-makes-patsubst-function-really.html&quot;&gt;the purpose of the &lt;code&gt;%&lt;/code&gt;&lt;/a&gt;, the results are often confusing.&lt;/p&gt;
&lt;p&gt;The 2 most important fn that do IO are:&lt;/p&gt;
&lt;ul readability=&quot;4.8991935483871&quot;&gt;&lt;li&gt;&lt;code&gt;wildcard()&lt;/code&gt; that in Linux/Mac/BSD uses glob(3) under the hood &amp;amp; &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/aa364418(v=vs.85).aspx&quot;&gt;smthg else&lt;/a&gt; in Windows;&lt;/li&gt;
&lt;li readability=&quot;13&quot;&gt;
&lt;p&gt;&lt;code&gt;shell()&lt;/code&gt; that is the equivalent of the bash command substitution (&lt;code&gt;`command`&lt;/code&gt;). Only in reality, Make’s&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$(shell find . -name \*.js)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;is more like typing&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ /bin/sh -c 'find . -name \*.js' | tr \\n ' '&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;in bash; i.e., after explicitly running the cmd under the particular shell, it replaces every newline w/ a space.&lt;/p&gt;
&lt;p&gt;On Windows it obviously cannot assume that bash is installed, thence is uses the horrible &lt;code&gt;cmd.exe&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Because of this historical Unix vs. Windows diff, the pattern lang in &lt;code&gt;windcard()&lt;/code&gt; &amp;amp; the command syntax in &lt;code&gt;shell()&lt;/code&gt; are both inherently non-portable, unfortunately.&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-why-doesnt-make-support-file-names-w-spaces&quot;&gt;Why doesn’t Make support file names w/ spaces?&lt;/h2&gt;
&lt;p&gt;Initially it was a cunning plan to remove the necessity of having a support for a special list data type. Perhaps &lt;a href=&quot;https://en.wikipedia.org/wiki/Stuart_Feldman&quot;&gt;Stuart Feldman&lt;/a&gt; could have used &lt;code&gt;,&lt;/code&gt; instead of a space for a delimiter, but that ship has long sailed. Also recall that many Unix utils that deal w/ file names return the list of file names join()’ed by a space or a newline. Thus the choice of a space char was very natural.&lt;/p&gt;
&lt;p&gt;As Make doesn’t have any concept of lists, rules get their prereqs as a string: after expanding all macros, a rule chops the string into pieces that all together look like an array of targets, where each target may or may not be a file name.&lt;/p&gt;
&lt;p&gt;In JS it would have looked like:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;&amp;gt; src = 'foo.js bar.js'
'foo.js bar.js'
&amp;gt; ` ${src} baz.css`.trim().split(/\s+/)
[ 'foo.js', 'bar.js', 'baz.css' ]&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The same goes for function arguments.&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-compile-everything-to-1-directory&quot;&gt;Compile everything to 1 directory&lt;/h2&gt;
&lt;p&gt;You may say it’s a matter of style, but I don’t like seeing the results of compilation scattered along the src files. I consider it a common antipattern. It’s popular, for makefiles that produce such output are the easiest to write, especially for novices. Partially this is also Make’s fault, for its collection of built-in metarules (observable via &lt;code&gt;make -p&lt;/code&gt;) has schooled ppl to write similar rules for their tools.&lt;/p&gt;
&lt;p&gt;To remove the compilation results, ppl write &lt;code&gt;clean&lt;/code&gt; targets, which are usually updated w/ the same consistency as comments in the code.&lt;/p&gt;
&lt;p&gt;Don’t write &lt;code&gt;clean&lt;/code&gt; or &lt;code&gt;nuke&lt;/code&gt; targets. If your output goes under 1 umbrella dir, then all you need to do for starting from the clean slate is to remove 1 directory.&lt;/p&gt;
&lt;p&gt;If you’re writing an SPA, copy your static assets &amp;amp; the relevant files from &lt;code&gt;node_modules&lt;/code&gt; to the output dir too. A good makefile leaves a ready to deploy directory that doesn’t depend on files left in the src dir.&lt;/p&gt;
&lt;p&gt;E.g., say the output directory is named &lt;code&gt;_out&lt;/code&gt;. Our src tree:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;.
├── src
│   ├── a.js
│   ├── b.js
│   ├── index.html
│   ├── style.css
│   └── main.js
└── Makefile&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;We need to transpile multiple &lt;code&gt;src/*.js&lt;/code&gt; files before combining them to a bundle.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ cat Makefile
out := _out
cache := $(out)/.cache
build := $(out)/development
mkdir = @mkdir -p $(dir $@)

all: $(build)/main.js

js.src := $(wildcard src/*.js)
js.dest := $(addprefix $(cache)/, $(js.src))

$(build)/main.js: $(js.dest)
        $(mkdir)
        browserify $(cache)/src/main.js -o $@

$(cache)/%.js: %.js
        $(mkdir)
        babel $&amp;lt; -o $@&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Run Make, &amp;amp; it creates the umbrella dir &amp;amp; populates it w/ the compilation results:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ make
babel src/a.js -o _out/.cache/src/a.js
babel src/b.js -o _out/.cache/src/b.js
babel src/main.js -o _out/.cache/src/main.js
browserify _out/.cache/src/main.js -o _out/development/main.js

$ tree --noreport -a _out
_out/
├── .cache
│   └── src
│       ├── a.js
│       ├── b.js
│       └── main.js
└── development
    └── main.js&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Of course to be able to test &lt;code&gt;_out/development&lt;/code&gt; in the browser we need to copy the static assets to the umbrella dir too. Adding this to the makefile&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$(build)/%: src/%
        $(mkdir)
        cp $&amp;lt; $@

static.src := $(wildcard src/*.css src/*.html)
static.dest := $(patsubst src/%, $(build)/%, $(static.src))

all: $(static.dest)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;… accomplishes our goal:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ make
cp src/style.css _out/development/style.css
cp src/index.html _out/development/index.html&lt;/code&gt;
&lt;/pre&gt;
&lt;h2 id=&quot;4b6d995-multiple-builds&quot;&gt;Multiple builds&lt;/h2&gt;
&lt;p&gt;It’s easy to add support for several builds from a single src directory. Make supports &lt;em&gt;conditional&lt;/em&gt; directives, w/ which you may alter parameters to transpilers, change the output dir, etc. E.g., the enhanced version of the makefile from the prev section:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;NODE_ENV ?= development
out := _out
cache := $(out)/.cache.$(NODE_ENV)
build := $(out)/$(NODE_ENV)
babel.opts := -s inline
browserify.opts := -d

ifeq ($(NODE_ENV), production)
babel.opts := --minified
browserify.opts :=
endif

mkdir = @mkdir -p $(dir $@)

all: $(build)/main.js

js.src := $(wildcard src/*.js)
js.dest := $(addprefix $(cache)/, $(js.src))

$(build)/main.js: $(js.dest)
        $(mkdir)
        browserify $(browserify.opts) $(cache)/src/main.js -o $@

$(cache)/%.js: %.js
        $(mkdir)
        babel $(babel.opts) $&amp;lt; -o $@&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Conditional directives are evaluated during the phase I. There’s also &lt;code&gt;if()&lt;/code&gt; fn that can be used in macros, but there’s no &lt;code&gt;eq()&lt;/code&gt; fn (it exists in the Make’s src code, but under ‘experimental’ flag), that limits the applicability of &lt;code&gt;if()&lt;/code&gt;. You can play w/ &lt;code&gt;filter()&lt;/code&gt; inside &lt;code&gt;if()&lt;/code&gt; but it quickly gets unreadable.&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-chain-of-rules&quot;&gt;Chain of rules&lt;/h2&gt;
&lt;p&gt;There are 2 categories of ppl:&lt;/p&gt;
&lt;ol type=&quot;1&quot; readability=&quot;3&quot;&gt;&lt;li readability=&quot;5&quot;&gt;
&lt;p&gt;those who write multiple rules, that work in a chain: e.g., first we transpile js, second we minify the transpiled output;&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;%.js.min: %.es5
%.es5: %.js&lt;/code&gt;
&lt;/pre&gt;&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;those who write a single rule .min.js → js, doing all steps in 1 recipe; there are 2 subcategories of such ppl as well:&lt;/p&gt;
&lt;ol type=&quot;a&quot; readability=&quot;1.5&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;those who create tmp files in the recipe;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;4&quot;&gt;
&lt;p&gt;those who use pipes, avoiding tmp files whatsoever; unfortunately I cannot recoment this method, for /bin/sh doesn’t signal an error if any of the cmds in a pipeline fail, except for the last one; with this approach you may (&amp;amp; will) easily end up w/ garbage in output or 0-length files.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;When deciding what target to build next, Make is capable of recognising which targets are temporary.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;foo.js.min: foo.es5
foo.es5: foo.js&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here, &lt;code&gt;foo.es5&lt;/code&gt; is an &lt;em&gt;intermediary&lt;/em&gt;, but as it’s explicitly mentioned in makefile, it won’t be recognized as such.&lt;/p&gt;
&lt;p&gt;The makefile below (taken from a simple &lt;a href=&quot;https://github.com/gromnitsky/shopping-hours&quot;&gt;shopping-hours&lt;/a&gt; program) uses the sequence of implicit metarules to create 2 UMD bundles: a minimized es5 &amp;amp; a usual js parcel:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;out := dist

mkdir = @mkdir -p $(dir $@)
bundle.name := $(out)/shopping_hours

compile: $(bundle.name).min.js

$(out)/%.min.js: $(out)/%.es5.js
        uglifyjs $&amp;lt; -o $@ -mc

$(out)/%.es5.js: $(out)/%.js
        babel --presets `npm -g root`/babel-preset-es2015 $&amp;lt; -o $@

$(bundle.name).js: index.js
        $(mkdir)
        browserify -s $(basename $(notdir $@)) $&amp;lt; -o $@&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;$ make
browserify -s shopping_hours index.js -o dist/shopping_hours.js
babel --presets `npm -g root`/babel-preset-es2015 dist/shopping_hours.js -o dist/shopping_hours.es5.js
uglifyjs dist/shopping_hours.es5.js -o dist/shopping_hours.min.js -mc
rm dist/shopping_hours.es5.js&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Notice the last line (the rm command). There’s nowhere such a line cound be found in the makefile! Make has automatically deduced that &lt;code&gt;dist/shopping_hours.es5.js&lt;/code&gt; vertex is temporal &amp;amp; auto removed it before exiting.&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-write-makefiles-in-shell-scripts-stead&quot;&gt;Write makefiles in shell scripts stead&lt;/h2&gt;
&lt;p&gt;If you already have a bunch of small .sh files–move them to 1 makefile in the form of 1 .sh file == 1 target + recipe. By doint this you’ll get for free:&lt;/p&gt;
&lt;ol type=&quot;a&quot;&gt;&lt;li&gt;the dependency management (what target to run first);&lt;/li&gt;
&lt;li&gt;command line args processing (foo=bar args).&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Make can run several recipes at once. Recall the Dilbert makefile. To download all the comics (starting from April 16, 1989) just generate the corresponding dates &amp;amp; pass them as targets. There is no need to modify the makefile itself.&lt;/p&gt;
&lt;p&gt;Generate the target names:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ seq `date -d 1989-04-16 +%s` $((60*60*24)) `date +%s` | xargs -Isec date -d @sec +%Y-%m-%d.gif &amp;gt; targets.txt
$ head -3 !$
1989-04-16.gif
1989-04-17.gif
1989-04-18.gif&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Run Make w/ 50 parallel jobs (on a 2nd thought, don’t type that):&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ make -j50 `cat targets.txt`&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;http://gromnitsky.users.sourceforge.net/articles/notes-for-new-make-users/nyan-cat.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;(I’ve got banned pretty quickly.)&lt;/p&gt;
&lt;p&gt;The beauty of the approach is that you may press Ctrl-C any time &amp;amp; when you run Make again it won’t re-download already processed pages.&lt;/p&gt;
&lt;p&gt;You can even mask the makefile for a standalone script: add a proper shebang line to the aforementioned Dilbert makefile:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ printf '%s\n\n' '#!/usr/bin/make -f' | cat - Makefile &amp;gt; dilbert
$ chmod +x !$&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Now users can run &lt;code&gt;dilbert today=1999-02-21&lt;/code&gt; w/o ever suspecting they are using Make.&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-tabs-shell&quot;&gt;Tabs &amp;amp; shell&lt;/h2&gt;
&lt;blockquote readability=&quot;9.5&quot;&gt;
&lt;blockquote readability=&quot;16&quot;&gt;
&lt;p&gt;‘Why the tab in column 1? Yacc was new, Lex was brand new. I hadn’t tried either, so I figured this would be a good excuse to learn. After getting myself snarled up with my first stab at Lex, I just did something simple with the pattern newline-tab. It worked, it stayed. And then a few weeks later I had a user population of about a dozen, most of them friends, and I didn’t want to screw up my embedded base. The rest, sadly, is history.’&lt;br/&gt;— Stuart Feldman&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;First, fix your editor, any decent one can highlight tabs. Second, if you’re still against tabs, redefine &lt;code&gt;.RECIPEPREFIX&lt;/code&gt; variable:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;space :=
space +=
.RECIPEPREFIX := $(space)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;w/ which you can use spaces before any recipe line.&lt;/p&gt;
&lt;p&gt;If you’re feeling mischievous, Make has divers ‘secret’ vars &amp;amp; targets for you. For instance, you don’t have to ‘struggle’ w/ the ancient bash, for it’s possible to program recipes in any language that supports evaluations straight from the command line. This’ll help your prank get started:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ cat Makefile
SHELL := node
.SHELLFLAGS := -e

date != console.log(new Date())
q:
        @console.log('today is $(date)')&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;$ make
today is 2018-03-03T10:02:05.789Z&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;!=&lt;/code&gt; is a syntactic sugar for the &lt;code&gt;shell()&lt;/code&gt; fn.&lt;/p&gt;
&lt;p&gt;Think that running every recipe line in a sub-shell is ‘expensive’? Make’s got you covered! Mentioning &lt;code&gt;.ONESHELL&lt;/code&gt; &lt;em&gt;target&lt;/em&gt; in a makefile sends all lines in a recipe to the sub-shell in bulk. If you resolve to profit by this circumstance, don’t complain about the consequences of inability to auto-detect errors in the middle of recipes.&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-canned-recipes-custom-functions&quot;&gt;Canned recipes, custom functions&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;http://gromnitsky.users.sourceforge.net/articles/notes-for-new-make-users/cans.svg&quot;/&gt;&lt;/p&gt;
&lt;p&gt;You can exploit macros as user-defined functions. Because a macro gets expanded every time it’s being accesed, it’s save to include autovars in it or refs to other macros. Perhaps, the 2 most common examples are:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;copy = cp $&amp;lt; $@
mkdir = @mkdir -p $(dir $@)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;as in&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;_build/%.html: src/%.html
        $(mkdir)
        $(copy)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Such macros are called &lt;em&gt;canned recipes&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;If you need to pass a parameter to a macro, use a special &lt;code&gt;call()&lt;/code&gt; fn to invoke it:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;find = $(shell find . -name \*.$1 -type f)
src := $(call find,js)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Because &lt;code&gt;find&lt;/code&gt; contains refs to params (&lt;code&gt;$1&lt;/code&gt;), it’s called a parametrised function.&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-deps&quot;&gt;Deps&lt;/h2&gt;
&lt;p&gt;Writing makefiles for small programs often means a manual dependency management. If some &lt;code&gt;out/bundle.js&lt;/code&gt; depends on several .js modules, it’s not hard to specify its prereqs manualy, but for large programs it becomes unmanageable.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://gromnitsky.blogspot.com/2016/07/generating-dependencies-automatically.html&quot;&gt;There are several ways&lt;/a&gt; to tackle the dependency problem, the most popular one is colloquially called Tromey’s Way.&lt;/p&gt;
&lt;p&gt;I’ve noticed that various (programming) books authors love to give examples that have little or no bearing w/ the subject, via explaining how they’ve solved some minuscule problem in the course of preparing their manuscript. This is just a ~26KB markdown file, but I feel that such a great tradition of ‘here’s an example from my book toolchain!’ should be continued.&lt;/p&gt;
&lt;p&gt;Before handing down the .md file to pandoc, I preprecess it w/ erb. This allows me to write&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;&amp;lt;%= File.read 'dilbert/Makefile' %&amp;gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;instead of a copy-pasting. But when I edit such referenced makefile I want the .md file to be auto-recompiled. Manually specifying all included files as prereqs is lame, so I have a lilliputian script that reads the .md file &amp;amp; prints:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;_out/web/index.html: dilbert/Makefile
_out/web/index.html: umbrella/ver1.mk
...&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;I inject those lines into a makefile (via Make’s &lt;code&gt;include&lt;/code&gt; directive). A simplified version of the whole scheme looks like:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;define make-depend
@mkdir -p $(cache)
./deps $@ &amp;lt; $&amp;lt; &amp;gt; $(cache)/$*.d
endef

$(out)/%.html: %.md
        $(mkdir)
        erb $&amp;lt; | pandoc -o $@
        $(make-depend)

-include $(cache)/index.d&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;define&lt;/code&gt; … &lt;code&gt;endef&lt;/code&gt; is a multiline macro. ‘deps’ is the script in question. The main trick here is to invoke the macro after the successful .md to .html transformation. If I edit the .md–it doesn’t matter if there are new &lt;code&gt;File.read&lt;/code&gt; commands in it, for Make rebuilds the .md file regardless, but if I change any of &lt;code&gt;File.read&lt;/code&gt;’ed files &amp;amp; then run Make, it picks up the proper prereq list &amp;amp; sees that it ought to recompile the .md.&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-watchers&quot;&gt;Watchers&lt;/h2&gt;
&lt;p&gt;To auto-recompile on changes w/o manualy invoking Make means using an external file “watcher”. I have &lt;a href=&quot;https://github.com/gromnitsky/watchthis&quot;&gt;my own little watcher&lt;/a&gt; that plays different sounds depending on the return value of Make.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;# npm i -g watchthis&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Then in the project dir:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ watchthis -e _out make test&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;See, how easy the life is, when you compile everything to 1 directory (&lt;code&gt;_out&lt;/code&gt;, in this example).&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-auto-restarting&quot;&gt;Auto-restarting&lt;/h2&gt;
&lt;p&gt;Make can auto-restart itself during a DAG construction if&lt;/p&gt;
&lt;ol type=&quot;a&quot;&gt;&lt;li&gt;it reads another makefile (via &lt;code&gt;include&lt;/code&gt; directive), &amp;amp;&lt;/li&gt;
&lt;li&gt;it has found a rule where the target == included makefile name.&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code&gt;include _out/.npm
_out/.npm: package.json
        npm i
        touch $@&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;How is this useful? If you have targets w/ prereqs from &lt;code&gt;node_modules&lt;/code&gt; dir (e.g., a css library or already minified bundles) those files must exist before Make runs, otherwise Make throws an error. The auto-restarting facility allows Make to set forth with a clean slate and read the makefile anew, this time having all vertices in the DAG you’ve specified.&lt;/p&gt;
&lt;p&gt;Files similar to &lt;code&gt;_out/.npm&lt;/code&gt; are called &lt;em&gt;empty targets&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-debugging&quot;&gt;Debugging&lt;/h2&gt;
&lt;p&gt;Many saints have suffered martyrdom. Many users of Make have learned to tolerate the absence of debugging facilities. There’s &lt;code&gt;--trace&lt;/code&gt; CLO but reading its output is tedious.&lt;/p&gt;
&lt;p&gt;You’d expect Make to have an option ‘print how you see this makefile after the phase I’ (or in another words, ‘show me how you have constructed the DAG’), but no such feature exist in a polished state. You resort to &lt;code&gt;-pk&lt;/code&gt; &amp;amp; grep:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ make -rR -pk -q | grep -v ^#&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;(&lt;code&gt;-rR&lt;/code&gt; disables a load of Make’s built-in implicit metarules/vars for C/C++). This also prints every env variable (for Make converts every env var into a macro (yes!), which is an interesting choice security-wise). We can write a little sh wrapper that reads all the env var names &amp;amp; filters them out alongside w/ often tangential build-in vars:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ cat make-phase1
#!/bin/sh

unset TERMCAP
re_env=$(env | cut -d= -f1 | tr \\n '|')
re_dot_smthg='\.[^ ]+'
re_autovars='[%*+&amp;lt;?^@][^ ]'
re_misc='GNUMAKEFLAGS|MAKE([^ ]+)?|MFLAGS|SUFFIXES|-\*-command-variables-\*-'

make -rR -pk -q &quot;$@&quot; \
    | egrep -v &quot;^(${re_env}#|$re_dot_smthg|$re_autovars|$re_misc) &quot; \
    | cat -s&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Then type ‘make-phase1’ &amp;amp; pass any params to it as you would’ve done w/ ‘make’.&lt;/p&gt;
&lt;p&gt;Even if you’re not &lt;a href=&quot;https://tenderlovemaking.com/2016/02/05/i-am-a-puts-debuggerer.html&quot;&gt;a puts debugger&lt;/a&gt;, Make forces you to become one.&lt;/p&gt;
&lt;p&gt;Everybody knows the trick how to print the value of the macro/variable, right? Create &lt;code&gt;debug.mk&lt;/code&gt; file somewhere not far away, e.g., in &lt;code&gt;~/lib/&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;p-%:
        @echo &quot;$(strip $($*))&quot; | tr ' ' \\n&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Then type:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ make -f ~/lib/debug.mk p-SHELL
/bin/sh&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Adding &lt;code&gt;-f makefile&lt;/code&gt; however many times you want &amp;amp; passing &lt;code&gt;p-macroName&lt;/code&gt;, as a target, prints the expanded value of the macro.&lt;/p&gt;
&lt;p&gt;There’s also a build-in &lt;code&gt;warning()&lt;/code&gt; fn that expands its param &amp;amp; prints it to the stderr. It could be inserted anywhere, for its actual expanded value is an empty string.&lt;/p&gt;
&lt;p&gt;Metarules could be frustrating if you’re unsure how the matching is done against provided targets. In the absence of a REPL of any kind, fall back to a helper .mk again:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ cat ~/lib/metarule.mk
match: $(T)

$(P):
        @echo '$$@ == $@'
        @echo '$$* == $*'&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;To work properly it shoult be run in some tmp dir:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ (cd /tmp; make -f ~/lib/metarule.mk P=out/%.js T=out/foo/bar.js)
$@ == out/foo/bar.js
$* == foo/bar&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;$*&lt;/code&gt; is a stem autovar that matches the &lt;code&gt;%&lt;/code&gt; portion of the metarule. &lt;code&gt;P&lt;/code&gt; is for ‘pattern’, &lt;code&gt;T&lt;/code&gt; is for ‘target’. You know that the target doesn’t match when Make fails:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ (cd /tmp; make -f ~/lib/metarule.mk P=out/%.js T=bar.js)
make: *** No rule to make target 'bar.js', needed by 'match'.  Stop.&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Make has a ‘dry run’ CLO (&lt;code&gt;-n&lt;/code&gt;) that just prints recipes instead of executing them. Combined w/ &lt;code&gt;-t&lt;/code&gt; option, you get a terse view of what targets Make is going to rebuild. E.g., dry run the final makefile from &lt;a href=&quot;http://gromnitsky.users.sourceforge.net/articles/notes-for-new-make-users/#4b6d995-compile-everything-to-1-directory&quot;&gt;Compile everything to 1 directory&lt;/a&gt; section:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ make -tn -f ver2.mk
touch _out/.cache/src/a.js
touch _out/.cache/src/b.js
touch _out/.cache/src/main.js
touch _out/development/main.js
touch _out/development/style.css
touch _out/development/index.html&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Beware that even w/ &lt;code&gt;-n&lt;/code&gt;, Make expands all vars &amp;amp; required macros, so if any of them contain the &lt;code&gt;shell()&lt;/code&gt; fn, Make executes it irrespective of the &lt;code&gt;-n&lt;/code&gt; presence.&lt;/p&gt;
&lt;h2 id=&quot;4b6d995-additional-reading&quot;&gt;Additional reading&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;http://gromnitsky.users.sourceforge.net/articles/notes-for-new-make-users/cromwellian-pikeman.svg&quot; class=&quot;cromwellian-pikeman&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Apart from the official manual, a &lt;a href=&quot;http://make.mad-scientist.net/papers/&quot;&gt;series of essays&lt;/a&gt; from the curren GNU Make maintainer (Paul Smith) are super informative. His profile at &lt;a href=&quot;https://stackoverflow.com/users/939557/madscientist&quot;&gt;stackoverflow&lt;/a&gt;, strangely enough, it’s mostly about Make.&lt;/p&gt;
&lt;p&gt;Eric Melski’s blog posts:&lt;/p&gt;
&lt;p&gt;John Graham-Cumming’s &lt;a href=&quot;https://www.amazon.com/GNU-Make-Book-John-Graham-Cumming/dp/1593276494&quot;&gt;The GNU Make Book&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The original Stuart Feldman’s paper (pdf, Bell Labs, 1978): &lt;a href=&quot;http://ewald.cas.usf.edu/teaching/2004F/5156/packet/packet2.docs/05.make.pdf&quot;&gt;Make–A Program for Maintaining Computer Programs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://archive.org/details/pdfy-MgN0H1joIoDVoIC7&quot;&gt;The AWK programming language&lt;/a&gt; book has a Make-like program that fits on &lt;em&gt;one&lt;/em&gt; page! (Ch.7, p.178.) Whilst it supports only static rules (no metarules, macros or functions), it’s easy to imagine how one could combine it w/ a decent preprocessor.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.conifersystems.com/whitepapers/gnu-make/&quot;&gt;What’s Wrong With GNU make?&lt;/a&gt; “paper” lists divers valid points, although in a somewhat disgruntled style.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://ftp.qucis.queensu.ca/home/cordy/Papers/MCAA_ICPC15_Makefiles.pdf&quot;&gt;Make It Simple – An Empirical Analysis of GNU Make Feature Use in Open Source Projects&lt;/a&gt; (2015) – a study that looks at the diff between handwritten vs. generated makefiles.&lt;/p&gt;
&lt;p&gt;If you have a compsci degree, you are accustomed to graphs, their terminology et al., thus when writing makefiles you should feel at home. My degree was in materials science/nanomaterials, therefore I’ve found the &lt;em&gt;Ch 14 Graph Algorithms&lt;/em&gt; of &lt;a href=&quot;https://www.amazon.com/Data-Structures-Algorithms-Java-6th-ebook/dp/B00JDRQF8C&quot;&gt;Data Structures and Algorithms in Java&lt;/a&gt; very helpful.&lt;/p&gt;
&lt;/body&gt;</description>
<pubDate>Mon, 05 Mar 2018 18:17:33 +0000</pubDate>
<dc:creator>henry_flower</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>http://gromnitsky.users.sourceforge.net/articles/notes-for-new-make-users/</dc:identifier>
</item>
<item>
<title>Is Loneliness a Health Epidemic?</title>
<link>https://www.nytimes.com/2018/02/09/opinion/sunday/loneliness-health.html</link>
<guid isPermaLink="true" >https://www.nytimes.com/2018/02/09/opinion/sunday/loneliness-health.html</guid>
<description>&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;591&quot; data-total-count=&quot;2318&quot; id=&quot;story-continues-3&quot;&gt;The other possible cause is the rise of communications technology, including smartphones, social media and the internet. A decade ago, companies like Facebook, Apple and Google pledged that their products would help create meaningful relationships and communities. Instead, we’ve used the media system to deepen existing divisions, at both the individual and group levels. We may have thousands of “friends” and “followers” on Facebook and Instagram, but when it comes to human relationships, it turns out there’s no substitute for building them the old-fashioned way, in person.&lt;/p&gt;


&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;234&quot; data-total-count=&quot;2552&quot;&gt;In light of these two trends, it’s easy to believe we’re experiencing an “epidemic” of loneliness and isolation. Surprisingly, though, the best data do not actually show drastic spikes in either loneliness or social isolation.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;582&quot; data-total-count=&quot;3134&quot;&gt;The main evidence for rising isolation comes from a widely reported sociology journal article claiming that in 2004, one in four Americans &lt;a href=&quot;http://journals.sagepub.com/doi/abs/10.1177/000312240607100301&quot;&gt;had no one&lt;/a&gt; in their life they felt they could confide in, compared with one in 10 during the 1980s. But that study turned out to be &lt;a href=&quot;http://journals.sagepub.com/doi/abs/10.1177/000312240907400408&quot;&gt;based on faulty data&lt;/a&gt;, and other research shows that the portion of Americans without a confidant is about the same as it has long been. Although one of the authors has &lt;a href=&quot;https://www.chronicle.com/article/The-Case-for-American/130480&quot;&gt;distanced himself from the paper&lt;/a&gt; (saying, “I no longer think it’s reliable”), scholars, journalists and policymakers continue to cite it.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;567&quot; data-total-count=&quot;3701&quot;&gt;The other data on loneliness are complicated and often contradictory, in part because there are so many different ways of measuring the phenomenon. But it’s clear that the loneliness statistics cited by those who say we have an epidemic are outliers. For example, one set of statistics comes from a study that counted as lonely people who said they felt “left out” or “isolated,” or “lacked companionship” — even just “some of the time.” That’s an exceedingly low bar, and surely not one we’d want doctors or policymakers to use in their work.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;319&quot; data-total-count=&quot;4020&quot;&gt;One reason we need to be careful about how we measure and respond to loneliness is that, as the University of Chicago psychologist John Cacioppo argues, an occasional and transitory feeling of loneliness can be healthy and productive. It’s a biological signal to ourselves that we need to build stronger social bonds.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;695&quot; data-total-count=&quot;4715&quot;&gt;Professor Cacioppo has spent much of his career documenting the dangers of loneliness. But it’s notable that he relies on more measured statistics in his own scientific papers than the statistics described above. One of his articles, from last year, reports that around 19 percent of older Americans said they had felt lonely for much of the week before they were surveyed, and that in Britain about 6 percent of adults said they felt lonely all or most of the time. Those are worrisome numbers, but they are quite similar to the numbers reported in Britain in 1948, when about 8 percent of older adults said they often or always felt lonely, and to those in previous American studies as well.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;210&quot; data-total-count=&quot;4925&quot;&gt;Professor Cacioppo is one of the leading voices advocating for better treatment of loneliness. But, as &lt;a href=&quot;https://www.psychologytoday.com/blog/connections/200905/epidemic-loneliness&quot;&gt;he has written&lt;/a&gt;, “to call it an epidemic of loneliness risks having it relegated to the advice columns.”&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;604&quot; data-total-count=&quot;5529&quot;&gt;In particular, overstating the problem can make it harder to make sure we are focusing on the people who need help the most. When Britain announced its new ministry, officials insisted that everyone, young or old, was at risk of loneliness. Yet the research tells us something more specific. In places like the United States and Britain, it’s the poor, unemployed, displaced and migrant populations that stand to suffer most from loneliness and isolation. Their lives are unstable, and so are their relationships. When they get lonely, they are the least able to get adequate social or medical support.&lt;/p&gt;
&lt;div id=&quot;story-ad-2&quot; class=&quot;story-ad ad ad-placeholder nocontent robots-nocontent&quot;&gt;

&lt;a class=&quot;visually-hidden skip-to-text-link&quot; href=&quot;https://www.nytimes.com/2018/02/09/opinion/sunday/loneliness-health.html#story-continues-4&quot;&gt;Continue reading the main story&lt;/a&gt;&lt;/div&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;230&quot; data-total-count=&quot;5759&quot; id=&quot;story-continues-4&quot;&gt;I don’t believe we have a loneliness epidemic. But millions of people are suffering from social disconnection. Whether or not they have a minister for loneliness, they deserve more attention and help than we’re offering today.&lt;/p&gt;
&lt;a class=&quot;visually-hidden skip-to-text-link&quot; href=&quot;https://www.nytimes.com/2018/02/09/opinion/sunday/loneliness-health.html#whats-next&quot;&gt;Continue reading the main story&lt;/a&gt;</description>
<pubDate>Mon, 05 Mar 2018 15:27:09 +0000</pubDate>
<dc:creator>dpflan</dc:creator>
<og:url>https://www.nytimes.com/2018/02/09/opinion/sunday/loneliness-health.html</og:url>
<og:type>article</og:type>
<og:title>Opinion | Is Loneliness a Health Epidemic?</og:title>
<og:description>Social disconnection is a serious matter. But let’s not whip up a panic.</og:description>
<og:image>https://static01.nyt.com/images/2018/02/11/opinion/sunday/11gray/11gray-facebookJumbo.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nytimes.com/2018/02/09/opinion/sunday/loneliness-health.html</dc:identifier>
</item>
<item>
<title>It’s time to ditch SMS-based 2 Factor Auth</title>
<link>https://dennisdel.com/blog/ditch-sms-2fa/</link>
<guid isPermaLink="true" >https://dennisdel.com/blog/ditch-sms-2fa/</guid>
<description>&lt;p&gt;It’s 2018, and it’s time we understand that &lt;strong&gt;SMS 2-factor authentication is not a good way to double-check the users’ credentials&lt;/strong&gt;. It’s been shown many times that phone numbers can be compromised.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://dennisdel.com/images/postmedia/ditch-sms-2fa/heading.jpg&quot; alt=&quot;Phone laying screen down on the pavement&quot;/&gt; (&lt;em&gt;Image sourced from &lt;a href=&quot;https://pixabay.com/en/iphone-6-apple-ios-iphone-ios-8-458150/&quot;&gt;Pixabay&lt;/a&gt;.&lt;/em&gt;)&lt;/p&gt;
&lt;h2 id=&quot;the-problem&quot;&gt;The problem&lt;/h2&gt;
&lt;p&gt;Here are just some examples of how cellphone number and customer data can be at risk:&lt;/p&gt;
&lt;p&gt;I can go on. The problem is so bad that the US Federal Trade Commission issued &lt;a href=&quot;https://www.ftc.gov/news-events/blogs/techftc/2016/06/your-mobile-phone-account-could-be-hijacked-identity-thief&quot;&gt;its own blog post&lt;/a&gt; documenting how these schemes work, referencing the &lt;a href=&quot;https://www.ftc.gov/tips-advice/business-center/guidance/fighting-identity-theft-red-flags-rule-how-guide-business&quot;&gt;Red Flags Rule guidance&lt;/a&gt;. And yet, we are proven over and over again that social engineering works against the consumer interests.&lt;/p&gt;
&lt;p&gt;Phone numbers are not reliable. Phone numbers can be hijacked. Phone numbers change. Phone numbers stop working when you are traveling and not roaming. Phone numbers don’t work when you are using in-flight Wi-Fi. Phone numbers can stop receiving text messages/calls for a million of other reasons, effectively either (&lt;strong&gt;1&lt;/strong&gt;) locking users out of an account or (&lt;strong&gt;2&lt;/strong&gt;) compromising the account altogether. And, as last week has shown, something that was registered as a 2FA number &lt;a href=&quot;https://www.theverge.com/2018/2/14/17014116/facebook-2fa-two-factor-authentication-auto-post-replies-status-updates-bug&quot;&gt;can potentially be used for completely non-security reasons&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;And yet, there are are still services that offer 2FA &lt;strong&gt;only&lt;/strong&gt; through SMS.&lt;/p&gt;
&lt;h2 id=&quot;what-are-the-alternatives&quot;&gt;What are the alternatives?&lt;/h2&gt;
&lt;p&gt;There are several, and depending on the service, some might be more convenient than others. &lt;a href=&quot;https://outlook.com&quot;&gt;Outlook&lt;/a&gt; uses &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/multi-factor-authentication/end-user/microsoft-authenticator-app-how-to&quot;&gt;Microsoft Authenticator&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://dennisdel.com/images/postmedia/ditch-sms-2fa/authenticator.png&quot; alt=&quot;Microsoft Authenticator asking for confirmation&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The way it works is it skips passwords altogether - once you connect the app to the account, you can remotely confirm the login by approving or denying the notification, and using TouchID (super-convenient, if you are on your iPhone) validating the request. This method stil assumes that your device has an active Internet connection&lt;/p&gt;
&lt;p&gt;There are alternatives where you can confirm your identity even if the device where you are authenticating has an Internet connection, and the “second factor” does not - apps like &lt;a href=&quot;https://authy.com/&quot;&gt;Authy&lt;/a&gt;, &lt;a href=&quot;https://www.microsoft.com/en-us/store/p/microsoft-authenticator/9nblgggzmcj6&quot;&gt;Microsoft Authenticator&lt;/a&gt; (yes, it supports standard tokens that are &lt;a href=&quot;https://en.wikipedia.org/wiki/Time-based_One-time_Password_Algorithm&quot;&gt;refreshed every 30 seconds&lt;/a&gt;) or &lt;a href=&quot;https://www.google.com/landing/2step/&quot;&gt;Google Authenticator&lt;/a&gt; solve the problem singlehandedly.&lt;/p&gt;
&lt;p&gt;Or use &lt;a href=&quot;https://en.wikipedia.org/wiki/Universal_2nd_Factor&quot;&gt;U2F&lt;/a&gt;-compatible devices like &lt;a href=&quot;https://en.wikipedia.org/wiki/YubiKey&quot;&gt;the YubiKey&lt;/a&gt; which is probably the &lt;a href=&quot;https://www.yubico.com/2016/02/use-of-fido-u2f-security-keys-focus-of-2-year-google-study/&quot;&gt;best way to go about secure authentication&lt;/a&gt; since it relies on a physical token be connected to the authenticating device.&lt;/p&gt;
&lt;p&gt;These methods survive phone number changes, and the only way to compromise them is to be in physical posession of an unlocked device. Not to say that operational security is not important, because people will &lt;a href=&quot;http://thedailywtf.com/articles/Security_by_Oblivity&quot;&gt;find a way&lt;/a&gt; &lt;a href=&quot;https://stackoverflow.com/questions/1983879/ocr-an-rsa-key-fob-security-token&quot;&gt;to compromise even the token-based approach&lt;/a&gt;, but it still is magnitudes safer than relying on SMS messages or phone calls.&lt;/p&gt;
&lt;p&gt;It’s by no means a trivial change on the provider side to properly implement token-based 2FA (either physical or numeric), but ultimately it will help users much more in the long run.&lt;/p&gt;
&lt;p&gt;It’s time SMS 2FA goes the way of the floppy disk.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://dennisdel.com/blog/&quot;&gt;Back to posts&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 05 Mar 2018 15:07:51 +0000</pubDate>
<dc:creator>dend</dc:creator>
<og:title>It's Time to Ditch SMS 2-Factor Verification</og:title>
<og:description>It’s 2018, and it’s time we understand that SMS 2-factor authentication is not a good way to double-check the users’ credentials. It’s been shown many times that phone numbers can be compromised. (Image sourced from Pixabay.) The problem Here are just some examples of how cellphone number and customer data can be at risk: Security Issue Date T-Mobile exposes endpoint with account details October 2017 Cellphone accounts hijacked to go after cryptocurrency owners August 2017 Hackers able to hijack cellphone accounts July 2017 Thieves able to port cellphone numbers March 2016 I can go on.</og:description>
<og:type>article</og:type>
<og:url>https://dennisdel.com/blog/ditch-sms-2fa/</og:url>
<og:image>https://dennisdel.com/images/postmedia/ditch-sms-2fa/heading.jpg</og:image>
<dc:language>en-us</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://dennisdel.com/blog/ditch-sms-2fa/</dc:identifier>
</item>
</channel>
</rss>