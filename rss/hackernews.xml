<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Hire people who aren’t proven</title>
<link>https://leonardofed.io/blog/startups-hiring.html</link>
<guid isPermaLink="true" >https://leonardofed.io/blog/startups-hiring.html</guid>
<description>&lt;p&gt;Hiring is broken on so many different levels, and it starts right there, at the job offer description. It then continues all the way down to the actual interview process.&lt;/p&gt;
&lt;p&gt;I'd like to unfold here some prescriptions about how I think startups should hire.&lt;/p&gt;
&lt;p&gt;Most jobs positions out there regarding the job role go like this &quot;10 years experience in &lt;code&gt;job role&lt;/code&gt;, proven experience in &lt;code&gt;whatever the responsibilities are&lt;/code&gt;, high proficiency in &lt;code&gt;infinite technology set&lt;/code&gt;, already built successful &lt;code&gt;whatever&lt;/code&gt;, ability to work well in a team, exceptional references from previous companies&quot;&lt;/p&gt;
&lt;p&gt;When I read such things, here's what I read instead: &quot;We're looking for a candidate who is: at least a &lt;code&gt;one time World Cup Champion&lt;/code&gt;, &lt;code&gt;two times FIFA World Cup awards winner&lt;/code&gt;, &lt;code&gt;best scorer&lt;/code&gt; of the season, proven ability to work well in a team, exceptional references from previous teams&quot;.&lt;/p&gt;
&lt;p&gt;How many people are really a good fit for this position? Two, maybe three on Earth. The reality is that you're not going to get them.&lt;/p&gt;
&lt;p&gt;If your company's job position looks like you're looking for unicorn you're doing it wrong and you'll never get what you're after.&lt;/p&gt;
&lt;p&gt;If there's anyone else in the world that can come up with your same conclusions with the same degree of confidence, it means that there are enough data points to objectively say this is a great player. If that's the case, you and your startup won't be able to hire the player. Someone will steal him from you.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;You have to go after people that aren't proven and you need to be really good at evaluating them with much less data points.&lt;/strong&gt; In short, you need to be extremely good at forecasting.&lt;/p&gt;
&lt;p&gt;Don't hire like FAANG companies, don't use their best practices, don't use their super oiled processes, don't play their same games with the same rules.&lt;/p&gt;
&lt;p&gt;Google, Amazon, Netflix, Apple have thousands of candidates and might need an object baseline to judge them. You don't.&lt;/p&gt;
&lt;p&gt;Google's interview best practices strictly focused on &lt;a href=&quot;https://www.quora.com/Why-do-interviewers-care-so-much-about-algorithm-and-data-structures/answer/Leonardo-Federico-1&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;algorithms and data structure&lt;/a&gt; questions won't help you in your interview process. Amazon's bar-raiser won't help you either.&lt;/p&gt;
&lt;p&gt;If you play their games when hiring people, you're going to lose every single battle.&lt;/p&gt;
&lt;p&gt;Instead of relying on easy observable data points and measurable metrics (coding challenges, rankings, pedigrees and riddles) look for answers in non-measurable realms, in domains where there are fewer, if not none, data points, looks for areas that are not easy measurable, where there's no yet predefined scripts or manuals, and where new simple heuristics can win overpowered standardized common wisdom.&lt;/p&gt;
&lt;p&gt;Here's what instead you should do in your hiring process, try to find an answer to these four questions:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Can this candidate do the job?&lt;/li&gt;
&lt;li&gt;Will this candidate be motivated?&lt;/li&gt;
&lt;li&gt;Will this candidate get along with coworkers?&lt;/li&gt;
&lt;li&gt;What this candidate will be in three, six, twelve months from now?&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Everything you ask and everything you do during the interview should have the ultimate object of augmenting the details of each one of those four questions.&lt;/p&gt;
&lt;p&gt;Discover how do they deal with complexity? Don't do whiteboard coding on riddles or puzzles. (This is partially the reason why I decided to start &lt;a href=&quot;https://type12.com&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Type12&lt;/a&gt; )&lt;/p&gt;
&lt;p&gt;Learn how do they respond to real-world problems? Don't ask &lt;em&gt;&quot;Why are manhole covers round?&quot;&lt;/em&gt; Who gives a shit to why are manhole covers round. Pair-program with them instead and learn how well they break through blocks.&lt;/p&gt;
&lt;p&gt;Don't ask questions that if you just happen to know the answer to, you're golden, and if you get stuck in a situation where you have to work something out on the fly, you can easily get stuck in a mental wedgie that makes you look like a complete moron.&lt;/p&gt;
&lt;p&gt;Cut luck out of your system.&lt;/p&gt;
&lt;p&gt;Ultimately, look for very-high-dimensional vectors, such as smartness, attitudes, motivation, dynamic learning, courage, that can't be easily tested or represented by a basis vector or on a scale.&lt;/p&gt;
&lt;p&gt;While on the surface these may sound just contrarian, what most of these do is to optimize for something long term/less measurable where the incumbents are constrained by time and what can be measured.&lt;/p&gt;
&lt;p&gt;That's where you discover talents, that's when you hire great people.&lt;/p&gt;
</description>
<pubDate>Thu, 27 Sep 2018 13:45:12 +0000</pubDate>
<dc:creator>type12</dc:creator>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://leonardofed.io/blog/startups-hiring.html</dc:identifier>
</item>
<item>
<title>Linus Torvalds: &amp;#039;I&amp;#039;ll never be cuddly but I can be more polite&amp;#039;</title>
<link>https://www.bbc.co.uk/news/technology-45664640</link>
<guid isPermaLink="true" >https://www.bbc.co.uk/news/technology-45664640</guid>
<description>&lt;figure class=&quot;media-landscape has-caption full-width lead&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                &lt;img class=&quot;js-image-replace&quot; alt=&quot;Linus Torvalds&quot; src=&quot;https://ichef.bbci.co.uk/news/320/cpsprodpb/C670/production/_103600805_linustorvalds.gif&quot; width=&quot;976&quot; height=&quot;549&quot;/&gt;&lt;span class=&quot;off-screen&quot;&gt;Image copyright&lt;/span&gt;
                 &lt;span class=&quot;story-image-copyright&quot;&gt;Getty Images&lt;/span&gt;
                
            &lt;/span&gt;
            
            &lt;figcaption class=&quot;media-caption&quot;&gt;&lt;span class=&quot;off-screen&quot;&gt;Image caption&lt;/span&gt;
                &lt;span class=&quot;media-caption__text&quot;&gt;
                    Mr Torvalds has a reputation for sending rude emails to fellow developers
                &lt;/span&gt;
            &lt;/figcaption&gt;&lt;/figure&gt;&lt;p class=&quot;story-body__introduction&quot;&gt;Linux founder Linus Torvalds has told the BBC that he is seeking professional help to become more empathetic towards fellow developers, but admits he may have to &quot;fake it until I make it&quot;.&lt;/p&gt;&lt;p&gt;Mr Torvalds stepped back from his role heading the organisation, following accusations of bullying and rudeness.&lt;/p&gt;&lt;p&gt;He admitted to bad behaviour but added that the Linux community also has to look at the way it conducts itself.&lt;/p&gt;&lt;p&gt;He told the BBC it had become &quot;a morass of nastiness&quot;.&lt;/p&gt;&lt;p&gt;Mr Torvalds developed the first version of the Linux operating system while studying at the University of Helsinki, Finland in 1991.&lt;/p&gt;&lt;p&gt;He has always had a reputation as someone who provides blunt feedback to engineers, with expletive-laden emails, once describing an Intel fix as &quot;complete and utter garbage&quot;.&lt;/p&gt;&lt;p&gt;In a TED (Technology, Entertainment, Design) Talk in 2016, &lt;a href=&quot;https://www.bbc.co.uk/news/technology-35599774&quot; class=&quot;story-body__link&quot;&gt;he spoke openly about how even as a child he was not a &quot;people person&quot;.&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The Linux kernel - the code that lets software and hardware work together - has since been through many revisions and now powers many of the world's web servers, including those of Google, PayPal, Amazon and eBay. It is also behind the two billion mobile phones using Android.&lt;/p&gt;&lt;p&gt;Mr Torvalds oversees every line of code added to the kernel, but in recent years the male-dominated community has become increasingly divided.&lt;/p&gt;&lt;p&gt;Rows about sexism and rudeness led to the creation of a Code of Conflict (CoC) in 2015 which was short - simply recommending people &quot;be excellent to each other&quot;.&lt;/p&gt;&lt;p&gt;That has now been replaced by a more detailed Code of Conduct - which retains the acronym, but attempts to be more inclusive and eliminate insulting and derogatory comments and behaviour.&lt;/p&gt;&lt;figure class=&quot;media-landscape has-caption full-width&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                
                
                
                
                 &lt;span class=&quot;off-screen&quot;&gt;Image copyright&lt;/span&gt;
                 &lt;span class=&quot;story-image-copyright&quot;&gt;Getty Images&lt;/span&gt;
                
            &lt;/span&gt;
            
            &lt;figcaption class=&quot;media-caption&quot;&gt;&lt;span class=&quot;off-screen&quot;&gt;Image caption&lt;/span&gt;
                &lt;span class=&quot;media-caption__text&quot;&gt;
                    Mr Torvalds has made no secret of the fact that he prefers technology to people
                &lt;/span&gt;
            &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;In an exclusive email to the BBC, Mr Torvalds shared his thoughts on his decision to temporarily step aside, the controversy behind the CoC, and the defects of the community he set up. &lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&quot;So I've obviously long been on record as wanting to deal with the technical side, and not really wanting to get involved in most other discussions.&lt;/p&gt;&lt;p&gt;&quot;Because technology is what I have always found interesting. People? Not my forte. Never has been, clearly. If you watched that TED talk, you'll know I wasn't a people person even as a child.&lt;/p&gt;&lt;p&gt;&quot;And if you have read any of the recent stories, you will now know at least one other reason why I've wanted to stay away from that whole discussion. Because it's not just my lack of people skills. It's the discussions themselves.&lt;/p&gt;&lt;p&gt;&quot;The advantage of concentrating on technology is that you can have some mostly objective measures, and some basis for agreement, and you can have a very nice and healthy community around it all. I really am motivated by the technology, but the community around Linux has been a big positive too.&lt;/p&gt;&lt;p&gt;&quot;But there are very tangible and immediate common goals in any technical project like Linux, and while there is occasionally disagreement about how to solve some particular issue, there is a very real cohesive force in that common goal of improving the project. &lt;/p&gt;&lt;p&gt;&quot;And even when there are disagreements, people in the end often have fairly clear and objective measures of what is better. Code that is faster, simpler, or handles more cases naturally is just objectively 'better', without people really having to argue too much about it.&lt;/p&gt;&lt;p&gt;&quot;In contrast, the arguments about behaviour never seem to end up having a common goal. Except, in some sense, the argument itself.&lt;/p&gt;&lt;p&gt;&quot;Have you read the Twitter feeds and other things by the people who seem to care more about the non-technical side? I think your 'hyped stories' is about as polite as you can put it. It's a morass of nastiness. Instead of a 'common goal', you end up with horrible fighting between different 'in-groups'.&lt;/p&gt;&lt;p&gt;&quot;It's very polarising, and both sides love egging the other side on. It's not even a 'discussion', it's just people shouting at each other.&lt;/p&gt;&lt;p&gt;&quot;That's actually the reason I for the longest time did not want to be involved with the whole CoC discussion in the first place. That whole subject seems to very easily just devolve and become unproductive. And I found a lot of the people who pushed for a CoC and criticised me for cursing to be hypocritical and pointless. I could easily point you to various tweet storms by people who criticise my 'white cis male' behaviour, while at the same time cursing more than I ever do.&lt;/p&gt;&lt;p&gt;&quot;So that's my excuse for dismissing a lot of the politically correct concerns for years. I felt it wasn't worth it. Anybody who uses the words 'white cis male privilege' was simply not worth my time even talking to, I felt.&lt;/p&gt;&lt;p&gt;&quot;And I'm still not apologising for my gender or the colour of my skin, or the fact that I happen to have the common sexual orientation.&lt;/p&gt;&lt;p&gt;&quot;What changed? Maybe it was me, but I was also made very aware of some of the behaviour of the 'other' side in the discussion.&lt;/p&gt;&lt;p&gt;&quot;Because I may have my reservations about excessive political correctness, but honestly, I absolutely do not want to be seen as being in the same camp as the low-life scum on the internet that think it's OK to be a white nationalist Nazi, and have some truly nasty misogynistic, homophobic or transphobic behaviour. And those people were complaining about too much political correctness too, and in the process just making my public stance look bad.&lt;/p&gt;&lt;p&gt;&quot;And don't get me wrong, please - I'm not making excuses for some of my own rather strong language. But I do claim that it never ever was any of that kind of nastiness. I got upset with bad code, and people who made excuses for it, and used some pretty strong language in the process. Not good behaviour, but not the racist/etc claptrap some people spout.&lt;/p&gt;&lt;p&gt;&quot;So in the end, my 'I really don't want to be too PC' stance simply became untenable. Partly because you definitely can find some emails from me that were simply completely unacceptable, and I need to fix that going forward. But to a large degree also because I don't want to be associated with a lot of the people who complain about excessive political correctness.&lt;/p&gt;&lt;figure class=&quot;media-landscape has-caption full-width&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                
                
                
                
                 &lt;span class=&quot;off-screen&quot;&gt;Image copyright&lt;/span&gt;
                 &lt;span class=&quot;story-image-copyright&quot;&gt;Getty Images&lt;/span&gt;
                
            &lt;/span&gt;
            
            &lt;figcaption class=&quot;media-caption&quot;&gt;&lt;span class=&quot;off-screen&quot;&gt;Image caption&lt;/span&gt;
                &lt;span class=&quot;media-caption__text&quot;&gt;
                    Mr Torvalds doubts he will ever be cuddly but can improve the way he handles people
                &lt;/span&gt;
            &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&quot;Am I turning into some cuddly people person? I'll admit that sounds very unlikely. I still care about the technology, and I'm still not exactly the most empathetic person. But I'm hoping I can at least 'fake it until I make it'. Part of that 'faking it' is definitely going to be a filter on my outgoing emails, but as mentioned, I'm actively also trying to find a professional therapist to talk to as well.&lt;/p&gt;&lt;p&gt;&quot;Will everybody be happy? No. People who don't like my blunt behaviour even when I'm not being actively nasty about it will just see that as 'look, nothing changed'. I'm trying to get rid of my outbursts, and be more polite about things, but technically wrong is still technically wrong, and I won't start accepting bad code just to make people feel better about themselves.&lt;/p&gt;&lt;p&gt;&quot;But if people at least realise that I'm not part of the disgusting underbelly of the internet that thinks it's OK to show the kind of behaviour you will find if you really have been reading up on the 'discussions' about the code of conduct, then even that will be a really good thing.&lt;/p&gt;&lt;p&gt;&quot;And again - the above is just my explanation of why I applied the CoC even if there is obviously discussion about it. We will have the maintainer summit in Edinburgh next month, and we'll talk about this issue a lot more. &lt;/p&gt;&lt;p&gt;&quot;In the meantime, I'm taking a break from the kernel and probably shouldn't talk to journalists.&quot;&lt;/p&gt;
            </description>
<pubDate>Thu, 27 Sep 2018 12:40:08 +0000</pubDate>
<dc:creator>Flenser</dc:creator>
<og:title>Linus Torvalds: I'm trying to be polite</og:title>
<og:type>article</og:type>
<og:description>In an exclusive conversation with the BBC, the Linux founder opens up.</og:description>
<og:url>https://www.bbc.com/news/technology-45664640</og:url>
<og:image>https://ichef.bbci.co.uk/news/1024/branded_news/C670/production/_103600805_linustorvalds.gif</og:image>
<dc:language>en-GB</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bbc.co.uk/news/technology-45664640</dc:identifier>
</item>
<item>
<title>After century of removing appendixes, docs find antibiotics can be enough</title>
<link>https://arstechnica.com/science/2018/09/after-century-of-removing-appendixes-docs-find-antibiotics-can-be-enough/</link>
<guid isPermaLink="true" >https://arstechnica.com/science/2018/09/after-century-of-removing-appendixes-docs-find-antibiotics-can-be-enough/</guid>
<description>&lt;img src=&quot;https://cdn.arstechnica.net/wp-content/uploads/2018/09/GettyImages-1031848012-800x534.jpg&quot; alt=&quot;Article intro image&quot;/&gt;&lt;div class=&quot;caption-text&quot;&gt;&lt;a href=&quot;https://cdn.arstechnica.net/wp-content/uploads/2018/09/GettyImages-1031848012.jpg&quot; class=&quot;enlarge-link&quot; data-height=&quot;3484&quot; data-width=&quot;5220&quot;&gt;Enlarge&lt;/a&gt; &lt;span class=&quot;sep&quot;&gt;/&lt;/span&gt; All this time!&lt;/div&gt;&lt;aside id=&quot;social-left&quot;&gt;&lt;a aria-describedby=&quot;70 posters participating&quot; class=&quot;comment-count icon-comment-bubble-down&quot; href=&quot;https://arstechnica.com/science/2018/09/after-century-of-removing-appendixes-docs-find-antibiotics-can-be-enough/?comments=1&quot;&gt;&lt;span class=&quot;comment-count-before&quot;&gt;reader comments&lt;/span&gt; &lt;span class=&quot;comment-count-number&quot;&gt;103&lt;/span&gt;&lt;/a&gt;
&lt;div class=&quot;share-links&quot;&gt;&lt;span&gt;Share this story&lt;/span&gt;    &lt;/div&gt;
&lt;/aside&gt;&lt;p&gt;After more than a century of slicing tiny, inflamed organs from people’s guts, doctors have found that surgery may not be necessary after all—a simple course of &lt;a href=&quot;https://jamanetwork.com/journals/jama/article-abstract/2703354&quot;&gt;antibiotics can be just as effective at treating appendicitis&lt;/a&gt; as going under the knife.&lt;/p&gt;
&lt;p&gt;The revelation comes from a large, randomized trial out of Finland, published Tuesday, September 25, in &lt;em&gt;JAMA&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Despite upending a long-held standard of care, the study’s finding is not entirely surprising; it follows several other randomized trials over the years that had carved out evidence that antibiotics alone can treat an acute appendicitis. Those studies, however, left some dangling questions, including if the antibiotics just improved the situation temporarily and if initial drug treatments left patients worse off later if they did need surgery.&lt;/p&gt;
&lt;p&gt;The new &lt;em&gt;JAMA&lt;/em&gt; study, with its full, five-year follow-up, effectively cauterised those remaining issues. Nearly two-thirds of the patients randomly assigned in the study to get antibiotics for an uncomplicated appendicitis didn’t end up needing surgery in the follow-up time, the Finnish authors, based at the University of Turku, report. And those drug-treated patients that did end up getting an appendectomy later were not worse off for the delay in surgery.&lt;/p&gt;
&lt;p&gt;“This long-term follow-up supports the feasibility of antibiotic treatment alone as an alternative to surgery for uncomplicated acute appendicitis,” the authors conclude.&lt;/p&gt;
&lt;p&gt;The finding suggests that many appendicitis patients could be spared the risks of surgical procedures, such as infections. They may also be able to save money by not needing such an invasive procedure (although the study didn’t compare costs), and they could reap the benefits of shorter treatment and recovery times. Researchers will have to collect more data to back up those benefits, though.&lt;/p&gt;
&lt;p&gt;For their initial look at the simpler appendicitis treatment, researchers led by Paulina Salminen randomly assigned 530 patients that showed up in the hospital with an acute, uncomplicated appendicitis to get either a standard, open surgery to remove their inflamed organ or a course of antibiotics. (By “uncomplicated,” the authors mean there weren’t other issues like perforation, abscess, or suspicion of a tumor.)&lt;/p&gt;
&lt;p&gt;The patients ranged in age from 18 to 60 and enrolled in the trial between November 2009 and June 2012. Those who went under the knife stayed in the hospital for a median of three days, while the antibiotic-treated patients stayed in the hospital for three days to get intravenous drugs, which were then followed by seven days of oral antibiotics out of the hospital.&lt;/p&gt;
&lt;p&gt;A couple of patients were lost in follow-up, including one from an unrelated death, leaving 272 patients in the surgery group and 256 in the antibiotic group.&lt;/p&gt;
&lt;p&gt;In the antibiotic group, 70 patients ended up having surgery within the first year of the treatment. Within the subsequent five years, 30 others also underwent surgery. That left 156 antibiotic-treated patients, or about 61 percent, who were able to escape the scalpel.&lt;/p&gt;
&lt;h2&gt;Vestigial medicine&lt;/h2&gt;
&lt;p&gt;The authors think that percentage could be even higher in follow-up studies. They note that the decision to undergo surgery after the initial randomization was entirely up to the patients’ treating surgeons—most of whom weren’t involved in the trial and some of whom were skeptical of the idea that antibiotics alone could treat appendicitis. This fact, the authors note, could have artificially inflated the number of people who ended up getting an appendectomy. They point out that seven of the 100 antibiotic-treated patients who underwent surgery didn’t actually have evidence of appendicitis at the time of their surgery, based on their medical records.&lt;/p&gt;
&lt;p&gt;Still, going with antibiotics first meant fewer complications and faster recoveries overall. The antibiotic group had a complication rate of 6.5 percent, whereas those assigned to surgery had a rate of 24 percent, mostly due to infections. Of the 100 antibiotic-treated patients who later had surgery, they had typical complication rates for the procedure. This suggests that delaying the surgery for this group didn’t lead to more problems.&lt;/p&gt;
&lt;p&gt;Complications or not, the antibiotic group overall took a median of 11 days of sick leave to recover, while the surgery group took 22 days.&lt;/p&gt;
&lt;p&gt;There were a couple of catches to the study that warrant follow-up. One big issue is that the study compared antibiotic treatment to standard, open surgery—not a more modern, minimally invasive laparoscopic surgery, which is now common in the US. If this had been the standard of care in the surgery group for this study, it might have shifted the cost-benefit scales, potentially reducing complication rates and recovery times.&lt;/p&gt;
&lt;p&gt;That said, the authors note that the antibiotic treatment was also heavy-handed in the study. The researchers went with a &quot;conservative&quot; three-day IV treatment followed by more oral antibiotics, which may have been overkill. They did this because “[w]hen this protocol was designed, there was little information available to guide the application of antibiotic treatment for appendicitis,” they note. Future studies could find that shorter, less intense courses of antibiotics could also do the trick, further reducing complication rates and treatment time.&lt;/p&gt;
&lt;p&gt;Last, the study didn’t compare costs of the interventions or the bills that would have been incurred by those in the two treatment groups. This will be another question to address in follow-up studies as doctors fine-tune the best way to handle appendicitis after all these years.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;JAMA&lt;/em&gt;, 2018. DOI: &lt;a href=&quot;http://dx.doi.org/10.1001/jama.2018.13201&quot;&gt;10.1001/jama.2018.13201&lt;/a&gt;  (&lt;a href=&quot;http://arstechnica.com/science/news/2010/03/dois-and-their-discontents-1.ars&quot;&gt;About DOIs&lt;/a&gt;).&lt;/p&gt;
</description>
<pubDate>Thu, 27 Sep 2018 07:40:29 +0000</pubDate>
<dc:creator>YeGoblynQueenne</dc:creator>
<og:url>https://arstechnica.com/science/2018/09/after-century-of-removing-appendixes-docs-find-antibiotics-can-be-enough/</og:url>
<og:title>After century of removing appendixes, docs find antibiotics can be enough</og:title>
<og:image>https://cdn.arstechnica.net/wp-content/uploads/2018/09/GettyImages-1031848012-760x380.jpg</og:image>
<og:description>In a five-year follow-up, nearly two-thirds of patients never needed surgery.</og:description>
<og:type>article</og:type>
<dc:language>en-us</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://arstechnica.com/science/2018/09/after-century-of-removing-appendixes-docs-find-antibiotics-can-be-enough/</dc:identifier>
</item>
<item>
<title>The future of Java and OpenJDK updates without Oracle support</title>
<link>https://developers.redhat.com/blog/2018/09/24/the-future-of-java-and-openjdk-updates-without-oracle-support/</link>
<guid isPermaLink="true" >https://developers.redhat.com/blog/2018/09/24/the-future-of-java-and-openjdk-updates-without-oracle-support/</guid>
<description>&lt;div class=&quot;entry-meta&quot;&gt;
&lt;div class=&quot;row entry-meta-row&quot;&gt;
&lt;div class=&quot; columns medium-16 small-24 author-post-info&quot;&gt;
&lt;div class=&quot;posted-on&quot;&gt;
&lt;div class=&quot;avatar-outer&quot;&gt;&lt;img class=&quot;avatar&quot; src=&quot;https://developers.redhat.com/blog/wp-content/uploads/2018/09/Paris-silhouette-Andrew-July-2011.tif&quot; alt=&quot;aphredhat&quot;/&gt;&lt;/div&gt;
By &lt;a title=&quot;Andrew Haley&quot; href=&quot;https://developers.redhat.com/blog/author/aphredhat/&quot;&gt;Andrew Haley&lt;/a&gt; &lt;time class=&quot;entry-date published&quot; datetime=&quot;2018-09-24T12:15:23+00:00&quot;&gt;September 24, 2018&lt;/time&gt;&lt;time class=&quot;updated&quot; datetime=&quot;2018-09-25T11:15:12+00:00&quot;&gt;September 25, 2018&lt;/time&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;columns medium-8 small-24 ratings-column&quot;&gt;
&lt;div class=&quot;post-ratings-container&quot;&gt;
&lt;div id=&quot;post-ratings-520407&quot; class=&quot;post-ratings&quot; data-nonce=&quot;14514865ba&quot;&gt;&lt;span class=&quot;vote-box&quot;&gt;&lt;img id=&quot;rating_520407_1&quot; src=&quot;https://developers.redhat.com/blog/wp-content/plugins/wp-postratings/images/thumbs/rating_1_on.gif&quot; alt=&quot;-1&quot; title=&quot;-1&quot; onmouseover=&quot;current_rating(520407, 1, '-1');&quot; onmouseout=&quot;ratings_off(1, 0, 0);&quot; onclick=&quot;rate_post();&quot; onkeypress=&quot;rate_post();&quot;/&gt;&lt;img id=&quot;rating_520407_2&quot; src=&quot;https://developers.redhat.com/blog/wp-content/plugins/wp-postratings/images/thumbs/rating_2_off.gif&quot; alt=&quot;+1&quot; title=&quot;+1&quot; onmouseover=&quot;current_rating(520407, 2, '+1');&quot; onmouseout=&quot;ratings_off(1, 0, 0);&quot; onclick=&quot;rate_post();&quot; onkeypress=&quot;rate_post();&quot;/&gt;&lt;/span&gt;  &lt;strong&gt;+12&lt;/strong&gt; rating, &lt;strong&gt;12&lt;/strong&gt; votes&lt;/div&gt;
&lt;div id=&quot;post-ratings-520407-loading&quot; class=&quot;post-ratings-loading&quot;&gt;&lt;img src=&quot;https://developers.redhat.com/blog/wp-content/plugins/wp-postratings/images/loading.gif&quot; width=&quot;16&quot; height=&quot;16&quot; class=&quot;post-ratings-image&quot;/&gt;Loading...&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;img width=&quot;640&quot; height=&quot;181&quot; src=&quot;https://developers.redhat.com/blog/wp-content/uploads/2018/09/OpenJDK_logo.svg_.png&quot; class=&quot;single-post-featured-img wp-post-image&quot; alt=&quot;OpenJDK logo&quot; srcset=&quot;https://developers.redhat.com/blog/wp-content/uploads/2018/09/OpenJDK_logo.svg_.png 1000w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/OpenJDK_logo.svg_-300x85.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/OpenJDK_logo.svg_-768x217.png 768w&quot; sizes=&quot;(max-width: 640px) 100vw, 640px&quot;/&gt;&lt;p&gt;Oracle recently announced that it would no longer supply free (as in beer) binary downloads for JDK releases after a six-month period, and neither would Oracle engineers write patches for OpenJDK bugs after that period. This has caused a great deal of concern among some Java users.&lt;/p&gt;
&lt;p&gt;From my point of view, this is little more than business as usual. Several years ago, the OpenJDK 6 updates (jdk6u) project was relinquished by Oracle and I assumed leadership, and then the same happened with OpenJDK 7. Subsequently, Andrew Brygin of Azul took over the leadership of OpenJDK 6. The OpenJDK Vulnerability Group, with members from many organizations, collaborates on critical security issues. With the help of the wider OpenJDK community and my team at Red Hat, we have continued to provide updates for critical bugs and security vulnerabilities at regular intervals. I can see no reason why this process should not work in the same way for OpenJDK 8 and the next long-term support release, OpenJDK 11.&lt;/p&gt;

&lt;span id=&quot;more-520407&quot;/&gt;
&lt;p&gt;I’m happy to assume leadership of the JDK 8 and 11 update projects if I have the support of the community.&lt;/p&gt;
&lt;p&gt;At Red Hat, we intend to provide support for OpenJDK 8 to our customers until 2023, and our policy of always “upstream first” implies that OpenJDK 8 will continue to be updated for critical bugs and security fixes until then. Something similar will happen for JDK 11.&lt;/p&gt;
&lt;p&gt;In addition to the people and organizations currently helping with OpenJDK updates, I have received offers of help from organizations not currently involved, in particular from Amazon Web Services. This bodes well, but it may take time to get everyone up to speed working as part of the community.&lt;/p&gt;
&lt;p&gt;There is also the question of back-porting important features from later OpenJDK releases to, for example, JDK 8. While new features, particularly performance-related ones, are undoubtedly nice to have, our first priority must be to not break anything: we must remember that we are stewards of a very precious piece of software. Only if we are sure that we’re not taking unnecessary risks should we do major back-ports. We also have to consider the maintenance burden. So, each proposal will have to be taken on its individual merits, and I don’t think we can have a one-size-fits-all policy for such things.&lt;/p&gt;
&lt;p&gt;One question which frequently arises is that of how people will get free downloads of compiled OpenJDK binaries, as opposed to the source code downloads that are provided by OpenJDK. I believe that the OpenJDK updates project itself should commit to providing binaries when releases are made. (Having said that, if you’re using some kind of Linux distribution, I would strongly recommend that you use the OpenJDK packages that are provided by the system and its package manager: you should get better integration and ease of updating that way. Some people might be worried that their chosen distribution will not build, test, and package OpenJDK correctly, but if you don’t trust your distribution to build packages, you shouldn’t be using it at all.)&lt;/p&gt;
&lt;p&gt;So, when we talk about OpenJDK binaries we’re mainly talking about Windows and Macintosh downloads. It will be up to the JDK updates projects to decide how and where to build the binaries. Having said that, my team at Red Hat is happy to commit to providing regularly updated, tested (and, in particular, TCK’d) Windows and Linux downloads, but we probably will need help building and testing on Macintosh. I’m sure we can get this done and we can continue to deserve the trust of Java users.&lt;/p&gt;
&lt;p&gt;Keeping Java updated in the absence of support from Oracle engineers will be a challenge to the Java community, but I believe it is one we should enthusiastically embrace. It is a golden opportunity for us, the community, to show what we can do. A truly open and transparent OpenJDK updates project will encourage wider participation and benefit all Java users.&lt;/p&gt;


&lt;div class=&quot;addtoany_share_save_container addtoany_content addtoany_content_bottom&quot;&gt;
&lt;div class=&quot;a2a_kit a2a_kit_size_16 addtoany_list&quot; data-a2a-url=&quot;https://developers.redhat.com/blog/2018/09/24/the-future-of-java-and-openjdk-updates-without-oracle-support/&quot; data-a2a-title=&quot;The future of Java and OpenJDK updates without Oracle support&quot;&gt;&lt;a class=&quot;a2a_dd addtoany_share_save addtoany_share&quot; href=&quot;https://www.addtoany.com/share&quot;&gt;&lt;img src=&quot;https://static.addtoany.com/buttons/favicon.png&quot; alt=&quot;Share&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Thu, 27 Sep 2018 05:37:58 +0000</pubDate>
<dc:creator>kjeetgill</dc:creator>
<og:type>article</og:type>
<og:title>The future of Java and OpenJDK updates without Oracle support - RHD Blog</og:title>
<og:description>This post describes the furture support for Java and OpenJDK updates, now that Oracle has announced that soon it will no longer supply free binaries for JDK releases or write patches for bugs.</og:description>
<og:url>https://developers.redhat.com/blog/2018/09/24/the-future-of-java-and-openjdk-updates-without-oracle-support/</og:url>
<og:image>https://developers.redhat.com/blog/wp-content/uploads/2018/09/OpenJDK_logo.svg_.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://developers.redhat.com/blog/2018/09/24/the-future-of-java-and-openjdk-updates-without-oracle-support/</dc:identifier>
</item>
<item>
<title>Facebook Is Giving Advertisers Access To Your Shadow Contact Information</title>
<link>https://gizmodo.com/facebook-is-giving-advertisers-access-to-your-shadow-co-1828476051</link>
<guid isPermaLink="true" >https://gizmodo.com/facebook-is-giving-advertisers-access-to-your-shadow-co-1828476051</guid>
<description>&lt;div class=&quot;img-wrapper lazy-image&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;img-permalink-sub-wrapper&quot;&gt;&lt;span class=&quot;js_lightbox-wrapper lightbox-wrapper&quot;&gt;&lt;img src=&quot;https://i.kinja-img.com/gawker-media/image/upload/s--ip_vWAav--/c_scale,f_auto,fl_progressive,q_80,w_800/ujcxx8sg5fwipdkvze7u.jpg&quot; class=&quot;lazyload ls-lazy-image-tag&quot; data-sizes=&quot;auto&quot; data-width=&quot;4200&quot; data-chomp-id=&quot;ujcxx8sg5fwipdkvze7u&quot; data-format=&quot;jpg&quot;/&gt;&lt;/span&gt;&lt;/div&gt;
Illustration: Angelica Alzona (Gizmodo Media Group)&lt;/div&gt;
&lt;p&gt;Last week, I ran an ad on Facebook that was targeted at a computer science professor named Alan Mislove. Mislove studies how privacy works on social networks and had a theory that Facebook is letting advertisers reach users with contact information collected in surprising ways. I was helping him test the theory by targeting him in a way Facebook had previously told me wouldn’t work. I directed the ad to display to a Facebook account connected to the landline number for Alan Mislove’s office, a number Mislove has never provided to Facebook. He saw the ad within hours.&lt;/p&gt;
&lt;div class=&quot;img-wrapper lazy-image&quot; readability=&quot;8&quot;&gt;
&lt;div class=&quot;img-permalink-sub-wrapper&quot;&gt;&lt;span class=&quot;js_lightbox-wrapper lightbox-wrapper&quot;&gt;&lt;img src=&quot;data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==&quot; class=&quot;lazyload ls-lazy-image-tag&quot; data-sizes=&quot;auto&quot; data-width=&quot;1023&quot; data-chomp-id=&quot;vq2h4pvl4xmy9sannbiy&quot; data-format=&quot;jpg&quot;/&gt;&lt;/span&gt;&lt;/div&gt;
What Facebook told Alan Mislove about the ad I targeted at his office landline numberScreenshot: Facebook (Alan Mislove)&lt;/div&gt;

&lt;div id=&quot;swappable-mobile-ad-container&quot; class=&quot;js_ad-mobile-dynamic swappable-mobile-ad-container js_ad-dynamic ad-mobile-dynamic movable-ad&quot;&gt;
&lt;div class=&quot;ad-unit ad-mobile&quot;&gt;
&lt;div class=&quot;ad-mobile-inner&quot;&gt;
&lt;p class=&quot;ad-label proxima&quot;&gt;&lt;small class=&quot;proxima&quot;&gt;Advertisement&lt;/small&gt;&lt;/p&gt;


&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;One of the many ways that ads get in front of your eyeballs on Facebook and Instagram is that the social networking giant lets an advertiser upload a list of phone numbers or email addresses it has on file; it will then put an ad in front of accounts associated with that contact information. A clothing retailer can put an ad for a dress in the Instagram feeds of women who have purchased from them before, a politician can place Facebook ads in front of anyone on his mailing list, or a casino can offer deals to the email addresses of people &lt;a href=&quot;https://www.listgiant.com/lists/gamblers/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'External link', 'https://www.listgiant.com/lists/gamblers/', {metric25:1})&quot;&gt;suspected of having a gambling addiction&lt;/a&gt;. Facebook calls this a “&lt;a href=&quot;https://www.facebook.com/business/a/custom-audiences&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'External link', 'https://www.facebook.com/business/a/custom-audiences', {metric25:1})&quot;&gt;custom audience&lt;/a&gt;.”&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;You might assume that you could go to your Facebook profile and look at your “contact and basic info” page to see what email addresses and phone numbers are associated with your account, and thus what advertisers can use to target you. But as is so often the case with this highly efficient data-miner posing as a way to keep in contact with your friends, it’s going about it in a less transparent and more invasive way.&lt;/p&gt;
&lt;p&gt;Facebook is not content to use the contact information you willingly put into your Facebook profile for advertising. It is also using contact information you handed over for security purposes and contact information you didn’t hand over at all, but that was collected from other people’s contact books, a hidden layer of details Facebook has about you that I’ve come to call “shadow contact information.” I managed to place an ad in front of Alan Mislove by targeting his shadow profile. This means that the junk email address that you hand over for discounts or for shady online shopping is likely associated with your account and being used to target you with ads.&lt;/p&gt;
&lt;div class=&quot;js_ad-mobile-dynamic js_ad-dynamic ad-mobile-dynamic movable-ad&quot;&gt;
&lt;div class=&quot;ad-unit ad-mobile&quot;&gt;
&lt;div class=&quot;ad-mobile-inner&quot;&gt;
&lt;p class=&quot;ad-label proxima&quot;&gt;&lt;small class=&quot;proxima&quot;&gt;Advertisement&lt;/small&gt;&lt;/p&gt;


&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;aside class=&quot;pullquote align--center&quot;&gt;&lt;span class=&quot;pullquote__content&quot;&gt;Facebook is not content to use the contact information you willingly put into your Facebook profile for advertising. It is also using contact information you handed over for security purposes and contact information you didn’t hand over at all.&lt;/span&gt;&lt;/aside&gt;&lt;p&gt;Facebook is not upfront about this practice. In fact, when I asked its PR team last year whether it was using shadow contact information for ads, they denied it. Luckily for those of us obsessed with &lt;a href=&quot;https://theoutline.com/post/5380/targeted-ad-creepy-surveillance-facebook-instagram-google-listening-not-alone?zd=1&amp;amp;zi=ucbo5ff2&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'External link', 'https://theoutline.com/post/5380/targeted-ad-creepy-surveillance-facebook-instagram-google-listening-not-alone?zd=1&amp;amp;zi=ucbo5ff2', {metric25:1})&quot;&gt;the uncannily accurate nature of ads on Facebook platforms&lt;/a&gt;, a group of academic researchers decided to do a deep dive into how Facebook custom audiences work to find out how users’ phone numbers and email addresses get sucked into the advertising ecosystem.&lt;/p&gt;
&lt;p&gt;Giridhari Venkatadri, Piotr Sapiezynski, and Alan Mislove of Northeastern University, along with Elena Lucherini of Princeton University, did a series of tests that involved handing contact information over to Facebook for a group of test accounts in different ways and then seeing whether that information could be used by an advertiser. They came up with a novel way to detect whether that information became available to advertisers by looking at the stats provided by Facebook about the size of an audience after contact information is uploaded. They go into this in greater length and technical detail &lt;a href=&quot;https://mislove.org/publications/PII-PETS.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'External link', 'https://mislove.org/publications/PII-PETS.pdf', {metric25:1})&quot;&gt;in their paper&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;js_ad-mobile-dynamic js_ad-dynamic ad-mobile-dynamic movable-ad&quot;&gt;
&lt;div class=&quot;ad-unit ad-mobile&quot;&gt;
&lt;div class=&quot;ad-mobile-inner&quot;&gt;
&lt;p class=&quot;ad-label proxima&quot;&gt;&lt;small class=&quot;proxima&quot;&gt;Advertisement&lt;/small&gt;&lt;/p&gt;


&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;They found that when a user gives Facebook a phone number for two-factor authentication or in order to receive alerts about new log-ins to a user’s account, that phone number became targetable by an advertiser within a couple of weeks. So users who want their accounts to be more secure are forced to make a privacy trade-off and allow advertisers to more easily find them on the social network. When asked about this, a Facebook spokesperson said that “we use the information people provide to offer a more personalized experience, including showing more relevant ads.” She said users bothered by this can set up two-factor authentication without using their phone numbers; Facebook stopped making a phone number mandatory for two-factor authentication &lt;a href=&quot;https://www.facebook.com/notes/facebook-security/two-factor-authentication-for-facebook-now-easier-to-set-up/10155341377090766/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'External link', 'https://www.facebook.com/notes/facebook-security/two-factor-authentication-for-facebook-now-easier-to-set-up/10155341377090766/', {metric25:1})&quot;&gt;four months ago&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The researchers also found that if User A, whom we’ll call Anna, shares her contacts with Facebook, including a previously unknown phone number for User B, whom we’ll call Ben, advertisers will be able to target Ben with an ad using that phone number, which I call “shadow contact information,” about a month later. Ben can’t access his shadow contact information, because that would violate Anna’s privacy, according to Facebook, so he can’t see it or delete it, and he can’t keep advertisers from using it either.&lt;/p&gt;
&lt;p&gt;The lead author on the paper, Giridhari Venkatadri, said this was the most surprising finding, that Facebook was targeted ads using information “that was not directly provided by the user, or even revealed to the user.”&lt;/p&gt;
&lt;div class=&quot;js_ad-mobile-dynamic js_ad-dynamic ad-mobile-dynamic movable-ad&quot;&gt;
&lt;div class=&quot;ad-unit ad-mobile&quot;&gt;
&lt;div class=&quot;ad-mobile-inner&quot;&gt;
&lt;p class=&quot;ad-label proxima&quot;&gt;&lt;small class=&quot;proxima&quot;&gt;Advertisement&lt;/small&gt;&lt;/p&gt;


&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;I’ve been &lt;a href=&quot;https://gizmodo.com/people-you-may-know-a-controversial-facebook-features-1827981959&quot; rel=&quot;nofollow&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'Internal link', 'https://gizmodo.com/people-you-may-know-a-controversial-facebook-features-1827981959', {metric25:1})&quot;&gt;trying&lt;/a&gt; to get Facebook to disclose shadow contact information to users for &lt;a href=&quot;https://gizmodo.com/how-facebook-figures-out-everyone-youve-ever-met-1819822691&quot; rel=&quot;nofollow&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'Internal link', 'https://gizmodo.com/how-facebook-figures-out-everyone-youve-ever-met-1819822691', {metric25:1})&quot;&gt;almost a year now&lt;/a&gt;. But it has even refused to disclose these shadow details to users in Europe, where privacy law is stronger and &lt;a href=&quot;https://gdpr-info.eu/art-15-gdpr/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'External link', 'https://gdpr-info.eu/art-15-gdpr/', {metric25:1})&quot;&gt;explicitly requires companies to tell users what data it has on them&lt;/a&gt;. A UK resident named Rob Blackie has been asking Facebook to hand over his shadow contact information for months, but Facebook told him it’s part of “confidential” algorithms, and “we are not in a position to provide you the precise details of our algorithms.”&lt;/p&gt;
&lt;p&gt;“People own their address books,” a Facebook spokesperson said by email. “We understand that in some cases this may mean that another person may not be able to control the contact information someone else uploads about them.”&lt;/p&gt;
&lt;p&gt;To test the shadow information finding, the researchers tried a real-world test. They uploaded a list of hundreds of landline numbers from Northeastern University. These are numbers that people who work for Northeastern are unlikely to have added to their accounts, though it’s very likely that the numbers would be in the address books of people who know them and who might have uploaded them to Facebook in order to “find friends.” The researchers found that many of these numbers could be targeted with ads, and when they ran an ad campaign, the ad turned up in the Facebook news feed of Mislove, whose landline had been included in the file; I confirmed this with my own test targeting his landline number.&lt;/p&gt;
&lt;div class=&quot;js_ad-mobile-dynamic js_ad-dynamic ad-mobile-dynamic movable-ad&quot;&gt;
&lt;div class=&quot;ad-unit ad-mobile&quot;&gt;
&lt;div class=&quot;ad-mobile-inner&quot;&gt;
&lt;p class=&quot;ad-label proxima&quot;&gt;&lt;small class=&quot;proxima&quot;&gt;Advertisement&lt;/small&gt;&lt;/p&gt;


&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;“It’s likely that he was shown the ad because someone else uploaded his contact information via contact importer,” a Facebook spokesperson confirmed when I told the company about the experiment.&lt;/p&gt;
&lt;p&gt;Facebook did not dispute any of the researchers’ findings. “We outline the information we receive and use for ads in our &lt;a href=&quot;https://www.facebook.com/privacy/explanation&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'External link', 'https://www.facebook.com/privacy/explanation', {metric25:1})&quot;&gt;data policy&lt;/a&gt;, and give people control over their ads experience including custom audiences, via &lt;a href=&quot;https://www.facebook.com/ads/preferences/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'External link', 'https://www.facebook.com/ads/preferences/', {metric25:1})&quot;&gt;their ad preferences&lt;/a&gt;,” said a spokesperson by email. “For more information about how to manage your preferences and the type of data we use to show people ads see &lt;a href=&quot;https://newsroom.fb.com/news/2018/04/data-and-advertising/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'External link', 'https://newsroom.fb.com/news/2018/04/data-and-advertising/', {metric25:1})&quot;&gt;this post&lt;/a&gt;.”&lt;/p&gt;
&lt;p&gt;In that post, “Hard Questions: What Information Do Facebook Advertisers Know About Me?”, Facebook’s vice president of ads Rob Goldman discusses how advertising works on Facebook and what you can do if “I don’t want my data used to show me ads.” The post doesn’t mention the surprising collection or use of contact information for targeted advertising that the researchers discovered.&lt;/p&gt;
&lt;div class=&quot;js_ad-mobile-dynamic js_ad-dynamic ad-mobile-dynamic movable-ad&quot;&gt;
&lt;div class=&quot;ad-unit ad-mobile&quot;&gt;
&lt;div class=&quot;ad-mobile-inner&quot;&gt;
&lt;p class=&quot;ad-label proxima&quot;&gt;&lt;small class=&quot;proxima&quot;&gt;Advertisement&lt;/small&gt;&lt;/p&gt;


&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;em&gt;“&lt;/em&gt;I think that many users don’t fully understand how ad targeting works today: that advertisers can literally specify exactly which users should see their ads by uploading the users’ email addresses, phone numbers, names+dates of birth, etc,” said Mislove. “In describing this work to colleagues, many computer scientists were surprised by this, and were even more surprised to learn that not only Facebook, but also Google, Pinterest, and Twitter all offer related services. Thus, we think there is a significant need to educate users about how exactly targeted advertising on such platforms works today.”&lt;/p&gt;


&lt;p&gt;While Facebook isn’t upfront about which of your contact information it uses for ads, it is upfront about which advertisers are getting access to you with it. Facebook’s “ad preferences” page has a section devoted to “&lt;a href=&quot;https://www.facebook.com/ads/preferences/?entry_product=ad_settings_screen&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'External link', 'https://www.facebook.com/ads/preferences/?entry_product=ad_settings_screen', {metric25:1})&quot;&gt;advertisers you’ve interacted with&lt;/a&gt;” where it will show you which advertisers have you in their contact list. My own list has over 300 advertisers on it, very few of whom to which I remember consciously giving my contact information—but if I did it would likely have been a junk email address so that I never had to hear from them again. Mislove says Facebook could be far more transparent here:&lt;/p&gt;
&lt;p&gt;“Facebook could also reveal to users &lt;em&gt;which&lt;/em&gt; [personal information] was used to target the delivered ad, helping users understand how their [information] is used by advertisers,” said Mislove by email. In other words, Facebook could tell me which email address or phone number all these advertisers have on me. With the involvement of shadow contact information, though, Facebook may have been avoiding that because it doesn’t want me to know what personal information &lt;em&gt;Facebook&lt;/em&gt; has on me.&lt;/p&gt;
&lt;div class=&quot;js_ad-mobile-dynamic js_ad-dynamic ad-mobile-dynamic movable-ad&quot;&gt;
&lt;div class=&quot;ad-unit ad-mobile&quot;&gt;
&lt;div class=&quot;ad-mobile-inner&quot;&gt;
&lt;p class=&quot;ad-label proxima&quot;&gt;&lt;small class=&quot;proxima&quot;&gt;Advertisement&lt;/small&gt;&lt;/p&gt;


&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;section class=&quot;quotable&quot; readability=&quot;1.2178770949721&quot;&gt;&lt;div class=&quot;quotable__side&quot;&gt;&lt;img src=&quot;https://i.kinja-img.com/gawker-media/image/upload/s--iAfrV56p--/c_fill,fl_progressive,g_center,h_200,q_80,w_200/l061sd370qmbkkovfqqk.jpg&quot; class=&quot;quotable__thumbnail&quot; alt=&quot;&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;quotable__main&quot; readability=&quot;20.703910614525&quot;&gt;
&lt;h4 class=&quot;quotable__header&quot;&gt;Contact the Special Projects Desk&lt;/h4&gt;
&lt;p class=&quot;quotable__content&quot;&gt;This post was produced by the &lt;a href=&quot;https://specialprojectsdesk.com&quot;&gt;Special Projects Desk of Gizmodo Media.&lt;/a&gt; Email us at &lt;a href=&quot;mailto:tips@gizmodomedia.com&quot;&gt;tips@gizmodomedia.com&lt;/a&gt;, or contact us securely using &lt;a href=&quot;https://specialprojectsdesk.com/how-to-contact-our-reporters-using-securedrop-1823763689&quot;&gt;SecureDrop&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/section&gt;&lt;p&gt;There are certainly creepier practices happening in the advertising industry, but it’s troubling this is happening at Facebook because of its representations about &lt;a href=&quot;https://www.facebook.com/about/ads&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'External link', 'https://www.facebook.com/about/ads', {metric25:1})&quot;&gt;letting you control your ad experience&lt;/a&gt;. It’s disturbing that Facebook is reducing the privacy of people who want their accounts to be more secure by using the information they provide for that purpose to data-mine them for ads. And it’s also troubling to discover another way in which shadow contact information is used, &lt;a href=&quot;https://gizmodo.com/how-facebook-figures-out-everyone-youve-ever-met-1819822691&quot; rel=&quot;nofollow&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'Internal link', 'https://gizmodo.com/how-facebook-figures-out-everyone-youve-ever-met-1819822691', {metric25:1})&quot;&gt;beyond friend recommendations&lt;/a&gt;, given that Facebook doesn’t let users see this information about themselves or let them delete it.&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Mislove thinks Facebook can make its platform more transparent by telling users everything it knows about them, including all the contact information it’s gathered from various sources, and how that information gets used. He suggests that Facebook let users see all the data it has on them and then let them specify whether it is correct and whether advertisers can use it.&lt;/p&gt;
&lt;div class=&quot;js_ad-mobile-dynamic js_ad-dynamic ad-mobile-dynamic movable-ad&quot;&gt;
&lt;div class=&quot;ad-unit ad-mobile&quot;&gt;
&lt;div class=&quot;ad-mobile-inner&quot;&gt;
&lt;p class=&quot;ad-label proxima&quot;&gt;&lt;small class=&quot;proxima&quot;&gt;Advertisement&lt;/small&gt;&lt;/p&gt;


&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Facebook has claimed that users already have extensive control over what information is made available to advertisers, but that’s not entirely true. When I asked the company last year about whether it used shadow contact information for ads, it gave me inaccurate information, and it hadn’t made the practice clear in its &lt;a href=&quot;https://www.facebook.com/ads/about/?entry_product=ad_preferences&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'External link', 'https://www.facebook.com/ads/about/?entry_product=ad_preferences', {metric25:1})&quot;&gt;extensive messaging&lt;/a&gt; to users about ads. It took academic researchers performing tests for months to unearth the truth. People are increasingly paranoid about the creepy accuracy of the ads they see online and don’t understand where the information is coming from that leads to that accuracy. It seems that, when it came to this particular practice, Facebook wanted to keep its users in the dark.&lt;/p&gt;
</description>
<pubDate>Thu, 27 Sep 2018 03:49:51 +0000</pubDate>
<dc:creator>dhotson</dc:creator>
<og:title>Facebook Is Giving Advertisers Access to Your Shadow Contact Information</og:title>
<og:type>article</og:type>
<og:image>https://i.kinja-img.com/gawker-media/image/upload/s--lA102VRi--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/ujcxx8sg5fwipdkvze7u.jpg</og:image>
<og:url>https://gizmodo.com/facebook-is-giving-advertisers-access-to-your-shadow-co-1828476051</og:url>
<og:description>Last week, I ran an ad on Facebook that was targeted at a computer science professor named Alan Mislove. Mislove studies how privacy works on social networks and had a theory that Facebook is letting advertisers reach users with contact information collected in surprising ways. I was helping him test the theory by targeting him in a way Facebook had previously told me wouldn’t work. I directed the ad to display to a Facebook account connected to the landline number for Alan Mislove’s office, a number Mislove has never provided to Facebook. He saw the ad within hours.</og:description>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://gizmodo.com/facebook-is-giving-advertisers-access-to-your-shadow-co-1828476051</dc:identifier>
</item>
<item>
<title>How to visualize decision trees</title>
<link>http://explained.ai/decision-tree-viz/index.html</link>
<guid isPermaLink="true" >http://explained.ai/decision-tree-viz/index.html</guid>
<description>&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot; /&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400&quot; /&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;css/article.css&quot; /&gt;&lt;title&gt;How to visualize decision trees&lt;/title&gt;&lt;meta property=&quot;og:title&quot; content=&quot;How to visualize decision tree&quot; /&gt;&lt;meta property=&quot;og:image&quot; content=&quot;http://explained.ai/decision-tree-viz/images/knowledge-TD-3-X.png&quot; /&gt;&lt;meta property=&quot;og:description&quot; content=&quot;Decision trees are the fundamental building block of gradient boosting machines and Random Forests(tm), probably the two most popular machine learning models for structured data. Visualizing decision trees is a tremendous aid when learning how these models work and when interpreting models. Unfortunately, current visualization packages are rudimentary and not immediately helpful to the novice. For example, we couldn't find a library that visualizes how decision nodes split up the feature space. So, we've created a general package (part of the animl library) for scikit-learn decision tree visualization and model interpretation.&quot; /&gt;&lt;meta property=&quot;og:url&quot; content=&quot;http://explained.ai/decision-tree-viz/index.html&quot; /&gt;&lt;meta property=&quot;og:type&quot; content=&quot;article&quot; /&gt;&lt;meta name=&quot;twitter:title&quot; content=&quot;How to visualize decision tree&quot; /&gt;&lt;meta name=&quot;twitter:card&quot; content=&quot;summary_large_image&quot; /&gt;&lt;meta name=&quot;twitter:site&quot; content=&quot;@the_antlr_guy&quot; /&gt;&lt;meta name=&quot;twitter:creator&quot; content=&quot;@the_antlr_guy&quot; /&gt;&lt;meta name=&quot;twitter:description&quot; content=&quot;Decision trees are the fundamental building block of gradient boosting machines and Random Forests(tm), probably the two most popular machine learning models for structured data. Visualizing decision trees is a tremendous aid when learning how these models work and when interpreting models. Unfortunately, current visualization packages are rudimentary and not immediately helpful to the novice. For example, we couldn't find a library that visualizes how decision nodes split up the feature space. So, we've created a general package (part of the animl library) for scikit-learn decision tree visualization and model interpretation.&quot; /&gt;&lt;meta name=&quot;twitter:image&quot; content=&quot;http://explained.ai/decision-tree-viz/images/knowledge-TD-3-X.png&quot; /&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;567.94348603826&quot;&gt;


&lt;p&gt;&lt;a href=&quot;http://parrt.cs.usfca.edu&quot;&gt;Terence Parr&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/groverpr&quot;&gt;Prince Grover&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(Terence teaches in &lt;a href=&quot;https://www.usfca.edu/arts-sciences/graduate-programs/data-science&quot;&gt;University of San Francisco's MS in Data Science program&lt;/a&gt; and Prince is an alumnus. You might know Terence as the creator of the ANTLR parser generator.)&lt;/p&gt;
&lt;p&gt;Please send comments, suggestions, or fixes to &lt;a href=&quot;mailto:parrt@cs.usfca.edu&quot;&gt;Terence&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Decision trees are the fundamental building block of &lt;a href=&quot;http://explained.ai/gradient-boosting/index.html&quot;&gt;gradient boosting machines&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_forest&quot;&gt;Random Forests&lt;/a&gt;™, probably the two most popular machine learning models for structured data. Visualizing decision trees is a tremendous aid when learning how these models work and when interpreting models. Unfortunately, current visualization packages are rudimentary and not immediately helpful to the novice. For example, we couldn't find a library that visualizes how decision nodes split up the feature space. It is also uncommon for libraries to support visualizing a specific feature vector as it weaves down through a tree's decision nodes; we could only find one image showing this.&lt;/p&gt;
&lt;p&gt;So, we've created a general package for &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn&quot;&gt;scikit-learn&lt;/a&gt; decision tree visualization and model interpretation, which we'll be using heavily in an upcoming &lt;a href=&quot;https://mlbook.explained.ai/&quot;&gt;machine learning book&lt;/a&gt; (written with &lt;a href=&quot;http://www.fast.ai/about/#jeremy&quot;&gt;Jeremy Howard&lt;/a&gt;). Here's a sample visualization for a tiny decision tree (click to enlarge):&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/samples/wine-TD-2.svg&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/samples/wine-TD-2.svg&quot; width=&quot;70%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This article demonstrates the results of this work, details the specific choices we made for visualization, and outlines the tools and techniques used in the implementation. The visualization software is part of a nascent Python machine learning library called &lt;a href=&quot;https://github.com/parrt/animl&quot;&gt;animl&lt;/a&gt;. We assume you're familiar with the basic mechanism of decision trees if you're interested in visualizing them, but let's start with a brief summary so that we're all using the same terminology. (If you're not familiar with decision trees, check out &lt;a href=&quot;http://course.fast.ai/ml&quot;&gt;fast.ai's Introduction to Machine Learning for Coders MOOC&lt;/a&gt;.)&lt;/p&gt;
&lt;h2 id=&quot;sec:1.1&quot;&gt;Decision tree review&lt;/h2&gt;
&lt;p&gt;A decision tree is a machine learning model based upon binary trees (trees with at most a left and right child). A decision tree learns the relationship between observations in a training set, represented as feature vectors &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; and target values &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;, by examining and condensing training data into a binary tree of interior nodes and leaf nodes. (Notation: vectors are in bold and scalars are in italics.)&lt;/p&gt;
&lt;p&gt;Each leaf in the decision tree is responsible for making a specific prediction. For regression trees, the prediction is a value, such as price. For classifier trees, the prediction is a target category (represented as an integer in scikit), such as cancer or not-cancer. A decision tree carves up the feature space into groups of observations that share similar target values and each leaf represents one of these groups. For regression, similarity in a leaf means a low variance among target values and, for classification, it means that most or all targets are of a single class.&lt;/p&gt;
&lt;p&gt;Any path from the root of the decision tree to a specific leaf predictor passes through a series of (internal) decision nodes. Each decision node compares a single feature's value in &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;, &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;, with a specific &lt;em&gt;split point&lt;/em&gt; value learned during training. For example, in a model predicting apartment rent prices, decision nodes would test features such as the number of bedrooms and number of bathrooms. (See &lt;strong&gt;Section 3&lt;/strong&gt; &lt;em&gt;Visualizing tree interpretation of a single observation&lt;/em&gt;.) Even in a classifier with discrete target values, decision nodes still compare numeric &lt;em&gt;feature&lt;/em&gt; values because scitkit's decision tree implementation assumes that all features are numeric. Categorical variables must be &lt;a href=&quot;https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/&quot;&gt;one hot encoded&lt;/a&gt;, &lt;a href=&quot;https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/&quot;&gt;binned&lt;/a&gt;, &lt;a href=&quot;http://forums.fast.ai/t/to-label-encode-or-one-hot-encode/6057&quot;&gt;label encoded&lt;/a&gt;, etc...&lt;/p&gt;
&lt;p&gt;To train a decision node, the model examines a subset of the training observations (or the full training set at the root). The node's feature and split point within that feature space are chosen during training to split the observations into left and right buckets (subsets) to maximize similarity as defined above. (This selection process is generally done through exhaustive comparison of features and feature values.) The left bucket has observations whose &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; feature values are all less than the split point and the right bucket has observations whose &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is greater than the split point. Tree construction proceeds recursively by creating decision nodes for the left bucket and the right bucket. Construction stops when some stopping criterion is reached, such as having less than five observations in the node.&lt;/p&gt;
&lt;h2 id=&quot;sec:1.2&quot;&gt;The key elements of decision tree visualization&lt;/h2&gt;
&lt;p&gt;Decision tree visualizations should highlight the following important elements, which we demonstrate below.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Decision node &lt;strong&gt;feature versus target value&lt;/strong&gt; distributions (which we call feature-target space in this article). We want to know how separable the target values are based upon the feature and a split point.&lt;/li&gt;
&lt;li&gt;Decision node &lt;strong&gt;feature name and feature split value&lt;/strong&gt;. We need to know which feature each decision node is testing and where in that space the nodes splits the observations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Leaf node purity&lt;/strong&gt;, which affects our prediction confidence. Leaves with low variance among the target values (regression) or an overwhelming majority target class (classification) are much more reliable predictors.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Leaf node prediction value&lt;/strong&gt;. What is this leaf actually predicting from the collection of target values?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Numbers of samples in decision nodes&lt;/strong&gt;. Sometimes it's useful to know where all most of the samples are being routed through the decision nodes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Numbers of samples in leaf nodes&lt;/strong&gt;. Our goal is a decision tree with fewer, larger and purer leaves. Nodes with too few samples are possible indications of overfitting.&lt;/li&gt;
&lt;li&gt;How a specific feature vector is &lt;strong&gt;run down the tree&lt;/strong&gt; to a leaf. This helps explain why a particular feature vector gets the prediction it does. For example, in a regression tree predicting apartment rent prices, we might find a feature vector routed into a high predicted price leaf because of a decision node that checks for more than three bedrooms.&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;sec:1.3&quot;&gt;Gallery of decision tree visualizations&lt;/h2&gt;
&lt;p&gt;Before digging into the previous state-of-the-art visualizations, we'd like to give a little spoiler to show what's possible. This section highlights some samples visualizations we built from scikit regression and classification decision trees on a few data sets. You can also check out the &lt;a href=&quot;https://github.com/parrt/animl/tree/master/testing/samples&quot;&gt;full gallery&lt;/a&gt; and &lt;a href=&quot;https://github.com/parrt/animl/blob/master/testing/gen_samples.py&quot;&gt;code to generate all samples&lt;/a&gt;.&lt;/p&gt;
&lt;center&gt;
&lt;/center&gt;
&lt;h2 id=&quot;sec:1.4&quot;&gt;A comparison to previous state-of-the-art visualizations&lt;/h2&gt;
&lt;p&gt;If you search for “visualizing decision trees” you will quickly find a &lt;strong&gt;Python&lt;/strong&gt; solution provided by the awesome scikit folks: &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html&quot;&gt;sklearn.tree.export_graphviz&lt;/a&gt;. With more work, you can find visualizations for &lt;a href=&quot;http://scikit-learn.org/stable/modules/tree.html&quot;&gt;R&lt;/a&gt; and even &lt;a href=&quot;http://support.sas.com/documentation/cdl/en/vaug/68027/HTML/default/viewer.htm#n0q3i0zwng79kin1kb1zvpo9k312.htm&quot;&gt;SAS&lt;/a&gt; and &lt;a href=&quot;https://www.ibm.com/support/knowledgecenter/en/SS4QC9/com.ibm.solutions.wa_an_overview.2.0.0.doc/wa_discover_viz_expl_insigths_dec_tree.html&quot;&gt;IBM&lt;/a&gt;. In this section, we collect the various decision tree visualizations we could find and compare them to the visualizations made by our &lt;span class=&quot;inlinecode&quot;&gt;animl&lt;/span&gt; library. We give a more detailed discussion of our visualizations in the next section.&lt;/p&gt;
&lt;p&gt;Let's start with the &lt;a href=&quot;http://scikit-learn.org/stable/modules/tree.html&quot;&gt;default scitkit visualization&lt;/a&gt; of a decision tree on the well-known &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris&quot;&gt;Iris&lt;/a&gt; data set (click on images to enlarge).&lt;/p&gt;
&lt;center&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;top&quot; width=&quot;50%&quot;&gt;&lt;/th&gt;
&lt;th align=&quot;center&quot; valign=&quot;top&quot; width=&quot;50%&quot;&gt;&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody readability=&quot;2&quot;&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;Default scikit Iris visualization&lt;/td&gt;
&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;Our &lt;span class=&quot;inlinecode&quot;&gt;animl&lt;/span&gt; Iris visualization&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/iris-scikit.png&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/iris-scikit.png&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/samples/iris-TD-5.svg&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/samples/iris-TD-5.svg&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;p&gt;The scikit tree does a good job of representing the tree structure, but we have a few quibbles. The colors aren't the best and it's not immediately obvious why some of the nodes are colored and some aren't. If the colors represent predicted class for this classifier, one would think just the leaves would be colored because only leaves have predictions. (It turns out the non-colored nodes have no majority prediction.) Including the gini coefficient (certainty score) costs space and doesn't help with interpretation. The count of samples of the various target classes in each node is somewhat useful, but a histogram would be even better. A target class color legend would be nice. Finally, using true and false as the edge labels isn't as clear as, say, labels &lt;img src=&quot;http://explained.ai/decision-tree-viz/images/eqn-524A50782178998021A88B8CD4C8DCD8-depth000.51.svg&quot; /&gt; and &lt;img src=&quot;http://explained.ai/decision-tree-viz/images/eqn-4EDC933D28BFE3F5EFFE94BF892DAD38-depth001.72.svg&quot; /&gt;. The most obvious difference is that our decision nodes show feature distributions as overlapping stacked-histograms, one histogram per target class. Also, our leaf size is proportional to the number of samples in that leaf.&lt;/p&gt;
&lt;p&gt;Scikit uses the same visualization approach for decision tree regressors. For example, here is scikit's visualization using the &lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html&quot;&gt;Boston&lt;/a&gt; data set, with &lt;span class=&quot;inlinecode&quot;&gt;animl&lt;/span&gt;'s version for comparison (click to enlarge images):&lt;/p&gt;
&lt;center&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;top&quot; width=&quot;50%&quot;&gt;&lt;/th&gt;
&lt;th align=&quot;center&quot; valign=&quot;top&quot; width=&quot;50%&quot;&gt;&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody readability=&quot;2&quot;&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;Default scikit Boston visualization&lt;/td&gt;
&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;Our &lt;span class=&quot;inlinecode&quot;&gt;animl&lt;/span&gt; Boston visualization&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/boston-scikit.png&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/boston-scikit.png&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/samples/boston-TD-3.svg&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/samples/boston-TD-3.svg&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;p&gt;In the scikit tree, it's not immediately clear what the use of color implies, but after studying the image, darker images indicate higher predicted target values. As before, our decision nodes show the feature space distribution, this time using a feature versus target value scatterplot. The leaves use strip plots to show the target value distribution; leaves with more dots naturally have a higher number of samples.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;R&lt;/strong&gt; programmers also have access to a package for &lt;a href=&quot;http://blog.revolutionanalytics.com/2013/06/plotting-classification-and-regression-trees-with-plotrpart.html&quot;&gt;visualizing decision trees&lt;/a&gt;, which gives similar results to scikit but with nicer edge labels:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/R-tree.png&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/R-tree.png&quot; width=&quot;40%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SAS&lt;/strong&gt; and &lt;strong&gt;IBM&lt;/strong&gt; also provide (non-Python-based) decision tree visualizations. Starting with SAS, we see that their decision nodes include a bar chart related to the node's sample target values and other details:&lt;/p&gt;
&lt;center&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;top&quot; width=&quot;50%&quot;&gt;&lt;/th&gt;
&lt;th align=&quot;center&quot; valign=&quot;top&quot; width=&quot;50%&quot;&gt;&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody readability=&quot;1&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;SAS visualization&lt;/td&gt;
&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;SAS visualization (best image quality we could find with numeric features)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/sas-tree.png&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/sas-tree.png&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/sas-tree2.jpeg&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/sas-tree2.jpeg&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;p&gt;Indicating the size of the left and right buckets via edge width is a nice touch. But, those bar charts are hard to interpret because they have no horizontal axis. Decision nodes testing categorical variables (left image) have exactly one bar per category, so they must represent simple category counts, rather than feature distributions. For numeric features (right image), SAS decision nodes show a histogram of either target or feature space (we can't tell from the image). SAS node bar charts / histograms appear to illustrate just target values, which tells us nothing about how the feature space was split.&lt;/p&gt;
&lt;p&gt;The SAS tree on the right appears to highlight a path through the decision tree for a specific unknown feature vector, but we couldn't find any other examples from other tools and libraries. The ability to visualize a specific vector run down the tree does not seem to be generally available.&lt;/p&gt;
&lt;p&gt;Moving on to IBM software, here is a nice visualization that also shows decision node category counts as bar charts, from &lt;a href=&quot;https://www.ibm.com/support/knowledgecenter/en/SS4QC9/com.ibm.solutions.wa_an_overview.2.0.0.doc/wa_discover_viz_expl_insigths_dec_tree.html&quot;&gt;IBM's Watson analytics&lt;/a&gt; (on the &lt;a href=&quot;https://www.kaggle.com/c/titanic/data&quot;&gt;TITANIC&lt;/a&gt; data set):&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/ibm-tree.png&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/ibm-tree.png&quot; width=&quot;60%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;IBM's earlier &lt;strong&gt;SPSS&lt;/strong&gt; product also had decision tree visualizations:&lt;/p&gt;
&lt;center&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;top&quot; width=&quot;50%&quot;&gt;&lt;/th&gt;
&lt;th align=&quot;center&quot; valign=&quot;top&quot; width=&quot;50%&quot;&gt;&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;SPSS visualization&lt;/td&gt;
&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;SPSS visualization&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/spss-tree.png&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/spss-tree.png&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/spss-tree2.png&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/spss-tree2.png&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;p&gt;These SPSS decision nodes seem to give the same SAS-like bar chart of sample target class counts.&lt;/p&gt;
&lt;p&gt;All of the visualizations we encountered from the major players were useful, but we were most inspired by the eye-popping visualizations in &lt;a href=&quot;http://www.r2d3.us/visual-intro-to-machine-learning-part-1/&quot;&gt;A visual introduction to machine learning&lt;/a&gt;, which shows an (animated) decision tree like this:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/vizml-tree.png&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/vizml-tree.png&quot; width=&quot;75%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This visualization has three unique characteristics over previous work, aside from the animation:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;the decision nodes show how the feature space is split&lt;/li&gt;
&lt;li&gt;the split points for decision nodes are shown visually (as a wedge) in the distribution&lt;/li&gt;
&lt;li&gt;the leaf size is proportional to the number of samples in that leaf&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;While that visualization is a hardcoded animation for educational purposes, it points in the right direction.&lt;/p&gt;
&lt;h2 id=&quot;sec:1.5&quot;&gt;Our decision tree visualizations&lt;/h2&gt;
&lt;p&gt;Other than the educational animation in &lt;a href=&quot;http://www.r2d3.us/visual-intro-to-machine-learning-part-1/&quot;&gt;A visual introduction to machine learning&lt;/a&gt;, we couldn't find a decision tree visualization package that illustrates how the feature space is split up at decision nodes (feature-target space). This is the critical operation performed during decision tree model training and is what newcomers should focus on, so we'll start by examining decision node visualizations for both classification and regression trees.&lt;/p&gt;
&lt;h3 id=&quot;sec:1.5.1&quot;&gt;Visualizing feature-target space&lt;/h3&gt;
&lt;p&gt;Training of a decision node chooses feature &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and split value within &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;'s range of values (feature space) to group samples with similar target values into two buckets. Just to be clear, training involves examining the relationship between features and target values. Unless decision nodes show feature-target space in some way, the viewer cannot see how and why training arrived at the split value. To highlight how decision nodes carve up the feature space, we trained a regressor and classifier with a single (AGE) feature (&lt;a href=&quot;https://github.com/parrt/animl/blob/master/testing/paper_examples.py&quot;&gt;code to generate images&lt;/a&gt;). Here's a regressor decision tree trained using a single feature from the Boston data, &lt;span class=&quot;inlinecode&quot;&gt;AGE&lt;/span&gt;, and with node ID labeling turned on for discussion purposes here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/boston-TD-AGE.svg&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/boston-TD-AGE.svg&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Horizontal dashed lines indicate the target mean for the left and right buckets in decision nodes; a vertical dashed line indicates the split point in feature space. The black wedge highlights the split point and identifies the exact split value. Leaf nodes indicate the target prediction (mean) with a dashed line.&lt;/p&gt;
&lt;p&gt;As you can see, each &lt;span class=&quot;inlinecode&quot;&gt;AGE&lt;/span&gt; feature axis uses the same range, rather than zooming in, to make it easier to compare decision nodes. As we descend through decision nodes, the sample &lt;span class=&quot;inlinecode&quot;&gt;AGE&lt;/span&gt; values are boxed into narrower and narrower regions. For example, the &lt;span class=&quot;inlinecode&quot;&gt;AGE&lt;/span&gt; feature space in node 0 is split into the regions of &lt;span class=&quot;inlinecode&quot;&gt;AGE&lt;/span&gt; future space shown in nodes 1 and 8. Node 1's space is then split into the chunks shown in nodes 2 and 5. The prediction leaves are not very pure because training a model on just a single variable leads to a poor model, but this restricted example demonstrates how decision trees carve up feature space.&lt;/p&gt;
&lt;p&gt;While a decision tree implementation is virtually identical for both classifier and regressor decision trees, the way we interpret them is very different, so our visualizations are distinct for the two cases. For a regressor, showing feature-target space is best done with a scatterplot of feature versus target. For classifiers, however, the target is a category, rather than a number, so we chose to illustrate feature-target space using histograms as an indicator of feature space distributions. Here's a classifier tree trained on the USER KNOWLEDGE data, again with a single feature (PEG) and with nodes labeled for discussion purposes:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/knowledge-TD-PEG.svg&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/knowledge-TD-PEG.svg&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ignoring color, the histogram shows the PEG feature space distribution. Adding color gives us an indication of the relationship between feature space and target class. For example, in node 0 we can see that samples with &lt;span class=&quot;inlinecode&quot;&gt;very_low&lt;/span&gt; target class are clustered at the low end of PEG feature space and samples with &lt;span class=&quot;inlinecode&quot;&gt;High&lt;/span&gt; target class are clustered at the high-end. As with the regressor, the feature space of a left child is everything to the left of the parent's split point in the same feature space; similarly for the right child. For example, combining the histograms of nodes 9 and 12 yields the histogram of node 8. We force the horizontal axis range to be the same for all PEG decision nodes so that decision nodes lower in the tree clearly box in narrower regions that are more and more pure.&lt;/p&gt;
&lt;p&gt;We use a stacked histogram so that overlap is clear in the feature space between samples with different target classes. Note that the height in the Y axis of the stacked histogram is the total number of samples from all classes; multiple class counts are stacked on top of each other.&lt;/p&gt;
&lt;p&gt;When there are more than four or five classes, the stacked histograms are difficult to read, so we recommend setting the histogram type parameter to &lt;span class=&quot;inlinecode&quot;&gt;bar&lt;/span&gt; not &lt;span class=&quot;inlinecode&quot;&gt;barstacked&lt;/span&gt; in this case. With high cardinality target categories, the overlapping distributions are harder to visualize and things break down, so we set a limit of 10 target classes. Here's a shallow tree example using the 10-class Digits data set using non-stacked histograms:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/samples/digits-TD-2.svg&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/samples/digits-TD-2.svg&quot; width=&quot;65%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;sec:1.5.2&quot;&gt;It's all about the details&lt;/h3&gt;
&lt;p&gt;Thus far we've skipped over many of the visual cues and details that we obsessed over during construction of the library and so we hit the key elements here.&lt;/p&gt;
&lt;p&gt;Our classifier tree visualizations use node size to give visual cues about the number of samples associated with each node. Histograms get proportionally shorter as the number of samples in the node decrease and leaf node diameters get smaller. The feature space (horizontal axis) is always the same width and the same range for a given feature, which makes it much easier to compare the feature-target spaces of different nodes. The bars of all histograms are the same width in pixels. We use just the start/stop range labels for both horizontal and vertical axes to reduce clutter.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/knowledge-dec-node.png&quot; width=&quot;35%&quot; /&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/knowledge-dec-node2.png&quot; width=&quot;35%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;We use a pie chart for classifier leaves, despite their bad reputation. For the purpose of indicating purity, the viewer only needs an indication of whether there is a single strong majority category. The viewer does not need to see the exact relationship between elements of the pie chart, which is one key area where pie charts fail. The color of the pie chart majority slice gives the leaf prediction.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/knowledge-leaf.png&quot; width=&quot;8%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Turning to regressor trees now, we make sure that the target (vertical) axis of all decision nodes is the same height and the same range to make comparing nodes easier. Regressor feature space (horizontal axis) is always the same width and the same range for a given feature. We set a low alpha for all scatterplot dots so that increased target value density corresponds to darker color.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/boston-dec-node.png&quot; width=&quot;30%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Regressor leaves also show the same range vertically for the target space. We use a &lt;a href=&quot;https://seaborn.pydata.org/generated/seaborn.stripplot.html&quot;&gt;strip plot&lt;/a&gt; rather than, say, a box plot, because the strip plot shows the distribution explicitly while implicitly showing the number of samples by the number of dots. (We also write out the number of samples in text for leaves.) The leaf prediction is the distribution center of mass (mean) of the strip plot, which we highlight with a dashed line.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/boston-leaf.png&quot; width=&quot;25%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;There are also a number of miscellaneous details that we think improve the quality of the diagrams:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Classifiers include a legend&lt;/li&gt;
&lt;li&gt;All colors were handpicked from colorblind safe palettes, one handpicked palette per number of target categories (2 through 10)&lt;/li&gt;
&lt;li&gt;We use a gray rather than black for text because it's easier on the eyes&lt;/li&gt;
&lt;li&gt;Lines are hairlines&lt;/li&gt;
&lt;li&gt;We draw outlines of bars in bar charts and slices in pie charts&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;run-X&quot;&gt;Visualizing tree interpretation of a single observation&lt;/h3&gt;
&lt;p&gt;To figure out how model training arrives at a specific tree, all of the action is in the feature-space splits of the decision nodes, which we just discussed. Now, let's take a look at visualizing how a specific feature vector yields a specific prediction. The key here is to examine the decisions taken along the path from the root to the leaf predictor node.&lt;/p&gt;
&lt;p&gt;Decision-making within a node is straightforward: take the left path if feature &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; in test vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; is less than the split point, otherwise take the right path. To highlight the decision-making process, we have to highlight the comparison operation. For decision nodes along the path to the leaf predictor node, we show an orange wedge at position &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; in the horizontal feature space. This makes the comparison easy to see; if the orange wedge is to the left of the black wedge, go left else go right. Decision nodes involved in the prediction process are surrounded by boxes with dashed lines and the child edges are thicker and colored orange. Here are two sample trees showing test vectors (click on images to expand):&lt;/p&gt;
&lt;center&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;top&quot; width=&quot;50%&quot;&gt;&lt;/th&gt;
&lt;th align=&quot;center&quot; valign=&quot;top&quot; width=&quot;50%&quot;&gt;&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody readability=&quot;2&quot;&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;KNOWLEDGE data with test vector&lt;/td&gt;
&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;Diabetes data with test vector&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/samples/knowledge-TD-3-X.svg&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/samples/knowledge-TD-3-X.svg&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/samples/diabetes-TD-3-X.svg&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/samples/diabetes-TD-3-X.svg&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;p&gt;The test vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; with feature names and values appears below the leaf predictor node (or to the right in left-to-right orientation). The test vector highlights the features used in one or more decision nodes. When the number of features reaches a threshold of 20 (10 for left-to-right orientation), test vectors do not show unused features to avoid unwieldly test vectors.&lt;/p&gt;
&lt;h3 id=&quot;sec:1.5.4&quot;&gt;Left-to-right orientation&lt;/h3&gt;
&lt;p&gt;Some users have a preference for left-to-right orientation instead of top-down and sometimes the nature of the tree simply flows better left-to-right. Sample feature vectors can still be run down the tree with the left-to-right orientation. Here are some examples (click on the images to enlarge):&lt;/p&gt;
&lt;center&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot; valign=&quot;top&quot; width=&quot;45%&quot;&gt;&lt;/th&gt;
&lt;th align=&quot;center&quot; valign=&quot;top&quot; width=&quot;45%&quot;&gt;&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody readability=&quot;2&quot;&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;Wine&lt;/td&gt;
&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;Diabetes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/samples/wine-LR-3.svg&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/samples/wine-LR-3.svg&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/samples/diabetes-LR-3.svg&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/samples/diabetes-LR-3.svg&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;Wine showing a prediction&lt;/td&gt;
&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;Diabetes showing a prediction&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/samples/wine-LR-2-X.svg&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/samples/wine-LR-2-X.svg&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot; valign=&quot;top&quot;&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/samples/diabetes-LR-2-X.svg&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/samples/diabetes-LR-2-X.svg&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;h3 id=&quot;sec:1.5.5&quot;&gt;Simplified non-fancy layout&lt;/h3&gt;
&lt;p&gt;To evaluate the generality of a decision tree, if it often helps to get a high-level overview of the tree. This generally means examining things like tree shape and size, but more importantly, it means looking at the leaves. We'd like to know how many samples each leaf has, how pure the target values are, and just generally where most of the weight of samples falls. Getting an overview is harder when the visualization is too large and so we provide a “non-fancy” option that generates smaller visualizations while retaining key leaf information. Here are a sample classifier and a regressor in non-fancy mode with top-down orientation:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/samples/knowledge-TD-15-X-simple.svg&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/samples/knowledge-TD-15-X-simple.svg&quot; width=&quot;55%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/samples/boston-TD-5-X-simple.svg&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/samples/boston-TD-5-X-simple.svg&quot; width=&quot;100%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;sec:1.6&quot;&gt;What we tried and rejected&lt;/h2&gt;
&lt;p&gt;Those interested in these tree visualizations from a design point of view might find it interesting to read about what we tried and rejected. Starting with classifiers, we thought that the histograms were a bit blocky and perhaps kernel density estimates would give a more accurate picture. We had decision nodes that looked like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/kde.png&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The problem is that decision nodes with only one or two samples gave extremely misleading distributions:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/kde-leaf.png&quot; width=&quot;12%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;We also experimented using bubble charts instead of histograms for classifier decision nodes:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/bubble.png&quot; width=&quot;70%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;These look really cool but, in the end, histograms are easier to read.&lt;/p&gt;
&lt;p&gt;Turning to regression trees, we considered using box plots to show the distribution of prediction values and also used a simple bar chart to show the number of samples:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/dual-leaf.png&quot; width=&quot;15%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;This dual plot for each leaf is less satisfying than the strip plot we use now. The box plot also doesn't show the distribution of target values nearly as well as a strip plot. Before the strip plot, we just laid out the target values using the sample index value as the horizontal axis:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/non-strip-plot.png&quot; width=&quot;30%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;That is misleading because the horizontal axis is usually feature space. We scrunched that into a strip plot.&lt;/p&gt;
&lt;h2 id=&quot;sec:1.7&quot;&gt;Code sample&lt;/h2&gt;
&lt;p&gt;This section gives a sample visualization for the Boston regression data set and the Wine classification data set. You can also check out the &lt;a href=&quot;https://github.com/parrt/animl/tree/master/testing/samples&quot;&gt;full gallery&lt;/a&gt; of sample visualizations and the &lt;a href=&quot;https://github.com/parrt/animl/blob/master/testing/gen_samples.py&quot;&gt;code to generate the samples&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;sec:1.7.1&quot;&gt;Boston regression tree visualization&lt;/h3&gt;
&lt;p&gt;Here is a code snippet to load the Boston data and train a regression tree with a maximum depth of three decision nodes:&lt;/p&gt;
&lt;p&gt;boston = load_boston() X_train = boston.data y_train = boston.target testX = X_train[5,:] regr = tree.DecisionTreeRegressor(max_depth=3) regr = regr.fit(X_train, y_train)&lt;/p&gt;
&lt;p&gt;The code to visualize the tree involves passing the tree model, the training data, feature and target names, and a test vector (if desired):&lt;/p&gt;
&lt;p&gt;viz = dtreeviz(regr, X_train, y_train, target_name='price', feature_names=boston.feature_names, X = testX) viz.save(&quot;boston.svg&quot;) # suffix determines the generated image format viz.view() # pop up window to display image&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/samples/boston-TD-3-X.svg&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/samples/boston-TD-3-X.svg&quot; width=&quot;45%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;sec:1.7.2&quot;&gt;Wine classification tree visualization&lt;/h3&gt;
&lt;p&gt;Here is a code snippet to load the Wine data and train a classifier tree with a maximum depth of three decision nodes:&lt;/p&gt;
&lt;p&gt;clf = tree.DecisionTreeClassifier(max_depth=3) wine = load_wine() clf.fit(wine.data, wine.target)&lt;/p&gt;
&lt;p&gt;Visualizing a classifier is the same as visualizing a regressor, except that the visualization needs the target class names:&lt;/p&gt;
&lt;p&gt;viz = dtreeviz(clf, wine.data, wine.target, target_name='wine', feature_names=wine.feature_names, class_names=list(wine.target_names)) viz.view()&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://explained.ai/decision-tree-viz/images/samples/wine-LR-3.svg&quot;&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/samples/wine-LR-3.svg&quot; width=&quot;45%&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In Jupyter notebooks, the object returned from &lt;span class=&quot;inlinecode&quot;&gt;dtreeviz()&lt;/span&gt; has a &lt;span class=&quot;inlinecode&quot;&gt;_repr_svg_()&lt;/span&gt; function that Jupyter uses to display the object automatically. See &lt;a href=&quot;https://github.com/parrt/animl/blob/master/notebooks/examples.ipynb&quot;&gt;notebook of examples&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;aside&quot; readability=&quot;15.588688946015&quot;&gt;&lt;strong&gt;BUG IN JUPYTER NOTEBOOK&lt;/strong&gt;&lt;br /&gt;Jupyter notebooks currently, as of September 2018, do not display the SVG generated by this library properly. The fonts etc... are all messed up:
&lt;p&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/bad-jupyter-svg.png&quot; width=&quot;45%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The good news is that github displays them properly as does &lt;a href=&quot;https://jupyterlab.readthedocs.io/en/stable/getting_started/overview.html&quot;&gt;JupyterLab&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Use &lt;span class=&quot;inlinecode&quot;&gt;Image(viz.topng())&lt;/span&gt; to display (poorly) in Juypter notebook or simply call &lt;span class=&quot;inlinecode&quot;&gt;viz.view()&lt;/span&gt;, which will pop up a window that displays things properly.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id=&quot;sec:1.8&quot;&gt;Our implementation&lt;/h2&gt;
&lt;p&gt;This project was very frustrating with lots of programming deadends, fiddling with parameters, working around bugs/limitations in tools and libraries, and creatively mashing up a bunch of existing tools. The only fun part was the (countless) sequence of experiments in visual design. We pushed through because it seemed likely that the machine learning community would find these visualization as useful as we will. This project represents about two months of trudging through stackoverflow, documentation, and hideous graphics programming.&lt;/p&gt;
&lt;p&gt;At the highest level, we used &lt;a href=&quot;https://matplotlib.org/&quot;&gt;matplotlib&lt;/a&gt; to generate images for decision and leaf nodes and combined them into a tree using the venerable &lt;a href=&quot;http://graphviz.org/&quot;&gt;graphviz&lt;/a&gt;. We also used HTML labels extensively in the graphviz tree description for layout and font specification purposes. The single biggest headache was convincing all components of our solution to produce high-quality vector graphics.&lt;/p&gt;
&lt;p&gt;Our initial coding experiments led us to create a shadow tree wrapping the decision trees created by scikit, so let's start with that.&lt;/p&gt;
&lt;h3 id=&quot;sec:1.8.1&quot;&gt;Shadow trees for scikit decision trees&lt;/h3&gt;
&lt;p&gt;The decision trees for classifiers and regressors from scikit-learnare built for efficiency, not necessarily ease of tree walking or extracting node information. We created &lt;a href=&quot;https://github.com/parrt/animl/blob/master/animl/trees.py&quot;&gt;animl.trees.ShadowDecTree&lt;/a&gt; and &lt;span class=&quot;inlinecode&quot;&gt;animl.trees.ShadowDecTreeNode&lt;/span&gt; classes as an easy-to-use (traditional binary tree) wrapper for all tree information. Here's how to create a shadow tree from a scikit classifier or regressor tree model:&lt;/p&gt;
&lt;p&gt;shadow_tree = ShadowDecTree(tree_model, X_train, y_train, feature_names, class_names)&lt;/p&gt;
&lt;p&gt;The shadow tree/node classes have a lot of methods that could be useful to other libraries and tools that need to walk scikit decision trees. For example, &lt;span class=&quot;inlinecode&quot;&gt;predict()&lt;/span&gt; not only runs a feature vector through the tree but also returns the path of visited nodes. The samples associated with any particular node can be had through &lt;span class=&quot;inlinecode&quot;&gt;node_samples()&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/ShadowDecTree.png&quot; width=&quot;44%&quot; /&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/ShadowDecTreeNode.png&quot; width=&quot;49%&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;sec:1.8.2&quot;&gt;Tool mashup&lt;/h3&gt;
&lt;p&gt;Graphviz dot tree layout language is very useful for getting decent tree layouts if you know all of the tricks, such as getting left children to appear to the left of right children with interconnecting hidden graph edges. For example, if you have two leaves, &lt;span class=&quot;inlinecode&quot;&gt;leaf4&lt;/span&gt; and &lt;span class=&quot;inlinecode&quot;&gt;leaf5&lt;/span&gt;, that must appear left to right on the same level, here is the graphviz magic:&lt;/p&gt;
&lt;p&gt;LSTAT3 -&amp;gt; leaf4 [penwidth=0.3 color=&quot;#444443&quot; label=&amp;lt;&amp;gt;] LSTAT3 -&amp;gt; leaf5 [penwidth=0.3 color=&quot;#444443&quot; label=&amp;lt;&amp;gt;] { rank=same; leaf4 -&amp;gt; leaf5 [style=invis] }&lt;/p&gt;
&lt;p&gt;We usually use HTML labels on graphviz nodes rather than just text labels because they give much more control over text display and provide an ability to show tabular data as actual tables. For example, when displaying a test vector run down the tree, the test vector is shown using an HTML table:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://explained.ai/decision-tree-viz/images/X.png&quot; width=&quot;25%&quot; /&gt;&lt;/p&gt;
&lt;p&gt;To generate generate images from graphviz files, we use the &lt;span class=&quot;inlinecode&quot;&gt;graphviz&lt;/span&gt; python package, which ends up &lt;span class=&quot;inlinecode&quot;&gt;exec&lt;/span&gt;ing the &lt;span class=&quot;inlinecode&quot;&gt;dot&lt;/span&gt; binary executable using one of its utility routines (&lt;span class=&quot;inlinecode&quot;&gt;run()&lt;/span&gt;). Occasionally, we used slightly different parameters on the &lt;span class=&quot;inlinecode&quot;&gt;dot&lt;/span&gt; command and so we just directly call &lt;span class=&quot;inlinecode&quot;&gt;run()&lt;/span&gt; like this for flexibility:&lt;/p&gt;
&lt;p&gt;cmd = [&quot;dot&quot;, &quot;-Tpng&quot;, &quot;-o&quot;, filename, dotfilename] stdout, stderr = run(cmd, capture_output=True, check=True, quiet=False)&lt;/p&gt;
&lt;p&gt;We also use the &lt;span class=&quot;inlinecode&quot;&gt;run()&lt;/span&gt; function to execute the &lt;span class=&quot;inlinecode&quot;&gt;pdf2svg&lt;/span&gt; (PDF to SVG conversion) tool, as described in the next section.&lt;/p&gt;
&lt;h3 id=&quot;sec:1.8.3&quot;&gt;Vector graphics via SVG&lt;/h3&gt;
&lt;p&gt;We use matplotlib to generate the decision and leaf nodes and, to get the images into a graphviz/dot image, we use HTML graphviz labels and then reference the generated images via &lt;span class=&quot;inlinecode&quot;&gt;img&lt;/span&gt; tags like this:&lt;/p&gt;
&lt;p&gt;&amp;lt;img src=&quot;/tmp/node3_94806.svg&quot;/&amp;gt;&lt;/p&gt;
&lt;p&gt;The 94806 number is the process ID, which helps isolate multiple instances of &lt;span class=&quot;inlinecode&quot;&gt;animl&lt;/span&gt; running on the same machine. Without this, it's possible for multiple processes to overwrite the same temporary files.&lt;/p&gt;
&lt;p&gt;Because we wanted scalable, vector graphics, we tried importing SVG images initially but we could not get graphviz to accept those files (pdf neither). It took us four hours to figure out that generating and importing SVG were two different things and we needed the following magic incantation on OS X using &lt;span class=&quot;inlinecode&quot;&gt;--with-librsvg&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;$ brew install graphviz --with-librsvg --with-app --with-pango&lt;/p&gt;
&lt;p&gt;Originally, when we resorted to generating PNG files from matplotlib, we set the dots per inch (dpi) to be 450 so that they looked okay on high resolution screens like the iMac. Unfortunately, that meant we had to specify the actual size we wanted for the overall tree using an HTML table in graphviz using &lt;span class=&quot;inlinecode&quot;&gt;width&lt;/span&gt; and &lt;span class=&quot;inlinecode&quot;&gt;height&lt;/span&gt; parameters on &lt;span class=&quot;inlinecode&quot;&gt;&amp;lt;td&amp;gt;&lt;/span&gt; tags. That cause a lot of trouble because we had to figure out what the aspect ratio was coming out of matplotlib. Once we moved to SVG files, we unnecessarily parsed the SVG files to get the size for use in the HTML; as we wrote this document we realized extracting the size information from SVG files was unnecessary.&lt;/p&gt;
&lt;p&gt;Unfortunately, graphviz's SVG output simply referenced the node files that we imported, rather than embedding the node images within the overall tree image. This is a very inconvenient form because sending a single tree visualization means sending a zip of files rather than a single file. We spent the time to parse SVG XML and embed all referenced images within a single large meta-SVG file. That worked great and there was much celebration.&lt;/p&gt;
&lt;p&gt;Then we noticed that graphviz does not properly handle text in HTML labels when generating SVG. For example, the text of classifier tree legends was cut off and overlapping. Rats.&lt;/p&gt;
&lt;p&gt;What finally worked to get a single clean SVG file was first generating a PDF file from graphviz and then converting the PDF to SVG with &lt;span class=&quot;inlinecode&quot;&gt;pdf2svg&lt;/span&gt; (&lt;span class=&quot;inlinecode&quot;&gt;pdf2cairo&lt;/span&gt; also appears to work).&lt;/p&gt;
&lt;p&gt;Then we noticed that Jupyter notebook has a bug where it does not display those SVG files properly (see above). Jupyter lab does handle the SVG properly as does github. We added a &lt;span class=&quot;inlinecode&quot;&gt;topng()&lt;/span&gt; method so users of Jupyter notebook can use &lt;span class=&quot;inlinecode&quot;&gt;Image(viz.topng())&lt;/span&gt; to get inline images. Better yet, call &lt;span class=&quot;inlinecode&quot;&gt;viz.view()&lt;/span&gt;, which will pop up a window that displays images properly.&lt;/p&gt;
&lt;h2 id=&quot;sec:1.9&quot;&gt;Lessons learned&lt;/h2&gt;
&lt;p&gt;Sometimes solving a programming problem is less about algorithms and more about working within the constraints and capabilities of the programming ecosystem, such as tools and libraries. That is definitely the case with this decision tree visualization software. The programming was not hard; it was more a matter of fearlessly bashing our way to victory through an appropriate mashup of graphics tools and libraries.&lt;/p&gt;
&lt;p&gt;Designing the actual visualization also required a seemingly infinite number of experiments and tweaks. Generating high quality vector-based images also required pathological determination and a trail of dead code left along the circuitous path to success.&lt;/p&gt;
&lt;p&gt;We are definitely not visualization aficionados, but for this specific problem we banged on it until we got effective diagrams. In &lt;a href=&quot;https://www.edwardtufte.com/tufte/courses&quot;&gt;Edward Tufte's seminar&lt;/a&gt; I learned that you can pack a lot of information into a rich diagram, as long as it's not an arbitrary mishmash; the human eye can resolve lots of details. We used a number of elements from the design palette to visualize decision trees: color, line thickness, line style, different kinds of plots, size (area, length, graph height, ...), color transparency (alpha), text styles (color, font, bold, italics, size), graph annotations, and visual flow. All visual elements had to be motivated. For example, we didn't use color just because colors are nice. We used color to highlight an important dimension (target category) because humans quickly and easily pick out color differences. Node size differences should also be easily picked out by humans. (is that a kitty cat or lion?), so we used that to indicate leaf size.&lt;/p&gt;
&lt;h2 id=&quot;sec:1.10&quot;&gt;Future work&lt;/h2&gt;
&lt;p&gt;The visualizations described in this document are part of the &lt;a href=&quot;https://github.com/parrt/animl&quot;&gt;animl&lt;/a&gt; machine learning library, which is just getting started. We'll likely moved the &lt;a href=&quot;https://github.com/parrt/random-forest-importances&quot;&gt;rfpimp&lt;/a&gt; permutation importance library into &lt;span class=&quot;inlinecode&quot;&gt;animl&lt;/span&gt; soon. At this point, we haven't tested the visualizations on anything but OS X. We'd welcome instructions from programmers on other platforms so that we could include those installation steps in the documentation.&lt;/p&gt;
&lt;p&gt;There are a couple of tweaks we like to do, such as bottom justifying the histograms and classifier trees so that it's easier to compare notes. Also, some of the wedge labels overlap with the axis labels. Finally, it would be interesting to see what the trees look like with incoming edge thicknesses proportional to the number of samples in that node.&lt;/p&gt;
&lt;/body&gt;</description>
<pubDate>Wed, 26 Sep 2018 19:35:36 +0000</pubDate>
<dc:creator>parrt</dc:creator>
<og:title>How to visualize decision tree</og:title>
<og:image>http://explained.ai/decision-tree-viz/images/knowledge-TD-3-X.png</og:image>
<og:description>Decision trees are the fundamental building block of gradient boosting machines and Random Forests(tm), probably the two most popular machine learning models for structured data. Visualizing decision trees is a tremendous aid when learning how these models work and when interpreting models. Unfortunately, current visualization packages are rudimentary and not immediately helpful to the novice. For example, we couldn't find a library that visualizes how decision nodes split up the feature space. So, we've created a general package (part of the animl library) for scikit-learn decision tree visualization and model interpretation.</og:description>
<og:url>http://explained.ai/decision-tree-viz/index.html</og:url>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>http://explained.ai/decision-tree-viz/index.html</dc:identifier>
</item>
<item>
<title>Stripe Is Now a $20B Company</title>
<link>https://www.bloomberg.com/news/articles/2018-09-26/payment-startup-stripe-is-now-a-20-billion-company?srnd=technology-vp</link>
<guid isPermaLink="true" >https://www.bloomberg.com/news/articles/2018-09-26/payment-startup-stripe-is-now-a-20-billion-company?srnd=technology-vp</guid>
<description>&lt;head&gt;&lt;title&gt;Terms of Service Violation&lt;/title&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;24&quot;&gt;
&lt;div class=&quot;container&quot; readability=&quot;9.4285714285714&quot;&gt;&lt;img src=&quot;https://www.bloomberg.com/graphics/assets/img/BB-Logo-2line.svg&quot; width=&quot;310&quot;/&gt;
&lt;p&gt;Your usage has been flagged as a violation of our &lt;a href=&quot;http://www.bloomberg.com/tos&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;terms of service&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For inquiries related to this message please &lt;a href=&quot;http://www.bloomberg.com/feedback&quot;&gt;contact support&lt;/a&gt;. For sales inquiries, please visit &lt;a href=&quot;http://www.bloomberg.com/professional/request-demo&quot;&gt;http://www.bloomberg.com/professional/request-demo&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;If you believe this to be in error, please confirm below that you are not a robot by clicking &quot;I'm not a robot&quot; below.&lt;/h3&gt;
&lt;br/&gt;&lt;h3&gt;Please make sure your browser supports JavaScript and cookies and that you are not blocking them from loading. For more information you can review the Terms of Service and Cookie Policy.&lt;/h3&gt;
&lt;br/&gt;&lt;h3 id=&quot;block_uuid&quot;&gt;Block reference ID:&lt;/h3&gt;
&lt;/div&gt;
&lt;/body&gt;</description>
<pubDate>Wed, 26 Sep 2018 18:04:41 +0000</pubDate>
<dc:creator>jonknee</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bloomberg.com/tosv2.html?vid=&amp;uuid=96a35dc0-c26e-11e8-9244-add989325389&amp;url=L25ld3MvYXJ0aWNsZXMvMjAxOC0wOS0yNi9wYXltZW50LXN0YXJ0dXAtc3RyaXBlLWlzLW5vdy1hLTIwLWJpbGxpb24tY29tcGFueT9zcm5kPXRlY2hub2xvZ3ktdnA=</dc:identifier>
</item>
<item>
<title>Ex-Google Employee Urges Lawmakers to Take on Company</title>
<link>https://www.nytimes.com/2018/09/26/technology/google-privacy-china-congress.html</link>
<guid isPermaLink="true" >https://www.nytimes.com/2018/09/26/technology/google-privacy-china-congress.html</guid>
<description>&lt;div readability=&quot;39.002375296912&quot;&gt;
&lt;div class=&quot;css-4w7y5l&quot; readability=&quot;24.37648456057&quot;&gt;
&lt;p class=&quot;css-xhhu0i e2kc3sl0&quot;&gt;SAN FRANCISCO — Google is facing increased scrutiny by lawmakers in Washington over its size and influence. Now, a research scientist who recently resigned from the company in protest is urging them on.&lt;/p&gt;
&lt;p class=&quot;css-xhhu0i e2kc3sl0&quot;&gt;In a &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://int.nyt.com/data/documenthelper/328-jack-poulson-dragonfly/87933ffa89dfa78d9007/optimized/full.pdf#page=1&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;harshly worded letter&lt;/a&gt; sent this week, the former employee, Jack Poulson, criticized Google’s handling of a project to build a version of its search engine that would be acceptable to the government of China. He said the project was a “catastrophic failure of the internal privacy review process.”&lt;/p&gt;
&lt;p class=&quot;css-xhhu0i e2kc3sl0&quot;&gt;He said lawmakers should increase transparency and oversight of the company and technology industry, saying that there is a “broad pattern of unaccountable decision making.”&lt;/p&gt;
&lt;p class=&quot;css-xhhu0i e2kc3sl0&quot;&gt;Dr. Poulson left the company after news articles revealed the existence of the project last month. It was first reported on by the Intercept news site.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-14jsv4e&quot;/&gt;&lt;/div&gt;&lt;div readability=&quot;43&quot;&gt;
&lt;div class=&quot;css-4w7y5l&quot; readability=&quot;31&quot;&gt;
&lt;p class=&quot;css-xhhu0i e2kc3sl0&quot;&gt;Google’s chief privacy officer, Keith Enright, testified on Wednesday before a congressional committee about the company’s approach to data protection. Executives from Apple, AT&amp;amp;T, Amazon, Twitter and Charter Communications also appeared at the hearing.&lt;/p&gt;
&lt;p class=&quot;css-xhhu0i e2kc3sl0&quot;&gt;Dr. Poulson said the Chinese project, called Dragonfly, had several “disturbing components.” A prototype, he said, would allow a partner company in China to view a person’s search history based on his or her phone number. He said the project also censored an extensive list of subjects that included information about air quality and China’s president, Xi Jinping.&lt;/p&gt;
&lt;p class=&quot;css-xhhu0i e2kc3sl0&quot;&gt;He also pointed lawmakers to commitments Google made as part of a settlement with the Federal Trade Commission in 2011. Google, among other requirements, must submit to regular privacy audits and follow a comprehensive privacy program under the settlement. The privacy program includes reviews of all Google products for privacy issues before they are released.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-14jsv4e&quot;/&gt;&lt;/div&gt;&lt;div readability=&quot;33.5&quot;&gt;
&lt;div class=&quot;css-4w7y5l&quot; readability=&quot;12&quot;&gt;
&lt;p class=&quot;css-xhhu0i e2kc3sl0&quot;&gt;Google’s privacy reviewers are assigned to analyze Google code and make sure it does not violate user privacy. But after Dragonfly became public, several reviewers said they had signed off on sections of code for Dragonfly without fully understanding the project or its privacy implications, according to two people familiar with the process. The people would speak only on the condition of anonymity to protect their relationships at the company.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-14jsv4e&quot;/&gt;&lt;/div&gt;&lt;div readability=&quot;51.939834024896&quot;&gt;
&lt;div class=&quot;css-4w7y5l&quot; readability=&quot;49.466508595139&quot;&gt;
&lt;p class=&quot;css-xhhu0i e2kc3sl0&quot;&gt;The reviewers, they said, felt that pertinent information about Dragonfly’s code had been withheld from them, and raised questions about the review process that went unanswered.&lt;/p&gt;
&lt;p class=&quot;css-xhhu0i e2kc3sl0&quot;&gt;In his testimony on Wednesday, Mr. Enright said Google was not close to releasing a search product in China.&lt;/p&gt;
&lt;p class=&quot;css-xhhu0i e2kc3sl0&quot;&gt;“If we were, in fact, to finalize a plan to launch a search product in China, my team would be actively engaged,” he said. “Our privacy and security controls would be followed, and any such project or product would follow and be consistent with our values in privacy and data protection.”&lt;/p&gt;
&lt;p class=&quot;css-xhhu0i e2kc3sl0&quot;&gt;Google on Monday released a &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://services.google.com/fh/files/blogs/google_framework_responsible_data_protection_regulation.pdf&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;framework&lt;/a&gt; for privacy legislation that describes to lawmakers how the company views its role in data protection.&lt;/p&gt;
&lt;p class=&quot;css-xhhu0i e2kc3sl0&quot;&gt;“Innovative uses of data shouldn’t be presumptively unlawful just because they are unprecedented, but organizations must account for and mitigate potential harms,” the framework says. “This includes taking particular care with sensitive information that can pose a significant risk. To enable organizations to develop effective mitigations, regulators should be clear about what constitutes a harm.”&lt;/p&gt;
&lt;p class=&quot;css-xhhu0i e2kc3sl0&quot;&gt;In a &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.blog.google/outreach-initiatives/public-policy/proposing-framework-data-protection-legislation/&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;blog post&lt;/a&gt;, Mr. Enright said the company supported comprehensive regulation on privacy. Google has also recently increased its privacy efforts, forming a team dedicated to privacy and data protection.&lt;/p&gt;
&lt;p class=&quot;css-xhhu0i e2kc3sl0&quot;&gt;Google left China in 2010, denouncing government censorship. That year the company also said it had discovered that Chinese hackers had attacked the company’s corporate infrastructure.&lt;/p&gt;
&lt;p class=&quot;css-xhhu0i e2kc3sl0&quot;&gt;“It should be pretty obvious that they should be asked what changed between 2010 and today,” said Cynthia Wong, a senior researcher at Human Rights Watch.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-14jsv4e&quot;/&gt;&lt;/div&gt;&lt;div readability=&quot;26.637583892617&quot;&gt;

&lt;div class=&quot;css-1gybuqn&quot; readability=&quot;6.7651006711409&quot;&gt;
&lt;p&gt;&lt;strong&gt;Interested in All Things Tech?&lt;/strong&gt; &lt;a href=&quot;https://www.nytimes.com/newsletters/signup/TU&quot;&gt;Get the Bits newsletter&lt;/a&gt; delivered to your inbox weekly for the latest from Silicon Valley and the technology industry.&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;</description>
<pubDate>Wed, 26 Sep 2018 17:21:07 +0000</pubDate>
<dc:creator>subdane</dc:creator>
<og:url>https://www.nytimes.com/2018/09/26/technology/google-privacy-china-congress.html</og:url>
<og:type>article</og:type>
<og:title>Ex-Google Employee Urges Lawmakers to Take On Company</og:title>
<og:image>https://static01.nyt.com/images/2018/09/27/business/27GOOGLEPRIVACY/27GOOGLEPRIVACY-facebookJumbo.jpg</og:image>
<og:description>A former research scientist at the tech giant said a project to build a censored search engine was a “catastrophic failure of the internal privacy review process.”</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nytimes.com/2018/09/26/technology/google-privacy-china-congress.html</dc:identifier>
</item>
<item>
<title>Introducing Oculus Quest, Our First 6DOF All-In-One VR System</title>
<link>https://www.oculus.com/blog/introducing-oculus-quest-our-first-6dof-all-in-one-vr-system-launching-spring-2019/</link>
<guid isPermaLink="true" >https://www.oculus.com/blog/introducing-oculus-quest-our-first-6dof-all-in-one-vr-system-launching-spring-2019/</guid>
<description>&lt;p class=&quot;_p-7 _p-9 _p-b&quot;&gt;&lt;em&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt; At 11:10 am PT, ILMxLAB Executive in Charge Vicki Dobbs Beck and Writer and Executive Producer David S. Goyer took the stage at OC5 to announce that the first episode of&lt;/em&gt; Vader Immortal: A Star Wars VR Series &lt;em&gt;will debut on Oculus Quest when it launches in Spring 2019. &lt;a href=&quot;https://www.oculus.com/blog/ilmxlab-announces-vader-immortal-a-star-wars-vr-series-launching-in-2019-on-oculus-quest/&quot; target=&quot;_blank&quot;&gt;Click here&lt;/a&gt; to learn more about this yet untold story in the&lt;/em&gt; Star Wars &lt;em&gt;universe.&lt;/em&gt;&lt;/p&gt;
&lt;p class=&quot;_p-7 _p-9 _p-b&quot;&gt;We’re excited to usher in the next era of VR gaming with the introduction of &lt;a href=&quot;https://www.oculus.com/quest/?intern_source=blog&amp;amp;intern_content=introducing-oculus-quest-our-first-6dof-all-in-one-vr-system-launching-spring-2019&quot; target=&quot;_blank&quot;&gt;Oculus Quest&lt;/a&gt;, our first all-in-one VR gaming system. Oculus Quest will launch in Spring 2019 for $399 USD. Offering six degrees of freedom and Touch controllers, Oculus Quest makes it easy to jump right into the action—with no PC, no wires, and no external sensors. We have over 50 titles lined up for launch, with even more in the works including some of your favorite Rift games like &lt;em&gt;&lt;a href=&quot;https://www.oculus.com/experiences/rift/1081190428622821/?intern_source=blog&amp;amp;intern_content=introducing-oculus-quest-our-first-6dof-all-in-one-vr-system-launching-spring-2019&quot; target=&quot;_blank&quot;&gt;Robo Recall&lt;/a&gt;&lt;/em&gt;, &lt;em&gt;&lt;a href=&quot;https://www.oculus.com/experiences/rift/866068943510454/?intern_source=blog&amp;amp;intern_content=introducing-oculus-quest-our-first-6dof-all-in-one-vr-system-launching-spring-2019&quot; target=&quot;_blank&quot;&gt;The Climb&lt;/a&gt;&lt;/em&gt;, and &lt;em&gt;&lt;a href=&quot;https://www.oculus.com/experiences/rift/1942343732456615/?intern_source=blog&amp;amp;intern_content=introducing-oculus-quest-our-first-6dof-all-in-one-vr-system-launching-spring-2019&quot; target=&quot;_blank&quot;&gt;Moss&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;


&lt;p class=&quot;_p-7 _p-9 _p-b&quot;&gt;&lt;strong&gt;Oculus Insight&lt;/strong&gt;&lt;br/&gt;We also unveiled Oculus Insight, our breakthrough technology that powers inside-out tracking, Guardian, and Touch controller tracking. This innovative system uses four ultra wide-angle sensors and computer vision algorithms to track your exact position in real time without any external sensors. Insight gives you a greater sense of immersion, presence, and mobility, plus the ability to go beyond room-scale. And we’ve brought over Guardian to help keep you safer while in VR. It’s easy to setup and experience whenever you want.&lt;/p&gt;
&lt;p class=&quot;_p-7 _p-9 _p-b&quot;&gt;&lt;strong&gt;The Best VR Games Deserve the Best VR Controllers&lt;/strong&gt;&lt;br/&gt;With the same buttons, thumbsticks, and sensors that have defined VR gaming, our intuitive Touch controllers bring your real hands into VR and let you easily and naturally interact with the world around you. By shipping Oculus Quest with Touch, everything developers have learned about game design for Rift applies to Oculus Quest. Now you can enjoy the best that VR gaming has to offer, starting at $399 USD for a 64GB headset—with the convenience and portability of all-in-one VR.&lt;/p&gt;
&lt;p class=&quot;_p-7 _p-9 _p-b&quot;&gt;&lt;strong&gt;Quality Meets Comfort&lt;/strong&gt;&lt;br/&gt;Oculus Quest includes the same best-of-class optics as Oculus Go with a display resolution of 1600x1440 per eye, while incorporating a lens spacing adjustment to help maximize visual comfort. And we’ve improved our built-in audio, so you get high-quality, immersive sound with even deeper bass.&lt;/p&gt;


&lt;p class=&quot;_p-7 _p-9 _p-b&quot;&gt;&lt;strong&gt;All in the Family&lt;/strong&gt;&lt;br/&gt;With the introduction of Oculus Quest, we’ve completed our first generation of best-in-class VR headsets. Oculus Go remains the easiest and most affordable way to get into VR, while Oculus Rift leverages the power of your PC to push the limits of what’s possible. Thanks to Oculus Quest, we’re now able to combine the best of both worlds and welcome even more people into the VR community.&lt;/p&gt;
&lt;p class=&quot;_p-7 _p-9 _p-b&quot;&gt;We can’t wait to share more with you soon. Stay up to date with Oculus Quest and other Oculus products at &lt;a href=&quot;https://www.oculus.com/quest/?intern_source=blog&amp;amp;intern_content=introducing-oculus-quest-our-first-6dof-all-in-one-vr-system-launching-spring-2019&quot; target=&quot;_blank&quot;&gt;oculus.com&lt;/a&gt;.&lt;/p&gt;
&lt;p class=&quot;_p-7 _p-9 _p-b&quot;&gt;— The Oculus Team&lt;/p&gt;
&lt;p class=&quot;_p-7 _p-9 _p-b&quot;&gt;&lt;em&gt;&lt;a href=&quot;https://outcastagency.app.box.com/v/OC5PressKit&quot; target=&quot;_blank&quot;&gt;Download the press kit.&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p class=&quot;_p-7 _p-9 _p-b&quot;&gt;&lt;em&gt;Oculus Quest has not been authorized as required by the rules of the Federal Communications Commission. Oculus Quest is not, and may not be, offered for sale or lease, or sold or leased, until authorization is obtained.&lt;/em&gt;&lt;/p&gt;
</description>
<pubDate>Wed, 26 Sep 2018 17:16:02 +0000</pubDate>
<dc:creator>MikusR</dc:creator>
<og:title>Introducing Oculus Quest, Our First 6DOF All-in-One VR System, Launching Spring 2019</og:title>
<og:description>We’re excited to usher in the next era of VR gaming with the introduction of Oculus Quest, our first all-in-one VR gaming system. Oculus Quest will launch in Spring 2019 for $399 USD. Offering six degrees of freedom and Touch controllers, Oculus Quest makes it easy to jump right into the action—with no PC, no wires, and no external sensors. We have over 50 titles lined up for launch, with even more in the works including some of your favorite Rift games like Robo Recall, The Climb, and Moss.</og:description>
<og:image>https://scontent.fphx1-1.fna.fbcdn.net/v/t39.2365-6/42452598_304204920361419_6026621272782274560_n.jpg?_nc_cat=105&amp;oh=d8ea17d8b7a1a83710fbc7637bb2db9b&amp;oe=5C5F77DE</og:image>
<og:type>article</og:type>
<og:url>https://www.oculus.com/blog/introducing-oculus-quest-our-first-6dof-all-in-one-vr-system-launching-spring-2019/</og:url>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.oculus.com/blog/introducing-oculus-quest-our-first-6dof-all-in-one-vr-system-launching-spring-2019/?_fb_noscript=1</dc:identifier>
</item>
<item>
<title>How we solved our office Wi-Fi problems</title>
<link>https://triplebyte.com/blog/how-triplebyte-solved-its-office-wifi-problems</link>
<guid isPermaLink="true" >https://triplebyte.com/blog/how-triplebyte-solved-its-office-wifi-problems</guid>
<description>&lt;p&gt;Our team just moved to a larger office in downtown San Francisco. On moving day, I was shocked to discover a bundle of rough-cut unterminated ethernet cables on one end, ripped-out punch-down jacks on the other, no uplink, and no Wi-Fi!&lt;/p&gt;
&lt;p&gt;There’s no IT team at startups, and as software engineers, we might be called on to step up in a pinch. Here’s a smorgasbord of suggestions — some well-known and others obscure — that helped me get a reliable network running fast.&lt;/p&gt;
&lt;h3&gt;Use one SSID for automatic roaming&lt;/h3&gt;
&lt;p&gt;Multiple access points should share the same SSID. They must have exactly the same security settings (same password, exact same mode, i.e. WPA2-PSK Personal) for clients to be able to automatically roam between APs.&lt;/p&gt;
&lt;p&gt;Multiple bands (2.4 GHz and 5 GHz) should also have the same SSID. Don’t put 5 GHz on its own SSID.&lt;/p&gt;
&lt;p&gt;If you use one single SSID globally, clients will choose the best AP and channel to connect to automatically. If you use separate SSIDs but they’re within overlapping coverage range, users need to connect and disconnect manually. This is a bad user experience and will often lead to laptop users remaining marginally connected to an AP they’re barely within range of.&lt;/p&gt;
&lt;p&gt;Also check out Apple’s docs on wireless roaming behavior for &lt;a href=&quot;https://support.apple.com/en-us/HT203068&quot;&gt;iOS&lt;/a&gt; and &lt;a href=&quot;https://support.apple.com/en-us/HT206207&quot;&gt;macOS&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Statically assign Wi-Fi channels&lt;/h3&gt;
&lt;p&gt;Automatic Wi-Fi channel selection is problematic because the AP can change channels whenever it likes, which causes a global disruption in service for connected clients.&lt;/p&gt;
&lt;p&gt;Statically assign different access points to different channels.&lt;/p&gt;
&lt;p&gt;Use a tool (such as the Wireless Diagnostics app built into macOS) to identify channels that aren’t in use by your next-door neighbors to avoid packet collisions and retransmits.&lt;/p&gt;
&lt;p&gt;Use non-overlapping channels. On 2.4 GHz, this means you should only choose channels 1, 6, or 11. On 5 GHz, it’s a bit more complicated, but any channel number is OK and non-overlapping if you're following my advice about 20 MHz bandwidth and avoiding DFS frequencies below.&lt;/p&gt;
&lt;h3&gt;Use narrow bandwidths (20 MHz only)&lt;/h3&gt;
&lt;p&gt;If you’re in a large office building, force your AP radios to use only 20 MHz bandwidth on both 2.4 GHz and 5 GHz bands. (If you’re in a more suburban or rural setting and have few other transmitters in radio range, you may be able to increase this.)&lt;/p&gt;
&lt;p&gt;Wider radio bandwidths of 40, 80, or 160 MHz definitely increase the maximum theoretical throughput — for example if you have only a single client and a single AP in a patch of rural farmland. But in a real-world noisy radio environment, a wider bandwidth also increases the probability of a radio packet loss, because a collision with a competing transmission on any sub-channel corrupts the entire signal — whether due to random noise or interference from another transmitter.&lt;/p&gt;
&lt;p&gt;Collisions are incredibly expensive as both transmitters have to back off and retry sending the packet again. In a noisy environment it may take a while before the wider bandwidth channel is entirely clear again, and devices may spend yet more time negotiating their channel bandwidths up and down dynamically.&lt;/p&gt;
&lt;p&gt;Another way to think about this is in terms of signal-to-noise ratio (SNR). Using a wider bandwidth spreads the same signal power over a wider slice of spectrum, so there's lower power spectral density (W/Hz), and higher total noise integrated over the wider frequency band.&lt;/p&gt;
&lt;p&gt;Optimize your user experience for minimal packet loss, rather than maximum theoretical throughput.&lt;/p&gt;
&lt;h3&gt;Avoid DFS channels&lt;/h3&gt;
&lt;p&gt;On the 5 GHz band, there are a set of extra channels which are made available using Dynamic Frequency Selection (DFS). This is a complicated process that involves clients listening for whether weather radars are using those frequencies, and changing channels if they are.&lt;/p&gt;
&lt;p&gt;The problem is that, by law, any detected interference must trigger the AP to stop transmitting immediately and go to a listen-only mode to find a new, unused channel. Obviously, this silencing behavior provides for bad user experience.&lt;/p&gt;
&lt;p&gt;To compound this problem, DFS channels may not be clearly marked on your AP’s configuration! The macOS Wireless Diagnostics tool shows the DFS status in the Info pane.&lt;/p&gt;
&lt;p&gt;In summary, just avoid using channels 52 through 144 on the 5 GHz band.&lt;/p&gt;
&lt;h3&gt;Learn to crimp and punch ethernet&lt;/h3&gt;
&lt;p&gt;I found existing ethernet cables running through the walls and floors, but they were all unterminated on both ends.&lt;/p&gt;
&lt;p&gt;For male cable ends: Buy an RJ-45 crimping tool and a bag of connectors. Cleanly cut the cable end. Trim away about 3/4” of the outer jacket without nicking the wires inside. Un-twist the twisted pairs and straighten the wires. Order them in a row in the correct order (orange-white, orange, green-white, blue, blue-white, green, brown-white, brown) with the wires facing toward the ceiling. Trim them all flush in a single cut. Slide the plug onto the wires with the latching tab facing away from you until the wires are flush with the end of the plug. Insert into the crimping tool, and crimp firmly. Double check the color ordering, that all pins are all seated puncturing down through the wire insulation, that the strain relief is on the cable jacket, and that the pins and plastic are undamaged.&lt;/p&gt;
&lt;p&gt;For female keystone jacks: Trim away the outer jacket. Un-twist the twisted pairs. Bend them at approximately a 90 degree angle to reach the appropriate teeth on the left or right sides of the jack. Use a punch-down tool (or even a small flat-head screwdriver) to push each wire down firmly until the teeth engage, puncturing through the insulation and grabbing the wire.&lt;/p&gt;
&lt;p&gt;Both of these skills take some physical practice and finesse. Don’t expect to produce working cables on your first try.&lt;/p&gt;
&lt;h3&gt;Always use EIA-586-B color coding&lt;/h3&gt;
&lt;p&gt;If you search for the specific mapping of wire colors to pins for ethernet cables, you’ll find two color-code standards: EIA-568-A and EIA-568-B. These color codes are also usually marked on female keystone jacks.&lt;/p&gt;
&lt;p&gt;Always use B. It’s the de-facto standard. Never use A unless you’re intentionally making a cross-over cable, which is effectively deprecated in today’s gigabit Auto MDI-X world, which automatically crosses over when needed within the device.&lt;/p&gt;
&lt;p&gt;Definitely don’t invent your own color ordering. It’s important for signal integrity to keep particular twisted wire pairs together.&lt;/p&gt;
&lt;h3&gt;Always test for gigabit&lt;/h3&gt;
&lt;p&gt;Ethernet ports autodetect whether to use 100 or 1000 Mbps when they’re first connected. Counting both ends of a cable, a working 100BASE-T (100 Mbps) connection requires only 8 of the 16 physical connections to be made successfully. A working 1000BASE-T (gigabit) connection requires all 16 of 16!&lt;/p&gt;
&lt;p&gt;If you’re new to making cables — or even if you’re experienced — it’s easy to have 1 not-quite-right connection every now and then. If you don’t explicitly test, you’ll end up with a seems-OK-but-actually-degraded user experience.&lt;/p&gt;
&lt;p&gt;You can check this with something like &lt;code&gt;ifconfig | grep media&lt;/code&gt;, or you can look at the LEDs on some ethernet switches.&lt;/p&gt;
&lt;p&gt;Note: if you have wireless clients connecting to an AP that seem to be maxing out internet speed tests almost exactly at the ~90-100 Mbps level, you probably have a bad ethernet connection to the AP. I'm speaking from experience on this one!&lt;/p&gt;
&lt;h3&gt;Identify power users and provide a wired option&lt;/h3&gt;
&lt;p&gt;I identified two types of internet power users at Triplebyte:&lt;/p&gt;
&lt;p&gt;On one end of the spectrum, our engineers are power users with large bulk downloads — tens-of-gigabyte downloads are typical. Throughput is more important than latency.&lt;/p&gt;
&lt;p&gt;On the other end, we have several teams with demand for low-latency interactive voice and video conferencing. Our customer success and sales teams communicate with companies hiring engineers. Our talent managers communicate with engineers going through our process. Our technical interviewers communicates with engineers signing up at the start of our process. All of these teams use software like Google Hangouts, Twilio (SIP), and Zoom. Latency and packet loss are far more important than throughput.&lt;/p&gt;
&lt;p&gt;Make wired gigabit ethernet easily available for power users. In my case, this involved wiring up every engineer’s desk plus all 11 conference and call rooms and distributing a stockpile of USB-C gigabit ethernet adapters. This empowered power users to guarantee a trouble-free network connection for themselves, and further removes high-priority traffic from the shared radio bands.&lt;/p&gt;
&lt;h3&gt;Physically protect networking equipment&lt;/h3&gt;
&lt;p&gt;Our fiber-to-ethernet transceiver, primary router, and gigabit ethernet switch are located in a cabinet that looks incredibly tempting to use for general office storage. It’s easy to imagine office supplies and extra company swag piling up on top!&lt;/p&gt;
&lt;p&gt;Post signs and notify people that this is fragile, important equipment — not a general storage area.&lt;/p&gt;
&lt;h3&gt;Use static IPs for infrastructure&lt;/h3&gt;
&lt;p&gt;Assign static IPs for infrastructure like access points. This makes them easy to reach when reconfiguration is needed, and avoids them having to pull their own addresses dynamically.&lt;/p&gt;
&lt;h3&gt;Have a large enough DHCP pool&lt;/h3&gt;
&lt;p&gt;We currently have a full-time team of 30, plus a rotating crew of remote technical interviewers who come in to our office for a few weeks of training before they go home to conduct interviews for us remotely. I expect each person to have at least two devices: laptop and phone. That's 60-80 IPs right off the bat.&lt;/p&gt;
&lt;p&gt;Many routers are configured with a fairly small IP pool allocated to DHCP out of the box. This will cause issues if the DHCP pool is exhausted. I configured our primary router to reserve 200 IPs for DHCP, leaving us about ~50 for static IP assignments. We use a 1-day DHCP lease time so unused addresses can be returned to the pool fairly quickly, while not requiring DHCP renewals during the workday.&lt;/p&gt;
&lt;p&gt;If you have multiple APs, make sure only a single device (usually your primary router which is doing NAT) is configured as a DHCP server.&lt;/p&gt;
&lt;h3&gt;Use fast DNS servers&lt;/h3&gt;
&lt;p&gt;When wired or wireless clients request an IP address via DHCP, they're also provided with DNS servers to use.&lt;/p&gt;
&lt;p&gt;I found &lt;a href=&quot;https://1.1.1.1/&quot;&gt;Cloudflare's 1.1.1.1 and 1.0.0.1&lt;/a&gt; provided the fastest query resolution at about 2ms, versus about 20ms for Google's 8.8.8.8 and 8.8.4.4, so I used these in the DHCP server configuration.&lt;/p&gt;
&lt;p&gt;Another option is to use your primary router as a caching DNS resolver, and then point all clients to it. On paper this makes a lot of sense as it can locally cache for faster performance. However, the public DNS servers are so fast that I decided to have each client use them directly, avoiding any potential local caching issues.&lt;/p&gt;
&lt;h3&gt;Check yourself on GeoIP&lt;/h3&gt;
&lt;p&gt;Our new public IP address doesn't have our San Francisco location in the &lt;a href=&quot;https://www.maxmind.com/en/locate-my-ip-address&quot;&gt;GeoIP database&lt;/a&gt; — it just says “United States.” I suspect that geographic-based DNS lookups and CDNs are giving us worse-than-optimal performance because of this. We requested a GeoIP database correction, but it hasn't been accepted yet.&lt;/p&gt;
&lt;p&gt;This issue also led to tools like &lt;a href=&quot;http://www.speedtest.net/&quot;&gt;Speedtest&lt;/a&gt; connecting to the wrong test servers many states away and severely underestimating performance.&lt;/p&gt;
&lt;h3&gt;Speed up S3 downloads with multiple parallel connections&lt;/h3&gt;
&lt;p&gt;I regularly download datasets from S3 which are single files that are gigabytes to tens of gigabytes. I found that single stream downloads from S3 are fairly slow — nowhere close to maximizing our downlink. The download can be split into multiple segments, downloaded concurrently, and reassembled, making the overall download substantially faster.&lt;/p&gt;
&lt;p&gt;For example, this downloads with 16 streams in parallel (&lt;code&gt;brew install aria2&lt;/code&gt; to install):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;aria2c -x 16 -s 16 -k 4M -o ${OUTPUT_FILENAME} ${DOWNLOAD_S3_URL}&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;Label and document everything&lt;/h3&gt;
&lt;p&gt;While the previous office tenants left me with dozens of hastily cut ethernet cables coming out from a wall panel, I'm thankful that the cables were individually numbered and labeled with permanent marker — phew!&lt;/p&gt;
&lt;p&gt;I created a shared Quip document detailing configuration settings for all APs and routers, screenshots, static IP assignments, wireless channel map, and upstream provider support contact info.&lt;/p&gt;
&lt;h3&gt;The results&lt;/h3&gt;
&lt;p&gt;For throughput: the Speedtest numbers speak for themselves. I see a symmetric 940 Mbps on wired and generally 100-150 Mbps wireless over our entire office.&lt;/p&gt;
&lt;p&gt;For reliability: we definitely had issues the first few days, but after debugging a few issues described above (DFS channels, 20 MHz radio bandwidth, and one AP on a non-gigabit ethernet cable), I'm happy to report that the network is basically trouble-free — i.e. no more complaints from the sales team!&lt;/p&gt;
&lt;p&gt;I highly recommend the &lt;a href=&quot;https://documentation.meraki.com/MR/Deployment_Guides/High_Density_Wi-Fi_Deployment_Guide_(CVD)&quot;&gt;High Density Wi-Fi Deployment Guide&lt;/a&gt; from Meraki if you're interested in reading more.&lt;/p&gt;
&lt;p&gt;If you're a software engineer interested in seeing offers from top tech companies that already have working Wi-Fi, you should &lt;a href=&quot;https://triplebyte.com/users/sign_up&quot;&gt;take the Triplebyte quiz&lt;/a&gt; — and if you care about compensation, see our &lt;a href=&quot;https://triplebyte.com/software-engineer-salary&quot;&gt;software engineer salary data&lt;/a&gt;.&lt;/p&gt;
</description>
<pubDate>Wed, 26 Sep 2018 17:01:12 +0000</pubDate>
<dc:creator>Harj</dc:creator>
<og:type>article</og:type>
<og:title>How Triplebyte solved its office Wi-Fi problems</og:title>
<og:description>Our team just moved to a larger office with unterminated ethernet cables, no uplink, and no Wi-Fi. Here’s a smorgasbord of suggestions — some well-known and others obscure — that helped me get a reliable network running fast.</og:description>
<og:url>https://triplebyte.com/blog/how-triplebyte-solved-its-office-wifi-problems</og:url>
<og:image>https://d25hn4jiqx5f7l.cloudfront.net/blog-posts/og_images/og/25_1537829008.png?1537829008</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://triplebyte.com/blog/how-triplebyte-solved-its-office-wifi-problems</dc:identifier>
</item>
</channel>
</rss>