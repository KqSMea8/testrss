<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Numpy: Plan for dropping Python 2.7 support</title>
<link>https://github.com/numpy/numpy/blob/master/doc/neps/dropping-python2.7-proposal.rst</link>
<guid isPermaLink="true" >https://github.com/numpy/numpy/blob/master/doc/neps/dropping-python2.7-proposal.rst</guid>
<description>
&lt;p&gt;The Python core team plans to stop supporting Python 2 in 2020. The NumPy project has supported both Python 2 and Python 3 in parallel since 2010, and has found that supporting Python 2 is an increasing burden on our limited resources; thus, we plan to eventually drop Python 2 support as well. Now that we're entering the final years of community-supported Python 2, the NumPy project wants to clarify our plans, with the goal of to helping our downstream ecosystem make plans and accomplish the transition with as little disruption as possible.&lt;/p&gt;
&lt;p&gt;Our current plan is as follows.&lt;/p&gt;
&lt;p&gt;Until &lt;strong&gt;December 31, 2018&lt;/strong&gt;, all NumPy releases will fully support both Python2 and Python3.&lt;/p&gt;
&lt;p&gt;Starting on &lt;strong&gt;January 1, 2019&lt;/strong&gt;, any new feature releases will support only Python3.&lt;/p&gt;
&lt;p&gt;The last Python2 supporting release will be designated as a long term support (LTS) release, meaning that we will continue to merge bug fixes and make bug fix releases for a longer period than usual. Specifically, it will be supported by the community until &lt;strong&gt;December 31, 2019&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;On &lt;strong&gt;January 1, 2020&lt;/strong&gt; we will raise a toast to Python2, and community support for the last Python2 supporting release will come to an end. However, it will continue to be available on PyPI indefinitely, and if any commercial vendors wish to extend the LTS support past this point then we are open to letting them use the LTS branch in the official NumPy repository to coordinate that.&lt;/p&gt;
&lt;p&gt;If you are a NumPy user who requires ongoing Python2 support in 2020 or later, then please contact your vendor. If you are a vendor who wishes to continue to support NumPy on Python2 in 2020+, please get in touch; ideally we'd like you to get involved in maintaining the LTS before it actually hits end of life so that we can make a clean handoff.&lt;/p&gt;
&lt;p&gt;To minimize disruption, running &lt;code&gt;pip install numpy&lt;/code&gt; on Python 2 will continue to give the last working release in perpetuity, but after January 1, 2019 it may not contain the latest features, and after January 1, 2020 it may not contain the latest bug fixes.&lt;/p&gt;
&lt;p&gt;For more information on the scientific Python ecosystem's transition to Python3 only, see the &lt;a href=&quot;http://www.python3statement.org/&quot;&gt;python3-statement&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For more information on porting your code to run on Python 3, see the &lt;a href=&quot;https://docs.python.org/3/howto/pyporting.html&quot;&gt;python3-howto&lt;/a&gt;.&lt;/p&gt;
</description>
<pubDate>Wed, 15 Nov 2017 21:23:40 +0000</pubDate>
<dc:creator>AndrewDucker</dc:creator>
<og:image>https://avatars1.githubusercontent.com/u/288276?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>numpy/numpy</og:title>
<og:url>https://github.com/numpy/numpy</og:url>
<og:description>numpy - Numpy main repository</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/numpy/numpy/blob/master/doc/neps/dropping-python2.7-proposal.rst</dc:identifier>
</item>
<item>
<title>Code together in real time with Teletype for Atom</title>
<link>https://blog.atom.io/2017/11/15/code-together-in-real-time-with-teletype-for-atom.html</link>
<guid isPermaLink="true" >https://blog.atom.io/2017/11/15/code-together-in-real-time-with-teletype-for-atom.html</guid>
<description>&lt;p&gt;&lt;img src=&quot;https://blog.atom.io/img/posts/teletype/original-teletype.jpg&quot; alt=&quot;Old-school teletype&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Writing code with another programmer is a great way to absorb knowledge, challenge yourself with new perspectives, and ultimately write better software. It can also be a fulfilling way to get to know the mind of another human being. Unfortunately, the &lt;em&gt;logistics&lt;/em&gt; of writing code with another programmer can be such a hassle that many people don’t bother. Here are some of the common obstacles:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Sharing the same physical machine is impossible for remote teams, and can be challenging to organize even when teammates share the same office.&lt;/li&gt;
&lt;li&gt;Cloud-based IDEs and remote &lt;code class=&quot;highlighter-rouge&quot;&gt;tmux&lt;/code&gt; sessions ask you to move your entire workflow into a hosted environment, which isn’t always possible or desirable.&lt;/li&gt;
&lt;li&gt;The connection latency of screen sharing can lead to an awkward dynamic where only one collaborator can comfortably edit.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Social coding shouldn’t have to be this hard! Today, we’re taking a first step toward making it just as easy to code together as it is to code alone with &lt;a href=&quot;https://teletype.atom.io&quot;&gt;Teletype for Atom&lt;/a&gt;. At the dawn of computing, teletypes were used to create a real-time circuit between two machines so that anything typed on one machine appeared at the other end immediately. Following in these electro-mechanical footsteps, Teletype for Atom wires the keystrokes of remote collaborators directly into your programming environment, enabling conflict-free, low-latency collaborative editing for any file you can open in Atom.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.atom.io/img/posts/teletype/teletype.png&quot; alt=&quot;Atom Teletype&quot;/&gt;&lt;/p&gt;
&lt;h2 id=&quot;getting-started-with-teletype&quot;&gt;Getting started with Teletype&lt;/h2&gt;
&lt;p&gt;Once you install Teletype via Atom’s settings view or &lt;code class=&quot;highlighter-rouge&quot;&gt;apm install teletype&lt;/code&gt; on the command line, you can open a “portal” into your local workspace from the new collaboration menu on the status bar.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.atom.io/img/posts/teletype/share.gif&quot; alt=&quot;Share your portal and copy its secret ID&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Then just share your portal’s secret ID with collaborators via your preferred chat service.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.atom.io/img/posts/teletype/invite.gif&quot; alt=&quot;Send the portal ID to a collaborator via a chat service&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Collaborators can enter your portal by clicking “Join” in the collaboration menu and entering the portal ID.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.atom.io/img/posts/teletype/join.gif&quot; alt=&quot;Once someone has your portal ID, they can join&quot;/&gt;&lt;/p&gt;
&lt;p&gt;After joining your portal, collaborators see a new tab in their workspace that lets them view and edit the contents of your active editor. Everyone gets their own cursor, and everyone can type at the same time, but since sharing is at the keystroke-level rather than the pixel-level, participants all keep their own custom key bindings, packages, and themes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.atom.io/img/posts/teletype/code-together.gif&quot; alt=&quot;Everyone gets a cursor&quot;/&gt;&lt;/p&gt;
&lt;p&gt;When you share a portal, your code stays on your local disk. As you switch between files, the contents of your current active editor are transmitted to collaborators so they can follow along, but otherwise your workflow will be unchanged.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.atom.io/img/posts/teletype/follow.gif&quot; alt=&quot;Guests follow the host&quot;/&gt;&lt;/p&gt;
&lt;h2 id=&quot;implementation-details&quot;&gt;Implementation details&lt;/h2&gt;
&lt;h3 id=&quot;conflict-free-editing&quot;&gt;Conflict-free editing&lt;/h3&gt;
&lt;p&gt;Collaborative editing is a tricky theoretical problem. To ensure responsive editing, each collaborator maintains their own replica of each document, applying local edits immediately before transmitting them to other collaborators. In the face of concurrency, edits end up being applied in a different order on each replica, but when the dust settles, all replicas need to have the same contents.&lt;/p&gt;
&lt;p&gt;After several late nights reading research papers, we ended up deciding to base Teletype on the theoretical framework of &lt;a href=&quot;https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type&quot;&gt;conflict-free replicated data types&lt;/a&gt;, or &lt;em&gt;CRDTs&lt;/em&gt;. Basically, CRDTs are data structures that always converge on the same representation when updated with the same set of operations, even if those operations are applied in different orders. The CRDT for text editing we wrote for the heart of this system is available as a standalone library called &lt;a href=&quot;https://github.com/atom/teletype-crdt&quot;&gt;teletype-crdt&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;connecting-peers&quot;&gt;Connecting peers&lt;/h3&gt;
&lt;p&gt;To connect collaborators, we use &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/WebRTC_API&quot;&gt;WebRTC&lt;/a&gt; &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/RTCDataChannel&quot;&gt;data channels&lt;/a&gt;. After an initial handshake that exchanges connection metadata via GitHub’s servers, all data flows over encrypted peer-to-peer connections. Our servers never see your files or edits, which maximizes your privacy and minimizes latency between you and your collaborators, regardless of your proximity to our data centers.&lt;/p&gt;
&lt;h3 id=&quot;editor-agnostic-client-library&quot;&gt;Editor-agnostic client library&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/atom/teletype&quot;&gt;Teletype Atom package&lt;/a&gt; implements UI components and Atom-specific integration code, but most of the logic lives in an editor-agnostic library called &lt;a href=&quot;https://github.com/atom/teletype-client&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;teletype-client&lt;/code&gt;&lt;/a&gt;. Our protocols and API may churn for a while as we develop the system, but it should be possible to integrate &lt;code class=&quot;highlighter-rouge&quot;&gt;teletype-client&lt;/code&gt; into any web-based application or even package it in an Electron-based server to talk to it from native editors. Supporting inter-operation between different text editors is definitely part of our long term vision.&lt;/p&gt;
&lt;h2 id=&quot;carriage-return-line-feed&quot;&gt;Carriage return, line feed&lt;/h2&gt;
&lt;p&gt;For now, Teletype only transmits text, so it’s a good idea to combine it with an application for voice communication. Traditional screen sharing can also be helpful for sharing the state of applications outside of Atom. Ultimately, we want to incorporate these kinds of features directly into the package along with a long list of other improvements.&lt;/p&gt;
&lt;p&gt;But rather than waiting for perfection, we are releasing a beta version of this package now, because we think it’s useful today. We’ve been using Teletype to build Teletype for a few months now, happily working together across two continents and three time zones. Daily teletyping has made us happier and more productive, and we hope it will do the same for you and your teammates.&lt;/p&gt;
&lt;p&gt;Visit &lt;a href=&quot;https://teletype.atom.io&quot;&gt;teletype.atom.io&lt;/a&gt; to start coding together in Atom today.&lt;/p&gt;
</description>
<pubDate>Wed, 15 Nov 2017 16:07:54 +0000</pubDate>
<dc:creator>hswolff</dc:creator>
<og:title>Code together in real time with Teletype for Atom</og:title>
<og:description>Writing code with another programmer is a great way to absorb knowledge, challenge yourself with new perspectives, and ultimately write better software. It can also be a fulfilling way to get to know the mind of another human being. Unfortunately, the logistics of writing code with another programmer can be such a hassle that many people don’t bother. Here are some of the common obstacles: Sharing the same physical machine is impossible for remote teams, and can be challenging to organize even when teammates share the same office. Cloud-based IDEs and remote tmux sessions ask you to move your entire workflow into a hosted environment, which isn’t always possible or desirable. The connection latency of screen sharing can lead to an awkward dynamic where only one collaborator can comfortably edit. Social coding shouldn’t have to be this hard! Today, we’re taking a first step toward making it just as easy to code together as it is to code alone with Teletype for Atom. At the dawn of computing, teletypes were used to create a real-time circuit between two machines so that anything typed on one machine appeared at the other end immediately. Following in these electro-mechanical footsteps, Teletype for Atom wires the keystrokes of remote collaborators directly into your programming environment, enabling conflict-free, low-latency collaborative editing for any file you can open in Atom.</og:description>
<og:url>http://blog.atom.io/2017/11/15/code-together-in-real-time-with-teletype-for-atom.html</og:url>
<og:image>http://blog.atom.io/img/posts/teletype/teletype.png</og:image>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.atom.io/2017/11/15/code-together-in-real-time-with-teletype-for-atom.html</dc:identifier>
</item>
<item>
<title>Visual Studio Live Share</title>
<link>https://code.visualstudio.com/blogs/2017/11/15/live-share</link>
<guid isPermaLink="true" >https://code.visualstudio.com/blogs/2017/11/15/live-share</guid>
<description>&lt;p&gt;November 15, 2017 Amanda Silver, &lt;a href=&quot;https://twitter.com/amandaksilver&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;@amandaksilver&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We are excited to announce that we’re working on “Visual Studio Live Share”, which enables developers using Visual Studio 2017 or Visual Studio Code to collaborate in real-time! Learn more about Live Share and the upcoming limited private preview &lt;a href=&quot;https://code.visualstudio.com/visual-studio-live-share&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;_what-is-live-share&quot; data-needslink=&quot;_what-is-live-share&quot;&gt;What is Live Share?&lt;/h2&gt;
&lt;p&gt;Live Share enables your team to quickly collaborate on the same codebase without the need to synchronize code or to configure the same development tools, settings, or environment.&lt;/p&gt;
&lt;p&gt;When it comes to Live Share, seeing is believing. Watch the following video to get an idea of what we are working on:&lt;/p&gt;

&lt;p&gt;When you share a collaborative session, your teammate sees the context of the workspace in their editor. This means your teammate can read the code you shared without having to clone a repo or install any dependencies your code relies on. They can use rich language features to navigate within the code; not only just opening other files as text but using semantic analysis-based navigation like Go to Definition or Peek.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://code.visualstudio.com/assets/blogs/2017/11/15/vs-code-ls-session.png&quot; alt=&quot;Live Sharing with VS Code&quot;/&gt;&lt;/p&gt;
&lt;p&gt;When your teammate edits a file, they get editor enhancements like IntelliSense, statement completion, and suggestions. Each of you can open files, navigate, edit code, highlight, or refactor - and changes are instantly reflected. As you edit you can see your teammate’s cursor, jump to the location of your teammate’s carat, or follow their actions.&lt;/p&gt;
&lt;p&gt;Collaborative debugging goes further, allowing you and your teammate to independently inspect objects using debugging features like hovers, locals and watches, the stack trace or the debug console. You are both able to set breakpoints and advance the debug cursor to step through the session.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://code.visualstudio.com/assets/blogs/2017/11/15/vs-code-ls-session2.png&quot; alt=&quot;Live Sharing with VS Code&quot;/&gt;&lt;/p&gt;
&lt;h2 id=&quot;_why-live-share&quot; data-needslink=&quot;_why-live-share&quot;&gt;Why Live Share?&lt;/h2&gt;
&lt;p&gt;While Microsoft Teams and Slack bring dynamic team collaboration into a digital form, there is more we can do to make it easier to work together in a development team. Screen-sharing or accessing a machine remotely means that only one person is in control while the other observes. Instant messaging, email, or other tools are great for basic messages but collaborating on a body of code often requires more than a code snippet or a single file to share the necessary context. Additionally, the validity of edits can be impacted by multiple files in the workspace. If you want to replicate your teammate’s environment, it just takes a lot of time to get everything set up.&lt;/p&gt;
&lt;p&gt;Live Share allows you to share the context of the code, so you get instant, bidirectional collaboration. Each of you can use a tool that you’ve personalized so you’re the most productive dev you can be. You can independently investigate an issue without stepping on each other, no need to hand-off control or deal with latency. Collaborate with shared focus only when you want to.&lt;/p&gt;
&lt;h2 id=&quot;_find-out-more&quot; data-needslink=&quot;_find-out-more&quot;&gt;Find out more&lt;/h2&gt;
&lt;p&gt;We are excited to give you an early glimpse into Live Share today. Although we are only sharing it as a demo today, we will be releasing a limited private preview soon. If you want to learn more and keep up with the project, you can sign up by clicking &lt;a href=&quot;https://aka.ms/vsls-signup&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Happy Coding!&lt;/p&gt;
&lt;p&gt;Amanda&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://twitter.com/amandaksilver&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;@amandaksilver&lt;/a&gt; &lt;a href=&quot;https://twitter.com/search?f=tweets&amp;amp;q=%23VSLiveShare&amp;amp;src=typd&quot;&gt;#VSLiveShare&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Wed, 15 Nov 2017 15:28:16 +0000</pubDate>
<dc:creator>benaadams</dc:creator>
<og:url>http://code.visualstudio.com/blogs/2017/11/15/live-share</og:url>
<og:type>article</og:type>
<og:title>Introducing Visual Studio Live Share</og:title>
<og:description>Real-time collaborative development</og:description>
<og:image>http://code.visualstudio.com/assets/blogs/2017/11/15/ls-social-resized.png</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://code.visualstudio.com/blogs/2017/11/15/live-share</dc:identifier>
</item>
<item>
<title>Keeping a Lab Notebook [pdf]</title>
<link>https://www.training.nih.gov/assets/Lab_Notebook_508_(new).pdf</link>
<guid isPermaLink="true" >https://www.training.nih.gov/assets/Lab_Notebook_508_(new).pdf</guid>
<description>&lt;a href=&quot;https://www.training.nih.gov/assets/Lab_Notebook_508_(new).pdf&quot;&gt;Download PDF&lt;/a&gt;</description>
<pubDate>Wed, 15 Nov 2017 15:24:27 +0000</pubDate>
<dc:creator>Tomte</dc:creator>
<og:url>http://code.visualstudio.com/blogs/2017/11/15/live-share</og:url>
<og:type>article</og:type>
<og:title>Introducing Visual Studio Live Share</og:title>
<og:description>Real-time collaborative development</og:description>
<og:image>http://code.visualstudio.com/assets/blogs/2017/11/15/ls-social-resized.png</og:image>
<dc:language>en</dc:language>
<dc:format>application/pdf</dc:format>
<dc:identifier>https://www.training.nih.gov/assets/Lab_Notebook_508_(new).pdf</dc:identifier>
</item>
<item>
<title>A Guide to Natural Language Processing</title>
<link>https://tomassetti.me/guide-natural-language-processing/</link>
<guid isPermaLink="true" >https://tomassetti.me/guide-natural-language-processing/</guid>
<description>&lt;h2&gt;What can you use Natural Language Processing for?&lt;/h2&gt;
&lt;p&gt;Natural Language Processing (NLP) comprises a set of techniques that can be used to achieve many different objectives. Take a look at the following table to figure out which technique can solve your particular problem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We are going to talk about parsing in the general sense of analyzing a document and extracting its meaning.&lt;/strong&gt; So, we are going to talk about actual parsing of natural languages, but we will spend most of the time on other techniques. When it comes to understanding programming languages parsing is the way to go, however you can pick specific alternatives for natural languages. In other words, we are mostly going to talk about what you would use instead of parsing, to accomplish your goals.&lt;/p&gt;
&lt;p&gt;For instance, if you wanted to find all for statements a programming language file, you would parse it and then count the number of for. Instead, you are probably going to use something like stemming to find all mentions of cats in a natural language document.&lt;/p&gt;
&lt;p&gt;This is necessary because &lt;a href=&quot;https://en.wikipedia.org/wiki/Formal_language&quot;&gt;the theory&lt;/a&gt; behind the parsing of natural languages might be the same one that is behind the parsing of programming languages, however the practice is very dissimilar. In fact, you are not going to build a parser for a natural language. That is unless you work in artificial intelligence or as researcher. You are even rarely going to use one. Rather you are going to find an algorithm that work a simplified model of the document that can only solve your specific problem.&lt;/p&gt;
&lt;p&gt;In short, you are going to find tricks to avoid to actually having to parse a natural language. That is why this area of computer science is usually called &lt;strong&gt;natural language processing&lt;/strong&gt; rather than natural language parsing.&lt;/p&gt;
&lt;div class=&quot;ck_form_container ck_slide_up&quot;&gt;

&lt;div class=&quot;ck_form ck_vertical_subscription_form&quot;&gt;
&lt;div class=&quot;ck_form_content&quot;&gt;
&lt;h3 class=&quot;ck_form_title&quot;&gt;The Guide to Natural Language Processing&lt;/h3&gt;
&lt;div class=&quot;ck_description&quot;&gt;&lt;span class=&quot;ck_image&quot;&gt;&lt;img alt=&quot;Guide_to_natural_language_processing&quot; src=&quot;https://i1.wp.com/s3.amazonaws.com/convertkit/subscription_forms/images/005/108/263/standard/Guide_to_Natural_Language_Processing.png?w=1500&amp;amp;ssl=1&quot; data-recalc-dims=&quot;1&quot;/&gt;&lt;/span&gt;
&lt;p&gt;Get the Guide to NLP delivered to your email and read it when you want on the device you want&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;h2&gt;Algorithms That Require Data&lt;/h2&gt;
&lt;p&gt;We are going to see specific solutions to each problem. Mind you that these specific solutions can be quite complex themselves. The more advanced they are, the less they rely on simple algorithms. Usually they need a vast database of data about the language. A logical consequence of this is that it is rarely easy to adopt a tool for one language to be used for another one. Or rather, the tool might work with few adaptations, but to build the database would require a lot of investment. So, for example, you would probably find a ready to use tool to create a summary of an English text, but maybe not one for an Italian one.&lt;/p&gt;
&lt;p&gt;For this reason, in this article we concentrate mostly on English language tools. Although we mention if these tools work for other languages. You do not need to know the theoretical differences between languages, such as the number of genders or cases they have. However, you should be aware that the more different a language is from English, the harder would be to apply these techniques or tools to it.&lt;/p&gt;
&lt;p&gt;For example, you should not expect to find tools that can work with Chinese (or rather the &lt;em&gt;Chinese writing system&lt;/em&gt;). It is not necessarily that these languages are harder to understand programmatically, but there might be less research on them or the methods might be completely different from the ones adopted for English.&lt;/p&gt;
&lt;h2&gt;The Structure of This Guide&lt;/h2&gt;
&lt;p&gt;This article is organized according to the tasks we want to accomplish. Which means that the tools and explanation are grouped according to the task they are used for. For instance, there is a section about measuring the properties of a text, such as its difficulty. They are also generally in ascending order of difficulty: it is easier to classify words than entire documents. We start with simple information retrieval techniques and we end in the proper field of natural language processing.&lt;/p&gt;
&lt;p&gt;We think it is the most useful way to provide the information you need: you need to do X, we directly show the methods and tools you can use.&lt;/p&gt;
&lt;h3&gt;Table Of Contents&lt;/h3&gt;
&lt;p&gt;The following table of contents shows the whole content of this guide.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://tomassetti.me/guide-natural-language-processing/#classWords&quot;&gt;Classifying Words&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tomassetti.me/guide-natural-language-processing/#classDocs&quot;&gt;Classifying Documents&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tomassetti.me/guide-natural-language-processing/#understandingDocs&quot;&gt;Understanding Documents&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tomassetti.me/guide-natural-language-processing/#summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;h2 id=&quot;classWords&quot;&gt;Classifying Words&lt;/h2&gt;
&lt;p&gt;With the expression classifying words, we intend to include techniques and libraries that group words together.&lt;/p&gt;
&lt;h3 id=&quot;grouping&quot;&gt;Grouping Similar Words&lt;/h3&gt;
&lt;p&gt;We are going to talk about two methods that can group together similar words, for the purpose of information retrieval. Basically, these are methods used to find the documents, with the words we care about, from a pool of all documents. That is useful because if a user search for documents containing the word &lt;code&gt;friend&lt;/code&gt; he is probably equally interested in documents containing &lt;code&gt;friends&lt;/code&gt; and possibly &lt;code&gt;friended&lt;/code&gt; and &lt;code&gt;friendship&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So, to be clear, in this section we are not going to talk about methods to group semantically connected words, such as identifying all pets or all English towns.&lt;/p&gt;
&lt;p&gt;The two methods are stemming and division of words into group of characters. The algorithms for the first ones are language dependent, while the ones for the second ones are not. We are going to examine each of them in separate paragraphs.&lt;/p&gt;
&lt;h4 id=&quot;stemming&quot;&gt;Stemming&lt;/h4&gt;
&lt;p&gt;Stemming is the process of finding the stem, or root, of a word. In this context, the stem is not necessarily the morphological root according to linguists. So, it is not the form of a word that you would find, say, in a vocabulary. For example, an algorithm may produce the stem &lt;code&gt;consol&lt;/code&gt; for the word &lt;code&gt;consoling&lt;/code&gt;. While in a vocabulary as a root you would find &lt;code&gt;console&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A typical application of stemming is grouping together all instances of words with the same stem for usage in a search library. So, if a user search for documents containing &lt;code&gt;friend&lt;/code&gt; he can also find ones with &lt;code&gt;friends&lt;/code&gt; or &lt;code&gt;friended&lt;/code&gt;.&lt;/p&gt;
&lt;h5 id=&quot;porter&quot;&gt;Porter Stemming Algorithm&lt;/h5&gt;
&lt;p&gt;Let’s talk about an algorithm that remove suffixes to find the stem: the effective and widely used Porter Stemming Algorithm. The algorithm was originally created by Martin Porter for English. There are also Porter based/inspired algorithms for other languages: such as French or Russian. You can find all of them at the website of &lt;a href=&quot;http://snowballstem.org/algorithms/&quot;&gt;Snowball&lt;/a&gt;. Snowball is a simple language to describe stemming algorithms, but the algorithms are also described in plain English.&lt;/p&gt;
&lt;p&gt;A complete description of the algorithm is beyond the scope of this guide. However, its foundation is easy to grasp. Fundamentally the algorithm divides a word in regions and then replace or remove certain suffixes, if they are completely contained in said region. So, for example, the Porter2 (i.e., the updated version) algorithm, state that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;R&lt;/em&gt;1 is the region after the first non-vowel following a vowel, or the end of the word if there is no such non-vowel&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And then that you should replace &lt;em&gt;-tional&lt;/em&gt; with &lt;em&gt;-tion&lt;/em&gt;, if it is found inside R1.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;the word &lt;code&gt;confrontational&lt;/code&gt; has as R1 region &lt;code&gt;-frontational&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-tional&lt;/code&gt; is completely contained in its R1&lt;/li&gt;
&lt;li&gt;so &lt;code&gt;confrontational&lt;/code&gt; becomes &lt;code&gt;confrontation&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The Porter Stemmer is purely algorithmic, it does not rely on an external database or computed rules (i.e., rules created according to a training database). This is a great advantage, because it makes it predictable and easy to implement. The disadvantage is that it cannot handle exceptional cases and known mistakes cannot be easily solved. For example, the algorithm creates the same stem for &lt;code&gt;university&lt;/code&gt; and &lt;code&gt;universal&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A Porter stemmer is not perfect, but it is simple, effective and easy to implement. For a language like English, a stemmer can be realized by any competent developer. So, there are many out there for all notable programming languages and we are not going to list them here.&lt;/p&gt;
&lt;h5&gt;Typical Issues with Other Languages&lt;/h5&gt;
&lt;p&gt;Most languages that are somewhat close to English, like German or even Romance languages, are generally easy to stem. Actually, the creation of the algorithm itself is complex and requires a great knowledge of the language. However, once somebody has done the hard work of creating an algorithm, implementing one is easy.&lt;/p&gt;
&lt;p&gt;In stemming there are many problems with two kinds of languages you will usually encounter. The first kind is &lt;a href=&quot;https://en.wikipedia.org/wiki/Agglutinative_language&quot;&gt;agglutinative languages&lt;/a&gt;. Setting aside the linguistic meaning of the expression, the issue is that agglutinative languages pile up prefixes and suffixes on the root of a word.&lt;/p&gt;
&lt;p&gt;In particular Turkish is problematic because is both an agglutinative language and a concatenative one. Which mean basically that in Turkish a word can represent a whole English sentence. This makes hard to develop a stemming algorithm for Turkish, but it also makes it less useful. That is because if you stem a Turkish word you might end up with one stem for each sentence, so you lose a lot of information.&lt;/p&gt;
&lt;p&gt;The second kind of issue is related to language with no clearly defined words. Chinese is the prime example as a language that has no alphabet, but only symbols that represent concepts. So, stemming has no meaning for Chinese. Even determining the boundaries of concepts is hard. The problem of dividing a text in its component words is called word segmentation. With English you can find the boundaries of words just by looking at whitespace or punctuation. There are no such things in Chinese.&lt;/p&gt;
&lt;h4 id=&quot;dividing&quot;&gt;Splitting Words&lt;/h4&gt;
&lt;p&gt;An alternative method to group together similar words relies on splitting them. The foundation of this method is taking apart words into sequence of characters. These characters are called &lt;em&gt;k-grams&lt;/em&gt;, but they are also known as &lt;em&gt;n-grams characters&lt;/em&gt; (n-grams might also indicate groups of words). The sequence of characters is built in a sliding manner, advancing by one character at each step, starting and ending with a special symbol that indicates the boundaries of the word. For example, the 3-grams for happy are:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;$ha&lt;/li&gt;
&lt;li&gt;hap&lt;/li&gt;
&lt;li&gt;app&lt;/li&gt;
&lt;li&gt;ppy&lt;/li&gt;
&lt;li&gt;py$&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;With the symbol &lt;code&gt;$&lt;/code&gt; used to indicate the beginning and the end of the word.&lt;/p&gt;
&lt;p&gt;The exact method used for search is beyond the scope of this article. In general terms: you apply the same process to the search term(s) and then compare the occurrences of the k-grams of the input with the one of the words in the documents. Usually you apply a statistical coefficient, like the &lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;Jaccard coefficient&lt;/a&gt;, to determine how much similar the words have to be to be grouped together (i.e., how many grams have to have in common). For example, with an high coefficient you might group together &lt;code&gt;cat&lt;/code&gt; and &lt;code&gt;cats&lt;/code&gt; or divide &lt;code&gt;cat&lt;/code&gt; and &lt;code&gt;catty&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It is important to note a couple of things: the order of the k-grams and spelling mistakes. The order of the k-grams does not matter, in theory you could have completely different words that happens to have the same k-grams. In practice, this does not happen. This method is imprecise, which means that it can also protect from spelling mistakes of the user. For example, even if the user input &lt;code&gt;locamotive&lt;/code&gt; instead of &lt;code&gt;locomotive&lt;/code&gt;, it will probably still show the correct results. That is because 7 of 10 3-grams matches; exact matches would rank higher, but the words locamotive does not exist and so it probably has no matches.&lt;/p&gt;
&lt;h5&gt;Limits and Effectiveness&lt;/h5&gt;
&lt;p&gt;The great advantage of this technique is that it is not just purely algorithmic and very simple, but it works with all languages. You do not need to build k-grams for English differently from the ones for French. You just take apart the words in the same way. Although it is important to note that the effectiveness is in the details: you have to pick the right number of &lt;em&gt;k&lt;/em&gt; to have the best results.&lt;/p&gt;
&lt;p&gt;The ideal number depends on the average length of the word in the language: it should be lower or equal than that. Different languages might have different values, but in general you can get away with 4 or 5. You will not have the absolute best results with only one choice, but it will work.&lt;/p&gt;
&lt;p&gt;The disadvantage is that it looks extremely stupid, let’s face it: it so simple that it should not work. But it actually does, &lt;a href=&quot;https://staff.fnwi.uva.nl/m.derijke/wp-content/papercite-data/pdf/hollink-monolingual-2004.pdf&quot;&gt;it works well if not better than stemming (PDF)&lt;/a&gt;. It is shamelessly effective, and it has many other uses. We are going to see one right now.&lt;/p&gt;
&lt;h5&gt;Generating Names&lt;/h5&gt;
&lt;p&gt;The general case of generating fake words that looks like real words is hard and of limited use. You could create phrases for a fake language, but that is pretty much it. However, it is possible to programmatically create realistic fake names for use in games or for any world building need.&lt;/p&gt;
&lt;p&gt;There are several feasible methods. The easiest works roughly like that:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;create a database of names of the same kind you want to generate (e.g., Roman names, Space Lizards Names, etc.)&lt;/li&gt;
&lt;li&gt;divide the input names in k-grams (e.g., 3-grams of Mark -&amp;gt; $ma – mar – ark – rk$)&lt;/li&gt;
&lt;li&gt;associate a probability to the k-grams: the more frequently they appear in the original database, the higher the chance they appear in the generated name)&lt;/li&gt;
&lt;li&gt;generate the new names&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;There are many variants, for instance you could combine a different number of k-grams for special purposes (e.g., all names start with a 2-gram, but end in a 4-gram).&lt;/p&gt;
&lt;p&gt;You could also improve the soundness of the generated names, simply by looking at the probabilities of the sequences appearing in a certain order. For example, if you randomly start with &lt;code&gt;ar&lt;/code&gt; the following syllable might be more likely &lt;code&gt;th&lt;/code&gt; than &lt;code&gt;or&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This method is not perfect, but it generally works good enough. You can see a few simple examples in &lt;a href=&quot;https://github.com/ftomassetti/langgen&quot;&gt;langgen&lt;/a&gt; or &lt;a href=&quot;https://github.com/Valkryst/VNameGenerator&quot;&gt;VNameGenerator&lt;/a&gt;, which shows variations of said method and a few others.&lt;/p&gt;
&lt;h2 id=&quot;classDocs&quot;&gt;Classifying Documents&lt;/h2&gt;
&lt;p&gt;In this section we include techniques and libraries that measure and identifies documents. For example, they can detect the language in which a document is written or measure how difficult it is to read it.&lt;/p&gt;
&lt;h3 id=&quot;measuring&quot;&gt;Text Metrics&lt;/h3&gt;
&lt;p&gt;There are two popular metrics of a text that can be easily implemented: reading time and difficulty of the text. These measurements are useful to inform the reader or to help the writer checking that the document respects certain criteria, such as being accessible to a young audience (i.e., low difficulty).&lt;/p&gt;
&lt;h4 id=&quot;readingTime&quot;&gt;Reading Time&lt;/h4&gt;
&lt;p&gt;The simplest way of measuring the reading time of a text is to calculate the words in the document, and then divide them by a predefined &lt;em&gt;words per minute&lt;/em&gt; (wpm) number. The words per minute figure represent the words read by an average reader in a minute. So, if a text has 1000 words and the &lt;em&gt;wpm&lt;/em&gt; is set to 200, the text would take 5 minutes to read.&lt;/p&gt;
&lt;p&gt;That is easy to understand and easy to implement. The catch is that you have to pick the correct wpm rate and that the rate varies according to each language. For example, English readers might read 230 wpm, but French readers might instead read 200 wpm. This is related to the length of the words and the natural flow of the language (i.e., a language could be more concise than another, for instance it might frequently omit subjects).&lt;/p&gt;
&lt;p&gt;The first issue is easily solved: for English most estimates put the correct wpm between 200 and 230. However, there is still the problem of dealing with different languages. This requires to have the correct data for each language and to be able to understand the language in which a text is written.&lt;/p&gt;
&lt;p&gt;To mitigate both problems you might opt to use &lt;a href=&quot;http://iovs.arvojournals.org/article.aspx?articleid=2166061&quot;&gt;a measurement of characters count&lt;/a&gt; in order to estimate the reading time. Basically, you remove the punctuation and spaces, then count the characters and divide the sum by 900-1000.&lt;/p&gt;
&lt;p&gt;On linguistic grounds the measure makes less sense, since people do not read single characters. However, the difference between languages are less evident by counting characters. For example, an &lt;a href=&quot;https://en.wikipedia.org/wiki/Agglutinative_language&quot;&gt;agglutinative language&lt;/a&gt; might have very long words, and thus fewer of them. So it ends up with a similar number of characters to a fusional language like English.&lt;/p&gt;
&lt;p&gt;This works better because the differences in speed of reading characters in each language is smaller as a percentage of the total speed. Imagine for example that the typical reader of English can read 200 wpm and 950 cpm, while the typical reader of French can read 250 wpm and 1000 cpm. The absolute difference is the same, but it is less relevant for reading characters. Of course, this is still less than ideal, but it is a simple solution.&lt;/p&gt;
&lt;p&gt;Neither of the measure consider the difficulty of the text. That texts that are difficult to read take more time to read, even with the same number of words or characters.&lt;/p&gt;
&lt;h4 id=&quot;readability&quot;&gt;Calculating the Readability of a Text&lt;/h4&gt;
&lt;p&gt;Usually the calculation of the readability of a text is linked to grades of education (i.e., years of schooling). So, an easy text might be one that can be read by 4th graders, while a harder one might need a 10th grade education. That is both a byproduct of the fact that the algorithms were created for educational purposes and because education is a useful anchor for ranking difficulty. Saying that a text is difficult in absolute terms is somewhat meaningless, saying that it is difficult for 7th-graders makes more sense.&lt;/p&gt;
&lt;p&gt;There are several formulas, but they are generally all based on the number of words and sentences in addition to either syllables or the number of difficult words. Let’s see two of the most famous: Flesch-Kincaid and Dale–Chall.&lt;/p&gt;
&lt;p&gt;None of these formulas is perfect, but both have been scientifically tested. The only caveat is that they should be used only for checking and not as a guideline. They work if you write normally. If you try to edit a text to lower the score, the results might be incorrect. For example, &lt;em&gt;if you use short words just to make a text seem easy, it looks bad&lt;/em&gt;.&lt;/p&gt;
&lt;h5&gt;Flesch-Kincaid Readability Formula&lt;/h5&gt;
&lt;p&gt;There are two variants of this formula: &lt;em&gt;Flesch reading ease&lt;/em&gt; and &lt;em&gt;Flesch–Kincaid grade level&lt;/em&gt;. They are equivalent, but one output a score (the higher it is, the easier is the text) and the other a corresponding US grade level. We are going to show the first one.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-full wp-image-3786&quot; src=&quot;https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/flesch.png?resize=800%2C60&amp;amp;ssl=1&quot; alt=&quot;Flesch Reading Ease&quot; srcset=&quot;https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/flesch.png?w=800&amp;amp;ssl=1 800w, https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/flesch.png?resize=300%2C23&amp;amp;ssl=1 300w, https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/flesch.png?resize=768%2C58&amp;amp;ssl=1 768w, https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/flesch.png?resize=705%2C53&amp;amp;ssl=1 705w, https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/flesch.png?resize=450%2C34&amp;amp;ssl=1 450w&quot; sizes=&quot;(max-width: 800px) 100vw, 800px&quot; data-recalc-dims=&quot;1&quot;/&gt;The readability is generally between 100 to 20. A result of 100 indicates a document that can be easily understood by a 11-years old student, while a result of 30 or less indicates a document suited for university graduates. You can find a more complete explanation and ranking table in &lt;a href=&quot;http://www.mang.canterbury.ac.nz/writing_guide/writing/flesch.shtml&quot;&gt;How to Write Plain English&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The different parameters can be obtained easily. Only calculating the total number of syllables requires a significant amount of work. The good news is that it is actually doable and there is a reliable algorithm for it. The bad news is that the author of TeX (Frank Liang) &lt;a href=&quot;https://www.tug.org/docs/liang/&quot;&gt;wrote his PhD thesis about his hyphenation algorithm&lt;/a&gt;. You can find an implementation and an accessible explanation of the algorithm in &lt;a href=&quot;https://github.com/mnater/Hyphenopoly&quot;&gt;Hyphenopoly.js&lt;/a&gt;. The two problems are equivalent, since you can only divide a word in two parts between two syllables.&lt;/p&gt;
&lt;p&gt;An alternative is to use a hack: instead of calculating syllables, count the vowels. This hack has been reporting to work for English, but it is not applicable to other languages. Although if you use it you lose the scientific validity of the formula and you just get a somewhat accurate number.&lt;/p&gt;
&lt;p&gt;The general structure of the formula has been applied to other languages (e.g., &lt;a href=&quot;https://it.wikipedia.org/wiki/Roberto_Vacca#Indice_di_leggibilit.C3.A0_Flesch-Vacca&quot;&gt;Flesch-Vacca for Italian&lt;/a&gt;), but each language have different coefficients.&lt;/p&gt;
&lt;h5&gt;Dale–Chall Readability Formula&lt;/h5&gt;
&lt;p&gt;This formula relies also on the number of words and sentences, but instead of syllables it uses the number of difficult words present in the text.&lt;/p&gt;
&lt;p&gt;A difficult word is defined as one that do not belong to a list of 3000 simple words, that 80% of fourth graders understand.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;size-full wp-image-3785 aligncenter&quot; src=&quot;https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/dale-chall.png?resize=800%2C60&amp;amp;ssl=1&quot; alt=&quot;Dale-Chall Readability Formula&quot; srcset=&quot;https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/dale-chall.png?w=800&amp;amp;ssl=1 800w, https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/dale-chall.png?resize=300%2C23&amp;amp;ssl=1 300w, https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/dale-chall.png?resize=768%2C58&amp;amp;ssl=1 768w, https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/dale-chall.png?resize=705%2C53&amp;amp;ssl=1 705w, https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/dale-chall.png?resize=450%2C34&amp;amp;ssl=1 450w&quot; sizes=&quot;(max-width: 800px) 100vw, 800px&quot; data-recalc-dims=&quot;1&quot;/&gt;Thus, the formula is easy to use and calculate. The only inconvenience is that you have to maintain a database of these 3000 words. We are not aware of the formula having been adapted to languages other than English.&lt;/p&gt;
&lt;p&gt;The formula generally output a score between 4 and 10. Less than 5 indicates a text suitable for 4th graders, a result of 9 or more indicates a text for college students. You can find a complete table of results at &lt;a href=&quot;http://www.readabilityformulas.com/new-dale-chall-readability-formula.php&quot;&gt;The New Dale-Chall Readability Formula&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It is natural to think that you could modify the formula, to calculate the difficulty in understanding a specialized text. That is to say you could define difficult words as words belonging to technical terminology. For example, you could calculate how difficult would be to understand a text for an average person, according to how much computer jargon it contains. Words like &lt;em&gt;parser&lt;/em&gt; or &lt;em&gt;compiler&lt;/em&gt; could be difficult, while &lt;em&gt;computer&lt;/em&gt; or &lt;em&gt;mouse&lt;/em&gt; could be considered easy. This might work: however you would have to calculate the correct coefficients yourself.&lt;/p&gt;
&lt;h3&gt;Identifying a Language&lt;/h3&gt;
&lt;p&gt;When you need to work with a natural language document the need to identifying a language comes up often. For starters, almost all the algorithms we have seen works only on a specific language and not all of them. Even overcoming this problem it is useful to be able to understand in which language is written a certain document. For instance, so that you can show or hide a document to certain users based on the language it understand. Imagine that a user search for documents containing the word &lt;code&gt;computer&lt;/code&gt;: if you simply return all documents that contain that word you will get results even in languages other than English. That is because the word has been adopted by other languages (e.g., Italian).&lt;/p&gt;
&lt;p&gt;Reliable language identification can be achieved with statistical methods. We are going to talk about two methods: a vocabulary based one and one based on frequencies of groups of letters.&lt;/p&gt;
&lt;p&gt;In theory you could also hack together ad-hoc methods based on language clues, such as the one listed in &lt;a href=&quot;https://en.wikipedia.org/wiki/Wikipedia:Language_recognition_chart&quot;&gt;Wikipedia:Language recognition chart&lt;/a&gt;. And then come up with a ranking method to order the probability of each language. The advantage of this approach is that you would not need any database. However, it has not been tested scientifically.&lt;/p&gt;
&lt;p&gt;For example, two features of Italian are: most words end with a vowel and the word &lt;code&gt;è&lt;/code&gt; (i.e., is) is quite common. So you could check for the presence and frequencies of these features in a certain document and use them to calculate the probability that said document were in Italian (e.g., at least 70% of the words ends in vowel -&amp;gt; +50% chance that the document is in Italian; the word &lt;code&gt;è&lt;/code&gt; is present in at least 10% phrases -&amp;gt; +50% chance that the document is in Italian).&lt;/p&gt;
&lt;h5&gt;Words in a Vocabulary&lt;/h5&gt;
&lt;p&gt;A basic method consists in comparing words in the text with the ones included in a vocabulary. First, you get a vocabulary of each language you care about. Then, you compare the text with the words in each vocabulary and count the number of occurrences. The vocabulary which includes the highest number of words denotes the language of the text.&lt;/p&gt;
&lt;p&gt;For example, the following phrase: &lt;code&gt;the cat was in the boudoir&lt;/code&gt;, would be identified as English because there are 5 English words (&lt;code&gt;the&lt;/code&gt;, &lt;code&gt;cat&lt;/code&gt;, &lt;code&gt;was&lt;/code&gt;, &lt;code&gt;in&lt;/code&gt;, &lt;code&gt;the&lt;/code&gt;) and 1 French word (&lt;code&gt;boudoir&lt;/code&gt;). In case you are wondering, a boudoir is essentially a female version of a man cave, but for sophisticated people.&lt;/p&gt;
&lt;p&gt;Despite its simplicity this method works well for large documents, but not so much for Twitter-like texts: texts that are short and frequently contains errors. The author of the &lt;a href=&quot;https://github.com/peterc/whatlanguage&quot;&gt;Whatlanguage&lt;/a&gt; library, which implements this method, suggest a minimum of 10 words for reliable identification.&lt;/p&gt;
&lt;h5&gt;Frequencies of Groups of Letters&lt;/h5&gt;
&lt;p&gt;The most currently used technique relies on building language models of the frequencies of groups of letters. We are again talking about k-grams, that have to be analyzed for each language. We are going to describe the procedure outlined by the original paper &lt;a href=&quot;http://odur.let.rug.nl/~vannoord/TextCat/textcat.pdf&quot;&gt;N-Gram-Based Text Categorization (PDF)&lt;/a&gt;, although an implementation might adopt a slight variation of it. Two well-known libraries implementing this method are: &lt;a href=&quot;https://github.com/wikimedia/wikimedia-textcat&quot;&gt;the PHP TextCat library by Wikimedia&lt;/a&gt; and &lt;a href=&quot;https://github.com/ivanakcheurov/ntextcat&quot;&gt;NTextCat&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First we are going to build a model for a language and then we are going to use it for scoring a document.&lt;/p&gt;
&lt;h6&gt;Building a Language Model&lt;/h6&gt;
&lt;p&gt;To create such language models, you need to have a large set of documents for each language. You divide the words in these documents to find the most used k-grams. The paper suggests calculating the number of k-grams for k from 1 to 5.&lt;/p&gt;
&lt;p&gt;Once you have the frequencies of k-grams, you order them in descending order to build a language model. For instance, a language model for English might have &lt;code&gt;th&lt;/code&gt; in first place, &lt;code&gt;ing&lt;/code&gt; in second and so on. In this final stage the actual number of occurrences it is not relevant, only their final ranking in frequency. That is to say the fact that &lt;code&gt;th&lt;/code&gt; might appear 2648 or 5895 times depends only on the size of your set of documents and it is irrelevant for the success of the method. What it is relevant is just the relative frequency in your set of documents and thus the respective ranking.&lt;/p&gt;
&lt;p&gt;Once you have build this language model you can use it to identify the language in which a document is written. You just have to apply the same procedure to build a document model of the text that you are trying to identify. So, you end up with a ranking of the frequencies of k-gram in the document.&lt;/p&gt;
&lt;p&gt;Finally, you calculate the differences between the rankings in the document and the ones for each language model you have. You could calculate this distance metric with many statistical formulas, but the paper uses a simple out-of-place method.&lt;/p&gt;
&lt;h6&gt;Scoring Languages&lt;/h6&gt;
&lt;p&gt;The method consists of comparing the position in each language model. For each position, you add a number equivalent to the differences of ranks between each language model and the document model. For example, if the language model for English put &lt;code&gt;th&lt;/code&gt; in first place, but the document model for the document that we are classifying put it in 6th place you add 5 to the English score. At the end of the process, the language with the lowest score should be the language of the text.&lt;/p&gt;
&lt;p&gt;Obviously, you do not compare all positions of k-grams up to the last one, because the lower you go the more the position becomes somewhat arbitrary. It will depend on the particular set of documents you will have chosen to build a language model. The paper uses a limit of 300 positions, but it also says that it depends on the short length of the documents the scientists have chosen. So, you would have to find the best limit for yourself.&lt;/p&gt;
&lt;p&gt;There is also the chance of wrongly classifying a text. This is more probable if a text is written in a language for which there is no model. In that case the algorithm might wrongly classify the text as one belonging to a language close to the real one. That could happen because the method find a language that has a good enough score for the document. The real language would have scored better , but it is not available, so the good enough score wins.&lt;/p&gt;
&lt;p&gt;For instance, if you have a model for Italian, but do not have one for Spanish, your software might classify a Spanish text as an Italian one because Italian is similar enough to Spanish and the closest you have to it.&lt;/p&gt;
&lt;p&gt;This problem can be mitigated by using a proper threshold. That is to say a match is found only if the input has a low score (compared to its length) for a language. Of course, now you have the problem of finding such proper threshold.&lt;/p&gt;
&lt;h6&gt;Limitations&lt;/h6&gt;
&lt;p&gt;This method works better with short texts, but it is not perfect. The suggested limit is based on the implementation and the quality of text and model. For example, NTextCat recommends a limit of 5 words. Generally speaking this method would not work reliably for Twitter messages or similar short and frequently incorrect texts. However, that would be true even for a human expert.&lt;/p&gt;
&lt;h2 id=&quot;understandingDocs&quot;&gt;Understanding Documents&lt;/h2&gt;
&lt;p&gt;This section contains more advanced libraries to understand documents. We use the term somewhat loosely: we talk about how a computer can extract or manage the content of a document beyond simple manipulation of words and characters.&lt;/p&gt;
&lt;p&gt;We are going to see how you can:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;generate summary of a document (i.e., an algorithmic answer to the question what is this article about?)&lt;/li&gt;
&lt;li&gt;sentiment analysis (i.e., does this document contain a positive or negative opinion?)&lt;/li&gt;
&lt;li&gt;parsing a document written in a natural language&lt;/li&gt;
&lt;li&gt;translate a document in another language&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;For the methods listed in the previous sections you could build a library yourself with a reasonable effort. From now on, it will get harder. That is because they might require vast amount of annotated data (e.g., a vocabulary having each word with the corresponding part of speech) or rely on complex machine learning methods. So, we will mostly suggest using libraries.&lt;/p&gt;
&lt;p&gt;This is an area with many open problems and active research, so you could find most libraries in Python, a language adopted by the research community. Though you could find the occasional research-ready library in another language.&lt;/p&gt;
&lt;p&gt;A final introductory note is that statistics and machine learning are the current kings of natural language processing. So there is probably somebody trying to use &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt; to accomplish each of these tasks (e.g., &lt;a href=&quot;https://github.com/hengluchang/deep-news-summarization&quot;&gt;deep news summarization&lt;/a&gt;). You might try that too, if you take in account a considerable amount of time for research.&lt;/p&gt;
&lt;h3 id=&quot;summaries&quot;&gt;Generation of Summaries&lt;/h3&gt;
&lt;p&gt;The creation of a summary, or a headline, to correctly represent the meaning of a document it is achievable with several methods. Some of them rely on information retrieval techniques, while others are more advanced. The theory is also divided in two strategies: extracting sentences or parts thereof from the original text, generating abstract summaries.&lt;/p&gt;
&lt;p&gt;The second strategy it is still an open area of research, so we will concentrate on the first one.&lt;/p&gt;
&lt;h4 id=&quot;sumBasic&quot;&gt;SumBasic&lt;/h4&gt;
&lt;p&gt;SumBasic is a method that relies on the probability of individual words being present in a sentence to determine the most representative sentence:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;First, you have to account the number of times a word appears in the whole document. With that you calculate the probability of each word appearing in the document. I.e., if the word appears 5 times and the document has 525 words its probability is 5/525.&lt;/li&gt;
&lt;li&gt;You calculate a weight for each sentence that is the average of the probabilities of all the words in the sentence. I.e., if a sentence contains three words with probability 3/525, 5/525 and 10/525, the weight would be 6/525.&lt;/li&gt;
&lt;li&gt;Finally you score the sentences by multiplying the highest probability word of each sentence with its weight. I.e., a sentence with a weight of 0.1 and whose best word had the probability of 0.5 would score 0.1 * 0. 5 = 0.05, while another one with weight 0.2 and a word with probability 0.4 would score 0.2 * 0.4 = 0.08.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Having found the best sentence, you recalculate the probabilities for each word in the chosen sentence. You recalculate the probabilities as if the chosen sentence was removed from the document. The idea is that the included sentence already contains a part of the whole meaning of the document. So that part become less important and this helps avoiding excessive repetition. You repeat the process until you reach the needed summary length.&lt;/p&gt;
&lt;p&gt;This technique is quite simple. It does not require to have a database of documents to build a general probability of a word appearing in any document. You just need to calculate the probabilities in each input document. However, for this to work you have to exclude what are called &lt;em&gt;stopwords&lt;/em&gt;. These are common words present in most documents, such as &lt;code&gt;the&lt;/code&gt; or &lt;code&gt;is&lt;/code&gt;. Otherwise you might include meaningless sentences that include lots of them. You could also include stemming to improve the results.&lt;/p&gt;
&lt;p&gt;It was first described in &lt;a href=&quot;http://www.cs.middlebury.edu/~mpettit/tr-2005-101.pdf&quot;&gt;The Impact of Frequency on Summarization (PDF)&lt;/a&gt;; there is an implementation available as &lt;a href=&quot;https://github.com/EthanMacdonald/SumBasic&quot;&gt;a Python library&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The approach based on frequencies is an old and popular one, because it is generally effective and simple to implement. SumBasic is good enough that is frequently used as a baseline in the literature. However, there are even simpler methods. For example, &lt;a href=&quot;https://github.com/neopunisher/Open-Text-Summarizer&quot;&gt;Open Text Summarizer&lt;/a&gt; is a 2003 library that uses an even simpler approach. Basically you count the frequency of each word, then you exclude the common English words (e.g., &lt;code&gt;the&lt;/code&gt;, &lt;code&gt;is&lt;/code&gt;) and finally you calculate the score of a sentence according to the frequencies of the word it contains.&lt;/p&gt;
&lt;h4 id=&quot;graphMethods&quot;&gt;Graph-based Methods: TextRank&lt;/h4&gt;
&lt;p&gt;There are more complex methods of calculating the relevance of the individual sentences. A couple of them take inspiration from PageRank: they are called LexRank and TextRank. They both rely on the relationship between different sentences to obtain a more sophisticate measurement of the importance of sentences, but they differ in the way they calculate similarity of sentences.&lt;/p&gt;
&lt;p&gt;PageRank measures the importance of a document according to the importance of other documents that links to it. The importance of each document, and thus each link, is computed recursively until a balance is reached.&lt;/p&gt;
&lt;p&gt;TextRank works on the same principle: the relationship between elements can be used to understand the importance of each individual element. TextRank actually uses a more complex formula than the original PageRank algorithm, because a link can be only present or not, while textual connections might be partially present. For instance, you might calculate that two sentences containing different words with the same stem (e.g., &lt;code&gt;cat&lt;/code&gt; and &lt;code&gt;cats&lt;/code&gt; both have &lt;code&gt;cat&lt;/code&gt; as their stem) are only partially related.&lt;/p&gt;
&lt;p&gt;The original paper describes a generic approach, rather than a specific method. In fact, it also describes two applications: keyword extraction and summarization. The key differences are:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;the units you choose as a foundation of the relationship&lt;/li&gt;
&lt;li&gt;the way you calculate the connection and its strength&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;For instance, you might choose as units n-grams of words or whole phrases. N-grams of words are sequences of n words, computed the same way you do k-gram for characters. So, for the phrase &lt;code&gt;dogs are better than cats&lt;/code&gt;, there are these 3-grams:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;dogs are better&lt;/li&gt;
&lt;li&gt;are better than&lt;/li&gt;
&lt;li&gt;better than cats&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Phrases might create weighted links according to how similar they are. Or they might simply create links according to the position they are (i.e., a phrase might link to the previous and following one). The method works the same.&lt;/p&gt;
&lt;h5&gt;TextRank for Sentence Extraction&lt;/h5&gt;
&lt;p&gt;TextRank for extracting phrases uses as a unit whole sentences, and as a similarity measure the number of words in common between them. So, if two phrases contain the words &lt;code&gt;tornado&lt;/code&gt;, &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;center&lt;/code&gt; they are more similar than if they contain only two common words. The similarity is normalized based on the length of the phrases. To avoid the issue of having longer phrases having higher similarity than shorter ones.&lt;/p&gt;
&lt;p&gt;The words used for the similarity measure could be stemmed. Stopwords are usually excluded by the calculation. A further improvement could be to also exclude verbs, although that might be complicated if you do not already have a way to identify the parts of speech.&lt;/p&gt;
&lt;p&gt;LexRank differs mainly because as a similarity measure it uses a standard TF-IDF (Term Frequency – Inverse Document Frequency). Basically with TF-IDF the value of individual words is first weighted according to how frequently they appear in all documents and in each specific document. For example, if you are summarizing articles for a car magazine, there will be a lot of occurrences of the word &lt;code&gt;car&lt;/code&gt; in every document. So, the word &lt;code&gt;car&lt;/code&gt; would be of little relevance for each document. However, the word &lt;code&gt;explosion&lt;/code&gt; would appear in few documents (hopefully), so it will matter more in each document it appears.&lt;/p&gt;
&lt;p&gt;The paper &lt;a href=&quot;http://web.eecs.umich.edu/%7Emihalcea/papers/mihalcea.emnlp04.pdf&quot;&gt;TextRank: Bringing Order into Texts (PDF)&lt;/a&gt; describe the approach. &lt;a href=&quot;https://github.com/jjangsangy/ExplainToMe&quot;&gt;ExplainToMe&lt;/a&gt; contains a Python implementation of TextRank.&lt;/p&gt;
&lt;h4 id=&quot;lsa&quot;&gt;Latent Semantic Analysis&lt;/h4&gt;
&lt;p&gt;The methods we have seen so far have a weakness: they do not take into account semantics. This weakness is evident when you consider that there are words that have similar meanings (i.e., synonyms) and that most words can have different meaning depending on the context (i.e., polysemy).  &lt;em&gt;Latent Semantic Analysis&lt;/em&gt; attempt to overcome these issues.&lt;/p&gt;
&lt;p&gt;The expression &lt;em&gt;Latent Semantic Analysis&lt;/em&gt; describes a technique more than a specific method. A technique that could be useful whenever you need to represent the meaning of words. It can be used for summarization, but also for search purposes, to find words like the query of the user. For instance, if the user search for &lt;code&gt;happiness&lt;/code&gt; a search library using LSA could also return results for &lt;code&gt;joy&lt;/code&gt;.&lt;/p&gt;
&lt;h5&gt;A Simple Description&lt;/h5&gt;
&lt;p&gt;The specific mathematical formulas are a bit complex and involve matrices and operations on them. However, the founding idea is quite simple: words with similar meaning will appear in similar parts of a text. So you start with a normal TF-IDF matrix. Such matrix contains nothing else than the frequencies of individual words, both inside a specific document and in all the documents evaluated.&lt;/p&gt;
&lt;p&gt;The problem is that we want to find a relation between words that do not necessarily appear together. For example, imagine that different documents contain phrases containing the words &lt;code&gt;joy&lt;/code&gt; and &lt;code&gt;happiness&lt;/code&gt; together other words &lt;code&gt;cookie&lt;/code&gt; or &lt;code&gt;chocolate&lt;/code&gt;. The words do not appear in the same sentence, but they appear in the same document. One document contains a certain number of such phrases: &lt;code&gt;a dog create happiness&lt;/code&gt; and &lt;code&gt;dogs bring joy to children&lt;/code&gt;. For this document, LSA should be able to find a connection between &lt;code&gt;joy&lt;/code&gt; and &lt;code&gt;happiness&lt;/code&gt; through their mutual connection with &lt;code&gt;dog&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The connection is build based on the frequency the words appear together or with related words in the whole set of documents. This allows to build connection even in a sentence or document where they do not appear together. So if &lt;code&gt;joy&lt;/code&gt; and &lt;code&gt;happiness&lt;/code&gt; appears frequently with &lt;code&gt;dog&lt;/code&gt;, LSA would associate the specific document with the words (&lt;code&gt;joy&lt;/code&gt;, &lt;code&gt;happiness&lt;/code&gt;) and &lt;code&gt;dog&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Basically, this technique will transform the original matrix from one linking each term with its frequency, into one with a (weighted) combination of terms linked to each document.&lt;/p&gt;
&lt;p&gt;The issue is that there are a lot of words, and combinations thereof, so you need to make a lot of calculation and simplifications. And that is where the complex math is needed.&lt;/p&gt;
&lt;p&gt;Once you have this matrix, the world is your oyster. That is to say you could use this measurement of meaning in any number of ways. For instance, you could find the most relevant phrase and then find the phrases with are most close to it, using a graph-based method.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.researchgate.net/publication/226424326_Text_Summarization_and_Singular_Value_Decomposition&quot;&gt;Text Summarization and Singular Value Decomposition&lt;/a&gt; describes one way to find the best sentences. The Python library &lt;a href=&quot;https://github.com/miso-belica/sumy&quot;&gt;sumy&lt;/a&gt; offers an implementation.&lt;/p&gt;
&lt;h4 id=&quot;otherMethods&quot;&gt;Other Methods and Libraries&lt;/h4&gt;
&lt;p&gt;The creation of summaries is a fertile area of research with many valid methods already devised. In fact, much more than the ones we have described here. They vary for the approaches and the objective they are designed for. For example, some are created specifically to provide an answer to a question of the user, others to summarize multiple documents, etc.&lt;/p&gt;
&lt;p&gt;You can read a brief taxonomy of other methods in &lt;a href=&quot;http://textmining.zcu.cz/publications/Z08.pdf&quot;&gt;Automatic Text Summarization (PDF)&lt;/a&gt;. The Python library &lt;a href=&quot;https://github.com/miso-belica/sumy&quot;&gt;sumy&lt;/a&gt;, that we have already mentioned, implements several methods, though not necessarily the ones mentioned in the paper.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://sourceforge.net/projects/classifier4j/&quot;&gt;Classifier4J&lt;/a&gt; (Java), &lt;a href=&quot;https://sourceforge.net/projects/nclassifier/&quot;&gt;NClassifier&lt;/a&gt; (C#) and &lt;a href=&quot;https://github.com/thavelick/summarize/&quot;&gt;Summarize&lt;/a&gt; (Python) implements a Bayes classifier in an algorithm described as such:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In order to summarize a document this algorithm first determines the frequencies of the words in the document. It then splits the document into a series of sentences. Then it creates a summary by including the first sentence that includes each of the most frequent words. Finally summary’s sentences are reordered to reflect that of those in the original document.&lt;/p&gt;
&lt;p&gt;– &lt;a href=&quot;https://github.com/thavelick/summarize/blob/master/summarize.py&quot;&gt;Summarize.py&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;These projects that implements a Bayes classifier are all dead, but they are useful to understand how the method could be implemented.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/DataTeaser/textteaser&quot;&gt;DataTeaser&lt;/a&gt; and &lt;a href=&quot;https://github.com/xiaoxu193/PyTeaser&quot;&gt;PyTeaser&lt;/a&gt; (both in Python, but originally DataTeaser was in Scala) use a custom approach that combines several simple measurements to create a summary of an article.&lt;/p&gt;
&lt;h4 id=&quot;otherApps&quot;&gt;Other Uses&lt;/h4&gt;
&lt;p&gt;You can apply the same techniques employed to create summaries to different tasks. That is particularly true for the more advanced and semantic based ones. Notice that the creation of only one summary for many documents is also a different task. That is because you have to take in account different documents lengths and avoid the repetitions, among other things.&lt;/p&gt;
&lt;p&gt;A natural application is the identification of similar documents. If you can devise a method to identify the most meaningful sentences of one document, you can also compare the meaning of two documents.&lt;/p&gt;
&lt;p&gt;Another objective with common techniques is information retrieval. In short, if a user search for one word, say &lt;code&gt;car&lt;/code&gt;, you could use some of these techniques to find documents containing also &lt;code&gt;automobile&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, there is &lt;em&gt;topic modelling&lt;/em&gt;, which consists in finding the topics of a collection of documents. In simple terms, it means grouping together words with similar themes. It uses more complex statistical methods that the one used for the creation of summaries. The current state of art is based upon a method called Latent Dirichlet allocation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://radimrehurek.com/gensim/index.html&quot;&gt;Gensim&lt;/a&gt; is a very popular and production-ready library, that have many such applications. Naturally is written in Python.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mallet.cs.umass.edu/&quot;&gt;Mallet&lt;/a&gt; is a Java library mainly designed for topic modelling.&lt;/p&gt;
&lt;h3 id=&quot;parsing&quot;&gt;Parsing Documents&lt;/h3&gt;
&lt;p&gt;Most computer languages are easy to parse. This is not true for natural languages. There are approaches that give good results, but ultimately this is still an open area of research. Fundamentally the issue is that the parsing a sentence (i.e., analyzing its syntax) and its meaning are interconnected in a natural language. A subject or a verb, a noun or an adverb are all words and most words that can be subject can also be object.&lt;/p&gt;
&lt;p&gt;In practical terms, this means that there are no ready to use libraries that are good for every use you can think of. We present some libraries that can be used for restricted tasks, such as recognizing parts of speech, that can also be of use for improving other methods, like the ones for creation of summaries.&lt;/p&gt;
&lt;p&gt;There is also the frustrating fact is that a lot of software is made by academic researchers. Which means that it could be easily abandoned for another approach or lacking documentation. You cannot really use a work-in-progress, badly maintained software for anything productive. Especially if you care about a language other than English, you might find yourself seeing a good working demo, which was written ten years ago, by somebody with no contact information and without any open source code available.&lt;/p&gt;
&lt;h4 id=&quot;needData&quot;&gt;You Need Data&lt;/h4&gt;
&lt;p&gt;To achieve any kind of result with parsing, or generally extracting information from a natural language document, you need a lot of data to train the algorithms. This group of data is called a &lt;em&gt;corpus&lt;/em&gt;. For use in a system that uses statistical or machine learning techniques, you might just need a lot of real world data possibly divided in the proper groups (e.g., Wikipedia articles divided by category).&lt;/p&gt;
&lt;p&gt;However, if you are using a smart system, you might need this corpus of data to be manually constructed or annotated (e.g., the word &lt;code&gt;dog&lt;/code&gt; is a noun that has these X possible meanings). A smart system is one that tries to imitate human understanding, or at a least that uses a process that can be followed by humans. For instance, a parser that relies on a grammar which uses rules such as &lt;code&gt;Phrase → Subject Verb&lt;/code&gt; (read: a phrase is made of a subject and a verb), but also defines several classes of verbs that humans would not normally use (e.g., verbs related to motion).&lt;/p&gt;
&lt;p&gt;In these cases, the corpus often uses a custom format and is built for specific needs. For example, this &lt;a href=&quot;http://www.cs.utexas.edu/users/ml/geo.html&quot;&gt;system that can answer geographical questions about United States&lt;/a&gt; uses information stored in a Prolog format. The natural consequence is that even what is generally available information, such as dictionary data, can be incompatible between different programs.&lt;/p&gt;
&lt;p&gt;On the other hand, there are also good databases that are so valuable that many programs are built around them. &lt;a href=&quot;https://en.wikipedia.org/wiki/WordNet&quot;&gt;WordNet&lt;/a&gt; is an example of such database. It is a lexical database that links groups of words with similar meaning (i.e., synonyms) with their associated definition. It works thus as both a dictionary and a thesaurus. The original version is for English, but it has inspired similar databases for other languages.&lt;/p&gt;
&lt;h4 id=&quot;things&quot;&gt;What You Can Do&lt;/h4&gt;
&lt;p&gt;We have presented some of the practical challenges to build your own library to understand text. And we have not even mentioned all the issues related to ambiguity of human languages. So differently from what we did for past sections we are just going to explain what you can do. We are not going to explain the algorithms used to realized them, both because there is no space and also without the necessary data they would be worthless. Instead in the next paragraph we are just going to introduce the most used libraries that you can use to achieve what you need.&lt;/p&gt;
&lt;h5&gt;Named-entity Recognition&lt;/h5&gt;
&lt;p&gt;Named-entity recognition basically means finding the entities mentioned in the document. For example, in the phrase &lt;code&gt;John Smith is going to Italy&lt;/code&gt;, it should identify &lt;code&gt;John Smith&lt;/code&gt; and &lt;code&gt;Italy&lt;/code&gt; as entities. It should also be able to correctly keep track of them in different documents.&lt;/p&gt;
&lt;h5&gt;Sentiment Analysis&lt;/h5&gt;
&lt;p&gt;Sentiment analysis classifies the sentiment represented by a phrase. In the most basic terms, it means understanding if a phrase indicates a positive or negative statement. A naive Bayes classifier can suffice for this level of understanding. It works in a similar way a spam filter works: it divides the messages into two categories (i.e., spam and non-spam) relying on the probability of each word being present in any of the two categories.&lt;/p&gt;
&lt;p&gt;An alternative is to manually associate an emotional ranking to a word. For example, a value between -10/-5 and 0 for &lt;code&gt;catastrophic&lt;/code&gt; and one between 0 and 5/10 for &lt;code&gt;nice&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you need a subtler evaluation you need to resort to machine learning techniques.&lt;/p&gt;
&lt;h5&gt;Parts of Speech Tagging&lt;/h5&gt;
&lt;p&gt;Parts of Speech Tagging (usually abbreviated as POS-tagging) indicates the identification and labelling of the different parts of speech (e.g., what is a noun, verb, adjective, etc.). While is an integral part of parsing, it can also be used to simplify other tasks. For instance, it can be used in the creation of summaries to simplify the sentences chosen for the summary (e.g., removing subordinates’ clauses).&lt;/p&gt;
&lt;h5&gt;Lemmatizer&lt;/h5&gt;
&lt;p&gt;A lemmatizer return the lemma for a given word and a part of speech tag. Basically, it gives the corresponding dictionary form of a word. In some ways it can be considered an advanced form of a stemmer. It can also be used for similar purposes, namely it can ensure that all different forms of a word are correctly linked to the same concepts.&lt;/p&gt;
&lt;p&gt;For instance, it can transform all instances of &lt;code&gt;cats&lt;/code&gt; in &lt;code&gt;cat&lt;/code&gt;, for search purposes. However, it can also distinguish between the cases of &lt;code&gt;run&lt;/code&gt; as in the verb &lt;code&gt;to run&lt;/code&gt; and &lt;code&gt;run&lt;/code&gt; as in the noun synonym of a &lt;code&gt;jog&lt;/code&gt;.&lt;/p&gt;
&lt;h5&gt;Chunking&lt;/h5&gt;
&lt;p&gt;Parts of speech tagging can be considered equivalent to lexing in natural languages. Chunking, also known as shallow parsing, is a step above parts of speech tagging, but one below the final parsing. It connects parts of speech in higher units of meaning, for example complements. Imagine the phrase &lt;code&gt;John always wins our matches of Russian roulette&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;a POS-tagger identifies that Russian is an adjective and roulette a noun&lt;/li&gt;
&lt;li&gt;a chunker groups together &lt;code&gt;(of) Russian roulette&lt;/code&gt; as a complement or two related parts of speech&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The chunker might work to produce units that are going to be used by a parser. It can also work independently, for example to help in named-entity recognition.&lt;/p&gt;
&lt;h5&gt;Parsing&lt;/h5&gt;
&lt;p&gt;The end result is the same as for computer languages: &lt;a href=&quot;https://tomassetti.me/guide-parsing-algorithms-terminology/#parsingTree&quot;&gt;a parse tree&lt;/a&gt;. Though the process is quite different, and it might start with a probabilistic grammar or even with no grammar at all. It also usually continues with a lot of probabilities and statistical methods.&lt;/p&gt;
&lt;p&gt;The following is a parse tree created by the Stanford Parser (we are going to see it later) for the phrase &lt;code&gt;My dog likes hunting cats and people&lt;/code&gt;. Groups of letters such as NP indicates parts of speech or complements.&lt;/p&gt;
&lt;div id=&quot;crayon-5a0cd4de3d095638979029&quot; class=&quot;crayon-syntax crayon-theme-son-of-obsidian crayon-font-monaco crayon-os-pc print-yes notranslate&quot; data-settings=&quot; minimize scroll-mouseover wrap&quot;&gt;
&lt;div class=&quot;crayon-plain-wrap&quot;&gt;
&lt;textarea class=&quot;crayon-plain print-no&quot; data-settings=&quot;dblclick&quot; readonly=&quot;readonly&quot;&gt;
(ROOT
  (S
    (NP (PRP$ My) (NN dog))
    (VP (VBZ likes)
      (NP (NN hunting) (NNS cats)
        (CC and)
        (NNS people)))))
&lt;/textarea&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-main&quot;&gt;
&lt;table class=&quot;crayon-table&quot;&gt;&lt;tr class=&quot;crayon-row&quot;&gt;&lt;td class=&quot;crayon-nums&quot; data-settings=&quot;show&quot;&gt;

&lt;/td&gt;
&lt;td class=&quot;crayon-code&quot;&gt;
&lt;div class=&quot;crayon-pre&quot;&gt;


&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5a0cd4de3d095638979029-3&quot;&gt;    (NP (PRP$ My) (NN dog))&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5a0cd4de3d095638979029-4&quot;&gt;    (VP (VBZ likes)&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5a0cd4de3d095638979029-5&quot;&gt;      (NP (NN hunting) (NNS cats)&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5a0cd4de3d095638979029-6&quot;&gt;        (CC and)&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5a0cd4de3d095638979029-7&quot;&gt;        (NNS people)))))&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h5&gt;Translation&lt;/h5&gt;
&lt;p&gt;The current best methods for automatic machine translation rely on machine learning. The good news is that this means you just need a great number of documents in the languages you care about, without any annotation. Typical sources of such texts are Wikipedia and the official documentation of the European Union (which requires documents to be translated in all the official languages of the Union).&lt;/p&gt;
&lt;p&gt;As anybody that have tried Google Translate or Bing Translator can attest, the results are generally good enough for understanding, but still often a bit off. They cannot substitute a human translator.&lt;/p&gt;
&lt;h4 id=&quot;libraries&quot;&gt;The Best Libraries Available&lt;/h4&gt;
&lt;p&gt;The following libraries can be used for multiple purposes, so we are going to divide this section by the title of the libraries. Most of them are in Python or Java.&lt;/p&gt;
&lt;h5&gt;Apache OpenNLP&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;The &lt;a href=&quot;http://opennlp.apache.org/&quot;&gt;Apache OpenNLP&lt;/a&gt; library is a machine learning based toolkit for the processing of natural language text. It supports the most common NLP tasks, such as tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, and coreference resolution. These tasks are usually required to build more advanced text processing services. OpenNLP also included maximum entropy and perceptron based machine learning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Apache OpenNLP is a Java library with an &lt;a href=&quot;http://opennlp.apache.org/docs/1.8.1/manual/opennlp.html&quot;&gt;excellent documentation&lt;/a&gt; that can fulfill most of the tasks we have just discussed, except for sentiment analysis and translation. The developers provide &lt;a href=&quot;http://opennlp.sourceforge.net/models-1.5/&quot;&gt;language models for a few languages in addition to English&lt;/a&gt;, the most notable are German, Spanish and Portuguese.&lt;/p&gt;
&lt;h5&gt;The Classical Language Toolkit&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;The &lt;a href=&quot;http://cltk.org/&quot;&gt;Classical Language Toolkit (CLTK)&lt;/a&gt; offers natural language processing (NLP) support for the languages of Ancient, Classical, and Medieval Eurasia. Greek and Latin functionality are currently most complete.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As the name implies the major feature of the Classical Language Toolkit is the support for classical (ancient) languages, such as Greek and Latin. It has basic NLP tools, such as a lemmatizer, but also indispensable tools to work with ancient languages, such as transliteration support, and peculiar things like &lt;a href=&quot;http://docs.cltk.org/en/latest/latin.html#clausulae-analysis&quot;&gt;Clausulae Analysis&lt;/a&gt;. It has a good documentation and it is your only choice for ancient languages.&lt;/p&gt;
&lt;h5&gt;FreeLing&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://nlp.lsi.upc.edu/freeling/&quot;&gt;FreeLing&lt;/a&gt; is a C++ library providing language analysis functionalities (morphological analysis, named entity detection, PoS-tagging, parsing, Word Sense Disambiguation, Semantic Role Labelling, etc.) for a variety of languages (English, Spanish, Portuguese, Italian, French, German, Russian, Catalan, Galician, Croatian, Slovene, among others).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is a library with a good documentation and even a &lt;a href=&quot;http://nlp.lsi.upc.edu/freeling/demo/demo.php&quot;&gt;demo&lt;/a&gt;. It supports many languages usually excluded by other tools, but it is released the Affero GPL which is probably the least user-friendly license ever conceived.&lt;/p&gt;
&lt;h5&gt;Moses&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://www.statmt.org/moses/index.php?n=Main.HomePage&quot;&gt;Moses&lt;/a&gt; is a statistical machine translation system that allows you to automatically train translation models for any language pair. All you need is a collection of translated texts (parallel corpus). Once you have a trained model, an efficient search algorithm quickly finds the highest probability translation among the exponential number of choices.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The only thing to add is that the system is written in C++ and there is ample documentation.&lt;/p&gt;
&lt;h5&gt;NLTK&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Natural Language Toolkit (NLTK) is probably the most known NLP library for Python. The library can accomplish many tasks in different ways (i.e., using different algorithms). It even has a good documentation (if you include the freely available book).&lt;/p&gt;
&lt;p&gt;Simply put: it is the standard library for NLP research. Though one issue that some people have is exactly that: it is designed for research and educational purposes. If there are ten ways to do something NLTK would allow you to choose among them all. The intended user is a person with a deep understanding of NLP&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://textblob.readthedocs.io/en/dev/index.html&quot;&gt;TextBlob&lt;/a&gt; is a library that builds upon NLTK (and Pattern) to simplify processing of textual data. The library also provides translation, but it does not implement it directly: it is simply an interface for Google Translate.&lt;/p&gt;
&lt;h5&gt;Pattern&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/clips/pattern&quot;&gt;Pattern&lt;/a&gt; is the most peculiar software in our collection because it is a collection of Python libraries for web mining. It has support for data mining from services such as Google and Twitter (i.e., it provide functions to directly search from Google/Twitter) , an HTML parser and many other things. Among these things there is natural language processing for English and a few other languages, including German, Spanish, French and Italian.&lt;/p&gt;
&lt;p&gt;Though English support is more advanced than the rest.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The pattern.en module contains a fast part-of-speech tagger for English (identifies nouns, adjectives, verbs, etc. in a sentence), sentiment analysis, tools for English verb conjugation and noun singularization &amp;amp; pluralization, and a WordNet interface.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The rest of the libraries can only support POS-tagging.&lt;/p&gt;
&lt;h5&gt;Polyglot&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;http://polyglot-nlp.com/&quot;&gt;Polyglot&lt;/a&gt; is a set of NLP libraries for many natural languages in Python. It looks great, although it has only little documentation.&lt;/p&gt;
&lt;p&gt;It supports fewer languages for the more advanced tasks, such as POS tagging (16) or named entity recognition (40). However, for sentiment analysis and language identification can work with more than a hundred of them.&lt;/p&gt;
&lt;h5&gt;Sentiment and Sentiment&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/thisandagain/sentiment&quot;&gt;Sentiment&lt;/a&gt; is JavaScript (Node.js) library for sentiment analysis. The library relies on AFINN (a collection of English words with an associated emotional value) and a similar database for Emoji. These database associate to each word/Emoji a positive or negative value, to indicate a positive or negative sentiment. For example, the word &lt;code&gt;joy&lt;/code&gt; has a score of &lt;code&gt;3&lt;/code&gt;, while &lt;code&gt;sad&lt;/code&gt; has &lt;code&gt;-2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The code for the library itself is quite trivial, but it works, and it is easy to use.&lt;/p&gt;
&lt;div id=&quot;crayon-5a0cd4de3d0a9068091606&quot; class=&quot;crayon-syntax crayon-theme-son-of-obsidian crayon-font-monaco crayon-os-pc print-yes notranslate&quot; data-settings=&quot; minimize scroll-mouseover wrap&quot;&gt;
&lt;div class=&quot;crayon-plain-wrap&quot;&gt;
&lt;textarea class=&quot;crayon-plain print-no&quot; data-settings=&quot;dblclick&quot; readonly=&quot;readonly&quot;&gt;
var sentiment = require('sentiment');

var r1 = sentiment('Cats are stupid.');
console.dir(r1);        // Score: -2, Comparative: -0.666

var r2 = sentiment('Cats are totally amazing!');
console.dir(r2);        // Score: 4, Comparative: 1
&lt;/textarea&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-main&quot;&gt;
&lt;table class=&quot;crayon-table&quot;&gt;&lt;tr class=&quot;crayon-row&quot;&gt;&lt;td class=&quot;crayon-nums&quot; data-settings=&quot;show&quot;&gt;

&lt;/td&gt;
&lt;td class=&quot;crayon-code&quot;&gt;
&lt;div class=&quot;crayon-pre&quot;&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5a0cd4de3d0a9068091606-1&quot;&gt;&lt;span class=&quot;crayon-t&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;crayon-v&quot;&gt;sentiment&lt;/span&gt; &lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;crayon-e&quot;&gt;require&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;'sentiment'&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;;&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5a0cd4de3d0a9068091606-3&quot;&gt;&lt;span class=&quot;crayon-t&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;crayon-v&quot;&gt;r1&lt;/span&gt; &lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;crayon-e&quot;&gt;sentiment&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;'Cats are stupid.'&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5a0cd4de3d0a9068091606-4&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;dir&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;r1&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;// Score: -2, Comparative: -0.666&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5a0cd4de3d0a9068091606-6&quot;&gt;&lt;span class=&quot;crayon-t&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;crayon-v&quot;&gt;r2&lt;/span&gt; &lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;crayon-e&quot;&gt;sentiment&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;'Cats are totally amazing!'&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5a0cd4de3d0a9068091606-7&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;dir&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;r2&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;// Score: 4, Comparative: 1&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Sentiment analysis using machine learning techniques.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That is the extent of the documentation for the Python library &lt;a href=&quot;https://github.com/vivekn/sentiment&quot;&gt;sentiment&lt;/a&gt;. Although there is also a &lt;a href=&quot;https://arxiv.org/abs/1305.6143&quot;&gt;paper&lt;/a&gt; and a &lt;a href=&quot;http://sentiment.vivekn.com/&quot;&gt;demo&lt;/a&gt;. The paper mentions that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We have explored different methods of improving the accuracy of a Naive Bayes classifier for sentiment analysis. We observed that a combination of methods like negation handling, word n-grams and feature selection by mutual information results in a significant improvement in accuracy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Which means that it can be a good starting point to understand how to build your own sentiment analysis library.&lt;/p&gt;
&lt;h5&gt;SpaCy&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;Industrial-Strength Natural Language Processing in Python&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The library &lt;a href=&quot;https://spacy.io/&quot;&gt;spaCy&lt;/a&gt; claims to be a much more efficient, ready for the real world and easy to use library than NLTK. In practical terms it has two advantages over NLTK:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;better performance&lt;/li&gt;
&lt;li&gt;it does not give you the chance of choosing among the many algorithms the one you think is best, instead it chooses the best one for each task. While less choices might seem bad, it can actually be a good thing. That is if you have no idea what the algorithms do and you have to learn them before making a decision.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;In practical terms it is a library that supports most of the basic tasks we mentioned (i.e., things like named entity recognition and POS-tagging, but not translation or parsing) with a great code-first documentation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/chartbeat-labs/textacy&quot;&gt;Textacy&lt;/a&gt; is a library built on top of spaCY for higher-level NLP tasks. Basically, it simplifies some things including features for cleaning data or managing it better.&lt;/p&gt;
&lt;h5&gt;The Stanford Natural Language Processing Group Software&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;The Stanford NLP Group makes some of our Natural Language Processing software available to everyone! We provide statistical NLP, deep learning NLP, and rule-based NLP tools for major computational linguistics problems, which can be incorporated into applications with human language technology needs. These packages are widely used in industry, academia, and government.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The Stanford NLP group creates and support many great tools that cover all the purposes we have just mentioned. The only thing missing is sentiment analysis. The most notable software are &lt;a href=&quot;https://stanfordnlp.github.io/CoreNLP/&quot;&gt;CoreNLP&lt;/a&gt; and &lt;a href=&quot;https://nlp.stanford.edu/software/lex-parser.html&quot;&gt;Parser&lt;/a&gt;. The parser can be seen in action in a &lt;a href=&quot;http://nlp.stanford.edu:8080/parser/index.jsp&quot;&gt;web demo&lt;/a&gt;. CoreNLP is a combination of several tools, including the parser.&lt;/p&gt;
&lt;p&gt;The tools are all in Java. The parser supports a few languages: English, Chinese, Arabic, Spanish, etc. The only downside is that the tools are licensed under the GPL. Commercial licensing is available for proprietary software.&lt;/p&gt;
&lt;h5&gt;Excluded Software&lt;/h5&gt;
&lt;p&gt;We think that the libraries we choose are the best ones for parsing, or processing, natural languages. However we excluded some other interesting software, which are usually mentioned, like &lt;a href=&quot;https://github.com/CogComp/cogcomp-nlp&quot;&gt;CogCompNLP&lt;/a&gt; or &lt;a href=&quot;https://gate.ac.uk/&quot;&gt;GATE&lt;/a&gt; for several reasons:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;there might have little to no documentation&lt;/li&gt;
&lt;li&gt;it might have a purely educational or any non-standard license&lt;/li&gt;
&lt;li&gt;it might not be designed for developers, but for end-users&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we have seen many ways to deal with a document in a natural language to get the information you need from it. Most of them tried to find smart ways to bypass the complex task of parsing natural language. Despite being hard to parse natural languages it is still possible to do so, if you use the libraries available.&lt;/p&gt;
&lt;p&gt;Essentially, when dealing with natural languages hacking a solution is the suggested way of doing things, since nobody can figure out how to do it properly.&lt;/p&gt;
&lt;p&gt;Where it was possible we explained the algorithms that you can use. For the most advanced tasks this would have been impractical, so we just pointed at ready-to-use libraries. In any case, if you think we missed something, be it a subject or an important library, please contact us.&lt;/p&gt;


&lt;div class=&quot;ck_form_container ck_inline&quot; data-ck-version=&quot;6&quot;&gt;
&lt;div class=&quot;ck_form ck_vertical_subscription_form&quot;&gt;
&lt;div class=&quot;ck_form_content&quot;&gt;
&lt;h3 class=&quot;ck_form_title&quot;&gt;Parsing: Tools and Libraries&lt;/h3&gt;
&lt;div class=&quot;ck_description&quot;&gt;&lt;span class=&quot;ck_image&quot;&gt;&lt;img alt=&quot;Parsing_-_tools_and_libraries_-_cover&quot; src=&quot;https://i0.wp.com/s3.amazonaws.com/convertkit/subscription_forms/images/005/053/473/standard/Parsing_-_Tools_and_Libraries_-_Cover.png?w=1500&amp;amp;ssl=1&quot; data-recalc-dims=&quot;1&quot;/&gt;&lt;/span&gt;
&lt;p&gt;Receive the guide to your inbox to read it on all your devices when you have time. Learn about parsing in Java, Python, C#, and JavaScript&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;span id=&quot;tve_leads_end_content&quot;/&gt;

</description>
<pubDate>Wed, 15 Nov 2017 10:12:41 +0000</pubDate>
<dc:creator>ftomassetti</dc:creator>
<og:type>article</og:type>
<og:title>A Guide to Natural Language Processing - Federico Tomassetti - Software Architect</og:title>
<og:description>Natural Language Processing (NLP) offers amazing possibilities to elaborate text and extract information from it. This guide is a complete overview of NLP.</og:description>
<og:url>https://tomassetti.me/guide-natural-language-processing/</og:url>
<og:image>https://tomassetti.me/wp-content/uploads/2017/10/Parsing-Natural-Languages.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://tomassetti.me/guide-natural-language-processing/</dc:identifier>
</item>
<item>
<title>Debian and GNOME announce plans to migrate communities to GitLab</title>
<link>https://about.gitlab.com/press/releases/2017-11-01-gitlab-transitions-contributor-license.html</link>
<guid isPermaLink="true" >https://about.gitlab.com/press/releases/2017-11-01-gitlab-transitions-contributor-license.html</guid>
<description>&lt;ol class=&quot;breadcrumb&quot;&gt;&lt;li&gt;You are here:&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://about.gitlab.com/press/&quot;&gt;Press and Logos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://about.gitlab.com/press/releases/&quot;&gt;Press releases&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;GitLab Transitions Contributor Licensing to Developer Certificate of Origin to Better Support Open Source Projects; Empower Contributors&lt;/li&gt;
&lt;/ol&gt;&lt;h3 id=&quot;open-source-project-customers-debian-and-gnome-announce-plans-to-migrate-communities-to-gitlab-applaud-the-direction&quot;&gt;Open source project customers Debian and GNOME announce plans to migrate communities to GitLab; applaud the direction&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;SAN FRANCISCO - November 1, 2017&lt;/strong&gt; - GitLab, a software product used by &lt;a href=&quot;https://about.gitlab.com/is-it-any-good/&quot;&gt;2/3 of all enterprises&lt;/a&gt;, today announced it was abandoning the industry-standard Contributor License Agreement (CLA) in favor of a Developer Certificate of Origin (DCO) and license. The DCO gives developers greater flexibility and portability for their contributions. The move has already attracted the attention of large open source projects who recognize the benefits. Debian and GNOME both plan to migrate their communities and open source projects to GitLab.&lt;/p&gt;
&lt;p&gt;GitLab’s move away from a CLA is meant to modernize its code hosting and collaborative development infrastructure for all open source projects. Additionally, requiring a CLA became problematic for developers who didn’t want to enter into legal terms; they weren’t reviewing the CLA contract and they effectively gave up their rights to own and contribute to open source code.&lt;/p&gt;
&lt;p&gt;“Many large open source projects want to be masters of their own destiny, but overly restrictive licensing can be a barrier to attracting talented contributors and driving innovation in the project,” said Sid Sijbrandij, CEO at GitLab. “With a DCO and license, developers no longer have to surrender their work and enter into legal terms. They will now have the freedom to contribute to open-source code and the flexibility to leverage their contributions as they need.”&lt;/p&gt;
&lt;p&gt;In comparison, other open source platforms, like GitHub, Phabricator, Fedora, Jenkins, and Elastic, all currently require a CLA. For established companies, a shift of this magnitude is not easy. But after evaluating the needs of larger open source projects such as Debian and GNOME, GitLab came to the conclusion that a DCO would better suit their efforts to modernize code hosting and collaborative-development infrastructure.&lt;/p&gt;
&lt;p&gt;“We’re thrilled to see GitLab simplifying and encouraging community contributions by switching from a CLA to the DCO,” said Chris Lamb, Debian Project Leader. “We recognize that making a change of this nature is not easy and we applaud the time, patience and thoughtful consideration GitLab has shown here.”&lt;/p&gt;
&lt;p&gt;&quot;We applaud GitLab for dropping their CLA in favor of a more OSS-friendly approach,&quot; said Carlos Soriano, Board Director at GNOME. &quot;Open source communities are born from a sea of contributions that come together and transform into projects. This gesture affirmed GitLab's willingness to protect the individual, their creative process, and most importantly, keeps intellectual property in the hands of the creator.&quot;&lt;/p&gt;
&lt;p&gt;In addition to moving toward DCO, GitLab will take internal actions to review code that is submitted, to help minimize the likelihood of anything problematic entering the base. GitLab has already begun making the switch with no added steps necessary from their user base. For more information on what this means for the broader GitLab community, please visit &lt;a href=&quot;https://about.gitlab.com/2017/11/01/gitlab-switches-to-dco-license/&quot;&gt;this link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;About GitLab&lt;/strong&gt;&lt;br/&gt;Since its incorporation in 2014, GitLab has quickly become the leading self-hosted Git repository management tool used by software development teams ranging from startups to global enterprise organizations. GitLab has since expanded its product offering to deliver an integrated source code management, code review, test/release automation, and application monitoring product that accelerates and simplifies the software development process. With one end-to-end software development product, GitLab helps teams eliminate unnecessary steps from their workflow, significantly reduce cycle time and focus exclusively on building great software. Today, more than 100,000 organizations, including Ticketmaster, ING, NASDAQ, Alibaba, Sony, and Intel; and millions of users, trust GitLab to bring their modern applications from idea to production, reliably and repeatedly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Media Contact&lt;/strong&gt;&lt;br/&gt;Nicole Plati&lt;br/&gt;gitlab@highwirepr.com&lt;br/&gt;415-963-4174 ext. 39&lt;/p&gt;
</description>
<pubDate>Wed, 15 Nov 2017 06:50:34 +0000</pubDate>
<dc:creator>l2dy</dc:creator>
<og:title>GitLab Transitions Contributor Licensing to Developer Certificate of Origin to Better Support Open Source Projects; Empower Contributors</og:title>
<og:type>article</og:type>
<og:url>https://about.gitlab.com/press/releases/2017-11-01-gitlab-transitions-contributor-license.html</og:url>
<og:image>https://about.gitlab.com/images/blogimages/gitlab-blog-cover.png</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://about.gitlab.com/press/releases/2017-11-01-gitlab-transitions-contributor-license.html</dc:identifier>
</item>
<item>
<title>Hotbed of Misinformation</title>
<link>https://www.tesla.com/blog/hotbed-misinformation</link>
<guid isPermaLink="true" >https://www.tesla.com/blog/hotbed-misinformation</guid>
<description>&lt;p&gt;Tesla is absolutely against any form of discrimination, harassment, or unfair treatment of any kind. When we hear complaints, we take them very seriously, investigate thoroughly and, if proven to be true, take immediate action.&lt;/p&gt;
&lt;p&gt;Everyone at Tesla, without exception, is required to go through an anti-discrimination course. Our human resources team also conducts regular in-person spot training sessions when an allegation or complaint has been made, even if the evidence is not conclusive enough to warrant disciplinary action. We have also created a dedicated team focused exclusively on investigating workplace concerns, recommending corrective actions and assisting managers with implementing those actions.&lt;/p&gt;
&lt;p&gt;Regarding yesterday’s lawsuit, several months ago we had already investigated disappointing behavior involving a group of individuals who worked on or near Marcus Vaughn’s team. At the time, our investigation identified a number of conflicting accusations and counter-accusations between several African-American and Hispanic individuals, alleging use of racial language, including the &quot;n-word&quot; and &quot;w-word,&quot; towards each other and a threat of violence. After a thorough investigation, immediate action was taken, which included terminating the employment of three of the individuals.&lt;/p&gt;
&lt;p&gt;We believe this was the fair and just response to the facts that we learned. There will be further action as necessary, including parting ways with anyone whose behavior prevents Tesla from being a great place to work and making sure we do everything possible to stop bad behavior from happening in the first place. Our company has more than 33,000 employees, with over 10,000 in the Fremont factory alone, so it is not humanly possible to stop all bad conduct, but we will do our best to make it is as close to zero as possible.&lt;/p&gt;
&lt;p margin-top:=&quot;&quot;&gt;There are a number of other false statements in the class action lawsuit alleging a so-called “hotbed of discrimination”:&lt;/p&gt;
&lt;p&gt;- There is only one actual plaintiff (Marcus Vaughn), not 100. The reference to 100 is a complete fabrication with no basis in fact at all.&lt;/p&gt;&lt;p&gt;- The plaintiff was employed by a temp agency, not by Tesla as claimed in the lawsuit.&lt;/p&gt;&lt;p&gt;- Marcus was not fired, he was on a six month temp contract that simply ended as contracted.&lt;/p&gt;&lt;p&gt;- His email to Elon was about his commute and Tesla’s shuttles, which was addressed as he requested. There was no mention of racial discrimination whatsoever.&lt;/p&gt;&lt;p&gt;- The trial lawyer who filed this lawsuit has a long track record of extorting money for meritless claims and using the threat of media attacks and expensive trial costs to get companies to settle. At Tesla, we would rather pay ten times the settlement demand in legal fees and fight to the ends of the Earth than give in to extortion and allow this abuse of the legal system.&lt;/p&gt;&lt;p&gt;- We would also like to clear up the description of Elon’s prior email to employees. It is dedicated to ensuring that Tesla employees always try to do the right thing, that being a jerk is not allowed, that everyone should be contributing to an atmosphere where people look forward to coming to work in the morning and that no one should feel excluded, uncomfortable, or unfairly treated. As one of many points in that email, Elon also explained that if someone makes an offensive or hurtful statement on a single occasion, but subsequently offers a sincere apology, then we believe that apology should be accepted. The counterpoint would be that a single careless comment should ruin a person’s life and career, even if they truly regret their action and do their best to make amends. That would be a cold world with no forgiveness and no heart.&lt;/p&gt;
&lt;p&gt;Elon’s full email is below:&lt;br/&gt;&lt;img src=&quot;https://www.tesla.com/sites/default/files/images/blogs/doing-the-right-thing.jpg&quot;/&gt;&lt;/p&gt;
</description>
<pubDate>Wed, 15 Nov 2017 05:42:23 +0000</pubDate>
<dc:creator>runesoerensen</dc:creator>
<og:title>Hotbed of Misinformation</og:title>
<og:description>Tesla is absolutely against any form of discrimination, harassment, or unfair treatment of any kind. When we hear complaints, we take them very seriously, investigate thoroughly and, if proven to be true, take immediate action.</og:description>
<og:image>https://www.tesla.com/sites/default/files/blog_images/tesla_announcement_social.jpg</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.tesla.com/blog/hotbed-misinformation</dc:identifier>
</item>
<item>
<title>Fearless Concurrency in Firefox Quantum</title>
<link>https://blog.rust-lang.org/2017/11/14/Fearless-Concurrency-In-Firefox-Quantum.html</link>
<guid isPermaLink="true" >https://blog.rust-lang.org/2017/11/14/Fearless-Concurrency-In-Firefox-Quantum.html</guid>
<description>&lt;p&gt;These days, Rust is used for &lt;a href=&quot;https://www.rust-lang.org/friends.html&quot;&gt;all kinds of things&lt;/a&gt;. But its founding application was &lt;a href=&quot;https://servo.org/&quot;&gt;Servo&lt;/a&gt;, an experimental browser engine.&lt;/p&gt;&lt;p&gt;Now, after years of effort, a major part of Servo is shipping in production: Mozilla is releasing &lt;a href=&quot;https://www.mozilla.org/en-US/firefox/quantum/&quot;&gt;Firefox Quantum&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Rust code &lt;a href=&quot;https://hacks.mozilla.org/2016/07/shipping-rust-in-firefox/&quot;&gt;began shipping in Firefox&lt;/a&gt; last year, starting with relatively small pilot projects like an MP4 metadata parser to replace some uses of libstagefright. These components performed well and caused effectively no crashes, but browser development had yet to see large benefits from the full power Rust could offer. This changes today.&lt;/p&gt;

&lt;p&gt;Firefox Quantum includes Stylo, a pure-Rust CSS engine that makes full use of Rust’s “&lt;a href=&quot;http://blog.rust-lang.org/2015/04/10/Fearless-Concurrency.html&quot;&gt;Fearless Concurrency&lt;/a&gt;” to speed up page styling. It’s the first major component of Servo to be integrated with Firefox, and is a major milestone for Servo, Firefox, and Rust. It replaces approximately 160,000 lines of C++ with 85,000 lines of Rust.&lt;/p&gt;
&lt;p&gt;When a browser is loading a web page, it looks at the CSS and parses the rules. It then determines which rules apply to which elements and their precedence, and “cascades” these down the DOM tree, computing the final style for each element. Styling is a top-down process: you need to know the style of a parent to calculate the styles of its children, but the styles of its children can be calculated independently thereafter.&lt;/p&gt;
&lt;p&gt;This top-down structure is ripe for parallelism; however, since styling is a complex process, it’s hard to get right. Mozilla made two previous attempts to parallelize its style system in C++, and both of them failed. But Rust’s fearless concurrency has made parallelism practical! We use &lt;a href=&quot;https://crates.io/crates/rayon&quot;&gt;rayon&lt;/a&gt; —one of the hundreds of &lt;a href=&quot;http://crates.io/&quot;&gt;crates&lt;/a&gt; Servo uses from Rust’s ecosystem — to drive a work-stealing cascade algorithm. You can read more about that in &lt;a href=&quot;https://hacks.mozilla.org/2017/08/inside-a-super-fast-css-engine-quantum-css-aka-stylo/&quot;&gt;Lin Clark’s post&lt;/a&gt;. Parallelism leads to a lot of performance improvements, including a 30% page load speedup for Amazon’s homepage.&lt;/p&gt;

&lt;p&gt;An example of Rust preventing thread safety bugs is how style information is shared in Stylo. Computed styles are grouped into “style structs” of related properties, e.g. there’s one for all the font properties, one for all the background properties, and so on. Now, most of these are shared; for example, the font of a child element is usually the same as its parent, and often sibling elements share styles even if they don’t have the same style as the parent. Stylo uses Rust’s atomically reference counted &lt;a href=&quot;https://doc.rust-lang.org/std/sync/struct.Arc.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Arc&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/a&gt; to share style structs between elements. &lt;code class=&quot;highlighter-rouge&quot;&gt;Arc&amp;lt;T&amp;gt;&lt;/code&gt; makes its contents immutable, so it’s thread safe — you can’t accidentally modify a style struct when there’s a chance it is being used by other elements.&lt;/p&gt;
&lt;p&gt;We supplement this immutable access with &lt;code class=&quot;highlighter-rouge&quot;&gt;Arc::make_mut()&lt;/code&gt;; for example, &lt;a href=&quot;https://github.com/servo/servo/blob/657b2339a1e68f3a9c4525f35db023d3f149ffac/components/style/values/computed/font.rs#L182&quot;&gt;this line&lt;/a&gt; calls &lt;code class=&quot;highlighter-rouge&quot;&gt;.mutate_font()&lt;/code&gt; (a thin wrapper around &lt;code class=&quot;highlighter-rouge&quot;&gt;Arc::make_mut()&lt;/code&gt; for the font style struct) to set the font size. If the given element is the only element that has a reference to this specific font struct, it will just mutate it in place. But if it is not, &lt;code class=&quot;highlighter-rouge&quot;&gt;make_mut()&lt;/code&gt; will copy the entire style struct into a new, unique reference, which will then be mutated in place and eventually stored on the element.&lt;/p&gt;
&lt;div class=&quot;language-rust highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;.builder&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.mutate_font&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.set_font_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;computed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;On the other hand, Rust guarantees that it is impossible to mutate the style of the &lt;em&gt;parent&lt;/em&gt; element, because it is &lt;a href=&quot;https://github.com/servo/servo/blob/657b2339a1e68f3a9c4525f35db023d3f149ffac/components/style/properties/properties.mako.rs#L2623-L2627&quot;&gt;kept behind an immutable reference&lt;/a&gt;. Rayon’s scoped threading functionality makes sure that there is no way for that struct to even obtain/store a mutable reference if it wanted to. The parent style is something which one thread was allowed to write to to create (when the parent element was being processed), after which everyone is only allowed to read from it. You’ll notice that the reference is a zero-overhead “borrowed pointer”, &lt;em&gt;not&lt;/em&gt; a reference counted pointer, because Rust and Rayon let you share data across threads without needing reference counting when they can guarantee that the data will be alive at least as long as the thread.&lt;/p&gt;
&lt;p&gt;Personally, my “aha, I now fully understand the power of Rust” moment was when thread safety issues cropped up on the C++ side. Browsers are complex beings, and despite Stylo being Rust code, it needs to call back into Firefox’s C++ code a lot. Firefox has a single “main thread” per process, and while it does use other threads they are relatively limited in what they do. Stylo, being quite parallel, occasionally calls into C++ code off the main thread. That was usually fine, but would regularly surface thread safety bugs in the C++ code when there was a cache or global mutable state involved, things which basically never were a problem on the Rust side.&lt;/p&gt;
&lt;p&gt;These bugs were not easy to notice, and were often very tricky to debug. And that was with only the &lt;em&gt;occasional&lt;/em&gt; call into C++ code off the main thread; It feels like if we had tried this project in pure C++ we’d be dealing with this far too much to be able to get anything useful done. And indeed, bugs like these have thwarted multiple attempts to parallelize styling in the past, both in Firefox and other browsers.&lt;/p&gt;

&lt;p&gt;Firefox developers had a great time learning and using Rust. People really enjoyed being able to aggressively write code without having to worry about safety, and many mentioned that Rust’s ownership model was close to how they implicitly reason about memory within Firefox’s large C++ codebase. It was refreshing to have fuzzers catch mostly explicit &lt;em&gt;panics&lt;/em&gt; in Rust code, which are much easier to debug and fix than segfaults and other memory safety issues on the C++ side.&lt;/p&gt;
&lt;p&gt;A conversation amongst Firefox developers that stuck with me — one that was included in Josh Matthews’ &lt;a href=&quot;https://www.joshmatthews.net/rbr17&quot;&gt;talk&lt;/a&gt; at Rust Belt Rust — was&lt;/p&gt;
&lt;blockquote readability=&quot;20&quot;&gt;
&lt;p&gt;&amp;lt;heycam&amp;gt; one of the best parts about stylo has been how much easier it has been to implement these style system optimizations that we need, because Rust&lt;/p&gt;
&lt;p&gt;&amp;lt;heycam&amp;gt; can you imagine if we needed to implement this all in C++ in the timeframe we have&lt;/p&gt;
&lt;p&gt;&amp;lt;heycam&amp;gt; yeah srsly&lt;/p&gt;
&lt;p&gt;&amp;lt;bholley&amp;gt; heycam: it’s so rare that we get fuzz bugs in rust code&lt;/p&gt;
&lt;p&gt;&amp;lt;bholley&amp;gt; heycam: considering all the complex stuff we’re doing&lt;/p&gt;
&lt;p&gt;*heycam remembers getting a bunch of fuzzer bugs from all kinds of style system stuff in gecko&lt;/p&gt;
&lt;p&gt;&amp;lt;bholley&amp;gt; heycam: think about how much time we could save if each one of those annoying compiler errors today was swapped for a fuzz bug tomorrow :-)&lt;/p&gt;
&lt;p&gt;&amp;lt;heycam&amp;gt; heh&lt;/p&gt;
&lt;p&gt;&amp;lt;njn&amp;gt; you guys sound like an ad for Rust&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Overall, Firefox Quantum benefits significantly from Stylo, and thus from Rust. Not only does it speed up page load, but it also speeds up interaction times since styling information can be recalculated much faster, making the entire experience smoother.&lt;/p&gt;
&lt;p&gt;But Stylo is only the beginning. There are two major Rust integrations getting close to the end of the pipeline. One is integrating &lt;a href=&quot;https://github.com/servo/webrender/&quot;&gt;Webrender&lt;/a&gt; into Firefox; Webrender &lt;a href=&quot;https://hacks.mozilla.org/2017/10/the-whole-web-at-maximum-fps-how-webrender-gets-rid-of-jank/&quot;&gt;heavily uses the GPU to speed up rendering&lt;/a&gt;. Another is &lt;a href=&quot;https://github.com/pcwalton/pathfinder&quot;&gt;Pathfinder&lt;/a&gt;, a project that offloads font rendering to the GPU. And beyond those, there remains Servo’s parallel layout and DOM work, which are continuing to grow and improve. Firefox has a very bright future ahead.&lt;/p&gt;
&lt;p&gt;As a Rust team member, I’m really happy to see Rust being successfully used in production to such great effect! As a Servo and Stylo developer, I’m grateful to the tools Rust gave us to be able to pull this off, and I’m happy to see a large component of Servo finally make its way to users!&lt;/p&gt;
&lt;p&gt;Experience the benefits of Rust yourself — try out &lt;a href=&quot;https://www.mozilla.org/en-US/firefox/quantum/&quot;&gt;Firefox Quantum&lt;/a&gt;!&lt;/p&gt;
</description>
<pubDate>Wed, 15 Nov 2017 03:27:20 +0000</pubDate>
<dc:creator>ahomescu1</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.rust-lang.org/2017/11/14/Fearless-Concurrency-In-Firefox-Quantum.html</dc:identifier>
</item>
<item>
<title>Fructan, not gluten, induces symptoms in patients with gluten sensitivity</title>
<link>http://www.gastrojournal.org/article/S0016-5085(17)36302-3/pdf</link>
<guid isPermaLink="true" >http://www.gastrojournal.org/article/S0016-5085(17)36302-3/pdf</guid>
<description>[unable to retrieve full-text content]&lt;p&gt;Article URL: &lt;a href=&quot;http://www.gastrojournal.org/article/S0016-5085(17)36302-3/pdf&quot;&gt;http://www.gastrojournal.org/article/S0016-5085(17)36302-3/pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=15699841&quot;&gt;https://news.ycombinator.com/item?id=15699841&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Points: 229&lt;/p&gt;&lt;p&gt;# Comments: 128&lt;/p&gt;</description>
<pubDate>Tue, 14 Nov 2017 22:33:51 +0000</pubDate>
<dc:creator>kmundnic</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.rust-lang.org/2017/11/14/Fearless-Concurrency-In-Firefox-Quantum.html</dc:identifier>
</item>
<item>
<title>“It is never a compiler error”</title>
<link>https://blog.plover.com/2017/11/12/</link>
<guid isPermaLink="true" >https://blog.plover.com/2017/11/12/</guid>
<description>&lt;p&gt;&lt;a class=&quot;storytitle&quot; name=&quot;compiler-error&quot; href=&quot;https://blog.plover.com/prog/compiler-error.html&quot; id=&quot;compiler-error&quot;&gt;No, it is not a compiler error. It is never a compiler error.&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;When I used to hang out in the &lt;code&gt;comp.lang.c&lt;/code&gt; Usenet group, back when there &lt;em&gt;was&lt;/em&gt; a &lt;code&gt;comp.lang.c&lt;/code&gt; Usenet group, people would show up fairly often with some program they had written that didn't work, and ask if their compiler had a bug. The compiler did not have a bug. The compiler never had a bug. The bug was always in the programmer's code and usually in their understanding of the language.&lt;/p&gt;
&lt;p&gt;When I worked at the University of Pennsylvania, a grad student posted to one of the internal bulletin boards looking for help with a program that didn't work. Another graduate student, a super-annoying know-it-all, said confidently that it was certainly a compiler bug. It was not a compiler bug. It was caused by a misunderstanding of the way arguments to unprototyped functions were automatically promoted.&lt;/p&gt;
&lt;p&gt;This is actually a subtle point, obscure and easily misunderstood. Most examples I have seen of people blaming the compiler are much sillier. I used to be on the mailing list for discussing the development of Perl 5, and people would show up from time to time to ask if Perl's &lt;code&gt;if&lt;/code&gt; statement was broken. This is a little mind-boggling, that someone could think this. Perl was first released in 1987. (How time flies!) The &lt;code&gt;if&lt;/code&gt; statement is not exactly an obscure or little-used feature. If there had been a bug in &lt;code&gt;if&lt;/code&gt; it would have been discovered and fixed by 1988. Again, the bug was always in the programmer's code and usually in their understanding of the language.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://groups.google.com/d/msg/comp.lang.perl.misc/l95-_OoO8ck/zG3ob9-b5wsJ&quot;&gt;Here's something I wrote in October 2000&lt;/a&gt;, which I think makes the case very clearly, this time concerning a claimed bug in the &lt;code&gt;stat()&lt;/code&gt; function, another feature that first appeared in Perl 1.000:&lt;/p&gt;
&lt;blockquote readability=&quot;21&quot;&gt;
&lt;p&gt;On the one hand, there's a chance that the compiler has a broken &lt;code&gt;stat&lt;/code&gt; and is subtracting 6 or something. Maybe that sounds likely to you but it sounds really weird to me. I cannot imagine how such a thing could possibly occur. Why 6? It all seems very unlikely.&lt;/p&gt;
&lt;p&gt;Well, in the absence of an alternative hypothesis, we have to take what we can get. But in this case, there is an alternative hypothesis! The alternative hypothesis is that [this person's] program has a bug.&lt;/p&gt;
&lt;p&gt;Now, which seems more likely to you?&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Weird, inexplicable compiler bug that nobody has ever seen before&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;or&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Programmer fucked up&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Hmmm. Let me think.&lt;/p&gt;
&lt;p&gt;I'll take Door #2, Monty.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Presumably I had to learn this myself at some point. A programmer can waste a lot of time looking for the bug in the compiler instead of looking for the bug in their program. I have a file of (obnoxious) Good Advice for Programmers that I wrote about twenty years ago, and one of these items is:&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;Looking for a compiler bug is the strategy of LAST resort. LAST resort.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Anyway, I will get to the point. As I mentioned a few months ago, &lt;a href=&quot;https://blog.plover.com/math/24-puzzle-3.html#phoneapp&quot;&gt;I built a simple phone app&lt;/a&gt; that Toph and I can use to find solutions to “twenty-four puzzles”. In these puzzles, you are given four single-digit numbers and you have to combine them arithmetically to total 24. Pennsylvania license plates have four digits, so as we drive around we play the game with the license plate numbers we see. Sometimes we can't solve a puzzle, and then we wonder: is it because there is no solution, or because we just couldn't find one? Then we ask the phone app.&lt;/p&gt;
&lt;p&gt;The other day we saw the puzzle «5 4 5 1», which is very easy, but I asked the phone app, to find out if there were any other solutions that we missed. And it announced `No solutions.” Which is wrong. So my program had a bug, as my programs often do.&lt;/p&gt;
&lt;p&gt;The app has a pre-populated dictionary containing all possible solutions to all the puzzles that have solutions, which I generated ahead of time and embedded into the app. My first guess was that bug had been in the process that generated this dictionary, and that it had somehow missed the solutions of «5 4 5 1». These would be indexed under the key &lt;code&gt;1455&lt;/code&gt;, which is the same puzzle, because each list of solutions is associated with the four input numbers in ascending order. Happily I still had the original file containing the dictionary data, but when I looked in it under &lt;code&gt;1455&lt;/code&gt; I saw exactly the two solutions that I expected to see.&lt;/p&gt;
&lt;p&gt;So then I looked into the app itself to see where the bug was. Code Studio's underlying language is Javascript, and Code Studio has a nice debugger. I ran the app under the debugger, and stopped in the relevant code, which was:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    var x = [getNumber(&quot;a&quot;), getNumber(&quot;b&quot;), getNumber(&quot;c&quot;), getNumber(&quot;d&quot;)].sort().join(&quot;&quot;);
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This constructs a hash key (&lt;code&gt;x&lt;/code&gt;) that is used to index into the canned dictionary of solutions. The &lt;code&gt;getNumber()&lt;/code&gt; calls were retrieving the four numbers from the app's menus, and I verified that the four numbers were «5 4 5 1» as they ought to be. But what I saw next astounded me: &lt;code&gt;x&lt;/code&gt; was not being set to &lt;code&gt;1455&lt;/code&gt; as it should have been. It was set to &lt;code&gt;4155&lt;/code&gt;, which was not in the dictionary. And it was set to &lt;code&gt;4155&lt;/code&gt; because&lt;/p&gt;
&lt;p&gt;the built-in &lt;code&gt;sort()&lt;/code&gt; function&lt;/p&gt;
&lt;p&gt;was sorting the numbers&lt;/p&gt;
&lt;p&gt;into&lt;/p&gt;
&lt;p&gt;the&lt;/p&gt;
&lt;p&gt;wrong&lt;/p&gt;
&lt;p&gt;order.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://pic.blog.plover.com/prog/compiler-error/WTF.jpg&quot; /&gt;&lt;/p&gt;
&lt;p&gt;For a while I could not believe my eyes. But after another fifteen or thirty minutes of tinkering, I sent off a bug report… no, I did not. I still didn't believe it. I asked the front-end programmers at my company what my mistake had been. Nobody had any suggestions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Then&lt;/em&gt; I sent off a bug report that began:&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;I think that Array.prototype.sort() returned a wrongly-sorted result when passed a list of four numbers. This seems impossible, but …&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I was about 70% expecting to get a reply back explaining what I had misunderstood about the behavior of Javascript's &lt;code&gt;sort()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But to my astonishment, the reply came back only an hour later:&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;Wow! You're absolutely right. We'll investigate this right away.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In case you're curious, the bug was as follows: The &lt;code&gt;sort()&lt;/code&gt; function was using a bubble sort. (This is of course a bad choice, and I think the maintainers plan to replace it.) The bubble sort makes several passes through the input, swapping items that are out of order. It keeps a count of the number of swaps in each pass, and if the number of swaps is zero, the array is already ordered and the sort can stop early and skip the remaining passes. The test for this was:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    if (changes &amp;lt;= 1) break;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;but it should have been:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    if (changes == 0) break;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Ouch.&lt;/p&gt;
&lt;p&gt;The Code Studio folks handled this very creditably, and did indeed fix it the same day. (&lt;a href=&quot;https://support.code.org/hc/en-us/requests/115548&quot;&gt;The support system ticket is available for your perusal&lt;/a&gt;, as is &lt;a href=&quot;https://github.com/code-dot-org/JS-Interpreter/pull/23&quot;&gt;the Github pull request with the fix&lt;/a&gt;, in case you are interested.)&lt;/p&gt;
&lt;p&gt;I still can't quite believe it. I feel as though I have accidentally spotted the Loch Ness Monster, or Bigfoot, or something like that, a strange and legendary monster that until now I thought most likely didn't exist.&lt;/p&gt;
&lt;p&gt;A bug in the &lt;code&gt;sort()&lt;/code&gt; function. O day and night, but this is wondrous strange!&lt;/p&gt;
&lt;p&gt;[ Addendum 20171113: Thanks to &lt;a href=&quot;https://www.reddit.com/user/spotter&quot;&gt;Reddit user spotter&lt;/a&gt; for pointing me to a related 2008 blog post of Jeff Atwood's, &lt;a href=&quot;https://blog.codinghorror.com/the-first-rule-of-programming-its-always-your-fault/&quot;&gt;“The First Rule of Programming: It's Always Your Fault”&lt;/a&gt;. ]&lt;/p&gt;
&lt;p&gt;[ Addendum 20171113: Yes, yes, I know &lt;code&gt;sort()&lt;/code&gt; is in the library, not in the compiler. I am using “compiler error” as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Synecdoche&quot;&gt;synecdoche&lt;/a&gt; for “system software error”. ]&lt;/p&gt;
&lt;p align=&quot;right&quot;&gt;&lt;em&gt;[&lt;a href=&quot;https://blog.plover.com/prog&quot;&gt;Other articles in category /prog&lt;/a&gt;] &lt;a href=&quot;https://blog.plover.com/prog/compiler-error.html&quot;&gt;permanent link&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;br /&gt;</description>
<pubDate>Tue, 14 Nov 2017 22:10:51 +0000</pubDate>
<dc:creator>zeveb</dc:creator>
<og:title>The Universe of Discourse : No, it is not a compiler error. It is never a compiler error.</og:title>
<og:type>article</og:type>
<og:image>https://pic.blog.plover.com/prog/compiler-error/WTF.jpg</og:image>
<og:url>https://blog.plover.com/aliens/dd/p20.html</og:url>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.plover.com/2017/11/12/</dc:identifier>
</item>
</channel>
</rss>