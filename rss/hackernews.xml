<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>We only hire the best means we only hire the trendiest</title>
<link>http://danluu.com/programmer-moneyball/</link>
<guid isPermaLink="true" >http://danluu.com/programmer-moneyball/</guid>
<description>&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;/&gt;&lt;title&gt;When we only hire the best means we only hire the trendiest&lt;/title&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;/&gt;&lt;link rel=&quot;icon&quot; href=&quot;data:;base64,iVBORw0KGgo=&quot;/&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;354.58657505285&quot;&gt;
&lt;p&gt;&lt;strong&gt;When we only hire the best means we only hire the trendiest&lt;/strong&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;An acquaintance of mine, let’s call him Mike, is looking for work after getting laid off from a contract role at Microsoft, which has happened to a lot of people I know. Like me, Mike has 11 years in industry. Unlike me, he doesn’t know a lot of folks at trendy companies, so I passed his resume around to some engineers I know at companies that are desperately hiring. My engineering friends thought Mike’s resume was fine, but most recruiters rejected him in the resume screening phase.&lt;/p&gt;
&lt;p&gt;When I asked why he was getting rejected, the typical response I got was:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Tech experience is in irrelevant tech&lt;/li&gt;
&lt;li&gt;“Experience is too random, with payments, mobile, data analytics, and UX.”&lt;/li&gt;
&lt;li&gt;Contractors are generally not the strongest technically&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;This response is something from a recruiter that was relayed to me through an engineer; the engineer was incredulous at the response from the recruiter. Just so we have a name, let’s call this company TrendCo. It’s one of the thousands of companies that claims to have world class engineers, hire only the best, etc. This is one company in particular, but it’s representative of a large class of companies and the responses Mike has gotten.&lt;/p&gt;
&lt;p&gt;Anyway, (1) is code for “Mike’s a .NET dev, and we don’t like people with Windows experience”.&lt;/p&gt;
&lt;p&gt;I’m familiar with TrendCo’s tech stack, which multiple employees have told me is “a tire fire”. Their core systems top out under 1k QPS, which has caused them to go down under load. Mike has worked on systems that can handle multiple orders of magnitude more load, but his experience is, apparently, irrelevant.&lt;/p&gt;
&lt;p&gt;(2) is hard to make sense of. I’ve interviewed at TrendCo and one of the selling points is that it’s a startup where you get to do a lot of different things. TrendCo almost exclusively hires generalists but Mike is, apparently, too general for them.&lt;/p&gt;
&lt;p&gt;(3), combined with (1), gets at what TrendCo’s real complaint with Mike is. He’s not their type. TrendCo’s median employee is a recent graduate from one of maybe ten “top” schools with 0-2 years of experience. They have a few experienced hires, but not many, and most of their experienced hires have something trendy on their resume, not a boring old company like Microsoft.&lt;/p&gt;
&lt;p&gt;Whether or not you think there’s anything wrong with having a type and rejecting people who aren’t your type, as &lt;a href=&quot;https://news.ycombinator.com/item?id=11290662&quot;&gt;Thomas Ptacek has observed&lt;/a&gt;, if your type is the same type everyone else is competing for, “you are competing for talent with the wealthiest (or most overfunded) tech companies in the market”.&lt;/p&gt;
&lt;p&gt;If &lt;a href=&quot;https://docs.google.com/spreadsheets/u/1/d/1UnLz40Our1Ids-O0sz26uPNCF6cQjwosrZQY4VLdflU/htmlview?pli=1&amp;amp;sle=true#&quot;&gt;you look at new grad hiring data&lt;/a&gt;, it looks like FB is offering people with zero experience &amp;gt; $100k/ salary, $100k signing bonus, and $150k in RSUs, for an amortized total comp &amp;gt; $160k/yr, including $240k in the first year. Google’s package has &amp;gt; $100k salary, a variable signing bonus in the $10k range, and $187k in RSUs. That comes in a bit lower than FB, but it’s much higher than most companies that claim to only hire the best are willing to pay for a new grad. Keep in mind that &lt;a href=&quot;https://danluu.com/startup-tradeoffs/#fn:C&quot;&gt;compensation can go much higher for contested candidates&lt;/a&gt;, and that &lt;a href=&quot;https://news.ycombinator.com/item?id=11314449&quot;&gt;compensation for experienced candidates is probably higher than you expect if you’re not a hiring manager who’s seen what competitive offers look like today&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;By going after people with the most sought after qualifications, TrendCo has narrowed their options down to either paying out the nose for employees, or offering non-competitive compensation packages. TrendCo has chosen the latter option, which partially explains why they have, proportionally, so few senior devs – the compensation delta increases as you get more senior, and you have to make a really compelling pitch to someone to get them to choose TrendCo when you’re offering $150k/yr less than the competition. And as people get more experience, they’re less likely to believe the part of the pitch that explains how much the stock options are worth.&lt;/p&gt;
&lt;p&gt;Just to be clear, I don’t have anything against people with trendy backgrounds. I know a lot of these people who have impeccable interviewing skills and got 5-10 strong offers last time they looked for work. I’ve worked with someone like that: he was just out of school, his total comp package was north of $200k/yr, and he was worth every penny. But think about that for a minute. He had strong offers from six different companies, of which he was going to accept at most one. Including lunch and phone screens, the companies put in an average of eight hours apiece interviewing him. And because they wanted to hire him so much, the companies that were really serious spent an average of another five hours apiece of engineer time trying to convince him to take their offer. Because these companies had, on average, a ⅙ chance of hiring this person, they have to spend at least an expected (8+5) * 6 = 78 hours of engineer time. People with great backgrounds are, on average, pretty great, but they’re really hard to hire. It’s much easier to hire people who are underrated, especially if you’re not paying market rates.&lt;/p&gt;
&lt;p&gt;I’ve seen this hyperfocus on hiring people with trendy backgrounds from both sides of the table, and it’s ridiculous from both sides.&lt;/p&gt;
&lt;p&gt;On the referring side of hiring, I tried to get a startup I was at to hire the most interesting and creative programmer I’ve ever met, who was tragically underemployed for years because of his low GPA in college. We declined to hire him and I was told that his low GPA meant that he couldn’t be very smart. Years later, Google took a chance on him and he’s been killing it since then. He actually convinced me to join Google, and &lt;a href=&quot;http://danluu.com/tech-discrimination/&quot;&gt;at Google, I tried to hire one of the most productive programmers I know, who was promptly rejected by a recruiter for not being technical enough&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On the candidate side of hiring, I’ve experienced both being in demand and being almost unhireable. Because I did my undergrad at Wisconsin, which is one of the 25 schools that claims to be a top 10 cs/engineering school, I had recruiters beating down my door when I graduated. But that’s silly – that I attended Wisconsin wasn’t anything about me; I just happened to grow up in the state of Wisconsin. If I grew up in Utah, I probably would have ended up going to school at Utah. When I’ve compared notes with folks who attended schools like Utah and Boise State, their education is basically the same as mine. Wisconsin’s rank as an engineering school comes from having professors who do great research which is, at best, weakly correlated to &lt;a href=&quot;http://danluu.com/teach-debugging/&quot;&gt;effectiveness at actually teaching undergrads&lt;/a&gt;. Despite getting the same engineering education you could get at hundreds of other schools, I had a very easy time getting interviews and finding a great job.&lt;/p&gt;
&lt;p&gt;I spent 7.5 years in that great job, at Centaur. Centaur has a pretty strong reputation among hardware companies in Austin who’ve been around for a while, and I had an easy time shopping for local jobs at hardware companies. But I don’t know of any software folks who’ve heard of Centaur, and as a result I couldn’t get an interview at most software companies. There were even a couple of cases where I had really strong internal referrals and the recruiters still didn’t want to talk to me, which I found funny and my friends found frustrating.&lt;/p&gt;
&lt;p&gt;When I could get interviews, they often went poorly. A typical rejection reason was something like “we process millions of transactions per day here and we really need someone with more relevant experience who can handle these things without ramping up”. And then Google took a chance on me and I was the second person on a project to get serious about deep learning performance, which was a 20%-time project until just before I joined. &lt;a href=&quot;https://www.google.com/patents/US20160342889&quot;&gt;We built the fastest deep learning system in the world&lt;/a&gt;. From what I hear, they’re now on the Nth generation of that project, but even the first generation thing we built has better per-node performance and performance per dollar than any other production system I know of today, years later (excluding follow-ons to that project, of course).&lt;/p&gt;
&lt;p&gt;While I was at Google I had recruiters pinging me about job opportunities all the time. And now that I’m at boring old Microsoft, I don’t get nearly as many recruiters reaching out to me. I’ve been considering looking for work and I wonder how trendy I’ll be if I do. Experience in irrelevant tech? Check! Random experience? Check! Contractor? Well, no. But two out of three ain’t bad.&lt;/p&gt;
&lt;p&gt;My point here isn’t anything about me. It’s that here’s this person who has wildly different levels of attractiveness to employers at various times, mostly due to superficial factors that don’t have much to do with actual productivity. This is a really common story among people who end up at Google. If you hired them before they worked at Google, you might have gotten a great deal! But no one (except Google) was willing to take that chance. There’s something to be said for paying more to get a known quantity, but a company like TrendCo that isn’t willing to do that cripples its hiring pipeline by only going after people with trendy resumes.&lt;/p&gt;
&lt;p&gt;I don’t mean to pick on startups like TrendCo in particular. Boring old companies have their version of what a trendy background is, too. A friend of mine who’s desperate to hire can’t do anything with some of the resumes I pass his way because his group isn’t allowed to hire anyone without a degree. Another person I know is in a similar situation because his group won’t talk to people who aren’t already employed.&lt;/p&gt;
&lt;p&gt;Not only are these decisions non-optimal for companies, they create a path dependence in employment outcomes that causes individual good (or bad) events to follow people around for decades. You can see similar effects in the literature on career earnings in a variety of fields.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=7260087&quot;&gt;Thomas Ptacek has this great line about how&lt;/a&gt; “we interview people whose only prior work experience is “Line of Business .NET Developer”, and they end up showing us how to write exploits for elliptic curve partial nonce bias attacks that involve Fourier transforms and BKZ lattice reduction steps that take 6 hours to run.” If you work at a company that doesn’t reject people out of hand for not being trendy, you’ll hear lots of stories like this. Some of the best people I’ve worked with went to schools you’ve never heard of and worked at companies you’ve never heard of until they ended up at Google. Some are still at companies you’ve never heard of.&lt;/p&gt;
&lt;p&gt;If you read &lt;a href=&quot;https://zachholman.com/talk/firing-people&quot;&gt;Zach Holman&lt;/a&gt;, you may recall that when he said that he was fired, someone responded with “If an employer has decided to fire you, then you’ve not only failed at your job, you’ve failed as a human being.” A lot of people treat employment status and credentials as measures of the inherent worth of individuals. But a large component of these markers of success, not to mention success itself, is luck.&lt;/p&gt;
&lt;h3 id=&quot;solutions&quot;&gt;Solutions?&lt;/h3&gt;
&lt;p&gt;I can understand why this happens. At an individual level, we’re prone to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Fundamental_attribution_error&quot;&gt;fundamental attribution error&lt;/a&gt;. At an organizational level, fast growing organizations burn a large fraction of their time on interviews, and the obvious way to cut down on time spent interviewing is to only interview people with “good” qualifications. Unfortunately, that’s counterproductive when you’re chasing after the same tiny pool of people as everyone else.&lt;/p&gt;
&lt;p&gt;Here are the beginnings of some ideas. I’m open to better suggestions!&lt;/p&gt;
&lt;h4 id=&quot;moneyball&quot;&gt;Moneyball&lt;/h4&gt;
&lt;p&gt;Billy Beane and Paul Depodesta took the Oakland A’s, a baseball franchise with nowhere near the budget of top teams, and created what was arguably the best team in baseball by finding and “hiring” players who were statistically underrated for their price. The thing I find really amazing about this is that they publicly talked about doing this, and then Michael Lewis wrote a book, titled &lt;a href=&quot;https://www.amazon.com/gp/product/0393324818/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&amp;amp;tag=abroaview-20&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;linkCode=as2&amp;amp;creativeASIN=0393324818&amp;amp;linkId=65d86d3a72b4c1ba73e8d3d52796eae1&quot;&gt;Moneyball&lt;/a&gt;, about them doing this. Despite the publicity, it took years for enough competitors to catch on enough that the A’s strategy stopped giving them a very large edge.&lt;/p&gt;
&lt;p&gt;You can see the exact same thing in software hiring. Thomas Ptacek has been talking about how they hired unusually effective people at Matasano for at least half a decade, maybe more. Google bigwigs regularly talk about the hiring data they have and what hasn’t worked. I believe they talked about how focusing on top schools wasn’t effective and didn’t turn up employees that have better performance years ago, but that doesn’t stop TrendCo from focusing hiring efforts on top schools.&lt;/p&gt;
&lt;h4 id=&quot;training-mentorship&quot;&gt;Training / mentorship&lt;/h4&gt;
&lt;p&gt;You see a lot of talk about moneyball, but for some reason people are less excited about… trainingball? Practiceball? Whatever you want to call taking people who aren’t “the best” and teaching them how to be “the best”.&lt;/p&gt;
&lt;p&gt;This is another one where it’s easy to see the impact through the lens of sports, because there is so much good performance data. Since it’s basketball season, if we look at college basketball, for example, we can identify a handful of programs that regularly take unremarkable inputs and produce good outputs. And that’s against a field of competitors where every team is expected to coach and train their players.&lt;/p&gt;
&lt;p&gt;When it comes to tech companies, most of the competition isn’t even trying. At the median large company, you get a couple days of “orientation”, which is mostly legal mumbo jumbo and paperwork, and the occasional “training”, which is usually a set of videos and a set of multiple-choice questions that are offered up for compliance reasons, not to teach anyone anything. And you’ll be assigned a mentor who, more likely than not, won’t provide any actual mentorship. Startups tend to be even worse! It’s not hard to do better than that.&lt;/p&gt;
&lt;p&gt;Considering how much money companies spend on &lt;a href=&quot;https://news.ycombinator.com/item?id=11314449&quot;&gt;hiring and retaining&lt;/a&gt; “the best”, you’d expect them to spend at least a (non-zero) fraction on training. It’s also quite strange that companies don’t focus more or training and mentorship when trying to recruit. Specific things I’ve learned in specific roles have been tremendously valuable to me, but it’s almost always either been a happy accident, or something I went out of my way to do. Most companies don’t focus on this stuff. Sure, recruiters will tell you that “you’ll learn so much more here than at Google, which will make you more valuable”, implying that it’s worth the $150k/yr pay cut, but if you ask them what, specifically, they do to make a better learning environment than Google, they never have a good answer.&lt;/p&gt;
&lt;h4 id=&quot;process-tools-culture&quot;&gt;Process / tools / culture&lt;/h4&gt;
&lt;p&gt;I’ve worked at two companies that both have effectively infinite resources to spend on tooling. One of them, let’s call them ToolCo, is really serious about tooling and invests heavily in tools. People describe tooling there with phrases like “magical”, “the best I’ve ever seen”, and “I can’t believe this is even possible”. And I can see why. For example, if you want to build a project that’s millions of lines of code, their build system will make that take somewhere between 5s and 20s (assuming you don’t enable &lt;a href=&quot;https://en.wikipedia.org/wiki/Interprocedural_optimization&quot;&gt;LTO&lt;/a&gt; or anything else that can’t be parallelized). In the course of a regular day at work you’ll use multiple tools that seem magical because they’re so far ahead of what’s available in the outside world.&lt;/p&gt;
&lt;p&gt;The other company, let’s call them ProdCo &lt;a href=&quot;http://yosefk.com/blog/people-can-read-their-managers-mind.html&quot;&gt;pays lip service to tooling, but doesn’t really value it&lt;/a&gt;. People describing ProdCo tools use phrases like “world class bad software” and “I am 2x less productive than I’ve ever been anywhere else”, and “I can’t believe this is even possible”. ProdCo has a paper on a new build system; their claimed numbers for speedup from parallelization/caching, onboarding time, and reliability, are at least two orders of magnitude worse than the equivalent at ToolCo. And, in my experience, the actual numbers are worse than the claims in the paper. In the course of a day of work at ProdCo, you’ll use multiple tools that are multiple orders of magnitude worse than the equivalent at ToolCo in multiple dimensions. These kinds of things add up and can easily make a larger difference than “hiring only the best”.&lt;/p&gt;
&lt;p&gt;Processes and culture also matter. I once worked on a team that didn’t use version control or have a bug tracker. For every no-brainer item on the &lt;a href=&quot;http://www.joelonsoftware.com/articles/fog0000000043.html&quot;&gt;Joel test&lt;/a&gt;, there are teams out there that make the wrong choice.&lt;/p&gt;
&lt;p&gt;Although I’ve only worked on one team that completely failed the Joel test, every team I’ve worked on has had glaring deficiencies that are technically trivial (but sometimes culturally difficult) to fix. When I was at Google, we had really bad communication problems between the two halves of our team that were in different locations. My fix was brain-dead simple: I started typing up meeting notes for all of our local meetings and discussions and taking questions from the remote team about things that surprised them in our notes. That’s something anyone could have done, and it was a huge productivity improvement for the entire team. I’ve literally never found an environment where you can’t massively improve productivity with something that trivial. Sometimes people don’t agree (e.g., it took months to get the non-version-control-using-team to use version control), but that’s a topic for another post.&lt;/p&gt;
&lt;p&gt;Programmers are woefully underutilized at most companies. What’s the point of &lt;a href=&quot;http://danluu.com/wat/&quot;&gt;hiring “the best” and then crippling them&lt;/a&gt;? You can get better results by hiring undistinguished folks and setting them up for success, and &lt;a href=&quot;https://twitter.com/patio11/status/706884144538648576&quot;&gt;it’s a lot cheaper&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;When I started programming, I heard a lot about how programmers are down to earth, not like those elitist folks who have uniforms involving suits and ties. You can even wear t-shirts to work! But if you think programmers aren’t elitist, try wearing a suit and tie to an interview sometime. You’ll have to go above and beyond to prove that you’re not a bad cultural fit. We like to think that we’re different from all those industries that judge people based on appearance, but we do the same thing, only instead of saying that people are a bad fit because they don’t wear ties, we say they’re a bad fit because they do, and instead of saying people aren’t smart enough because they don’t have the right pedigree… wait, that’s exactly the same.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;See also: &lt;a href=&quot;http://danluu.com/hiring-lemons/&quot;&gt;developer hiring and the market for lemons&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;Thanks to Kelley Eskridge, Laura Lindzey, John Hergenroeder, Kamal Marhubi, Julia Evans, Steven McCarthy, Lindsey Kuper, Leah Hanson, Darius Bacon, Pierre-Yves Baccou, Kyle Littler, Jorge Montero, and Mark Dominus for discussion/comments/corrections.&lt;/small&gt;&lt;/p&gt;


&lt;footer&gt;
&lt;/footer&gt;&lt;/body&gt;</description>
<pubDate>Tue, 31 Oct 2017 06:26:51 +0000</pubDate>
<dc:creator>indy</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://danluu.com/programmer-moneyball/</dc:identifier>
</item>
<item>
<title>HTTP 103 – An HTTP Status Code for Indicating Hints</title>
<link>https://datatracker.ietf.org/doc/draft-ietf-httpbis-early-hints/</link>
<guid isPermaLink="true" >https://datatracker.ietf.org/doc/draft-ietf-httpbis-early-hints/</guid>
<description>&lt;h2&gt;An HTTP Status Code for Indicating Hints&lt;br/&gt;&lt;small&gt;draft-ietf-httpbis-early-hints-05&lt;/small&gt;&lt;/h2&gt;


&lt;pre&gt;
HTTP Working Group                                                K. Oku
Internet-Draft                                                    Fastly
Intended status: Experimental                           October 28, 2017
Expires: May 1, 2018

                An HTTP Status Code for Indicating Hints
                   draft-ietf-httpbis-early-hints-05

&lt;span class=&quot;m_h&quot;&gt;Abstract&lt;/span&gt;

   This memo introduces an informational HTTP status code that can be
   used to convey hints that help a client make preparations for
   processing the final response.

Note to Readers

   Discussion of this draft takes place on the HTTP working group
   mailing list (ietf-http-wg@w3.org), which is archived at
   https://lists.w3.org/Archives/Public/ietf-http-wg/ .

   Working Group information can be found at https://httpwg.github.io/ ;
   source code and issues list for this draft can be found at
   https://github.com/httpwg/http-extensions/labels/early-hints .

&lt;span class=&quot;m_h&quot;&gt;Status of This Memo&lt;/span&gt;

   This Internet-Draft is submitted in full conformance with the
   provisions of BCP 78 and BCP 79.

   Internet-Drafts are working documents of the Internet Engineering
   Task Force (IETF).  Note that other groups may also distribute
   working documents as Internet-Drafts.  The list of current Internet-
   Drafts is at http://datatracker.ietf.org/drafts/current/.

   Internet-Drafts are draft documents valid for a maximum of six months
   and may be updated, replaced, or obsoleted by other documents at any
   time.  It is inappropriate to use Internet-Drafts as reference
   material or to cite them other than as &quot;work in progress.&quot;

   This Internet-Draft will expire on May 1, 2018.

&lt;span class=&quot;m_h&quot;&gt;Copyright Notice&lt;/span&gt;

   Copyright (c) 2017 IETF Trust and the persons identified as the
   document authors.  All rights reserved.

&lt;span class=&quot;m_ftr&quot;&gt;Oku                        Expires May 1, 2018                  [Page 1]&lt;/span&gt;
&lt;span class=&quot;m_hdr&quot;&gt;Internet-Draft                 Early Hints                  October 2017&lt;/span&gt;

   This document is subject to BCP 78 and the IETF Trust's Legal
   Provisions Relating to IETF Documents
   (http://trustee.ietf.org/license-info) in effect on the date of
   publication of this document.  Please review these documents
   carefully, as they describe your rights and restrictions with respect
   to this document.  Code Components extracted from this document must
   include Simplified BSD License text as described in Section 4.e of
   the Trust Legal Provisions and are provided without warranty as
   described in the Simplified BSD License.

&lt;span class=&quot;m_h&quot;&gt;Table of Contents&lt;/span&gt;

   1.  Introduction  . . . . . . . . . . . . . . . . . . . . . . . .   2
     1.1.  Notational Conventions  . . . . . . . . . . . . . . . . .   3
   2.  103 Early Hints . . . . . . . . . . . . . . . . . . . . . . .   3
   3.  Security Considerations . . . . . . . . . . . . . . . . . . .   5
   4.  IANA Considerations . . . . . . . . . . . . . . . . . . . . .   6
   5.  References  . . . . . . . . . . . . . . . . . . . . . . . . .   6
     5.1.  Normative References  . . . . . . . . . . . . . . . . . .   6
     5.2.  Informative References  . . . . . . . . . . . . . . . . .   6
   Appendix A.  Changes  . . . . . . . . . . . . . . . . . . . . . .   6
     A.1.  Since draft-ietf-httpbis-early-hints-04 . . . . . . . . .   6
     A.2.  Since draft-ietf-httpbis-early-hints-03 . . . . . . . . .   7
     A.3.  Since draft-ietf-httpbis-early-hints-02 . . . . . . . . .   7
     A.4.  Since draft-ietf-httpbis-early-hints-01 . . . . . . . . .   7
     A.5.  Since draft-ietf-httpbis-early-hints-00 . . . . . . . . .   7
   Appendix B.  Acknowledgements . . . . . . . . . . . . . . . . . .   7
   Author's Address  . . . . . . . . . . . . . . . . . . . . . . . .   7

&lt;span class=&quot;m_h&quot;&gt;1.  Introduction&lt;/span&gt;

   It is common for HTTP responses to contain links to external
   resources that need to be fetched prior to their use; for example,
   rendering HTML by a Web browser.  Having such links available to the
   client as early as possible helps to minimize perceived latency.

   The &quot;preload&quot; ([Preload]) link relation can be used to convey such
   links in the Link header field of an HTTP response.  However, it is
   not always possible for an origin server to generate the header block
   of a final response immediately after receiving a request.  For
   example, the origin server might delegate a request to an upstream
   HTTP server running at a distant location, or the status code might
   depend on the result of a database query.

   The dilemma here is that even though it is preferable for an origin
   server to send some header fields as soon as it receives a request,
   it cannot do so until the status code and the full header fields of
   the final HTTP response are determined.

&lt;span class=&quot;m_ftr&quot;&gt;Oku                        Expires May 1, 2018                  [Page 2]&lt;/span&gt;
&lt;/pre&gt;
&lt;a class=&quot;btn btn-default btn-block&quot; href=&quot;https://datatracker.ietf.org/doc/draft-ietf-httpbis-early-hints/?include_text=1&quot;&gt; Show full document text&lt;/a&gt;</description>
<pubDate>Mon, 30 Oct 2017 23:52:56 +0000</pubDate>
<dc:creator>snomad</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://datatracker.ietf.org/doc/draft-ietf-httpbis-early-hints/</dc:identifier>
</item>
<item>
<title>Butterfly iQ – A cheap handheld ultrasound tool with AI smarts inside</title>
<link>https://spectrum.ieee.org/the-human-os/biomedical/imaging/new-ultrasound-on-a-chip-tool-could-revolutionize-medical-imaging</link>
<guid isPermaLink="true" >https://spectrum.ieee.org/the-human-os/biomedical/imaging/new-ultrasound-on-a-chip-tool-could-revolutionize-medical-imaging</guid>
<description>&lt;p&gt;Jonathan Rothberg, a entrepreneur who prides himself on drastically disrupting the biomedical industry every so often, has typically big claims for his new product. The &lt;a href=&quot;https://www.butterflynetwork.com/&quot;&gt;Butterfly iQ&lt;/a&gt;, a cheap handheld ultrasound tool with AI smarts tucked inside, will 1) revolutionize medical imaging in hospitals and clinics, 2) change the game in global health, and 3) eventually become a consumer product that will be as ubiquitous as the household thermometer, he says. &lt;/p&gt;
&lt;p&gt;Today, Rothberg’s startup &lt;a href=&quot;https://www.butterflynetwork.com/about.html&quot;&gt;Butterfly Network&lt;/a&gt; unveiled the tool and announced its FDA clearance for 13 clinical applications, including cardiac scans, fetal and obstetric exams, and musculoskeletal checks. Rather than using a dedicated piece of hardware for the controls and image display, the iQ works with the user’s iPhone. The company says it will start shipping units in 2018 at an initial price of about $2,000.&lt;/p&gt;
&lt;div class=&quot;imgWrapper rt med&quot;&gt;&lt;img alt=&quot;Photo shows a medical professional holding a smartphone in one hand and an ultrasound wand in the other hand&quot; src=&quot;https://spectrum.ieee.org/image/Mjk3NDU1Mg.jpeg&quot;/&gt; Photo: Butterfly Network&lt;/div&gt;
&lt;p&gt;But that’s just the beginning, Rothberg tells &lt;em&gt;IEEE Spectrum&lt;/em&gt;. He expects to bring the price down on the handheld gadget, and is already looking ahead to radically new products. “In the next two years we’ll release a patch that uses ultrasound to monitor patients, and a pill you can swallow to look at cancer from within the body,” he says.&lt;/p&gt;
&lt;p&gt;All these form factors are possible because Butterfly uses a very different technology than conventional ultrasound. Its “ultrasound on a chip” takes advantage of the mass-market fabrication techniques perfected for computer chips, Rothberg says. “We put all the elements onto a semiconductor wafer, then we can dice up the wafer to make 48 ultra low-cost ultrasound machines,” he says.&lt;/p&gt;
&lt;p&gt;Today’s ultrasound systems use &lt;a href=&quot;https://en.wikipedia.org/wiki/Piezoelectricity&quot;&gt;piezoelectric&lt;/a&gt; crystals, which convert electrical energy into vibrations in the form of ultrasonic waves. A typical system has a display screen on a bulky cart with several wands for imaging at different depths within the body. These machines can cost upwards of $100,000. While a few smaller and more cheaper devices exist, such as GE’s handheld &lt;a href=&quot;http://www3.gehealthcare.com/en/products/categories/ultrasound/vscan_family&quot;&gt;Vscan products&lt;/a&gt;, they still use pricey piezoelectric technology and require multiple probes, bringing the price to something between $8,000 and $20,000.&lt;/p&gt;
&lt;p&gt;Developing the iQ’s chip-based technology was a two-step process. First, Butterfly’s engineers replaced the piezoelectrics with a micromachine that acts like a tiny drum to generate vibrations. Inside this “c&lt;span&gt;apacitive micromachined ultrasound transducer” (&lt;a href=&quot;https://kyg.stanford.edu/research&quot;&gt;CMUT&lt;/a&gt;), an applied voltage moves a membrane to send ultrasonic waves into the body. The waves that bounce back from various body tissues move the membrane and are registered as an electric signal, which creates the image. Butterfly based its technology on &lt;a href=&quot;https://spectrum.ieee.org/biomedical/imaging/nextgen-ultrasound&quot;&gt;research done&lt;/a&gt; by Stanford professor &lt;a href=&quot;https://kyg.stanford.edu/professor-khuri-yakub-biography&quot;&gt;Pierre Khuri-Yakub&lt;/a&gt;, who serves on Butterfly’s scientific advisory board.&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;imgWrapper rt med&quot;&gt;&lt;img alt=&quot;Photo shows a semiconductor chip held between finger and thumb.&quot; src=&quot;https://spectrum.ieee.org/image/Mjk3NDU2MA.jpeg&quot;/&gt; Photo: Butterfly Network&lt;/div&gt;
&lt;p&gt;Rothberg explains that typical ultrasound systems require separate probes for different clinical applications because the crystals have to be tuned at the time of manufacture to produce the right type of ultrasonic wave for imaging at a particular depth. But the Butterfly iQ can be tuned on the fly. “We have 10,000 of these micromachine transducers on a probe, and that gives us a monster dynamic range,” he says. &quot;We can make them buzz at 1 megahertz if we want to go deep, or 5 megahertz if we want to go shallow.”&lt;/p&gt;
&lt;p&gt;The second innovation was to do away with the wiring that connects a typical piezoelectric probe to the electronic controls and displays. Butterfly’s micromachines are attached directly to a semiconductor layer that contains all the necessary amplifiers, signal processors, and so on.&lt;/p&gt;
&lt;p&gt;Independent experts say the technology sounds promising, but they’ll wait to see if the Butterfly iQ can live up to Rothberg’s claims. “People in medicine tend to be conservative, and I’m skeptical when someone claims to have found a wonderdrug or cure,” says &lt;a href=&quot;http://emergency.med.ufl.edu/about-us/fellow/torben-becker-md-phd/&quot;&gt;Torben Becker&lt;/a&gt;&lt;span&gt;, an emergency room physician at the University of Florida hospital who has researched the use of portable ultrasound tools by &lt;a href=&quot;http://www.tandfonline.com/doi/abs/10.1080/10903127.2017.1358783&quot;&gt;paramedics&lt;/a&gt; and in the &lt;a href=&quot;http://onlinelibrary.wiley.com/doi/10.1111/tmi.12657/full&quot;&gt;developing world&lt;/a&gt;. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Becker says portable ultrasound tools are currently a niche technology—some ER doctors are experimenting with them, he says, but they haven’t caught on. “There’s definitely an argument to be made for having something that you can pull out of your pocket and use in an exam,” he says. But the portable tools he has tried thus far had inferior image quality to the big systems, Becker says. He also ran into difficulties with transmitting the images to the hospital’s database for storage. “And saving those images is required to bill for the scan,” he notes.&lt;/p&gt;
&lt;p&gt;Beyond price and portability, the Butterfly iQ’s other big selling point is its incorporation of artificial intelligence for both image acquisition and analysis. The Butterfly engineers trained the software on vast datasets of ultrasound images, teaching it the difference between a high- and poor-quality image for body parts like the heart. When the user brings the probe to a patient’s chest for a cardiac exam, the iPhone display helps them find the right spot. It also does some simple analysis of the resulting ultrasound image, such as measuring the “ejection fraction” that indicates how well a heart is pumping out blood.  &lt;/p&gt;
&lt;p&gt;At Butterfly Network’s New York City office, two members of the machine learning team kindly gave &lt;em&gt;IEEE Spectrum&lt;/em&gt; a demonstration of the technology at work:&lt;/p&gt;
&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; gesture=&quot;media&quot; height=&quot;349&quot; src=&quot;https://www.youtube.com/embed/vWftey4ag3o?rel=0&quot; width=&quot;620&quot;&gt;[embedded content]&lt;/iframe&gt;
&lt;p&gt;The AI component is what makes the iQ a potential game-changer for both health care in the developing world and for well-off consumers. In the global health context, Butterfly Network envisions the tool being used at rural health clinics where the staff doesn’t have expertise with ultrasound; with the iQ’s guidance system, they could nevertheless acquire a proper image and either send it to an expert or use the system’s guidance to figure out the next step. &lt;/p&gt;
&lt;p&gt;In the United States and other prosperous parts of the world, the tool could empower consumers, relieve anxiety, and reduce health care costs by eliminating many doctor visits, says John Martin, Butterfly Network’s chief medical officer. “I absolutely think the ultrasound device will be in everybody’s house,” Martin says. For example, if a child falls and complains of a pain in the arm, the parents could do a quick ultrasound to check the bone, maybe sending that image to their pediatrician rather than making tracks to the ER.   &lt;/p&gt;
&lt;p&gt;The FDA hasn’t yet approved the tool for at-home use, but Martin says the path to that approval is well established. &lt;span&gt;“W&lt;/span&gt;hen the first thermometers were made, when the first blood pressure cuffs were made, they were only in hospitals,” he says. “Think about defibrillators, which used to only be in hospitals but are now in every mall and office—someone off the street can now deliver an electric shock to someone’s heart.” If Butterfly Network has its way, a stranger off the street could one day easily take a gander at your innards. &lt;/p&gt;
</description>
<pubDate>Mon, 30 Oct 2017 20:43:40 +0000</pubDate>
<dc:creator>taion</dc:creator>
<og:title>New &quot;Ultrasound on a Chip&quot; Tool Could Revolutionize Medical Imaging</og:title>
<og:url>https://spectrum.ieee.org/the-human-os/biomedical/imaging/new-ultrasound-on-a-chip-tool-could-revolutionize-medical-imaging</og:url>
<og:description>Entrepreneur and disruptor Jonathan Rothberg launches an ultrasound tool from his latest company, Butterfly Networks</og:description>
<og:image>https://spectrum.ieee.org/image/Mjk3MzIxOQ.jpeg</og:image>
<og:type>blog-the-human-os</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://spectrum.ieee.org/the-human-os/biomedical/imaging/new-ultrasound-on-a-chip-tool-could-revolutionize-medical-imaging</dc:identifier>
</item>
<item>
<title>Simplicity: A New Language for Blockchains [pdf]</title>
<link>https://blockstream.com/simplicity.pdf</link>
<guid isPermaLink="true" >https://blockstream.com/simplicity.pdf</guid>
<description>&lt;a href=&quot;https://blockstream.com/simplicity.pdf&quot;&gt;Download PDF&lt;/a&gt;</description>
<pubDate>Mon, 30 Oct 2017 20:00:44 +0000</pubDate>
<dc:creator>TD-Linux</dc:creator>
<og:title>New &quot;Ultrasound on a Chip&quot; Tool Could Revolutionize Medical Imaging</og:title>
<og:url>https://spectrum.ieee.org/the-human-os/biomedical/imaging/new-ultrasound-on-a-chip-tool-could-revolutionize-medical-imaging</og:url>
<og:description>Entrepreneur and disruptor Jonathan Rothberg launches an ultrasound tool from his latest company, Butterfly Networks</og:description>
<og:image>https://spectrum.ieee.org/image/Mjk3MzIxOQ.jpeg</og:image>
<og:type>blog-the-human-os</og:type>
<dc:language>en</dc:language>
<dc:format>application/pdf</dc:format>
<dc:identifier>https://blockstream.com/simplicity.pdf</dc:identifier>
</item>
<item>
<title>Ask HN: We have a great team and capital but can&amp;#039;t find a good idea</title>
<link>https://news.ycombinator.com/item?id=15588361</link>
<guid isPermaLink="true" >https://news.ycombinator.com/item?id=15588361</guid>
<description>I have spent the past year looking for good ideas for a startup and have little to show for it.
&lt;p&gt;--------------------&lt;/p&gt;&lt;p&gt;-Things that we have:-&lt;/p&gt;
&lt;p&gt;1) A great early team, made up of myself (just a normal bizdev guy) + 1 talented former private equity partner (who made a fortune on Wall Street) + 1 talented CMU CompSci PhD. The three of us have been friends for years and work together really well.&lt;/p&gt;
&lt;p&gt;2) Investors that are willing to write blank checks (within reason) because of the trust in the team (mainly former colleagues of the PE guy).&lt;/p&gt;
&lt;p&gt;3) Cash in the bank to continue experimenting&lt;/p&gt;
&lt;p&gt;4) An interest in SaaS and ML (and some experience in the latter)&lt;/p&gt;
&lt;p&gt;-Things that we don't have:-&lt;/p&gt;
&lt;p&gt;1) Ideas of what to do&lt;/p&gt;
&lt;p&gt;--------------------&lt;/p&gt;
&lt;p&gt;We have read EVERYTHING on how to come up with startups ideas (ranging from Paul Graham essays to The Mom Test). We have ran interviews with friends in corporate and startups, asked old colleagues, attented conferences, organised meetups in our city, a ton of time spent networking, etc.&lt;/p&gt;
&lt;p&gt;The few product ideas we came up with following the above process we dropped, often because we discovered that that space is ultra crowded or commooditized.&lt;/p&gt;
&lt;p&gt;We will not give up but are getting unsure on how to break the stalemate.&lt;/p&gt;
&lt;p&gt;Any tips or advice?&lt;/p&gt;
&lt;p&gt;Thanks!&lt;/p&gt;
</description>
<pubDate>Mon, 30 Oct 2017 19:57:31 +0000</pubDate>
<dc:creator>0bsidian</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://news.ycombinator.com/item?id=15588361</dc:identifier>
</item>
<item>
<title>Scaling the GitLab database</title>
<link>https://about.gitlab.com/2017/10/02/scaling-the-gitlab-database/</link>
<guid isPermaLink="true" >https://about.gitlab.com/2017/10/02/scaling-the-gitlab-database/</guid>
<description>&lt;p&gt;For a long time GitLab.com used a single PostgreSQL database server and a single replica for disaster recovery purposes. This worked reasonably well for the first few years of GitLab.com's existence, but over time we began seeing more and more problems with this setup. In this article we'll take a look at what we did to help solve these problems for both GitLab.com and self-hosted GitLab instances.&lt;/p&gt;
&lt;p&gt;For example, the database was under constant pressure, with CPU utilization hovering around 70 percent almost all the time. Not because we used all available resources in the best way possible, but because we were bombarding the server with too many (badly optimized) queries. We realized we needed a better setup that would allow us to balance the load and make GitLab.com more resilient to any problems that may occur on the primary database server.&lt;/p&gt;
&lt;p&gt;When tackling these problems using PostgreSQL there are essentially four techniques you can apply:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Optimize your application code so the queries are more efficient (and ideally use fewer resources).&lt;/li&gt;
&lt;li&gt;Use a connection pooler to reduce the number of database connections (and associated resources) necessary.&lt;/li&gt;
&lt;li&gt;Balance the load across multiple database servers.&lt;/li&gt;
&lt;li&gt;Shard your database.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Optimizing the application code is something we have been working on actively for the past two years, but it's not a final solution. Even if you improve performance, when traffic also increases you may still need to apply the other two techniques. For the sake of this article we'll skip over this particular subject and instead focus on the other techniques.&lt;/p&gt;
&lt;h2 id=&quot;connection-pooling&quot;&gt;Connection pooling&lt;/h2&gt;
&lt;p&gt;In PostgreSQL a connection is handled by starting an OS process which in turn needs a number of resources. The more connections (and thus processes), the more resources your database will use. PostgreSQL also enforces a maximum number of connections as defined in the &lt;a href=&quot;https://www.postgresql.org/docs/9.6/static/runtime-config-connection.html#GUC-MAX-CONNECTIONS&quot;&gt;max_connections&lt;/a&gt; setting. Once you hit this limit PostgreSQL will reject new connections. Such a setup can be illustrated using the following diagram:&lt;/p&gt;
&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;https://about.gitlab.com/images/scaling-the-gitlab-database/postgresql.svg&quot; alt=&quot;PostgreSQL Diagram&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Here our clients connect directly to PostgreSQL, thus requiring one connection per client.&lt;/p&gt;
&lt;p&gt;By pooling connections we can have multiple client-side connections reuse PostgreSQL connections. For example, without pooling we'd need 100 PostgreSQL connections to handle 100 client connections; with connection pooling we may only need 10 or so PostgreSQL connections depending on our configuration. This means our connection diagram will instead look something like the following:&lt;/p&gt;
&lt;p class=&quot;text-center&quot;&gt;&lt;img src=&quot;https://about.gitlab.com/images/scaling-the-gitlab-database/pooler.svg&quot; alt=&quot;Connection Pooling Diagram&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Here we show an example where four clients connect to pgbouncer but instead of using four PostgreSQL connections we only need two of them.&lt;/p&gt;
&lt;p&gt;For PostgreSQL there are two connection poolers that are most commonly used:&lt;/p&gt;
&lt;p&gt;pgpool is a bit special because it does much more than just connection pooling: it has a built-in query caching mechanism, can balance load across multiple databases, manage replication, and more.&lt;/p&gt;
&lt;p&gt;On the other hand pgbouncer is much simpler: all it does is connection pooling.&lt;/p&gt;
&lt;h2 id=&quot;database-load-balancing&quot;&gt;Database load balancing&lt;/h2&gt;
&lt;p&gt;Load balancing on the database level is typically done by making use of PostgreSQL's &quot;&lt;a href=&quot;https://www.postgresql.org/docs/9.6/static/hot-standby.html&quot;&gt;hot standby&lt;/a&gt;&quot; feature. A hot-standby is a PostgreSQL replica that allows you to run read-only SQL queries, contrary to a regular standby that does not allow any SQL queries to be executed. To balance load you'd set up one or more hot-standby servers and somehow balance read-only queries across these hosts while sending all other operations to the primary. Scaling such a setup is fairly easy: simply add more hot-standby servers (if necessary) as your read-only traffic increases.&lt;/p&gt;
&lt;p&gt;Another benefit of this approach is having a more resilient database cluster. Web requests that only use a secondary can continue to operate even if the primary server is experiencing issues; though of course you may still run into errors should those requests end up using the primary.&lt;/p&gt;
&lt;p&gt;This approach however can be quite difficult to implement. For example, explicit transactions must be executed on the primary since they may contain writes. Furthermore, after a write we want to continue using the primary for a little while because the changes may not yet be available on the hot-standby servers when using asynchronous replication.&lt;/p&gt;
&lt;h2 id=&quot;sharding&quot;&gt;Sharding&lt;/h2&gt;
&lt;p&gt;Sharding is the act of horizontally partitioning your data. This means that data resides on specific servers and is retrieved using a shard key. For example, you may partition data per project and use the project ID as the shard key. Sharding a database is interesting when you have a very high write load (as there's no other easy way of balancing writes other than perhaps a multi-master setup), or when you have &lt;em&gt;a lot&lt;/em&gt; of data and you can no longer store it in a conventional manner (e.g. you simply can't fit it all on a single disk).&lt;/p&gt;
&lt;p&gt;Unfortunately the process of setting up a sharded database is a massive undertaking, even when using software such as &lt;a href=&quot;https://www.citusdata.com/&quot;&gt;Citus&lt;/a&gt;. Not only do you need to set up the infrastructure (which varies in complexity depending on whether you run it yourself or use a hosted solution), but you also need to adjust large portions of your application to support sharding.&lt;/p&gt;
&lt;h3 id=&quot;cases-against-sharding&quot;&gt;Cases against sharding&lt;/h3&gt;
&lt;p&gt;On GitLab.com the write load is typically very low, with most of the database queries being read-only queries. In very exceptional cases we may spike to 1500 tuple writes per second, but most of the time we barely make it past 200 tuple writes per second. On the other hand we can easily read up to 10 million tuples per second on any given secondary.&lt;/p&gt;
&lt;p&gt;Storage-wise, we also don't use that much data: only about 800 GB. A large portion of this data is data that is being migrated in the background. Once those migrations are done we expect our database to shrink in size quite a bit.&lt;/p&gt;
&lt;p&gt;Then there's the amount of work required to adjust the application so all queries use the right shard keys. While quite a few of our queries usually include a project ID which we could use as a shard key, there are also many queries where this isn't the case. Sharding would also affect the process of contributing changes to GitLab as every contributor would now have to make sure a shard key is present in their queries.&lt;/p&gt;
&lt;p&gt;Finally, there is the infrastructure that's necessary to make all of this work. Servers have to be set up, monitoring has to be added, engineers have to be trained so they are familiar with this new setup, the list goes on. While hosted solutions may remove the need for managing your own servers it doesn't solve all problems. Engineers still have to be trained and (most likely very expensive) bills have to be paid. At GitLab we also highly prefer to ship the tools we need so the community can make use of them. This means that if we were going to shard the database we'd have to ship it (or at least parts of it) in our Omnibus packages. The only way you can make sure something you ship works is by running it yourself, meaning we wouldn't be able to use a hosted solution.&lt;/p&gt;
&lt;p&gt;Ultimately we decided against sharding the database because we felt it was an expensive, time-consuming, and complex solution to a problem we do not have.&lt;/p&gt;
&lt;h2 id=&quot;connection-pooling-for-gitlab&quot;&gt;Connection pooling for GitLab&lt;/h2&gt;
&lt;p&gt;For connection pooling we had two main requirements:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;It has to work well (obviously).&lt;/li&gt;
&lt;li&gt;It has to be easy to ship in our Omnibus packages so our users can also take advantage of the connection pooler.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Reviewing the two solutions (pgpool and pgbouncer) was done in two steps:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Perform various technical tests (does it work, how easy is it to configure, etc).&lt;/li&gt;
&lt;li&gt;Find out what the experiences are of other users of the solution, what problems they ran into and how they dealt with them, etc.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;pgpool was the first solution we looked into, mostly because it seemed quite attractive based on all the features it offered. Some of the data from our tests can be found in &lt;a href=&quot;https://gitlab.com/gitlab-com/infrastructure/issues/259#note_23464570&quot;&gt;this&lt;/a&gt; comment.&lt;/p&gt;
&lt;p&gt;Ultimately we decided against using pgpool based on a number of factors. For example, pgpool does not support sticky connections. This is problematic when performing a write and (trying to) display the results right away. Imagine creating an issue and being redirected to the page, only to run into an HTTP 404 error because the server used for any read-only queries did not yet have the data. One way to work around this would be to use synchronous replication, but this brings many other problems to the table; problems we prefer to avoid.&lt;/p&gt;
&lt;p&gt;Another problem is that pgpool's load balancing logic is decoupled from your application and operates by parsing SQL queries and sending them to the right server. Because this happens outside of your application you have very little control over which query runs where. This may actually be beneficial to some because you don't need additional application logic, but it also prevents you from adjusting the routing logic if necessary.&lt;/p&gt;
&lt;p&gt;Configuring pgpool also proved quite difficult due to the sheer number of configuration options. Perhaps the final nail in the coffin was the feedback we got on pgpool from those having used it in the past. The feedback we received regarding pgpool was usually negative, though not very detailed in most cases. While most of the complaints appeared to be related to earlier versions of pgpool it still made us doubt if using it was the right choice.&lt;/p&gt;
&lt;p&gt;The feedback combined with the issues described above ultimately led to us deciding against using pgpool and using pgbouncer instead. We performed a similar set of tests with pgbouncer and were very satisfied with it. It's fairly easy to configure (and doesn't have that much that needs configuring in the first place), relatively easy to ship, focuses only on connection pooling (and does it really well), and had very little (if any) noticeable overhead. Perhaps my only complaint would be that the pgbouncer website can be a little bit hard to navigate.&lt;/p&gt;
&lt;p&gt;Using pgbouncer we were able to drop the number of active PostgreSQL connections from a few hundred to only 10-20 by using transaction pooling. We opted for using transaction pooling since Rails database connections are persistent. In such a setup, using session pooling would prevent us from being able to reduce the number of PostgreSQL connections, thus brining few (if any) benefits. By using transaction pooling we were able to drop PostgreSQL's &lt;code&gt;max_connections&lt;/code&gt; setting from 3000 (the reason for this particular value was never really clear) to 300. pgbouncer is configured in such a way that even at peak capacity we will only need 200 connections; giving us some room for additional connections such as &lt;code&gt;psql&lt;/code&gt; consoles and maintenance tasks.&lt;/p&gt;
&lt;p&gt;A side effect of using transaction pooling is that you cannot use prepared statements, as the &lt;code&gt;PREPARE&lt;/code&gt; and &lt;code&gt;EXECUTE&lt;/code&gt; commands may end up running in different connections; producing errors as a result. Fortunately we did not measure any increase in response timings when disabling prepared statements, but we &lt;em&gt;did&lt;/em&gt; measure a reduction of roughly 20 GB in memory usage on our database servers.&lt;/p&gt;
&lt;p&gt;To ensure both web requests and background jobs have connections available we set up two separate pools: one pool of 150 connections for background processing, and a pool of 50 connections for web requests. For web requests we rarely need more than 20 connections, but for background processing we can easily spike to a 100 connections simply due to the large number of background processes running on GitLab.com.&lt;/p&gt;
&lt;p&gt;Today we ship pgbouncer as part of GitLab EE's High Availability package. For more information you can refer to &lt;a href=&quot;https://docs.gitlab.com/ee/administration/high_availability/alpha_database.html&quot;&gt;&quot;Omnibus GitLab PostgreSQL High Availability.&quot;&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;database-load-balancing-for-gitlab&quot;&gt;Database load balancing for GitLab&lt;/h2&gt;
&lt;p&gt;With pgpool and its load balancing feature out of the picture we needed something else to spread load across multiple hot-standby servers.&lt;/p&gt;
&lt;p&gt;For (but not limited to) Rails applications there is a library called &lt;a href=&quot;https://github.com/taskrabbit/makara&quot;&gt;Makara&lt;/a&gt; which implements load balancing logic and includes a default implementation for ActiveRecord. Makara however has some problems that were a deal-breaker for us. For example, its support for sticky connections is very limited: when you perform a write the connection will stick to the primary using a cookie, with a fixed TTL. This means that if replication lag is greater than the TTL you may still end up running a query on a host that doesn't have the data you need.&lt;/p&gt;
&lt;p&gt;Makara also requires you to configure quite a lot, such as all the database hosts and their roles, with no service discovery mechanism (our current solution does not yet support this either, though it's planned for the near future). Makara also &lt;a href=&quot;https://github.com/taskrabbit/makara/issues/151&quot;&gt;does not appear to be thread-safe&lt;/a&gt;, which is problematic since Sidekiq (the background processing system we use) is multi-threaded. Finally, we wanted to have control over the load balancing logic as much as possible.&lt;/p&gt;
&lt;p&gt;Besides Makara there's also &lt;a href=&quot;https://github.com/thiagopradi/octopus&quot;&gt;Octopus&lt;/a&gt; which has some load balancing mechanisms built in. Octopus however is geared towards database sharding and not just balancing of read-only queries. As a result we did not consider using Octopus.&lt;/p&gt;
&lt;p&gt;Ultimately this led to us building our own solution directly into GitLab EE. The merge request adding the initial implementation can be found &lt;a href=&quot;https://gitlab.com/gitlab-org/gitlab-ee/merge_requests/1283&quot;&gt;here&lt;/a&gt;, though some changes, improvements, and fixes were applied later on.&lt;/p&gt;
&lt;p&gt;Our solution essentially works by replacing &lt;code&gt;ActiveRecord::Base.connection&lt;/code&gt; with a proxy object that handles routing of queries. This ensures we can load balance as many queries as possible, even queries that don't originate directly from our own code. This proxy object in turn determines what host a query is sent to based on the methods called, removing the need for parsing SQL queries.&lt;/p&gt;
&lt;h3 id=&quot;sticky-connections&quot;&gt;Sticky connections&lt;/h3&gt;
&lt;p&gt;Sticky connections are supported by storing a pointer to the current PostgreSQL WAL position the moment a write is performed. This pointer is then stored in Redis for a short duration at the end of a request. Each user is given their own key so that the actions of one user won't lead to all other users being affected. In the next request we get the pointer and compare this with all the secondaries. If all secondaries have a WAL pointer that exceeds our pointer we know they are in sync and we can safely use a secondary for our read-only queries. If one or more secondaries are not yet in sync we will continue using the primary until they are in sync. If no write is performed for 30 seconds and all the secondaries are still not in sync we'll revert to using the secondaries in order to prevent somebody from ending up running queries on the primary forever.&lt;/p&gt;
&lt;p&gt;Checking if a secondary has caught up is quite simple and is implemented in &lt;code&gt;Gitlab::Database::LoadBalancing::Host#caught_up?&lt;/code&gt; as follows:&lt;/p&gt;
&lt;pre class=&quot;highlight ruby&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;caught_up?&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;location&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;quote&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;location&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;query&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;SELECT NOT pg_is_in_recovery() OR &quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;pg_xlog_location_diff(pg_last_xlog_replay_location(), &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;) &amp;gt;= 0 AS result&quot;&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;select_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;first&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'result'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'t'&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ensure&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;release_connection&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Most of the code here is standard Rails code to run raw queries and grab the results. The most interesting part is the query itself, which is as follows:&lt;/p&gt;
&lt;pre class=&quot;highlight sql&quot;&gt;
&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NOT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pg_is_in_recovery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;OR&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pg_xlog_location_diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pg_last_xlog_replay_location&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WAL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;POINTER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;&quot;
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;WAL-POINTER&lt;/code&gt; is the WAL pointer as returned by the PostgreSQL function &lt;code&gt;pg_current_xlog_insert_location()&lt;/code&gt;, which is executed on the primary. In the above code snippet the pointer is passed as an argument, which is then quoted/escaped and passed to the query.&lt;/p&gt;
&lt;p&gt;Using the function &lt;code&gt;pg_last_xlog_replay_location()&lt;/code&gt; we can get the WAL pointer of a secondary, which we can then compare to our primary pointer using &lt;code&gt;pg_xlog_location_diff()&lt;/code&gt;. If the result is greater than 0 we know the secondary is in sync.&lt;/p&gt;
&lt;p&gt;The check &lt;code&gt;NOT pg_is_in_recovery()&lt;/code&gt; is added to ensure the query won't fail when a secondary that we're checking was &lt;em&gt;just&lt;/em&gt; promoted to a primary and our GitLab process is not yet aware of this. In such a case we simply return &lt;code&gt;true&lt;/code&gt; since the primary is always in sync with itself.&lt;/p&gt;
&lt;h3 id=&quot;background-processing&quot;&gt;Background processing&lt;/h3&gt;
&lt;p&gt;Our background processing code &lt;em&gt;always&lt;/em&gt; uses the primary since most of the work performed in the background consists of writes. Furthermore we can't reliably use a hot-standby as we have no way of knowing whether a job should use the primary or not as many jobs are not directly tied into a user.&lt;/p&gt;
&lt;h3 id=&quot;connection-errors&quot;&gt;Connection errors&lt;/h3&gt;
&lt;p&gt;To deal with connection errors our load balancer will not use a secondary if it is deemed to be offline, plus connection errors on any host (including the primary) will result in the load balancer retrying the operation a few times. This ensures that we don't immediately display an error page in the event of a hiccup or a database failover. While we also deal with &lt;a href=&quot;https://www.postgresql.org/docs/current/static/hot-standby.html#HOT-STANDBY-CONFLICT&quot;&gt;hot standby conflicts&lt;/a&gt; on the load balancer level we ended up enabling &lt;code&gt;hot_standby_feedback&lt;/code&gt; on our secondaries as doing so solved all hot-standby conflicts without having any negative impact on table bloat.&lt;/p&gt;
&lt;p&gt;The procedure we use is quite simple: for a secondary we'll retry a few times with no delay in between. For a primary we'll retry the operation a few times using an exponential backoff.&lt;/p&gt;
&lt;p&gt;For more information you can refer to the source code in GitLab EE:&lt;/p&gt;
&lt;p&gt;Database load balancing was first introduced in GitLab 9.0 and &lt;em&gt;only&lt;/em&gt; supports PostgreSQL. More information can be found in the &lt;a href=&quot;https://about.gitlab.com/2017/03/22/gitlab-9-0-released/&quot;&gt;9.0 release post&lt;/a&gt; and the &lt;a href=&quot;https://docs.gitlab.com/ee/administration/database_load_balancing.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;crunchy-data&quot;&gt;Crunchy Data&lt;/h2&gt;
&lt;p&gt;In parallel to working on implementing connection pooling and load balancing we were working with &lt;a href=&quot;https://www.crunchydata.com/&quot;&gt;Crunchy Data&lt;/a&gt;. Until very recently I was the only &lt;a href=&quot;https://about.gitlab.com/handbook/infrastructure/database/&quot;&gt;database specialist&lt;/a&gt; which meant I had a lot of work on my plate. Furthermore my knowledge of PostgreSQL internals and its wide range of settings is limited (or at least was at the time), meaning there's only so much I could do. Because of this we hired Crunchy to help us out with identifying problems, investigating slow queries, proposing schema optimisations, optimising PostgreSQL settings, and much more.&lt;/p&gt;
&lt;p&gt;For the duration of this cooperation most work was performed in confidential issues so we could share private data such as log files. With the cooperation coming to an end we have removed sensitive information from some of these issues and opened them up to the public. The primary issue was &lt;a href=&quot;https://gitlab.com/gitlab-com/infrastructure/issues/1448&quot;&gt;gitlab-com/infrastructure#1448&lt;/a&gt;, which in turn led to many separate issues being created and resolved.&lt;/p&gt;
&lt;p&gt;The benefit of this cooperation was immense as it helped us identify and solve many problems, something that would have taken me months to identify and solve if I had to do this all by myself.&lt;/p&gt;
&lt;p&gt;Fortunately we recently managed to hire our &lt;a href=&quot;https://gitlab.com/_stark&quot;&gt;second database specialist&lt;/a&gt; and we hope to grow the team more in the coming months.&lt;/p&gt;
&lt;h2 id=&quot;combining-connection-pooling-and-database-load-balancing&quot;&gt;Combining connection pooling and database load balancing&lt;/h2&gt;
&lt;p&gt;Combining connection pooling and database load balancing allowed us to drastically reduce the number of resources necessary to run our database cluster as well as spread load across our hot-standby servers. For example, instead of our primary having a near constant CPU utilisation of 70 percent today it usually hovers between 10 percent and 20 percent, while our two hot-standby servers hover around 20 percent most of the time:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://about.gitlab.com/images/scaling-the-gitlab-database/cpu-percentage.png&quot; alt=&quot;CPU Percentage&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Here &lt;code&gt;db3.cluster.gitlab.com&lt;/code&gt; is our primary while the other two hosts are our secondaries.&lt;/p&gt;
&lt;p&gt;Other load-related factors such as load averages, disk usage, and memory usage were also drastically improved. For example, instead of the primary having a load average of around 20 it barely goes above an average of 10:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://about.gitlab.com/images/scaling-the-gitlab-database/load-averages.png&quot; alt=&quot;CPU Percentage&quot;/&gt;&lt;/p&gt;
&lt;p&gt;During the busiest hours our secondaries serve around 12 000 transactions per second (roughly 740 000 per minute), while the primary serves around 6 000 transactions per second (roughly 340 000 per minute):&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://about.gitlab.com/images/scaling-the-gitlab-database/transactions.png&quot; alt=&quot;Transactions Per Second&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately we don't have any data on the transaction rates prior to deploying pgbouncer and our database load balancer.&lt;/p&gt;
&lt;p&gt;An up-to-date overview of our PostgreSQL statistics can be found at our &lt;a href=&quot;http://monitor.gitlab.net/dashboard/db/postgres-stats?refresh=5m&amp;amp;orgId=1&quot;&gt;public Grafana dashboard&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some of the settings we have set for pgbouncer are as follows:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Setting&lt;/th&gt;
&lt;th&gt;Value&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;default_pool_size&lt;/td&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;reserve_pool_size&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;reserve_pool_timeout&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;max_client_conn&lt;/td&gt;
&lt;td&gt;2048&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;pool_mode&lt;/td&gt;
&lt;td&gt;transaction&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;server_idle_timeout&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;With that all said there is still some work left to be done such as: implementing service discovery (&lt;a href=&quot;https://gitlab.com/gitlab-org/gitlab-ee/issues/2042&quot;&gt;#2042&lt;/a&gt;), improving how we check if a secondary is available (&lt;a href=&quot;https://gitlab.com/gitlab-org/gitlab-ee/issues/2866&quot;&gt;#2866&lt;/a&gt;), and ignoring secondaries that are too far behind the primary (&lt;a href=&quot;https://gitlab.com/gitlab-org/gitlab-ee/issues/2197&quot;&gt;#2197&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;It's worth mentioning that we currently do not have any plans of turning our load balancing solution into a standalone library that you can use outside of GitLab, instead our focus is on providing a solid load balancing solution for GitLab EE.&lt;/p&gt;
&lt;p&gt;If this has gotten you interested and you enjoy working with databases, improving application performance, and adding database-related features to GitLab (such as &lt;a href=&quot;https://gitlab.com/gitlab-org/gitlab-ee/issues/2042&quot;&gt;service discovery&lt;/a&gt;) you should definitely check out the &lt;a href=&quot;https://about.gitlab.com/jobs/specialist/database/&quot;&gt;job opening&lt;/a&gt; and the &lt;a href=&quot;https://about.gitlab.com/handbook/infrastructure/database/&quot;&gt;database specialist handbook entry&lt;/a&gt; for more information.&lt;/p&gt;

</description>
<pubDate>Mon, 30 Oct 2017 16:43:04 +0000</pubDate>
<dc:creator>fanf2</dc:creator>
<og:title>Scaling the GitLab database</og:title>
<og:type>article</og:type>
<og:url>https://about.gitlab.com/2017/10/02/scaling-the-gitlab-database/</og:url>
<og:image>https://about.gitlab.com/images/tweets/scaling-gitlab-database-tweet.png</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://about.gitlab.com/2017/10/02/scaling-the-gitlab-database/</dc:identifier>
</item>
<item>
<title>The State of Data Science and Machine Learning</title>
<link>https://www.kaggle.com/surveys/2017</link>
<guid isPermaLink="true" >https://www.kaggle.com/surveys/2017</guid>
<description>[unable to retrieve full-text content]&lt;p&gt;Article URL: &lt;a href=&quot;https://www.kaggle.com/surveys/2017&quot;&gt;https://www.kaggle.com/surveys/2017&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=15586323&quot;&gt;https://news.ycombinator.com/item?id=15586323&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Points: 235&lt;/p&gt;&lt;p&gt;# Comments: 42&lt;/p&gt;</description>
<pubDate>Mon, 30 Oct 2017 16:26:41 +0000</pubDate>
<dc:creator>ishan_dikshit</dc:creator>
<og:title>The State of ML and Data Science 2017</og:title>
<og:url>https://www.kaggle.com/surveys/2017</og:url>
<og:description>A big picture view of the state of data science and machine learning that shares who is working with data, whatâ€™s happening at the cutting edge of machine learning across industries, and how new data scientists can best break into the field.</og:description>
<og:type>article</og:type>
<og:image>https://polygraph-cool.github.io/kaggle_survey/assets/img/og-img.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.kaggle.com/surveys/2017</dc:identifier>
</item>
<item>
<title>Do you need a VPN?</title>
<link>https://blog.mozilla.org/internetcitizen/2017/08/29/do-you-need-a-vpn</link>
<guid isPermaLink="true" >https://blog.mozilla.org/internetcitizen/2017/08/29/do-you-need-a-vpn</guid>
<description>&lt;p&gt;&lt;strong&gt;E&lt;/strong&gt;arly this year, the &lt;a href=&quot;http://www.npr.org/sections/alltechconsidered/2017/03/28/521813464/as-congress-repeals-internet-privacy-rules-putting-your-options-in-perspective&quot;&gt;U.S. Congress rolled back Internet privacy rules&lt;/a&gt;, giving service providers free reign to track, store and sell browsing data. In July, the U.S. Department of Justice (DoJ) &lt;a href=&quot;http://www.npr.org/sections/thetwo-way/2017/08/15/543782396/doj-demands-files-on-anti-trump-activists-and-a-web-hosting-company-resists&quot;&gt;issued a warrant to DreamHost&lt;/a&gt;, asking for a list of everyone &lt;a href=&quot;https://www.theguardian.com/world/2017/aug/14/donald-trump-inauguration-protest-website-search-warrant-dreamhost&quot;&gt;who visited DisruptJ20.org&lt;/a&gt; — a site used to plan protests at President Trump’s inauguration. Both events raise important questions about online privacy, and many consumers are turning to Virtual Private Networks (VPN).&lt;/p&gt;
&lt;h2&gt;More private than private browsing&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.mozilla.org/firefox/features/private-browsing/?utm_source=blog.mozilla.org&amp;amp;utm_campaign=internet_citizen&amp;amp;utm_medium=referral&quot;&gt;Firefox private browsing with tracking protection&lt;/a&gt; is great at protecting you from invasive trackers and keeping your browser history secret, but when you surf the web, you leave footprints that Firefox can’t erase — your IP address is logged at the sites you visit and your ISP may keep records. That usually isn’t an issue, but the sites you visit could expose you to &lt;a href=&quot;http://www.npr.org/sections/thetwo-way/2017/08/15/543782396/doj-demands-files-on-anti-trump-activists-and-a-web-hosting-company-resists&quot;&gt;unwanted attention from government agencies&lt;/a&gt; or even hackers. A VPN can hide those footprints from prying eyes and add an extra layer of security against hackers.&lt;/p&gt;
&lt;h2&gt;What is a VPN?&lt;/h2&gt;
&lt;p&gt;A VPN is a secure connection between your computer and a server. All your Internet traffic and browsing data goes through that remote server. To the outside world, the anonymous server is doing the browsing, not you. ISPs, government agencies, hackers or anyone else can’t track your activity online.&lt;/p&gt;
&lt;h2&gt;How does a VPN work?&lt;/h2&gt;
&lt;p&gt;When you connect to a VPN, you create a secure, encrypted tunnel between your computer and the VPN remote server. The data is essentially gibberish to anyone who intercepts it. Your ISP, government or hackers won’t know which websites you visit. And conversely, the websites you visit won’t know where you are. Typically, logging in to a VPN is as easy as entering a password and clicking a button on a VPN client or a &lt;a href=&quot;https://addons.mozilla.org&quot;&gt;web browser extension&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Are VPNs truly private?&lt;/h2&gt;
&lt;p&gt;Unfortunately, no. The VPN provider can still log your browsing data. You are essentially putting your trust in your VPN provider. Will your provider hand over info when pressed? Will they log your browser data and sell it at a later date?&lt;/p&gt;
&lt;p&gt;ManyVPN providers are trustworthy and vow to keep customer info private, but some are downright nefarious. Researchers &lt;a href=&quot;https://research.csiro.au/ng/wp-content/uploads/sites/106/2016/08/paper-1.pdf&quot;&gt;recently tested 300 free VPN apps&lt;/a&gt; on Google Play and found that nearly 40 percent installed malware or &lt;a href=&quot;https://www.cisecurity.org/malvertising/&quot;&gt;malvertising&lt;/a&gt; on users’ machines. (NPR has a &lt;a href=&quot;http://www.npr.org/sections/alltechconsidered/2017/08/17/543716811/turning-to-vpns-for-online-privacy-you-might-be-putting-your-data-at-risk&quot;&gt;brilliant article about VPNs and privacy here&lt;/a&gt;.) The lesson? Pick a VPN provider you can really trust.&lt;/p&gt;
&lt;p&gt;Also realize that there are things beyond your VPN provider’s control. If you live under a repressive regime a VPN might not let you tunnel under restrictions to access blocked sites.&lt;/p&gt;
&lt;h2&gt;How do you pick a VPN provider?&lt;/h2&gt;
&lt;p&gt;It may be tempting to turn to a free VPN provider, but many simply don’t deliver a great experience. Some sell your data (anonymized) to advertisers in order to survive. Other VPN services run ads. Some may be free and secure, but are painfully slow.&lt;/p&gt;
&lt;p&gt;It can also be tricky to pick a good paid VPN service. For example, a provider may offer secure connections and ultimate privacy, but a limited number of server locations. Your browsing data may not be as anonymized as you’d like.&lt;/p&gt;
&lt;p&gt;Here are some questions you should ask when considering a VPN provider:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;What kind of data, if any, does the VPN provider collect about your browsing?&lt;/li&gt;
&lt;li&gt;How long does it keep this data?&lt;/li&gt;
&lt;li&gt;Are there any restrictions?&lt;/li&gt;
&lt;li&gt;Where are the VPN servers?&lt;/li&gt;
&lt;li&gt;How do you pay for the VPN service?&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;That last question can be really tricky. If you pay for the VPN service with credit card or PayPal, how private will it be? If you’re after ultimate privacy and security, look for a service that accepts payment from anonymous services like Bitcoin.&lt;/p&gt;
&lt;p&gt;There are many, many VPN providers, and Mozilla can’t recommend any specific service. &lt;a href=&quot;http://www.pcworld.com/article/3198369/privacy/best-vpn-services-apps-reviews-buying-advice.html&quot;&gt;PC World, however, has reviewed a ton of VPN services&lt;/a&gt; and ranked them all. The winner? Currently it’s Sweden-based &lt;a href=&quot;https://www.mullvad.net/&quot;&gt;Mullvad&lt;/a&gt;, which doesn’t even keep your email address. Instead, it auto-generates an anonymous account number when you create an account. The service is &lt;a href=&quot;http://www.pcworld.com/article/3201724/data-center-cloud/mullvad-vpn-review.html&quot;&gt;reportedly fast and ultra secure&lt;/a&gt;. Speedy services &lt;a href=&quot;https://www.cyberghostvpn.com&quot;&gt;CyberGhost&lt;/a&gt; and &lt;a href=&quot;https://www.tunnelbear.com/&quot;&gt;TunnelBear&lt;/a&gt; also ranked highly, and &lt;a href=&quot;https://go.nordvpn.net/&quot;&gt;NordVPN&lt;/a&gt; was called out as a great way to watch blocked U.S. Netflix shows and other services while abroad. &lt;a href=&quot;https://getfoxyproxy.org/&quot;&gt;FoxyProxy&lt;/a&gt;, by Mozillian Eric Jung, offers VPN service in more than 68 countries.&lt;/p&gt;
&lt;p&gt;Additionally, there are &lt;a href=&quot;https://addons.mozilla.org/en-US/firefox/search/?q=vpn&amp;amp;appver=&amp;amp;platform=&quot;&gt;many browser extensions&lt;/a&gt; that can add a VPN service right to your browser, protecting your browser data as you surf.&lt;/p&gt;
&lt;p&gt;VPNs are surging in popularity, thanks to &lt;a href=&quot;http://www.npr.org/sections/alltechconsidered/2017/03/28/521813464/as-congress-repeals-internet-privacy-rules-putting-your-options-in-perspective&quot;&gt;recent rollbacks in Internet privacy rules&lt;/a&gt; and &lt;a href=&quot;http://www.npr.org/sections/thetwo-way/2017/08/15/543782396/doj-demands-files-on-anti-trump-activists-and-a-web-hosting-company-resists&quot;&gt;government sleuthing&lt;/a&gt;. But they may not be as safe as they claim. Do your research, ask the right questions, and make sure you find a service provider that delivers on their promise.&lt;/p&gt;
</description>
<pubDate>Mon, 30 Oct 2017 15:35:40 +0000</pubDate>
<dc:creator>thesumofall</dc:creator>
<og:url>https://blog.mozilla.org/internetcitizen/2017/08/29/do-you-need-a-vpn</og:url>
<og:title>Do you need a VPN? | Mozilla Internet Citizen</og:title>
<og:description>VPN usage is surging, thanks to recent rollbacks in Internet privacy rules and government sleuthing. But they may not be as safe as they claim.</og:description>
<og:image>https://blog.mozilla.org/internetcitizen/files/2017/08/moz_irl_social_surveillance_creepy-eyes-ears_pattern-1400x770.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.mozilla.org/internetcitizen/2017/08/29/do-you-need-a-vpn/</dc:identifier>
</item>
<item>
<title>Google, Facebook, and Amazon have fundamentally transformed the web</title>
<link>https://staltz.com/the-web-began-dying-in-2014-heres-how.html</link>
<guid isPermaLink="true" >https://staltz.com/the-web-began-dying-in-2014-heres-how.html</guid>
<description>&lt;div class=&quot;centered&quot;&gt;

&lt;h2 class=&quot;post-title&quot;&gt;The Web began dying in 2014, here's how&lt;/h2&gt;
&lt;h3 class=&quot;post-date&quot;&gt;30 Oct 2017&lt;/h3&gt;
&lt;/div&gt;
&lt;p&gt;Before the year 2014, there were many people using Google, Facebook, and Amazon. Today, there are still many people using services from those three tech giants (respectively, GOOG, FB, AMZN). Not much has changed, and quite literally the user interface and features on those sites has remained mostly untouched. However, the underlying dynamics of power on the Web have drastically changed, and those three companies are at the center of a fundamental transformation of the Web.&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;em&gt;It looks like nothing changed since 2014, but GOOG and FB now have direct influence over 70%+ of internet traffic.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Internet activity itself hasn’t slowed down. It maintains a steady growth, both in amount of users and amount of websites:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://staltz.com/img/chart-sites-and-users.png&quot;&gt;&lt;img src=&quot;https://staltz.com/img/chart-sites-and-users.png&quot; alt=&quot;Amount of internet users and websites from 2011 to 2017&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(Sources: &lt;a href=&quot;https://news.netcraft.com/archives/category/web-server-survey&quot;&gt;https://news.netcraft.com/archives/category/web-server-survey&lt;/a&gt; and &lt;a href=&quot;http://www.internetlivestats.com/internet-users/&quot;&gt;http://www.internetlivestats.com/internet-users/&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;What has changed over the last 4 years is market share of traffic on the Web. It looks like nothing has changed, but GOOG and FB now have direct influence over 70%+ of internet traffic. Mobile internet traffic is now the &lt;a href=&quot;https://www.statista.com/statistics/306528/share-of-mobile-internet-traffic-in-global-regions/&quot;&gt;majority of traffic worldwide&lt;/a&gt; and in Latin America alone, GOOG and FB services have had 60% of mobile traffic in 2015, growing to 70% by the end of 2016. The remaining 30% of traffic is shared among all other mobile apps and websites. Mobile devices are primarily used for accessing GOOG and FB networks.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://staltz.com/img/internet-latin-america-2016.png&quot;&gt;&lt;img src=&quot;https://staltz.com/img/internet-latin-america-2016.png&quot; alt=&quot;Share of internet traffic in Latin America from 2015 to 2016&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(Source: &lt;a href=&quot;https://www.sandvine.com/resources/global-internet-phenomena/2016/north-america-and-latin-america.html&quot;&gt;https://www.sandvine.com/resources/global-internet-phenomena/2016/north-america-and-latin-america.html&lt;/a&gt;)&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;em&gt;The press, unlike before, depends on GOOG-FB to stay in business.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Another demonstration of GOOG and FB dominance can be seen among media websites. The most popular web properties that don’t belong to GOOG nor FB are usually from the press. For instance, in the USA there are &lt;a href=&quot;https://www.statista.com/statistics/271412/most-visited-us-web-properties-based-on-number-of-visitors/&quot;&gt;6 media sites in the top 10 websites&lt;/a&gt;; in Brazil there are &lt;a href=&quot;https://www.statista.com/statistics/254727/most-visited-web-properties-in-brazil/&quot;&gt;6 media sites in the top 10&lt;/a&gt;; in UK it is &lt;a href=&quot;https://www.statista.com/statistics/272871/leading-internet-properties-in-the-uk-by-unique-visitors/&quot;&gt;5 out 10&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;From where do media sites get their traffic? Prior to 2014, Search Engine Optimization (SEO) was a common practice among Web Developers to improve their site for Google searches, since it accounted for approximately 35% of traffic, while more than 50% of traffic came from various other places on the Web. SEO was important, while Facebook presence was nice-to-have. Over the next 3 years, traffic from Facebook grew to be approximately &lt;a href=&quot;http://www.reuters.com/article/us-usa-internet-socialmedia/two-thirds-of-american-adults-get-news-from-social-media-survey-idUSKCN1BJ2A8&quot;&gt;45%&lt;/a&gt;, surpassing the status that Search traffic had. In 2017, the Media depends on both Google and Facebook for page views, since it’s the majority of their traffic.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://staltz.com/img/referral-to-top-publishers.png&quot;&gt;&lt;img src=&quot;https://staltz.com/img/referral-to-top-publishers.png&quot; alt=&quot;Referral source of traffic to top web publishers&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(Source: &lt;a href=&quot;https://blog.parse.ly/post/2855/facebook-continues-to-beat-google-in-sending-traffic-to-top-publishers/&quot;&gt;https://blog.parse.ly/post/2855/facebook-continues-to-beat-google-in-sending-traffic-to-top-publishers/&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The relationship between media sites and the two tech giants is &lt;a href=&quot;https://www.theverge.com/2017/7/10/15948196&quot;&gt;difficult&lt;/a&gt;. In 2014, FB built Facebook Paper as an attempt to have a larger control over news consumption. Their tactic failed, but their strategy persisted through different means such as Facebook Instant Articles. The media, being dependent on social traffic and threatened by the social behemoth, reacted. They &lt;a href=&quot;https://www.theverge.com/2017/4/16/15314210&quot;&gt;pulled out support for Instant Articles&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Meanwhile, GOOG notices how its Search traffic hadn’t improved, while Facebook had picked up steam, so GOOG launches their Instant Articles alternative called Accelerated Mobile Pages (AMP) and proactively starts serving articles from GOOG servers instead of directing traffic to media sites. The press reacts similarly to how they did for FB: reported bold stories about the Search behemoth’s thirst for control over news consumption.&lt;/p&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;&lt;em&gt;GOOG and FB ceased competing directly, focusing on what they do best instead.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Data shows FB has dramatically improved its dominance on the Web, while Google Search hasn’t significantly changed. How exactly did FB achieve that, and what events were key to that development? Prior to 2014, both companies had a portfolio of multiple web services. GOOG hadn’t yet become Alphabet, so it’s focus was difused. GOOG was trying to enter the social market, first with Google Wave, then Google Buzz, Orkut, and Google+. In total, &lt;a href=&quot;https://www.geckoboard.com/tech-acquisitions/&quot;&gt;GOOG has acquired 18 companies from the social media category&lt;/a&gt;, of which only 1 acquisition happened post-2014, while 5 of those happened in 2010 alone. FB was competing in the search market, through Bing, in partnership with MSFT.&lt;/p&gt;
&lt;p&gt;During 2014, FB apparently reorganized itself to focus on social only. In February, it bought WhatsApp, for 11 times the price GOOG bought YouTube. In December, it canceled its Bing partnership with MSFT. User retention on Facebook.com grew steadily (see chart below). Through its four simple products, Facebook, WhatsApp, Messenger, and Instagram, FB had become the social superpower.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://staltz.com/img/facebook-retention.png&quot;&gt;&lt;img src=&quot;https://staltz.com/img/facebook-retention.png&quot; alt=&quot;Facebook daily active users (DAU) divided by Facebook monthly active users (MAU)&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(Sources: &lt;a href=&quot;https://www.statista.com/statistics/346167/facebook-global-dau/&quot;&gt;https://www.statista.com/statistics/346167/facebook-global-dau/&lt;/a&gt; and &lt;a href=&quot;https://www.statista.com/statistics/264810/number-of-monthly-active-facebook-users-worldwide/&quot;&gt;https://www.statista.com/statistics/264810/number-of-monthly-active-facebook-users-worldwide/&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Similarly, GOOG in 2014 started reorganizing itself to focus on artificial intelligence only. In January 2014, GOOG bought DeepMind, and in September they shutdown Orkut (one of their few social products which had momentary success in some countries) forever. The Alphabet Inc restructuring was announced in August 2015 but it likely took many months of meetings and bureaucracy. The restructuring was important to focus the web-oriented departments at GOOG towards a simple mission. GOOG sees no future in the simple Search market, and announces to be migrating “From Search to Suggest” (in Eric Schmidt’s &lt;a href=&quot;https://www.youtube.com/watch?v=Hg_KxXhhsGg&quot;&gt;own words&lt;/a&gt;) and being an “AI first company” (in Sundar Pichai’s &lt;a href=&quot;https://www.youtube.com/watch?v=8Og2BnpBhkM&quot;&gt;own words&lt;/a&gt;). GOOG is currently slightly behind FB in terms of how fast it is growing its dominance of the web, but due to their technical expertise, vast budget, influence and vision, in the long run its AI assets will play a massive role on the internet. They know what they are doing.&lt;/p&gt;
&lt;p&gt;These are no longer the same companies as 4 years ago. GOOG is not anymore an internet company, it’s &lt;em&gt;the knowledge internet company&lt;/em&gt;. FB is not an internet company, it’s &lt;em&gt;the social internet company&lt;/em&gt;. They used to attempt to compete, and this competition kept the internet market diverse. Today, however, they seem mostly satisfied with their orthogonal dominance of parts of the Web, and we are losing diversity of choices. Which leads us to another part of the internet: e-commerce and AMZN.&lt;/p&gt;
&lt;p&gt;AMZN does not focus on making profit.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://staltz.com/img/amazon-profits.png&quot;&gt;&lt;img src=&quot;https://staltz.com/img/amazon-profits.png&quot; alt=&quot;Amazon's Long-Term Growth&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(Sources: &lt;a href=&quot;https://www.statista.com/statistics/346167/facebook-global-dau/&quot;&gt;https://www.statista.com/statistics/346167/facebook-global-dau/&lt;/a&gt; and &lt;a href=&quot;https://www.statista.com/statistics/264810/number-of-monthly-active-facebook-users-worldwide/&quot;&gt;https://www.statista.com/statistics/264810/number-of-monthly-active-facebook-users-worldwide/&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Instead, its mission is to seek market leadership, crushing competitors in the USA.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://staltz.com/img/amazon-outgrown-competitors.jpg&quot;&gt;&lt;img src=&quot;https://staltz.com/img/amazon-outgrown-competitors.jpg&quot; alt=&quot;Amazon has outgrown competitors&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I could elaborate on how AMZN is &lt;em&gt;the e-commerce company&lt;/em&gt;, but I would be just repeating &lt;a href=&quot;https://www.youtube.com/watch?v=GWBjUsmO-Lw&quot;&gt;Scott Galloway’s exposure of this topic&lt;/a&gt;. It’s worth watching his talks.&lt;/p&gt;
&lt;h2 id=&quot;what-the-web-was-and-what-it-became&quot;&gt;What the Web was and what it became&lt;/h2&gt;
&lt;p&gt;The events and data above describe how three internet companies have acquired massive influence on the Web, but why does that imply the beginning of the Web’s death? To answer that, we need to reflect on what the Web is.&lt;/p&gt;
&lt;p&gt;The original vision for the Web according to its creator, Tim Berners-Lee, was a space with multilateral publishing and consumption of information. It was a peer-to-peer vision with no dependency on a single party. Tim himself claims &lt;a href=&quot;https://www.theguardian.com/technology/2017/mar/11/tim-berners-lee-web-inventor-save-internet&quot;&gt;the Web is dying&lt;/a&gt;: the Web he wanted and the Web he got are no longer the same.&lt;/p&gt;
&lt;blockquote readability=&quot;44.753170731707&quot;&gt;
&lt;p&gt;&lt;strong&gt;Doesn’t GOOG defend in the open Web?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;GOOG, as a company born from the Web, has helped take it forward both technologically and in adoption. That is undeniable. They still lead efforts to improve the open Web, such as &lt;a href=&quot;https://developers.google.com/web/progressive-web-apps/&quot;&gt;advocacy of Progressive Web Apps (PWAs)&lt;/a&gt; over native mobile apps.&lt;/p&gt;
&lt;p&gt;Isn’t GOOG trying to guarantee the open Web stays alive? Not necessarily. GOOG’s goal is to gather as much rich data as possible, and build AI. Their mission is to have an AI provide timely and personalized information to us, not specifically to have websites provide information. Any GOOG concerted efforts are aligned to the AI mission.&lt;/p&gt;
&lt;p&gt;Mobile usage is on the rise – having already crossed desktop as the primary channel for internet usage – and native mobile apps are so far the best way of providing good user experience on mobile. GOOG collects little or no data from native mobile apps, to some extent on Android, but specially on iOS. PWAs happen to live in the neutral and open Web, and are better suited for data collection while providing great user experience on mobile.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://staltz.com/img/desktop-vs-mobile-2.jpg&quot;&gt;&lt;img src=&quot;https://staltz.com/img/desktop-vs-mobile-2.jpg&quot; alt=&quot;Desktop versus Mobile internet usage&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GOOG promotes lock-in and proprietary technologies such as Firebase and Google-dependent AMP installations as much as it advocates open PWAs. GOOG does not consistently defend the open Web. They &lt;a href=&quot;https://www.eff.org/deeplinks/2013/05/google-abandons-open-standards-instant-messaging&quot;&gt;dropped XMPP in Gtalk&lt;/a&gt;, and Gtalk itself was deprecated, favoring Google Hangouts with a proprietary protocol. Chrome Web Store is a walled garden like App Store. They &lt;a href=&quot;http://edition.cnn.com/2013/03/14/tech/web/google-reader-discontinued/index.html&quot;&gt;shutdown Google Reader&lt;/a&gt; based on RSS, an open standard. &lt;a href=&quot;https://www.forbes.com/sites/moorinsights/2017/05/22/google-cloud-tpu-strategic-implications-for-google-nvidia-and-the-machine-learning-industry/3/#60053cf7513d&quot;&gt;Google Cloud TPU&lt;/a&gt; is proprietary hardware that only exists in their datacenters, supporting their open source framework TensorFlow. Google Inbox suffers “proprietary creep”: non-standard, closed algorithms that promise to organize your life, an essential component of a lock-in based business model.&lt;/p&gt;
&lt;p&gt;GOOG is a huge company where employees have autonomy and multiple projects and efforts are occurring. Big efforts, though, are coherent, concerted, and well aligned with its mission: to be an AI-first company, an AI that is closed and lives in their cloud.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;From the 90s until the 2010s, the Web we have experienced has been, albeit somewhat imperfectly, faithful to its original purpose. The Web’s diversity has granted space for multiple businesses to innovate and thrive, independent hobbyist communities to grow, and personal sites to be hosted on whatever physical servers can host them. The internet’s infrastructural diversity is directly tied to the success of diverse Web businesses and communities. The Web’s openness is vital for its security, accessibility, innovation and competitiveness.&lt;/p&gt;
&lt;p&gt;After 2014, we started losing the benefits of the internet’s infrastructural and economical diversity. It is difficult to compete with AMZN’s and GOOG’s Cloud Services, which host a massive amount of sites for other businesses. Any website aspiring for significant traffic depends on Search and Social traffic.&lt;/p&gt;
&lt;h2 id=&quot;what-the-web-will-become-under-goog-fb-amzn&quot;&gt;What the Web will become under GOOG-FB-AMZN&lt;/h2&gt;
&lt;p&gt;The following analysis is an extrapolation for the future, based on the current state of the Web and strategies made public by executives at GOOG-FB-AMZN.&lt;/p&gt;
&lt;p&gt;The War for Net Neutrality in the USA won a &lt;a href=&quot;https://www.theverge.com/2014/2/25/5431382&quot;&gt;battle in 2014&lt;/a&gt;, but in 2017 we are seeing a &lt;a href=&quot;https://www.theverge.com/2017/7/12/15715030&quot;&gt;second battle&lt;/a&gt; which is more likely to be lost. Internet Service Providers (ISPs) are probably soon going to dictate what traffic can or cannot arrive at people’s end devices. GOOG-FB-AMZN traffic would be the most common, due to their popularity among internet users. Because of this market demand, ISPs will likely provide cheap plans with access to GOOG-FB-AMZN, while offering more expensive plans with full internet access. &lt;a href=&quot;https://www.reddit.com/r/technology/comments/79770i/in_portugal_with_no_net_neutrality_internet/&quot;&gt;It is already a reality in Portugal&lt;/a&gt;. This would grow even more the dominance the three tech giants already enjoy. There would be no more economical incentive for smaller businesses to have independent websites, and a gradual migration towards Facebook Pages would make more sense. Smaller e-commerce sites would be bought by AMZN or go bankrupt. Because most internet users couldn’t open all the sites, GOOG would have little incentive to be a mere bridge between people and sites.&lt;/p&gt;
&lt;p&gt;GOOG’s shift away from Search is a sign how they are growing their strategy beyond the Web. For many years, Google used to be just a tool that played the important role of assisting the Web, by indexing it. Lately, however, it is not attractive for Google to be a mere search engine of the Web. For the purposes expressed in their &lt;a href=&quot;https://www.theguardian.com/technology/2014/nov/03/larry-page-google-dont-be-evil-sergey-brin&quot;&gt;mission statement&lt;/a&gt;, “to organize the world’s information and make it universally accessible and useful”, the search engine approach has been exhausted. The multi-second path from search query, to search results, to webpage, to information, is too long to provide an ideal user experience. Their goal is to cut the middlemen in that path. They have tried to cut out the results page with their “I’m feeling lucky” button, but without intelligent analysis they cannot reliably take shortcuts in that path. With AI, they believe they can shorten the path to just one step, “get information”, even without searching for it in the first place. That’s the purpose of Suggest.&lt;/p&gt;
&lt;p&gt;As an index, people have different expectations on search result neutrality. Some want Google Search to be entirely neutral, some demand immediate action to remove some results. The European Union has both &lt;a href=&quot;https://www.cnet.com/news/google-must-delete-search-results-rules-european-court/&quot;&gt;demanded GOOG to comply with removal requests&lt;/a&gt;, and &lt;a href=&quot;https://www.bloomberg.com/news/articles/2017-06-27/google-gets-record-2-7-billion-eu-fine-for-skewing-searches&quot;&gt;fined GOOG for not being neutral in shopping queries&lt;/a&gt;. It is not beneficial for GOOG to assume the role of an impartial arbitrer of content, since it’s not supporting their business model. Quite the contrary, they are under public scrutiny from multiple governments, potentially risking their reputation.&lt;/p&gt;
&lt;p&gt;The Suggest strategy is being currently deployed through Google Now, Google Assistant, Android notifications, and Google Home. None of these mentioned technologies are part of Web, in other words, not part of “browser-land” made of websites. The internet is just the underlying transport layer for data from their cloud to end-user devices, but the Web itself is being bypassed. &lt;a href=&quot;http://www.hollywoodreporter.com/news/google-chairman-eric-schmidt-internet-765989&quot;&gt;Schmidt’s vision for the future&lt;/a&gt; is one where internet services are ubiquitous and personalized, as opposed to an experience contained in web browsers in desktop machines.&lt;/p&gt;
&lt;p&gt;Similarly, while AMZN’s business still relies on traffic to their desktop web portal (accounting for 33% of sales), a &lt;a href=&quot;https://www.statista.com/statistics/690366/amazon-purchase-channels-usa/&quot;&gt;large portion&lt;/a&gt; (25%) of their sales happen through mobile apps, not to mention Amazon Echo. Like Google Home, Amazon Echo bypasses the Web and uses the internet just for communication between cloud and end user. In these new non-web contexts, tech giants have more authority over data traffic. They can even directly block each other, like &lt;a href=&quot;http://www.reuters.com/article/us-amazon-com-google/amazon-says-google-has-pulled-youtube-from-echo-show-device-in-tech-face-off-idUSKCN1C20A8&quot;&gt;GOOG recently cut support for YouTube traffic in Amazon Echo devices&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;the-appleification-of-tech-giants&quot;&gt;The Appleification of tech giants&lt;/h2&gt;
&lt;p&gt;GOOG, MSFT, FB, and AMZN are mimicking AAPL’s strategy of building brand loyalty around high-end devices. Through a process I call “Appleification”, they are (1) setting up walled gardens, (2) becoming hardware companies, and (3) marketing the design while designing for the market. It is a threat to AAPL itself, because they are behind the other giants when it comes to big data collection and its uses. While AAPL’s early and bold introduction of an App Store shook the Web as the dominant software distribution platform, it wasn’t enough to replace it. The next wave of walled gardens might look different: less noticeable, but nonetheless disruptive to the Web.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://staltz.com/img/goog-devices.jpg&quot;&gt;&lt;img src=&quot;https://staltz.com/img/goog-devices.jpg&quot; alt=&quot;GOOG devices&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;There is a tendency at GOOG-FB-AMZN to bypass the Web which is motivated by user experience and efficient communication, not by an agenda to avoid browsers. In the knowledge internet and the commerce internet, being efficient to provide what users want is the goal. In the social internet, the goal is to provide an efficient channel for communication between people. This explains FB’s 10-year strategy with Augmented Reality (AR) and Virtual Reality (VR) as the next medium for social interactions through the internet. This strategy would also bypass the Web, proving how more natural social AR would be than social real-time texting in browsers. Already today, most people on the internet communicate with other people via a mobile app, not via a browser.&lt;/p&gt;
&lt;p&gt;The common pattern among these three internet giants is to grow beyond browsers, creating new virtual contexts where data is created and shared. The Web may die like most other technologies do: simply by becoming less attractive than newer technologies. And like most obsolete technologies, they don’t suddenly disappear, neither do they disappear completely. You can still buy a Walkman and listen to a tape with it, but the technology has nevertheless lost its collective relevance. The Web’s death will come as a gradual decay of its necessity, not as a dramatic loss.&lt;/p&gt;
&lt;h2 id=&quot;the-trinet&quot;&gt;The Trinet&lt;/h2&gt;
&lt;p&gt;The internet will survive longer than the Web will. GOOG-FB-AMZN will still depend on submarine internet cables (the “&lt;a href=&quot;https://en.wikipedia.org/wiki/Internet_backbone&quot;&gt;Backbone&lt;/a&gt;”), because it is a technical success. That said, many aspects of the internet will lose their relevance, and the underlying infrastructure could be optimized only for GOOG traffic, FB traffic, and AMZN traffic. It wouldn’t conceptually be anymore a “network of networks”, but just a “network of three networks”, the &lt;em&gt;Trinet&lt;/em&gt;, if you will. The concept of workplace network which gave birth to the internet infrastructure would migrate to a more abstract level: Facebook Groups, Google Hangouts, G Suite, and other competing services which can be acquired by a tech giant. Workplace networks are already today emulated in software as a service, not as traditional Local Area Networks. To improve user experience, the Trinet would be a technical evolution of the internet. These efforts are already happening today, &lt;a href=&quot;https://www.nextplatform.com/2017/07/17/google-wants-rewire-internet/&quot;&gt;at GOOG&lt;/a&gt;. In the long-term, supporting routing for the old internet and the old Web would be an overhead, so it could be beneficial to cut support for the diverse internet on the protocol and hardware level. Access to the old internet could be emulated on GOOG’s cloud accessed through the Trinet, much like how &lt;a href=&quot;https://win95.ajf.me/&quot;&gt;Windows 95 can be today emulated in your browser&lt;/a&gt;. ISPs would recognize the obsolence of the internet and support the Trinet only, driven by market demand for optimal user experience from GOOG-FB-AMZN.&lt;/p&gt;
&lt;p&gt;Perhaps a future with great user experience in AR, VR, hands-free commerce and knowledge sharing could evoke an optimistic perspective for what these tech giants are building. But 25 years of the Web has gotten us used to foundational freedoms that we take for granted. We forget how useful it has been to remain anonymous and control what we share, or how easy it was to start an internet startup with its own independent servers operating with the same rights GOOG servers have. On the Trinet, if you are permanently banned from GOOG or FB, you would have no alternative. You could even be restricted from creating a new account. As private businesses, GOOG, FB, and AMZN don’t need to guarantee you access to their networks. You do not have a legal right to an account in their servers, and as societies we aren’t demanding for these rights as vehemently as we could, to counter the strategies that tech giants are putting forward.&lt;/p&gt;
&lt;p&gt;The Web and the internet have represented freedom: efficient and unsupervised exchange of information between people of all nations. In the Trinet, we will have even more vivid exchange of information between people, but we will sacrifice freedom. Many of us will wake up to the tragedy of this tradeoff only once it is reality.&lt;/p&gt;
&lt;p&gt;If you liked this article, consider sharing &lt;a href=&quot;https://twitter.com/intent/tweet?original_referer=https%3A%2F%2Fstaltz.com%2Fthe-web-began-dying-in-2014-heres-how.html&amp;amp;text=The%20Web%20began%20dying%20in%202014,%20here's%20how&amp;amp;tw_p=tweetbutton&amp;amp;url=https%3A%2F%2Fstaltz.com%2Fthe-web-began-dying-in-2014-heres-how.html&amp;amp;via=andrestaltz&quot; rel=&quot;nofollow&quot; target=&quot;_blank&quot; title=&quot;tweeting&quot;&gt;(tweeting)&lt;/a&gt; it to your followers.&lt;/p&gt;
&lt;p class=&quot;verify-in-keybase&quot;&gt;You can make sure that the author wrote this post by copy-pasting &lt;a href=&quot;https://raw.githubusercontent.com/staltz/staltz.com/master/signed_posts/2017-10-30-the-web-began-dying-in-2014-heres-how.md.asc&quot;&gt;this signature&lt;/a&gt; into &lt;a href=&quot;https://keybase.io/verify&quot;&gt;this Keybase page&lt;/a&gt;.&lt;/p&gt;
</description>
<pubDate>Mon, 30 Oct 2017 12:23:48 +0000</pubDate>
<dc:creator>staltz</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://staltz.com/the-web-began-dying-in-2014-heres-how.html</dc:identifier>
</item>
<item>
<title>Anvil: full stack web apps built only with Python</title>
<link>https://anvil.works/</link>
<guid isPermaLink="true" >https://anvil.works/</guid>
<description>&lt;p class=&quot;large&quot;&gt;Supercharge your team's productivity with Anvil training.&lt;/p&gt;
&lt;p class=&quot;large&quot;&gt;We offer customised courses, from beginners to expert. As well as extensive industry experience, our expert instructors have taught programming at the University of Cambridge, in schools, and at non-profit organisations.&lt;/p&gt;
&lt;p class=&quot;large&quot;&gt;We offer in-person training, ongoing support, and pair programming to get your project off to a flying start.&lt;/p&gt;
&lt;center id=&quot;training-btn&quot;&gt;&lt;button class=&quot;btn btn-lg btn-primary&quot; onclick=&quot;$('#training-info').slideDown(); $('#training-btn').slideUp();&quot;&gt;Request training information&lt;/button&gt;&lt;/center&gt;


</description>
<pubDate>Mon, 30 Oct 2017 10:52:48 +0000</pubDate>
<dc:creator>galfarragem</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://anvil.works/</dc:identifier>
</item>
</channel>
</rss>