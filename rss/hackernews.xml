<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Smugmug Acquires Flickr</title>
<link>https://smugmug.com/together</link>
<guid isPermaLink="true" >https://smugmug.com/together</guid>
<description>&lt;br/&gt;&lt;div class=&quot;sky sky-2&quot;&gt;
                                
                                &lt;p&gt;But Wait...&lt;/p&gt;
                            &lt;/div&gt;&lt;br/&gt;&lt;div class=&quot;sky sky-4 small&quot; readability=&quot;11&quot;&gt;
                                
                                &lt;p&gt;
                                    SmugMug has acquired Flickr. &lt;br/&gt;If you use our products today, rest easy, they aren't going anywhere.&lt;br/&gt;The future is bright, but we'll only get there together.&lt;br/&gt;Let's do this.&lt;/p&gt;
                            &lt;/div&gt;

                        </description>
<pubDate>Fri, 20 Apr 2018 22:31:37 +0000</pubDate>
<dc:creator>uptown</dc:creator>
<og:url>https://www.smugmug.com/together</og:url>
<og:title>Together, SmugMug+Flickr</og:title>
<og:description>Together is better.</og:description>
<og:image>https://photos.smugmug.com/photos/i-bxWKMS5/0/580fae3e/XL/i-bxWKMS5-XL.png</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.smugmug.com/together/</dc:identifier>
</item>
<item>
<title>FFmpeg 4.0 released</title>
<link>http://ffmpeg.org/index.html#pr4.0</link>
<guid isPermaLink="true" >http://ffmpeg.org/index.html#pr4.0</guid>
<description>&lt;div class=&quot;row&quot; readability=&quot;8.2087912087912&quot;&gt;
&lt;p&gt;
&lt;h2 class=&quot;description&quot;&gt;A complete, cross-platform solution to record, convert and stream audio and video.&lt;/h2&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;div class=&quot;well example&quot; readability=&quot;6.0618556701031&quot;&gt;
&lt;h3&gt;Converting &lt;strong&gt;video&lt;/strong&gt; and &lt;strong&gt;audio&lt;/strong&gt; has never been so easy.&lt;/h3&gt;
&lt;pre&gt;
$ ffmpeg -i input.mp4 output.avi
&lt;/pre&gt;
&lt;div class=&quot;text-right&quot;&gt;&lt;a href=&quot;http://ffmpeg.org/about.html&quot; class=&quot;btn btn-success btn-lg&quot;&gt;Discover more&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;pr4.0&quot;&gt;April 20th, 2018, FFmpeg 4.0 &quot;Wu&quot;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_4.0&quot;&gt;FFmpeg 4.0 &quot;Wu&quot;&lt;/a&gt;, a new major release, is now available! Some of the highlights:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Bitstream filters for editing metadata in H.264, HEVC and MPEG-2 streams&lt;/li&gt;
&lt;li&gt;Experimental MagicYUV encoder&lt;/li&gt;
&lt;li&gt;TiVo ty/ty+ demuxer&lt;/li&gt;
&lt;li&gt;Intel QSV-accelerated MJPEG encoding&lt;/li&gt;
&lt;li&gt;native aptX and aptX HD encoder and decoder&lt;/li&gt;
&lt;li&gt;NVIDIA NVDEC-accelerated H.264, HEVC, MJPEG, MPEG-1/2/4, VC1, VP8/9 hwaccel decoding&lt;/li&gt;
&lt;li&gt;Intel QSV-accelerated overlay filter&lt;/li&gt;
&lt;li&gt;mcompand audio filter&lt;/li&gt;
&lt;li&gt;acontrast audio filter&lt;/li&gt;
&lt;li&gt;OpenCL overlay filter&lt;/li&gt;
&lt;li&gt;video mix filter&lt;/li&gt;
&lt;li&gt;video normalize filter&lt;/li&gt;
&lt;li&gt;audio lv2 wrapper filter&lt;/li&gt;
&lt;li&gt;VAAPI MJPEG and VP8 decoding&lt;/li&gt;
&lt;li&gt;AMD AMF H.264 and HEVC encoders&lt;/li&gt;
&lt;li&gt;video fillborders filter&lt;/li&gt;
&lt;li&gt;video setrange filter&lt;/li&gt;
&lt;li&gt;support LibreSSL (via libtls)&lt;/li&gt;
&lt;li&gt;Dropped support for building for Windows XP. The minimum supported Windows version is Windows Vista.&lt;/li&gt;
&lt;li&gt;deconvolve video filter&lt;/li&gt;
&lt;li&gt;entropy video filter&lt;/li&gt;
&lt;li&gt;hilbert audio filter source&lt;/li&gt;
&lt;li&gt;aiir audio filter&lt;/li&gt;
&lt;li&gt;Removed the ffserver program&lt;/li&gt;
&lt;li&gt;Removed the ffmenc and ffmdec muxer and demuxer&lt;/li&gt;
&lt;li&gt;VideoToolbox HEVC encoder and hwaccel&lt;/li&gt;
&lt;li&gt;VAAPI-accelerated ProcAmp (color balance), denoise and sharpness filters&lt;/li&gt;
&lt;li&gt;Add android_camera indev&lt;/li&gt;
&lt;li&gt;codec2 en/decoding via libcodec2&lt;/li&gt;
&lt;li&gt;native SBC encoder and decoder&lt;/li&gt;
&lt;li&gt;drmeter audio filter&lt;/li&gt;
&lt;li&gt;hapqa_extract bitstream filter&lt;/li&gt;
&lt;li&gt;filter_units bitstream filter&lt;/li&gt;
&lt;li&gt;AV1 Support through libaom&lt;/li&gt;
&lt;li&gt;E-AC-3 dependent frames support&lt;/li&gt;
&lt;li&gt;bitstream filter for extracting E-AC-3 core&lt;/li&gt;
&lt;li&gt;Haivision SRT protocol via libsrt&lt;/li&gt;
&lt;li&gt;vfrdet filter&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.&lt;/p&gt;
&lt;h3 id=&quot;pr3.4&quot;&gt;October 15th, 2017, FFmpeg 3.4 &quot;Cantor&quot;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_3.4&quot;&gt;FFmpeg 3.4 &quot;Cantor&quot;&lt;/a&gt;, a new major release, is now available! Some of the highlights:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;deflicker video filter&lt;/li&gt;
&lt;li&gt;doubleweave video filter&lt;/li&gt;
&lt;li&gt;lumakey video filter&lt;/li&gt;
&lt;li&gt;pixscope video filter&lt;/li&gt;
&lt;li&gt;oscilloscope video filter&lt;/li&gt;
&lt;li&gt;update cuvid/nvenc headers to Video Codec SDK 8.0.14&lt;/li&gt;
&lt;li&gt;afir audio filter&lt;/li&gt;
&lt;li&gt;scale_cuda CUDA based video scale filter&lt;/li&gt;
&lt;li&gt;librsvg support for svg rasterization&lt;/li&gt;
&lt;li&gt;crossfeed audio filter&lt;/li&gt;
&lt;li&gt;spec compliant VP9 muxing support in MP4&lt;/li&gt;
&lt;li&gt;surround audio filter&lt;/li&gt;
&lt;li&gt;sofalizer filter switched to libmysofa&lt;/li&gt;
&lt;li&gt;Gremlin Digital Video demuxer and decoder&lt;/li&gt;
&lt;li&gt;headphone audio filter&lt;/li&gt;
&lt;li&gt;superequalizer audio filter&lt;/li&gt;
&lt;li&gt;roberts video filter&lt;/li&gt;
&lt;li&gt;additional frame format support for Interplay MVE movies&lt;/li&gt;
&lt;li&gt;support for decoding through D3D11VA in ffmpeg&lt;/li&gt;
&lt;li&gt;limiter video filter&lt;/li&gt;
&lt;li&gt;libvmaf video filter&lt;/li&gt;
&lt;li&gt;Dolby E decoder and SMPTE 337M demuxer&lt;/li&gt;
&lt;li&gt;unpremultiply video filter&lt;/li&gt;
&lt;li&gt;tlut2 video filter&lt;/li&gt;
&lt;li&gt;floodfill video filter&lt;/li&gt;
&lt;li&gt;pseudocolor video filter&lt;/li&gt;
&lt;li&gt;raw G.726 muxer and demuxer, left- and right-justified&lt;/li&gt;
&lt;li&gt;NewTek NDI input/output device&lt;/li&gt;
&lt;li&gt;FITS demuxer and decoder&lt;/li&gt;
&lt;li&gt;FITS muxer and encoder&lt;/li&gt;
&lt;li&gt;despill video filter&lt;/li&gt;
&lt;li&gt;haas audio filter&lt;/li&gt;
&lt;li&gt;SUP/PGS subtitle muxer&lt;/li&gt;
&lt;li&gt;convolve video filter&lt;/li&gt;
&lt;li&gt;VP9 tile threading support&lt;/li&gt;
&lt;li&gt;KMS screen grabber&lt;/li&gt;
&lt;li&gt;CUDA thumbnail filter&lt;/li&gt;
&lt;li&gt;V4L2 mem2mem HW assisted codecs&lt;/li&gt;
&lt;li&gt;Rockchip MPP hardware decoding&lt;/li&gt;
&lt;li&gt;vmafmotion video filter&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.&lt;/p&gt;
&lt;h3 id=&quot;pr3.3&quot;&gt;April 13th, 2017, FFmpeg 3.3 &quot;Hilbert&quot;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_3.3&quot;&gt;FFmpeg 3.3 &quot;Hilbert&quot;&lt;/a&gt;, a new major release, is now available! Some of the highlights:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Apple Pixlet decoder&lt;/li&gt;
&lt;li&gt;NewTek SpeedHQ decoder&lt;/li&gt;
&lt;li&gt;QDMC audio decoder&lt;/li&gt;
&lt;li&gt;PSD (Photoshop Document) decoder&lt;/li&gt;
&lt;li&gt;FM Screen Capture decoder&lt;/li&gt;
&lt;li&gt;ScreenPressor decoder&lt;/li&gt;
&lt;li&gt;XPM decoder&lt;/li&gt;
&lt;li&gt;DNxHR decoder fixes for HQX and high resolution videos&lt;/li&gt;
&lt;li&gt;ClearVideo decoder (partial)&lt;/li&gt;
&lt;li&gt;16.8 and 24.0 floating point PCM decoder&lt;/li&gt;
&lt;li&gt;Intel QSV-accelerated VP8 video decoding&lt;/li&gt;
&lt;li&gt;native Opus encoder&lt;/li&gt;
&lt;li&gt;DNxHR 444 and HQX encoding&lt;/li&gt;
&lt;li&gt;Quality improvements for the (M)JPEG encoder&lt;/li&gt;
&lt;li&gt;VAAPI-accelerated MPEG-2 and VP8 encoding&lt;/li&gt;
&lt;li&gt;premultiply video filter&lt;/li&gt;
&lt;li&gt;abitscope multimedia filter&lt;/li&gt;
&lt;li&gt;readeia608 filter&lt;/li&gt;
&lt;li&gt;threshold filter&lt;/li&gt;
&lt;li&gt;midequalizer filter&lt;/li&gt;
&lt;li&gt;MPEG-7 Video Signature filter&lt;/li&gt;
&lt;li&gt;add internal ebur128 library, remove external libebur128 dependency&lt;/li&gt;
&lt;li&gt;Intel QSV video scaling and deinterlacing filters&lt;/li&gt;
&lt;li&gt;Sample Dump eXchange demuxer&lt;/li&gt;
&lt;li&gt;MIDI Sample Dump Standard demuxer&lt;/li&gt;
&lt;li&gt;Scenarist Closed Captions demuxer and muxer&lt;/li&gt;
&lt;li&gt;Support MOV with multiple sample description tables&lt;/li&gt;
&lt;li&gt;Pro-MPEG CoP #3-R2 FEC protocol&lt;/li&gt;
&lt;li&gt;Support for spherical videos&lt;/li&gt;
&lt;li&gt;CrystalHD decoder moved to new decode API&lt;/li&gt;
&lt;li&gt;configure now fails if autodetect-libraries are requested but not found&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.&lt;/p&gt;
&lt;h3 id=&quot;gsoc2016finalreport&quot;&gt;October 30th, 2016, Results: Summer Of Code 2016.&lt;/h3&gt;
&lt;p&gt;This has been a long time coming but we wanted to give a proper closure to our participation in this run of the program and it takes time. Sometimes it's just to get the final report for each project trimmed down, others, is finalizing whatever was still in progress when the program finished: final patches need to be merged, TODO lists stabilized, future plans agreed; you name it.&lt;/p&gt;
&lt;p&gt;Without further ado, here's the silver-lining for each one of the projects we sought to complete during this Summer of Code season:&lt;/p&gt;
&lt;h4&gt;FFv1 (Mentor: Michael Niedermayer)&lt;/h4&gt;
&lt;p&gt;Stanislav Dolganov designed and implemented experimental support for motion estimation and compensation in the lossless FFV1 codec. The design and implementation is based on the snow video codec, which uses OBMC. Stanislav's work proved that significant compression gains can be achieved with inter frame compression. FFmpeg welcomes Stanislav to continue working beyond this proof of concept and bring its advances into the official FFV1 specification within the IETF.&lt;/p&gt;
&lt;h4&gt;Self test coverage (Mentor: Michael Niedermayer)&lt;/h4&gt;
&lt;p&gt;Petru Rares Sincraian added several self-tests to FFmpeg and successfully went through the in-some-cases tedious process of fine tuning tests parameters to avoid known and hard to avoid problems, like checksum mismatches due to rounding errors on the myriad of platforms we support. His work has improved the code coverage of our self tests considerably.&lt;/p&gt;
&lt;h4&gt;MPEG-4 ALS encoder implementation (Mentor: Thilo Borgmann)&lt;/h4&gt;
&lt;p&gt;Umair Khan updated and integrated the ALS encoder to fit in the current FFmpeg codebase. He also implemented a missing feature for the ALS decoder that enables floating-point sample decoding. FFmpeg support for MPEG-4 ALS has been improved significantly by Umair's work. We welcome him to keep maintaining his improvements and hope for great contributions to come.&lt;/p&gt;
&lt;h4&gt;Tee muxer improvements (Mentor: Marton Balint)&lt;/h4&gt;
&lt;p&gt;Ján Sebechlebský's generic goal was to improve the tee muxer so it tolerated blocking IO and allowed transparent error recovery. During the design phase it turned out that this functionality called for a separate muxer, so Ján spent his summer working on the so-called FIFO muxer, gradually fixing issues all over the codebase. He succeeded in his task, and the FIFO muxer is now part of the main repository, alongside several other improvements he made in the process.&lt;/p&gt;
&lt;h4&gt;TrueHD encoder (Mentor: Rostislav Pehlivanov)&lt;/h4&gt;
&lt;p&gt;Jai Luthra's objective was to update the out-of-tree and pretty much abandoned MLP (Meridian Lossless Packing) encoder for libavcodec and improve it to enable encoding to the TrueHD format. For the qualification period the encoder was updated such that it was usable and throughout the summer, successfully improved adding support for multi-channel audio and TrueHD encoding. Jai's code has been merged into the main repository now. While a few problems remain with respect to LFE channel and 32 bit sample handling, these are in the process of being fixed such that effort can be finally put in improving the encoder's speed and efficiency.&lt;/p&gt;
&lt;h4&gt;Motion interpolation filter (Mentor: Paul B Mahol)&lt;/h4&gt;
&lt;p&gt;Davinder Singh investigated existing motion estimation and interpolation approaches from the available literature and previous work by our own: Michael Niedermayer, and implemented filters based on this research. These filters allow motion interpolating frame rate conversion to be applied to a video, for example, to create a slow motion effect or change the frame rate while smoothly interpolating the video along the motion vectors. There's still work to be done to call these filters 'finished', which is rather hard all things considered, but we are looking optimistically at their future.&lt;/p&gt;
&lt;p&gt;And that's it. We are happy with the results of the program and immensely thankful for the opportunity of working with such an amazing set of students. We can be a tough crowd but our mentors did an amazing job at hand holding our interns through their journey. Thanks also to Google for this wonderful program and to everyone that made room in their busy lives to help making GSoC2016 a success. See you in 2017!&lt;/p&gt;
&lt;h3 id=&quot;sdl1&quot;&gt;September 24th, 2016, SDL1 support dropped.&lt;/h3&gt;
&lt;p&gt;Support for the SDL1 library has been dropped, due to it no longer being maintained (as of January, 2012) and it being superseded by the SDL2 library. As a result, the SDL1 output device has also been removed and replaced by an SDL2 implementation. Both the ffplay and opengl output devices have been updated to support SDL2.&lt;/p&gt;
&lt;h3 id=&quot;pr3.1.2&quot;&gt;August 9th, 2016, FFmpeg 3.1.2 &quot;Laplace&quot;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_3.1&quot;&gt;FFmpeg 3.1.2&lt;/a&gt;, a new point release from the 3.1 release branch, is now available! It fixes several bugs.&lt;/p&gt;
&lt;p&gt;We recommend users, distributors, and system integrators, to upgrade unless they use current git master.&lt;/p&gt;
&lt;h3 id=&quot;ffserv&quot;&gt;July 10th, 2016, ffserver program being dropped&lt;/h3&gt;
&lt;p&gt;After thorough deliberation, we're announcing that we're about to drop the ffserver program from the project starting with the next release. ffserver has been a problematic program to maintain due to its use of internal APIs, which complicated the recent cleanups to the libavformat library, and block further cleanups and improvements which are desired by API users and will be easier to maintain. Furthermore the program has been hard for users to deploy and run due to reliability issues, lack of knowledgable people to help and confusing configuration file syntax. Current users and members of the community are invited to write a replacement program to fill the same niche that ffserver did using the new APIs and to contact us so we may point users to test and contribute to its development.&lt;/p&gt;
&lt;h3 id=&quot;pr3.1.1&quot;&gt;July 1st, 2016, FFmpeg 3.1.1 &quot;Laplace&quot;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_3.1&quot;&gt;FFmpeg 3.1.1&lt;/a&gt;, a new point release from the 3.1 release branch, is now available! It mainly deals with a few ABI issues introduced in the previous release.&lt;/p&gt;
&lt;p&gt;We strongly recommend users, distributors, and system integrators, especially those who experienced issues upgrading from 3.0, to upgrade unless they use current git master.&lt;/p&gt;
&lt;h3 id=&quot;pr3.1&quot;&gt;June 27th, 2016, FFmpeg 3.1 &quot;Laplace&quot;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_3.1&quot;&gt;FFmpeg 3.1 &quot;Laplace&quot;&lt;/a&gt;, a new major release, is now available! Some of the highlights:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;DXVA2-accelerated HEVC Main10 decoding&lt;/li&gt;
&lt;li&gt;fieldhint filter&lt;/li&gt;
&lt;li&gt;loop video filter and aloop audio filter&lt;/li&gt;
&lt;li&gt;Bob Weaver deinterlacing filter&lt;/li&gt;
&lt;li&gt;firequalizer filter&lt;/li&gt;
&lt;li&gt;datascope filter&lt;/li&gt;
&lt;li&gt;bench and abench filters&lt;/li&gt;
&lt;li&gt;ciescope filter&lt;/li&gt;
&lt;li&gt;protocol blacklisting API&lt;/li&gt;
&lt;li&gt;MediaCodec H264 decoding&lt;/li&gt;
&lt;li&gt;VC-2 HQ RTP payload format (draft v1) depacketizer and packetizer&lt;/li&gt;
&lt;li&gt;VP9 RTP payload format (draft v2) packetizer&lt;/li&gt;
&lt;li&gt;AudioToolbox audio decoders&lt;/li&gt;
&lt;li&gt;AudioToolbox audio encoders&lt;/li&gt;
&lt;li&gt;coreimage filter (GPU based image filtering on OSX)&lt;/li&gt;
&lt;li&gt;libdcadec removed&lt;/li&gt;
&lt;li&gt;bitstream filter for extracting DTS core&lt;/li&gt;
&lt;li&gt;ADPCM IMA DAT4 decoder&lt;/li&gt;
&lt;li&gt;musx demuxer&lt;/li&gt;
&lt;li&gt;aix demuxer&lt;/li&gt;
&lt;li&gt;remap filter&lt;/li&gt;
&lt;li&gt;hash and framehash muxers&lt;/li&gt;
&lt;li&gt;colorspace filter&lt;/li&gt;
&lt;li&gt;hdcd filter&lt;/li&gt;
&lt;li&gt;readvitc filter&lt;/li&gt;
&lt;li&gt;VAAPI-accelerated format conversion and scaling&lt;/li&gt;
&lt;li&gt;libnpp/CUDA-accelerated format conversion and scaling&lt;/li&gt;
&lt;li&gt;Duck TrueMotion 2.0 Real Time decoder&lt;/li&gt;
&lt;li&gt;Wideband Single-bit Data (WSD) demuxer&lt;/li&gt;
&lt;li&gt;VAAPI-accelerated H.264/HEVC/MJPEG encoding&lt;/li&gt;
&lt;li&gt;DTS Express (LBR) decoder&lt;/li&gt;
&lt;li&gt;Generic OpenMAX IL encoder with support for Raspberry Pi&lt;/li&gt;
&lt;li&gt;IFF ANIM demuxer &amp;amp; decoder&lt;/li&gt;
&lt;li&gt;Direct Stream Transfer (DST) decoder&lt;/li&gt;
&lt;li&gt;loudnorm filter&lt;/li&gt;
&lt;li&gt;MTAF demuxer and decoder&lt;/li&gt;
&lt;li&gt;MagicYUV decoder&lt;/li&gt;
&lt;li&gt;OpenExr improvements (tile data and B44/B44A support)&lt;/li&gt;
&lt;li&gt;BitJazz SheerVideo decoder&lt;/li&gt;
&lt;li&gt;CUDA CUVID H264/HEVC decoder&lt;/li&gt;
&lt;li&gt;10-bit depth support in native utvideo decoder&lt;/li&gt;
&lt;li&gt;libutvideo wrapper removed&lt;/li&gt;
&lt;li&gt;YUY2 Lossless Codec decoder&lt;/li&gt;
&lt;li&gt;VideoToolbox H.264 encoder&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.&lt;/p&gt;
&lt;h3 id=&quot;gsoc2016&quot;&gt;March 16th, 2016, Google Summer of Code&lt;/h3&gt;
&lt;p&gt;FFmpeg has been accepted as a &lt;a href=&quot;https://summerofcode.withgoogle.com/&quot;&gt;Google Summer of Code&lt;/a&gt; open source organization. If you wish to participate as a student see our &lt;a href=&quot;https://trac.ffmpeg.org/wiki/SponsoringPrograms/GSoC/2016&quot;&gt;project ideas page&lt;/a&gt;. You can already get in contact with mentors and start working on qualification tasks as well as register at google and submit your project proposal draft. Good luck!&lt;/p&gt;
&lt;h3 id=&quot;pr3.0&quot;&gt;February 15th, 2016, FFmpeg 3.0 &quot;Einstein&quot;&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_3.0&quot;&gt;FFmpeg 3.0 &quot;Einstein&quot;&lt;/a&gt;, a new major release, is now available! Some of the highlights:&lt;/p&gt;
&lt;p&gt;We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.&lt;/p&gt;
&lt;h3 id=&quot;removing_external_aac_encoders&quot;&gt;January 30, 2016, Removing support for two external AAC encoders&lt;/h3&gt;
&lt;p&gt;We have just removed support for VisualOn AAC encoder (libvo-aacenc) and libaacplus in FFmpeg master.&lt;/p&gt;
&lt;p&gt;Even before marking our internal AAC encoder as &lt;a href=&quot;http://ffmpeg.org/index.html#aac_encoder_stable&quot;&gt;stable&lt;/a&gt;, it was known that libvo-aacenc was of an inferior quality compared to our native one for most samples. However, the VisualOn encoder was used extensively by the Android Open Source Project, and we would like to have a tested-and-true stable option in our code base.&lt;/p&gt;
&lt;p&gt;When first committed in 2011, libaacplus filled in the gap of encoding High Efficiency AAC formats (HE-AAC and HE-AACv2), which was not supported by any of the encoders in FFmpeg at that time.&lt;/p&gt;
&lt;p&gt;The circumstances for both have changed. After the work spearheaded by Rostislav Pehlivanov and Claudio Freire, the now-stable FFmpeg native AAC encoder is ready to compete with much more mature encoders. The Fraunhofer FDK AAC Codec Library for Android was added in 2012 as the fourth supported external AAC encoder, and the one with the best quality and the most features supported, including HE-AAC and HE-AACv2.&lt;/p&gt;
&lt;p&gt;Therefore, we have decided that it is time to remove libvo-aacenc and libaacplus. If you are currently using libvo-aacenc, prepare to transition to the native encoder (&lt;code&gt;aac&lt;/code&gt;) when updating to the next version of FFmpeg. In most cases it is as simple as merely swapping the encoder name. If you are currently using libaacplus, start using FDK AAC (&lt;code&gt;libfdk_aac&lt;/code&gt;) with an appropriate &lt;code&gt;profile&lt;/code&gt; option to select the exact AAC profile that fits your needs. In both cases, you will enjoy an audible quality improvement and as well as fewer licensing headaches.&lt;/p&gt;
&lt;p&gt;Enjoy!&lt;/p&gt;
&lt;h3 id=&quot;pr2.8.5&quot;&gt;January 16, 2016, FFmpeg 2.8.5, 2.7.5, 2.6.7, 2.5.10&lt;/h3&gt;
&lt;p&gt;We have made several new point releases (&lt;strong&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_2.8&quot;&gt;2.8.5&lt;/a&gt;, &lt;a href=&quot;http://ffmpeg.org/download.html#release_2.7&quot;&gt;2.7.5&lt;/a&gt;, &lt;a href=&quot;http://ffmpeg.org/download.html#release_2.6&quot;&gt;2.6.7&lt;/a&gt;, &lt;a href=&quot;http://ffmpeg.org/download.html#release_2.5&quot;&gt;2.5.10&lt;/a&gt;&lt;/strong&gt;). They fix various bugs, as well as CVE-2016-1897 and CVE-2016-1898. Please see the changelog for each release for more details.&lt;/p&gt;
&lt;p&gt;We recommend users, distributors and system integrators to upgrade unless they use current git master.&lt;/p&gt;
&lt;h3 id=&quot;aac_encoder_stable&quot;&gt;December 5th, 2015, The native FFmpeg AAC encoder is now stable!&lt;/h3&gt;
&lt;p&gt;After seven years the native FFmpeg AAC encoder has had its experimental flag removed and declared as ready for general use. The encoder is transparent at 128kbps for most samples tested with artifacts only appearing in extreme cases. Subjective quality tests put the encoder to be of equal or greater quality than most of the other encoders available to the public.&lt;/p&gt;
&lt;p&gt;Licensing has always been an issue with encoding AAC audio as most of the encoders have had a license making FFmpeg unredistributable if compiled with support for them. The fact that there now exists a fully open and truly free AAC encoder integrated directly within the project means a lot to those who wish to use accepted and widespread standards.&lt;/p&gt;
&lt;p&gt;The majority of the work done to bring the encoder up to quality was started during this year's GSoC by developer Claudio Freire and Rostislav Pehlivanov. Both continued to work on the encoder with the latter joining as a developer and mainainer, working on other parts of the project as well. Also, thanks to &lt;a href=&quot;http://d.hatena.ne.jp/kamedo2/&quot;&gt;Kamedo2&lt;/a&gt; who does comparisons and tests, the original authors and all past and current contributors to the encoder. Users are suggested and encouraged to use the encoder and provide feedback or breakage reports through our &lt;a href=&quot;https://trac.ffmpeg.org/&quot;&gt;bug tracker&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A big thank you note goes to our newest supporters: MediaHub and Telepoint. Both companies have donated a dedicated server with free of charge internet connectivity. Here is a little bit about them in their own words:&lt;/p&gt;
&lt;ul readability=&quot;1.9683098591549&quot;&gt;&lt;li readability=&quot;2.9239436619718&quot;&gt;
&lt;p&gt;&lt;a href=&quot;http://www.telepoint.bg/en/&quot;&gt;Telepoint&lt;/a&gt; is the biggest carrier-neutral data center in Bulgaria. Located in the heart of Sofia on a cross-road of many Bulgarian and International networks, the facility is a fully featured Tier 3 data center that provides flexible customer-oriented colocation solutions (ranging from a server to a private collocation hall) and a high level of security.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;MediaHub Ltd. is a Bulgarian IPTV platform and services provider which uses FFmpeg heavily since it started operating a year ago. &lt;em&gt;&quot;Donating to help keep FFmpeg online is our way of giving back to the community&quot;&lt;/em&gt; .&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Thanks Telepoint and MediaHub for their support!&lt;/p&gt;
&lt;h3 id=&quot;gsoc2015_result&quot;&gt;September 29th, 2015, GSoC 2015 results&lt;/h3&gt;
&lt;p&gt;FFmpeg participated to the latest edition of the &lt;a href=&quot;http://www.google-melange.com/gsoc/homepage/google/gsoc2015&quot;&gt;Google Summer of Code&lt;/a&gt; Project. FFmpeg got a total of 8 assigned projects, and 7 of them were successful.&lt;/p&gt;
&lt;p&gt;We want to thank &lt;a href=&quot;https://www.google.com&quot;&gt;Google&lt;/a&gt;, the participating students, and especially the mentors who joined this effort. We're looking forward to participating in the next GSoC edition!&lt;/p&gt;
&lt;p&gt;Below you can find a brief description of the final outcome of each single project.&lt;/p&gt;
&lt;h4&gt;Basic servers for network protocols, mentee: Stephan Holljes, mentor: Nicolas George&lt;/h4&gt;
&lt;p&gt;Stephan Holljes's project for this session of Google Summer of Code was to implement basic HTTP server features for libavformat, to complement the already present HTTP client and RTMP and RTSP server code.&lt;/p&gt;
&lt;p&gt;The first part of the project was to make the HTTP code capable of accepting a single client; it was completed partly during the qualification period and partly during the first week of the summer. Thanks to this work, it is now possible to make a simple HTTP stream using the following commands:&lt;/p&gt;
&lt;pre&gt;
    ffmpeg -i /dev/video0 -listen 1 -f matroska \
    -c:v libx264 -preset fast -tune zerolatency http://:8080
    ffplay http://localhost:8080/
  
&lt;/pre&gt;
&lt;p&gt;The next part of the project was to extend the code to be able to accept several clients, simultaneously or consecutively. Since libavformat did not have an API for that kind of task, it was necessary to design one. This part was mostly completed before the midterm and applied shortly afterwards. Since the ffmpeg command-line tool is not ready to serve several clients, the test ground for that new API is an example program serving hard-coded content.&lt;/p&gt;
&lt;p&gt;The last and most ambitious part of the project was to update ffserver to make use of the new API. It would prove that the API is usable to implement real HTTP servers, and expose the points where more control was needed. By the end of the summer, a first working patch series was undergoing code review.&lt;/p&gt;
&lt;h4&gt;Browsing content on the server, mentee: Mariusz Szczepańczyk, mentor: Lukasz Marek&lt;/h4&gt;
&lt;p&gt;Mariusz finished an API prepared by the FFmpeg community and implemented Samba directory listing as qualification task.&lt;/p&gt;
&lt;p&gt;During the program he extended the API with the possibility to remove and rename files on remote servers. He completed the implementation of these features for file, Samba, SFTP, and FTP protocols.&lt;/p&gt;
&lt;p&gt;At the end of the program, Mariusz provided a sketch of an implementation for HTTP directory listening.&lt;/p&gt;
&lt;h4&gt;Directshow digital video capture, mentee: Mate Sebok, mentor: Roger Pack&lt;/h4&gt;
&lt;p&gt;Mate was working on directshow input from digital video sources. He got working input from ATSC input sources, with specifiable tuner.&lt;/p&gt;
&lt;p&gt;The code has not been committed, but a patch of it was sent to the ffmpeg-devel mailing list for future use.&lt;/p&gt;
&lt;p&gt;The mentor plans on cleaning it up and committing it, at least for the ATSC side of things. Mate and the mentor are still working trying to finally figure out how to get DVB working.&lt;/p&gt;
&lt;h4&gt;Implementing full support for 3GPP Timed Text Subtitles, mentee: Niklesh Lalwani, mentor: Philip Langdale&lt;/h4&gt;
&lt;p&gt;Niklesh's project was to expand our support for 3GPP Timed Text subtitles. This is the native subtitle format for mp4 containers, and is interesting because it's usually the only subtitle format supported by the stock playback applications on iOS and Android devices.&lt;/p&gt;
&lt;p&gt;ffmpeg already had basic support for these subtitles which ignored all formatting information - it just provided basic plain-text support.&lt;/p&gt;
&lt;p&gt;Niklesh did work to add support on both the encode and decode side for text formatting capabilities, such as font size/colour and effects like bold/italics, highlighting, etc.&lt;/p&gt;
&lt;p&gt;The main challenge here is that Timed Text handles formatting in a very different way from most common subtitle formats. It uses a binary encoding (based on mp4 boxes, naturally) and stores information separately from the text itself. This requires additional work to track which parts of the text formatting applies to, and explicitly dealing with overlapping formatting (which other formats support but Timed Text does not) so it requires breaking the overlapping sections into separate non-overlapping ones with different formatting.&lt;/p&gt;
&lt;p&gt;Finally, Niklesh had to be careful about not trusting any size information in the subtitles - and that's no joke: the now infamous Android stagefright bug was in code for parsing Timed Text subtitles.&lt;/p&gt;
&lt;p&gt;All of Niklesh's work is committed and was released in ffmpeg 2.8.&lt;/p&gt;
&lt;h4&gt;libswscale refactoring, mentee: Pedro Arthur, mentors: Michael Niedermayer, Ramiro Polla&lt;/h4&gt;
&lt;p&gt;Pedro Arthur has modularized the vertical and horizontal scalers. To do this he designed and implemented a generic filter framework and moved the existing scaler code into it. These changes now allow easily adding removing, splitting or merging processing steps. The implementation was benchmarked and several alternatives were tried to avoid speed loss.&lt;/p&gt;
&lt;p&gt;He also added gamma corrected scaling support. An example to use gamma corrected scaling would be:&lt;/p&gt;
&lt;pre&gt;
    ffmpeg -i input -vf scale=512:384:gamma=1 output
  
&lt;/pre&gt;
&lt;p&gt;Pedro has done impressive work considering the short time available, and he is a FFmpeg committer now. He continues to contribute to FFmpeg, and has fixed some bugs in libswscale after GSoC has ended.&lt;/p&gt;
&lt;h4&gt;AAC Encoder Improvements, mentee: Rostislav Pehlivanov, mentor: Claudio Freire&lt;/h4&gt;
&lt;p&gt;Rostislav Pehlivanov has implemented PNS, TNS, I/S coding and main prediction on the native AAC encoder. Of all those extensions, only TNS was left in a less-than-usable state, but the implementation has been pushed (disabled) anyway since it's a good basis for further improvements.&lt;/p&gt;
&lt;p&gt;PNS replaces noisy bands with a single scalefactor representing the energy of that band, gaining in coding efficiency considerably, and the quality improvements on low bitrates are impressive for such a simple feature.&lt;/p&gt;
&lt;p&gt;TNS still needs some polishing, but has the potential to reduce coding artifacts by applying noise shaping in the temporal domain (something that is a source of annoying, notable distortion on low-entropy bands).&lt;/p&gt;
&lt;p&gt;Intensity Stereo coding (I/S) can double coding efficiency by exploiting strong correlation between stereo channels, most effective on pop-style tracks that employ panned mixing. The technique is not as effective on classic X-Y recordings though.&lt;/p&gt;
&lt;p&gt;Finally, main prediction improves coding efficiency by exploiting correlation among successive frames. While the gains have not been huge at this point, Rostislav has remained active even after the GSoC, and is polishing both TNS and main prediction, as well as looking for further improvements to make.&lt;/p&gt;
&lt;p&gt;In the process, the MIPS port of the encoder was broken a few times, something he's also working to fix.&lt;/p&gt;
&lt;h4&gt;Animated Portable Network Graphics (APNG), mentee: Donny Yang, mentor: Paul B Mahol&lt;/h4&gt;
&lt;p&gt;Donny Yang implemented basic keyframe only APNG encoder as the qualification task. Later he wrote interframe compression via various blend modes. The current implementation tries all blend modes and picks one which takes the smallest amount of memory.&lt;/p&gt;
&lt;p&gt;Special care was taken to make sure that the decoder plays correctly all files found in the wild and that the encoder produces files that can be played in browsers that support APNG.&lt;/p&gt;
&lt;p&gt;During his work he was tasked to fix any encountered bug in the decoder due to the fact that it doesn't match APNG specifications. Thanks to this work, a long standing bug in the PNG decoder has been fixed.&lt;/p&gt;
&lt;p&gt;For latter work he plans to continue working on the encoder, making it possible to select which blend modes will be used in the encoding process. This could speed up encoding of APNG files.&lt;/p&gt;
&lt;h3 id=&quot;pr2.8&quot;&gt;September 9th, 2015, FFmpeg 2.8&lt;/h3&gt;
&lt;p&gt;We published release &lt;strong&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_2.8&quot;&gt;2.8&lt;/a&gt;&lt;/strong&gt; as new major version. It contains all features and bug fixes of the git master branch from September 8th. Please see the &lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/FFmpeg/FFmpeg/release/2.8/Changelog&quot;&gt;changelog&lt;/a&gt;&lt;/strong&gt; for a list of the most important changes.&lt;/p&gt;
&lt;p&gt;We recommend users, distributors and system integrators to upgrade unless they use current git master.&lt;/p&gt;
&lt;h3 id=&quot;message&quot;&gt;August 1st, 2015, A message from the FFmpeg project&lt;/h3&gt;
&lt;p&gt;Dear multimedia community,&lt;/p&gt;
&lt;p&gt;The resignation of Michael Niedermayer as leader of FFmpeg yesterday has come by surprise. He has worked tirelessly on the FFmpeg project for many years and we must thank him for the work that he has done. We hope that in the future he will continue to contribute to the project. In the coming weeks, the FFmpeg project will be managed by the active contributors.&lt;/p&gt;
&lt;p&gt;The last four years have not been easy for our multimedia community - both contributors and users. We should now look to the future, try to find solutions to these issues, and to have reconciliation between the forks, which have split the community for so long.&lt;/p&gt;
&lt;p&gt;Unfortunately, much of the disagreement has taken place in inappropriate venues so far, which has made finding common ground and solutions difficult. We aim to discuss this in our communities online over the coming weeks, and in person at the &lt;a href=&quot;https://www.videolan.org/videolan/events/vdd15/&quot;&gt;VideoLAN Developer Days&lt;/a&gt; in Paris in September: a neutral venue for the entire open source multimedia community.&lt;/p&gt;
&lt;p&gt;The FFmpeg project.&lt;/p&gt;
&lt;h3 id=&quot;needhost&quot;&gt;July 4th, 2015, FFmpeg needs a new host&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt; We have received more than 7 offers for hosting and servers, thanks a lot to everyone!&lt;/p&gt;
&lt;p&gt;After graciously hosting our projects (&lt;a href=&quot;http://www.ffmpeg.org&quot;&gt;FFmpeg&lt;/a&gt;, &lt;a href=&quot;http://www.mplayerhq.hu&quot;&gt;MPlayer&lt;/a&gt; and &lt;a href=&quot;http://rtmpdump.mplayerhq.hu&quot;&gt;rtmpdump&lt;/a&gt;) for 4 years, Arpi (our hoster) has informed us that we have to secure a new host somewhere else immediately.&lt;/p&gt;
&lt;p&gt;If you want to host an open source project, please let us know, either on &lt;a href=&quot;http://ffmpeg.org/mailman/listinfo/ffmpeg-devel&quot;&gt;ffmpeg-devel&lt;/a&gt; mailing list or irc.freenode.net #ffmpeg-devel.&lt;/p&gt;
&lt;p&gt;We use about 4TB of storage and at least 4TB of bandwidth / month for various mailing lists, &lt;a href=&quot;http://trac.ffmpeg.org&quot;&gt;trac&lt;/a&gt;, &lt;a href=&quot;http://samples.ffmpeg.org&quot;&gt;samples repo&lt;/a&gt;, svn, etc.&lt;/p&gt;
&lt;h3 id=&quot;pr2.6.1&quot;&gt;March 16, 2015, FFmpeg 2.6.1&lt;/h3&gt;
&lt;p&gt;We have made a new major release (&lt;strong&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_2.6&quot;&gt;2.6&lt;/a&gt;&lt;/strong&gt;) and now one week afterward 2.6.1. It contains all features and bugfixes of the git master branch from the 6th March. Please see the &lt;strong&gt;&lt;a href=&quot;http://git.videolan.org/?p=ffmpeg.git;a=blob;f=RELEASE_NOTES;hb=release/2.6&quot;&gt;Release Notes&lt;/a&gt;&lt;/strong&gt; for a list of note-worthy changes.&lt;/p&gt;
&lt;p&gt;We recommend users, distributors and system integrators to upgrade unless they use current git master.&lt;/p&gt;
&lt;h3 id=&quot;gsoc2015&quot;&gt;March 4, 2015, Google Summer of Code&lt;/h3&gt;
&lt;p&gt;FFmpeg has been accepted as a &lt;a href=&quot;http://www.google-melange.com/gsoc/homepage/google/gsoc2015&quot;&gt;Google Summer of Code&lt;/a&gt; Project. If you wish to participate as a student see our &lt;a href=&quot;https://trac.ffmpeg.org/wiki/SponsoringPrograms/GSoC/2015&quot;&gt;project ideas page&lt;/a&gt;. You can already get in contact with mentors and start working on qualification tasks. Registration at Google for students will open March 16th. Good luck!&lt;/p&gt;
&lt;h3 id=&quot;clt2015&quot;&gt;March 1, 2015, Chemnitzer Linux-Tage&lt;/h3&gt;
&lt;p&gt;We happily announce that FFmpeg will be represented at Chemnitzer Linux-Tage (CLT) in Chemnitz, Germany. The event will take place on 21st and 22nd of March.&lt;/p&gt;
&lt;p&gt;More information can be found &lt;a href=&quot;https://chemnitzer.linux-tage.de/2015/en/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We demonstrate usage of FFmpeg, answer your questions and listen to your problems and wishes. &lt;strong&gt;If you have media files that cannot be processed correctly with FFmpeg, be sure to have a sample with you so we can have a look!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For the first time in our CLT history, there will be an &lt;strong&gt;FFmpeg workshop&lt;/strong&gt;! You can read the details &lt;a href=&quot;https://chemnitzer.linux-tage.de/2015/de/programm/beitrag/209&quot;&gt;here&lt;/a&gt;. The workshop is targeted at FFmpeg beginners. First the basics of multimedia will be covered. Thereafter you will learn how to use that knowledge and the FFmpeg CLI tools to analyse and process media files. The workshop is in German language only and prior registration is necessary. The workshop will be on Saturday starting at 10 o'clock.&lt;/p&gt;
&lt;p&gt;We are looking forward to meet you (again)!&lt;/p&gt;
&lt;h3 id=&quot;pr2.5&quot;&gt;December 5, 2014, FFmpeg 2.5&lt;/h3&gt;
&lt;p&gt;We have made a new major release (&lt;strong&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_2.5&quot;&gt;2.5&lt;/a&gt;&lt;/strong&gt;) It contains all features and bugfixes of the git master branch from the 4th December. Please see the &lt;strong&gt;&lt;a href=&quot;http://git.videolan.org/?p=ffmpeg.git;a=blob;f=RELEASE_NOTES;hb=release/2.5&quot;&gt;Release Notes&lt;/a&gt;&lt;/strong&gt; for a list of note-worthy changes.&lt;/p&gt;
&lt;p&gt;We recommend users, distributors and system integrators to upgrade unless they use current git master.&lt;/p&gt;
&lt;h3 id=&quot;ffmpeg_back_in_sid&quot;&gt;October 10, 2014, FFmpeg is in Debian unstable again&lt;/h3&gt;
&lt;p&gt;We wanted you to know there are &lt;a href=&quot;https://packages.debian.org/search?keywords=ffmpeg&amp;amp;searchon=sourcenames&amp;amp;suite=unstable&amp;amp;section=main&quot;&gt;FFmpeg packages in Debian unstable&lt;/a&gt; again. &lt;strong&gt;A big thank-you to Andreas Cadhalpun and all the people that made it possible.&lt;/strong&gt; It has been anything but simple.&lt;/p&gt;
&lt;p&gt;Unfortunately that was already the easy part of this news. The bad news is the packages probably won't migrate to Debian testing to be in the upcoming release codenamed jessie. &lt;a href=&quot;https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=763148&quot;&gt;Read the argumentation over at Debian.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;However things will come out in the end, we hope for your continued remarkable support!&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;opw03&quot;&gt;October 8, 2014, FFmpeg secured a place in OPW!&lt;/h3&gt;
&lt;p&gt;Thanks to a generous 6K USD donation by Samsung (Open Source Group), FFmpeg will be welcoming at least 1 &quot;Outreach Program for Women&quot; intern to work with our community for an initial period starting December 2014 (through March 2015).&lt;/p&gt;
&lt;p&gt;We all know FFmpeg is used by the industry, but even while there are countless products building on our code, it is not at all common for companies to step up and help us out when needed. So a big thank-you to Samsung and the OPW program committee!&lt;/p&gt;
&lt;p&gt;If you are thinking on participating in OPW as an intern, please take a look at our &lt;a href=&quot;https://trac.ffmpeg.org/wiki/SponsoringPrograms/OPW/2014-12&quot;&gt;OPW wiki page&lt;/a&gt; for some initial guidelines. The page is still a work in progress, but there should be enough information there to get you started. If you, on the other hand, are thinking on sponsoring work on FFmpeg through the OPW program, please get in touch with us at opw@ffmpeg.org. With your help, we might be able to secure some extra intern spots for this round!&lt;/p&gt;
&lt;h3 id=&quot;pr2.4&quot;&gt;September 15, 2014, FFmpeg 2.4&lt;/h3&gt;
&lt;p&gt;We have made a new major release (&lt;strong&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_2.4&quot;&gt;2.4&lt;/a&gt;&lt;/strong&gt;) It contains all features and bugfixes of the git master branch from the 14th September. Please see the &lt;strong&gt;&lt;a href=&quot;http://git.videolan.org/?p=ffmpeg.git;a=blob;f=RELEASE_NOTES;hb=release/2.4&quot;&gt;Release Notes&lt;/a&gt;&lt;/strong&gt; for a list of note-worthy changes.&lt;/p&gt;
&lt;p&gt;We recommend users, distributors and system integrators to upgrade unless they use current git master.&lt;/p&gt;
&lt;h3 id=&quot;pr2.3.3&quot;&gt;August 20, 2014, FFmpeg 2.3.3, 2.2.7, 1.2.8&lt;/h3&gt;
&lt;p&gt;We have made several new point releases (&lt;strong&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_2.3&quot;&gt;2.3.3&lt;/a&gt;, &lt;a href=&quot;http://ffmpeg.org/download.html#release_2.2&quot;&gt;2.2.7&lt;/a&gt;, &lt;a href=&quot;http://ffmpeg.org/download.html#release_1.2&quot;&gt;1.2.8&lt;/a&gt;&lt;/strong&gt;). They fix various bugs, as well as CVE-2014-5271 and CVE-2014-5272. Please see the changelog for more details.&lt;/p&gt;
&lt;p&gt;We recommend users, distributors and system integrators to upgrade unless they use current git master.&lt;/p&gt;
&lt;h3 id=&quot;opw02&quot;&gt;July 29, 2014, Help us out securing our spot in OPW&lt;/h3&gt;
&lt;p&gt;Following our previous post regarding our participation on this year's OPW (Outreach Program for Women), we are now reaching out to our users (both individuals and companies) to help us gather the needed money to secure our spot in the program.&lt;br/&gt;We need to put together 6K USD as a minimum but securing more funds would help us towards getting more than one intern.&lt;br/&gt;You can donate by credit card using &lt;a href=&quot;https://co.clickandpledge.com/advanced/default.aspx?wid=56226&quot;&gt;Click&amp;amp;Pledge&lt;/a&gt; and selecting the &quot;OPW&quot; option. If you would like to donate by money transfer or by check, please get in touch by &lt;a href=&quot;mailto:opw@ffmpeg.org&quot;&gt;e-mail&lt;/a&gt; and we will get back to you with instructions.&lt;br/&gt;Thanks!&lt;/p&gt;
&lt;h3 id=&quot;newweb&quot;&gt;July 20, 2014, New website&lt;/h3&gt;
&lt;p&gt;The FFmpeg project is proud to announce a brand new version of the website made by &lt;a href=&quot;http://db0.fr&quot;&gt;db0&lt;/a&gt;. While this was initially motivated by the need for a larger menu, the whole website ended up being redesigned, and most pages got reworked to ease navigation. We hope you'll enjoy browsing it.&lt;/p&gt;
&lt;h3 id=&quot;pr2.3&quot;&gt;July 17, 2014, FFmpeg 2.3&lt;/h3&gt;
&lt;p&gt;We have made a new major release (&lt;strong&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_2.3&quot;&gt;2.3&lt;/a&gt;&lt;/strong&gt;) It contains all features and bugfixes of the git master branch from the 16th July. Please see the &lt;strong&gt;&lt;a href=&quot;http://git.videolan.org/?p=ffmpeg.git;a=blob;f=RELEASE_NOTES;hb=489d066&quot;&gt;Release Notes&lt;/a&gt;&lt;/strong&gt; for a list of note-worthy changes.&lt;/p&gt;
&lt;p&gt;We recommend users, distributors and system integrators to upgrade unless they use current git master.&lt;/p&gt;
&lt;h3 id=&quot;opw01&quot;&gt;July 3, 2014, FFmpeg and the Outreach Program For Women&lt;/h3&gt;
&lt;p&gt;FFmpeg has started the process to become an OPW includer organization for the next round of the program, with internships starting December 9. The &lt;a href=&quot;https://gnome.org/opw/&quot;&gt;OPW&lt;/a&gt; aims to &quot;Help women (cis and trans) and genderqueer to get involved in free and open source software&quot;. Part of the process requires securing funds to support at least one internship (6K USD), so if you were holding on your donation to FFmpeg, this is a great chance for you to come forward, get in touch and help both the project and a great initiative!&lt;/p&gt;
&lt;p&gt;We have set up an &lt;a href=&quot;mailto:opw@ffmpeg.org&quot;&gt;email address&lt;/a&gt; you can use to contact us about donations and general inquires regarding our participation in the program. Hope to hear from you soon!&lt;/p&gt;
&lt;h3 id=&quot;pr2.2.4&quot;&gt;June 29, 2014, FFmpeg 2.2.4, 2.1.5, 2.0.5, 1.2.7, 1.1.12, 0.10.14&lt;/h3&gt;
&lt;p&gt;We have made several new point releases (&lt;strong&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_2.2&quot;&gt;2.2.4&lt;/a&gt;, &lt;a href=&quot;http://ffmpeg.org/download.html#release_2.1&quot;&gt;2.1.5&lt;/a&gt;, &lt;a href=&quot;http://ffmpeg.org/download.html#release_2.0&quot;&gt;2.0.5&lt;/a&gt;, &lt;a href=&quot;http://ffmpeg.org/download.html#release_1.2&quot;&gt;1.2.7&lt;/a&gt;, &lt;a href=&quot;http://ffmpeg.org/download.html#release_1.1&quot;&gt;1.1.12&lt;/a&gt;, &lt;a href=&quot;http://ffmpeg.org/download.html#release_0.10&quot;&gt;0.10.14&lt;/a&gt;&lt;/strong&gt;). They fix a &lt;a href=&quot;http://blog.securitymouse.com/2014/06/raising-lazarus-20-year-old-bug-that.html&quot;&gt;security issue in the LZO implementation&lt;/a&gt;, as well as several other bugs. See the git log for details.&lt;/p&gt;
&lt;p&gt;We recommend users, distributors and system integrators to upgrade unless they use current git master.&lt;/p&gt;
&lt;h3 id=&quot;lt2014&quot;&gt;May 1, 2014, LinuxTag&lt;/h3&gt;
&lt;p&gt;Once again FFmpeg will be represented at LinuxTag in Berlin, Germany. The event will take place from 8th to 10th of May. Please note that this year's LinuxTag is at a different location closer to the city center.&lt;/p&gt;
&lt;p&gt;We will have a shared booth with XBMC and VideoLAN. &lt;strong&gt;If you have media files that cannot be processed correctly with FFmpeg, be sure to have a sample with you so we can have a look!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;More information about LinuxTag can be found &lt;a href=&quot;http://www.linuxtag.org/2014/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We are looking forward to see you in Berlin!&lt;/p&gt;
&lt;h3 id=&quot;heartbleed&quot;&gt;April 18, 2014, OpenSSL Heartbeat bug&lt;/h3&gt;
&lt;p&gt;Our server hosting the Trac issue tracker was vulnerable to the attack against OpenSSL known as &quot;heartbleed&quot;. The OpenSSL software library was updated on 7th of April, shortly after the vulnerability was publicly disclosed. We have changed the private keys (and certificates) for all FFmpeg servers. The details were sent to the mailing lists by Alexander Strasser, who is part of the project server team. Here is a link to the user mailing list &lt;a href=&quot;https://lists.ffmpeg.org/pipermail/ffmpeg-user/2014-April/020968.html&quot;&gt;archive&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;We encourage you to read up on &lt;a href=&quot;https://www.schneier.com/blog/archives/2014/04/heartbleed.html&quot;&gt;&quot;OpenSSL heartbleed&quot;&lt;/a&gt;. &lt;strong&gt;It is possible that login data for the issue tracker was exposed to people exploiting this security hole. You might want to change your password in the tracker and everywhere else you used that same password.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;pr2.2.1&quot;&gt;April 11, 2014, FFmpeg 2.2.1&lt;/h3&gt;
&lt;p&gt;We have made a new point releases (&lt;strong&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_2.2&quot;&gt;2.2.1&lt;/a&gt;&lt;/strong&gt;). It contains bug fixes for Tickets #2893, #3432, #3469, #3486, #3495 and #3540 as well as several other fixes. See the git log for details.&lt;/p&gt;
&lt;h3 id=&quot;pr2.2&quot;&gt;March 24, 2014, FFmpeg 2.2&lt;/h3&gt;
&lt;p&gt;We have made a new major release (&lt;strong&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_2.2&quot;&gt;2.2&lt;/a&gt;&lt;/strong&gt;) It contains all features and bugfixes of the git master branch from 1st March. A partial list of new stuff is below:&lt;/p&gt;
&lt;pre&gt;
    - HNM version 4 demuxer and video decoder
    - Live HDS muxer
    - setsar/setdar filters now support variables in ratio expressions
    - elbg filter
    - string validation in ffprobe
    - support for decoding through VDPAU in ffmpeg (the -hwaccel option)
    - complete Voxware MetaSound decoder
    - remove mp3_header_compress bitstream filter
    - Windows resource files for shared libraries
    - aeval filter
    - stereoscopic 3d metadata handling
    - WebP encoding via libwebp
    - ATRAC3+ decoder
    - VP8 in Ogg demuxing
    - side &amp;amp; metadata support in NUT
    - framepack filter
    - XYZ12 rawvideo support in NUT
    - Exif metadata support in WebP decoder
    - OpenGL device
    - Use metadata_header_padding to control padding in ID3 tags (currently used in
    MP3, AIFF, and OMA files), FLAC header, and the AVI &quot;junk&quot; block.
    - Mirillis FIC video decoder
    - Support DNx444
    - libx265 encoder
    - dejudder filter
    - Autodetect VDA like all other hardware accelerations
  
&lt;/pre&gt;
&lt;p&gt;We recommend users, distributors and system integrators to upgrade unless they use current git master.&lt;/p&gt;
&lt;h3 id=&quot;clt2014&quot;&gt;February 3, 2014, Chemnitzer Linux-Tage&lt;/h3&gt;
&lt;p&gt;We happily announce that FFmpeg will be represented at `Chemnitzer Linux-Tage' in Chemnitz, Germany. The event will take place on 15th and 16th of March.&lt;/p&gt;
&lt;p&gt;More information can be found &lt;a href=&quot;http://chemnitzer.linux-tage.de/2014/en/info/&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We invite you to visit us at our booth located in the Linux-Live area! There we will demonstrate usage of FFmpeg, answer your questions and listen to your problems and wishes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you have media files that cannot be processed correctly with FFmpeg, be sure to have a sample with you so we can have a look!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are looking forward to meet you (again)!&lt;/p&gt;
&lt;h3 id=&quot;trac_sec&quot;&gt;February 9, 2014, trac.ffmpeg.org / trac.mplayerhq.hu Security Breach&lt;/h3&gt;
&lt;p&gt;The server on which FFmpeg and MPlayer Trac issue trackers were installed was compromised. The affected server was taken offline and has been replaced and all software reinstalled. FFmpeg Git, releases, FATE, web and mailinglists are on other servers and were not affected. We believe that the original compromise happened to a server, unrelated to FFmpeg and MPlayer, several months ago. That server was used as a source to clone the VM that we recently moved Trac to. It is not known if anyone used the backdoor that was found.&lt;/p&gt;
&lt;p&gt;We recommend all users to change their passwords. &lt;strong&gt;Especially users who use a password on Trac that they also use elsewhere, should change that password at least elsewhere.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;ffmpeg_rfp&quot;&gt;November 12, 2013, FFmpeg RFP in Debian&lt;/h3&gt;
&lt;p&gt;Since the splitting of Libav the Debian/Ubuntu maintainers have followed the Libav fork. Many people have requested the packaging of ffmpeg in Debian, as it is more feature-complete and in many cases less buggy.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://cynic.cc/blog/&quot;&gt;Rogério Brito&lt;/a&gt;, a Debian developer, has proposed a Request For Package (RFP) in the Debian bug tracking system.&lt;/p&gt;
&lt;p&gt;Please let the Debian and Ubuntu developers know that you support packaging of the real FFmpeg! See Debian &lt;a href=&quot;http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=729203&quot;&gt;ticket #729203&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h3 id=&quot;pr2.1&quot;&gt;October 28, 2013, FFmpeg 2.1&lt;/h3&gt;
&lt;p&gt;We have made a new major release (&lt;strong&gt;&lt;a href=&quot;http://ffmpeg.org/download.html#release_2.1&quot;&gt;2.1&lt;/a&gt;&lt;/strong&gt;) It contains all features and bugfixes of the git master branch from 28th October. A partial list of new stuff is below:&lt;/p&gt;
&lt;pre&gt;
    - aecho filter
    - perspective filter ported from libmpcodecs
    - ffprobe -show_programs option
    - compand filter
    - RTMP seek support
    - when transcoding with ffmpeg (i.e. not streamcopying), -ss is now accurate
    even when used as an input option. Previous behavior can be restored with
    the -noaccurate_seek option.
    - ffmpeg -t option can now be used for inputs, to limit the duration of
    data read from an input file
    - incomplete Voxware MetaSound decoder
    - read EXIF metadata from JPEG
    - DVB teletext decoder
    - phase filter ported from libmpcodecs
    - w3fdif filter
    - Opus support in Matroska
    - FFV1 version 1.3 is stable and no longer experimental
    - FFV1: YUVA(444,422,420) 9, 10 and 16 bit support
    - changed DTS stream id in lavf mpeg ps muxer from 0x8a to 0x88, to be
    more consistent with other muxers.
    - adelay filter
    - pullup filter ported from libmpcodecs
    - ffprobe -read_intervals option
    - Lossless and alpha support for WebP decoder
    - Error Resilient AAC syntax (ER AAC LC) decoding
    - Low Delay AAC (ER AAC LD) decoding
    - mux chapters in ASF files
    - SFTP protocol (via libssh)
    - libx264: add ability to encode in YUVJ422P and YUVJ444P
    - Fraps: use BT.709 colorspace by default for yuv, as reference fraps decoder does
    - make decoding alpha optional for prores, ffv1 and vp6 by setting
    the skip_alpha flag.
    - ladspa wrapper filter
    - native VP9 decoder
    - dpx parser
    - max_error_rate parameter in ffmpeg
    - PulseAudio output device
    - ReplayGain scanner
    - Enhanced Low Delay AAC (ER AAC ELD) decoding (no LD SBR support)
    - Linux framebuffer output device
    - HEVC decoder, raw HEVC demuxer, HEVC demuxing in TS, Matroska and MP4
    - mergeplanes filter
  
&lt;/pre&gt;
&lt;p&gt;We recommend users, distributors and system integrators to upgrade unless they use current git master.&lt;/p&gt;

</description>
<pubDate>Fri, 20 Apr 2018 21:33:46 +0000</pubDate>
<dc:creator>frakturfreund</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://ffmpeg.org/index.html</dc:identifier>
</item>
<item>
<title>Designing very large JavaScript applications</title>
<link>https://medium.com/@cramforce/designing-very-large-javascript-applications-6e013a3291a3</link>
<guid isPermaLink="true" >https://medium.com/@cramforce/designing-very-large-javascript-applications-6e013a3291a3</guid>
<description>&lt;p name=&quot;ce3f&quot; id=&quot;ce3f&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;This is a mildly edited transcript of my JSConf Australia talk. &lt;a href=&quot;https://www.youtube.com/watch?v=ZZmUwXEiPm4&quot; data-href=&quot;https://www.youtube.com/watch?v=ZZmUwXEiPm4&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Watch the whole talk on YouTube&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*DqvlkOgHSKmp5Tu1eX5mdw.png&quot; data-width=&quot;2012&quot; data-height=&quot;1128&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*DqvlkOgHSKmp5Tu1eX5mdw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*DqvlkOgHSKmp5Tu1eX5mdw.png&quot;/&gt;&lt;/div&gt;
Slide text: Hello, I used to build very large JavaScript applications.
&lt;p name=&quot;bac5&quot; id=&quot;bac5&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Hello, I used to build very large JavaScript applications. I don’t really do that anymore, so I thought it was a good time to give a bit of a retrospective and share what I learned. Yesterday I was having a beer at the conference party and I was asked: “Hey Malte, what actually gives you the right, the authority, to talk about the topic?” and I suppose answering this is actually on topic for this talk, although I usually find it a bit weird to talk about myself. So, I build this JavaScript framework at Google. It is used by Photos, Sites, Plus, Drive, Play, the search engine, all these sites. Some of them are pretty large, you might have used a few of them.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*v0r4OVf-RXr9ePakdmv5LQ.png&quot; data-width=&quot;2016&quot; data-height=&quot;1136&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*v0r4OVf-RXr9ePakdmv5LQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*v0r4OVf-RXr9ePakdmv5LQ.png&quot;/&gt;&lt;/div&gt;
Slide text: I thought React was good.
&lt;p name=&quot;a1a0&quot; id=&quot;a1a0&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;This Javascript framework is not open source. The reason it is not open source is that it kind of came out at the same time as React and I was like “Does the world really need another JS framework to choose from?”. Google already has a few of those–Angular and Polymer–and felt like another one would confuse people, so I just thought we’d just keep it to ourselves. But besides not being open source, I think there is a lot to learn from it and it is worth sharing the things we learned along the way.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*LL3uYYDMT5uIFRxR_7JxPQ.png&quot; data-width=&quot;2016&quot; data-height=&quot;1134&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*LL3uYYDMT5uIFRxR_7JxPQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*LL3uYYDMT5uIFRxR_7JxPQ.png&quot;/&gt;&lt;/div&gt;
Picture of lots of people.
&lt;p name=&quot;e419&quot; id=&quot;e419&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;So, let’s talk about very large applications and the things they have in common. Certainly that there might be a lot of developers. It might be a few dozens or even more–and these are humans with feelings and interpersonal problems and you may have to factor that in.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*WEH24kaBbar8-1gzN_AO3w.png&quot; data-width=&quot;2016&quot; data-height=&quot;1130&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*WEH24kaBbar8-1gzN_AO3w.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*WEH24kaBbar8-1gzN_AO3w.png&quot;/&gt;&lt;/div&gt;
Picture of very old building.
&lt;p name=&quot;c9c5&quot; id=&quot;c9c5&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;And even if your team is not as big, maybe you’ve been working on the thing for a while, and maybe you’re not even the first person maintaining it, you might not have all the context, there might be stuff that you don’t really understand, there might be other people in your team that don’t understand everything about the application. These are the things we have to think about when we build very large applications.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*fzb42X35lNGmkQHhJLhEBQ.png&quot; data-width=&quot;2006&quot; data-height=&quot;1128&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*fzb42X35lNGmkQHhJLhEBQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*fzb42X35lNGmkQHhJLhEBQ.png&quot;/&gt;&lt;/div&gt;
Tweet saying: A team of senior engineers without junior engineers is a team of engineers.
&lt;p name=&quot;d2eb&quot; id=&quot;d2eb&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Another thing I wanted to do here is to give this a bit of context in terms of our careers. I think many of us would consider themselves senior engineers. Or we are not quite there yet, but we want to become one. What I think being senior means is that I’d be able to solve almost every problem that somebody might throw at me. I know my tools, I know my domain. And the other important part of that job is that I make the junior engineers eventually be senior engineers.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*xpRJ1dXHMlFq1V4oDKU__w.png&quot; data-width=&quot;2006&quot; data-height=&quot;1130&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*xpRJ1dXHMlFq1V4oDKU__w.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*xpRJ1dXHMlFq1V4oDKU__w.png&quot;/&gt;&lt;/div&gt;
Slide text: Junior -&amp;gt; Senior -&amp;gt; ?
&lt;p name=&quot;12f1&quot; id=&quot;12f1&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;But what happens is that at some point we may wonder “what might be the next step?”. When we reached that seniority stage, what is the next thing we are going to do? For some of us the answer may be management, but I don’t think that should be the answer for everyone, because not everyone should be a manager, right? Some of us are really great engineers and why shouldn’t we get to do that for the rest of our lives?&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*wL5wiTWICj1keue9YZOAhQ.png&quot; data-width=&quot;3344&quot; data-height=&quot;1892&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*wL5wiTWICj1keue9YZOAhQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*wL5wiTWICj1keue9YZOAhQ.png&quot;/&gt;&lt;/div&gt;
Slide text: “I know how I would solve the problem”
&lt;p name=&quot;2420&quot; id=&quot;2420&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;I want to propose a way to level up above that senior level. The way I would talk about myself as a senior engineer is that I’d say “I know how I would solve the problem” and because I know how I would solve it I could also teach someone else to do it.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*UyLoKH7y54JAYigVlwCJpQ.png&quot; data-width=&quot;3348&quot; data-height=&quot;1892&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*UyLoKH7y54JAYigVlwCJpQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*UyLoKH7y54JAYigVlwCJpQ.png&quot;/&gt;&lt;/div&gt;
Slide text: “I know how others would solve the problem”
&lt;p name=&quot;032b&quot; id=&quot;032b&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;And my theory is that the next level is that I can say about myself “I know how &lt;em class=&quot;markup--em markup--p-em&quot;&gt;others&lt;/em&gt; would solve the problem”.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*zBBGLRIZw94gp54pspvx-g.png&quot; data-width=&quot;2006&quot; data-height=&quot;1126&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*zBBGLRIZw94gp54pspvx-g.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*zBBGLRIZw94gp54pspvx-g.png&quot;/&gt;&lt;/div&gt;
Slide text: “I can anticipate how API choices and abstractions impact the way other people would solve the problem.”
&lt;p name=&quot;fb50&quot; id=&quot;fb50&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Let’s make that a bit more concrete. You make that sentence: “I can anticipate how the API choices that I’m making, or the abstractions that I’m introducing into a project, how they impact how other people would solve a problem.” I think this is a powerful concept that allows me to reason about how the choices I’m making impact an application.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*LnDv6Ry0Hq2MaQEARaD8rg.png&quot; data-width=&quot;2002&quot; data-height=&quot;1128&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*LnDv6Ry0Hq2MaQEARaD8rg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*LnDv6Ry0Hq2MaQEARaD8rg.png&quot;/&gt;&lt;/div&gt;
Slide text: An application of empathy.
&lt;p name=&quot;3706&quot; id=&quot;3706&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;I would call this an application of empathy. You’re thinking with other software engineers and you’re thinking about how what you do and the APIs that you are giving them, how they impact how they write software.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*pnYiZTAfQqsbeS7kVkLe_g.png&quot; data-width=&quot;2010&quot; data-height=&quot;1122&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*pnYiZTAfQqsbeS7kVkLe_g.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*pnYiZTAfQqsbeS7kVkLe_g.png&quot;/&gt;&lt;/div&gt;
Slide text: Empathy on easy mode.
&lt;p name=&quot;4ad1&quot; id=&quot;4ad1&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Luckily this is empathy on easy mode. Empathy is generally hard, and this is still very hard. But at least the people that you are having empathy with, they are also other software engineers. And so while they might be very different from you, they at least have in common that they are building software. This type of empathy is really something you can get quite good at as you gain more experience.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*Op0wLWIqwZ-A5iSuWrqtKA.png&quot; data-width=&quot;2010&quot; data-height=&quot;1126&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*Op0wLWIqwZ-A5iSuWrqtKA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*Op0wLWIqwZ-A5iSuWrqtKA.png&quot;/&gt;&lt;/div&gt;
Slide text: Programming model
&lt;p name=&quot;c6c5&quot; id=&quot;c6c5&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Thinking about these topics there is one really important term that I want to talk about, which is the programming model–a word that I’m going to use a lot. It stands for “given a set of APIs, or of libraries, or of frameworks, or of tools–how do people write software in that context.” And my talk is really about, how subtle changes in APIs and so forth, how they impact the programming model.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*zuLA-tH9b8k4i1yfKMScmA.png&quot; data-width=&quot;2014&quot; data-height=&quot;1130&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*zuLA-tH9b8k4i1yfKMScmA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*zuLA-tH9b8k4i1yfKMScmA.png&quot;/&gt;&lt;/div&gt;
Slide text: Programming model impact examples: React, Preact, Redux, Date picker from npm, npm
&lt;p name=&quot;9633&quot; id=&quot;9633&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;I want to give a few examples of things that impact the programming model: Let’s say you have an Angular project and you say “I’m going to port this to React” that is obviously going to change how people write software, right? But then you’re like “Ah, 60KB for a bit of virtual DOM munging, let’s switch to Preact”–that is an API compatible library, it is not going to change how people write software, just because you make that choice. Maybe then you’re like “this is all really complex, I should have something orchestrating how my application works, I’m going to introduce Redux.”–that is going to change how people write software. You then get this requirement “we need a date picker” and you go to npm, there are 500 results, you pick one. Does it really matter which one you pick? It definitely won’t change how you write software. But having npm at your fingertips, this vast collection of modules, having that around absolutely changes how you write software. Of course, these are just a few examples of things that might or might impact how people write software.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*KfcGnWC3WcwBqGYLPiybgw.png&quot; data-width=&quot;2008&quot; data-height=&quot;1128&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*KfcGnWC3WcwBqGYLPiybgw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*KfcGnWC3WcwBqGYLPiybgw.png&quot;/&gt;&lt;/div&gt;
Slide text: Code splitting
&lt;p name=&quot;ed1d&quot; id=&quot;ed1d&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Now I want to talk about one aspect that all large JavaScript applications have in common, when you deliver them to users: Which is that they eventually get so big that you don’t want to deliver them all at once. And for this we’ve all introduced this technique called code splitting. What code splitting means is that you define a set of bundles for your application. So, you’re saying “Some users only use this part of my app, some users use another part”, and so you put together bundles that only get downloaded when the part of an application that a user is actually dealing with is executed. This is something all of us can do. Like many things it was invented by the closure compiler–at least in the JavaScript world. But I think the most popular way of doing code splitting is with webpack. And if you are using RollupJS, which is super awesome, they just recently added support for it as well. Definitely something y’all should do, but there are some things to think about when you introduce this to an application, because it does have impact on the programming model.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*vAR8HCbwiwX8bVa0xIsk6g.png&quot; data-width=&quot;2014&quot; data-height=&quot;1128&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*vAR8HCbwiwX8bVa0xIsk6g.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*vAR8HCbwiwX8bVa0xIsk6g.png&quot;/&gt;&lt;/div&gt;
Slide text: Sync -&amp;gt; Async
&lt;p name=&quot;c4fb&quot; id=&quot;c4fb&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;You have things that used to be sync that now become async. Without code splitting your application is nice and simple. There is this one big thing. It starts up, and then it is stable, you can reason about it, you don’t have to wait for stuff. With code splitting, you might sometimes say “Oh, I need that bundle”, so you now need to go to the network, and you have to factor in that this can happen, and so the applications becomes more complex.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*DqT7As1rm_M9cxyW1RIW6w.png&quot; data-width=&quot;2006&quot; data-height=&quot;1126&quot; data-is-featured=&quot;true&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*DqT7As1rm_M9cxyW1RIW6w.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*DqT7As1rm_M9cxyW1RIW6w.png&quot;/&gt;&lt;/div&gt;
Slide text: Human
&lt;p name=&quot;c9e2&quot; id=&quot;c9e2&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Also, we have humans entering the field, because code splitting requires you to define bundles, and it requires you to think about when to load them, so these humans, engineers on your team, they now have to make decisions what is going into which bundle and when to load that bundle. Every time you have a human involved, that clearly impacts the programming model, because they have to think about such things.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*0jNa8A5ciY6pCJCN65vLiA.png&quot; data-width=&quot;2014&quot; data-height=&quot;1130&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*0jNa8A5ciY6pCJCN65vLiA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*0jNa8A5ciY6pCJCN65vLiA.png&quot;/&gt;&lt;/div&gt;
Slide text: Route based code splitting
&lt;p name=&quot;eac7&quot; id=&quot;eac7&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;There is one very established way that solves this problem, that gets the human out of the mess when doing code splitting, which is called route based code splitting. If you’re not using code splitting yet, that is probably how you should do it as a first cut. Routes are the baseline URL structure of your application. You might, for example, have your product pages on `/product/` and you might have your category pages somewhere else. You just make each route one bundle, and your router in your application now understands there is code splitting. And whenever the user goes to a route, the router loads the associated bundle, and then within that route you can forget about code splitting existing. Now you are back to the programming model that is almost the same as having a big bundle for everything. It is a really nice way to do this, and definitely a good first step.&lt;/p&gt;
&lt;p name=&quot;40e0&quot; id=&quot;40e0&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;But the title of this talk is designing &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;VERY&lt;/strong&gt; large JavaScript applications, and they quickly become so big that a single bundle per route might not be feasible anymore, because the routes themselves become very big. I actually have a good example for an application that is big enough.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*ox94bGuhxWXE-OubL7St6w.png&quot; data-width=&quot;2010&quot; data-height=&quot;1132&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*ox94bGuhxWXE-OubL7St6w.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*ox94bGuhxWXE-OubL7St6w.png&quot;/&gt;&lt;/div&gt;
Google Search query screenshot for “public speaking 101”
&lt;p name=&quot;915e&quot; id=&quot;915e&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;I was figuring out how to become a public speaker coming up to this talk, and I get this nice list of blue links. You could totally envision that this page fits well into a single route bundle.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*P-XiIPnuzq9_KLA1nG-uRA.png&quot; data-width=&quot;2014&quot; data-height=&quot;1130&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*P-XiIPnuzq9_KLA1nG-uRA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*P-XiIPnuzq9_KLA1nG-uRA.png&quot;/&gt;&lt;/div&gt;
Google Search query screenshot for “weath”
&lt;p name=&quot;9f62&quot; id=&quot;9f62&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;But then I was wondering about the weather because California had a rough winter, and suddenly there was this completely different module. So, this seemingly simple route is more complicated than we thought.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*Y7e5LoeBggY01aRkJAiwWA.png&quot; data-width=&quot;2010&quot; data-height=&quot;1132&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*Y7e5LoeBggY01aRkJAiwWA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*Y7e5LoeBggY01aRkJAiwWA.png&quot;/&gt;&lt;/div&gt;
Google Search query screenshot for “20 usd to aud”
&lt;p name=&quot;369f&quot; id=&quot;369f&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;And then I was invited to this conference, and was checking out how much 1 US dollar is in Australian dollars, and there is this complex currency converter. Obviously there is about 1000s more of these specialized modules, and it infeasible to put them all in one bundle, because that bundle would be a few megabytes in size, and users would become really unhappy.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*qZhd4a0S-CCB5mUiN3fo5Q.png&quot; data-width=&quot;2012&quot; data-height=&quot;1122&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*qZhd4a0S-CCB5mUiN3fo5Q.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*qZhd4a0S-CCB5mUiN3fo5Q.png&quot;/&gt;&lt;/div&gt;
Slide text: Lazy load at component level?
&lt;p name=&quot;8d58&quot; id=&quot;8d58&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;So, we can’t just use route based code splitting, we have to come up with a different way of doing it. Route based code splitting was nice, because you split your app at the coarsest level, and everything further down could ignore it. Since I like simple things, how about doing super fine-grained instead of super coarse-grained splitting. Let’s think about what would happen if we lazy loaded every single component of our website. That seems really nice from an efficiency point of view when you only think about bandwidth. It might be super bad from other point of views like latency, but it is certainly worth a consideration.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*Lr2hIk4eH9uU33e77zeSmA.png&quot; data-width=&quot;2008&quot; data-height=&quot;1128&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*Lr2hIk4eH9uU33e77zeSmA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*Lr2hIk4eH9uU33e77zeSmA.png&quot;/&gt;&lt;/div&gt;
Slide text: React component statically depend on their children.
&lt;p name=&quot;e6f3&quot; id=&quot;e6f3&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;But let’s imagine, for example, your application uses React. And in React components statically depend on their children. That means if you stop doing that because you are lazy loading your children, then it changes your programming model, and things stop being so nice.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*SWkk2vyn344qCNCPSIkXPA.png&quot; data-width=&quot;2010&quot; data-height=&quot;1126&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*SWkk2vyn344qCNCPSIkXPA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*SWkk2vyn344qCNCPSIkXPA.png&quot;/&gt;&lt;/div&gt;
ES6 import example.
&lt;p name=&quot;8101&quot; id=&quot;8101&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Let’s say you have a currency converter component that you want to put on your search page, you import it, right? That is the normal way of doing it in ES6 modules.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*RxlHaYEav0OaODKYKiUubw.png&quot; data-width=&quot;2008&quot; data-height=&quot;1124&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*RxlHaYEav0OaODKYKiUubw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*RxlHaYEav0OaODKYKiUubw.png&quot;/&gt;&lt;/div&gt;
Loadable component example.
&lt;p name=&quot;66fa&quot; id=&quot;66fa&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;But if you want to lazy load it, you get code like this where you use dynamic import, which is a new fancy thing to lazy load ES6 modules and you wrap it in a loadable component. There are certainly 500 million ways to do this, and I am not a React expert, but all of these will change how you write the application.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*N5AMAbobPjsO_lXCPt9-ZA.png&quot; data-width=&quot;2006&quot; data-height=&quot;1122&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*N5AMAbobPjsO_lXCPt9-ZA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*N5AMAbobPjsO_lXCPt9-ZA.png&quot;/&gt;&lt;/div&gt;
Slide text: Static -&amp;gt; Dynanic
&lt;p name=&quot;7cf9&quot; id=&quot;7cf9&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;And things aren’t as nice anymore–something that was static, now becomes dynamic, which is another red flag for the programming model changing.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*j9OB_yjli59MZMyIs9V0_A.png&quot; data-width=&quot;2006&quot; data-height=&quot;1128&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*j9OB_yjli59MZMyIs9V0_A.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*j9OB_yjli59MZMyIs9V0_A.png&quot;/&gt;&lt;/div&gt;
Slide text: Who decides what to lazy load when?
&lt;p name=&quot;a925&quot; id=&quot;a925&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;You have to suddenly wonder: “Who decides what to lazy load when” because that is going to impact the latency of your application.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*rsJ-C7ph0BrJiwTjHKv6_w.png&quot; data-width=&quot;2010&quot; data-height=&quot;1124&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*rsJ-C7ph0BrJiwTjHKv6_w.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*rsJ-C7ph0BrJiwTjHKv6_w.png&quot;/&gt;&lt;/div&gt;
Slide text: Static or dynamic?
&lt;p name=&quot;a9c7&quot; id=&quot;a9c7&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The human is there again and they have to think about “there is static import, there is dynamic import, when do I use which?”. Getting this wrong is really bad because one static import, when it should have been dynamic suddenly may put stuff into the same bundle that shouldn’t be. These are the things that are going to go wrong when you have a lot of engineers over long periods of time.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*QGoX4bYhEAuNjuKwQhQ0hg.png&quot; data-width=&quot;2010&quot; data-height=&quot;1126&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*QGoX4bYhEAuNjuKwQhQ0hg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*QGoX4bYhEAuNjuKwQhQ0hg.png&quot;/&gt;&lt;/div&gt;
Slide text: Split logic and rendering
&lt;p name=&quot;850c&quot; id=&quot;850c&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Now I’m going to talk about how Google actually does this and what is one way to get a good programming model, while also achieving good performance. What we do is we take our components and we split them by rendering logic, and by application logic, like what happens when you press a button on that currency converter.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*vMskVnAwJgkZmvl4E-8E4Q.png&quot; data-width=&quot;2010&quot; data-height=&quot;1126&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*vMskVnAwJgkZmvl4E-8E4Q.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*vMskVnAwJgkZmvl4E-8E4Q.png&quot;/&gt;&lt;/div&gt;
Slide text: Only load logic if it was rendered.
&lt;p name=&quot;a69e&quot; id=&quot;a69e&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;So, now we have two separate things, and we only ever load the application logic for a component when we previously rendered it. This turns out to be a very simple model, because you can simply server side render a page, and then whatever was actually rendered, triggers downloading the associated application bundles. This puts the human out of the system, as loading is triggered automatically by rendering.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*Doqt-GOkUp13Qgk5r7WR1g.png&quot; data-width=&quot;2012&quot; data-height=&quot;1128&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*Doqt-GOkUp13Qgk5r7WR1g.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*Doqt-GOkUp13Qgk5r7WR1g.png&quot;/&gt;&lt;/div&gt;
Slide text: Currency converter on search result page.
&lt;p name=&quot;0584&quot; id=&quot;0584&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;This model may seem nice, but it does have some tradeoffs. If you know how server side rendering typically works in frameworks like React or Vue.js, what they do is a process called hydration. The way hydration works, is you server side render something, and then on the client you render it again, which means you have to load the code to render something that is already on the page, which is incredibly wasteful both in terms of loading the code and in terms of executing it. It is a bunch of wasted bandwidth, it is a bunch of wasted CPU–but it is really nice, because you get to ignore on the client side that you server side rendered something. The method we use at Google is not like that. So, if you design this very large application, you have think about: Do I take that super fast method that is more complicated, or do I go with hydration which is less efficient, but such a nice programming model? You will have to make this decision.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*uteTbmuKZF1wGvoysgsBYw.png&quot; data-width=&quot;2006&quot; data-height=&quot;1124&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*uteTbmuKZF1wGvoysgsBYw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*uteTbmuKZF1wGvoysgsBYw.png&quot;/&gt;&lt;/div&gt;
Slide text: 2017 Happy New Year
&lt;p name=&quot;bf5a&quot; id=&quot;bf5a&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;My next topic is my favorite problem in computer science–which is not naming things, although I probably gave this a bad name. It is the &lt;em class=&quot;markup--em markup--p-em&quot;&gt;“2017 holiday special problem”&lt;/em&gt;. Who here has ever written some code, and now it is no longer needed but it is still in your codebase? … This happens, and I think CSS is particularly famous for it. You have this one big CSS file. There is this selector in there. Who really knows whether that still matches anything in your app? So, you end up just keeping it there. I think the CSS community is at the forefront of a revolution, because they realized this is a problem, and they created solutions like CSS-in-JS. With that you have a single file component, the 2017HolidaySpecialComponent, and you can say “it is not 2017 anymore” and you can delete the whole component and everything is gone in one swoop. That makes it very easy to delete code. I think this is a very big idea, and it should be applied to more than just CSS.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*rkAN_sLohIO63JCOTZ1JgA.png&quot; data-width=&quot;2000&quot; data-height=&quot;1124&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*rkAN_sLohIO63JCOTZ1JgA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*rkAN_sLohIO63JCOTZ1JgA.png&quot;/&gt;&lt;/div&gt;
Slide text: Avoid central configuration at all cost
&lt;p name=&quot;bd5a&quot; id=&quot;bd5a&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;I want to give a few examples of this general idea that you want to avoid central configuration of your application at all cost, because central configuration, like having a central CSS file, makes it very hard to delete code.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*-OoPTo-xaxFr2YOGGFnapw.png&quot; data-width=&quot;3348&quot; data-height=&quot;1876&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*-OoPTo-xaxFr2YOGGFnapw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*-OoPTo-xaxFr2YOGGFnapw.png&quot;/&gt;&lt;/div&gt;
Slide text: routes.js
&lt;p name=&quot;3406&quot; id=&quot;3406&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;I was talking before about routes in your application. Many applications would have a file like “routes.js” that has all your routes, and then those routes map themselves to some root component. That is an example of central configuration, something you do not want in a large application. Because with this some engineer says “Do I still need that root component? I need to update that other file, that is owned by some other team. Not sure I’m allowed to change it. Maybe I’ll do it tomorrow”. With that these files becomes addition-only.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*NsqgsGwmgEcy_PedNzmnbQ.png&quot; data-width=&quot;3352&quot; data-height=&quot;1876&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*NsqgsGwmgEcy_PedNzmnbQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*NsqgsGwmgEcy_PedNzmnbQ.png&quot;/&gt;&lt;/div&gt;
Slide text: webpack.config.js
&lt;p name=&quot;56ca&quot; id=&quot;56ca&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Another example of this anti-pattern is the webpack.config.js file, where you have this one thing that is assumed to build your entire application. That might go fine for a while, but eventually needing to know about every aspect of what some other team did somewhere in the app just doesn’t scale. Once again, we need a pattern to emerge how to decentralize the configuration of our build process.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*L7ZmdS2JvqwWJySz-X50xw.png&quot; data-width=&quot;3348&quot; data-height=&quot;1876&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*L7ZmdS2JvqwWJySz-X50xw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*L7ZmdS2JvqwWJySz-X50xw.png&quot;/&gt;&lt;/div&gt;
Slide text: package.json
&lt;p name=&quot;a36a&quot; id=&quot;a36a&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Here is a good example: package.json, which is used by npm. Every package says “I have these dependencies, this is how you run me, this is how you build me”. Obviously there can’t be one giant configuration file for all of npm. That just wouldn’t work with hundreds of thousands of files. It would definitely get you a lot of merge conflicts in git. Sure, npm is very big, but I’d argue that many of our applications get big enough that we have to worry about the same kind of problems and have to adopt the same kind of patterns. I don’t have all the solutions, but I think that the idea that CSS-in-JS brought to the table is going to come to other aspects of our applications.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*E_g_WgMXGuJtyG-F4AGTNg.png&quot; data-width=&quot;2010&quot; data-height=&quot;1122&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*E_g_WgMXGuJtyG-F4AGTNg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*E_g_WgMXGuJtyG-F4AGTNg.png&quot;/&gt;&lt;/div&gt;
Slide text: Dependency trees
&lt;p name=&quot;c4aa&quot; id=&quot;c4aa&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;More abstractly I would describe this idea that we take responsibility for how our application is designed in the abstract, how it is organized, as &lt;em class=&quot;markup--em markup--p-em&quot;&gt;taking responsibility of shaping the dependency tree of our application&lt;/em&gt;. When I say “dependency” I mean that very abstractly. It could be module dependencies, it could be data dependencies, service dependencies, there are many different kinds.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*DfOMmyxC4guVZkyQ4IlF7g.png&quot; data-width=&quot;2008&quot; data-height=&quot;1122&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*DfOMmyxC4guVZkyQ4IlF7g.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*DfOMmyxC4guVZkyQ4IlF7g.png&quot;/&gt;&lt;/div&gt;
Slide text: Example dependency tree with router and 3 root components.
&lt;p name=&quot;dde3&quot; id=&quot;dde3&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Obviously, we all have super complicated applications, but I’m going to use a very simple example. It has only 4 components. It has a router that knows how to go from one route of your application to the next, and it has a few root components, A, B, and C.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*CivPR-20NP0dXlIkWfBk6w.png&quot; data-width=&quot;2006&quot; data-height=&quot;1122&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*CivPR-20NP0dXlIkWfBk6w.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*CivPR-20NP0dXlIkWfBk6w.png&quot;/&gt;&lt;/div&gt;
Slide text: The central import problem.
&lt;p name=&quot;65aa&quot; id=&quot;65aa&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;As I mentioned before this has the central import problem.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*Y9AgFj90bpFsKq6e7o7Jbw.png&quot; data-width=&quot;2008&quot; data-height=&quot;1124&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*Y9AgFj90bpFsKq6e7o7Jbw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*Y9AgFj90bpFsKq6e7o7Jbw.png&quot;/&gt;&lt;/div&gt;
Slide text: Example dependency tree with router and 3 root components. Router imports root components.
&lt;p name=&quot;49a7&quot; id=&quot;49a7&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Because the router now has to import all the root components, and if you want to delete one of them you have to go to the router, you have to delete the import, you have to delete the route, and eventually you have the holiday special 2017 problem.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*isSwE9e1XLiEw9sbZHwmQQ.png&quot; data-width=&quot;3348&quot; data-height=&quot;1878&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*isSwE9e1XLiEw9sbZHwmQQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*isSwE9e1XLiEw9sbZHwmQQ.png&quot;/&gt;&lt;/div&gt;
Slide text: Import -&amp;gt; Enhance
&lt;p name=&quot;d05b&quot; id=&quot;d05b&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;We at Google have come up with a solution for this, that I want to introduce to you, which I don’t think we have ever talked about. We invented a new concept. It is called enhance. It is something you use instead of import.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*7yPG-uXeixsnQk3k-X9UXw.png&quot; data-width=&quot;3336&quot; data-height=&quot;1848&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*7yPG-uXeixsnQk3k-X9UXw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*7yPG-uXeixsnQk3k-X9UXw.png&quot;/&gt;&lt;/div&gt;
Slide text: Import -&amp;gt; Enhance
&lt;p name=&quot;acca&quot; id=&quot;acca&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;In fact, it is the opposite of import. It is a reverse dependency. If you enhance a module, you make that module have a dependency on you.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*bDH4yzG0mrrYlrs2C9twsA.png&quot; data-width=&quot;2010&quot; data-height=&quot;1120&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*bDH4yzG0mrrYlrs2C9twsA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*bDH4yzG0mrrYlrs2C9twsA.png&quot;/&gt;&lt;/div&gt;
Slide text: Example dependency tree with router and 3 root components. Root components enhance router.
&lt;p name=&quot;d715&quot; id=&quot;d715&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Looking at the dependency graph, what happens it that there are still the same components, but the arrows point in the opposite direction. So, instead of the router importing the root component, the root components announce themselves using enhance to the router. This means I can get rid of a root component by just deleting the file. Because it is no longer enhancing the router, that is the only operation you have to do to delete the component.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*HDW95QuGKQCsXqwXiUtB5g.png&quot; data-width=&quot;2006&quot; data-height=&quot;1120&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*HDW95QuGKQCsXqwXiUtB5g.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*HDW95QuGKQCsXqwXiUtB5g.png&quot;/&gt;&lt;/div&gt;
Slide text: Who decides when to use enhance?
&lt;p name=&quot;868d&quot; id=&quot;868d&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;That is really nice, if it wasn’t for the humans again. They now have to think about “Do I import something, or do I use enhance? Which one do I use under which circumstances?”.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*Hr47VQZYSKiBuDap2XgbbQ.png&quot; data-width=&quot;2012&quot; data-height=&quot;1126&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*Hr47VQZYSKiBuDap2XgbbQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*Hr47VQZYSKiBuDap2XgbbQ.png&quot;/&gt;&lt;/div&gt;
Image: Danger. Hazardous chemicals.
&lt;p name=&quot;e4f6&quot; id=&quot;e4f6&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;This is particular bad case of this problem, because the power of enhancing a module, of being able to make everything else in the system have a dependency on you is very powerful and very dangerous if gotten wrong. It is easy to imagine that this might lead to really bad situations. So, at Google we decided it is a nice idea, but we make it illegal, nobody gets to use it–with one exception: generated code. It is a really good fit for generated code actually, and it solves some of the inherent problems of generated code. With generated code you sometimes have to import files you can’t even see, have to guess their names. If, however, the generated file is just there in the shadows and enhances whatever it needs, then you don’t have these problems. You never have to know about these files at all. They just magically enhance the central registry.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*od_6cmgitlBJk1g9QxU7Ng.png&quot; data-width=&quot;2008&quot; data-height=&quot;1128&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*od_6cmgitlBJk1g9QxU7Ng.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*od_6cmgitlBJk1g9QxU7Ng.png&quot;/&gt;&lt;/div&gt;
Slide text: Single file component pointing to its parts that enhance a router.
&lt;p name=&quot;7824&quot; id=&quot;7824&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Let’s take a look at a concrete example. We have our single file component here. We run a code generator on it and we extract this little route definition file from it. And that route file just says “Hey Router, here I am, please import me”. And obviously you can use this pattern for all kinds of other things. Maybe you are using GraphQL and your router should know about your data dependency, then you can just use the same pattern.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*Tg_CvUNzT9K0tbzIVC79kw.png&quot; data-width=&quot;2010&quot; data-height=&quot;1128&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*Tg_CvUNzT9K0tbzIVC79kw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*Tg_CvUNzT9K0tbzIVC79kw.png&quot;/&gt;&lt;/div&gt;
Slide text: The base bundle
&lt;p name=&quot;34c7&quot; id=&quot;34c7&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Unfortunately this is not all we need to know. There is my second favorite problem in computer science which I call the “&lt;em class=&quot;markup--em markup--p-em&quot;&gt;Base bundle pile of trash”&lt;/em&gt;. The base bundle in your graph of bundles in your application is the one bundle that will always get loaded–independent of how the user interacts with the application. So, it is particularly important, because if it is big, then everything further down will also be big. If it small, then dependent bundles at least have a chance of being small as well. A little anecdote: At some point I joined the Google Plus JavaScript infrastructure team, and I found out that their base bundle had 800KB of JavaScript. So, my warning to you is: If you want to be more successful than Google Plus, don’t have 800KB of JS in your base bundle. Unfortunately it is very easy to get to such a bad state.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*wW_u72nFdPiKjEINH4ubDg.png&quot; data-width=&quot;2012&quot; data-height=&quot;1126&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*wW_u72nFdPiKjEINH4ubDg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*wW_u72nFdPiKjEINH4ubDg.png&quot;/&gt;&lt;/div&gt;
Slide text: Base bundle pointing to 3 different dependencies.
&lt;p name=&quot;e8e8&quot; id=&quot;e8e8&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Here is an example. Your base bundle needs to depend on the routes, because when you go from A to B, you need to already know the route for B, so it has to always be around. But what you really don’t want in the base bundle is any form of UI code, because depending on how a user enters your app, there might be different UI. So, for example the date picker should absolutely not be in your base bundle, and neither should the checkout flow. But how do we prevent that? Unfortunately imports are very fragile. You might innocently import that cool &lt;em class=&quot;markup--em markup--p-em&quot;&gt;util&lt;/em&gt; package, because it has a function to make random numbers. And now somebody says “I need a utility for self driving cars” and suddenly you import the machine learning algorithms for self driving cars into your base bundle. Things like that can happen very easily since imports are transitive, and so things tend to pile up over time.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*myk-tffGyQx74OIZT4n0mw.png&quot; data-width=&quot;2004&quot; data-height=&quot;1130&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*myk-tffGyQx74OIZT4n0mw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*myk-tffGyQx74OIZT4n0mw.png&quot;/&gt;&lt;/div&gt;
Slide text: Forbidden dependency tests.
&lt;p name=&quot;0f82&quot; id=&quot;0f82&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The solution we found for this are &lt;em class=&quot;markup--em markup--p-em&quot;&gt;forbidden dependency tests&lt;/em&gt;. Forbidden dependency tests are a way to assert that for example your base bundle does not depend on any UI.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*vDtioYTfzhCB9e7jc9A4pg.png&quot; data-width=&quot;2006&quot; data-height=&quot;1126&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*vDtioYTfzhCB9e7jc9A4pg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*vDtioYTfzhCB9e7jc9A4pg.png&quot;/&gt;&lt;/div&gt;
Slide text: Assert that base bundle does not depend on React.Component
&lt;p name=&quot;1daa&quot; id=&quot;1daa&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Let’s take a look at a concrete example. In React every component needs to inherit from React.Component. So , if your goal is that no UI could ever be in the base bundle just add this one test that asserts that React.Component is not a transitive dependency of your base bundle.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*s5rDafWJi90dcrlEQSAepg.png&quot; data-width=&quot;2008&quot; data-height=&quot;1120&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*s5rDafWJi90dcrlEQSAepg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*s5rDafWJi90dcrlEQSAepg.png&quot;/&gt;&lt;/div&gt;
Forbidden dependencies crossed out.
&lt;p name=&quot;4a91&quot; id=&quot;4a91&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Looking at the previous example again, you just get a test failure when someone wants to add the date picker. And these test failures are typically very easy to fix right then, because usually that person didn’t really mean to add the dependency–it just crept in through some transitive path. Compare this to when this dependency would have been around for 2 years because you didn’t have a test. In those cases it is typically extremely hard to refactor your code to get rid of the dependency.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*ONmcxDRRdY9DpR8QfwMj4g.png&quot; data-width=&quot;2006&quot; data-height=&quot;1120&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*ONmcxDRRdY9DpR8QfwMj4g.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*ONmcxDRRdY9DpR8QfwMj4g.png&quot;/&gt;&lt;/div&gt;
Slide text: The most natural path
&lt;p name=&quot;765f&quot; id=&quot;765f&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Ideally though, you find that most natural path.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*7XRIRO-_Y165Gn7Zff_fKQ.png&quot; data-width=&quot;2008&quot; data-height=&quot;1128&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*7XRIRO-_Y165Gn7Zff_fKQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*7XRIRO-_Y165Gn7Zff_fKQ.png&quot;/&gt;&lt;/div&gt;
Slide text: Most straightforward way must be the right way.
&lt;p name=&quot;27d5&quot; id=&quot;27d5&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;You want to get to a state where whatever the engineers on your team do, the most straightforward way is also the right way–so that they don’t get off the path, so that they naturally do the right thing.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*T6E-ExC2HWa0X--OiJ_vAA.png&quot; data-width=&quot;2012&quot; data-height=&quot;1124&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*T6E-ExC2HWa0X--OiJ_vAA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*T6E-ExC2HWa0X--OiJ_vAA.png&quot;/&gt;&lt;/div&gt;
Slide text: Otherwise add a test that ensure the right way,
&lt;p name=&quot;6cc9&quot; id=&quot;6cc9&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;This might not always be possible. In that case just add a test. But this is not something that many people feel empowered to do. But &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;please feel empowered to add tests to your application that ensure the major invariants of your infrastructure&lt;/strong&gt;. Tests are not only for testing that your math functions do the right thing. They are also for infrastructure and for the major design features of your application.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*y3COuLXS8b1vAQQESjp30Q.png&quot; data-width=&quot;2010&quot; data-height=&quot;1128&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*y3COuLXS8b1vAQQESjp30Q.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*y3COuLXS8b1vAQQESjp30Q.png&quot;/&gt;&lt;/div&gt;
Slide text: Avoid human judgement outside of application domain.
&lt;p name=&quot;ab16&quot; id=&quot;ab16&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Try to avoid human judgement whenever possible outside of the application domain. When working on an application we have to understand the business, but not every engineer in your organization can and will understand how code splitting works. And they don’t need to do that. Try to introduce these thing into your application in a way that is fine when not everybody understands them and keeps the complexity in their heads.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*CqeGbdnSFMRPtZWPIRZCvw.png&quot; data-width=&quot;2004&quot; data-height=&quot;1126&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*CqeGbdnSFMRPtZWPIRZCvw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*CqeGbdnSFMRPtZWPIRZCvw.png&quot;/&gt;&lt;/div&gt;
Slide text: Make it easy to delete code.
&lt;p name=&quot;86ce&quot; id=&quot;86ce&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;And really just make it easy to delete code. My talk is called “building very large JavaScript applications”. The best advice I can give: Don’t let your applications get very large. The best way to not get there is to delete stuff before it is too late.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*Mt_beSIamHND0E6NjBBetA.png&quot; data-width=&quot;2006&quot; data-height=&quot;1120&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*Mt_beSIamHND0E6NjBBetA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*Mt_beSIamHND0E6NjBBetA.png&quot;/&gt;&lt;/div&gt;
Slide text: No abstraction is better than the wrong abstraction.
&lt;p name=&quot;9553&quot; id=&quot;9553&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;I want to address just one more point, which is that people sometimes say that having no abstractions at all is better than having the wrong abstractions. What this really means is that the cost of the wrong abstraction is very high, so be careful. I think this is sometimes misinterpreted. It does not mean that you should have no abstractions. It just means you have to be very careful.&lt;/p&gt;
&lt;blockquote name=&quot;d28d&quot; id=&quot;d28d&quot; class=&quot;graf graf--pullquote graf-after--p&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;W&lt;em class=&quot;markup--em markup--pullquote-em&quot;&gt;e have to become good at finding the right abstractions&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*oNXlH0ththqRlPeRm2z0Sw.png&quot; data-width=&quot;2008&quot; data-height=&quot;1120&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*oNXlH0ththqRlPeRm2z0Sw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*oNXlH0ththqRlPeRm2z0Sw.png&quot;/&gt;&lt;/div&gt;
Slide text: Empathy and experience -&amp;gt; Right abstractions.
&lt;p name=&quot;1fe3&quot; id=&quot;1fe3&quot; class=&quot;graf graf--p graf-after--figure graf--trailing&quot;&gt;As I was saying at the start of the presentation: The way to get there is to use empathy and think with your engineers on your team about how they will use your APIs and how they will use your abstractions. Experience is how you flesh out that empathy over time. Put together, empathy and experience is what enables you to choose the right abstractions for your application&lt;/p&gt;
</description>
<pubDate>Fri, 20 Apr 2018 20:28:29 +0000</pubDate>
<dc:creator>kawera</dc:creator>
<og:title>Designing very large (JavaScript) applications – Malte Ubl – Medium</og:title>
<og:url>https://medium.com/@cramforce/designing-very-large-javascript-applications-6e013a3291a3</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*DqT7As1rm_M9cxyW1RIW6w.png</og:image>
<og:description>This is a mildly edited transcript of my JSConf Australia talk. Watch the whole talk on YouTube.</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://medium.com/@cramforce/designing-very-large-javascript-applications-6e013a3291a3</dc:identifier>
</item>
<item>
<title>Rethinking GPS: Engineering Next-Gen Location at Uber</title>
<link>https://ubere.ng/2qLlaFH</link>
<guid isPermaLink="true" >https://ubere.ng/2qLlaFH</guid>
<description>&lt;p&gt;&lt;span&gt;Location and navigation using&lt;/span&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Global_Positioning_System&quot;&gt;&lt;span&gt;global positioning systems&lt;/span&gt;&lt;/a&gt; &lt;span&gt;(GPS) is deeply embedded in our daily lives, and is particularly crucial to Uber’s services. To orchestrate quick, efficient pickups, our GPS technologies need to know the locations of matched riders and drivers, as well as provide navigation guidance from a driver’s current location to where the rider needs to be picked up, and then, to the rider’s chosen destination. For this process to work seamlessly, the location estimates for riders and drivers need to be as precise as possible.  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Since&lt;/span&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Global_Positioning_System&quot;&gt;&lt;span&gt;the (literal!) launch of GPS in 1973&lt;/span&gt;&lt;/a&gt;&lt;span&gt;, we have advanced our understanding of the world, experienced exponential growth in the computational power available to us, and developed powerful algorithms to model uncertainty from fields like robotics. While our lives have become increasingly dependent on GPS, the fundamentals of how GPS works have not changed that much, which leads to significant performance limitations. In our opinion, it is time to rethink some of the starting assumptions that were true in 1973 regarding where and how we use GPS, as well as the computational power and additional information we can bring to bear to improve it.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;While GPS works well under clear skies, its location estimates can be wildly inaccurate (with a margin of error of 50 meters or more) when we need it the most: in densely populated and highly built-up urban areas, where many of our users are located. To overcome this challenge, we developed a software upgrade to GPS for Android which substantially improves location accuracy in urban environments via a client-server architecture that utilizes 3D maps and performs sophisticated probabilistic computations on GPS data available through&lt;/span&gt; &lt;a href=&quot;https://developer.android.com/guide/topics/sensors/gnss.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;Android’s GNSS APIs&lt;/span&gt;&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;In this article, we discuss why GPS can perform poorly in urban environments and outline how we fix it using advanced signal processing algorithms deployed at scale on our server infrastructure.&lt;/span&gt;&lt;/p&gt;
&lt;div id=&quot;attachment_1324&quot; class=&quot;wp-caption aligncenter&quot; readability=&quot;35&quot;&gt;&lt;a href=&quot;http://eng.uber.com/wp-content/uploads/2017/07/image3.gif&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-1324 size-full&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2017/07/image3.gif&quot; alt=&quot;&quot; width=&quot;1416&quot; height=&quot;980&quot;/&gt;&lt;/a&gt;
&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 1: The above GIF offers a comparison of standard GPS (red) against our improved location estimate (blue) for a pickup from Uber HQ in San Francisco. Our estimated location closely follows the true path taken by the rider, while GPS shows very large excursions.&lt;/p&gt;
&lt;/div&gt;

&lt;h3&gt;&lt;span&gt;A bit of background on GPS/GNSS&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;Before discussing our approach in detail, let us do a quick recap of how GPS works in order to understand why it can be inaccurate in high-rise urban environments.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;GPS is a network of more than 30 satellites operated by the U.S. government, orbiting the earth at an altitude of about 20,000 kilometers. (Most cell phones these days can pick up similar Russian “&lt;/span&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/GLONASS&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;GLONASS&lt;/span&gt;&lt;/a&gt;&lt;span&gt;” satellites too.)  These satellites send out radio frequency signals that GPS receivers, such as those found in cell phones, can lock onto. Importantly, these satellites advertise the time at which they launch their signals.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;For each satellite whose signal the receiver processes, the difference between reception time and launch time (time-of-flight), multiplied by the speed of light, is called the&lt;/span&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Pseudorange&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;pseudorange&lt;/span&gt;&lt;/a&gt;&lt;em&gt;&lt;span&gt;.&lt;/span&gt;&lt;/em&gt; &lt;span&gt;If the satellite and receiver’s clocks are synchronized, and the signal travels along the straight line-of-sight path, then this would equal the actual distance to the satellite. However, the clocks are not synchronized, so the receiver needs to solve for four unknowns, its own 3D coordinates on the globe, and its clock bias. Thus, we need a minimum of four satellites (four equations) to solve for these four unknowns.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;If we ignore clock bias, we can intuitively interpret the location estimate performed by the GPS receiver by intersecting spheres centered at the satellites with the radius of each sphere given by the pseudorange. In practice, a GPS receiver processes signals from a significantly larger number of satellites (up to 20 GPS and GLONASS satellites are visible in an open field), and having more than the minimum number of equations provides extra robustness to noise, blockages, etc. &lt;/span&gt;&lt;span&gt;In addition to GPS and GLONASS, some new/future receivers can/will process signals from other satellite systems.  Other navigation satellite systems coming online are&lt;/span&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Galileo_(satellite_navigation)&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;Galileo&lt;/span&gt;&lt;/a&gt;&lt;span&gt;, operated by the European Union,&lt;/span&gt; &lt;span&gt;IRNSS&lt;/span&gt; &lt;span&gt;in India, and&lt;/span&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/BeiDou_Navigation_Satellite_System&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;BeiDou&lt;/span&gt;&lt;/a&gt;&lt;span&gt;, operated by China. The more general term&lt;/span&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Satellite_navigation&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;GNSS&lt;/span&gt;&lt;/a&gt; &lt;span&gt;(global navigation satellite systems) encompasses these systems. (We will use this term in the remainder of the article.)  &lt;/span&gt;&lt;/p&gt;
&lt;div id=&quot;attachment_1325&quot; class=&quot;wp-caption aligncenter&quot; readability=&quot;34&quot;&gt;&lt;img class=&quot;wp-image-1325&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2017/07/image1-4-968x1024.png&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;635&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2017/07/image1-4-968x1024.png 968w, https://eng.uber.com/wp-content/uploads/2017/07/image1-4-284x300.png 284w, https://eng.uber.com/wp-content/uploads/2017/07/image1-4-768x812.png 768w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 2: In this simplified interpretation of GPS receiver computation, spheres intersect at the center of known satellite locations.&lt;/p&gt;
&lt;/div&gt;

&lt;h3&gt;&lt;span&gt;Why GNSS location is inaccurate in urban environments&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;A major assumption behind GNSS-based positioning is that the receiver has a direct line-of-sight to each satellite whose pseudorange it is computing. This works seamlessly in open terrain but really breaks down in urban environments, as shown in Figure 3, below:&lt;/span&gt;&lt;/p&gt;
&lt;div id=&quot;attachment_3071&quot; class=&quot;wp-caption aligncenter&quot; readability=&quot;32&quot;&gt;&lt;a href=&quot;http://eng.uber.com/wp-content/uploads/2018/04/image9.jpg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-3071&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2018/04/image9.jpg&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;338&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2018/04/image9.jpg 1920w, https://eng.uber.com/wp-content/uploads/2018/04/image9-300x169.jpg 300w, https://eng.uber.com/wp-content/uploads/2018/04/image9-768x432.jpg 768w, https://eng.uber.com/wp-content/uploads/2018/04/image9-1024x576.jpg 1024w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;/a&gt;
&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 3: Line-of-sight blockage and strong reflections can cause large GPS errors.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;span&gt;Buildings often block the lines of sight to satellites, so the receiver frequently processes signals corresponding to strong reflections off of other buildings. The significant inaccuracy (positive offsets) in pseudoranges resulting from this phenomenon can lead to errors in position estimates that can be 50 meters or more in urban canyons. Most of us who have wandered,  driven around, or requested an Uber in big cities have experienced these problems first hand.&lt;/span&gt;&lt;/p&gt;

&lt;h3&gt;&lt;span&gt;Satellite signal strengths to the rescue&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;Our approach to improving location accuracy makes a feature out of the very blockage of GNSS signals that causes trouble for standard receivers.  How? For Android phones, the LocationManager API provides not just the phone’s position estimate, but also the signal-to-noise ratio (SNR) for each GNSS satellite in view. If we put this “signal strength” information together with 3D maps, then we can obtain very valuable location information. Figure 4, below, shows a simplified version of how satellite SNRs and 3D maps can be used to infer which side of the street we are on:&lt;/span&gt;&lt;/p&gt;
&lt;div id=&quot;attachment_3072&quot; class=&quot;wp-caption aligncenter&quot; readability=&quot;35&quot;&gt;&lt;a href=&quot;http://eng.uber.com/wp-content/uploads/2018/04/image8.jpg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-3072&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2018/04/image8.jpg&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;338&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2018/04/image8.jpg 1920w, https://eng.uber.com/wp-content/uploads/2018/04/image8-300x169.jpg 300w, https://eng.uber.com/wp-content/uploads/2018/04/image8-768x432.jpg 768w, https://eng.uber.com/wp-content/uploads/2018/04/image8-1024x576.jpg 1024w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;/a&gt;
&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 4: Satellite signal strengths, when combined with 3D maps, provide valuable location information.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;span&gt;Zooming into the details, our approach relies on putting the following intuition in a mathematical framework: if the SNR for a satellite is low, then the line-of-sight path is probably blocked or shadowed; if the SNR is high, then the LOS is probably clear. The qualifier “probably” is crucial here: even when the receiver is in a shadowed area, strong reflected signals can still reach it, and even if it is in a clear area, the received signal can be weak (because of destructive interference between LOS and reflected paths, a phenomenon referred to as&lt;/span&gt; &lt;a href=&quot;http://www.radio-electronics.com/info/propagation/multipath/multipath-fading.php&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;multipath fading&lt;/span&gt;&lt;/a&gt;&lt;span&gt;).  Also, in general, the 3D map is not entirely accurate, and certainly does not capture random blockages by large moving objects not in the map, like trucks. This adds additional uncertainty to the process.  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;Probabilistic shadow matching using ray tracing&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;While the intuition on satellite signal strengths carrying useful location information is sound, it must be fleshed out within a probabilistic framework. For any possible location for the receiver, we can check whether the ray joining the location to the satellite is blocked using our 3D map. Now, using a model for the probability distribution of the SNR under LOS and shadowed conditions, we determine the likelihood of the SNR measured for that satellite. For example, if the location is shadowed, then the likelihood of a high SNR is low. The overall likelihood of a given location, based on the satellite SNRs, is the product of the likelihoods corresponding to the different satellites. By doing this over a grid of possible locations, we obtain a likelihood surface—or heat map—of possible receiver locations, based on satellite signal strengths alone. We call this procedure&lt;/span&gt; &lt;a href=&quot;http://www.insidegnss.com/node/4628&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;probabilistic shadow matching&lt;/span&gt;&lt;/a&gt;&lt;em&gt;&lt;span&gt;.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;div id=&quot;attachment_1328&quot; class=&quot;wp-caption aligncenter&quot; readability=&quot;33&quot;&gt;&lt;img class=&quot;wp-image-1328&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2017/07/image8-2-1024x622.png&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;364&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2017/07/image8-2-1024x622.png 1024w, https://eng.uber.com/wp-content/uploads/2017/07/image8-2-300x182.png 300w, https://eng.uber.com/wp-content/uploads/2017/07/image8-2-768x467.png 768w, https://eng.uber.com/wp-content/uploads/2017/07/image8-2.png 1508w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 5: Ray tracing from one possible location to each satellite for probabilistic shadow matching. This is done for thousands of hypothesized locations.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;span&gt;The likelihood surface, or heat map, from probabilistic shadow matching summarizes the information from satellite SNR measurements. However, as we see from Figure 6 below, this heat map can be pretty complicated. It can have many distinct, widely separated hotpots (local maxima) often corresponding to a given side of the street, but sometimes still in the wrong location (i.e., phantoms).  In order to narrow down our location estimate and to avoid locking onto the phantoms, we must now fuse this information with even more information.&lt;/span&gt;&lt;/p&gt;
&lt;div id=&quot;attachment_1329&quot; class=&quot;wp-caption aligncenter&quot; readability=&quot;38&quot;&gt;&lt;img class=&quot;wp-image-1329&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2017/07/image9-2-1024x530.png&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;310&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2017/07/image9-2-1024x530.png 1024w, https://eng.uber.com/wp-content/uploads/2017/07/image9-2-300x155.png 300w, https://eng.uber.com/wp-content/uploads/2017/07/image9-2-768x397.png 768w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 6. A location heat map computed based on satellite signal strengths can have many hotspots. In the above example, our improved location estimate (blue path, black uncertainty ellipse) follows ground truth (yellow path), whereas standard GPS (red path, gray uncertainty ellipse) is inaccurate.&lt;/p&gt;
&lt;/div&gt;
&lt;h4&gt;&lt;span&gt;Information fusion via particle filter&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;&lt;span&gt;For Android phones, the information we use in addition to satellite signal strengths is usually just the standard GNSS position fix, but can also be Android Fused locations, which may include WiFi-based positioning. Since this location can be very inaccurate, single time instant (one-shot) fusion of GNSS fix with shadow matching likelihoods typically leads to poor performance.  In order to take advantage of the information from satellite signal strengths, we trust GPS less in built-up areas (the gray GPS uncertainty ellipse in Figure 6 is a typical model that we use, while the black uncertainty ellipse for improved GPS is an output of our algorithm). We therefore use past measurements and constrain the location evolution over time using a motion model adapted to the application (e.g., pedestrian vs. vehicular motion). This is accomplished by using a&lt;/span&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Particle_filter&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;particle filter&lt;/span&gt;&lt;/a&gt;&lt;span&gt;, which approximates the probability distribution of the receiver’s location at any given time by a set of weighted particles. In other words, we estimate where the phone is using thousands of hypothesized locations (i.e., particles).  &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Over time, the probability weights and particle locations evolve based on the measurements and the motion model. Since the heat map from probabilistic shadow matching has so many local maxima and because the GNSS fix can have such large outliers, we cannot use standard techniques such as the&lt;/span&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Kalman_filter&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;Kalman filter&lt;/span&gt;&lt;/a&gt; &lt;span&gt;or the&lt;/span&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Extended_Kalman_filter&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;extended Kalman filter&lt;/span&gt;&lt;/a&gt;&lt;span&gt;, which rely on the tracked probability distribution being well approximated by a bell-shaped&lt;/span&gt; &lt;a href=&quot;https://en.wikipedia.org/wiki/Normal_distribution&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;Gaussian distribution&lt;/span&gt;&lt;/a&gt;&lt;span&gt;. The particle filter allows us to approximate arbitrary distributions, at the expense of higher complexity, and this is where our server infrastructure comes in.&lt;/span&gt;&lt;/p&gt;
&lt;div id=&quot;attachment_1330&quot; class=&quot;wp-caption aligncenter&quot; readability=&quot;36&quot;&gt;&lt;img class=&quot;wp-image-1330&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2017/07/image5-3-1024x493.png&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;289&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2017/07/image5-3-1024x493.png 1024w, https://eng.uber.com/wp-content/uploads/2017/07/image5-3-300x145.png 300w, https://eng.uber.com/wp-content/uploads/2017/07/image5-3-768x370.png 768w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 7: The location estimate obtained as the weighted centroid of the hotspot provided by the particle filter often corrects very large GPS errors. The uncertainty radius (white circle) for improved GPS is based on the spread of the particle set, and is often a more realistic measure than the small uncertainty radius (black circle) typically reported for raw GPS even when the actual position errors are large.&lt;/p&gt;
&lt;/div&gt;

&lt;h3&gt;&lt;span&gt;From signal processing to software at scale&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;The combination of particle filtering and ray tracing introduces complexity to the back-end server ecosystem, making for a very stateful service.&lt;/span&gt;&lt;/p&gt;
&lt;div id=&quot;attachment_1331&quot; class=&quot;wp-caption aligncenter&quot; readability=&quot;39&quot;&gt;&lt;img class=&quot;wp-image-1331&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2017/07/image2-2-1024x707.png&quot; alt=&quot;&quot; width=&quot;600&quot; height=&quot;415&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2017/07/image2-2-1024x707.png 1024w, https://eng.uber.com/wp-content/uploads/2017/07/image2-2-300x207.png 300w, https://eng.uber.com/wp-content/uploads/2017/07/image2-2-768x531.png 768w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 8: Uber’s GPS improvement system is composed of a particle filter service, 3D map tile management service, a manager service, Uber HTTP API, and cloud storage, and integrates with other Uber services.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;span&gt;There are two kinds of state at play: per-user particle filter state and per-region 3D maps used for ray tracing. The use of particle filters necessitates a level of server affinity. Each new request to our service must be routed to the same back-end server for processing in order to update the correct particle filter. Additionally, due to the large size of 3D maps, each back-end server can only hold a few small sections of the 3D world in RAM.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Since each server can only hold a few square kilometers of map data, not all servers are capable of serving all users. Essentially, implementing the back-end systems for our solution necessitated the creation of a sticky session routing layer that takes server 3D map state into account. In addition to internal tests and performance evaluations, we also run spot checks on our own Android devices using an internal version of the Uber rider app, as illustrated in Figure 9, below:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter wp-image-3074&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2018/04/image4-1.png&quot; alt=&quot;&quot; width=&quot;300&quot; height=&quot;110&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2018/04/image4-1.png 778w, https://eng.uber.com/wp-content/uploads/2018/04/image4-1-300x110.png 300w, https://eng.uber.com/wp-content/uploads/2018/04/image4-1-768x280.png 768w&quot; sizes=&quot;(max-width: 300px) 100vw, 300px&quot;/&gt;&lt;/p&gt;
&lt;div id=&quot;attachment_3073&quot; class=&quot;wp-caption aligncenter&quot; readability=&quot;33&quot;&gt;&lt;a href=&quot;http://eng.uber.com/wp-content/uploads/2018/04/image7.png&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-3073&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2018/04/image7.png&quot; alt=&quot;&quot; width=&quot;400&quot; height=&quot;711&quot; srcset=&quot;https://eng.uber.com/wp-content/uploads/2018/04/image7.png 1080w, https://eng.uber.com/wp-content/uploads/2018/04/image7-169x300.png 169w, https://eng.uber.com/wp-content/uploads/2018/04/image7-768x1365.png 768w, https://eng.uber.com/wp-content/uploads/2018/04/image7-576x1024.png 576w&quot; sizes=&quot;(max-width: 400px) 100vw, 400px&quot;/&gt;&lt;/a&gt;
&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 9: Red dot/blue dot comparison on our internal version of the rider app allows Uber employees to spot check our solution anywhere in the world.&lt;/p&gt;
&lt;/div&gt;

&lt;h3&gt;&lt;span&gt;Moving forward&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;Accurate estimation of rider and driver location is a crucial requirement for fulfilling Uber’s mission of providing transportation as reliable as running water, everywhere, for everyone.&lt;/span&gt; &lt;span&gt;To meet our mission, the Sensing, Intelligence, and Research team is working on a variety of approaches for improving location with creative use of sensors and computation on mobile devices, coupled with the computational power of our server infrastructure. The combination of advanced signal processing, machine learning algorithms, and software at scale has huge potential, and we are always looking for talented and highly motivated individuals (&lt;/span&gt;&lt;a href=&quot;https://www.uber.com/careers/list/14345/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;software and algorithms engineers&lt;/span&gt;&lt;/a&gt;&lt;span&gt;,&lt;/span&gt; &lt;a href=&quot;https://www.uber.com/careers/list/13203/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;data visualization engineers&lt;/span&gt;&lt;/a&gt;&lt;span&gt;, and &lt;/span&gt;&lt;a href=&quot;https://www.uber.com/careers/list/37852/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;span&gt;machine learning engineers&lt;/span&gt;&lt;/a&gt;&lt;span&gt;) to join us to help realize this potential.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;span&gt;Danny Iland, Andrew Irish, Upamanyu Madhow, &amp;amp; Brian Sandler are members of Uber’s Sensing, Inference and Research team. Danny, Andrew, and Upamanyu were part of the original group that led this research at the University of California, Santa Barbara. After spinning this work into a startup, they demonstrated server-based particle filtering for location improvement in San Francisco using a 3D map constructed from publicly available aerial LiDAR data. They joined Uber in July 2016.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;



&lt;div id=&quot;sexy-author-bio&quot; class=&quot;danny-iland&quot; readability=&quot;8.0480769230769&quot;&gt;&lt;div id=&quot;sab-social-wrapper&quot;&gt;&lt;a id=&quot;sab-sablinkedin&quot; href=&quot;https://www.linkedin.com/in/daniel-iland-33b252b/&quot; target=&quot;_top&quot;&gt;&lt;img id=&quot;sig-sablinkedin&quot; alt=&quot;Danny Iland on Linkedin&quot; src=&quot;https://eng.uber.com/wp-content/plugins/sexy-author-bio/public/assets/images/flat-circle/sablinkedin.png&quot;/&gt;&lt;/a&gt;&lt;/div&gt;

&lt;div id=&quot;sab-gravatar&quot;&gt;&lt;a href=&quot;https://eng.uber.com/author/danny-iland/&quot; target=&quot;_top&quot;&gt;&lt;img alt=&quot;Danny Iland&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2018/04/up_8f7223e3-b7a5-432b-b56f-ff7197ba05bf-1.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;Danny Iland is a senior software engineer on Uber’s Sensing, Inference, and Research team.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;sexy-author-bio&quot; class=&quot;andrew-irish&quot; readability=&quot;7.9811320754717&quot;&gt;&lt;div id=&quot;sab-social-wrapper&quot;&gt;&lt;a id=&quot;sab-sablinkedin&quot; href=&quot;https://www.linkedin.com/in/andrew-irish-565b6324/&quot; target=&quot;_top&quot;&gt;&lt;img id=&quot;sig-sablinkedin&quot; alt=&quot;Andrew Irish on Linkedin&quot; src=&quot;https://eng.uber.com/wp-content/plugins/sexy-author-bio/public/assets/images/flat-circle/sablinkedin.png&quot;/&gt;&lt;/a&gt;&lt;/div&gt;

&lt;div id=&quot;sab-gravatar&quot;&gt;&lt;a href=&quot;https://eng.uber.com/author/andrew-irish/&quot; target=&quot;_top&quot;&gt;&lt;img alt=&quot;Andrew Irish&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2018/04/up_aea689ce-7016-4311-b4ce-edd8ca585f9c.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;Andrew Irish is a senior software engineer on Uber’s Sensing, Inference, and Research team.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;sexy-author-bio&quot; class=&quot;upamanyu-madhow&quot; readability=&quot;8.1455696202532&quot;&gt;&lt;div id=&quot;sab-social-wrapper&quot;&gt;&lt;a id=&quot;sab-sablinkedin&quot; href=&quot;https://www.linkedin.com/in/upamanyu-madhow-7a124512/&quot; target=&quot;_top&quot;&gt;&lt;img id=&quot;sig-sablinkedin&quot; alt=&quot;Upamanyu Madhow on Linkedin&quot; src=&quot;https://eng.uber.com/wp-content/plugins/sexy-author-bio/public/assets/images/flat-circle/sablinkedin.png&quot;/&gt;&lt;/a&gt;&lt;/div&gt;

&lt;div id=&quot;sab-gravatar&quot;&gt;&lt;a href=&quot;https://eng.uber.com/author/upamanyu-madhow/&quot; target=&quot;_top&quot;&gt;&lt;img alt=&quot;Upamanyu Madhow&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2018/04/bg_919763ee-15a3-45ca-a907-6ea7c99faab9-mpI9EDSTIR.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;Upamanyu Madhow is a researcher at Uber and a professor of Electrical and Computer Engineering at the University of California, Santa Barbara.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;sexy-author-bio&quot; class=&quot;brian-sandler&quot; readability=&quot;9.2168674698795&quot;&gt;&lt;div id=&quot;sab-social-wrapper&quot;&gt;&lt;a id=&quot;sab-sablinkedin&quot; href=&quot;https://www.linkedin.com/in/sandlerbrian/&quot; target=&quot;_top&quot;&gt;&lt;img id=&quot;sig-sablinkedin&quot; alt=&quot;Brian Sandler on Linkedin&quot; src=&quot;https://eng.uber.com/wp-content/plugins/sexy-author-bio/public/assets/images/flat-circle/sablinkedin.png&quot;/&gt;&lt;/a&gt;&lt;/div&gt;

&lt;div id=&quot;sab-gravatar&quot;&gt;&lt;a href=&quot;https://eng.uber.com/author/brian-sandler/&quot; target=&quot;_top&quot;&gt;&lt;img alt=&quot;Brian Sandler&quot; src=&quot;https://eng.uber.com/wp-content/uploads/2018/04/0.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;Brian Sandler was a summer intern on Uber’s Sensing, Inference, and Research team and is currently a Ph.D student with the University of Pennsylvania.&lt;/p&gt;
&lt;/div&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;div class=&quot;post-meta&quot;&gt;
&lt;ul&gt;&lt;li&gt;Categories: &lt;a href=&quot;https://eng.uber.com/category/general-engineering/&quot; rel=&quot;category tag&quot;&gt;General Engineering&lt;/a&gt; &lt;span class=&quot;muted&quot;&gt;/&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Tags: &lt;a href=&quot;https://eng.uber.com/tag/3d-mapping/&quot; rel=&quot;tag&quot;&gt;3D mapping&lt;/a&gt;, &lt;a href=&quot;https://eng.uber.com/tag/android/&quot; rel=&quot;tag&quot;&gt;Android&lt;/a&gt;, &lt;a href=&quot;https://eng.uber.com/tag/beidou/&quot; rel=&quot;tag&quot;&gt;BeiDou&lt;/a&gt;, &lt;a href=&quot;https://eng.uber.com/tag/galileo/&quot; rel=&quot;tag&quot;&gt;Galileo&lt;/a&gt;, &lt;a href=&quot;https://eng.uber.com/tag/global-positioning-system/&quot; rel=&quot;tag&quot;&gt;Global Positioning System&lt;/a&gt;, &lt;a href=&quot;https://eng.uber.com/tag/glonass/&quot; rel=&quot;tag&quot;&gt;GLONASS&lt;/a&gt;, &lt;a href=&quot;https://eng.uber.com/tag/gps/&quot; rel=&quot;tag&quot;&gt;GPS&lt;/a&gt;, &lt;a href=&quot;https://eng.uber.com/tag/inference/&quot; rel=&quot;tag&quot;&gt;Inference&lt;/a&gt;, &lt;a href=&quot;https://eng.uber.com/tag/maps/&quot; rel=&quot;tag&quot;&gt;maps&lt;/a&gt;, &lt;a href=&quot;https://eng.uber.com/tag/particle-filter/&quot; rel=&quot;tag&quot;&gt;Particle Filter&lt;/a&gt;, &lt;a href=&quot;https://eng.uber.com/tag/probabilistic-shadow-matching/&quot; rel=&quot;tag&quot;&gt;Probabilistic Shadow Matching&lt;/a&gt;, &lt;a href=&quot;https://eng.uber.com/tag/ray-tracing/&quot; rel=&quot;tag&quot;&gt;Ray Tracing&lt;/a&gt;, &lt;a href=&quot;https://eng.uber.com/tag/research/&quot; rel=&quot;tag&quot;&gt;Research&lt;/a&gt;, &lt;a href=&quot;https://eng.uber.com/tag/sensing/&quot; rel=&quot;tag&quot;&gt;Sensing&lt;/a&gt;, &lt;a href=&quot;https://eng.uber.com/tag/signal-processing/&quot; rel=&quot;tag&quot;&gt;Signal Processing&lt;/a&gt;, &lt;a href=&quot;https://eng.uber.com/tag/uber-engineering/&quot; rel=&quot;tag&quot;&gt;Uber Engineering&lt;/a&gt;, &lt;a href=&quot;https://eng.uber.com/tag/uber-maps/&quot; rel=&quot;tag&quot;&gt;Uber Maps&lt;/a&gt;, &lt;a href=&quot;https://eng.uber.com/tag/ucsb/&quot; rel=&quot;tag&quot;&gt;UCSB&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;

</description>
<pubDate>Fri, 20 Apr 2018 18:27:40 +0000</pubDate>
<dc:creator>mkvorwerck</dc:creator>
<og:type>article</og:type>
<og:title>Rethinking GPS: Engineering Next-Gen Location at Uber</og:title>
<og:description>Uber’s Sensing, Inference, and Research team released a software upgrade for GPS on Android phones that significantly improves location accuracy in urban environments.</og:description>
<og:url>https://eng.uber.com/rethinking-gps/</og:url>
<og:image>http://eng.uber.com/wp-content/uploads/2018/04/Facebook.jpg</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://eng.uber.com/rethinking-gps/</dc:identifier>
</item>
<item>
<title>Wells Fargo Hit with $1B in Fines</title>
<link>https://www.npr.org/sections/thetwo-way/2018/04/20/604279604/wells-fargo-hit-with-1-billion-in-fines-over-consumer-abuses</link>
<guid isPermaLink="true" >https://www.npr.org/sections/thetwo-way/2018/04/20/604279604/wells-fargo-hit-with-1-billion-in-fines-over-consumer-abuses</guid>
<description>&lt;div id=&quot;res604304922&quot; class=&quot;bucketwrap image large&quot;&gt;
&lt;div class=&quot;imagewrap&quot; data-crop-type=&quot;&quot;&gt;&lt;img src=&quot;https://media.npr.org/assets/img/2018/04/20/gettyimages-861003112_slide-bd3ac74af19e8183d9efdf1642d6f283f55431fb-s1100-c15.jpg&quot; data-original=&quot;https://media.npr.org/assets/img/2018/04/20/gettyimages-861003112_slide-bd3ac74af19e8183d9efdf1642d6f283f55431fb-s1100.jpg&quot; class=&quot;img lazyOnLoad&quot; alt=&quot;&quot;/&gt;
&lt;/div&gt;
&lt;div class=&quot;credit-caption&quot;&gt;
&lt;div class=&quot;caption-wrap&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;caption&quot; aria-label=&quot;Image caption&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;The Consumer Financial Protection Bureau is levying a $1 billion fine against Wells Fargo as punishment for the banking giant's actions in its mortgage and auto loan businesses. &lt;strong class=&quot;credit&quot; aria-label=&quot;Image credit&quot;&gt;Spencer Platt/Getty Images&lt;/strong&gt; &lt;strong class=&quot;hide-caption&quot;&gt;hide caption&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;strong class=&quot;toggle-caption&quot;&gt;toggle caption&lt;/strong&gt;&lt;/div&gt;
&lt;span class=&quot;credit&quot; aria-label=&quot;Image credit&quot;&gt;Spencer Platt/Getty Images&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;enlarge_measure&quot;&gt;
&lt;div class=&quot;img_wrap&quot;&gt;&lt;img data-original=&quot;https://media.npr.org/assets/img/2018/04/20/gettyimages-861003112_slide-bd3ac74af19e8183d9efdf1642d6f283f55431fb-s1200.jpg&quot; alt=&quot;&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;enlarge_html&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;image_data&quot; readability=&quot;8&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;The Consumer Financial Protection Bureau is levying a $1 billion fine against Wells Fargo as punishment for the banking giant's actions in its mortgage and auto loan businesses.&lt;/p&gt;
&lt;span class=&quot;credit&quot; aria-label=&quot;Image credit&quot;&gt;Spencer Platt/Getty Images&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The Consumer Financial Protection Bureau is levying a $1 billion fine against Wells Fargo — a record for the agency — as punishment for the banking giant's actions in its mortgage and auto loan businesses.&lt;/p&gt;
&lt;p&gt;Wells Fargo's &quot;conduct caused and was likely to cause substantial injury to consumers,&quot; the agency said in its filings about the bank.&lt;/p&gt;
&lt;p&gt;Wells Fargo broke the law by charging some consumers too much over mortgage interest rate-lock extensions and by running a mandatory insurance program that added insurance costs and fees into some borrowers' auto loans, &lt;a href=&quot;https://www.consumerfinance.gov/about-us/newsroom/bureau-consumer-financial-protection-announces-settlement-wells-fargo-auto-loan-administration-and-mortgage-practices/&quot;&gt;the CFPB said&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&quot;res604308916&quot; class=&quot;bucketwrap internallink insettwocolumn inset2col&quot;&gt;
&lt;div class=&quot;bucket img&quot;&gt;&lt;a id=&quot;featuredStackSquareImage583014020&quot; href=&quot;https://www.npr.org/sections/thetwo-way/2018/02/03/583014020/fed-slaps-unusual-penalty-on-wells-fargo-following-widespread-consumer-abuses&quot; data-metrics=&quot;{&amp;quot;category&amp;quot;:&amp;quot;Story to Story&amp;quot;,&amp;quot;action&amp;quot;:&amp;quot;Click Internal Link&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;https:\/\/www.npr.org\/sections\/thetwo-way\/2018\/02\/03\/583014020\/fed-slaps-unusual-penalty-on-wells-fargo-following-widespread-consumer-abuses&amp;quot;}&quot;&gt;&lt;img src=&quot;https://media.npr.org/assets/img/2018/02/03/ap_1205060105615_sq-b7476b9f811b18e99d7e86854601fcc344acb491-s100-c15.jpg&quot; data-original=&quot;https://media.npr.org/assets/img/2018/02/03/ap_1205060105615_sq-b7476b9f811b18e99d7e86854601fcc344acb491-s100.jpg&quot; class=&quot;img lazyOnLoad&quot; alt=&quot;Fed Slaps Unusual Penalty On Wells Fargo Following 'Widespread Consumer Abuses'&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The CFPB said Friday that the penalty is part of a settlement with Wells Fargo, which has pledged to repair the financial harm to consumers.&lt;/p&gt;
&lt;p&gt;Because of the penalties, Wells Fargo says, it is adjusting its preliminary financial results for the first quarter of 2018, shifting $800 million in its balance sheet — and dropping its net income for the quarter to $4.7 billion.&lt;/p&gt;
&lt;p&gt;The new federal action against the bank comes less than two years after &lt;a href=&quot;https://www.npr.org/sections/thetwo-way/2016/09/08/493130449/wells-fargo-to-pay-around-190-million-over-fake-accounts-that-sparked-bonuses&quot;&gt;Wells Fargo was fined $185 million&lt;/a&gt; over what the CFPB called &quot;the widespread illegal practice of secretly opening unauthorized deposit and credit card accounts.&quot;&lt;/p&gt;

&lt;p&gt;Those earlier penalties included a $100 million fine by the CFPB — a record at the time. The new punishment stems from the agency's findings that Wells Fargo abused its relationship with home and auto loan borrowers.&lt;/p&gt;
&lt;p&gt;Wells Fargo was also punished by the U.S. Office of the Comptroller of the Currency over its risk management practices, with the agency collecting a $500 million penalty as part of the fines announced Friday.&lt;/p&gt;
&lt;aside id=&quot;ad-backstage-wrap&quot; aria-label=&quot;advertisement&quot;&gt;
&lt;/aside&gt;&lt;div id=&quot;res604315593&quot; class=&quot;bucketwrap internallink insettwocolumn inset2col&quot;&gt;
&lt;div class=&quot;bucket img&quot;&gt;&lt;a id=&quot;featuredStackSquareImage604236275&quot; href=&quot;https://www.npr.org/sections/thetwo-way/2018/04/20/604236275/national-teachers-union-cuts-ties-with-wells-fargo-over-banks-ties-to-nra-guns&quot; data-metrics=&quot;{&amp;quot;category&amp;quot;:&amp;quot;Story to Story&amp;quot;,&amp;quot;action&amp;quot;:&amp;quot;Click Internal Link&amp;quot;,&amp;quot;label&amp;quot;:&amp;quot;https:\/\/www.npr.org\/sections\/thetwo-way\/2018\/04\/20\/604236275\/national-teachers-union-cuts-ties-with-wells-fargo-over-banks-ties-to-nra-guns&amp;quot;}&quot;&gt;&lt;img src=&quot;https://media.npr.org/assets/img/2018/04/20/ap_17305670835737_sq-bab6ab2418fe429653ba75315877775e5e6ba365-s100-c15.jpg&quot; data-original=&quot;https://media.npr.org/assets/img/2018/04/20/ap_17305670835737_sq-bab6ab2418fe429653ba75315877775e5e6ba365-s100.jpg&quot; class=&quot;img lazyOnLoad&quot; alt=&quot;National Teachers Union Cuts Ties With Wells Fargo Over Bank's Ties To NRA, Guns&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Along with treating its customers unfairly, the OCC said, Wells Fargo had failed to maintain a compliance risk management program that was appropriate for a bank of its size and complexity.&lt;/p&gt;
&lt;p&gt;That failure, the OCC said, led Wells Fargo to &quot;engage in reckless unsafe or unsound practices and violations of law.&quot;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Auto loans, insurance and fees&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Problems in the way the Wells Fargo auto loan unit handled consumers' accounts exposed people to hundreds or thousands of dollars in premiums and fees. The issues were also found to have possibly contributed to thousands of cars being repossessed.&lt;/p&gt;
&lt;p&gt;The CFPB said problems with the auto loan unit persisted for more than 10 years, from October 2005 to September 2016.&lt;/p&gt;
&lt;p&gt;Lenders can require borrowers to maintain insurance on their vehicles — and if a borrower doesn't do that, there is a process that allows lenders to arrange for what is called force-placed insurance and add that cost to the loan. But Wells Fargo acknowledged that of the roughly 2 million car loans that it put into that program, it &quot;forcibly placed duplicative or unnecessary insurance on hundreds of thousands of those borrowers' vehicles.&quot;&lt;/p&gt;
&lt;p&gt;For some borrowers, the bank also improperly maintained those force-placed policies on their accounts even after they secured adequate insurance.&lt;/p&gt;
&lt;p&gt;According to the CFPB, &quot;if borrowers failed to pay the amounts [Wells Fargo] charged them for the Force-Placed Insurance, they faced additional fees and, in some instances, experienced delinquency, loan default, and even repossession.&quot;&lt;/p&gt;
&lt;p&gt;In one five-year period from 2011 to 2016, Wells Fargo acknowledged in the settlement, the extra costs of force-placed insurance may have played a role in at least 27,000 customers having their vehicles repossessed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Home loan rate locks&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Wells Fargo failed to follow its own policies in how it charged fees over locking in mortgage interest rates beyond the standard guaranteed window, the CFPB said, adding that the bank charged customers for the rate extension — even in cases in which the bank itself was the reason for delays in closing on a home loan.&lt;/p&gt;
&lt;p&gt;The problems persisted for several years after the bank's internal audit identified the risk of harming consumers, according to the government's filing about the settlement.&lt;/p&gt;
&lt;p&gt;Wells Fargo unfairly and inconsistently applied its policy on rate-extension fees from September 2013 through February 2017, the agency said.&lt;/p&gt;
&lt;p&gt;In February, Wells Fargo also faced a government reprimand in February, when the Federal Reserve took the rare step of &quot;restricting Wells Fargo's growth and demanding the replacement of four board members in response to 'widespread consumer abuses and compliance breakdowns' at the bank,&quot; &lt;a href=&quot;https://www.npr.org/sections/thetwo-way/2018/02/03/583014020/fed-slaps-unusual-penalty-on-wells-fargo-following-widespread-consumer-abuses&quot;&gt;as NPR reported&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;At the time, the Fed faulted Wells Fargo for maintaining a business strategy that prioritizes its own growth at the expense of risk management.&lt;/p&gt;
</description>
<pubDate>Fri, 20 Apr 2018 16:43:47 +0000</pubDate>
<dc:creator>smaili</dc:creator>
<og:title>Wells Fargo Hit With $1 Billion In Fines Over Home And Auto Loan Abuses</og:title>
<og:url>https://www.npr.org/sections/thetwo-way/2018/04/20/604279604/wells-fargo-hit-with-1-billion-in-fines-over-consumer-abuses</og:url>
<og:type>article</og:type>
<og:description>Some consumers were charged too much to extend the lock on their mortgage interest rates, and the bank's mandatory insurance program added unneeded costs and fees to borrowers' auto loans.</og:description>
<og:image>https://media.npr.org/assets/img/2018/04/20/gettyimages-861003112_wide-1d7e110f7ec152de8452cf597d4f09bfbe2ca3c4.jpg?s=1400</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.npr.org/sections/thetwo-way/2018/04/20/604279604/wells-fargo-hit-with-1-billion-in-fines-over-consumer-abuses</dc:identifier>
</item>
<item>
<title>Facebook has auto-enrolled users into a facial recognition test in Europe</title>
<link>https://techcrunch.com/2018/04/20/just-say-no/</link>
<guid isPermaLink="true" >https://techcrunch.com/2018/04/20/just-say-no/</guid>
<description>&lt;p&gt;Facebook users in Europe are reporting that the company has started giving them the option to turn on its controversial facial recognition technology.&lt;/p&gt;
&lt;p&gt;Jimmy Nsubuga, a journalist at &lt;a href=&quot;http://metro.co.uk/2018/03/09/facebook-begins-test-banned-facial-recognition-system-uk-europe-7374305/&quot;&gt;Metro&lt;/a&gt;, is among several European Facebook users who have reporting getting notifications asking if they want to turn on face recognition technology.&lt;/p&gt;
&lt;p&gt;Facebook has previously said an opt-in option would be pushed out to all European users, and also globally, as part of changes to its T&amp;amp;Cs and consent flow.&lt;/p&gt;
&lt;p&gt;In Europe the company is hoping to convince users to voluntarily allow it to deploy the privacy-hostile tech — which was turned off in the bloc after regulatory pressure, back in &lt;a href=&quot;https://techcrunch.com/2012/09/21/facebook-turns-off-facial-recognition-in-the-eu-gets-the-all-clear-from-irelands-data-protection-commissioner-on-its-review/&quot;&gt;2012&lt;/a&gt;, when Facebook began using facial recognition to offer features such as automatically tagging users in photo uploads.&lt;/p&gt;
&lt;p&gt;But under &lt;a href=&quot;https://techcrunch.com/2018/04/17/facebook-gdpr-changes/&quot;&gt;impending changes to its T&amp;amp;Cs&lt;/a&gt; — ostensibly to comply with the EU’s incoming GDPR data protection standard — the company has crafted a manipulative consent flow that tries to sell people on giving it their data; including filling in its own facial recognition blanks by convincing Europeans to agree to it grabbing and using their biometric data after all.&lt;/p&gt;
&lt;p&gt;Users who choose not to switch on facial recognition still have to click through a ‘continue’ screen before they get to the off switch. On this screen Facebook attempts to convince them to turn it on — using manipulative examples of how the tech can “protect” them.&lt;/p&gt;
&lt;p&gt;As another Facebook user who has also already received the notifications — journalist, Jennifer Baker — points out, what it’s doing here is incredibly disingenuous — because it’s using fear to try to manipulate people’s choices.&lt;/p&gt;

&lt;p&gt;Under the EU’s incoming data protection framework Facebook cannot automatically opt users into facial recognition — it has to convince people to switch the tech on themselves. So it is emphasizing that users can choose whether or not to enable the technology.&lt;/p&gt;
&lt;p&gt;But data protection experts we spoke to &lt;a href=&quot;https://techcrunch.com/2018/04/18/data-experts-on-facebooks-gdpr-changes-expect-lawsuits/&quot;&gt;earlier this week&lt;/a&gt; do not believe Facebook’s approach to consent will be legal under GDPR.&lt;/p&gt;
&lt;p&gt;Essentially, this is big data-powered manipulation of human decision-making — until the ‘right’ answer (for Facebook’s business) is ‘selected’ by the user. In other words, not freely given, informed consent at all.&lt;/p&gt;
&lt;p&gt;Legal challenges are certain at this point.&lt;/p&gt;
&lt;p&gt;A Facebook spokeswoman confirmed to TechCrunch that any European users who are being asked about the tech now, ahead of the May 25 GDPR deadline, are part of its rollout of platform changes intended to comply with the incoming standard.&lt;/p&gt;
&lt;p&gt;“The flow is not a test, it is part of a rollout we are doing across the EU,” she said. “We are asking people for opt-in consent for three things — third party data for ads, facial recognition and the permission to process their sensitive data.”&lt;/p&gt;
&lt;p&gt;She also confirmed that Facebook did run a test of “a very similar version of this flow to a small percentage of users in the EU back in March”, adding: “The flow + wording was broadly the same. At all times it was opt-in.”&lt;/p&gt;
&lt;p&gt;The problem is, given Facebook controls the entire consent flow, and can rely on big data insights gleaned from its own platform (of 2BN+ users), this is not even remotely a fair fight. Manipulated acceptance is not consent.&lt;/p&gt;
&lt;p&gt;But legal challenges take time. And in the meanwhile Facebook users are being &lt;a href=&quot;https://techcrunch.com/2018/04/17/facebook-gdpr-changes/&quot;&gt;socially engineered&lt;/a&gt;, with selective examples and friction, into agreeing with things that align with the company’s data-harvesting business interests — handing over sensitive personal data without understanding the full implications of doing so.&lt;/p&gt;
&lt;p&gt;It’s not clear exactly how many Facebook users were part of the earlier flow test. It’s likely the company used the aforementioned variations in wording to determine — via an A/B testing process — which consent screens were most successful at convincing people to accept the &lt;a href=&quot;https://techcrunch.com/2018/02/08/chinese-police-are-getting-smart-glasses/&quot;&gt;highly privacy-hostile technology&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Last month — when Facebook &lt;a href=&quot;https://www.siliconrepublic.com/enterprise/gdpr-facebook-facial-recognition&quot;&gt;said&lt;/a&gt; it would be rolling out “a limited test of some of the additional choices we’ll ask people to make as part of GDPR” — it also said it would start “by asking only a small percentage of people so that we can be sure everything is working properly”.&lt;/p&gt;
&lt;p&gt;Interestingly it did not put a number on how many people were involved in that test. And Facebook’s spokeswoman did not provide an answer when we asked.&lt;/p&gt;
&lt;p&gt;The company was likely hoping the test would not attract too much attention — given how much GDPR news is flowing through its PR channels, and how much attention the topic is generally sucking up.&lt;/p&gt;
&lt;p&gt;But depending on how successful those tests prove to be at convincing Europeans to let it have and use their facial biometric data, millions of additional Facebook users could soon be providing the company with fresh streams of sensitive data — and having their fundamental rights trampled on, yet again, thanks to a very manipulative consent flow.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This article was updated with a series of corrections after Facebook confirmed the notifications are in fact the rollout of its new consent flow, not part of the earlier tests. It has also told us categorically that no users were auto-enrolled in facial recognition tech in Europe — even in the test. So we’ve updated this article accordingly. &lt;/em&gt;&lt;/p&gt;
</description>
<pubDate>Fri, 20 Apr 2018 14:35:01 +0000</pubDate>
<dc:creator>Ours90</dc:creator>
<og:title>Facebook starts its facial recognition push to Europeans</og:title>
<og:description>Facebook users in Europe are reporting that the company has started giving them the option to turn on its controversial facial recognition technology. Jimmy Nsubuga, a journalist at Metro, is among several European Facebook users who have reporting getting notifications asking if they want to turn …</og:description>
<og:image>https://techcrunch.com/wp-content/uploads/2015/10/shutterstock_113956645.jpg?w=584</og:image>
<og:url>http://social.techcrunch.com/2018/04/20/just-say-no/</og:url>
<og:type>article</og:type>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://techcrunch.com/2018/04/20/just-say-no/</dc:identifier>
</item>
<item>
<title>The ‘Terms and Conditions’ Reckoning Is Coming</title>
<link>https://www.bloomberg.com/news/articles/2018-04-20/uber-paypal-face-reckoning-over-opaque-terms-and-conditions</link>
<guid isPermaLink="true" >https://www.bloomberg.com/news/articles/2018-04-20/uber-paypal-face-reckoning-over-opaque-terms-and-conditions</guid>
<description>&lt;p&gt;&lt;span class=&quot;lede-media-image__caption caption&quot;&gt;Facebook CEO Mark Zuckerberg at a hearing before the House Energy and Commerce Committee in Washington on April 11, 2018.&lt;/span&gt;&lt;/p&gt;&lt;div readability=&quot;105.54983693437&quot;&gt;
&lt;p&gt;Eleanor Margolis had used PayPal for more than a decade when the online payment provider blocked her account in January. The reason: She was 16 years old when she signed up, and &lt;a href=&quot;https://www.bloomberg.com/quote/PYPL:US&quot; itemprop=&quot;StoryLink&quot; itemscope=&quot;itemscope&quot;&gt;PayPal Holdings Inc.&lt;/a&gt; insists she should have known the minimum age is 18, because the rule is clearly stated in terms and conditions she agreed to. Clearly stated, that is, in a document longer than &lt;em&gt;The Great Gatsby&lt;/em&gt;—almost 50,000 words spread across 21 separate web pages. “They didn’t have any checks in place to make sure I was over 18,” says Margolis, now 28. “Instead, they contact me 12 years later. It’s completely absurd.”&lt;/p&gt;


&lt;p&gt;Personal finance forums online are brimming with complaints from hundreds of PayPal customers who say they’ve been suspended because they signed up before age 18. PayPal declined to comment on any specific cases, but says it’s appropriate to close accounts created by underage people “to ensure our customers have full legal capacity to accept our user agreement.” While that may seem “heavy-handed,” says Sarah Kenshall, a technology attorney with law firm Burges Salmon, the company is within its rights because the users clicked to agree to the rules—however difficult the language might be to understand.&lt;/p&gt;


&lt;p&gt;Websites have long required users to plow through pages of dense legalese to use their services, knowing that few ever give the documents more than a cursory glance. In 2005 security-software provider PC Pitstop LLC promised a $1,000 prize to the first user to spot the offer deep in its terms and conditions; it took four months before the reward was claimed. The incomprehensibility of user agreements is poised to change as tech giants such as Uber Technologies Inc. and Facebook Inc. confront pushback for mishandling user information, and the European Union prepares to implement new privacy rules called the &lt;a href=&quot;https://www.bloomberg.com/news/articles/2018-03-20/how-europe-s-gdpr-will-mean-your-data-belongs-to-you-quicktake&quot; itemprop=&quot;StoryLink&quot; itemscope=&quot;itemscope&quot; target=&quot;_blank&quot;&gt;General Data Protection Regulation&lt;/a&gt;, or GDPR. The measure underscores “the requirement for clear and plain language when explaining consent,” British Information Commissioner Elizabeth Denham &lt;a href=&quot;https://iconewsblog.org.uk/2017/08/16/consent-is-not-the-silver-bullet-for-gdpr-compliance/&quot; itemprop=&quot;StoryLink&quot; itemscope=&quot;itemscope&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;wrote on her blog&lt;/a&gt; last year.&lt;/p&gt;


&lt;aside class=&quot;pullquote&quot; data-align=&quot;center&quot; readability=&quot;3&quot;&gt;&lt;p class=&quot;passage&quot;&gt;“I’m a lawyer, and I have no idea what that means”&lt;/p&gt;
&lt;/aside&gt;
&lt;p&gt;During two days of testimony before the U.S. Congress this month, Mark Zuckerberg, &lt;a href=&quot;https://www.bloomberg.com/quote/FB:US&quot; itemprop=&quot;StoryLink&quot; itemscope=&quot;itemscope&quot;&gt;Facebook&lt;/a&gt;’s chief executive officer, was repeatedly chastised for burying important information in text that’s rarely read. Waving a 2-inch-thick printed version of the social network’s user agreement, Senator Lindsey Graham quoted a line from the first page, then intoned: “I’m a lawyer, and I have no idea what that means.” The South Carolina Republican later asked Zuckerberg whether he thinks consumers understand what they’re signing up for. The Facebook CEO’s response: “I don’t think the average person likely reads that whole document.”&lt;/p&gt;


&lt;p&gt;GDPR, which comes into force in Europe in May and calls for fines as high as 4 percent of a company’s global revenue for violations, will make it tougher to get away with book-length user agreements, says Eduardo Ustaran, co-director of the cybersecurity practice at law firm Hogan Lovells. He suggests that companies streamline their rules and make sure they’re written in plain English. If a typical user wouldn’t understand the documents, the consent that companies rely on for their business activities would be legally invalid. “Your whole basis for using people’s personal data would disappear,” Ustaran says.&lt;/p&gt;
&lt;aside class=&quot;inline-newsletter&quot; data-state=&quot;ready&quot;/&gt;&lt;p&gt;Companies are &lt;a href=&quot;https://www.bloomberg.com/news/articles/2018-03-22/it-ll-cost-billions-for-companies-to-comply-with-europe-s-new-data-law&quot; itemprop=&quot;StoryLink&quot; itemscope=&quot;itemscope&quot; target=&quot;_blank&quot;&gt;scrambling&lt;/a&gt; to ensure their user agreements comply with the law, says Julian Saunders, founder of Port.im, a British software maker that helps businesses adapt to GDPR. But he says many website owners aren’t yet explicit enough in stating why they’re collecting a consumer’s information, which other companies might gain access to it, and how people can ensure their data are deleted if they request it. Saunders says he’s signed up 100 businesses for the service and urges them to bend over backward in helping users understand the details. “Areas that used to get hidden in the small print of terms and conditions should now be exposed,” he says.&lt;/p&gt;
&lt;p&gt;Martin Garner, an analyst at technology consultancy CCS Insight, suggests companies walk readers through their policies step by step. That way they could opt out of selected provisions—limiting, for instance, third parties that can gain access to the data or restricting the kinds of information companies may stockpile. Much of what’s in the terms and conditions might be affected by the settings a user chooses, and including that information in the initial agreement unnecessarily complicates the document. “Users typically only have the choice of accepting the terms and conditions in their entirety or not using the service at all,” Garner says. Companies must “pay much closer attention to explaining to users how their data will be stored and used and getting them to consent to that explicitly.”&lt;/p&gt;
&lt;/div&gt;&lt;p&gt;&lt;span class=&quot;bottom-line__label&quot;&gt;BOTTOM LINE -&lt;/span&gt; &lt;span class=&quot;bottom-line__text&quot;&gt;To comply with new EU data regulations, website owners are scaling back and simplifying complex user agreements that can be longer than many novels.&lt;/span&gt;&lt;/p&gt;</description>
<pubDate>Fri, 20 Apr 2018 14:26:39 +0000</pubDate>
<dc:creator>adventured</dc:creator>
<og:description>Everyone from Uber to PayPal is facing a backlash against their impenetrable legalese.</og:description>
<og:image>https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iCE3hTL.yfH0/v0/1200x630.jpg</og:image>
<og:title>The ‘Terms and Conditions’ Reckoning Is Coming</og:title>
<og:type>article</og:type>
<og:url>https://www.bloomberg.com/news/articles/2018-04-20/uber-paypal-face-reckoning-over-opaque-terms-and-conditions</og:url>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bloomberg.com/news/articles/2018-04-20/uber-paypal-face-reckoning-over-opaque-terms-and-conditions</dc:identifier>
</item>
<item>
<title>Show HN: A curated library of free music for content creators</title>
<link>http://cchound.com/</link>
<guid isPermaLink="true" >http://cchound.com/</guid>
<description>[unable to retrieve full-text content]&lt;p&gt;Article URL: &lt;a href=&quot;http://cchound.com/&quot;&gt;http://cchound.com/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=16884979&quot;&gt;https://news.ycombinator.com/item?id=16884979&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Points: 221&lt;/p&gt;&lt;p&gt;# Comments: 30&lt;/p&gt;</description>
<pubDate>Fri, 20 Apr 2018 14:24:03 +0000</pubDate>
<dc:creator>aabergkvist</dc:creator>
<og:title>cchound</og:title>
<og:url>http://cchound.com/?og=1</og:url>
<og:description>A curation of CC licensed music from various artists and genres for you to use, however you like, in...</og:description>
<og:type>tumblr-feed:tumblelog</og:type>
<og:image>http://assets.tumblr.com/images/default_avatar/sphere_open_128.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>http://cchound.com/</dc:identifier>
</item>
<item>
<title>Xz format inadequate for long-term archiving (2017)</title>
<link>https://www.nongnu.org/lzip/xz_inadequate.html</link>
<guid isPermaLink="true" >https://www.nongnu.org/lzip/xz_inadequate.html</guid>
<description>&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt;&lt;title&gt;Xz format inadequate for long-term archiving&lt;/title&gt;&lt;meta name=&quot;keywords&quot; content=&quot;xz, lzip, LZMA, bzip2, gzip, data compression, long-term archiving&quot; /&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;612.4181184669&quot;&gt;

&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;One of the challenges of digital preservation is the evaluation of data formats. It is important to choose well-designed data formats for long-term archiving. This article describes the reasons why the xz compressed data format is inadequate for long-term archiving and &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#fragmented&quot;&gt;inadvisable&lt;/a&gt; for data sharing and for free software distribution. The relevant weaknesses and design errors in the xz format are analyzed and, where applicable, compared with the corresponding behavior of the bzip2, gzip and lzip formats. Key findings include: (1) safe interoperability among xz implementations is not guaranteed; (2) xz's extensibility is unreasonable and problematic; (3) xz is vulnerable to unprotected flags and length fields; (4) LZMA2 is &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#glossary&quot;&gt;unsafe&lt;/a&gt; and less efficient than the original LZMA; (5) xz includes useless features that increase the number of false positives for corruption; (6) xz shows inconsistent behavior with respect to trailing data; (7) error detection in xz is several times less accurate than in bzip2, gzip and lzip.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Disclosure statement&lt;/strong&gt;: The author is also author of the lzip format.&lt;/p&gt;
&lt;h2&gt;1 Introduction&lt;/h2&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;There are two ways of constructing a software design: One way is to make it so simple that there are obviously no deficiencies and the other way is to make it so complicated that there are no obvious deficiencies. The first method is far more difficult.&lt;br /&gt;-- C.A.R. Hoare&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;Perfection is reached, not when there is no longer anything to add, but when there is no longer anything to take away.&lt;br /&gt;-- Antoine de Saint-Exupery&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Both the xz compressed data format and its predecessor lzma-alone have serious design flaws. But while lzma-alone is a toy format lacking fundamental features, xz is a complex container format full of contradictions. For example, xz tries to appear as a very safe format by offering &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#glossary&quot;&gt;overkill&lt;/a&gt; check sequences like SHA-256 but, at the same time it &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#unprot_len&quot;&gt;fails to protect&lt;/a&gt; the length fields needed to decompress the data in the first place. These defects make xz inadequate for long-term archiving and reduce its value as a general-purpose compressed data format.&lt;/p&gt;
&lt;p&gt;This article analyzes the &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#refXz_format&quot;&gt;xz compressed data format&lt;/a&gt;, what is to mean the way bits are arranged in xz compressed files and the consequences of such arrangement. This article is about formats, not programs. In particular this article is not about bugs in any compression tool. The fact that the xz reference tool (xz-utils) has had more bugs than bzip2 and lzip combined is mainly a consequence of the complexity and bad design of the xz format. Also the uninformative error messages provided by the xz tool reflect the extreme difficulty of finding out what failed in case of corruption in a xz file.&lt;/p&gt;
&lt;p&gt;This article started with a series of posts to the debian-devel mailing list &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#refDebian&quot;&gt;[Debian]&lt;/a&gt;, where it became clear that nobody had analyzed xz in any depth before adopting it in the Debian package format. The same unthinking adoption of xz seems to have happened in major free software projects, like GNU Coreutils and Linux. In my opinion, it is a mistake for any widely used project to become an early adopter of a new data format; it may cause a lot of trouble if any serious defect is later discovered in the format.&lt;/p&gt;
&lt;h2&gt;2 The reasons why the xz format is inadequate for long-term archiving&lt;/h2&gt;
&lt;h3&gt;2.1 Xz is a container format&lt;/h3&gt;
&lt;p&gt;On Unix-like systems, where a tool is supposed to do one thing and do it well, compressed file formats are usually formed by the compressed data, preceded by a header containing the parameters needed for decompression, and followed by a trailer containing integrity information. Bzip2, gzip and lzip formats are designed this way, minimizing both overhead and false positives.&lt;/p&gt;
&lt;p&gt;On the contrary, xz is a container format which currently contains another container format (LZMA2), which in turn contains an arbitrary mix of LZMA data and uncompressed data. In spite of implementing just one compression algorithm, xz already manages 3 levels of headers, which increases its &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#glossary&quot;&gt;fragility&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The xz format has more overhead than bzip2, gzip or lzip, most of it either not properly designed (e.g., unprotected headers) or plain useless (padding). In fact, a xz stream can contain such a large amount of overhead that the format designers deemed necessary to compress the overhead using &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#vli&quot;&gt;unsafe methods&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There is no reason to use a container format for a general-purpose compressor. The right way of implementing a new compression algorithm is to provide a &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#extensibility&quot;&gt;version number&lt;/a&gt; in the header, and the right way of implementing binary filters is to write a preprocessor that applies the filter to the data before feeding them to the compressor. (See for example &lt;a href=&quot;http://www.cs.cmu.edu/~ckingsf/software/mince/&quot;&gt;mince&lt;/a&gt;).&lt;/p&gt;
&lt;h3&gt;&lt;a name=&quot;fragmented&quot; href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#fragmented&quot; id=&quot;fragmented&quot;&gt;2.2&lt;/a&gt; Xz is fragmented by design&lt;/h3&gt;
&lt;p&gt;Xz was designed as a fragmented format. Xz implementations may choose what subset of the format they support. In particular, integrity checking in xz offers multiple choices of check types, all of them optional except CRC32 which is recommended. (See &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#refXz_format&quot;&gt;[Xz format]&lt;/a&gt;, section 2.1.1.2 'Stream Flags'. See also &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#refRFC2119&quot;&gt;[RFC 2119]&lt;/a&gt; for the definitions of 'optional' and 'recommended'). Safe interoperability among xz implementations is not guaranteed. For example the xz-embedded decompressor does not support the optional check types. Other xz implementations may choose to not support integrity checking at all.&lt;/p&gt;
&lt;p&gt;The xz reference tool (xz-utils) ignores the recommendation of the xz format specification and uses by default an optional check type (CRC64) in the files it produces. This prevents decompressors that do not support the optional check types from verifying the integrity of the data. Using --check=crc32 when creating the file makes integrity checking work on the xz-embedded decompressor, but as CRC32 is just recommended, it does not guarantee that integrity checking will work on all xz compliant decompressors. Distributing software in xz format can only be guaranteed to be safe if the distributor controls the decompressor run by the user (or can force the use of external means of integrity checking). Error detection in the xz format is broken; depending on how the file was created and on what decompressor is available, the integrity check in xz is sometimes performed and sometimes not. The latter is usually the case for the tarballs released in xz format by GNU and Linux when they are decompressed with the xz-embedded decompressor (see the third xz test in &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#refBenchmark&quot;&gt;[benchmark]&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Fragmentation (subformat proliferation) hinders interoperability and complicates the management of large archives. The lack of guaranteed integrity checking increases the probability of undetected corruption. Bzip2, gzip and lzip are free from these defects; any decompressor can decompress and verify the integrity of any file in the corresponding format.&lt;/p&gt;
&lt;h3&gt;2.3 Xz is unreasonably extensible&lt;/h3&gt;
&lt;p&gt;The design of the xz format is based on the false idea that better compression algorithms can be mass-produced like cars in a factory. It has room for 2^63 filters, which can then be combined to make an even larger number of algorithms. Xz reserves less than 0.8% of filter IDs for custom filters, but even this small range provides about 8 million custom filter IDs for each human inhabitant on earth. There is not the slightest justification for such egregious level of extensibility. Every useless choice allowed by a format takes space and makes corruption both more probable and more difficult to recover from.&lt;/p&gt;
&lt;p&gt;The basic ideas of compression algorithms were discovered early in the history of computer science. LZMA is based on ideas discovered in the 1970s. Don't expect an algorithm much better than LZMA to appear anytime soon, much less several of them in a row.&lt;/p&gt;
&lt;p&gt;In 2008 one of the designers of xz (Lasse Collin) warned me that lzip would become stuck with LZMA while others moved to LZMA2, LZMA3, LZMH, and other algorithms. Now xz-utils is usually unable to match the compression ratio of lzip because LZMA2 has more overhead than LZMA and, as expected, no new algorithms have been added to xz-utils.&lt;/p&gt;
&lt;h3&gt;&lt;a name=&quot;extensibility&quot; href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#extensibility&quot; id=&quot;extensibility&quot;&gt;2.4&lt;/a&gt; Xz's extensibility is problematic&lt;/h3&gt;
&lt;p&gt;The xz format lacks a version number field. The only reliable way of knowing if a given version of a xz decompressor can decompress a given file is by trial and error. The 'file' utility does not provide any help:&lt;/p&gt;
&lt;pre&gt;
$ file COPYING.*
COPYING.lz: lzip compressed data, version: 1
COPYING.xz: XZ compressed data
&lt;/pre&gt;
&lt;p&gt;Xz-utils can report the minimum version of xz-utils required to decompress a given file, but it must decode each block header in the file to find it out, and only can report older versions of xz-utils. If a newer version of xz-utils is required, it can't be known which one. The report is also useless to know what version of other decompressors (for example 7-zip) could decompress the file. Note that the version reported may be unable to decompress the file if xz-utils was built &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#fragmented&quot;&gt;without support&lt;/a&gt; for some feature present in the file.&lt;/p&gt;
&lt;p&gt;The extensibility of bzip2 and lzip is better. Both formats provide a version field. Therefore it is trivial for them to seamlessly and reliably incorporate a new compression algorithm while making clear what version of the tool is required to decompress a given file; tool_version &amp;gt;= file_version. If an algorithm much better than LZMA is found, a version 2 lzip format (perfectly fit to the new algorithm) can be designed, along with a version 2 lzip tool able to decompress the old and new formats transparently. Bzip2 is already a &quot;version 2&quot; format. The reason why bzip2 does not decompress bzip files is that the original bzip format was abandoned because of problems with software patents.&lt;/p&gt;
&lt;p&gt;The extensibility of gzip is obsolete mainly because of the 32-bit uncompressed size (ISIZE) field.&lt;/p&gt;
&lt;h3&gt;&lt;a name=&quot;unprot_len&quot; href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#unprot_len&quot; id=&quot;unprot_len&quot;&gt;2.5&lt;/a&gt; Xz fails to protect the length of variable size fields&lt;/h3&gt;
&lt;p&gt;According to &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#refKoopman&quot;&gt;[Koopman]&lt;/a&gt; (p. 50), one of the &quot;Seven Deadly Sins&quot; (i.e., bad ideas) of CRC and checksum use is failing to protect a message length field. This causes vulnerabilities due to framing errors. Note that the effects of a framing error in a data stream are more serious than what Figure 1 suggests. Not only data at a random position are interpreted as the CRC. Whatever data that follow the bogus CRC will be interpreted as the beginning of the following field, preventing the successful decoding of any remaining data in the stream.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://www.nongnu.org/lzip/corrupted_len.png&quot; alt=&quot;Corruption of message length field&quot; /&gt;&lt;br /&gt;Figure 1. Corruption of message length field. Source: &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#refKoopman&quot;&gt;[Koopman]&lt;/a&gt;, p. 30.&lt;/p&gt;
&lt;p&gt;Except the 'Backward Size' field in the stream footer, none of the many length fields in the xz format is protected by a check sequence of any kind. Not even a parity bit. All of them suffer from the framing vulnerability illustrated in the picture above. In particular every LZMA2 header contains one 16-bit unprotected length field. Some length fields in the xz format are of variable size themselves, adding a new failure mode to xz not found in the other three formats; double framing error.&lt;/p&gt;
&lt;p&gt;Bzip2 is affected by this defect to a lesser extent; it contains two unprotected length fields in each block header. Gzip may be considered free from this defect because its only top-level unprotected length field (XLEN) can be validated using the LEN fields in the extra subfields. Lzip is free from this defect.&lt;/p&gt;
&lt;p&gt;Optional fields are just as unsafe as unprotected length fields if the flag that indicates the presence of the optional field is itself unprotected. The result is the same; framing errors. Again, except the 'Stream Flags' field, none of those flags in the xz format is protected by a check sequence. In particular the critically important 'Block Flags' field in block headers and bit 6 in the control byte of the numerous LZMA2 headers are not protected.&lt;/p&gt;
&lt;p&gt;Bzip2 contains 16 unprotected flags for optional huffman bitmaps in each block header. Gzip just contains one byte with four unprotected flags for optional fields in its header. Lzip is free from optional fields.&lt;/p&gt;
&lt;h3&gt;&lt;a name=&quot;vli&quot; href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#vli&quot; id=&quot;vli&quot;&gt;2.6&lt;/a&gt; Xz uses variable-length integers unsafely&lt;/h3&gt;
&lt;p&gt;Xz stores many (potentially large) numbers using a variable-length representation terminated by a byte with the most significant bit (msb) cleared. In case of corruption, not only the value of the field may become incorrect, the size of the field may also change, causing a framing error in the following fields. Xz uses such variable-length integers to store the size of other fields. In case of corruption in the size field, both the position and the size of the target field may become incorrect, causing a double framing error. See for example &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#refXz_format&quot;&gt;[Xz format]&lt;/a&gt;, section 3.1.5 'Size of Properties' in 'List of Filter Flags'. Bzip2, gzip and lzip store all fields representing numbers in a safe fixed-length representation.&lt;/p&gt;
&lt;p&gt;Xz features a monolithic index that is specially vulnerable to cascading framing errors. Some design errors of the xz index are:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;The number of records is coded as an unprotected variable-length integer vulnerable to double framing error.&lt;/li&gt;
&lt;li&gt;The size of the index is not stored anywhere. It must be calculated by decoding the whole index and can't be verified. ('Backward Size' stores the size of the index rounded up to the next multiple of four bytes, not the real size).&lt;/li&gt;
&lt;li&gt;When reading from unseekable sources, it delays the verification of the block sizes until the end of the stream and requires a potentially huge amount of RAM (up to 16 GiB), unless such verification is made by hashing, in which case it can't be known what blocks failed the test. The safe and efficient way is to verify the sizes of each block as soon as it is processed, as gzip and lzip do.&lt;/li&gt;
&lt;li&gt;The list of records is made of variable-length integers concatenated together. Regarding corruption it acts as one potentially very long unprotected variable-length integer. Just one bit flip in the msb of any byte causes the remaining records to be read incorrectly. It also causes the size of the index to be calculated incorrectly, losing the position of the CRC32 and the stream footer.&lt;/li&gt;
&lt;li&gt;Each record stores the size (not the position) of the corresponding block, but xz's block headers do not provide an identification string that could validate the block size. Therefore, just one bit flip in any 'Unpadded Size' field causes the positions of the remaining blocks to be calculated incorrectly. By contrast, lzip provides a distributed index where each member size is validated by the presence of the ID string in the corresponding member header. Neither the bzip2 format nor the gzip format do provide an index.&lt;/li&gt;
&lt;/ol&gt;&lt;h3&gt;2.7 LZMA2 is unsafe and less efficient than the original LZMA&lt;/h3&gt;
&lt;p&gt;The xz-utils manual says that LZMA2 is an updated version of LZMA to fix some practical issues of LZMA. This wording suggests that LZMA2 is some sort of improved LZMA algorithm. (After all, the 'A' in LZMA stands for 'algorithm'). But LZMA2 is a container format that divides LZMA data into chunks in an unsafe way. In practice, for compressible data, LZMA2 is just LZMA with 0.015%-3% more overhead. The maximum compression ratio of LZMA is about 7051:1, but LZMA2 is limited to 6843:1 approximately.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#refLZMA2_format&quot;&gt;[LZMA2 format]&lt;/a&gt; contains an arbitrary mix of LZMA packets and uncompressed data packets. Each packet starts with a header that is not protected by any check sequence in spite of containing the type and size of the following data. Therefore, every bit flip in a LZMA2 header causes either a &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#unprot_len&quot;&gt;framing error&lt;/a&gt; or a desynchronization of the decoder. In any case it is usually not possible to decode the remaining data in the block or even to know what failed. Compare this with &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#refDeflate&quot;&gt;[Deflate]&lt;/a&gt; which at least does protect the length field of its non-compressed blocks. (Deflate's compressed blocks do not have a length field).&lt;/p&gt;
&lt;p&gt;Note that of the 3 levels of headers in a xz file (stream, block, LZMA2), the most numerous LZMA2 headers are the ones not protected by a check sequence. There is usually one stream header and one block header in a xz file, but there is at least one LZMA2 header for every 64 KiB of LZMA2 data in the file. In extreme cases the LZMA2 headers can make up to a 3% of the size of the file:&lt;/p&gt;
&lt;pre&gt;
-rw-r--r-- 1 14208 Oct 21 17:26 100MBzeros.lz
-rw-r--r-- 1 14195 Oct 21 17:26 100MBzeros.lzma
-rw-r--r-- 1 14676 Oct 21 17:26 100MBzeros.xz
&lt;/pre&gt;
&lt;p&gt;The files above were produced by lzip (.lz) and xz-utils (.lzma, .xz). The LZMA stream is identical in the .lz and .lzma files above; they just differ in the header and trailer. The .xz file is larger than the other two mainly because of the 50 LZMA2 headers it contains. LZMA2 headers make xz both more fragile and less efficient (see the xz tests in &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#refBenchmark&quot;&gt;[benchmark]&lt;/a&gt;). Additionally, corruption in the uncompressed packets of a LZMA2 stream can't be detected by the decoder, leaving the check sequence as the only way of detecting errors there.&lt;/p&gt;
&lt;p&gt;On the other hand, the original LZMA data stream provides &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#lzma_adc&quot;&gt;embedded error detection&lt;/a&gt;. Any distance larger than the dictionary size acts as a forbidden symbol, allowing the decoder to detect the approximate position of errors, and leaving very little work for the check sequence in the detection of errors.&lt;/p&gt;
&lt;p&gt;LZMA2 could have been safer and more efficient if only its designers had copied the structure of Deflate; terminate compressed blocks with a marker, and protect the length of uncompressed blocks. This would have reduced the overhead, and therefore the number of false positives, in the files above by a factor of 25. For compressible files, that only need a header and a marker, the improvement is usually of 8 times less overhead per mebibyte of compressed size (about 500 times less overhead for a file of 64 MiB).&lt;/p&gt;
&lt;h3&gt;&lt;a name=&quot;alignment&quot; href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#alignment&quot; id=&quot;alignment&quot;&gt;2.8&lt;/a&gt; The 4 byte alignment is unjustified&lt;/h3&gt;
&lt;p&gt;Xz is the only format of the four considered here whose parts are (arbitrarily) aligned to a multiple of four bytes. The size of a xz file must also be a multiple of four bytes for no reason. To achieve this, xz includes padding everywhere; after headers, blocks, the index, and the whole stream. The bad news is that if the (useless) padding is altered in any way, &quot;the decoder &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#misguided&quot;&gt;MUST indicate an error&lt;/a&gt;&quot; according to the xz format specification.&lt;/p&gt;
&lt;p&gt;Neither gzip nor lzip include any padding. Bzip2 includes a minimal amount of padding (at most 7 bits) at the end of the whole stream, but it ignores any corruption in the padding.&lt;/p&gt;
&lt;p&gt;Xz justifies alignment as being perhaps able to increase speed and compression ratio (see &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#refXz_format&quot;&gt;[Xz format]&lt;/a&gt;, section 5.1 'Alignment'), but such increases can't happen because:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;The only last filter in xz is LZMA2, whose output does not need any alignment.&lt;/li&gt;
&lt;li&gt;The output of the non-last filters in the chain is not stored in the file. Therefore it can't be &quot;later compressed with an external compression tool&quot; as stated in the xz format specification.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;One additional problem of the xz alignment is that four bytes are not enough; the IA64 filter has an alignment of 16 bytes. Alignment is a property of each filter that can only be managed by the archiver, not a property of the whole compressed stream. Even the xz format specification recognizes that alignment of input data is the job of the archiver, not of the compressor.&lt;/p&gt;
&lt;p&gt;The conclusion is that the 4 byte alignment is a misfeature that wastes space, increases the number of false positives for corruption, and &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#burst&quot;&gt;worsens the burst error detection in the stream footer&lt;/a&gt; without producing any benefit at all.&lt;/p&gt;
&lt;h3&gt;2.9 Trailing data&lt;/h3&gt;
&lt;p&gt;If you want to create a compressed file and then &lt;a href=&quot;http://www.nongnu.org/lzip/manual/lzip_manual.html#Trailing-data&quot;&gt;append some data&lt;/a&gt; to it, for example a cryptographically secure hash, xz won't allow you to do so. The xz format specification forbids the appending of data to a file, except what it defines as 'stream padding'. In addition to telling you what you can't do with your files, defining stream padding makes xz show inconsistent behavior with respect to trailing data. Xz accepts the addition of any multiple of 4 null bytes to a file. But if the number of null bytes appended is not a multiple of 4, or if any of the bytes is non-null, then the decoder &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#misguided&quot;&gt;must indicate an error&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A format that reports as corrupt the only surviving copy of an important file just because cp had a glitch and appended some garbage at the end of the file is not well suited for long-term archiving. The worst thing is that the xz format specification does not offer any compliant way of ignoring such trailing data. Once a xz file gets any trailing data appended, it must be manually removed to make the file compliant again.&lt;/p&gt;
&lt;p&gt;In a vain attempt to avoid such inconsistent behavior, xz-utils provides the option '--single-stream', which is just plain wrong for multi-stream files because it makes the decompressor ignore everything beyond the first stream, discarding any remaining valid streams and silently truncating the decompressed data:&lt;/p&gt;
&lt;pre&gt;
cat file1.xz file2.xz file3.sig &amp;gt; file.xz
xz -d file.xz                         # indicates an error
xz -d --single stream file.xz         # causes silent data loss
xz -kd --single-stream file.xz        # causes silent truncation
&lt;/pre&gt;
&lt;p&gt;The '--single-stream' option violates the xz format specification which requires the decoder to indicate an error if the stream padding does not meet its requirements. The xz format should provide a compliant way to ignore any trailing data after the last stream, just like bzip2, gzip and lzip do by default.&lt;/p&gt;
&lt;h3&gt;2.10 Xz's error detection has low accuracy&lt;/h3&gt;
&lt;p&gt;&quot;There can be safety tradeoffs with the addition of an error-detection scheme. As with almost all fault tolerance mechanisms, there is a tradeoff between availability and integrity. That is, techniques that increase integrity tend to reduce availability and vice versa. Employing error detection by adding a check sequence to a &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#glossary&quot;&gt;dataword&lt;/a&gt; increases integrity, but decreases availability. The decrease in availability happens through false-positive detections. These failures preclude the use of some data that otherwise would not have been rejected had it not been for the addition of error-detection coding&quot;. (&lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#refKoopman&quot;&gt;[Koopman]&lt;/a&gt;, p. 33).&lt;/p&gt;
&lt;p&gt;But the tradeoff between availability and integrity is different for data transmission than for data archiving. When transmitting data, usually the most important consideration is to avoid undetected errors (false negatives for corruption), because a retransmission can be requested if an error is detected. Archiving, on the other hand, usually implies that if a file is reported as corrupt, &quot;retransmission&quot; is not possible. Obtaining another copy of the file may be difficult or impossible. Therefore &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#glossary&quot;&gt;accuracy&lt;/a&gt; (freedom from mistakes) in the detection of errors becomes the most important consideration.&lt;/p&gt;
&lt;p&gt;Two error models have been used to measure the accuracy in the detection of errors. The first model consists of one or more random bit flips affecting just one byte in the compressed file. The second model consists of zeroed 512-byte blocks aligned to a 512-byte boundary, simulating a whole sector I/O error. Just one zeroed block per trial. The first model is considered the most important because bit flips happen even in the most expensive hardware &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#refMSL&quot;&gt;[MSL]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Verification of data integrity in compressed files is different from other cases (like Ethernet packets) because the data that can become corrupted are the compressed data, but the data that are verified (the dataword) are the decompressed data. Decompression can cause error multiplication; even a single-bit error in the compressed data may produce any random number of errors in the decompressed data, or even modify the size of the decompressed data.&lt;/p&gt;
&lt;p&gt;Because of the error multiplication caused by decompression, the error model seen by the check sequence is one of unconstrained random data corruption. (Remember that the check sequence verifies the integrity of the decompressed data). This means that the choice of error-detection code (CRC or hash) is largely irrelevant, and that the probability of an error being undetected by the check sequence (Pudc) is 1 / (2^n) for a check sequence of n bits. (See &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#refKoopman&quot;&gt;[Koopman]&lt;/a&gt;, p. 5). Note that if some errors do not produce error multiplication, a CRC is then preferable to a hash of the same size because of the burst error detection capabilities of the CRC.&lt;/p&gt;
&lt;p&gt;Decompression algorithms are usually able to detect some errors in the compressed data (for example a backreference to a point before the beginning of the data). Therefore, the total probability of an undetected error (Pud = false negative) is the product of the probability of the error being undetected by the decoder (Pudd) and the probability of the error being undetected by the check sequence (Pudc): &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#glossary&quot;&gt;Pud&lt;/a&gt; = Pudd * Pudc.&lt;/p&gt;
&lt;p&gt;It is also possible that a small error in the compressed data does not alter at all the decompressed data. Therefore, for maximum availability, only the decompressed data should be tested for errors. Testing the compressed data beyond what is needed to perform the decompression increases the number of false positives much more than it can reduce the number of undetected errors.&lt;/p&gt;
&lt;p&gt;Of course, error multiplication was not applied in the analysis of fields that are not compressed, for example 'Block Header'. Burst error detection was also considered for the 'Stream Flags' and 'Stream Footer' fields.&lt;/p&gt;
&lt;p&gt;Trial decompressions were performed using the 'unzcrash' tool included in the &lt;a href=&quot;http://www.nongnu.org/lzip/lziprecover.html&quot;&gt;lziprecover&lt;/a&gt; package.&lt;/p&gt;
&lt;p&gt;The following sections describe the places in the xz format where error detection suffers from low accuracy and explain the cause of the inaccuracy in each case.&lt;/p&gt;
&lt;h4&gt;2.10.1 The 'Stream Flags' field&lt;/h4&gt;
&lt;p&gt;A well-known property of CRCs is their ability to detect burst errors up to the size of the CRC itself. Using a CRC larger than the dataword is an error because a CRC just as large as the dataword equally detects all errors while it produces a lower number of false positives.&lt;/p&gt;
&lt;p&gt;In spite of the mathematical property described above, the 16-bit 'Stream Flags' field in the xz stream header is protected by a CRC32 twice as large as the field itself, providing an unreliable error detection where 2 of every 3 reported errors is a false positive. The inaccuracy reaches 67%. CRC16 is a better choice from any point of view. It can still detect all errors in 'Stream Flags', but produces half the false positives as CRC32.&lt;/p&gt;
&lt;p&gt;Note that a copy of the 'Stream Flags', also protected by a CRC32, is stored in the stream footer. With such amount of redundancy xz should be able to repair a fully corrupted 'Stream Flags'. Instead of this the format specifies that if one of the copies, or one of the CRCs, or the backward size in the stream footer gets any damage, the decoder must indicate an error. The result is that getting a false positive for corruption related to the 'Stream Flags' is 7 times more probable than getting real corruption in the 'Stream Flags' themselves.&lt;/p&gt;
&lt;h4&gt;2.10.2 The 'Stream Footer' field&lt;/h4&gt;
&lt;p&gt;The 'Stream Footer' field contains the rounded up size of the index field and a copy of the 'Stream Flags' field from the stream header, both protected by a CRC32. The inaccuracy of the error detection for this field reaches a 40%; 2 of every 5 reported errors is a false positive.&lt;/p&gt;
&lt;p&gt; The CRC32 in 'Stream Footer' provides a reduced burst error detection because it is stored at front instead of back of codeword. (See &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#refKoopman&quot;&gt;[Koopman]&lt;/a&gt;, p. C-20). Testing has found several undetected burst errors of 31 bits in this field, while a CRC32 correctly placed would have detected all burst errors up to 32 bits. The reason adduced by the xz format specification for this misplacement is to keep the four-byte fields aligned to a multiple of four bytes, but &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#alignment&quot;&gt;the 4 byte alignment is unjustified&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;2.10.3 The 'Block Header' field&lt;/h4&gt;
&lt;p&gt;The 'Block Header' is of variable size. Therefore the inaccuracy of the error detection varies between 0.4% and 58%, being usually of a 58% (7 of every 12 reported errors are false positives). As shown in the graph below, CRC16 would have been a more accurate choice for any size of 'Block Header'. But inaccuracy is a minor problem compared with the &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#unprot_len&quot;&gt;lack of protection&lt;/a&gt; of the 'Block Header Size' and 'Block Flags' fields.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;640&quot; height=&quot;480&quot; src=&quot;https://www.nongnu.org/lzip/header_crc_inaccuracy.png&quot; alt=&quot;Block header CRC inaccuracy&quot; /&gt;&lt;br /&gt;Figure 2. Inaccuracy of block header CRC for all possible header sizes.&lt;/p&gt;
&lt;h4&gt;2.10.4 The 'Block Check' field&lt;/h4&gt;
&lt;p&gt;Xz supports several types of check sequences (CS) for the decompressed data; none, CRC32, CRC64 and SHA-256. Each check sequence provides better accuracy than the next larger one up to a certain compressed size. For the single-byte error model, the inaccuracy for each compressed size and CS size is calculated by the following formula (all sizes in bytes):&lt;/p&gt;
&lt;p&gt;Inaccuracy = ( compressed_size * Pudc + CS_size ) / ( compressed_size + CS_size )&lt;/p&gt;
&lt;p&gt;Applying the formula above it results that CRC32 provides more accurate error detection than CRC64 up to a compressed size of about 16 GiB, and more accurate than SHA-256 up to 112 GiB. It should be noted that SHA-256 provides worse accuracy than CRC64 for all possible block sizes.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;640&quot; height=&quot;480&quot; src=&quot;https://www.nongnu.org/lzip/data_crc_inaccuracy.png&quot; alt=&quot;Block check inaccuracy&quot; /&gt;&lt;br /&gt;Figure 3. Inaccuracy of block check up to 1 GB of compressed size.&lt;/p&gt;
&lt;p&gt;For the zeroed-block error model, the inaccuracy curves are similar to the ones in figure 3, except that they have discontinuities because a false positive can be produced only if the last block is suitably aligned.&lt;/p&gt;
&lt;p&gt; The results above assume that the decoder does not detect any errors, but testing shows that, on large enough files, the &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#glossary&quot;&gt;Pudd&lt;/a&gt; of a pure LZMA decoder like the one in lzip is of about 2.52e-7 for the single-byte error model. More precisely, 277.24 million trial decompressions on files ranging from 1 kB to 217 MB of compressed size resulted in 70 errors undetected by the decoder (all of them detected by the CRC). This additional detection capability reduces the Pud by the same factor. (In fact the reduction of Pud is larger because 9 of the 70 errors didn't cause error multiplication; they produced just one wrong byte in the decompressed data, which is guaranteed to be detected by the CRC). The estimated Pud for lzip, based on these data, is of about 2.52e-7 * 2.33e-10 = 5.88e-17.&lt;/p&gt;
&lt;p&gt;For the zeroed-block error model, the additional detection capability of a pure LZMA decoder is probably much larger. A LZMA stream is a check sequence in itself, and large errors seem less probable to escape detection than small ones. In fact, the lzip decoder detected the error in all the 2 million trial decompressions run with a zeroed-block. The xz decoder can't achieve such performance because LZMA2 includes uncompressed packets, where the decoder can't detect any errors.&lt;/p&gt;
&lt;p&gt;There is a good reason why bzip2, gzip, lzip and most other compressed formats use a 32-bit check sequence; it provides for an optimal detection of errors. Larger check sequences may (or may not) reduce the number of false negatives at the cost of always increasing the number of false positives. But significantly reducing the number of false negatives may be impossible if the number of false negatives is already insignificant, as is the case in bzip2, gzip and lzip files. On the other hand, the number of false positives increases linearly with the size of the check sequence. CRC64 doubles the number of false positives of CRC32, and SHA-256 produces 8 times more false positives than CRC32, decreasing the accuracy of the error detection instead of increasing it.&lt;/p&gt;
&lt;p&gt;Increasing the probability of a false positive for corruption in the long-term storage of valuable data is a bad idea. This is why the lzip format, designed for long-term archiving, provides 3 factor integrity checking and the decompressor reports mismatches in each factor separately. This way if just one byte in one factor fails but the other two factors match the data, it probably means that the data are intact and the corruption just affects the mismatching check sequence. GNU gzip also reports mismatches in its 2 factors separately, but does not report the exact values, making it more difficult to tell real corruption from a false positive. Bzip2 reports separately its 2 levels of CRCs, allowing the detection of some false positives.&lt;/p&gt;
&lt;p&gt;Being able to produce files without a check sequence for the decompressed data may help xz to rank higher on decompression benchmarks, but is a very bad idea for long-term archiving. The whole idea of supporting several check types is wrong. It &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#fragmented&quot;&gt;fragments the format&lt;/a&gt; and introduces a point of failure in the xz stream; if the corruption affects the stream flags, xz won't be able to verify the integrity of the data because the type and size of the check sequence are lost.&lt;/p&gt;
&lt;h3&gt;&lt;a name=&quot;misguided&quot; href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#misguided&quot; id=&quot;misguided&quot;&gt;2.11&lt;/a&gt; Xz's error detection is misguided&lt;/h3&gt;
&lt;p&gt;Xz tries to detect errors in parts of the compressed file that do not affect decompression (for example in padding bytes), obviating the fact that nobody is interested in the integrity of the compressed file; it is the integrity of the decompressed data what matters. Note that the xz format specification sets more strict requirements for the integrity of the padding than for the integrity of the payload. The specification &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#fragmented&quot;&gt;does not guarantee&lt;/a&gt; that the integrity of the decompressed data will be verified, but it mandates that the decompression must be aborted as soon as a damaged padding byte is found. (See sections 2.2, 3.1.6, 3.3 and 4.4 of &lt;a href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#refXz_format&quot;&gt;[Xz format]&lt;/a&gt;). Xz goes so far as to &quot;protect&quot; padding bytes with a CRC32. This behavior of xz just causes unnecessary data loss.&lt;/p&gt;
&lt;p&gt;Checking the integrity of the decompressed data is important because it not only guards against corruption in the compressed file, but also against memory errors, undetected bugs in the decompressor, etc.&lt;/p&gt;
&lt;p&gt;The only reason to be concerned about the integrity of the compressed file itself is to be sure that it has not been modified or replaced with other file. But no amount of strictness in the decompressor can guarantee that a file has not been modified or replaced. Some other means must be used for this purpose, for example an external cryptographically secure hash of the file.&lt;/p&gt;
&lt;h3&gt;2.12 Xz does not provide any data recovery means&lt;/h3&gt;
&lt;p&gt;File corruption is an unlikely event. Being unable to restore a file because the backup copy is also damaged is even less likely. But unlikely events happen constantly to somebody somewhere. This is why tools like ddrescue, bzip2recover and lziprecover exist in the first place. Lziprecover defines itself as &quot;a last line of defense for the case where the backups are also damaged&quot;.&lt;/p&gt;
&lt;p&gt;The safer a format is, the easier it is to develop a capable recovery tool for it. Neither xz nor gzip do provide any recovery tool. Bzip2 provides bzip2recover, which can help to manually assemble a correct file from the undamaged blocks of two or more copies. Lzip provides lziprecover, which can produce a correct file by merging the good parts of two or more damaged copies and can additionally repair slightly damaged files without the need of a backup copy.&lt;/p&gt;
&lt;h2&gt;3 Then, why some free software projects use xz?&lt;/h2&gt;
&lt;p&gt;Because evaluating formats is difficult and most free software projects are not concerned about long-term archiving, or even about format quality. Therefore they tend to use the most advertised formats. Both lzma-alone and xz have gained some popularity in spite of their defects mainly because they are associated to the popular 7-zip archiver.&lt;/p&gt;
&lt;p&gt;This of course is sad because we software developers are among the few people who are able to understand the strengths and weaknesses of formats. We have a moral duty to choose wisely the formats we use because everybody else will blindly use whatever formats we choose.&lt;/p&gt;
&lt;h2&gt;4 Conclusions&lt;/h2&gt;
&lt;p&gt;There are several reasons why the xz compressed data format should not be used for long-term archiving, specially of valuable data. To begin with, xz is a complex container format. Using a complex format for long-term archiving would be a bad idea even if the format were well-designed, which xz is not. In general, the more complex the format, the less probable that it can be decoded in the future by a digital archaeologist. For long-term archiving, simple is robust.&lt;/p&gt;
&lt;p&gt;Xz is fragmented by design. Xz implementations may choose what subset of the format they support. They may even choose to not support integrity checking at all. Safe interoperability among xz implementations is not guaranteed, which makes the use of xz inadvisable not only for long-term archiving, but also for data sharing and for free software distribution. Xz is also unreasonably extensible; it has room for trillions of compression algorithms, but currently only supports one, LZMA2, which in spite of its name is not an improved version of LZMA, but an unsafe container for LZMA data. Such egregious level of extensibility makes corruption both more probable and more difficult to recover from. Additionally, the xz format lacks a version number field, which makes xz's extensibility problematic.&lt;/p&gt;
&lt;p&gt;Xz fails to protect critical fields like length fields and flags signalling the presence of optional fields. Xz uses variable-length integers unsafely, specially when they are used to store the size of other fields or when they are concatenated together. These defects make xz fragile, meaning that most of the times when it reports a false positive, the decoder state is so mangled that it is unable to recover the decompressed data.&lt;/p&gt;
&lt;p&gt;Error detection in the xz format is less accurate than in bzip2, gzip and lzip formats mainly because of false positives, and specially if an overkill check sequence like SHA-256 is used in xz. Another cause of false positives is that xz tries to detect errors in parts of the compressed file that do not affect decompression, like the padding added to keep the useless 4 byte alignment. In total xz reports several times more false positives than bzip2, gzip or lzip, and every false positive may result in unnecessary loss of data.&lt;/p&gt;
&lt;p&gt;All these defects and design errors reduce the value of xz as a general-purpose format because anybody wanting to archive a file already compressed in xz format will have to either leave it as-is and face a larger risk of losing the data, or waste time recompressing the data into a format more suitable for long-term archiving.&lt;/p&gt;
&lt;p&gt;The weird combination of unprotected critical fields, overkill check sequences, and padding bytes &quot;protected&quot; by a CRC32 can only be explained by the inexperience of the designers of xz. It is said that given enough eyeballs, all bugs are shallow. But the adoption of xz by several GNU/Linux distributions shows that if those eyeballs lack the required experience, it may take too long for them to find the bugs. It would be an improvement for data safety if compressed data formats intended for broad use were designed by experts and peer reviewed before publication. This would help to avoid design errors like those of xz, which are very difficult to fix once a format is in use.&lt;/p&gt;
&lt;h2&gt;5 References&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&lt;a name=&quot;refXz_format&quot; href=&quot;http://tukaani.org/xz/xz-file-format.txt&quot; id=&quot;refXz_format&quot;&gt;Xz format specification, version 1.0.4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a name=&quot;refDebian&quot; href=&quot;http://lists.debian.org/debian-devel/2015/06/msg00173.html&quot; id=&quot;refDebian&quot;&gt;Adding support for LZIP to dpkg, using that instead of xz, archive wide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a name=&quot;refKoopman&quot; href=&quot;http://repository.cmu.edu/ece/293&quot; id=&quot;refKoopman&quot;&gt;Selection of Cyclic Redundancy Code and Checksum Algorithms to Ensure Critical Data Integrity. Philip Koopman, Kevin Driscoll, Brendan Hall. 2015&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a name=&quot;refLZMA2_format&quot; href=&quot;http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Markov_chain_algorithm#LZMA2_format&quot; id=&quot;refLZMA2_format&quot;&gt;Wikipedia - LZMA, section 'LZMA2 format'&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a name=&quot;refDeflate&quot; href=&quot;http://tools.ietf.org/html/rfc1951&quot; id=&quot;refDeflate&quot;&gt;DEFLATE Compressed Data Format Specification version 1.3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a name=&quot;refMSL&quot; href=&quot;http://llis.nasa.gov/lesson/11201&quot; id=&quot;refMSL&quot;&gt;Mars Science Laboratory (MSL) Sol-200 Anomaly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a name=&quot;refBzip2&quot; href=&quot;http://en.wikipedia.org/wiki/Bzip2#File_format&quot; id=&quot;refBzip2&quot;&gt;Wikipedia - Bzip2, section 'File format'&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a name=&quot;refGzip&quot; href=&quot;http://tools.ietf.org/html/rfc1952&quot; id=&quot;refGzip&quot;&gt;GZIP file format specification version 4.3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a name=&quot;refLzip&quot; href=&quot;http://www.nongnu.org/lzip/manual/lzip_manual.html#File-format&quot; id=&quot;refLzip&quot;&gt;Lzip manual, section 'File format'&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a name=&quot;refBenchmark&quot; href=&quot;http://www.nongnu.org/lzip/lzip_benchmark.html&quot; id=&quot;refBenchmark&quot;&gt;Bzip2, gzip, lzip and xz benchmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a name=&quot;refRFC2119&quot; href=&quot;http://tools.ietf.org/html/rfc2119&quot; id=&quot;refRFC2119&quot;&gt;Key words for use in RFCs to Indicate Requirement Levels&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;&lt;a name=&quot;glossary&quot; href=&quot;https://www.nongnu.org/lzip/xz_inadequate.html#glossary&quot; id=&quot;glossary&quot;&gt;6&lt;/a&gt; Glossary&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;^: Exponentiation symbol.&lt;/li&gt;
&lt;li&gt;Accuracy: Freedom from mistakes. Is defined as 1 - inaccuracy.&lt;/li&gt;
&lt;li&gt;Bit flip: Error that inverts a single bit value.&lt;/li&gt;
&lt;li&gt;Burst Error: A set of bit errors contained within a span shorter than the length of the codeword. The burst length is the distance between the first and last bit errors. Zero or more of the intervening bits may be erroneous.&lt;/li&gt;
&lt;li&gt;Check sequence: An error-detecting code value stored with associated data. A dataword plus a check sequence makes up a codeword.&lt;/li&gt;
&lt;li&gt;Codeword: A dataword combined with a check sequence.&lt;/li&gt;
&lt;li&gt;Dataword: The data being protected by a check sequence. A dataword can be any number of bits and is independent of the machine word size (e.g., a dataword could be 64 Megabytes).&lt;/li&gt;
&lt;li&gt;False negative: Undetected error producing incorrect decompressed data.&lt;/li&gt;
&lt;li&gt;False positive: Inability or refusal to decompress the data in a damaged compressed file even if the compressed data proper remain decompressible. (i.e., either the compressed data have not suffered any damage or the damage does not hinder decompression).&lt;/li&gt;
&lt;li&gt;Fragility: Inability to recover the decompressed data (for example decompressing to standard output) in case of a false positive.&lt;/li&gt;
&lt;li&gt;Inaccuracy: Ratio of error detection mistakes. Is defined as ( false_negatives + false_positives ) / total_cases.&lt;/li&gt;
&lt;li&gt;Overhead: Everything in the file except the compressed data proper. (Headers, check sequences, padding, etc).&lt;/li&gt;
&lt;li&gt;Overkill: An unnecessary excess of whatever is needed to achieve a goal.&lt;/li&gt;
&lt;li&gt;Pud: Probability of an undetected error (false negative). Pud is equal to Pudd * Pudc.&lt;/li&gt;
&lt;li&gt;Pudc: Probability of an error undetected by the check sequence.&lt;/li&gt;
&lt;li&gt;Pudd: Probability of an error undetected by the decoder.&lt;/li&gt;
&lt;li&gt;Unsafe: 1 Likely to cause severe data loss even in case of the smallest corruption (a single bit flip). 2 Likely to produce false negatives.&lt;/li&gt;
&lt;/ul&gt;&lt;hr /&gt;&lt;p&gt;Copyright © 2016 Antonio Diaz Diaz.&lt;/p&gt;
&lt;p&gt;You are free to copy and distribute this article without limitation, but you are not allowed to modify it.&lt;/p&gt;
&lt;p&gt;First published: 2016-06-11&lt;br /&gt;Updated: 2017-07-03&lt;/p&gt;
&lt;/body&gt;</description>
<pubDate>Fri, 20 Apr 2018 14:03:37 +0000</pubDate>
<dc:creator>pandalicious</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nongnu.org/lzip/xz_inadequate.html</dc:identifier>
</item>
<item>
<title>Towards Scala 3</title>
<link>http://www.scala-lang.org/blog/2018/04/19/scala-3.html</link>
<guid isPermaLink="true" >http://www.scala-lang.org/blog/2018/04/19/scala-3.html</guid>
<description>&lt;div class=&quot;blog-detail-head&quot;&gt;
&lt;div&gt;
&lt;p&gt;Thursday 19 April 2018&lt;/p&gt;
&lt;p&gt;Martin Odersky&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Now that Scala 2.13 is only a few months away, it’s time to consider the roadmap beyond it. It’s been no secret that the work on &lt;a href=&quot;https://github.com/lampepfl/dotty&quot;&gt;Dotty&lt;/a&gt; over the last 5 years was intended to explore what a new Scala could look like. We are now at a stage where we can commit: Dotty will become Scala 3.0.&lt;/p&gt;
&lt;p&gt;Of course, this statement invites many follow-up questions. Here are some answers we can already give today. We expect there will be more questions and answers as things shape up.&lt;/p&gt;
&lt;h3 id=&quot;when-will-it-come-out&quot;&gt;When will it come out?&lt;/h3&gt;
&lt;p&gt;The intent is to publish the final Scala 3.0 soon after Scala 2.14. At the current release schedule (which might still change), that means early 2020.&lt;/p&gt;
&lt;h3 id=&quot;what-is-scala-214-for&quot;&gt;What is Scala 2.14 for?&lt;/h3&gt;
&lt;p&gt;Scala 2.14’s main focus will be on smoothing the migration to Scala 3. It will do this by defining migration tools, shim libraries, and targeted deprecations, among others.&lt;/p&gt;
&lt;h3 id=&quot;whats-new-in-scala-3&quot;&gt;What’s new in Scala 3?&lt;/h3&gt;
&lt;p&gt;Scala has pioneered the fusion of object-oriented and functional programming in a typed setting. Scala 3 will be a big step towards realizing the full potential of these ideas. Its main objectives are to&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;become more opinionated by promoting programming idioms we found to work well,&lt;/li&gt;
&lt;li&gt;simplify where possible,&lt;/li&gt;
&lt;li&gt;eliminate inconsistencies and surprising behavior,&lt;/li&gt;
&lt;li&gt;build on strong foundations to ensure the design hangs well together,&lt;/li&gt;
&lt;li&gt;consolidate language constructs to improve the language’s consistency, safety, ergonomics, and performance.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The main language changes, either implemented or projected, are listed in the &lt;a href=&quot;http://dotty.epfl.ch/docs/reference/overview.html&quot;&gt;Reference section on the Dotty website&lt;/a&gt;. Many of the new features will be submitted to the &lt;a href=&quot;https://docs.scala-lang.org/sips&quot;&gt;SIP&lt;/a&gt; process, subject to approval.&lt;/p&gt;
&lt;p&gt;It’s worth emphasizing that Scala 2 and Scala 3 are fundamentally the same language. The compiler is new, but nearly everything Scala programmers already know about Scala 2 applies to Scala 3 as well, and most ordinary Scala 2 code will also work on Scala 3 with only minor changes.&lt;/p&gt;
&lt;h3 id=&quot;what-about-migration&quot;&gt;What about migration?&lt;/h3&gt;
&lt;p&gt;As with previous Scala upgrades, Scala 3 is not binary compatible with Scala 2. They are mostly source compatible, but differences exist. However:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Scala 3 code can use Scala 2 artifacts because the Scala 3 compiler understands the classfile format for sources compiled with Scala 2.12 and upwards.&lt;/li&gt;
&lt;li&gt;Scala 3 and Scala 2 share the same standard library.&lt;/li&gt;
&lt;li&gt;With some small tweaks it is possible to cross-build code for both Scala 2 and 3. We will provide a guide defining the shared language subset that can be compiled under both versions.&lt;/li&gt;
&lt;li&gt;The Scala 3 compiler has a &lt;code class=&quot;highlighter-rouge&quot;&gt;-language:Scala2&lt;/code&gt; option that lets it compile most Scala 2 code and at the same time highlights necessary rewritings as migration warnings.&lt;/li&gt;
&lt;li&gt;The compiler can perform many of the rewritings automatically using a &lt;code class=&quot;highlighter-rouge&quot;&gt;-rewrite&lt;/code&gt; option.&lt;/li&gt;
&lt;li&gt;Migration through automatic rewriting will also be offered through the &lt;a href=&quot;https://github.com/scalacenter/scalafix&quot;&gt;scalafix&lt;/a&gt; tool, which can convert sources to the cross-buildable language subset without requiring Scala 3 to be installed.&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;whats-the-expected-state-of-tool-support&quot;&gt;What’s the expected state of tool support?&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;Compiler: The Scala 3 compiler &lt;code class=&quot;highlighter-rouge&quot;&gt;dotc&lt;/code&gt; has been used to compile itself and a growing set of libraries for a number of years now.&lt;/li&gt;
&lt;li&gt;IDEs: IDE support is provided by having &lt;code class=&quot;highlighter-rouge&quot;&gt;dotc&lt;/code&gt; implement LSP, the &lt;a href=&quot;https://langserver.org&quot;&gt;Language Server Protocol&lt;/a&gt;, including standard operations such as completion and hyperlinking and more advanced ones such as find references or rename. There’s a &lt;a href=&quot;http://dotty.epfl.ch/docs/usage/ide-support.html&quot;&gt;VS Code plugin&lt;/a&gt; incorporating these operations. JetBrains has also released a first version of Scala 3 support in their &lt;a href=&quot;https://blog.jetbrains.com/scala/2017/03/23/scala-plugin-for-intellij-idea-2017-1-cleaner-ui-sbt-shell-repl-worksheet-akka-support-and-more&quot;&gt;Scala IntelliJ plugin&lt;/a&gt;, and we intend to work with them on further improvements.&lt;/li&gt;
&lt;li&gt;REPL: A friendly REPL is supported by the compiler&lt;/li&gt;
&lt;li&gt;Docs: A revamped Scaladoc tool generates docs for viewing in a browser and (in the future) also in the IDE..&lt;/li&gt;
&lt;li&gt;Build tools: There is a Dotty/Scala 3 plugin for &lt;a href=&quot;https://www.scala-sbt.org&quot;&gt;sbt&lt;/a&gt;, and we will also work on Scala 3 integration in other build tools.&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;what-about-stability&quot;&gt;What about stability?&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;A &lt;a href=&quot;https://github.com/lampepfl/dotty-community-build&quot;&gt;community build&lt;/a&gt; contains some &lt;a href=&quot;https://github.com/lampepfl/dotty-community-build/blob/master/src/test/scala/dotty/communitybuild/CommunityBuildTest.scala&quot;&gt;initial open source projects&lt;/a&gt; that are compiled nightly using Scala 3. We plan to add a lot more projects to the build between now and the final release.&lt;/li&gt;
&lt;li&gt;We plan to use the period of developer previews to ensure that core projects are published for Scala 3.&lt;/li&gt;
&lt;li&gt;We have incorporated most of the Scala 2 regression tests in the Scala 3 test suite and will keep including new tests.&lt;/li&gt;
&lt;li&gt;In the near future we plan to build all Scala 3 tools using a previous version of the &lt;code class=&quot;highlighter-rouge&quot;&gt;dotc&lt;/code&gt; compiler itself. So far all tools are built first with the current Scala compiler and then again with &lt;code class=&quot;highlighter-rouge&quot;&gt;dotc&lt;/code&gt;. Basing the build exclusively on Scala 3 has the advantage that it lets us “eat our own dog food” and try out the usability of Scala 3’s new language feature on a larger scale.&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;when-can-i-try-it-out&quot;&gt;When can I try it out?&lt;/h3&gt;
&lt;p&gt;You can start working with Dotty now. See the &lt;a href=&quot;http://dotty.epfl.ch/docs/contributing/getting-started.html&quot;&gt;getting started&lt;/a&gt; guide. Dotty releases are published every 6 weeks. We expect to be in feature-freeze and to release developer previews for Scala 3.0 in the first half of 2019.&lt;/p&gt;
&lt;h3 id=&quot;what-about-macros&quot;&gt;What about macros?&lt;/h3&gt;
&lt;p&gt;Stay tuned! We are about to release another blog post specifically about that issue.&lt;/p&gt;
&lt;h3 id=&quot;how-can-i-help&quot;&gt;How can I help?&lt;/h3&gt;
&lt;p&gt;Scala 3 is developed completely in the open at &lt;a href=&quot;https://github.com/lampepfl/dotty&quot;&gt;https://github.com/lampepfl/dotty&lt;/a&gt;. Get involved there, by fixing and opening issues, making pull requests, and participating in the discussions.&lt;/p&gt;
</description>
<pubDate>Fri, 20 Apr 2018 12:07:04 +0000</pubDate>
<dc:creator>kushti</dc:creator>
<og:title>Towards Scala 3</og:title>
<og:url>/blog/2018/04/19/scala-3.html</og:url>
<og:image>/resources/img/scala-spiral-3d-2-toned-down.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.scala-lang.org/blog/2018/04/19/scala-3.html</dc:identifier>
</item>
</channel>
</rss>