<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Reading privileged memory with a side-channel</title>
<link>https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html</link>
<guid isPermaLink="true" >https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html</guid>
<description>&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Posted by Jann Horn, Project Zero&lt;/span&gt;&lt;/div&gt;
&lt;strong id=&quot;docs-internal-guid-c66856d3-bdac-1e12-91fc-544573d27dc6&quot;&gt;&lt;br/&gt;&lt;/strong&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We have discovered that CPU data cache timing can be abused to efficiently leak information out of mis-speculated execution, leading to (at worst) arbitrary virtual memory read vulnerabilities across local security boundaries in various contexts.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Variants of this issue are known to affect many modern processors, including certain processors by Intel, AMD and ARM. For a few Intel and AMD CPU models, we have exploits that work against real software. We reported this issue to Intel, AMD and ARM on 2017-06-01 [1]&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;So far, there are three known variants of the issue:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Variant 1: bounds check bypass (&lt;/span&gt;&lt;span&gt;CVE-2017-5753)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Variant 2: branch target injection (&lt;/span&gt;&lt;span&gt;CVE-2017-5715)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Variant 3: rogue data cache load (&lt;/span&gt;&lt;span&gt;CVE-2017-5754)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Before the issues described here were publicly disclosed, Daniel Gruss, Moritz Lipp, Yuval Yarom, Paul Kocher, Daniel Genkin, Michael Schwarz, Mike Hamburg, Stefan Mangard, Thomas Prescher and Werner Haas also reported them; their [writeups/blogposts/paper drafts] are at:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;During the course of our research, we developed the following proofs of concept (PoCs):&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ol&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A PoC that demonstrates the basic principles behind variant 1 in userspace on the tested Intel Haswell Xeon CPU, the AMD FX CPU, the AMD PRO CPU and an ARM Cortex A57 [2].&lt;/span&gt; &lt;span&gt;This PoC only tests for the ability to read data inside mis-speculated execution within the same process, without crossing any privilege boundaries.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A PoC for variant 1 that, when running with normal user privileges under a modern Linux kernel with a distro-standard config, can perform arbitrary reads in a 4GiB range [3]&lt;/span&gt; &lt;span&gt;in kernel virtual memory on the Intel Haswell Xeon CPU. If the kernel's BPF JIT is enabled (non-default configuration), it also works on the AMD PRO CPU. On the Intel Haswell Xeon CPU, kernel virtual memory can be read at a rate of around 2000 bytes per second after around 4 seconds of startup time. [4]&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A PoC for variant 2 that, when running with root privileges inside a KVM guest created using virt-manager on the Intel Haswell Xeon CPU, with a specific (now outdated) version of Debian's distro kernel&lt;/span&gt; &lt;span&gt;[5] running on the host, can read host kernel memory at a rate of around 1500 bytes/second, with room for optimization. Before the attack can be performed, some initialization has to be performed that takes roughly between 10 and 30 minutes for a machine with 64GiB of RAM; the needed time should scale roughly linearly with the amount of host RAM. (If 2MB hugepages are available to the guest, the initialization should be much faster, but that hasn't been tested.)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A PoC for variant 3 that, when running with normal user privileges, can read kernel memory on the Intel Haswell Xeon CPU under some precondition. We believe that this precondition is that the targeted kernel memory is present in the L1D cache.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;For interesting resources around this topic, look down into the &quot;Literature&quot; section.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A warning regarding explanations about processor internals in this blogpost: This blogpost contains a lot of speculation about hardware internals based on observed behavior, which might not necessarily correspond to what processors are actually doing.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We have some ideas on possible mitigations and provided some of those ideas to the processor vendors; however, we believe that the processor vendors are in a much better position than we are to design and evaluate mitigations, and we expect them to be the source of authoritative guidance.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The PoC code and the writeups that we sent to the CPU vendors will be made available at a later date.&lt;/span&gt;&lt;/div&gt;

&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz (called &quot;Intel Haswell Xeon CPU&quot; in the rest of this document)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;AMD FX(tm)-8320 Eight-Core Processor (called &quot;AMD FX CPU&quot; in the rest of this document)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;AMD PRO A8-9600 R7, 10 COMPUTE CORES 4C+6G (called &quot;AMD PRO CPU&quot; in the rest of this document)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;An ARM Cortex A57 core of a Google Nexus 5x phone [6]&lt;/span&gt; &lt;span&gt;(called &quot;ARM Cortex A57&quot; in the rest of this document)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;retire:&lt;/span&gt; &lt;span&gt;An instruction retires when its results, e.g. register writes and memory writes, are committed and made visible to the rest of the system. Instructions can be executed out of order, but must always retire in order.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;logical processor core:&lt;/span&gt; &lt;span&gt;A logical processor core is what the operating system sees as a processor core. With hyperthreading enabled, the number of logical cores is a multiple of the number of physical cores.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;cached/uncached data:&lt;/span&gt; &lt;span&gt;In this blogpost, &quot;uncached&quot; data is data that is only present in main memory, not in any of the cache levels of the CPU. Loading uncached data will typically take over 100 cycles of CPU time.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;speculative execution:&lt;/span&gt; &lt;span&gt;A processor can execute past a branch without knowing whether it will be taken or where its target is, therefore executing instructions before it is known whether they should be executed. If this speculation turns out to have been incorrect, the CPU can discard the resulting state without architectural effects and continue execution on the correct execution path. Instructions do not retire before it is known that they are on the correct execution path.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;mis-speculation window:&lt;/span&gt; &lt;span&gt;The time window during which the CPU speculatively executes the wrong code and has not yet detected that mis-speculation has occurred.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section explains the common theory behind all three variants and the theory behind our PoC for variant 1 that, when running in userspace under a Debian distro kernel, can perform arbitrary reads in a 4GiB region of kernel memory in at least the following configurations:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel Haswell Xeon CPU, eBPF JIT is off (default state)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel Haswell Xeon CPU, eBPF JIT is on (non-default state)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;AMD PRO CPU, eBPF JIT is on (non-default state)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The state of the eBPF JIT can be toggled using the net.core.bpf_jit_enable sysctl.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Theoretical explanation&lt;/span&gt;&lt;/h2&gt;

&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Branch prediction predicts the branch target and enables the&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;processor to begin executing instructions long before the branch&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;true execution path is known.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In section 2.3.5.2 (&quot;L1 DCache&quot;):&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Loads can:&lt;/span&gt;&lt;/div&gt;

&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Be carried out speculatively, before preceding branches are resolved.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Take cache misses out of order and in an overlapped manner.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel's Software Developer's Manual&lt;/span&gt; &lt;span&gt;[7] states in Volume 3A, section 11.7 (&quot;Implicit Caching (Pentium 4, Intel Xeon, and P6 family processors&quot;):&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Implicit caching occurs when a memory element is made potentially cacheable, although the element may never have been accessed in the normal von Neumann sequence. Implicit caching occurs on the P6 and more recent processor families due to aggressive prefetching, branch prediction, and TLB miss handling. Implicit caching is an extension of the behavior of existing Intel386, Intel486, and Pentium processor systems, since software running on these processor families also has not been able to deterministically predict the behavior of instruction prefetch.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Consider the code sample below. If&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt; &lt;span&gt;is uncached, the processor can speculatively load data from&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[untrusted_offset_from_caller]&lt;/span&gt;&lt;span&gt;. This is an out-of-bounds read. That should not matter because the processor will effectively roll back the execution state when the branch has executed; none of the speculatively executed instructions will retire (e.g. cause registers etc. to be affected).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned long length;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned char data[];&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array *arr1 = ...;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;unsigned long untrusted_offset_from_caller = ...;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;if (untrusted_offset_from_caller &amp;lt;&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt;&lt;span&gt;) {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned char value =&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[untrusted_offset_from_caller]&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;&lt;/div&gt;


&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;However, in the following code sample, there's an issue. If&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[0x200]&lt;/span&gt; &lt;span&gt;and&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[0x300]&lt;/span&gt; &lt;span&gt;are not cached, but all other accessed data is, and the branch conditions are predicted as true, the processor can do the following speculatively before&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt; &lt;span&gt;has been loaded and the execution is re-steered:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;load&lt;/span&gt; &lt;span&gt;value =&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[&lt;/span&gt;&lt;span&gt;untrusted_offset_from_caller&lt;/span&gt;&lt;span&gt;]&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;start a load from a data-dependent offset in&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data&lt;/span&gt;&lt;span&gt;, loading the corresponding cache line into the L1 cache&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned long length;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned char data[];&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array *arr1 = ...; /* small array */&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array *arr2 = ...; /* array of size 0x400 */&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;/* &amp;gt;0x400&lt;/span&gt; &lt;span&gt;(OUT OF BOUNDS!)&lt;/span&gt; &lt;span&gt;*/&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;unsigned long untrusted_offset_from_caller = ...;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;if (&lt;/span&gt;&lt;span&gt;untrusted_offset_from_caller&lt;/span&gt; &lt;span&gt;&amp;lt;&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt;&lt;span&gt;) {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned char value =&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[&lt;/span&gt;&lt;span&gt;untrusted_offset_from_caller&lt;/span&gt;&lt;span&gt;]&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned long index2 = ((value&amp;amp;1)*0x100)+0x200;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; if (index2 &amp;lt; arr2-&amp;gt;length) {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;   unsigned char value2 =&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[index2]&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;&lt;/div&gt;


&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;After the execution has been returned to the non-speculative path because the processor has noticed that&lt;/span&gt; &lt;span&gt;untrusted_offset_from_caller&lt;/span&gt; &lt;span&gt;is bigger than&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt;&lt;span&gt;, the cache line containing&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[index2]&lt;/span&gt; &lt;span&gt;stays in the L1 cache. By measuring the time required to load&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[0x200]&lt;/span&gt; &lt;span&gt;and&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[0x300]&lt;/span&gt;&lt;span&gt;, an attacker can then determine whether the value of&lt;/span&gt; &lt;span&gt;index2&lt;/span&gt; &lt;span&gt;during speculative execution was 0x200 or 0x300 - which discloses whether&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[&lt;/span&gt;&lt;span&gt;untrusted_offset_from_caller&lt;/span&gt;&lt;span&gt;]&lt;/span&gt;&lt;span&gt;&amp;amp;1&lt;/span&gt; &lt;span&gt;is 0 or 1.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To be able to actually use this behavior for an attack, an attacker needs to be able to cause the execution of such a vulnerable code pattern in the targeted context with an out-of-bounds index. For this, the vulnerable code pattern must either be present in existing code, or there must be an interpreter or JIT engine that can be used to generate the vulnerable code pattern. So far, we have not actually identified any existing, exploitable instances of the vulnerable code pattern; the PoC for leaking kernel memory using variant 1 uses the eBPF interpreter or the eBPF JIT engine, which are built into the kernel and accessible to normal users.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A minor variant of this could be to instead use an out-of-bounds read to a function pointer to gain control of execution in the mis-speculated path. We did not investigate this variant further.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Attacking the kernel&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section describes in more detail how variant 1 can be used to leak Linux kernel memory using the eBPF bytecode interpreter and JIT engine. While there are many interesting potential targets for variant 1 attacks, we chose to attack the Linux in-kernel eBPF JIT/interpreter because it provides more control to the attacker than most other JITs.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The Linux kernel supports eBPF since version 3.18. Unprivileged userspace code can supply bytecode to the kernel that is verified by the kernel and then:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;either interpreted by an in-kernel bytecode interpreter&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;or translated to native machine code that also runs in kernel context using a JIT engine (which translates individual bytecode instructions without performing any further optimizations)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Execution of the bytecode can be triggered by attaching the eBPF bytecode to a socket as a filter and then sending data through the other end of the socket.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Whether the JIT engine is enabled depends on a run-time configuration setting - but at least on the tested Intel processor, the attack works independent of that setting.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Unlike classic BPF, eBPF has data types like data arrays and function pointer arrays into which eBPF bytecode can index. Therefore, it is possible to create the code pattern described above in the kernel using eBPF bytecode.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;eBPF's data arrays are less efficient than its function pointer arrays, so the attack will use the latter where possible.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Both machines on which this was tested have no SMAP, and the PoC relies on that (but it shouldn't be a precondition in principle).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Additionally, at least on the Intel machine on which this was tested, bouncing modified cache lines between cores is slow, apparently because the MESI protocol is used for cache coherence [8]&lt;/span&gt;&lt;span&gt;. Changing the reference counter of an eBPF array on one physical CPU core causes the cache line containing the reference counter to be bounced over to that CPU core, making reads of the reference counter on all other CPU cores slow until the changed reference counter has been written back to memory. Because the length and the reference counter of an eBPF array are stored in the same cache line, this also means that changing the reference counter on one physical CPU core causes reads of the eBPF array's length to be slow on other physical CPU cores (intentional false sharing).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The attack uses two eBPF programs. The first one tail-calls through a page-aligned eBPF function pointer array&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;at a configurable index. In simplified terms, this program is used to determine the address of&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;by guessing the offset from&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;to a userspace address and tail-calling through&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;at the guessed offsets. To cause the branch prediction to predict that the offset is below the length of&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt;&lt;span&gt;, tail calls to an in-bounds index are performed in between. To increase the mis-speculation window, the cache line containing the length of&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;is bounced to another core. To test whether an offset guess was successful, it can be tested whether the userspace address has been loaded into the cache.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Because such straightforward brute-force guessing of the address would be slow, the following optimization is used: 2&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;adjacent userspace memory mappings [9]&lt;/span&gt;&lt;span&gt;, each consisting of 2&lt;/span&gt;&lt;span&gt;4&lt;/span&gt; &lt;span&gt;pages, are created at the userspace address&lt;/span&gt; &lt;span&gt;user_mapping_area&lt;/span&gt;&lt;span&gt;, covering a total area of 2&lt;/span&gt;&lt;span&gt;31&lt;/span&gt; &lt;span&gt;bytes. Each mapping maps the same physical pages, and all mappings are present in the pagetables.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-64YRnkgMujY/Wk064WtrSRI/AAAAAAAACPg/O4ZxlUiWerACL61P490xrVMWSmHTwQflQCEwYBhgL/s1600/image3.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;138&quot; data-original-width=&quot;624&quot; height=&quot;139&quot; src=&quot;https://3.bp.blogspot.com/-64YRnkgMujY/Wk064WtrSRI/AAAAAAAACPg/O4ZxlUiWerACL61P490xrVMWSmHTwQflQCEwYBhgL/s640/image3.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This permits the attack to be carried out in steps of 2&lt;/span&gt;&lt;span&gt;31&lt;/span&gt; &lt;span&gt;bytes. For each step, after causing an out-of-bounds access through&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt;&lt;span&gt;, only one cache line each from the first 2&lt;/span&gt;&lt;span&gt;4&lt;/span&gt; &lt;span&gt;pages of&lt;/span&gt; &lt;span&gt;user_mapping_area&lt;/span&gt; &lt;span&gt;have to be tested for cached memory. Because the L3 cache is physically indexed, any access to a virtual address mapping a physical page will cause all other virtual addresses mapping the same physical page to become cached as well.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;When this attack finds a hit—a cached memory location—the upper 33 bits of the kernel address are known (because they can be derived from the address guess at which the hit occurred), and the low 16 bits of the address are also known (from the offset inside&lt;/span&gt; &lt;span&gt;user_mapping_area&lt;/span&gt; &lt;span&gt;at which the hit was found). The remaining part of the address of&lt;/span&gt; &lt;span&gt;user_mapping_area&lt;/span&gt; &lt;span&gt;is the middle.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-kSdcfy3GVSw/Wk065qWZZaI/AAAAAAAACPs/7HRZEXij3rIqvAf4gq8QpyHKvJsT2pYIQCEwYBhgL/s1600/image5.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;149&quot; data-original-width=&quot;624&quot; height=&quot;152&quot; src=&quot;https://1.bp.blogspot.com/-kSdcfy3GVSw/Wk065qWZZaI/AAAAAAAACPs/7HRZEXij3rIqvAf4gq8QpyHKvJsT2pYIQCEwYBhgL/s640/image5.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The remaining bits in the middle can be determined by bisecting the remaining address space: Map two physical pages to adjacent ranges of virtual addresses, each virtual address range the size of half of the remaining search space, then determine the remaining address bit-wise.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;At this point, a second eBPF program can be used to actually leak data. In pseudocode, this program looks as follows:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t bitmask = &amp;lt;runtime-configurable&amp;gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t bitshift_selector = &amp;lt;runtime-configurable&amp;gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t prog_array_base_offset = &amp;lt;runtime-configurable&amp;gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t secret_data_offset = &amp;lt;runtime-configurable&amp;gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;// index will be bounds-checked by the runtime,&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;// but the bounds check will be bypassed speculatively&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t secret_data = bpf_map_read(array=victim_array, index=secret_data_offset);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;// select a single bit, move it to a specific position, and add the base offset&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t progmap_index = (((secret_data &amp;amp; bitmask) &amp;gt;&amp;gt; bitshift_selector) &amp;lt;&amp;lt; 7) + prog_array_base_offset;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;bpf_tail_call(prog_map, progmap_index);&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This program reads 8-byte-aligned 64-bit values from an eBPF data array &quot;&lt;/span&gt;&lt;span&gt;victim_map&lt;/span&gt;&lt;span&gt;&quot; at a runtime-configurable offset and bitmasks and bit-shifts the value so that one bit is mapped to one of two values that are 2&lt;/span&gt;&lt;span&gt;7&lt;/span&gt; &lt;span&gt;bytes apart (sufficient to not land in the same or adjacent cache lines when used as an array index). Finally it adds a 64-bit offset, then uses the resulting value as an offset into&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;for a tail call.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This program can then be used to leak memory by repeatedly calling the eBPF program with an out-of-bounds offset into&lt;/span&gt; &lt;span&gt;victim_map&lt;/span&gt; &lt;span&gt;that specifies the data to leak and an out-of-bounds offset into&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;that causes&lt;/span&gt; &lt;span&gt;prog_map + offset&lt;/span&gt; &lt;span&gt;to point to a userspace memory area. Misleading the branch prediction and bouncing the cache lines works the same way as for the first eBPF program, except that now, the cache line holding the length of&lt;/span&gt; &lt;span&gt;victim_map&lt;/span&gt; &lt;span&gt;must also be bounced to another core.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section describes the theory behind our PoC for variant 2 that, when running with root privileges inside a KVM guest created using virt-manager on the Intel Haswell Xeon CPU, with a specific version of Debian's distro kernel running on the host, can read host kernel memory at a rate of around 1500 bytes/second.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Basics&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Prior research (see the Literature section at the end) has shown that it is possible for code in separate security contexts to influence each other's branch prediction. So far, this has only been used to infer information about where code is located (in other words, to create interference from the victim to the attacker); however, the basic hypothesis of this attack variant is that it can also be used to redirect execution of code in the victim context (in other words, to create interference from the attacker to the victim; the other way around).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-O6-JHlkvq5U/Wk064bpm2qI/AAAAAAAACPk/rjpYr72vyF0jYASgT-w4NBFQXC3hgIZZwCEwYBhgL/s1600/image2.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;200&quot; data-original-width=&quot;278&quot; src=&quot;https://3.bp.blogspot.com/-O6-JHlkvq5U/Wk064bpm2qI/AAAAAAAACPk/rjpYr72vyF0jYASgT-w4NBFQXC3hgIZZwCEwYBhgL/s1600/image2.png&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The basic idea for the attack is to target victim code that contains an indirect branch whose target address is loaded from memory and flush the cache line containing the target address out to main memory. Then, when the CPU reaches the indirect branch, it won't know the true destination of the jump, and it won't be able to calculate the true destination until it has finished loading the cache line back into the CPU, which takes a few hundred cycles. Therefore, there is a time window of typically over 100 cycles in which the CPU will speculatively execute instructions based on branch prediction.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Haswell branch prediction internals&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Some of the internals of the branch prediction implemented by Intel's processors have already been published; however, getting this attack to work properly required significant further experimentation to determine additional details.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section focuses on the branch prediction internals that were experimentally derived from the Intel Haswell Xeon CPU.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Haswell seems to have multiple branch prediction mechanisms that work very differently:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A generic branch predictor that can only store one target per source address; used for all kinds of jumps, like absolute jumps, relative jumps and so on.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A specialized indirect call predictor that can store multiple targets per source address; used for indirect calls.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;(There is also a specialized return predictor, according to Intel's optimization manual, but we haven't analyzed that in detail yet. If this predictor could be used to reliably dump out some of the call stack through which a VM was entered, that would be very interesting.)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Generic predictor&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The generic branch predictor, as documented in prior research, only uses the lower 31 bits of the address of the last byte of the source instruction for its prediction. If, for example, a branch target buffer (BTB) entry exists for a jump from 0x4141.0004.1000 to 0x4141.0004.5123, the generic predictor will also use it to predict a jump from 0x4242.0004.1000. When the higher bits of the source address differ like this, the higher bits of the predicted destination change together with it—in this case, the predicted destination address will be 0x4242.0004.5123—so apparently this predictor doesn't store the full, absolute destination address.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Before the lower 31 bits of the source address are used to look up a BTB entry, they are folded together using XOR. Specifically, the following bits are folded together:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;
&lt;table&gt;&lt;colgroup&gt;&lt;col width=&quot;*&quot;/&gt;&lt;col width=&quot;*&quot;/&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x100.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x200.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x400.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x800.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x2000.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x4000.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In other words, if a source address is XORed with both numbers in a row of this table, the branch predictor will not be able to distinguish the resulting address from the original source address when performing a lookup. For example, the branch predictor is able to distinguish source addresses 0x100.0000 and 0x180.0000, and it can also distinguish source addresses 0x100.0000 and 0x180.8000, but it can't distinguish source addresses 0x100.0000 and 0x140.2000 or source addresses 0x100.0000 and 0x180.4000. In the following, this will be referred to as aliased source addresses.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;When an aliased source address is used, the branch predictor will still predict the same target as for the unaliased source address. This indicates that the branch predictor stores a truncated absolute destination address, but that hasn't been verified.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Based on observed maximum forward and backward jump distances for different source addresses, the low 32-bit half of the target address could be stored as an absolute 32-bit value with an additional bit that specifies whether the jump from source to target crosses a 2&lt;/span&gt;&lt;span&gt;32&lt;/span&gt; &lt;span&gt;boundary; if the jump crosses such a boundary, bit 31 of the source address determines whether the high half of the instruction pointer should increment or decrement.&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Indirect call predictor&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The inputs of the BTB lookup for this mechanism seem to be:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The low 12 bits of the address of the source instruction (we are not sure whether it's the address of the first or the last byte) or a subset of them.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The branch history buffer state.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;If the indirect call predictor can't resolve a branch, it is resolved by the generic predictor instead. Intel's optimization manual hints at this behavior: &quot;Indirect Calls and Jumps. These may either be predicted as having a monotonic target or as having targets that vary in accordance with recent program behavior.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The branch history buffer (BHB) stores information about the last 29 taken branches - basically a fingerprint of recent control flow - and is used to allow better prediction of indirect calls that can have multiple targets.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The update function of the BHB works as follows (in pseudocode;&lt;/span&gt; &lt;span&gt;src&lt;/span&gt; &lt;span&gt;is the address of the last byte of the source instruction,&lt;/span&gt; &lt;span&gt;dst&lt;/span&gt; &lt;span&gt;is the destination address):&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;void bhb_update(uint58_t *bhb_state, unsigned long src, unsigned long dst) {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state &amp;lt;&amp;lt;= 2;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (dst &amp;amp; 0x3f);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0xc0) &amp;gt;&amp;gt; 6;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0xc00) &amp;gt;&amp;gt; (10 - 2);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0xc000) &amp;gt;&amp;gt; (14 - 4);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0x30) &amp;lt;&amp;lt; (6 - 4);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0x300) &amp;lt;&amp;lt; (8 - 8);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0x3000) &amp;gt;&amp;gt; (12 - 10);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0x30000) &amp;gt;&amp;gt; (16 - 12);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0xc0000) &amp;gt;&amp;gt; (18 - 14);&lt;/span&gt;&lt;/div&gt;

&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Some of the bits of the BHB state seem to be folded together further using XOR when used for a BTB access, but the precise folding function hasn't been understood yet.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The BHB is interesting for two reasons. First, knowledge about its approximate behavior is required in order to be able to accurately cause collisions in the indirect call predictor. But it also permits dumping out the BHB state at any repeatable program state at which the attacker can execute code - for example, when attacking a hypervisor, directly after a hypercall. The dumped BHB state can then be used to fingerprint the hypervisor or, if the attacker has access to the hypervisor binary, to determine the low 20 bits of the hypervisor load address (in the case of KVM: the low 20 bits of the load address of kvm-intel.ko).&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Reverse-Engineering Branch Predictor Internals&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This subsection describes how we reverse-engineered the internals of the Haswell branch predictor. Some of this is written down from memory, since we didn't keep a detailed record of what we were doing.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We initially attempted to perform BTB injections into the kernel using the generic predictor, using the knowledge from prior research that the generic predictor only looks at the lower half of the source address and that only a partial target address is stored. This kind of worked - however, the injection success rate was very low, below 1%. (This is the method we used in our preliminary PoCs for method 2 against modified hypervisors running on Haswell.)&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We decided to write a userspace test case to be able to more easily test branch predictor behavior in different situations.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Based on the assumption that branch predictor state is shared between hyperthreads [10]&lt;/span&gt;&lt;span&gt;, we wrote a program of which two instances are each pinned to one of the two logical processors running on a specific physical core, where one instance attempts to perform branch injections while the other measures how often branch injections are successful. Both instances were executed with ASLR disabled and had the same code at the same addresses. The injecting process performed indirect calls to a function that accesses a (per-process) test variable; the measuring process performed indirect calls to a function that tests, based on timing, whether the per-process test variable is cached, and then evicts it using CLFLUSH. Both indirect calls were performed through the same callsite. Before each indirect call, the function pointer stored in memory was flushed out to main memory using CLFLUSH to widen the speculation time window. Additionally, because of the reference to &quot;recent program behavior&quot; in Intel's optimization manual, a bunch of conditional branches that are always taken were inserted in front of the indirect call.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In this test, the injection success rate was above 99%, giving us a base setup for future experiments.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-RMKau2Jstd0/Wk066oeGPUI/AAAAAAAACP0/sb5X95OHei0JV0xhCQDRhaYhRsLvx9ZmACEwYBhgL/s1600/image7.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;644&quot; data-original-width=&quot;618&quot; height=&quot;640&quot; src=&quot;https://2.bp.blogspot.com/-RMKau2Jstd0/Wk066oeGPUI/AAAAAAAACP0/sb5X95OHei0JV0xhCQDRhaYhRsLvx9ZmACEwYBhgL/s640/image7.png&quot; width=&quot;612&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We then tried to figure out the details of the prediction scheme. We assumed that the prediction scheme uses a global branch history buffer of some kind.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To determine the duration for which branch information stays in the history buffer, a conditional branch that is only taken in one of the two program instances was inserted in front of the series of always-taken conditional jumps, then the number of always-taken conditional jumps (N) was varied. The result was that for N=25, the processor was able to distinguish the branches (misprediction rate under 1%), but for N=26, it failed to do so (misprediction rate over 99%).&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Therefore, the branch history buffer had to be able to store information about at least the last 26 branches.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The code in one of the two program instances was then moved around in memory. This revealed that only the lower 20 bits of the source and target addresses have an influence on the branch history buffer.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Testing with different types of branches in the two program instances revealed that static jumps, taken conditional jumps, calls and returns influence the branch history buffer the same way; non-taken conditional jumps don't influence it; the address of the last byte of the source instruction is the one that counts; IRETQ doesn't influence the history buffer state (which is useful for testing because it permits creating program flow that is invisible to the history buffer).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Moving the last conditional branch before the indirect call around in memory multiple times revealed that the branch history buffer contents can be used to distinguish many different locations of that last conditional branch instruction. This suggests that the history buffer doesn't store a list of small history values; instead, it seems to be a larger buffer in which history data is mixed together.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;However, a history buffer needs to &quot;forget&quot; about past branches after a certain number of new branches have been taken in order to be useful for branch prediction. Therefore, when new data is mixed into the history buffer, this can not cause information in bits that are already present in the history buffer to propagate downwards - and given that, upwards combination of information probably wouldn't be very useful either. Given that branch prediction also must be very fast, we concluded that it is likely that the update function of the history buffer left-shifts the old history buffer, then XORs in the new state (see diagram).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-crbBTenupqw/Wk066CBQ2qI/AAAAAAAACPw/GaUAa5tUDE8dV9tZ6bt-QWGLY5klZM5FgCEwYBhgL/s1600/image6.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;117&quot; data-original-width=&quot;624&quot; height=&quot;118&quot; src=&quot;https://1.bp.blogspot.com/-crbBTenupqw/Wk066CBQ2qI/AAAAAAAACPw/GaUAa5tUDE8dV9tZ6bt-QWGLY5klZM5FgCEwYBhgL/s640/image6.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;If this assumption is correct, then the history buffer contains a lot of information about the most recent branches, but only contains as many bits of information as are shifted per history buffer update about the last branch about which it contains any data. Therefore, we tested whether flipping different bits in the source and target addresses of a jump followed by 32 always-taken jumps with static source and target allows the branch prediction to disambiguate an indirect call. [11]&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;With 32 static jumps in between, no bit flips seemed to have an influence, so we decreased the number of static jumps until a difference was observable. The result with 28 always-taken jumps in between was that bits 0x1 and 0x2 of the target and bits 0x40 and 0x80 of the source had such an influence; but flipping both 0x1 in the target and 0x40 in the source or 0x2 in the target and 0x80 in the source did not permit disambiguation. This shows that the per-insertion shift of the history buffer is 2 bits and shows which data is stored in the least significant bits of the history buffer. We then repeated this with decreased amounts of fixed jumps after the bit-flipped jump to determine which information is stored in the remaining bits.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Reading host memory from a KVM guest&lt;/span&gt;&lt;/h2&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Locating the host kernel&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Our PoC locates the host kernel in several steps. The information that is determined and necessary for the next steps of the attack consists of:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;lower 20 bits of the address of kvm-intel.ko&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;full address of kvm.ko&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;full address of vmlinux&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Looking back, this is unnecessarily complicated, but it nicely demonstrates the various techniques an attacker can use. A simpler way would be to first determine the address of vmlinux, then bisect the addresses of kvm.ko and kvm-intel.ko.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In the first step, the address of kvm-intel.ko is leaked. For this purpose, the branch history buffer state after guest entry is dumped out. Then, for every possible value of bits 12..19 of the load address of kvm-intel.ko, the expected lowest 16 bits of the history buffer are computed based on the load address guess and the known offsets of the last 8 branches before guest entry, and the results are compared against the lowest 16 bits of the leaked history buffer state.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The branch history buffer state is leaked in steps of 2 bits by measuring misprediction rates of an indirect call with two targets. One way the indirect call is reached is from a vmcall instruction followed by a series of N branches whose relevant source and target address bits are all zeroes. The second way the indirect call is reached is from a series of controlled branches in userspace that can be used to write arbitrary values into the branch history buffer.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Misprediction rates are measured as in the section &quot;Reverse-Engineering Branch Predictor Internals&quot;, using one call target that loads a cache line and another one that checks whether the same cache line has been loaded.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-juuq6gYvhn4/Wk065XER9yI/AAAAAAAACPo/EuTi1HzweloKCChZajY7FUDEtHXhs12ZwCEwYBhgL/s1600/image4.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;462&quot; data-original-width=&quot;451&quot; height=&quot;640&quot; src=&quot;https://3.bp.blogspot.com/-juuq6gYvhn4/Wk065XER9yI/AAAAAAAACPo/EuTi1HzweloKCChZajY7FUDEtHXhs12ZwCEwYBhgL/s640/image4.png&quot; width=&quot;624&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;With N=29, mispredictions will occur at a high rate if the controlled branch history buffer value is zero because all history buffer state from the hypercall has been erased. With N=28, mispredictions will occur if the controlled branch history buffer value is one of 0&amp;lt;&amp;lt;(28*2), 1&amp;lt;&amp;lt;(28*2), 2&amp;lt;&amp;lt;(28*2), 3&amp;lt;&amp;lt;(28*2) - by testing all four possibilities, it can be detected which one is right. Then, for decreasing values of N, the four possibilities are {0|1|2|3}&amp;lt;&amp;lt;(28*2) | (history_buffer_for(N+1) &amp;gt;&amp;gt; 2). By repeating this for decreasing values for N, the branch history buffer value for N=0 can be determined.&lt;/span&gt;&lt;/div&gt;

&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-uAWgXXpjs8I/Wk064FS-g3I/AAAAAAAACPc/0nh9nbSn0EkTXgzbbuY1TRROTR02GWIFQCEwYBhgL/s1600/image1.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;315&quot; data-original-width=&quot;457&quot; height=&quot;440&quot; src=&quot;https://4.bp.blogspot.com/-uAWgXXpjs8I/Wk064FS-g3I/AAAAAAAACPc/0nh9nbSn0EkTXgzbbuY1TRROTR02GWIFQCEwYBhgL/s640/image1.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;At this point, the low 20 bits of kvm-intel.ko are known; the next step is to roughly locate kvm.ko.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;For this, the generic branch predictor is used, using data inserted into the BTB by an indirect call from kvm.ko to kvm-intel.ko that happens on every hypercall; this means that the source address of the indirect call has to be leaked out of the BTB.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;kvm.ko will probably be located somewhere in the range from&lt;/span&gt; &lt;span&gt;0xffffffffc0000000&lt;/span&gt; &lt;span&gt;to&lt;/span&gt; &lt;span&gt;0xffffffffc4000000&lt;/span&gt;&lt;span&gt;, with page alignment (0x1000). This means that the first four entries in the table in the section &quot;Generic Predictor&quot; apply; there will be 2&lt;/span&gt;&lt;span&gt;4&lt;/span&gt;&lt;span&gt;-1=15 aliasing addresses for the correct one. But that is also an advantage: It cuts down the search space from 0x4000 to 0x4000/2&lt;/span&gt;&lt;span&gt;4&lt;/span&gt;&lt;span&gt;=1024.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To find the right address for the source or one of its aliasing addresses, code that loads data through a specific register is placed at all possible call targets (the leaked low 20 bits of kvm-intel.ko plus the in-module offset of the call target plus a multiple of 2&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;&lt;span&gt;) and indirect calls are placed at all possible call sources. Then, alternatingly, hypercalls are performed and indirect calls are performed through the different possible non-aliasing call sources, with randomized history buffer state that prevents the specialized prediction from working. After this step, there are 2&lt;/span&gt;&lt;span&gt;16&lt;/span&gt; &lt;span&gt;remaining possibilities for the load address of kvm.ko.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Next, the load address of vmlinux can be determined in a similar way, using an indirect call from vmlinux to kvm.ko. Luckily, none of the bits which are randomized in the load address of vmlinux  are folded together, so unlike when locating kvm.ko, the result will directly be unique. vmlinux has an alignment of 2MiB and a randomization range of 1GiB, so there are still only 512 possible addresses.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Because (as far as we know) a simple hypercall won't actually cause indirect calls from vmlinux to kvm.ko, we instead use port I/O from the status register of an emulated serial port, which is present in the default configuration of a virtual machine created with virt-manager.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The only remaining piece of information is which one of the 16 aliasing load addresses of kvm.ko is actually correct. Because the source address of an indirect call to kvm.ko is known, this can be solved using bisection: Place code at the various possible targets that, depending on which instance of the code is speculatively executed, loads one of two cache lines, and measure which one of the cache lines gets loaded.&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Identifying cache sets&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The PoC assumes that the VM does not have access to hugepages.To discover eviction sets for all L3 cache sets with a specific alignment relative to a 4KiB page boundary, the PoC first allocates 25600 pages of memory. Then, in a loop, it selects random subsets of all remaining unsorted pages such that the expected number of sets for which an eviction set is contained in the subset is 1, reduces each subset down to an eviction set by repeatedly accessing its cache lines and testing whether the cache lines are always cached (in which case they're probably not part of an eviction set) and attempts to use the new eviction set to evict all remaining unsorted cache lines to determine whether they are in the same cache set [12].&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Locating the host-virtual address of a guest page&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Because this attack uses a FLUSH+RELOAD approach for leaking data, it needs to know the host-kernel-virtual address of one guest page. Alternative approaches such as PRIME+PROBE should work without that requirement.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The basic idea for this step of the attack is to use a branch target injection attack against the hypervisor to load an attacker-controlled address and test whether that caused the guest-owned page to be loaded. For this, a gadget that simply loads from the memory location specified by R8 can be used - R8-R11 still contain guest-controlled values when the first indirect call after a guest exit is reached on this kernel build.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We expected that an attacker would need to either know which eviction set has to be used at this point or brute-force it simultaneously; however, experimentally, using random eviction sets works, too. Our theory is that the observed behavior is actually the result of L1D and L2 evictions, which might be sufficient to permit a few instructions worth of speculative execution.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The host kernel maps (nearly?) all physical memory in the physmap area, including memory assigned to KVM guests. However, the location of the physmap is randomized (with a 1GiB alignment), in an area of size 128PiB. Therefore, directly bruteforcing the host-virtual address of a guest page would take a long time. It is not necessarily impossible; as a ballpark estimate, it should be possible within a day or so, maybe less, assuming 12000 successful injections per second and 30 guest pages that are tested in parallel; but not as impressive as doing it in a few minutes.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To optimize this, the problem can be split up: First, brute-force the physical address using a gadget that can load from physical addresses, then brute-force the base address of the physmap region. Because the physical address can usually be assumed to be far below 128PiB, it can be brute-forced more efficiently, and brute-forcing the base address of the physmap region afterwards is also easier because then address guesses with 1GiB alignment can be used.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To brute-force the physical address, the following gadget can be used:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9def:       4c 89 c0                mov    rax,r8&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9df2:       4d 63 f9                movsxd r15,r9d&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9df5:       4e 8b 04 fd c0 b3 a6    mov    r8,QWORD PTR [r15*8-0x7e594c40]&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9dfc:       81&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9dfd:       4a 8d 3c 00             lea    rdi,[rax+r8*1]&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9e01:       4d 8b a4 00 f8 00 00    mov    r12,QWORD PTR [r8+rax*1+0xf8]&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9e08:       00&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This gadget permits loading an 8-byte-aligned value from the area around the kernel text section by setting R9 appropriately, which in particular permits loading&lt;/span&gt; &lt;span&gt;page_offset_base&lt;/span&gt;&lt;span&gt;, the start address of the physmap. Then, the value that was originally in R8 - the physical address guess minus 0xf8 - is added to the result of the previous load, 0xfa is added to it, and the result is dereferenced.&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Cache set selection&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To select the correct L3 eviction set, the attack from the following section is essentially executed with different eviction sets until it works.&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Leaking data&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;At this point, it would normally be necessary to locate gadgets in the host kernel code that can be used to actually leak data by reading from an attacker-controlled location, shifting and masking the result appropriately and then using the result of that as offset to an attacker-controlled address for a load. But piecing gadgets together and figuring out which ones work in a speculation context seems annoying. So instead, we decided to use the eBPF interpreter, which is built into the host kernel - while there is no legitimate way to invoke it from inside a VM, the presence of the code in the host kernel's text section is sufficient to make it usable for the attack, just like with ordinary ROP gadgets.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The eBPF interpreter entry point has the following function signature:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;static unsigned int __bpf_prog_run(void *ctx, const struct bpf_insn *insn)&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The second parameter is a pointer to an array of statically pre-verified eBPF instructions to be executed - which means that&lt;/span&gt; &lt;span&gt;__bpf_prog_run()&lt;/span&gt; &lt;span&gt;will not perform any type checks or bounds checks. The first parameter is simply stored as part of the initial emulated register state, so its value doesn't matter.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The eBPF interpreter provides, among other things:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;multiple emulated 64-bit registers&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;64-bit immediate writes to emulated registers&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;memory reads from addresses stored in emulated registers&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;bitwise operations (including bit shifts) and arithmetic operations&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To call the interpreter entry point, a gadget that gives RSI and RIP control given R8-R11 control and controlled data at a known memory location is necessary. The following gadget provides this functionality:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff81514edd:       4c 89 ce                mov    rsi,r9&lt;/span&gt;&lt;span&gt;&lt;br class=&quot;kix-line-break&quot;/&gt;&lt;/span&gt;&lt;span&gt;ffffffff81514ee0:       41 ff 90 b0 00 00 00    call   QWORD PTR [r8+0xb0]&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Now, by pointing R8 and R9 at the mapping of a guest-owned page in the physmap, it is possible to speculatively execute arbitrary unvalidated eBPF bytecode in the host kernel. Then, relatively straightforward bytecode can be used to leak data into the cache.&lt;/span&gt;&lt;/div&gt;


&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In summary, an attack using this variant of the issue attempts to read kernel memory from userspace without misdirecting the control flow of kernel code. This works by using the code pattern that was used for the previous variants, but in userspace. The underlying idea is that the permission check for accessing an address might not be on the critical path for reading data from memory to a register, where the permission check could have significant performance impact. Instead, the memory read could make the result of the read available to following instructions immediately and only perform the permission check asynchronously, setting a flag in the reorder buffer that causes an exception to be raised if the permission check fails.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We do have a few additions to make to Anders Fogh's blogpost:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Imagine the following instruction executed in usermode&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;mov rax,[somekernelmodeaddress]&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It will cause an interrupt when retired, [...]&quot;&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It is also possible to already execute that instruction behind a high-latency mispredicted branch to avoid taking a page fault. This might also widen the speculation window by increasing the delay between the read from a kernel address and delivery of the associated exception.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;First, I call a syscall that touches this memory. Second, I use the prefetcht0 instruction to improve my odds of having the address loaded in L1.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;When we used prefetch instructions after doing a syscall, the attack stopped working for us, and we have no clue why. Perhaps the CPU somehow stores whether access was denied on the last access and prevents the attack from working if that is the case?&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Fortunately I did not get a slow read suggesting that Intel null’s the result when the access is not allowed.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;That (read from kernel address returns all-zeroes) seems to happen for memory that is not sufficiently cached but for which pagetable entries are present, at least after repeated read attempts. For unmapped memory, the kernel address read does not return a result at all.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We believe that our research provides many remaining research topics that we have not yet investigated, and we encourage other public researchers to look into these.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section contains an even higher amount of speculation than the rest of this blogpost - it contains untested ideas that might well be useless.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Leaking without data cache timing&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It would be interesting to explore whether there are microarchitectural attacks other than measuring data cache timing that can be used for exfiltrating data out of speculative execution.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Other microarchitectures&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Our research was relatively Haswell-centric so far. It would be interesting to see details e.g. on how the branch prediction of other modern processors works and how well it can be attacked.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Other JIT engines&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We developed a successful variant 1 attack against the JIT engine built into the Linux kernel. It would be interesting to see whether attacks against more advanced JIT engines with less control over the system are also practical - in particular, JavaScript engines.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;More efficient scanning for host-virtual addresses and cache sets&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In variant 2, while scanning for the host-virtual address of a guest-owned page, it might make sense to attempt to determine its L3 cache set first. This could be done by performing L3 evictions using an eviction pattern through the physmap, then testing whether the eviction affected the guest-owned page.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The same might work for cache sets - use an L1D+L2 eviction set to evict the function pointer in the host kernel context, use a gadget in the kernel to evict an L3 set using physical addresses, then use that to identify which cache sets guest lines belong to until a guest-owned eviction set has been constructed.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Dumping the complete BTB state&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Given that the generic BTB seems to only be able to distinguish 2&lt;/span&gt;&lt;span&gt;31-8&lt;/span&gt; &lt;span&gt;or fewer source addresses, it seems feasible to dump out the complete BTB state generated by e.g. a hypercall in a timeframe around the order of a few hours. (Scan for jump sources, then for every discovered jump source, bisect the jump target.) This could potentially be used to identify the locations of functions in the host kernel even if the host kernel is custom-built.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The source address aliasing would reduce the usefulness somewhat, but because target addresses don't suffer from that, it might be possible to correlate (source,target) pairs from machines with different KASLR offsets and reduce the number of candidate addresses based on KASLR being additive while aliasing is bitwise.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This could then potentially allow an attacker to make guesses about the host kernel version or the compiler used to build it based on jump offsets or distances between functions.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Variant 2: Leaking with more efficient gadgets&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;If sufficiently efficient gadgets are used for variant 2, it might not be necessary to evict host kernel function pointers from the L3 cache at all; it might be sufficient to only evict them from L1D and L2.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Various speedups&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In particular the variant 2 PoC is still a bit slow. This is probably partly because:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It only leaks one bit at a time; leaking more bits at a time should be doable.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It heavily uses IRETQ for hiding control flow from the processor.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It would be interesting to see what data leak rate can be achieved using variant 2.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Leaking or injection through the return predictor&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;If the return predictor also doesn't lose its state on a privilege level change, it might be useful for either locating the host kernel from inside a VM (in which case bisection could be used to very quickly discover the full address of the host kernel) or injecting return targets (in particular if the return address is stored in a cache line that can be flushed out by the attacker and isn't reloaded before the return instruction).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;However, we have not performed any experiments with the return predictor that yielded conclusive results so far.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Leaking data out of the indirect call predictor&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We have attempted to leak target information out of the indirect call predictor, but haven't been able to make it work.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The following statement were provided to us regarding this issue from the vendors to whom Project Zero disclosed this vulnerability:&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;No current statement provided at this time.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;AMD&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;No current statement provided at this time.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;ARM&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Arm recognises that the speculation functionality of many modern high-performance processors, despite working as intended, can be used in conjunction with the timing of cache operations to leak some information as described in this blog. Correspondingly, Arm has developed software mitigations that we recommend be deployed.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Arm has included a detailed technical whitepaper as well as links to information from some of Arm’s architecture partners regarding their specific implementations and mitigations.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Note that some of these documents - in particular Intel's documentation - change over time, so quotes from and references to it may not reflect the latest version of Intel's documentation.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Placing data immediately following an indirect branch can cause a performance problem. If the data consists of all zeros, it looks like a long stream of ADDs to memory destinations and this can cause resource conflicts and slow down branch recovery. Also, data immediately following indirect branches may appear as branches to the branch predication [sic] hardware, which can branch off to execute other data pages. This can lead to subsequent self-modifying code problems.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Loads can:[...]Be carried out speculatively, before preceding branches are resolved.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Software should avoid writing to a code page in the same 1-KByte subpage that is being executed or fetching code in the same 2-KByte subpage of that is being written. In addition, sharing a page containing directly or speculatively executed code with another processor as a data page can trigger an SMC condition that causes the entire pipeline of the machine and the trace cache to be cleared. This is due to the self-modifying code condition.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;if mapped as WB or WT, there is a potential for speculative processor reads to bring the data into the caches&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Failure to map the region as WC may allow the line to be speculatively read into the processor caches (via the wrong path of a mispredicted branch).&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1507.06955.pdf&quot;&gt;&lt;span&gt;https://arxiv.org/pdf/1507.06955.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;: The rowhammer.js research by Daniel Gruss, Clémentine Maurice and Stefan Mangard contains information about L3 cache eviction patterns that we reused in the KVM PoC to evict a function pointer.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://www.sophia.re/thesis.pdf&quot;&gt;&lt;span&gt;https://www.sophia.re/thesis.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;: Sophia D'Antoine wrote a thesis that shows that opcode scheduling can theoretically be used to transmit data between hyperthreads.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://gruss.cc/files/kaiser.pdf&quot;&gt;&lt;span&gt;https://gruss.cc/files/kaiser.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;: Daniel Gruss, Moritz Lipp, Michael Schwarz, Richard Fellner, Clémentine Maurice, and Stefan Mangard wrote a paper on mitigating microarchitectural issues caused by pagetable sharing between userspace and the kernel.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;&lt;span&gt;[2]&lt;/span&gt; &lt;span&gt;The precise model names are listed in the section &quot;Tested Processors&quot;. The code for reproducing this is in the writeup_files.tar archive in our bugtracker, in the folders userland_test_x86 and userland_test_aarch64.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;[3]&lt;/span&gt; &lt;span&gt;The attacker-controlled offset used to perform an out-of-bounds access on an array by this PoC is a 32-bit value, limiting the accessible addresses to a 4GiB window in the kernel heap area.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;[4]&lt;/span&gt; &lt;span&gt;This PoC won't work on CPUs with SMAP support; however, that is not a fundamental limitation.&lt;/span&gt;&lt;/div&gt;

&lt;div&gt;&lt;span&gt;[6]&lt;/span&gt; &lt;span&gt;The phone was running an Android build from May 2017.&lt;/span&gt;&lt;/div&gt;


&lt;div&gt;&lt;span&gt;[9]&lt;/span&gt; &lt;span&gt;More than 2&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;mappings would be more efficient, but the kernel places a hard cap of 2&lt;/span&gt;&lt;span&gt;16&lt;/span&gt; &lt;span&gt;on the number of VMAs that a process can have.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;[10]&lt;/span&gt; &lt;span&gt;Intel's optimization manual states that &quot;In the first implementation of HT Technology, the physical execution resources are shared and the architecture state is duplicated for each logical processor&quot;, so it would be plausible for predictor state to be shared. While predictor state could be tagged by logical core, that would likely reduce performance for multithreaded processes, so it doesn't seem likely.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;[11]&lt;/span&gt; &lt;span&gt;In case the history buffer was a bit bigger than we had measured, we added some margin - in particular because we had seen slightly different history buffer lengths in different experiments, and because 26 isn't a very round number.&lt;/span&gt;&lt;/div&gt;



</description>
<pubDate>Wed, 03 Jan 2018 22:28:18 +0000</pubDate>
<dc:creator>ccurrens</dc:creator>
<og:url>https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html</og:url>
<og:title>Reading privileged memory with a side-channel</og:title>
<og:description>Posted by Jann Horn, Project Zero We have discovered that CPU data cache timing can be abused to efficiently leak information out of mi...</og:description>
<og:image>https://3.bp.blogspot.com/-64YRnkgMujY/Wk064WtrSRI/AAAAAAAACPg/O4ZxlUiWerACL61P490xrVMWSmHTwQflQCEwYBhgL/w1200-h630-p-k-no-nu/image3.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html</dc:identifier>
</item>
<item>
<title>Reading privileged memory with a side-channel</title>
<link>https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html</link>
<guid isPermaLink="true" >https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html</guid>
<description>&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Posted by Jann Horn, Project Zero&lt;/span&gt;&lt;/div&gt;
&lt;strong id=&quot;docs-internal-guid-c66856d3-bdac-1e12-91fc-544573d27dc6&quot;&gt;&lt;br/&gt;&lt;/strong&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We have discovered that CPU data cache timing can be abused to efficiently leak information out of mis-speculated execution, leading to (at worst) arbitrary virtual memory read vulnerabilities across local security boundaries in various contexts.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Variants of this issue are known to affect many modern processors, including certain processors by Intel, AMD and ARM. For a few Intel and AMD CPU models, we have exploits that work against real software. We reported this issue to Intel, AMD and ARM on 2017-06-01 [1]&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;So far, there are three known variants of the issue:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Variant 1: bounds check bypass (&lt;/span&gt;&lt;span&gt;CVE-2017-5753)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Variant 2: branch target injection (&lt;/span&gt;&lt;span&gt;CVE-2017-5715)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Variant 3: rogue data cache load (&lt;/span&gt;&lt;span&gt;CVE-2017-5754)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Before the issues described here were publicly disclosed, Daniel Gruss, Moritz Lipp, Yuval Yarom, Paul Kocher, Daniel Genkin, Michael Schwarz, Mike Hamburg, Stefan Mangard, Thomas Prescher and Werner Haas also reported them; their [writeups/blogposts/paper drafts] are at:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;During the course of our research, we developed the following proofs of concept (PoCs):&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ol&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A PoC that demonstrates the basic principles behind variant 1 in userspace on the tested Intel Haswell Xeon CPU, the AMD FX CPU, the AMD PRO CPU and an ARM Cortex A57 [2].&lt;/span&gt; &lt;span&gt;This PoC only tests for the ability to read data inside mis-speculated execution within the same process, without crossing any privilege boundaries.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A PoC for variant 1 that, when running with normal user privileges under a modern Linux kernel with a distro-standard config, can perform arbitrary reads in a 4GiB range [3]&lt;/span&gt; &lt;span&gt;in kernel virtual memory on the Intel Haswell Xeon CPU. If the kernel's BPF JIT is enabled (non-default configuration), it also works on the AMD PRO CPU. On the Intel Haswell Xeon CPU, kernel virtual memory can be read at a rate of around 2000 bytes per second after around 4 seconds of startup time. [4]&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A PoC for variant 2 that, when running with root privileges inside a KVM guest created using virt-manager on the Intel Haswell Xeon CPU, with a specific (now outdated) version of Debian's distro kernel&lt;/span&gt; &lt;span&gt;[5] running on the host, can read host kernel memory at a rate of around 1500 bytes/second, with room for optimization. Before the attack can be performed, some initialization has to be performed that takes roughly between 10 and 30 minutes for a machine with 64GiB of RAM; the needed time should scale roughly linearly with the amount of host RAM. (If 2MB hugepages are available to the guest, the initialization should be much faster, but that hasn't been tested.)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A PoC for variant 3 that, when running with normal user privileges, can read kernel memory on the Intel Haswell Xeon CPU under some precondition. We believe that this precondition is that the targeted kernel memory is present in the L1D cache.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;For interesting resources around this topic, look down into the &quot;Literature&quot; section.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A warning regarding explanations about processor internals in this blogpost: This blogpost contains a lot of speculation about hardware internals based on observed behavior, which might not necessarily correspond to what processors are actually doing.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We have some ideas on possible mitigations and provided some of those ideas to the processor vendors; however, we believe that the processor vendors are in a much better position than we are to design and evaluate mitigations, and we expect them to be the source of authoritative guidance.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The PoC code and the writeups that we sent to the CPU vendors will be made available at a later date.&lt;/span&gt;&lt;/div&gt;

&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz (called &quot;Intel Haswell Xeon CPU&quot; in the rest of this document)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;AMD FX(tm)-8320 Eight-Core Processor (called &quot;AMD FX CPU&quot; in the rest of this document)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;AMD PRO A8-9600 R7, 10 COMPUTE CORES 4C+6G (called &quot;AMD PRO CPU&quot; in the rest of this document)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;An ARM Cortex A57 core of a Google Nexus 5x phone [6]&lt;/span&gt; &lt;span&gt;(called &quot;ARM Cortex A57&quot; in the rest of this document)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;retire:&lt;/span&gt; &lt;span&gt;An instruction retires when its results, e.g. register writes and memory writes, are committed and made visible to the rest of the system. Instructions can be executed out of order, but must always retire in order.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;logical processor core:&lt;/span&gt; &lt;span&gt;A logical processor core is what the operating system sees as a processor core. With hyperthreading enabled, the number of logical cores is a multiple of the number of physical cores.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;cached/uncached data:&lt;/span&gt; &lt;span&gt;In this blogpost, &quot;uncached&quot; data is data that is only present in main memory, not in any of the cache levels of the CPU. Loading uncached data will typically take over 100 cycles of CPU time.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;speculative execution:&lt;/span&gt; &lt;span&gt;A processor can execute past a branch without knowing whether it will be taken or where its target is, therefore executing instructions before it is known whether they should be executed. If this speculation turns out to have been incorrect, the CPU can discard the resulting state without architectural effects and continue execution on the correct execution path. Instructions do not retire before it is known that they are on the correct execution path.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;mis-speculation window:&lt;/span&gt; &lt;span&gt;The time window during which the CPU speculatively executes the wrong code and has not yet detected that mis-speculation has occurred.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section explains the common theory behind all three variants and the theory behind our PoC for variant 1 that, when running in userspace under a Debian distro kernel, can perform arbitrary reads in a 4GiB region of kernel memory in at least the following configurations:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel Haswell Xeon CPU, eBPF JIT is off (default state)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel Haswell Xeon CPU, eBPF JIT is on (non-default state)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;AMD PRO CPU, eBPF JIT is on (non-default state)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The state of the eBPF JIT can be toggled using the net.core.bpf_jit_enable sysctl.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Theoretical explanation&lt;/span&gt;&lt;/h2&gt;

&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Branch prediction predicts the branch target and enables the&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;processor to begin executing instructions long before the branch&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;true execution path is known.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In section 2.3.5.2 (&quot;L1 DCache&quot;):&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Loads can:&lt;/span&gt;&lt;/div&gt;

&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Be carried out speculatively, before preceding branches are resolved.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Take cache misses out of order and in an overlapped manner.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel's Software Developer's Manual&lt;/span&gt; &lt;span&gt;[7] states in Volume 3A, section 11.7 (&quot;Implicit Caching (Pentium 4, Intel Xeon, and P6 family processors&quot;):&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Implicit caching occurs when a memory element is made potentially cacheable, although the element may never have been accessed in the normal von Neumann sequence. Implicit caching occurs on the P6 and more recent processor families due to aggressive prefetching, branch prediction, and TLB miss handling. Implicit caching is an extension of the behavior of existing Intel386, Intel486, and Pentium processor systems, since software running on these processor families also has not been able to deterministically predict the behavior of instruction prefetch.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Consider the code sample below. If&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt; &lt;span&gt;is uncached, the processor can speculatively load data from&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[untrusted_offset_from_caller]&lt;/span&gt;&lt;span&gt;. This is an out-of-bounds read. That should not matter because the processor will effectively roll back the execution state when the branch has executed; none of the speculatively executed instructions will retire (e.g. cause registers etc. to be affected).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned long length;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned char data[];&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array *arr1 = ...;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;unsigned long untrusted_offset_from_caller = ...;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;if (untrusted_offset_from_caller &amp;lt;&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt;&lt;span&gt;) {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned char value =&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[untrusted_offset_from_caller]&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;&lt;/div&gt;


&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;However, in the following code sample, there's an issue. If&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[0x200]&lt;/span&gt; &lt;span&gt;and&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[0x300]&lt;/span&gt; &lt;span&gt;are not cached, but all other accessed data is, and the branch conditions are predicted as true, the processor can do the following speculatively before&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt; &lt;span&gt;has been loaded and the execution is re-steered:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;load&lt;/span&gt; &lt;span&gt;value =&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[&lt;/span&gt;&lt;span&gt;untrusted_offset_from_caller&lt;/span&gt;&lt;span&gt;]&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;start a load from a data-dependent offset in&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data&lt;/span&gt;&lt;span&gt;, loading the corresponding cache line into the L1 cache&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned long length;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned char data[];&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array *arr1 = ...; /* small array */&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array *arr2 = ...; /* array of size 0x400 */&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;/* &amp;gt;0x400&lt;/span&gt; &lt;span&gt;(OUT OF BOUNDS!)&lt;/span&gt; &lt;span&gt;*/&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;unsigned long untrusted_offset_from_caller = ...;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;if (&lt;/span&gt;&lt;span&gt;untrusted_offset_from_caller&lt;/span&gt; &lt;span&gt;&amp;lt;&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt;&lt;span&gt;) {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned char value =&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[&lt;/span&gt;&lt;span&gt;untrusted_offset_from_caller&lt;/span&gt;&lt;span&gt;]&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned long index2 = ((value&amp;amp;1)*0x100)+0x200;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; if (index2 &amp;lt; arr2-&amp;gt;length) {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;   unsigned char value2 =&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[index2]&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;&lt;/div&gt;


&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;After the execution has been returned to the non-speculative path because the processor has noticed that&lt;/span&gt; &lt;span&gt;untrusted_offset_from_caller&lt;/span&gt; &lt;span&gt;is bigger than&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt;&lt;span&gt;, the cache line containing&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[index2]&lt;/span&gt; &lt;span&gt;stays in the L1 cache. By measuring the time required to load&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[0x200]&lt;/span&gt; &lt;span&gt;and&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[0x300]&lt;/span&gt;&lt;span&gt;, an attacker can then determine whether the value of&lt;/span&gt; &lt;span&gt;index2&lt;/span&gt; &lt;span&gt;during speculative execution was 0x200 or 0x300 - which discloses whether&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[&lt;/span&gt;&lt;span&gt;untrusted_offset_from_caller&lt;/span&gt;&lt;span&gt;]&lt;/span&gt;&lt;span&gt;&amp;amp;1&lt;/span&gt; &lt;span&gt;is 0 or 1.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To be able to actually use this behavior for an attack, an attacker needs to be able to cause the execution of such a vulnerable code pattern in the targeted context with an out-of-bounds index. For this, the vulnerable code pattern must either be present in existing code, or there must be an interpreter or JIT engine that can be used to generate the vulnerable code pattern. So far, we have not actually identified any existing, exploitable instances of the vulnerable code pattern; the PoC for leaking kernel memory using variant 1 uses the eBPF interpreter or the eBPF JIT engine, which are built into the kernel and accessible to normal users.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A minor variant of this could be to instead use an out-of-bounds read to a function pointer to gain control of execution in the mis-speculated path. We did not investigate this variant further.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Attacking the kernel&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section describes in more detail how variant 1 can be used to leak Linux kernel memory using the eBPF bytecode interpreter and JIT engine. While there are many interesting potential targets for variant 1 attacks, we chose to attack the Linux in-kernel eBPF JIT/interpreter because it provides more control to the attacker than most other JITs.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The Linux kernel supports eBPF since version 3.18. Unprivileged userspace code can supply bytecode to the kernel that is verified by the kernel and then:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;either interpreted by an in-kernel bytecode interpreter&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;or translated to native machine code that also runs in kernel context using a JIT engine (which translates individual bytecode instructions without performing any further optimizations)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Execution of the bytecode can be triggered by attaching the eBPF bytecode to a socket as a filter and then sending data through the other end of the socket.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Whether the JIT engine is enabled depends on a run-time configuration setting - but at least on the tested Intel processor, the attack works independent of that setting.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Unlike classic BPF, eBPF has data types like data arrays and function pointer arrays into which eBPF bytecode can index. Therefore, it is possible to create the code pattern described above in the kernel using eBPF bytecode.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;eBPF's data arrays are less efficient than its function pointer arrays, so the attack will use the latter where possible.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Both machines on which this was tested have no SMAP, and the PoC relies on that (but it shouldn't be a precondition in principle).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Additionally, at least on the Intel machine on which this was tested, bouncing modified cache lines between cores is slow, apparently because the MESI protocol is used for cache coherence [8]&lt;/span&gt;&lt;span&gt;. Changing the reference counter of an eBPF array on one physical CPU core causes the cache line containing the reference counter to be bounced over to that CPU core, making reads of the reference counter on all other CPU cores slow until the changed reference counter has been written back to memory. Because the length and the reference counter of an eBPF array are stored in the same cache line, this also means that changing the reference counter on one physical CPU core causes reads of the eBPF array's length to be slow on other physical CPU cores (intentional false sharing).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The attack uses two eBPF programs. The first one tail-calls through a page-aligned eBPF function pointer array&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;at a configurable index. In simplified terms, this program is used to determine the address of&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;by guessing the offset from&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;to a userspace address and tail-calling through&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;at the guessed offsets. To cause the branch prediction to predict that the offset is below the length of&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt;&lt;span&gt;, tail calls to an in-bounds index are performed in between. To increase the mis-speculation window, the cache line containing the length of&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;is bounced to another core. To test whether an offset guess was successful, it can be tested whether the userspace address has been loaded into the cache.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Because such straightforward brute-force guessing of the address would be slow, the following optimization is used: 2&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;adjacent userspace memory mappings [9]&lt;/span&gt;&lt;span&gt;, each consisting of 2&lt;/span&gt;&lt;span&gt;4&lt;/span&gt; &lt;span&gt;pages, are created at the userspace address&lt;/span&gt; &lt;span&gt;user_mapping_area&lt;/span&gt;&lt;span&gt;, covering a total area of 2&lt;/span&gt;&lt;span&gt;31&lt;/span&gt; &lt;span&gt;bytes. Each mapping maps the same physical pages, and all mappings are present in the pagetables.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-64YRnkgMujY/Wk064WtrSRI/AAAAAAAACPg/O4ZxlUiWerACL61P490xrVMWSmHTwQflQCEwYBhgL/s1600/image3.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;138&quot; data-original-width=&quot;624&quot; height=&quot;139&quot; src=&quot;https://3.bp.blogspot.com/-64YRnkgMujY/Wk064WtrSRI/AAAAAAAACPg/O4ZxlUiWerACL61P490xrVMWSmHTwQflQCEwYBhgL/s640/image3.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This permits the attack to be carried out in steps of 2&lt;/span&gt;&lt;span&gt;31&lt;/span&gt; &lt;span&gt;bytes. For each step, after causing an out-of-bounds access through&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt;&lt;span&gt;, only one cache line each from the first 2&lt;/span&gt;&lt;span&gt;4&lt;/span&gt; &lt;span&gt;pages of&lt;/span&gt; &lt;span&gt;user_mapping_area&lt;/span&gt; &lt;span&gt;have to be tested for cached memory. Because the L3 cache is physically indexed, any access to a virtual address mapping a physical page will cause all other virtual addresses mapping the same physical page to become cached as well.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;When this attack finds a hit—a cached memory location—the upper 33 bits of the kernel address are known (because they can be derived from the address guess at which the hit occurred), and the low 16 bits of the address are also known (from the offset inside&lt;/span&gt; &lt;span&gt;user_mapping_area&lt;/span&gt; &lt;span&gt;at which the hit was found). The remaining part of the address of&lt;/span&gt; &lt;span&gt;user_mapping_area&lt;/span&gt; &lt;span&gt;is the middle.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-kSdcfy3GVSw/Wk065qWZZaI/AAAAAAAACPs/7HRZEXij3rIqvAf4gq8QpyHKvJsT2pYIQCEwYBhgL/s1600/image5.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;149&quot; data-original-width=&quot;624&quot; height=&quot;152&quot; src=&quot;https://1.bp.blogspot.com/-kSdcfy3GVSw/Wk065qWZZaI/AAAAAAAACPs/7HRZEXij3rIqvAf4gq8QpyHKvJsT2pYIQCEwYBhgL/s640/image5.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The remaining bits in the middle can be determined by bisecting the remaining address space: Map two physical pages to adjacent ranges of virtual addresses, each virtual address range the size of half of the remaining search space, then determine the remaining address bit-wise.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;At this point, a second eBPF program can be used to actually leak data. In pseudocode, this program looks as follows:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t bitmask = &amp;lt;runtime-configurable&amp;gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t bitshift_selector = &amp;lt;runtime-configurable&amp;gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t prog_array_base_offset = &amp;lt;runtime-configurable&amp;gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t secret_data_offset = &amp;lt;runtime-configurable&amp;gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;// index will be bounds-checked by the runtime,&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;// but the bounds check will be bypassed speculatively&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t secret_data = bpf_map_read(array=victim_array, index=secret_data_offset);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;// select a single bit, move it to a specific position, and add the base offset&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t progmap_index = (((secret_data &amp;amp; bitmask) &amp;gt;&amp;gt; bitshift_selector) &amp;lt;&amp;lt; 7) + prog_array_base_offset;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;bpf_tail_call(prog_map, progmap_index);&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This program reads 8-byte-aligned 64-bit values from an eBPF data array &quot;&lt;/span&gt;&lt;span&gt;victim_map&lt;/span&gt;&lt;span&gt;&quot; at a runtime-configurable offset and bitmasks and bit-shifts the value so that one bit is mapped to one of two values that are 2&lt;/span&gt;&lt;span&gt;7&lt;/span&gt; &lt;span&gt;bytes apart (sufficient to not land in the same or adjacent cache lines when used as an array index). Finally it adds a 64-bit offset, then uses the resulting value as an offset into&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;for a tail call.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This program can then be used to leak memory by repeatedly calling the eBPF program with an out-of-bounds offset into&lt;/span&gt; &lt;span&gt;victim_map&lt;/span&gt; &lt;span&gt;that specifies the data to leak and an out-of-bounds offset into&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;that causes&lt;/span&gt; &lt;span&gt;prog_map + offset&lt;/span&gt; &lt;span&gt;to point to a userspace memory area. Misleading the branch prediction and bouncing the cache lines works the same way as for the first eBPF program, except that now, the cache line holding the length of&lt;/span&gt; &lt;span&gt;victim_map&lt;/span&gt; &lt;span&gt;must also be bounced to another core.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section describes the theory behind our PoC for variant 2 that, when running with root privileges inside a KVM guest created using virt-manager on the Intel Haswell Xeon CPU, with a specific version of Debian's distro kernel running on the host, can read host kernel memory at a rate of around 1500 bytes/second.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Basics&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Prior research (see the Literature section at the end) has shown that it is possible for code in separate security contexts to influence each other's branch prediction. So far, this has only been used to infer information about where code is located (in other words, to create interference from the victim to the attacker); however, the basic hypothesis of this attack variant is that it can also be used to redirect execution of code in the victim context (in other words, to create interference from the attacker to the victim; the other way around).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-O6-JHlkvq5U/Wk064bpm2qI/AAAAAAAACPk/rjpYr72vyF0jYASgT-w4NBFQXC3hgIZZwCEwYBhgL/s1600/image2.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;200&quot; data-original-width=&quot;278&quot; src=&quot;https://3.bp.blogspot.com/-O6-JHlkvq5U/Wk064bpm2qI/AAAAAAAACPk/rjpYr72vyF0jYASgT-w4NBFQXC3hgIZZwCEwYBhgL/s1600/image2.png&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The basic idea for the attack is to target victim code that contains an indirect branch whose target address is loaded from memory and flush the cache line containing the target address out to main memory. Then, when the CPU reaches the indirect branch, it won't know the true destination of the jump, and it won't be able to calculate the true destination until it has finished loading the cache line back into the CPU, which takes a few hundred cycles. Therefore, there is a time window of typically over 100 cycles in which the CPU will speculatively execute instructions based on branch prediction.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Haswell branch prediction internals&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Some of the internals of the branch prediction implemented by Intel's processors have already been published; however, getting this attack to work properly required significant further experimentation to determine additional details.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section focuses on the branch prediction internals that were experimentally derived from the Intel Haswell Xeon CPU.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Haswell seems to have multiple branch prediction mechanisms that work very differently:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A generic branch predictor that can only store one target per source address; used for all kinds of jumps, like absolute jumps, relative jumps and so on.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A specialized indirect call predictor that can store multiple targets per source address; used for indirect calls.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;(There is also a specialized return predictor, according to Intel's optimization manual, but we haven't analyzed that in detail yet. If this predictor could be used to reliably dump out some of the call stack through which a VM was entered, that would be very interesting.)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Generic predictor&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The generic branch predictor, as documented in prior research, only uses the lower 31 bits of the address of the last byte of the source instruction for its prediction. If, for example, a branch target buffer (BTB) entry exists for a jump from 0x4141.0004.1000 to 0x4141.0004.5123, the generic predictor will also use it to predict a jump from 0x4242.0004.1000. When the higher bits of the source address differ like this, the higher bits of the predicted destination change together with it—in this case, the predicted destination address will be 0x4242.0004.5123—so apparently this predictor doesn't store the full, absolute destination address.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Before the lower 31 bits of the source address are used to look up a BTB entry, they are folded together using XOR. Specifically, the following bits are folded together:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;
&lt;table&gt;&lt;colgroup&gt;&lt;col width=&quot;*&quot;/&gt;&lt;col width=&quot;*&quot;/&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x100.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x200.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x400.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x800.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x2000.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x4000.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In other words, if a source address is XORed with both numbers in a row of this table, the branch predictor will not be able to distinguish the resulting address from the original source address when performing a lookup. For example, the branch predictor is able to distinguish source addresses 0x100.0000 and 0x180.0000, and it can also distinguish source addresses 0x100.0000 and 0x180.8000, but it can't distinguish source addresses 0x100.0000 and 0x140.2000 or source addresses 0x100.0000 and 0x180.4000. In the following, this will be referred to as aliased source addresses.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;When an aliased source address is used, the branch predictor will still predict the same target as for the unaliased source address. This indicates that the branch predictor stores a truncated absolute destination address, but that hasn't been verified.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Based on observed maximum forward and backward jump distances for different source addresses, the low 32-bit half of the target address could be stored as an absolute 32-bit value with an additional bit that specifies whether the jump from source to target crosses a 2&lt;/span&gt;&lt;span&gt;32&lt;/span&gt; &lt;span&gt;boundary; if the jump crosses such a boundary, bit 31 of the source address determines whether the high half of the instruction pointer should increment or decrement.&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Indirect call predictor&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The inputs of the BTB lookup for this mechanism seem to be:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The low 12 bits of the address of the source instruction (we are not sure whether it's the address of the first or the last byte) or a subset of them.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The branch history buffer state.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;If the indirect call predictor can't resolve a branch, it is resolved by the generic predictor instead. Intel's optimization manual hints at this behavior: &quot;Indirect Calls and Jumps. These may either be predicted as having a monotonic target or as having targets that vary in accordance with recent program behavior.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The branch history buffer (BHB) stores information about the last 29 taken branches - basically a fingerprint of recent control flow - and is used to allow better prediction of indirect calls that can have multiple targets.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The update function of the BHB works as follows (in pseudocode;&lt;/span&gt; &lt;span&gt;src&lt;/span&gt; &lt;span&gt;is the address of the last byte of the source instruction,&lt;/span&gt; &lt;span&gt;dst&lt;/span&gt; &lt;span&gt;is the destination address):&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;void bhb_update(uint58_t *bhb_state, unsigned long src, unsigned long dst) {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state &amp;lt;&amp;lt;= 2;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (dst &amp;amp; 0x3f);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0xc0) &amp;gt;&amp;gt; 6;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0xc00) &amp;gt;&amp;gt; (10 - 2);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0xc000) &amp;gt;&amp;gt; (14 - 4);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0x30) &amp;lt;&amp;lt; (6 - 4);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0x300) &amp;lt;&amp;lt; (8 - 8);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0x3000) &amp;gt;&amp;gt; (12 - 10);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0x30000) &amp;gt;&amp;gt; (16 - 12);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0xc0000) &amp;gt;&amp;gt; (18 - 14);&lt;/span&gt;&lt;/div&gt;

&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Some of the bits of the BHB state seem to be folded together further using XOR when used for a BTB access, but the precise folding function hasn't been understood yet.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The BHB is interesting for two reasons. First, knowledge about its approximate behavior is required in order to be able to accurately cause collisions in the indirect call predictor. But it also permits dumping out the BHB state at any repeatable program state at which the attacker can execute code - for example, when attacking a hypervisor, directly after a hypercall. The dumped BHB state can then be used to fingerprint the hypervisor or, if the attacker has access to the hypervisor binary, to determine the low 20 bits of the hypervisor load address (in the case of KVM: the low 20 bits of the load address of kvm-intel.ko).&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Reverse-Engineering Branch Predictor Internals&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This subsection describes how we reverse-engineered the internals of the Haswell branch predictor. Some of this is written down from memory, since we didn't keep a detailed record of what we were doing.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We initially attempted to perform BTB injections into the kernel using the generic predictor, using the knowledge from prior research that the generic predictor only looks at the lower half of the source address and that only a partial target address is stored. This kind of worked - however, the injection success rate was very low, below 1%. (This is the method we used in our preliminary PoCs for method 2 against modified hypervisors running on Haswell.)&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We decided to write a userspace test case to be able to more easily test branch predictor behavior in different situations.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Based on the assumption that branch predictor state is shared between hyperthreads [10]&lt;/span&gt;&lt;span&gt;, we wrote a program of which two instances are each pinned to one of the two logical processors running on a specific physical core, where one instance attempts to perform branch injections while the other measures how often branch injections are successful. Both instances were executed with ASLR disabled and had the same code at the same addresses. The injecting process performed indirect calls to a function that accesses a (per-process) test variable; the measuring process performed indirect calls to a function that tests, based on timing, whether the per-process test variable is cached, and then evicts it using CLFLUSH. Both indirect calls were performed through the same callsite. Before each indirect call, the function pointer stored in memory was flushed out to main memory using CLFLUSH to widen the speculation time window. Additionally, because of the reference to &quot;recent program behavior&quot; in Intel's optimization manual, a bunch of conditional branches that are always taken were inserted in front of the indirect call.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In this test, the injection success rate was above 99%, giving us a base setup for future experiments.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-RMKau2Jstd0/Wk066oeGPUI/AAAAAAAACP0/sb5X95OHei0JV0xhCQDRhaYhRsLvx9ZmACEwYBhgL/s1600/image7.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;644&quot; data-original-width=&quot;618&quot; height=&quot;640&quot; src=&quot;https://2.bp.blogspot.com/-RMKau2Jstd0/Wk066oeGPUI/AAAAAAAACP0/sb5X95OHei0JV0xhCQDRhaYhRsLvx9ZmACEwYBhgL/s640/image7.png&quot; width=&quot;612&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We then tried to figure out the details of the prediction scheme. We assumed that the prediction scheme uses a global branch history buffer of some kind.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To determine the duration for which branch information stays in the history buffer, a conditional branch that is only taken in one of the two program instances was inserted in front of the series of always-taken conditional jumps, then the number of always-taken conditional jumps (N) was varied. The result was that for N=25, the processor was able to distinguish the branches (misprediction rate under 1%), but for N=26, it failed to do so (misprediction rate over 99%).&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Therefore, the branch history buffer had to be able to store information about at least the last 26 branches.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The code in one of the two program instances was then moved around in memory. This revealed that only the lower 20 bits of the source and target addresses have an influence on the branch history buffer.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Testing with different types of branches in the two program instances revealed that static jumps, taken conditional jumps, calls and returns influence the branch history buffer the same way; non-taken conditional jumps don't influence it; the address of the last byte of the source instruction is the one that counts; IRETQ doesn't influence the history buffer state (which is useful for testing because it permits creating program flow that is invisible to the history buffer).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Moving the last conditional branch before the indirect call around in memory multiple times revealed that the branch history buffer contents can be used to distinguish many different locations of that last conditional branch instruction. This suggests that the history buffer doesn't store a list of small history values; instead, it seems to be a larger buffer in which history data is mixed together.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;However, a history buffer needs to &quot;forget&quot; about past branches after a certain number of new branches have been taken in order to be useful for branch prediction. Therefore, when new data is mixed into the history buffer, this can not cause information in bits that are already present in the history buffer to propagate downwards - and given that, upwards combination of information probably wouldn't be very useful either. Given that branch prediction also must be very fast, we concluded that it is likely that the update function of the history buffer left-shifts the old history buffer, then XORs in the new state (see diagram).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-crbBTenupqw/Wk066CBQ2qI/AAAAAAAACPw/GaUAa5tUDE8dV9tZ6bt-QWGLY5klZM5FgCEwYBhgL/s1600/image6.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;117&quot; data-original-width=&quot;624&quot; height=&quot;118&quot; src=&quot;https://1.bp.blogspot.com/-crbBTenupqw/Wk066CBQ2qI/AAAAAAAACPw/GaUAa5tUDE8dV9tZ6bt-QWGLY5klZM5FgCEwYBhgL/s640/image6.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;If this assumption is correct, then the history buffer contains a lot of information about the most recent branches, but only contains as many bits of information as are shifted per history buffer update about the last branch about which it contains any data. Therefore, we tested whether flipping different bits in the source and target addresses of a jump followed by 32 always-taken jumps with static source and target allows the branch prediction to disambiguate an indirect call. [11]&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;With 32 static jumps in between, no bit flips seemed to have an influence, so we decreased the number of static jumps until a difference was observable. The result with 28 always-taken jumps in between was that bits 0x1 and 0x2 of the target and bits 0x40 and 0x80 of the source had such an influence; but flipping both 0x1 in the target and 0x40 in the source or 0x2 in the target and 0x80 in the source did not permit disambiguation. This shows that the per-insertion shift of the history buffer is 2 bits and shows which data is stored in the least significant bits of the history buffer. We then repeated this with decreased amounts of fixed jumps after the bit-flipped jump to determine which information is stored in the remaining bits.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Reading host memory from a KVM guest&lt;/span&gt;&lt;/h2&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Locating the host kernel&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Our PoC locates the host kernel in several steps. The information that is determined and necessary for the next steps of the attack consists of:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;lower 20 bits of the address of kvm-intel.ko&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;full address of kvm.ko&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;full address of vmlinux&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Looking back, this is unnecessarily complicated, but it nicely demonstrates the various techniques an attacker can use. A simpler way would be to first determine the address of vmlinux, then bisect the addresses of kvm.ko and kvm-intel.ko.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In the first step, the address of kvm-intel.ko is leaked. For this purpose, the branch history buffer state after guest entry is dumped out. Then, for every possible value of bits 12..19 of the load address of kvm-intel.ko, the expected lowest 16 bits of the history buffer are computed based on the load address guess and the known offsets of the last 8 branches before guest entry, and the results are compared against the lowest 16 bits of the leaked history buffer state.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The branch history buffer state is leaked in steps of 2 bits by measuring misprediction rates of an indirect call with two targets. One way the indirect call is reached is from a vmcall instruction followed by a series of N branches whose relevant source and target address bits are all zeroes. The second way the indirect call is reached is from a series of controlled branches in userspace that can be used to write arbitrary values into the branch history buffer.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Misprediction rates are measured as in the section &quot;Reverse-Engineering Branch Predictor Internals&quot;, using one call target that loads a cache line and another one that checks whether the same cache line has been loaded.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-juuq6gYvhn4/Wk065XER9yI/AAAAAAAACPo/EuTi1HzweloKCChZajY7FUDEtHXhs12ZwCEwYBhgL/s1600/image4.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;462&quot; data-original-width=&quot;451&quot; height=&quot;640&quot; src=&quot;https://3.bp.blogspot.com/-juuq6gYvhn4/Wk065XER9yI/AAAAAAAACPo/EuTi1HzweloKCChZajY7FUDEtHXhs12ZwCEwYBhgL/s640/image4.png&quot; width=&quot;624&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;With N=29, mispredictions will occur at a high rate if the controlled branch history buffer value is zero because all history buffer state from the hypercall has been erased. With N=28, mispredictions will occur if the controlled branch history buffer value is one of 0&amp;lt;&amp;lt;(28*2), 1&amp;lt;&amp;lt;(28*2), 2&amp;lt;&amp;lt;(28*2), 3&amp;lt;&amp;lt;(28*2) - by testing all four possibilities, it can be detected which one is right. Then, for decreasing values of N, the four possibilities are {0|1|2|3}&amp;lt;&amp;lt;(28*2) | (history_buffer_for(N+1) &amp;gt;&amp;gt; 2). By repeating this for decreasing values for N, the branch history buffer value for N=0 can be determined.&lt;/span&gt;&lt;/div&gt;

&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-uAWgXXpjs8I/Wk064FS-g3I/AAAAAAAACPc/0nh9nbSn0EkTXgzbbuY1TRROTR02GWIFQCEwYBhgL/s1600/image1.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;315&quot; data-original-width=&quot;457&quot; height=&quot;440&quot; src=&quot;https://4.bp.blogspot.com/-uAWgXXpjs8I/Wk064FS-g3I/AAAAAAAACPc/0nh9nbSn0EkTXgzbbuY1TRROTR02GWIFQCEwYBhgL/s640/image1.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;At this point, the low 20 bits of kvm-intel.ko are known; the next step is to roughly locate kvm.ko.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;For this, the generic branch predictor is used, using data inserted into the BTB by an indirect call from kvm.ko to kvm-intel.ko that happens on every hypercall; this means that the source address of the indirect call has to be leaked out of the BTB.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;kvm.ko will probably be located somewhere in the range from&lt;/span&gt; &lt;span&gt;0xffffffffc0000000&lt;/span&gt; &lt;span&gt;to&lt;/span&gt; &lt;span&gt;0xffffffffc4000000&lt;/span&gt;&lt;span&gt;, with page alignment (0x1000). This means that the first four entries in the table in the section &quot;Generic Predictor&quot; apply; there will be 2&lt;/span&gt;&lt;span&gt;4&lt;/span&gt;&lt;span&gt;-1=15 aliasing addresses for the correct one. But that is also an advantage: It cuts down the search space from 0x4000 to 0x4000/2&lt;/span&gt;&lt;span&gt;4&lt;/span&gt;&lt;span&gt;=1024.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To find the right address for the source or one of its aliasing addresses, code that loads data through a specific register is placed at all possible call targets (the leaked low 20 bits of kvm-intel.ko plus the in-module offset of the call target plus a multiple of 2&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;&lt;span&gt;) and indirect calls are placed at all possible call sources. Then, alternatingly, hypercalls are performed and indirect calls are performed through the different possible non-aliasing call sources, with randomized history buffer state that prevents the specialized prediction from working. After this step, there are 2&lt;/span&gt;&lt;span&gt;16&lt;/span&gt; &lt;span&gt;remaining possibilities for the load address of kvm.ko.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Next, the load address of vmlinux can be determined in a similar way, using an indirect call from vmlinux to kvm.ko. Luckily, none of the bits which are randomized in the load address of vmlinux  are folded together, so unlike when locating kvm.ko, the result will directly be unique. vmlinux has an alignment of 2MiB and a randomization range of 1GiB, so there are still only 512 possible addresses.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Because (as far as we know) a simple hypercall won't actually cause indirect calls from vmlinux to kvm.ko, we instead use port I/O from the status register of an emulated serial port, which is present in the default configuration of a virtual machine created with virt-manager.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The only remaining piece of information is which one of the 16 aliasing load addresses of kvm.ko is actually correct. Because the source address of an indirect call to kvm.ko is known, this can be solved using bisection: Place code at the various possible targets that, depending on which instance of the code is speculatively executed, loads one of two cache lines, and measure which one of the cache lines gets loaded.&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Identifying cache sets&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The PoC assumes that the VM does not have access to hugepages.To discover eviction sets for all L3 cache sets with a specific alignment relative to a 4KiB page boundary, the PoC first allocates 25600 pages of memory. Then, in a loop, it selects random subsets of all remaining unsorted pages such that the expected number of sets for which an eviction set is contained in the subset is 1, reduces each subset down to an eviction set by repeatedly accessing its cache lines and testing whether the cache lines are always cached (in which case they're probably not part of an eviction set) and attempts to use the new eviction set to evict all remaining unsorted cache lines to determine whether they are in the same cache set [12].&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Locating the host-virtual address of a guest page&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Because this attack uses a FLUSH+RELOAD approach for leaking data, it needs to know the host-kernel-virtual address of one guest page. Alternative approaches such as PRIME+PROBE should work without that requirement.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The basic idea for this step of the attack is to use a branch target injection attack against the hypervisor to load an attacker-controlled address and test whether that caused the guest-owned page to be loaded. For this, a gadget that simply loads from the memory location specified by R8 can be used - R8-R11 still contain guest-controlled values when the first indirect call after a guest exit is reached on this kernel build.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We expected that an attacker would need to either know which eviction set has to be used at this point or brute-force it simultaneously; however, experimentally, using random eviction sets works, too. Our theory is that the observed behavior is actually the result of L1D and L2 evictions, which might be sufficient to permit a few instructions worth of speculative execution.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The host kernel maps (nearly?) all physical memory in the physmap area, including memory assigned to KVM guests. However, the location of the physmap is randomized (with a 1GiB alignment), in an area of size 128PiB. Therefore, directly bruteforcing the host-virtual address of a guest page would take a long time. It is not necessarily impossible; as a ballpark estimate, it should be possible within a day or so, maybe less, assuming 12000 successful injections per second and 30 guest pages that are tested in parallel; but not as impressive as doing it in a few minutes.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To optimize this, the problem can be split up: First, brute-force the physical address using a gadget that can load from physical addresses, then brute-force the base address of the physmap region. Because the physical address can usually be assumed to be far below 128PiB, it can be brute-forced more efficiently, and brute-forcing the base address of the physmap region afterwards is also easier because then address guesses with 1GiB alignment can be used.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To brute-force the physical address, the following gadget can be used:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9def:       4c 89 c0                mov    rax,r8&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9df2:       4d 63 f9                movsxd r15,r9d&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9df5:       4e 8b 04 fd c0 b3 a6    mov    r8,QWORD PTR [r15*8-0x7e594c40]&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9dfc:       81&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9dfd:       4a 8d 3c 00             lea    rdi,[rax+r8*1]&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9e01:       4d 8b a4 00 f8 00 00    mov    r12,QWORD PTR [r8+rax*1+0xf8]&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9e08:       00&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This gadget permits loading an 8-byte-aligned value from the area around the kernel text section by setting R9 appropriately, which in particular permits loading&lt;/span&gt; &lt;span&gt;page_offset_base&lt;/span&gt;&lt;span&gt;, the start address of the physmap. Then, the value that was originally in R8 - the physical address guess minus 0xf8 - is added to the result of the previous load, 0xfa is added to it, and the result is dereferenced.&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Cache set selection&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To select the correct L3 eviction set, the attack from the following section is essentially executed with different eviction sets until it works.&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Leaking data&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;At this point, it would normally be necessary to locate gadgets in the host kernel code that can be used to actually leak data by reading from an attacker-controlled location, shifting and masking the result appropriately and then using the result of that as offset to an attacker-controlled address for a load. But piecing gadgets together and figuring out which ones work in a speculation context seems annoying. So instead, we decided to use the eBPF interpreter, which is built into the host kernel - while there is no legitimate way to invoke it from inside a VM, the presence of the code in the host kernel's text section is sufficient to make it usable for the attack, just like with ordinary ROP gadgets.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The eBPF interpreter entry point has the following function signature:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;static unsigned int __bpf_prog_run(void *ctx, const struct bpf_insn *insn)&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The second parameter is a pointer to an array of statically pre-verified eBPF instructions to be executed - which means that&lt;/span&gt; &lt;span&gt;__bpf_prog_run()&lt;/span&gt; &lt;span&gt;will not perform any type checks or bounds checks. The first parameter is simply stored as part of the initial emulated register state, so its value doesn't matter.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The eBPF interpreter provides, among other things:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;multiple emulated 64-bit registers&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;64-bit immediate writes to emulated registers&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;memory reads from addresses stored in emulated registers&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;bitwise operations (including bit shifts) and arithmetic operations&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To call the interpreter entry point, a gadget that gives RSI and RIP control given R8-R11 control and controlled data at a known memory location is necessary. The following gadget provides this functionality:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff81514edd:       4c 89 ce                mov    rsi,r9&lt;/span&gt;&lt;span&gt;&lt;br class=&quot;kix-line-break&quot;/&gt;&lt;/span&gt;&lt;span&gt;ffffffff81514ee0:       41 ff 90 b0 00 00 00    call   QWORD PTR [r8+0xb0]&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Now, by pointing R8 and R9 at the mapping of a guest-owned page in the physmap, it is possible to speculatively execute arbitrary unvalidated eBPF bytecode in the host kernel. Then, relatively straightforward bytecode can be used to leak data into the cache.&lt;/span&gt;&lt;/div&gt;


&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In summary, an attack using this variant of the issue attempts to read kernel memory from userspace without misdirecting the control flow of kernel code. This works by using the code pattern that was used for the previous variants, but in userspace. The underlying idea is that the permission check for accessing an address might not be on the critical path for reading data from memory to a register, where the permission check could have significant performance impact. Instead, the memory read could make the result of the read available to following instructions immediately and only perform the permission check asynchronously, setting a flag in the reorder buffer that causes an exception to be raised if the permission check fails.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We do have a few additions to make to Anders Fogh's blogpost:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Imagine the following instruction executed in usermode&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;mov rax,[somekernelmodeaddress]&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It will cause an interrupt when retired, [...]&quot;&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It is also possible to already execute that instruction behind a high-latency mispredicted branch to avoid taking a page fault. This might also widen the speculation window by increasing the delay between the read from a kernel address and delivery of the associated exception.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;First, I call a syscall that touches this memory. Second, I use the prefetcht0 instruction to improve my odds of having the address loaded in L1.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;When we used prefetch instructions after doing a syscall, the attack stopped working for us, and we have no clue why. Perhaps the CPU somehow stores whether access was denied on the last access and prevents the attack from working if that is the case?&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Fortunately I did not get a slow read suggesting that Intel null’s the result when the access is not allowed.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;That (read from kernel address returns all-zeroes) seems to happen for memory that is not sufficiently cached but for which pagetable entries are present, at least after repeated read attempts. For unmapped memory, the kernel address read does not return a result at all.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We believe that our research provides many remaining research topics that we have not yet investigated, and we encourage other public researchers to look into these.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section contains an even higher amount of speculation than the rest of this blogpost - it contains untested ideas that might well be useless.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Leaking without data cache timing&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It would be interesting to explore whether there are microarchitectural attacks other than measuring data cache timing that can be used for exfiltrating data out of speculative execution.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Other microarchitectures&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Our research was relatively Haswell-centric so far. It would be interesting to see details e.g. on how the branch prediction of other modern processors works and how well it can be attacked.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Other JIT engines&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We developed a successful variant 1 attack against the JIT engine built into the Linux kernel. It would be interesting to see whether attacks against more advanced JIT engines with less control over the system are also practical - in particular, JavaScript engines.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;More efficient scanning for host-virtual addresses and cache sets&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In variant 2, while scanning for the host-virtual address of a guest-owned page, it might make sense to attempt to determine its L3 cache set first. This could be done by performing L3 evictions using an eviction pattern through the physmap, then testing whether the eviction affected the guest-owned page.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The same might work for cache sets - use an L1D+L2 eviction set to evict the function pointer in the host kernel context, use a gadget in the kernel to evict an L3 set using physical addresses, then use that to identify which cache sets guest lines belong to until a guest-owned eviction set has been constructed.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Dumping the complete BTB state&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Given that the generic BTB seems to only be able to distinguish 2&lt;/span&gt;&lt;span&gt;31-8&lt;/span&gt; &lt;span&gt;or fewer source addresses, it seems feasible to dump out the complete BTB state generated by e.g. a hypercall in a timeframe around the order of a few hours. (Scan for jump sources, then for every discovered jump source, bisect the jump target.) This could potentially be used to identify the locations of functions in the host kernel even if the host kernel is custom-built.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The source address aliasing would reduce the usefulness somewhat, but because target addresses don't suffer from that, it might be possible to correlate (source,target) pairs from machines with different KASLR offsets and reduce the number of candidate addresses based on KASLR being additive while aliasing is bitwise.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This could then potentially allow an attacker to make guesses about the host kernel version or the compiler used to build it based on jump offsets or distances between functions.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Variant 2: Leaking with more efficient gadgets&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;If sufficiently efficient gadgets are used for variant 2, it might not be necessary to evict host kernel function pointers from the L3 cache at all; it might be sufficient to only evict them from L1D and L2.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Various speedups&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In particular the variant 2 PoC is still a bit slow. This is probably partly because:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It only leaks one bit at a time; leaking more bits at a time should be doable.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It heavily uses IRETQ for hiding control flow from the processor.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It would be interesting to see what data leak rate can be achieved using variant 2.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Leaking or injection through the return predictor&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;If the return predictor also doesn't lose its state on a privilege level change, it might be useful for either locating the host kernel from inside a VM (in which case bisection could be used to very quickly discover the full address of the host kernel) or injecting return targets (in particular if the return address is stored in a cache line that can be flushed out by the attacker and isn't reloaded before the return instruction).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;However, we have not performed any experiments with the return predictor that yielded conclusive results so far.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Leaking data out of the indirect call predictor&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We have attempted to leak target information out of the indirect call predictor, but haven't been able to make it work.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The following statement were provided to us regarding this issue from the vendors to whom Project Zero disclosed this vulnerability:&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;No current statement provided at this time.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;AMD&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;No current statement provided at this time.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;ARM&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Arm recognises that the speculation functionality of many modern high-performance processors, despite working as intended, can be used in conjunction with the timing of cache operations to leak some information as described in this blog. Correspondingly, Arm has developed software mitigations that we recommend be deployed.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Arm has included a detailed technical whitepaper as well as links to information from some of Arm’s architecture partners regarding their specific implementations and mitigations.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Note that some of these documents - in particular Intel's documentation - change over time, so quotes from and references to it may not reflect the latest version of Intel's documentation.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Placing data immediately following an indirect branch can cause a performance problem. If the data consists of all zeros, it looks like a long stream of ADDs to memory destinations and this can cause resource conflicts and slow down branch recovery. Also, data immediately following indirect branches may appear as branches to the branch predication [sic] hardware, which can branch off to execute other data pages. This can lead to subsequent self-modifying code problems.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Loads can:[...]Be carried out speculatively, before preceding branches are resolved.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Software should avoid writing to a code page in the same 1-KByte subpage that is being executed or fetching code in the same 2-KByte subpage of that is being written. In addition, sharing a page containing directly or speculatively executed code with another processor as a data page can trigger an SMC condition that causes the entire pipeline of the machine and the trace cache to be cleared. This is due to the self-modifying code condition.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;if mapped as WB or WT, there is a potential for speculative processor reads to bring the data into the caches&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Failure to map the region as WC may allow the line to be speculatively read into the processor caches (via the wrong path of a mispredicted branch).&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1507.06955.pdf&quot;&gt;&lt;span&gt;https://arxiv.org/pdf/1507.06955.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;: The rowhammer.js research by Daniel Gruss, Clémentine Maurice and Stefan Mangard contains information about L3 cache eviction patterns that we reused in the KVM PoC to evict a function pointer.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://www.sophia.re/thesis.pdf&quot;&gt;&lt;span&gt;https://www.sophia.re/thesis.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;: Sophia D'Antoine wrote a thesis that shows that opcode scheduling can theoretically be used to transmit data between hyperthreads.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://gruss.cc/files/kaiser.pdf&quot;&gt;&lt;span&gt;https://gruss.cc/files/kaiser.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;: Daniel Gruss, Moritz Lipp, Michael Schwarz, Richard Fellner, Clémentine Maurice, and Stefan Mangard wrote a paper on mitigating microarchitectural issues caused by pagetable sharing between userspace and the kernel.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;&lt;span&gt;[2]&lt;/span&gt; &lt;span&gt;The precise model names are listed in the section &quot;Tested Processors&quot;. The code for reproducing this is in the writeup_files.tar archive in our bugtracker, in the folders userland_test_x86 and userland_test_aarch64.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;[3]&lt;/span&gt; &lt;span&gt;The attacker-controlled offset used to perform an out-of-bounds access on an array by this PoC is a 32-bit value, limiting the accessible addresses to a 4GiB window in the kernel heap area.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;[4]&lt;/span&gt; &lt;span&gt;This PoC won't work on CPUs with SMAP support; however, that is not a fundamental limitation.&lt;/span&gt;&lt;/div&gt;

&lt;div&gt;&lt;span&gt;[6]&lt;/span&gt; &lt;span&gt;The phone was running an Android build from May 2017.&lt;/span&gt;&lt;/div&gt;


&lt;div&gt;&lt;span&gt;[9]&lt;/span&gt; &lt;span&gt;More than 2&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;mappings would be more efficient, but the kernel places a hard cap of 2&lt;/span&gt;&lt;span&gt;16&lt;/span&gt; &lt;span&gt;on the number of VMAs that a process can have.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;[10]&lt;/span&gt; &lt;span&gt;Intel's optimization manual states that &quot;In the first implementation of HT Technology, the physical execution resources are shared and the architecture state is duplicated for each logical processor&quot;, so it would be plausible for predictor state to be shared. While predictor state could be tagged by logical core, that would likely reduce performance for multithreaded processes, so it doesn't seem likely.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;[11]&lt;/span&gt; &lt;span&gt;In case the history buffer was a bit bigger than we had measured, we added some margin - in particular because we had seen slightly different history buffer lengths in different experiments, and because 26 isn't a very round number.&lt;/span&gt;&lt;/div&gt;



</description>
<pubDate>Wed, 03 Jan 2018 22:20:48 +0000</pubDate>
<dc:creator>brandon</dc:creator>
<og:url>https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html</og:url>
<og:title>Reading privileged memory with a side-channel</og:title>
<og:description>Posted by Jann Horn, Project Zero We have discovered that CPU data cache timing can be abused to efficiently leak information out of mi...</og:description>
<og:image>https://3.bp.blogspot.com/-64YRnkgMujY/Wk064WtrSRI/AAAAAAAACPg/O4ZxlUiWerACL61P490xrVMWSmHTwQflQCEwYBhgL/w1200-h630-p-k-no-nu/image3.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html</dc:identifier>
</item>
<item>
<title>Intel Responds to Security Research Findings</title>
<link>https://newsroom.intel.com/news/intel-responds-to-security-research-findings/</link>
<guid isPermaLink="true" >https://newsroom.intel.com/news/intel-responds-to-security-research-findings/</guid>
<description>&lt;p&gt;Intel and other technology companies have been made aware of new security research describing software analysis methods that, when used for malicious purposes, have the potential to improperly gather sensitive data from computing devices that are operating as designed. Intel believes these exploits do not have the potential to corrupt, modify or delete data.&lt;/p&gt;
&lt;p&gt;Recent reports that these exploits are caused by a “bug” or a “flaw” and are unique to Intel products are incorrect. Based on the analysis to date, many types of computing devices — with many different vendors’ processors and operating systems — are susceptible to these exploits.&lt;/p&gt;
&lt;p&gt;Intel is committed to product and customer security and is working closely with many other technology companies, including AMD, ARM Holdings and several operating system vendors, to develop an industry-wide approach to resolve this issue promptly and constructively. Intel has begun providing software and firmware updates to mitigate these exploits. Contrary to some reports, any performance impacts are workload-dependent, and, for the average computer user, should not be significant and will be mitigated over time.&lt;/p&gt;
&lt;p&gt;Intel is committed to the industry best practice of responsible disclosure of potential security issues, which is why Intel and other vendors had planned to disclose this issue next week when more software and firmware updates will be available. However, Intel is making this statement today because of the current inaccurate media reports.&lt;/p&gt;
&lt;p&gt;Check with your operating system vendor or system manufacturer and apply any available updates as soon as they are available. Following good security practices that protect against malware in general will also help protect against possible exploitation until updates can be applied.&lt;/p&gt;
&lt;p&gt;Intel believes its products are the most secure in the world and that, with the support of its partners, the current solutions to this issue provide the best possible security for its customers.&lt;/p&gt;
</description>
<pubDate>Wed, 03 Jan 2018 20:06:10 +0000</pubDate>
<dc:creator>runesoerensen</dc:creator>
<og:type>article</og:type>
<og:title>Intel Responds to Security Research Findings</og:title>
<og:description>Intel Corporation and other technology companies have been made aware of new security research describing software analysis methods that, when used for malicious purposes, have the potential to improperly gather sensitive data from computing devices that are operating as designed. Intel believes these exploits do not have the potential to corrupt, modify or delete data.</og:description>
<og:url>https://newsroom.intel.com/news/intel-responds-to-security-research-findings/</og:url>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://newsroom.intel.com/news/intel-responds-to-security-research-findings/</dc:identifier>
</item>
<item>
<title>After beating cable lobby, Colorado city moves ahead with muni broadband</title>
<link>https://arstechnica.com/tech-policy/2018/01/colorado-city-to-build-fiber-broadband-network-with-net-neutrality/</link>
<guid isPermaLink="true" >https://arstechnica.com/tech-policy/2018/01/colorado-city-to-build-fiber-broadband-network-with-net-neutrality/</guid>
<description>&lt;img src=&quot;https://cdn.arstechnica.net/wp-content/uploads/2017/11/anti-muni-broadband-ad-fort-collins-800x473.jpg&quot;/&gt;&lt;div class=&quot;caption-text&quot;&gt;&lt;a href=&quot;https://cdn.arstechnica.net/wp-content/uploads/2017/11/anti-muni-broadband-ad-fort-collins.jpg&quot; class=&quot;enlarge-link&quot; data-height=&quot;481&quot; data-width=&quot;814&quot;&gt;Enlarge&lt;/a&gt; &lt;span class=&quot;sep&quot;&gt;/&lt;/span&gt; Still from an industry-funded ad warning against municipal broadband in Fort Collins, Colorado.&lt;/div&gt;&lt;aside id=&quot;social-left&quot;&gt;&lt;a title=&quot;65 posters participating&quot; class=&quot;comment-count icon-comment-bubble-down&quot; href=&quot;https://arstechnica.com/tech-policy/2018/01/colorado-city-to-build-fiber-broadband-network-with-net-neutrality/?comments=1&quot;&gt;&lt;span class=&quot;comment-count-before&quot;&gt;reader comments&lt;/span&gt; &lt;span class=&quot;comment-count-number&quot;&gt;109&lt;/span&gt;&lt;/a&gt;
&lt;div class=&quot;share-links&quot;&gt;&lt;span&gt;Share this story&lt;/span&gt;    &lt;/div&gt;
&lt;/aside&gt;&lt;p&gt;The city council in Fort Collins, Colorado, last night voted to move ahead with a municipal fiber broadband network providing gigabit speeds, two months after the cable industry failed to stop the project.&lt;/p&gt;
&lt;p&gt;Last night's city council vote came after residents of Fort Collins &lt;a href=&quot;https://arstechnica.com/tech-policy/2017/11/voters-reject-cable-lobby-misinformation-campaign-against-muni-broadband/&quot;&gt;approved a ballot question&lt;/a&gt; that authorized the city to build a broadband network. The ballot question, passed in November, didn't guarantee that the network would be built because city council approval was still required, but that hurdle is now cleared. Residents approved the ballot question despite an anti-municipal broadband &lt;a href=&quot;https://arstechnica.com/tech-policy/2017/11/comcast-has-a-lot-to-lose-if-municipal-broadband-takes-off/&quot;&gt;lobbying campaign&lt;/a&gt; backed by groups funded by Comcast and CenturyLink.&lt;/p&gt;
&lt;p&gt;The Fort Collins City Council voted 7-0 to approve the broadband-related measures, a city government spokesperson confirmed to Ars today.&lt;/p&gt;
&lt;p&gt;&quot;Last night's three unanimous votes begin the process of building our city's own broadband network,&quot; Glen Akins, a resident who helped lead the pro-municipal broadband campaign, told Ars today. &quot;We're extremely pleased the entire city council voted to support the network after the voters' hard fought election victory late last year. The municipal broadband network will make Fort Collins an even more incredible place to live.&quot;&lt;/p&gt;
&lt;h2&gt;Net neutrality and privacy&lt;/h2&gt;
&lt;p&gt;While the Federal Communications Commission has &lt;a href=&quot;https://arstechnica.com/tech-policy/2017/12/goodbye-net-neutrality-ajit-pais-fcc-votes-to-allow-blocking-and-throttling/&quot;&gt;voted to eliminate&lt;/a&gt; the nation's net neutrality rules, the municipal broadband network will be neutral and without data caps.&lt;/p&gt;
&lt;p&gt;&quot;The network will deliver a 'net-neutral' competitive unfettered data offering that does not impose caps or usage limits on one use of data over another (i.e., does not limit streaming or charge rates based on type of use),&quot; a new &lt;a href=&quot;https://www-static.bouldercolorado.gov/docs/Study_Session_Boulder_s_Broadband_Initiative-1-201801021108.pdf?_ga=2.40571111.459175782.1514908137-605171352.1513615528&quot;&gt;planning document&lt;/a&gt; says. &quot;All application providers (data, voice, video, cloud services) are equally able to provide their services, and consumers' access to advanced data opens up the marketplace.&quot;&lt;/p&gt;
&lt;p&gt;The city will also be &lt;a href=&quot;https://www.fcgov.com/broadband/&quot;&gt;developing policies&lt;/a&gt; to protect consumers' privacy. FCC privacy rules that would have protected all Americans were &lt;a href=&quot;https://arstechnica.com/tech-policy/2017/03/for-sale-your-private-browsing-history/&quot;&gt;eliminated&lt;/a&gt; by the Republican-controlled Congress last year.&lt;/p&gt;
&lt;p&gt;The items approved last night (detailed &lt;a href=&quot;http://citydocs.fcgov.com/?cmd=convert&amp;amp;vid=72&amp;amp;docid=3087233&amp;amp;dt=AGENDA+ITEM&amp;amp;doc_download_date=JAN-02-2018&amp;amp;ITEM_NUMBER=20&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://citydocs.fcgov.com/?cmd=convert&amp;amp;vid=72&amp;amp;docid=3087234&amp;amp;dt=SUMMARY+AGENDA+-+ELECTRIC+UTILITY+ENTERPRISE&amp;amp;doc_download_date=JAN-02-2018&amp;amp;ITEM_NUMBER=&quot;&gt;here&lt;/a&gt;) provide a $1.8 million loan from the city's general fund to the electric utility for first-year start-up costs related to building telecommunications facilities and services. Later, bonds will be &quot;issued to support the total broadband build out,&quot; the measure says.&lt;/p&gt;
&lt;p&gt;The city intends to provide gigabit service for $70 a month or less and a cheaper Internet tier. Underground wiring for improved reliability and &quot;universal coverage&quot; are two of the key goals listed in the measure.&lt;/p&gt;
&lt;aside class=&quot;pullbox sidebar story-sidebar right&quot;&gt;
&lt;/aside&gt;&lt;p&gt;Building a citywide network is a lengthy process—the city says its goal is to be done in &quot;less than five years.&quot;&lt;/p&gt;
&lt;h2&gt;Telecom lobby failure&lt;/h2&gt;
&lt;p&gt;The telecom industry-led campaign against the project &lt;a href=&quot;https://www.fcgov.com/cityclerk/reports-2017nov.php&quot;&gt;spent&lt;/a&gt; more than $900,000, most of which was supplied by the Colorado Cable Telecommunications Association. Comcast is a member of that lobby group.&lt;/p&gt;
&lt;p&gt;Fort Collins Mayor Wade Troxell criticized incumbent ISPs and the local Chamber of Commerce for spreading &quot;misinformation&quot; to voters, &lt;em&gt;The Coloradoan&lt;/em&gt; &lt;a href=&quot;http://www.coloradoan.com/story/news/2017/11/07/fort-collins-broadband-election-passes-results/840551001/&quot;&gt;reported&lt;/a&gt; at the time.&lt;/p&gt;
&lt;p&gt;The pro-municipal broadband effort led by community members won despite spending just $15,000. More than &lt;a href=&quot;http://www.coloradoan.com/story/news/2017/11/06/election-day-2017-live-results-fort-collins-larimer-county-loveland-timnath-wellington/836496001/&quot;&gt;57 percent&lt;/a&gt; of voters approved the measure.&lt;/p&gt;
&lt;p&gt;&quot;We're incredibly excited about the voting results from last night,&quot; Colin Garfield, who led the residents' pro-broadband effort, told Ars today. &quot;The tireless work our committee performed and the voice of the voters have been rewarded.&quot;&lt;/p&gt;
</description>
<pubDate>Wed, 03 Jan 2018 18:00:45 +0000</pubDate>
<dc:creator>jseliger</dc:creator>
<og:url>https://arstechnica.com/tech-policy/2018/01/colorado-city-to-build-fiber-broadband-network-with-net-neutrality/</og:url>
<og:title>After beating cable lobby, Colorado city moves ahead with muni broadband</og:title>
<og:image>https://cdn.arstechnica.net/wp-content/uploads/2017/11/anti-muni-broadband-ad-fort-collins-760x380.jpg</og:image>
<og:description>Fort Collins plans universal broadband, net neutrality, and gigabit speeds.</og:description>
<og:type>article</og:type>
<dc:language>en-us</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://arstechnica.com/tech-policy/2018/01/colorado-city-to-build-fiber-broadband-network-with-net-neutrality/</dc:identifier>
</item>
<item>
<title>Spotify files for its IPO</title>
<link>https://www.axios.com/exclusive-spotify-files-for-its-ipo-2522109160.html</link>
<guid isPermaLink="true" >https://www.axios.com/exclusive-spotify-files-for-its-ipo-2522109160.html</guid>
<description>&lt;p&gt;Here's what the major companies have said so far.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intel:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&quot;Intel and other technology companies have been made aware of new security research describing software analysis methods that, when used for malicious purposes, have the potential to improperly gather sensitive data from computing devices that are operating as designed. Intel believes these exploits do not have the potential to corrupt, modify or delete data.&lt;/p&gt;
&lt;p&gt;&quot;Recent reports that these exploits are caused by a &quot;bug&quot; or a &quot;flaw&quot; and are unique to Intel products are incorrect. Based on the analysis to date, many types of computing devices — with many different vendors' processors and operating systems — are susceptible to these exploits.&quot;&lt;/p&gt;
&lt;p&gt;Click &lt;a href=&quot;https://newsroom.intel.com/news/intel-responds-to-security-research-findings/&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt; for the rest of Intel's statement&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: On a conference call, Intel said it doesn't expect a significant financial impact from the issue.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Microsoft:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Microsoft is updating Windows 10 today with a special fix for the issue and also making available updates for Windows 7 and Windows 8.&lt;/p&gt;
&lt;p&gt;&quot;We're aware of this industry-wide issue and have been working closely with chip manufacturers to develop and test mitigations to protect our customers. We are in the process of deploying mitigations to cloud services and have also released security updates to protect Windows customers against vulnerabilities affecting supported hardware chips from Intel, ARM, and AMD. We have not received any information to indicate that these vulnerabilities had been used to attack our customers.&quot;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Google:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Researchers from Google's Project Zero &lt;a href=&quot;https://googleprojectzero.blogspot.com&quot; target=&quot;_blank&quot;&gt;found the vulnerabilities&lt;/a&gt; last year and reported them to Intel, AMD and ARM in June 2017. In a &lt;a href=&quot;https://security.googleblog.com/2018/01/todays-cpu-vulnerability-what-you-need.html&quot; target=&quot;_blank&quot;&gt;blog post&lt;/a&gt;, Google disclosed what product actions it is taking with regards to Android, Chrome OS and the Google Cloud. It said other products, such as Chromecast and the Chrome browser aren't affected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Amazon:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&quot;This is a vulnerability that has existed for more than 20 years in modern processor architectures like Intel, AMD, and ARM across servers, desktops, and mobile devices. All but a small single-digit percentage of instances across the Amazon EC2 fleet are already protected. The remaining ones will be completed in the next several hours. We will keep customers apprised of additional information with updates to our security bulletin, which can be found &lt;a href=&quot;https://aws.amazon.com/security/security-bulletins/AWS-2018-013/&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&quot;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AMD:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&quot;To be clear, the security research team identified three variants targeting speculative execution. The threat and the response to the three variants differ by microprocessor company, and AMD is not susceptible to all three variants. Due to differences in AMD's architecture, we believe there is a near zero risk to AMD processors at this time. We expect the security research to be published later today and will provide further updates at that time.&quot;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ARM:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&quot;Arm (has) been working together with Intel and AMD to address a side-channel analysis method which exploits speculative execution techniques used in certain high-end processors, including some of our Cortex-A processors. This method requires malware running locally and could result in data being accessed from privileged memory. Please note that our Cortex-M processors, which are pervasive in low-power, connected IoT devices, are not impacted.&lt;/p&gt;
&lt;p&gt;&quot;We are encouraging our silicon partners to implement the software mitigations developed if their chips are impacted.&quot;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Apple&lt;/strong&gt; has not yet responded to requests for comment.&lt;/p&gt;

</description>
<pubDate>Wed, 03 Jan 2018 15:39:05 +0000</pubDate>
<dc:creator>rahulchhabra07</dc:creator>
<og:type>article</og:type>
<og:url>https://www.axios.com/exclusive-spotify-files-for-its-ipo-2522109160.html</og:url>
<og:image>https://axios-img.rbl.ms/simage/https%3A%2F%2Fassets.rbl.ms%2F17038699%2F1200x600.jpg/2000%2C2000/k2tffC0%2B1lRQsw%2FN/img.jpg</og:image>
<og:title>Exclusive: Spotify files for its IPO</og:title>
<og:description>Music streaming giant submitted confidential paperwork to the SEC.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.axios.com/exclusive-spotify-files-for-its-ipo-2522109160.html</dc:identifier>
</item>
<item>
<title>Intel Confronts Potential ‘PR Nightmare’ With Reported Chip Flaw</title>
<link>https://www.bloomberg.com/news/articles/2018-01-03/amd-soars-after-rival-intel-said-to-reveal-processor-flaw?cmpid=socialflow-twitter-business</link>
<guid isPermaLink="true" >https://www.bloomberg.com/news/articles/2018-01-03/amd-soars-after-rival-intel-said-to-reveal-processor-flaw?cmpid=socialflow-twitter-business</guid>
<description>&lt;p&gt;A report that &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.bloomberg.com/quote/INTC:US&quot; title=&quot;Company Overview&quot; rel=&quot;nofollow noopener&quot;&gt;Intel Corp.&lt;/a&gt; chips are vulnerable to hackers raised concerns about the company’s main products and brand.&lt;/p&gt;

&lt;p&gt;On Tuesday, the technology website &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.theregister.co.uk/2018/01/02/intel_cpu_design_flaw/&quot; title=&quot;The Register&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;The Register&lt;/a&gt; said a bug lets some software gain access to parts of a computer’s memory that are set aside to protect things like passwords. All computers with Intel chips from the past 10 years appear to be affected, the report said, and patches to &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.bloomberg.com/quote/MSFT:US&quot; title=&quot;Company Profile&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Microsoft Corp.&lt;/a&gt;’s Windows and &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.bloomberg.com/quote/AAPL:US&quot; title=&quot;Company Profile&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Apple Inc.&lt;/a&gt;’s OS X operating systems will be required. The security updates may slow down older machinery by as much as 30 percent, according to The Register.&lt;/p&gt;

&lt;p&gt;Flaws in the designs of microprocessors, which go through rigorous testing and verification, are usually easily fixed by patches in the code that they use to communicate with the rest of the computer. But if the error can’t be fixed easily in software, it could be necessary to redesign the chip, which can be extremely costly and time consuming.&lt;/p&gt;


&lt;p&gt;Intel is expected to release a statement, but hasn’t yet commented on the issue. Historically, the way companies respond to such issues and how quickly they address them has determined how big the problem becomes.&lt;/p&gt;

&lt;p&gt;“This is a potential PR nightmare,” said Dan Ives, head of tech research at GBH Insights. “They need to get ahead of this and try to contain any of the damage to customers as well to the brand.”&lt;/p&gt;
&lt;p&gt;The report hit Intel shares, which fell as much as 5.5 percent, the steepest drop since October 2016. It gave a boost to rivals &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.bloomberg.com/quote/AMD:US&quot; title=&quot;Company Profile&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Advanced Micro Devices Inc.&lt;/a&gt;, which surged as much as 8.8 percent, and Nvidia Corp., which jumped 6.3 percent.&lt;/p&gt;
&lt;p&gt;The vulnerability may have consequences beyond just computers, and may not be the result of a design or testing error. All modern microprocessors, including those that run smartphones, are built to essentially guess what functions they’re likely to be asked to run next. By queuing up possible executions in advance, they’re able to crunch data and run software much faster.&lt;/p&gt;
&lt;p&gt;The problem in this case, according to people familiar with the issue, is that this predictive loading of instructions allows access to data that’s normally cordoned off securely. That means, in theory, that malicious code could find a way to access information that would otherwise be out of reach, such as passwords.&lt;/p&gt;
&lt;aside class=&quot;inline-newsletter&quot; data-state=&quot;ready&quot;/&gt;
&lt;p&gt;Chip design flaws are exceedingly rare. More than 20 years ago, a college professor discovered a problem with how early versions of Intel’s Pentium chip calculated numbers. Rival International Business Machines Corp. was able to make use of the finding and claim Intel products would cause frequent problems for consumers’ computers. While that didn’t happen, Intel had to recall some chips and took a charge of more than $400 million.&lt;/p&gt;
&lt;p&gt;Intel’s microprocessors are the fundamental building block of the internet, corporate networks and PCs. The company has added to its designs over the years trying to make computers less vulnerable to attack, arguing that hardware security is typically tougher to crack than software.&lt;/p&gt;
&lt;p&gt;The Santa Clara, California-based company’s chips have more than 80 percent market share in PCs overall and more than 90 percent in laptops and servers.&lt;/p&gt;
&lt;p&gt;Programmers have been working for two months to try to provide a software patch that addresses the issue, The Register said, adding that Microsoft was expected to release a fix soon.&lt;/p&gt;
</description>
<pubDate>Wed, 03 Jan 2018 13:43:24 +0000</pubDate>
<dc:creator>el_duderino</dc:creator>
<og:description>A report that Intel Corp. chips are vulnerable to hackers raised concerns about the company’s main products and brand.</og:description>
<og:image>https://assets.bwbx.io/images/users/iqjWHBFdfxIU/i_3npkofw1g8/v0/1200x799.jpg</og:image>
<og:title>Intel Confronts Potential ‘PR Nightmare’ With Reported Chip Flaw</og:title>
<og:type>article</og:type>
<og:url>https://www.bloomberg.com/news/articles/2018-01-03/amd-soars-after-rival-intel-said-to-reveal-processor-flaw</og:url>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bloomberg.com/news/articles/2018-01-03/amd-soars-after-rival-intel-said-to-reveal-processor-flaw?cmpid=socialflow-twitter-business</dc:identifier>
</item>
<item>
<title>Teens Aren’t Partying Anymore</title>
<link>https://www.wired.com/story/why-teens-arent-partying-anymore/</link>
<guid isPermaLink="true" >https://www.wired.com/story/why-teens-arent-partying-anymore/</guid>
<description>&lt;div id=&quot;&quot;&gt;&lt;p&gt;&lt;img src=&quot;https://www.wired.com/wp-content/uploads/2017/05/1t6Jsgbu_bU84ZZkgxw8G8A-3.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kevin and I sit down&lt;/strong&gt; at two desks just outside his third period class at a high school in northern San Diego. He is 17 years old and Asian American, with spiky black hair, fashionable glasses, and a wan smile. He is the oldest of three children, with his parents expecting another child in a few months. Until recently, the family lived in an apartment, where the noise from his younger siblings was deafening. Perhaps as a result, he is unusually empathetic for a teenage boy. “Been doing this all day?” he asks as I take a drink of water before beginning our interview.&lt;/p&gt;
&lt;p&gt;Kevin is not the most organized student: He initially neglects to have his dad sign the back of the permission slip, and when I talk to the class later, he forgets his question by the time I call on him. But when I ask him what makes his generation different, he doesn’t hesitate: “I feel like we don’t party as much. People stay in more often. My generation lost interest in socializing in person—they don’t have physical get-togethers, they just text together, and they can just stay at home.”&lt;/p&gt;

&lt;p&gt;Kevin is onto something. For example, iGen teens—those who were born in 1995 and later, grew up with cell phones, had an Instagram page before they started high school, and do not remember a time before the internet—spend less time at parties than any previous generation. The trends are similar for college students, who are asked how many hours a week they spent at parties during their senior year in high school. In 2016, they said two hours a week—only a third of the time GenX students spent at parties in 1987. The decline in partying is not due to iGen’ers’ studying more; homework time is the same or lower. The trend is also not due to immigration or changes in ethnic composition; the decline is nearly identical among white teens.&lt;/p&gt;
&lt;div class=&quot;inset-left-component inset-left-component--article&quot;&gt;
&lt;ul class=&quot;inset-left-component--article__list&quot;&gt;&lt;li&gt;
&lt;h4 name=&quot;inset-left&quot; class=&quot;inset-left-component__el&quot;&gt;MORE OF THE TOP TECH BOOKS OF 2017&lt;/h4&gt;
&lt;/li&gt;
&lt;li class=&quot;article-list-item-embed-component__post&quot; readability=&quot;23&quot;&gt;

&lt;div class=&quot;article-list-item-embed-component__description&quot; readability=&quot;32&quot;&gt;
&lt;p&gt;&lt;span&gt;The Backchannel Team&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;article-list-item-embed-component__title&quot;&gt;The Top Tech Books of 2017: Part I&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li class=&quot;article-list-item-embed-component__post&quot; readability=&quot;23&quot;&gt;

&lt;div class=&quot;article-list-item-embed-component__description&quot; readability=&quot;32&quot;&gt;
&lt;p&gt;&lt;span&gt;The Backchannel Team&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;article-list-item-embed-component__title&quot;&gt;The Top Tech Books of 2017: Part II&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;
&lt;p&gt;Priya, a high school freshman, says she hasn’t been to any parties and doesn’t want to. “What you read in books is, like, oh my God, high school has all these football games and parties, and when you come there, eh, no one really does it. No one is really that interested—including me.” In the San Diego State University freshman survey, several mentioned that the high school parties they had gone to had been adult-run affairs, not exactly the ragers memorialized in the 1980s John Hughes movies, where kids got drunk and wrecked their parents’ houses. “The only parties I went to in high school were birthday parties, and they were almost always supervised or included an adult somewhere,” noted Nick, 18.&lt;/p&gt;
&lt;p&gt;Why are parties less popular? Kevin has an explanation for that: “People party because they’re bored—they want something to do. Now we have Netflix—you can watch series nonstop. There’s so many things to do on the web.” He might be right—with so much entertainment at home, why party? Teens also have other ways to connect and communicate, including the social media websites they spend so much time on. The party is constant, and it’s on Snapchat.&lt;/p&gt;
&lt;/div&gt;&lt;div id=&quot;&quot;&gt;&lt;p&gt;&lt;img src=&quot;https://www.wired.com/wp-content/uploads/2017/05/1AwTXMT2omVX-1Q8BM3cD-A-4.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;h3&gt;Just Hangin’&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Maybe parties aren’t for&lt;/strong&gt; this cautious, career-focused generation. Especially with the declining popularity of alcohol, perhaps iGen’ers are eschewing parties in favor of just hanging out with their friends.&lt;/p&gt;
&lt;p&gt;Except they’re not. The number of teens who get together with their friends every day has been cut in half in just fifteen years, with especially steep declines recently.&lt;/p&gt;
&lt;p&gt;This might be the most definitive evidence that iGen’ers spend less time interacting with their peers face-to-face than any previous generation—it’s not just parties or craziness but merely getting together with friends, spending time hanging out. That’s something nearly everyone does: nerds and jocks, introverted teens and extroverted ones, poor kids and rich kids, C students and A students, stoners and clean-cut kids. It doesn’t have to involve spending money or going someplace cool—it’s just being with your friends. And teens are doing it much less.&lt;/p&gt;
&lt;p&gt;The college student survey allows a more precise look at in-person social interaction, as it asks students how many hours a week they spend on those activities. College students in 2016 (vs. the late 1980s) spent four fewer hours a week socializing with their friends and three fewer hours a week partying—so seven hours a week less on in-person social interaction. That means iGen’ers were seeing their friends in person an hour less a day than GenX’ers and early Millennials did. An hour a day less spent with friends is an hour a day less spent building social skills, negotiating relationships, and navigating emotions. Some parents might see it as an hour a day saved for more productive activities, but the time has not been replaced with homework; it’s been replaced with screen time.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.wired.com/wp-content/uploads/2017/05/1uW_l9n54f47SZbPxRBEq2A-3.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;/div&gt;</description>
<pubDate>Wed, 03 Jan 2018 13:09:50 +0000</pubDate>
<dc:creator>SQL2219</dc:creator>
<og:type>article</og:type>
<og:title>Why Teens Aren’t Partying Anymore | Backchannel</og:title>
<og:description>Teens now have so many ways to connect and communicate that there’s no need to gather in person. The party is constant, and it’s on Snapchat.</og:description>
<og:image>https://media.wired.com/photos/5a2eacbca850e23a4736f3ed/191:100/pass/iStock-619269222.jpg</og:image>
<og:url>https://www.wired.com/story/why-teens-arent-partying-anymore/</og:url>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.wired.com/story/why-teens-arent-partying-anymore/</dc:identifier>
</item>
<item>
<title>Tesla Model 3 Sets New EV Cannonball Run Record</title>
<link>http://www.thedrive.com/new-cars/17312/tesla-model-3-sets-new-ev-cannonball-run-record-of-50-hours-16-minutes</link>
<guid isPermaLink="true" >http://www.thedrive.com/new-cars/17312/tesla-model-3-sets-new-ev-cannonball-run-record-of-50-hours-16-minutes</guid>
<description>&lt;div class=&quot;article-head&quot;&gt;

&lt;h2 class=&quot;dek&quot;&gt;Alex Roy joined owner Dan Zorrilla in one of the first customer cars off the assembly line.&lt;/h2&gt;


&lt;/div&gt;
&lt;div class=&quot;hero-image&quot;&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;&lt;img class=&quot;image-fade-in&quot; alt=&quot;&quot; src=&quot;http://imagesvc.timeincapp.com/v3/foundry/image/?q=70&amp;amp;w=1440&amp;amp;url=https%3A%2F%2Ftimedotcom.files.wordpress.com%2F2018%2F01%2Fimg_27521.jpg%3Fquality%3D85&quot; srcset=&quot;http://imagesvc.timeincapp.com/v3/foundry/image/?q=70&amp;amp;w=1440&amp;amp;url=https%3A%2F%2Ftimedotcom.files.wordpress.com%2F2018%2F01%2Fimg_27521.jpg%3Fquality%3D85 1x,http://imagesvc.timeincapp.com/v3/foundry/image/?q=70&amp;amp;w=1920&amp;amp;url=https%3A%2F%2Ftimedotcom.files.wordpress.com%2F2018%2F01%2Fimg_27521.jpg%3Fquality%3D85 1.5x&quot; onload=&quot;this.style.opacity=1&quot;/&gt;Alex Roy&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;has-ad-column&quot;&gt;

&lt;div class=&quot;articleFragment paragraph&quot;&gt;
&lt;p&gt;Approximately one hundred years ago, Erwin &quot;Cannonball&quot; Baker began driving cross-country, as quickly as possible, in anything he could get his hands on. His point: to demonstrate the reliability, range, and ease of refueling internal combustion cars.&lt;/p&gt;
&lt;p&gt;On Thursday, December 28th, 2017, Alex Roy joined Daniel Zorrilla, a Tesla Model 3 owner, to test the range and reliability of that vehicle—which happens to be one of the first delivered Model 3 customer cars. The pair departed the Portofino Inn in Redondo Beach, California; their final destination was the Red Ball garage in New York City. The two completed the cross-country drive in 50 hours and 16 minutes, setting a new electric Cannonball Run record.&lt;/p&gt;
&lt;p&gt;Total time: 50 hours, 16 minutes, 32 seconds&lt;br/&gt;Total mileage: 2860 miles&lt;br/&gt;Charging cost: $100.95&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;figure-wrapper&quot;&gt;&lt;img itemprop=&quot;associatedMedia&quot; class=&quot;figure-image&quot; src=&quot;http://imagesvc.timeincapp.com/v3/foundry/image/?q=60&amp;amp;url=https%3A%2F%2Fs3.amazonaws.com%2Fthe-drive-staging%2Fmessage-editor%252F1514931372860-img_3264.jpg&quot; alt=&quot;&quot;/&gt;
&lt;/div&gt;
&lt;div class=&quot;articleFragment paragraph&quot;&gt;
&lt;p&gt;Here's the timelapse and GPS track of their journey:&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div itemprop=&quot;associatedMedia&quot; class=&quot;video-youtube&quot;&gt;&lt;iframe class=&quot;video-youtube__video&quot; src=&quot;https://www.youtube.com/embed/Ylh0QyfmaYc?rel=0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/div&gt;
&lt;div class=&quot;has-ad-column&quot;&gt;

&lt;div class=&quot;articleFragment paragraph&quot;&gt;
&lt;p&gt;A detailed article will follow soon.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;section class=&quot;related-content related-content--rich&quot;&gt;&lt;div class=&quot;title-message&quot;&gt;MORE TO READ&lt;/div&gt;
&lt;ul&gt;&lt;li class=&quot;related-article&quot;&gt;

&lt;div class=&quot;title&quot;&gt;Eleven Worst Cars For A Cannonball Run Record Attempt&lt;/div&gt;
&lt;div class=&quot;dek&quot;&gt;These will take you straight to jail. Or a mechanic. Or worse.&lt;/div&gt;

&lt;/li&gt;
&lt;li class=&quot;related-article&quot;&gt;

&lt;div class=&quot;title&quot;&gt;The Coldest Cannonball Run Record. On Record.&lt;/div&gt;
&lt;div class=&quot;dek&quot;&gt;An icepick in the face of commoditized gravitas.&lt;/div&gt;

&lt;/li&gt;
&lt;li class=&quot;related-article&quot;&gt;

&lt;div class=&quot;title&quot;&gt;Dear Elon Musk: You Need Me For the Self-Driving Tesla Cannonball Run&lt;/div&gt;
&lt;div class=&quot;dek&quot;&gt;Extraordinary claims require extraordinary evidence.&lt;/div&gt;

&lt;/li&gt;
&lt;li class=&quot;related-article&quot;&gt;

&lt;div class=&quot;title&quot;&gt;Cannonball Run Founder Dies, New “Cannonball Run” Spits on His Grave&lt;/div&gt;
&lt;div class=&quot;dek&quot;&gt;Legal Car Rally Trades On Legendary Illegal Race, For Profit&lt;/div&gt;

&lt;/li&gt;
&lt;li class=&quot;related-article&quot;&gt;

&lt;div class=&quot;title&quot;&gt;Anything Can Happen On A Non-Stop, Low-Dollar, Faux-Cannonball Run&lt;/div&gt;
&lt;div class=&quot;dek&quot;&gt;Police, mechanical failure and fatigue await would-be Cannonballers.&lt;/div&gt;

&lt;/li&gt;
&lt;li class=&quot;nativo-related-ad&quot;&gt;

&lt;/li&gt;
&lt;li class=&quot;nativo-related-ad&quot;&gt;

&lt;/li&gt;
&lt;/ul&gt;&lt;/section&gt;

</description>
<pubDate>Wed, 03 Jan 2018 12:22:32 +0000</pubDate>
<dc:creator>espo</dc:creator>
<og:type>article</og:type>
<og:title>Tesla Model 3 Sets New EV Cannonball Run Record of 50 Hours, 16 Minutes</og:title>
<og:description>Alex Roy joined owner Dan Zorrilla in one of the first customer cars off the assembly line.</og:description>
<og:image>https://timedotcom.files.wordpress.com/2018/01/img_27521.jpg?quality=85</og:image>
<og:url>http://www.thedrive.com/new-cars/17312/tesla-model-3-sets-new-ev-cannonball-run-record-of-50-hours-16-minutes</og:url>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.thedrive.com/new-cars/17312/tesla-model-3-sets-new-ev-cannonball-run-record-of-50-hours-16-minutes</dc:identifier>
</item>
<item>
<title>The ‘app’ you can’t trash: how SIP is broken in High Sierra</title>
<link>https://eclecticlight.co/2018/01/02/the-app-you-cant-trash-how-sip-is-broken-in-high-sierra/</link>
<guid isPermaLink="true" >https://eclecticlight.co/2018/01/02/the-app-you-cant-trash-how-sip-is-broken-in-high-sierra/</guid>
<description>&lt;p&gt;When you install something, you expect to be able to remove it too. But when a reader came to uninstall BlueStacks, an Android emulator, from his Mac running High Sierra 10.13.2, he found the way blocked. The Finder kindly informed him that “The operation can’t be completed because you don’t have the necessary permission.”&lt;/p&gt;
&lt;p&gt;The moment that we see the word &lt;em&gt;permission,&lt;/em&gt; all becomes clear: it’s a permissions problem. So the next step is to select the offending item in the Finder, press Command-I to bring up the Get Info dialog, and change the permissions. It does, though, leave the slight puzzle as to why the Finder didn’t simply prompt for authentication instead of cussedly refusing.&lt;/p&gt;
&lt;p&gt;Sure enough, after trying that, the app still won’t go and the error message is unchanged.&lt;/p&gt;
&lt;p&gt;Another strange thing about this ‘app’ is that it’s not an app at all. Tucked away in a mysterious folder, new to High Sierra, in /Library/StagedExtensions/Applications, its icon is defaced to indicate that the user can’t even run it. Neither did the user install it there.&lt;/p&gt;
&lt;p&gt;Trying to remove it using a conventional Terminal command&lt;br/&gt;&lt;code&gt;sudo rm -rf /Library/StagedExtensions/Applications/BlueStacks.app&lt;/code&gt;&lt;br/&gt;also fails, with the report &lt;code&gt;Operation not permitted&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;High Sierra leaves the user wondering what has happened. There’s nothing in Apple’s scant documentation to explain how this strange situation has arisen, and seemingly nothing more that the user can do to discover what is wrong, or to do anything about it.&lt;/p&gt;
&lt;p&gt;The clue comes from probing around in Terminal, specifically using a command like&lt;br/&gt;&lt;code&gt;ls -lO /Library&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Try that in High Sierra, and you’ll see&lt;br/&gt;&lt;code&gt;drwxr-xr-x@ 4 root wheel restricted 128 2 Jan 13:03 StagedExtensions&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;There are two relevant pieces of information revealed: the @ sign shows that directory has extended attributes (xattrs), and the word &lt;code&gt;restricted&lt;/code&gt; that it is protected by System Integrity Protection (SIP). A quick peek inside /Library/StagedExtensions/Applications/BlueStacks.app shows that it is a stub of an app, lacking any main code, but it does contain a kernel extension (KEXT) which is also protected by SIP, by virtue of being inside a SIP-protected folder.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;&amp;gt; ls -lO /Library/StagedExtensions/Applications&lt;br/&gt;drwxr-xr-x 3 root wheel restricted 96 2 Jan 13:03 BlueStacks.app&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;So how did this third-party kernel extension end up in this mysterious folder, complete with SIP protection? Surely SIP is there to protect macOS, not third-party app components installed later by the user? Who or what enabled SIP on that extension, and how can it be removed?&lt;/p&gt;
&lt;p&gt;Perhaps not unsurprisingly, even Apple’s &lt;a href=&quot;https://developer.apple.com/library/content/technotes/tn2459/_index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;developer documentation&lt;/a&gt; doesn’t seem to answer any of those questions. So here is what I have been able to discover.&lt;/p&gt;
&lt;p&gt;High Sierra has a new mechanism for handling third-party kernel extensions (User-Approved Kernel Extension Loading, or UAKL), which requires the user to authorise them. When a third-party installer tries to install a kernel extension, you see the warning&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;alignnone&quot;&gt;&lt;img data-attachment-id=&quot;30789&quot; data-permalink=&quot;https://eclecticlight.co/sipperms01/&quot; data-orig-file=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms01.jpg?w=940&quot; data-orig-size=&quot;422,174&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;sipperms01&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms01.jpg?w=940?w=300&quot; data-large-file=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms01.jpg?w=940?w=422&quot; class=&quot;alignnone size-full wp-image-30789&quot; src=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms01.jpg?w=940&quot; alt=&quot;sipperms01&quot; srcset=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms01.jpg 422w, https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms01.jpg?w=150 150w, https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms01.jpg?w=300 300w&quot; sizes=&quot;(max-width: 422px) 100vw, 422px&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Assuming that you open Security preferences, you will there click on the &lt;strong&gt;Allow&lt;/strong&gt; button to permit the extension to be loaded.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;alignnone&quot;&gt;&lt;img data-attachment-id=&quot;30790&quot; data-permalink=&quot;https://eclecticlight.co/sipperms02/&quot; data-orig-file=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms02.jpg?w=940&quot; data-orig-size=&quot;668,574&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;sipperms02&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms02.jpg?w=940?w=300&quot; data-large-file=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms02.jpg?w=940?w=668&quot; class=&quot;alignnone size-full wp-image-30790&quot; src=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms02.jpg?w=940&quot; alt=&quot;sipperms02&quot; srcset=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms02.jpg 668w, https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms02.jpg?w=150 150w, https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms02.jpg?w=300 300w&quot; sizes=&quot;(max-width: 668px) 100vw, 668px&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;High Sierra then packages the extension in the form of a non-executable stub app, which it installs in /Library/StagedExtensions/Applications. What you see there looks like a mutated form of the app.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;alignnone&quot;&gt;&lt;img data-attachment-id=&quot;30791&quot; data-permalink=&quot;https://eclecticlight.co/sipperms03/&quot; data-orig-file=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms03.jpg?w=940&quot; data-orig-size=&quot;150,125&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;sipperms03&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms03.jpg?w=940?w=150&quot; data-large-file=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms03.jpg?w=940?w=150&quot; class=&quot;alignnone size-full wp-image-30791&quot; src=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms03.jpg?w=940&quot; alt=&quot;sipperms03&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When you then try to remove the app proper, you and it will both think that it has gone for good.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;alignnone&quot;&gt;&lt;img data-attachment-id=&quot;30792&quot; data-permalink=&quot;https://eclecticlight.co/sipperms04/&quot; data-orig-file=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms04.jpg?w=940&quot; data-orig-size=&quot;422,139&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;sipperms04&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms04.jpg?w=940?w=300&quot; data-large-file=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms04.jpg?w=940?w=422&quot; class=&quot;alignnone size-full wp-image-30792&quot; src=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms04.jpg?w=940&quot; alt=&quot;sipperms04&quot; srcset=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms04.jpg 422w, https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms04.jpg?w=150 150w, https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms04.jpg?w=300 300w&quot; sizes=&quot;(max-width: 422px) 100vw, 422px&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But in truth, its kernel extension has been left in /Library/StagedExtensions/Applications/, looking just like an app.&lt;/p&gt;
&lt;p&gt;In its infinite wisdom, Apple has given the folder /Library/StagedExtensions the full protection of SIP, by attaching a com.apple.rootless xattr to it.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;alignnone&quot;&gt;&lt;img data-attachment-id=&quot;30793&quot; data-permalink=&quot;https://eclecticlight.co/sipperms05/&quot; data-orig-file=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms05.jpg?w=940&quot; data-orig-size=&quot;479,366&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;sipperms05&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms05.jpg?w=940?w=300&quot; data-large-file=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms05.jpg?w=940?w=479&quot; class=&quot;alignnone size-full wp-image-30793&quot; src=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms05.jpg?w=940&quot; alt=&quot;sipperms05&quot; srcset=&quot;https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms05.jpg 479w, https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms05.jpg?w=150 150w, https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms05.jpg?w=300 300w&quot; sizes=&quot;(max-width: 479px) 100vw, 479px&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;My reading of that xattr is that only Apple’s KernelExtensionManagement service can give permission for changes to be made within that folder, and the folders within it.&lt;/p&gt;
&lt;p&gt;So now the user cannot touch that residual extension, and they certainly can’t uninstall, move, or trash it. Until the user can gain access to that volume with its SIP inactive, that stub app and the extension inside it stay put. It has been suggested that macOS automatically cleans /Library/StagedExtensions, although I have yet to see any evidence of that occurring. Thus SIP prevents the user from uninstalling a third-party app which the user installed, even though the kernel extension might be rendering macOS unstable, or have other significant side-effects.&lt;/p&gt;
&lt;p&gt;The solution is to restart in Recovery mode, and delete the stub app using Terminal there, with a command like&lt;br/&gt;&lt;code&gt;rm -rf /Volumes/Macintosh\ HD/Library/StagedExtensions/Applications/BlueStacks.app&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You don’t need to alter SIP there, as SIP is only applied to the startup volume. As you have now started up from the Recovery volume, SIP no longer protects the contents of your normal startup volume.&lt;/p&gt;
&lt;p&gt;This is such a good piece of security that, when some malware does manage to slip an evil kernel extension past a user and is rewarded with the protection of SIP, neither the user nor any anti-malware tool will be able to remove that extension, unless the user restarts from a different boot volume, or KernelExtensionManagement allows it.&lt;/p&gt;
&lt;p&gt;Unless I’m missing something here, this doesn’t seem particularly good.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Thanks to @Roller_ for the novel problem.)&lt;/em&gt;&lt;/p&gt;
&lt;div id=&quot;jp-post-flair&quot; class=&quot;sharedaddy sharedaddy-dark sd-like-enabled sd-sharing-enabled&quot;&gt;
&lt;div class=&quot;sharedaddy sd-sharing-enabled&quot;&gt;
&lt;div class=&quot;robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing&quot;&gt;
&lt;h3 class=&quot;sd-title&quot;&gt;Share this:&lt;/h3&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded&quot; id=&quot;like-post-wrapper-83108039-30787-5a4d6e8342e32&quot; data-src=&quot;//widgets.wp.com/likes/#blog_id=83108039&amp;amp;post_id=30787&amp;amp;origin=eclecticlightdotcom.wordpress.com&amp;amp;obj_id=83108039-30787-5a4d6e8342e32&quot; data-name=&quot;like-post-frame-83108039-30787-5a4d6e8342e32&quot;&gt;
&lt;h3 class=&quot;sd-title&quot;&gt;Like this:&lt;/h3&gt;
&lt;div class=&quot;likes-widget-placeholder post-likes-widget-placeholder&quot;&gt;&lt;span class=&quot;button&quot;&gt;&lt;span&gt;Like&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;loading&quot;&gt;Loading...&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
</description>
<pubDate>Wed, 03 Jan 2018 08:25:54 +0000</pubDate>
<dc:creator>0x0</dc:creator>
<og:type>article</og:type>
<og:title>The ‘app’ you can’t trash: how SIP is broken in High Sierra</og:title>
<og:url>https://eclecticlight.co/2018/01/02/the-app-you-cant-trash-how-sip-is-broken-in-high-sierra/</og:url>
<og:description>When you can’t remove an app which you installed just a few minutes ago, you know there’s something wrong. Here’s the solution, and why this is bad security.</og:description>
<og:image>https://eclecticlightdotcom.files.wordpress.com/2018/01/sipperms05.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://eclecticlight.co/2018/01/02/the-app-you-cant-trash-how-sip-is-broken-in-high-sierra/</dc:identifier>
</item>
<item>
<title>Why the U.S. Spends So Much More Than Other Nations on Health Care</title>
<link>https://www.nytimes.com/2018/01/02/upshot/us-health-care-expensive-country-comparison.html</link>
<guid isPermaLink="true" >https://www.nytimes.com/2018/01/02/upshot/us-health-care-expensive-country-comparison.html</guid>
<description>&lt;div&gt;
&lt;div id=&quot;top-wrapper&quot; class=&quot;ResponsiveAd-topAd--3uZAv&quot;&gt;
&lt;div id=&quot;top-slug&quot; class=&quot;ResponsiveAd-hide--3eu5-&quot;&gt;
&lt;p&gt;Advertisement&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;The United States spends almost twice as much on health care, as a percentage of its economy, as other advanced industrialized countries — totaling &lt;a rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot; href=&quot;https://www.healthaffairs.org/doi/abs/10.1377/hlthaff.2017.1299&quot; title=&quot;&quot; class=&quot;styles-link--1Tap3&quot;&gt;$3.3 trillion, or 17.9 percent&lt;/a&gt; of gross domestic product in 2016.&lt;/p&gt;
&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;But a few decades ago American health care spending &lt;a rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot; href=&quot;http://www.commonwealthfund.org/publications/issue-briefs/2015/oct/us-health-care-from-a-global-perspective&quot; title=&quot;&quot; class=&quot;styles-link--1Tap3&quot;&gt;was much closer&lt;/a&gt; to that of peer nations.&lt;/p&gt;
&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;What happened?&lt;/p&gt;
&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;A large part of the answer can be found in the title of a 2003 paper in Health Affairs by the Princeton University health economist Uwe Reinhardt: “&lt;a rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot; href=&quot;https://www.healthaffairs.org/doi/abs/10.1377/hlthaff.22.3.89&quot; title=&quot;&quot; class=&quot;styles-link--1Tap3&quot;&gt;It’s the prices, stupid.&lt;/a&gt;”&lt;/p&gt;
&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;The study, also written by Gerard Anderson, Peter Hussey and Varduhi Petrosyan, found that people in the United States typically use about the same amount of health care as people in other wealthy countries do, but pay a lot more for it.&lt;/p&gt;
&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;Ashish Jha, a physician with the Harvard T.H. Chan School of Public Health and the director of the Harvard Global Health Institute, studies how health systems from various countries compare in terms of prices and health care use. “What was true in 2003 remains so today,” he said. “The U.S. just isn’t that different from other developed countries in how much health care we use. It is very different in how much we pay for it.”&lt;/p&gt;

&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;A recent study in JAMA by scholars from the Institute for Health Metrics and Evaluation in Seattle and the U.C.L.A. David Geffen School of Medicine also points to prices as a likely culprit. &lt;a rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot; href=&quot;https://jamanetwork.com/journals/jama/fullarticle/2661579&quot; title=&quot;&quot; class=&quot;styles-link--1Tap3&quot;&gt;Their study&lt;/a&gt; spanned 1996 to 2013 and analyzed U.S. personal health spending by the size of the population; its age; and the amount of disease present in it.&lt;/p&gt;
&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;They also examined how much health care we use in terms of such things as doctor visits, days in the hospital and prescriptions. They looked at what happens during those visits and hospital stays (called care intensity), combined with the price of that care.&lt;/p&gt;
&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;The researchers looked at the breakdown for 155 different health conditions separately. Since their data included only personal health care spending, it did not account for spending in the health sector not directly attributed to care of patients, like hospital construction and administrative costs connected to running Medicaid and Medicare.&lt;/p&gt;
&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;Over all, the researchers found that American personal health spending grew by about $930 billion between 1996 and 2013, from $1.2 trillion to $2.1 trillion (amounts adjusted for inflation). This was a huge increase, far outpacing overall economic growth. The health sector grew at a 4 percent annual rate, while the overall economy grew at a 2.4 percent rate.&lt;/p&gt;
&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;You’d expect some growth in health care spending over this span from the increase in population size and the aging of the population. But that explains less than half of the spending growth. After accounting for those kinds of demographic factors, which we can do very little about, health spending still grew by about $574 billion from 1996 to 2013.&lt;/p&gt;
&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;Did the increasing sickness in the American population explain much of the rest of the growth in spending? Nope. Measured by how much we spend, we’ve actually gotten a bit healthier. Change in health status was associated with a decrease in health spending — 2.4 percent — not an increase. A great deal of this decrease can be attributed to factors related to cardiovascular diseases, which were associated with about a 20 percent reduction in spending.&lt;/p&gt;
&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;This could be a result of greater use of statins for cholesterol or reduced smoking rates, though the study didn’t point to specific causes. On the other hand, increases in diabetes and low back and neck pain were associated with spending growth, but not enough to offset the decrease from cardiovascular and other diseases.&lt;/p&gt;
&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;Did we spend more time in the hospital? No, though we did have more doctor visits and used more prescription drugs. These tend to be less costly than hospital stays, so, on balance, changes in health care use were associated with a minor reduction (2.5 percent) in health care spending.&lt;/p&gt;
&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;That leaves what happens during health care visits and hospital stays (care intensity) and the price of those services and procedures.&lt;/p&gt;
&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;Did we do more for patients in each health visit or inpatient stay? Did we charge more? The JAMA study found that, together, these accounted for 63 percent of the increase in spending from 1996 to 2013. In other words, most of the explanation for American health spending growth — and why it has pulled away from health spending in other countries — is that more is done for patients during hospital stays and doctor visits, they’re charged more per service, or both.&lt;/p&gt;
&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;Though the JAMA study could not separate care intensity and price, other research blames prices more. For example, &lt;a rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot; href=&quot;http://www.nber.org/papers/w23117&quot; title=&quot;&quot; class=&quot;styles-link--1Tap3&quot;&gt;one study&lt;/a&gt; found that the spending growth for treating patients between 2003 and 2007 is almost entirely because of a growth in prices, with little contribution from growth in the quantity of treatment services provided. &lt;a rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot; href=&quot;http://www.oecd-ilibrary.org/social-issues-migration-health/comparing-price-levels-of-hospital-services-across-countries_5km91p4f3rzw-en&quot; title=&quot;&quot; class=&quot;styles-link--1Tap3&quot;&gt;Another study&lt;/a&gt; found that U.S. hospital prices are 60 percent higher than those in Europe. &lt;a rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot; href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/24219951&quot; title=&quot;&quot; class=&quot;styles-link--1Tap3&quot;&gt;Other studies&lt;/a&gt; also &lt;a rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot; href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/23070492&quot; title=&quot;&quot; class=&quot;styles-link--1Tap3&quot;&gt;point to prices&lt;/a&gt; as a major factor in American health care spending growth.&lt;/p&gt;
&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;There are ways to combat high health care prices. One is an &lt;a rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot; href=&quot;https://www.washingtonpost.com/blogs/wonkblog/post/all-payer-rate-setting-and-health-reforms-underpants-gnomes-strategy/2011/06/02/AG3SfHHH_blog.html?utm_term=.1c3a6c90c415&quot; title=&quot;&quot; class=&quot;styles-link--1Tap3&quot;&gt;all-payer system&lt;/a&gt;, like that seen in Maryland. This regulates prices so that all insurers and public programs pay the same amount. A single-payer system could also regulate prices. If attempted nationally, or even in a state, either of these would be met with resistance from all those who directly benefit from high prices, including physicians, hospitals, pharmaceutical companies — and pretty much every other provider of health care in the United States.&lt;/p&gt;
&lt;p class=&quot;Paragraph-paragraph--2eXNE elementStyles-paragraph--3EIcW elementStyles-toneNews--sRTft&quot;&gt;Higher prices aren’t all bad for consumers. They probably lead to &lt;a rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot; href=&quot;https://www.nytimes.com/2017/10/09/upshot/can-the-us-repair-its-health-care-while-keeping-its-innovation-edge.html?_r=0&quot; title=&quot;&quot; class=&quot;styles-link--1Tap3&quot;&gt;some increased innovation&lt;/a&gt;, which confers benefits to patients globally. Though it’s reasonable to push back on high health care prices, there may be a limit to how far we should.&lt;/p&gt;
&lt;div class=&quot;bottom-of-article&quot; readability=&quot;25.112454655381&quot;&gt;

&lt;div class=&quot;elementStyles-bios--3rrhp elementStyles-toneNews--sRTft&quot; readability=&quot;7.8056426332288&quot;&gt;
&lt;div readability=&quot;10.036723163842&quot;&gt;
&lt;p&gt;Austin Frakt is director of the Partnered Evidence-Based Policy Resource Center at the V.A. Boston Healthcare System; associate professor with Boston University’s School of Public Health; and adjunct associate professor with the Harvard T.H. Chan School of Public Health. He blogs at &lt;a href=&quot;https://theincidentaleconomist.com/&quot;&gt;The Incidental Economist&lt;/a&gt;, and you can follow him on Twitter. &lt;a class=&quot;elementStyles-biolink--aF6NI&quot; target=&quot;_blank&quot; href=&quot;https://twitter.com/afrakt&quot;&gt;@afrakt&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;


&lt;/div&gt;

&lt;div id=&quot;bottom-wrapper&quot; class=&quot;ResponsiveAd-bottomAd--1hHJW ResponsiveAd-storyBodyAd--35v2w&quot;&gt;
&lt;div id=&quot;bottom-slug&quot; class=&quot;ResponsiveAd-adSlug--3H3QM&quot;&gt;
&lt;p&gt;Advertisement&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
</description>
<pubDate>Wed, 03 Jan 2018 07:38:40 +0000</pubDate>
<dc:creator>yeukhon</dc:creator>
<og:url>https://www.nytimes.com/2018/01/02/upshot/us-health-care-expensive-country-comparison.html</og:url>
<og:type>article</og:type>
<og:title>Why the U.S. Spends So Much More Than Other Nations on Health Care</og:title>
<og:image>https://static01.nyt.com/images/2018/01/02/upshot/03up-healthspend/03up-healthspend-facebookJumbo-v4.jpg</og:image>
<og:description>Studies point to a simple reason, the prices, not to the amount of care. And lowering prices would upset a lot of people in the health industry.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nytimes.com/2018/01/02/upshot/us-health-care-expensive-country-comparison.html</dc:identifier>
</item>
</channel>
</rss>