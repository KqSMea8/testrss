<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Senate votes to reinstate net neutrality</title>
<link>https://www.theverge.com/2018/5/16/17357592/net-neutrality-senate-vote-cra-reinstate-fcc-rules</link>
<guid isPermaLink="true" >https://www.theverge.com/2018/5/16/17357592/net-neutrality-senate-vote-cra-reinstate-fcc-rules</guid>
<description>&lt;p id=&quot;15Fqhn&quot;&gt;The Senate has voted to save net neutrality, but don’t get your hopes up: there’s still a long, likely impossible journey ahead if the policy is to be saved in the immediate future.&lt;/p&gt;
&lt;p id=&quot;U729tV&quot;&gt;In a 52–47 vote today, senators voted to overturn the Federal Communication Commission’s Restoring Internet Freedom Order, which took net neutrality rules off the books. They were able to do so &lt;a href=&quot;https://www.theverge.com/2018/5/3/17314404/net-neutrality-cra-congressional-review-act-markey-senate&quot;&gt;using the Congressional Review Act, or CRA&lt;/a&gt;, which allows Congress to reverse recent decisions by government agencies. Republican control of Congress means that such a measure wouldn’t normally even make it up for a vote; but the CRA allows senators to force a vote by obtaining 30 signatures.&lt;/p&gt;
&lt;p id=&quot;gaB3BR&quot;&gt;All 49 Democrats voted in favor, as well as Republican Senators Susan Collins, of Maine; John Kennedy, of Louisiana; and Lisa Murkowski, of Alaska.&lt;/p&gt;
&lt;div class=&quot;c-float-right&quot;&gt;
&lt;aside id=&quot;vPlfiO&quot;&gt;&lt;q&gt;The House needs a full majority to force a vote; then there’s Trump&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id=&quot;nrk6sD&quot;&gt;While advocates have argued that this is a step toward reinstating net neutrality, it’s really a long-shot attempt that seems to be meant more to get the issue back on voters’ minds — and to force politicians to take a position ahead of what’s expected to be a tumultuous midterm election.&lt;/p&gt;
&lt;p id=&quot;XR2gWL&quot;&gt;In order for net neutrality to actually be reinstated, two more things have to happen. First, the House has to use the CRA to overturn the policy as well. That’s even harder. Instead of 30 signatures, net neutrality supporters have to collect signatures from a full majority of House members. Even if they get every single Democrat on board — and they don’t have that yet — they’d still need the support of 22 Republicans. And finally, if that happened and they all voted to reverse the policy, it’d still have to get signed by President Trump, who is not a fan of the policy.&lt;/p&gt;
&lt;p id=&quot;pn0wQd&quot;&gt;While it’s obviously an uphill battle, net neutrality advocates seem to be holding out hope that they could actually get through the House, too. There’s a degree of bipartisan agreement that something needs to be done on net neutrality. And with midterms coming up, representatives in challenging districts may be more inclined to support the popular, consumer-friendly policy. As for Trump, well, you never know exactly how he’s going to wake up each day, or so the argument goes.&lt;/p&gt;
&lt;p id=&quot;cJkDhk&quot;&gt;In reality, this is more about setting up whatever comes next for net neutrality, likely a few years down the line. The general consensus at this point is that net neutrality is now out of the FCC’s hands, and that Congress will have to act to reinstate some of its outgoing rules. It’s not at all clear how soon that’ll happen, but forcing Congress to take a vote helps to clarify the playing field and make sure it’s something legislators are thinking about.&lt;/p&gt;
</description>
<pubDate>Wed, 16 May 2018 20:06:03 +0000</pubDate>
<dc:creator>kposehn</dc:creator>
<og:description>Next stop, the House</og:description>
<og:image>https://cdn.vox-cdn.com/thumbor/P2hEoHE1iBY_6bGnCX4bnQRAHTI=/0x648:5184x3362/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/10845751/951573798.jpg.jpg</og:image>
<og:title>Senate votes to reinstate net neutrality — but it has a long way to go</og:title>
<og:type>article</og:type>
<og:url>https://www.theverge.com/2018/5/16/17357592/net-neutrality-senate-vote-cra-reinstate-fcc-rules</og:url>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.theverge.com/2018/5/16/17357592/net-neutrality-senate-vote-cra-reinstate-fcc-rules</dc:identifier>
</item>
<item>
<title>The Entire Economy Is MoviePass Now</title>
<link>https://www.nytimes.com/2018/05/16/technology/moviepass-economy-startups.html</link>
<guid isPermaLink="true" >https://www.nytimes.com/2018/05/16/technology/moviepass-economy-startups.html</guid>
<description>&lt;p class=&quot;css-1cy1v93 e2kc3sl0&quot;&gt;Perhaps the buzziest money-loser of the year is MoviePass, which has upended the film industry by essentially giving away millions of free movie tickets. Until recently, MoviePass members could pay $9.95 for a monthly subscription that allowed them to watch up to one movie per day in theaters, with MoviePass paying the face value of the ticket on a preloaded debit card. Since the average cost of a movie ticket in the United States is around $9, going to just two movies per month resulted in a good deal for the customer, and a loss for the company. (MoviePass has started &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.theverge.com/2018/4/27/17291242/moviepass-unlimited-movie-deal-repeat-viewings-theater-blackouts&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;placing more restrictions&lt;/a&gt; on which films its customers can see, perhaps in an effort to trim costs.)&lt;/p&gt;
&lt;p class=&quot;css-1cy1v93 e2kc3sl0&quot;&gt;MoviePass’s business model — which &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://slate.com/business/2018/05/moviepass-appears-to-be-running-very-low-on-money.html&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;Slate described&lt;/a&gt; as “creatively lighting money aflame in order to subsidize the movie-going habits of some 3 million customers” — has turbocharged its growth. And the company maintains that it can make money by striking revenue-sharing deals with theater chains, or charging movie studios to advertise inside its app.&lt;/p&gt;
&lt;p class=&quot;css-1cy1v93 e2kc3sl0&quot;&gt;But investors aren’t convinced. Shares of MoviePass’s parent company, Helios and Matheson Analytics, have fallen more than 90 percent since October, and the company recently reported that it has been burning through its cash reserves, spending an average of $21.7 million per month with just $15.5 million left in the bank at the end of April. On Tuesday, Helios reported that MoviePass lost $98.3 million in the first quarter, despite adding more than a million net subscribers.&lt;/p&gt;
&lt;p class=&quot;css-1cy1v93 e2kc3sl0&quot;&gt;Mitch Lowe, the chief executive of MoviePass, told me in a phone interview this week that the company’s financial troubles have been exaggerated. The company has access to a $300 million equity line of credit that will keep it solvent, he said, and blamed the company’s competitors, such as large theater chains, for sowing the seeds of doubt.&lt;/p&gt;
&lt;p class=&quot;css-1cy1v93 e2kc3sl0&quot;&gt;“They smell blood in the water, so they’re spreading rumors and hypotheses,” he said.&lt;/p&gt;
&lt;p class=&quot;css-1cy1v93 e2kc3sl0&quot;&gt;Ultimately, companies like MoviePass illustrate the perilous tightrope many growing businesses must walk. Spend too little on acquiring new customers and drawing business away from your competitors, and you won’t make it off the ground. Give too many freebies away, and you risk running out of cash before you’re big enough to cash in.&lt;/p&gt;
&lt;p class=&quot;css-1cy1v93 e2kc3sl0&quot;&gt;“Pricing can be strategic,” said Kara Nortman, a partner at Upfront Ventures, which invests in technology companies. “If you can attract a lot of consumers to your product or service, it gives you a lot more power with incumbents who are limiting your growth.”&lt;/p&gt;
&lt;p class=&quot;css-1cy1v93 e2kc3sl0&quot;&gt;The king of money-losers, of course, is Amazon, which went years without turning a profit. Instead, it plowed billions of dollars back into its business, building out its e-commerce infrastructure and jump-starting side efforts like Amazon Web Services and Amazon Prime Video. Those years of investments paid off, and Amazon is now the second most valuable company in the world, with $1.6 billion in profit last quarter alone.&lt;/p&gt;
&lt;p class=&quot;css-1cy1v93 e2kc3sl0&quot;&gt;Not every company can repeat Amazon’s success. Just ask any of the dozens of “Uber for X” start-ups that raised millions of dollars to disrupt industries like laundry, parking and grocery delivery by offering cut-rate promotional deals, only to &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.nytimes.com/2016/03/24/technology/the-uber-model-it-turns-out-doesnt-translate.html&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;run out of capital&lt;/a&gt; before customers latched on. Or consider crash-and-burn cases like Beepi, a used car marketplace that blew through nearly $150 million in venture capital before &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://techcrunch.com/2016/12/07/used-car-marketplace-beepi-shuts-down-outside-of-ca-merges-with-stealth-fair-com/&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;shutting down in 2016&lt;/a&gt;. (Happily, not before &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://splinternews.com/i-bought-a-car-from-the-uber-for-used-cars-start-up-1793845289&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;I bought a car through the service&lt;/a&gt; for thousands of dollars less than its market value. Thanks, venture capitalists!)&lt;/p&gt;
</description>
<pubDate>Wed, 16 May 2018 19:16:55 +0000</pubDate>
<dc:creator>jds375</dc:creator>
<og:url>https://www.nytimes.com/2018/05/16/technology/moviepass-economy-startups.html</og:url>
<og:type>article</og:type>
<og:title>The Entire Economy Is MoviePass Now. Enjoy It While You Can.</og:title>
<og:image>https://static01.nyt.com/images/2018/05/17/business/17Roose.print/merlin_138197655_676241b8-67e3-4499-b081-510bac0cfcca-facebookJumbo.jpg</og:image>
<og:description>Inspired by Silicon Valley’s hyper-growth, companies elsewhere are burning cash in hopes of being the next big thing.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nytimes.com/2018/05/16/technology/moviepass-economy-startups.html</dc:identifier>
</item>
<item>
<title>HoweyCoins – An educational tool to alert investors to possible fraud</title>
<link>https://www.howeycoins.com/index.html</link>
<guid isPermaLink="true" >https://www.howeycoins.com/index.html</guid>
<description>&lt;h3&gt;INVESTMENT LADDER&lt;/h3&gt;
&lt;p class=&quot;lead mb-0&quot;&gt;Investors can purchase HoweyCoins with any major credit card, widely-circulated coin, or with TravExcoins, our exclusive e-commerce partner in the travel and luxury goods investment area. Investment Discounting Ladder:&lt;/p&gt;
&lt;p class=&quot;lead mb-0&quot;&gt;HoweyCoins platform stands as one of the largest cryptocurrency platforms ever built. Recent market surveys expect the luxury travel industry to set a world-record high of over $1.5 trillion this year. The vast majority of these business and vacation transactions require processing, centralized currency and, most importantly, nickel and dime fees that add up to literally billions. HoweyCoins utilize the latest crypto-technology to allow travelers to purchase all segments without these limitations, allowing HoweyCoin users to buy, sell, and trade in a frictionless environment – where they use HoweyCoins to purchase travel OR as a government-backed, freely tradable investment – or both!&lt;/p&gt;
</description>
<pubDate>Wed, 16 May 2018 18:44:23 +0000</pubDate>
<dc:creator>ChrisArchitect</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.howeycoins.com/index.html</dc:identifier>
</item>
<item>
<title>AI and Compute</title>
<link>https://blog.openai.com/ai-and-compute/?</link>
<guid isPermaLink="true" >https://blog.openai.com/ai-and-compute/?</guid>
<description>&lt;p&gt;We're releasing an analysis showing that since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with a 3.5 month-doubling time (by comparison, Moore's Law &lt;a href=&quot;https://www.nature.com/articles/s41928-017-0005-9&quot;&gt;had&lt;/a&gt; an 18-month doubling period). Since 2012, this metric has grown by more than 300,000x (an 18-month doubling period would yield only a 12x increase). Improvements in compute have been a key component of AI progress, so as long as this trend continues, it's worth preparing for the implications of systems far outside today's capabilities.&lt;/p&gt;&lt;div id=&quot;log&quot;&gt;&lt;img src=&quot;https://blog.openai.com/content/images/2018/05/compute_diagram-log@2x-3.png&quot;/&gt;&lt;/div&gt;
&lt;div id=&quot;linear&quot;&gt;&lt;img src=&quot;https://blog.openai.com/content/images/2018/05/compute_diagram-linear@2x-5.png&quot;/&gt;&lt;/div&gt;
&lt;p&gt;&lt;input type=&quot;radio&quot; class=&quot;switch-input&quot; name=&quot;view1&quot; value=&quot;log-scale&quot; id=&quot;log-scale&quot; onclick=&quot;toggle('log')&quot; checked=&quot;checked&quot;/&gt;&lt;label for=&quot;log-scale&quot; class=&quot;switch-label switch-label-off&quot;&gt;Log Scale&lt;/label&gt; &lt;input type=&quot;radio&quot; class=&quot;switch-input&quot; name=&quot;view1&quot; value=&quot;linear-scale&quot; id=&quot;linear-scale&quot; onclick=&quot;toggle('linear')&quot;/&gt;&lt;label for=&quot;linear-scale&quot; class=&quot;switch-label switch-label-on&quot;&gt;Linear Scale&lt;/label&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The chart shows the total amount of compute, in petaflop/s-days, that was used to train selected results that are relatively well known, used a lot of compute for their time, and gave enough information to estimate the compute used. A petaflop/s-day (pfs-day) consists of performing 10&lt;sup&gt;15&lt;/sup&gt; neural net operations per second for one day, or a total of about 10&lt;sup&gt;20&lt;/sup&gt; operations. The compute-time product serves as a mental convenience, similar to kW-hr for energy. We don’t measure peak theoretical FLOPS of the hardware but instead try to estimate the number of actual operations performed. We count adds and multiplies as separate operations, we count any add or multiply as a single operation regardless of numerical precision (making “FLOP” a slight misnomer), and we ignore &lt;a href=&quot;http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf&quot;&gt;ensemble models&lt;/a&gt;. Example calculations that went into this graph are provided in this &lt;a href=&quot;https://blog.openai.com/ai-and-compute/?#appendixmethods&quot;&gt;appendix&lt;/a&gt;. Doubling time for line of best fit shown is 3.43 months.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Three factors drive the advance of AI: algorithmic innovation, data (which can be either supervised data or interactive environments), and the amount of compute available for training. Algorithmic innovation and data are difficult to track, but compute is unusually quantifiable, providing an opportunity to measure one input to AI progress. Of course, the use of massive compute sometimes just exposes the shortcomings of our current algorithms. But at least within many current domains, more compute seems to lead &lt;a href=&quot;https://arxiv.org/abs/1712.00409&quot;&gt;predictably to better performance&lt;/a&gt;, and is often complementary to algorithmic advances.&lt;/p&gt;
&lt;p&gt;For this analysis, we believe the relevant number is not the speed of a single GPU, nor the capacity of the biggest datacenter, but the amount of compute that is used to train a single model — this is the number most likely to correlate to how powerful our best models are. Compute per model differs greatly from total bulk compute because &lt;a href=&quot;http://learningsys.org/nips17/assets/slides/dean-nips17.pdf&quot;&gt;limits on parallelism&lt;/a&gt; (both hardware and algorithmic) have constrained how big a model can be or how much it can be usefully trained. Of course, important breakthroughs are still made with &lt;a href=&quot;https://blog.openai.com/ai-and-compute/?#appendixrecentnovelresultsthatusedmodestamountsofcompute&quot;&gt;modest amounts&lt;/a&gt; of compute — this analysis just covers compute capability.&lt;/p&gt;
&lt;p&gt;The trend represents an increase by roughly a factor of 10 each year. It’s been partly driven by custom hardware that allows more operations to be performed per second for a given price (GPUs and TPUs), but it’s been primarily propelled by researchers repeatedly finding ways to use more chips in parallel and being willing to pay the economic cost of doing so.&lt;/p&gt;

&lt;p&gt;Looking at the graph we can roughly see four distinct eras:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Before 2012: It was uncommon to use GPUs for ML, making any of the results in the graph difficult to achieve.&lt;/li&gt;
&lt;li&gt;2012 to 2014: Infrastructure to train on many GPUs was uncommon, so most results used 1-8 GPUs rated at 1-2 TFLOPS for a total of 0.001-0.1 pfs-days.&lt;/li&gt;
&lt;li&gt;2014 to 2016: Large-scale results used 10-100 GPUs rated at 5-10 TFLOPS, resulting in 0.1-10 pfs-days. Diminishing returns on data parallelism meant that larger training runs had limited value.&lt;/li&gt;
&lt;li&gt;2016 to 2017: Approaches that allow greater algorithmic parallelism such as &lt;a href=&quot;https://arxiv.org/abs/1711.04325&quot;&gt;huge batch sizes&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1611.01578&quot;&gt;architecture search&lt;/a&gt;, and &lt;a href=&quot;https://arxiv.org/pdf/1705.08439.pdf&quot;&gt;expert iteration&lt;/a&gt;, along with specialized hardware such as TPU’s and faster interconnects, have greatly increased these limits, at least for some applications.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;AlphaGoZero/AlphaZero is the most visible public example of massive algorithmic parallelism, but many other applications at this scale are now algorithmically possible, and may already be happening in a production context.&lt;/p&gt;

&lt;p&gt;We see multiple reasons to believe that the trend in the graph could continue. Many &lt;a href=&quot;https://www.nytimes.com/2018/01/14/technology/artificial-intelligence-chip-start-ups.html&quot;&gt;hardware startups&lt;/a&gt; are developing AI-specific chips, some of which claim they will achieve a substantial increase in FLOPS/Watt (which is correlated to FLOPS/$) over the next 1-2 years. There may also be gains from simply reconfiguring hardware to do the same number of operations for &lt;a href=&quot;http://www.fast.ai/2018/04/30/dawnbench-fastai/&quot;&gt;less economic cost&lt;/a&gt;. On the parallelism side, many of the recent algorithmic innovations described above could in principle be combined multiplicatively — for example, architecture search and massively parallel SGD.&lt;/p&gt;
&lt;p&gt;On the other hand, cost will eventually limit the parallelism side of the trend and physics will limit the chip efficiency side. We believe the largest training runs today employ hardware that cost in the single digit millions of dollars to purchase (although the amortized cost is much lower). But the majority of neural net compute today is still spent on inference (deployment), not training, meaning companies can repurpose or afford to purchase much larger fleets of chips for training. Therefore, if sufficient economic incentive exists, we could see even more massively parallel training runs, and thus the continuation of this trend for several more years. The world’s total hardware budget is &lt;a href=&quot;https://www.statista.com/statistics/422802/hardware-spending-forecast-worldwide/&quot;&gt;1 trillion dollars&lt;/a&gt; a year, so absolute limits remain far away. Overall, given the data above, the precedent for exponential trends in computing, work on ML specific hardware, and the economic incentives at play, we think it’d be a mistake to be confident this trend won’t continue in the short term.&lt;/p&gt;
&lt;p&gt;Past trends are not sufficient to predict how long the trend will continue into the future, or what will happen while it continues. But even the reasonable potential for rapid increases in capabilities means it is critical to start addressing both &lt;a href=&quot;https://blog.openai.com/concrete-ai-safety-problems/&quot;&gt;safety&lt;/a&gt; and &lt;a href=&quot;https://blog.openai.com/preparing-for-malicious-uses-of-ai/&quot;&gt;malicious use of AI&lt;/a&gt; today. Foresight is essential to &lt;a href=&quot;https://oversight.house.gov/wp-content/uploads/2018/04/Clark-OpenAI-Statement-AI-III-4-18.pdf&quot;&gt;responsible policymaking&lt;/a&gt; and responsible technological development, and we must get out ahead of these trends rather than belatedly reacting to them.&lt;/p&gt;
&lt;p&gt;(If you'd like to help make sure that &lt;a href=&quot;https://blog.openai.com/openai-charter/&quot;&gt;AI progress benefits all of humanity&lt;/a&gt;, &lt;a href=&quot;https://openai.com/jobs/&quot;&gt;join us&lt;/a&gt; at OpenAI. Our research and engineering roles range from &lt;a href=&quot;https://jobs.lever.co/openai/588c1d80-4632-4d5c-a535-9f2c8c80c501&quot;&gt;machine learning researchers&lt;/a&gt; to &lt;a href=&quot;https://jobs.lever.co/openai/638c06a8-4058-4c3d-9aef-6ee0528fb3bf&quot;&gt;policy researchers&lt;/a&gt; to &lt;a href=&quot;https://jobs.lever.co/openai/f163bf64-278e-417b-ad2e-5e508a29eb71&quot;&gt;infrastructure engineers&lt;/a&gt;.)&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;Two methodologies were used to generate these data points. When we had enough information, we directly counted the number of FLOPs (adds and multiplies) in the described architecture per training example and multiplied by the total number of forward and backward passes during training. When we didn’t have enough information to directly count FLOPs, we looked GPU training time and total number of GPUs used and assumed a utilization efficiency (usually 0.33). For the majority of the papers we were able to use the first method, but for a significant minority we relied on the second, and we computed both whenever possible as a consistency check. In the majority of cases we also confirmed with the authors. The calculations are not intended to be precise but we aim to be correct within a factor 2-3. We provide some example calculations below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example of Method 1: Counting operations in the model&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This method is particularly easy to use when the authors give the number of operations used in a forward pass, as in the &lt;a href=&quot;https://arxiv.org/pdf/1409.4842.pdf&quot;&gt;Resnet paper&lt;/a&gt; (the Resnet-151 model in particular):&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;(add-multiplies per forward pass) * (2 FLOPs/add-multiply) * (3 for forward and backward pass) * (number of examples in dataset) * (number of epochs)
= (11.4 * 10^9) * 2 * 3 * (1.2 * 10^6 images) * 128
= 10,000 PF = 0.117 pfs-days
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Operations can also be counted programmatically for a known model architecture in some deep learning frameworks, or we can simply count operations manually. If a paper gives enough information to make this calculation, it will be quite accurate, but in some cases papers don’t contain all the necessary information and authors aren’t able to reveal it publicly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example of Method 2: GPU Time&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If we can’t count operations directly, we can instead look at how many GPUs were trained for how long, and use reasonable guesses at GPU utilization to try to estimate the number of operations performed. We emphasize that here we are not counting peak theoretical FLOPS, but using an assumed fraction of theoretical FLOPS to try to guess at actual FLOPS. We typically assume a 33% utilization for GPUs and a 17% utilization for CPU’s, based on our own experience, except where we have more specific information (e.g. we spoke to the author or the work was done at OpenAI).&lt;/p&gt;
&lt;p&gt;As an example, in the &lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot;&gt;AlexNet paper&lt;/a&gt; it’s stated that “our network takes between five and six days to train on two GTX 580 3GB GPUs”. Under our assumptions this implies a total compute of:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Number of GPUs * (peta-flops/GTX580) * days trained * estimated utilization
= 2 * (1.58 * 10 ^ -3 PF) * 5.5 * 0.33
= 500 PF = 0.0058 pfs-days 
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This method is more approximate and can easily be off by a factor of 2 or occasionally more; our aim is only to estimate the order of magnitude. In practice when both methods are available they often line up quite well (for AlexNet we can also directly count the operations, which gives us 0.0054 pfs-days vs 0.0058 with the GPU time method).&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;1.2M images * 90 epochs * 0.75 GFLOPS * (2 add-multiply) * (3 backward pass) 
= 470 PF = 0.0054 pfs-days
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Selected Additional Calculations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1207.0580&quot;&gt;Dropout&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Method 2:

1 GPU * 4 days * 1.54 TFLOPS/GTX 580 * 0.33 utilization 
= 184 PF = 0.0021 pfs-days
&lt;/code&gt;
&lt;/pre&gt;
&lt;hr class=&quot;appendix&quot;/&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1311.2901&quot;&gt;Visualizing and Understanding Conv Nets&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Method 2:

1 GPU * 12 days * 1.54 TFLOPS/GTX 580 * 0.33 utilization 
= 532 PF = 0.0062 pfs-days
&lt;/code&gt;
&lt;/pre&gt;
&lt;hr class=&quot;appendix&quot;/&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1312.5602&quot;&gt;DQN&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Method 1:

Network is 84x84x3 input, 16, 8x8, stride 4, 32 4x4 stride 2, 256 fully connected
First layer: 20*20*3*16*8*8 = 1.23M add-multiplies
Second layer: 9*9*16*32*4*4 = 0.66M add-multiplies
Third layer: 9*9*32*256 = 0.66M add-mutliplies
Total ~ 2.55M add-multiplies
2.5 MFLOPs * 5M updates * 32 batch size * 2 multiply-add * 3 backward pass
= 2.3 PF = 2.7e-5 pfs-days
&lt;/code&gt;
&lt;/pre&gt;
&lt;hr class=&quot;appendix&quot;/&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1409.3215&quot;&gt;Seq2Seq&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Method 1:

(348M + 304M) words * 0.380 GF * 2 add-multiply * 3 backprop * 7.5 epoch
= 7,300 PF = 0.085 pfs-days
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;Method 2:

10 days * 8 GPU’s * 3.5 TFLOPS/ K20 GPU * 0.33 utilization 
= 8,100 PF = 0.093 pfs-days
&lt;/code&gt;
&lt;/pre&gt;
&lt;hr class=&quot;appendix&quot;/&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;VGG&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Method 1:

1.2 M images * 74 epochs * 16 GFLOPS * 2 add-multiply * 3 backward pass 
= 8524 PF = 0.098 pfs-days
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;Method 2:

4 Titan Black GPU’s * 15 days * 5.1 TFLOPS/GPU * 0.33 utilization 
= 10,000 PF = 0.12 pfs-days
&lt;/code&gt;
&lt;/pre&gt;
&lt;hr class=&quot;appendix&quot;/&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.02595&quot;&gt;DeepSpeech2&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Method 1:

1 timestep = (1280 hidden units)^2 * (7 RNN layers * 4 matrices for bidirectional + 2 DNN layers) * (2 for doubling parameters from 36M to 72M) = 98 MFLOPs
20 epochs * 12,000 hours * 3600 seconds/hour * 50 samples/sec * 98 MFLOPs * 3 add-multiply * 2 backprop 
= 26,000 PF = 0.30 pfs-days
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;Method 2:

16 TitanX GPU’s * 5 days * 6 TFLOPS/GPU * 0.50 utilization 
= 21,000 PF = 0.25 pfs-days
&lt;/code&gt;
&lt;/pre&gt;
&lt;hr class=&quot;appendix&quot;/&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1610.02357&quot;&gt;Xception&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Method 2:

60 K80 GPU’s * 30 days * 8.5 TFLOPS/GPU * 0.33 utilization 
= 4.5e5 PF = 5.0 pfs-days
&lt;/code&gt;
&lt;/pre&gt;
&lt;hr class=&quot;appendix&quot;/&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.01578&quot;&gt;Neural Architecture Search&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Method 1:

50 epochs * 50,000 images * 10.0 GFLOPSs * 12800 networks * 2 add-multiply * 3 backward pass 
= 1.9e6 PF = 22 pfs-days
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;Method 2:

800 K40’s * 28 days * 4.2 TFLOPS/GPU * 0.33 utilization 
= 2.8e6 PF = 31 pfs-days
Details given in a [later paper](https://arxiv.org/pdf/1707.07012.pdf).
&lt;/code&gt;
&lt;/pre&gt;
&lt;hr class=&quot;appendix&quot;/&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.08144&quot;&gt;Neural Machine Translation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Method 2:

sqrt(10 * 100) factor added because production model used 2-3 orders of magnitude more data, but only 1 epoch rather than 10.
96 K80 GPU’s * 9 days * 8.5 TFLOPS * 0.33 utilization * sqrt(10 * 100)  
= 6.9e6 PF = 79 pfs-days
&lt;/code&gt;
&lt;/pre&gt;
&lt;hr class=&quot;appendix&quot;/&gt;
&lt;p&gt;Massive compute is certainly not a requirement to produce important results. Many recent noteworthy results have used only modest amounts of compute. Here are some examples of results using modest compute that gave enough information to estimate their compute. We didn’t use multiple methods to estimate the compute for these models, and for upper bounds we made conservative estimates around any missing information, so they have more overall uncertainty. They aren’t material to our quantitative analysis, but we still think they are interesting and worth sharing:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention is all you need:&lt;/a&gt; 0.089 pfs-days (6/2017)&lt;br/&gt;&lt;a href=&quot;https://arxiv.org/abs/1412.6980&quot;&gt;Adam Optimizer:&lt;/a&gt; less than 0.0007 pfs-days (12/2014)&lt;br/&gt;&lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;Learning to Align and Translate:&lt;/a&gt; 0.018 pfs-days (09/2014)&lt;br/&gt;&lt;a href=&quot;https://arxiv.org/abs/1406.2661&quot;&gt;GANs:&lt;/a&gt; less than 0.006 pfs-days (6/2014)&lt;br/&gt;&lt;a href=&quot;https://arxiv.org/abs/1310.4546&quot;&gt;Word2Vec:&lt;/a&gt; less than 0.00045 pfs-days (10/2013)&lt;br/&gt;&lt;a href=&quot;https://arxiv.org/abs/1312.6114&quot;&gt;Variational Auto Encoders:&lt;/a&gt; less than 0.0000055 pfs-days (12/2013)&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;em&gt;The authors thank Katja Grace, Geoffrey Irving, Jack Clark, Thomas Anthony, and Michael Page for assistance with this post.&lt;/em&gt;&lt;/p&gt;
</description>
<pubDate>Wed, 16 May 2018 16:29:41 +0000</pubDate>
<dc:creator>gdb</dc:creator>
<og:type>article</og:type>
<og:title>AI and Compute</og:title>
<og:description>Since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with a 3.5 month doubling time (by comparison, Moore's Law had an 18 month doubling period).</og:description>
<og:url>https://blog.openai.com/ai-and-compute/</og:url>
<og:image>https://blog.openai.com/content/images/2018/05/compute_diagram@2x-1.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.openai.com/ai-and-compute/?</dc:identifier>
</item>
<item>
<title>Salesforce CEO Benioff calls for national privacy law</title>
<link>https://www.salesforce.com/company/news-press/stories/2018/5/051618/</link>
<guid isPermaLink="true" >https://www.salesforce.com/company/news-press/stories/2018/5/051618/</guid>
<description>&lt;p&gt;&lt;em&gt;                    Watch Salesforce Chairman and CEO Marc Benioff's appearance on CBS This Morning.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Speaking on &lt;a href=&quot;https://www.cbsnews.com/video/salesforce-ceo-marc-benioff-calls-for-national-privacy-law/&quot;&gt;CBS This Morning&lt;/a&gt; today with Gayle King, Norah O'Donnell and John Dickerson, Salesforce Chairman and CEO Marc Benioff called for a national privacy law.&lt;/p&gt;
&lt;p&gt;“You can see that our industry is going through a very significant crisis of trust. We've seen that over the last year with Uber and Facebook and other companies,” Benioff said. “In some ways, you could say that Facebook has become the new cigarettes in our industry. That is, it's a technology that is addictive, it may not be that great for you and it might be something you don't want to go back to. Maybe it's time for the government to step in and regulate not just that product but our industry.”&lt;/p&gt;
&lt;p&gt;“We really need in this country a national privacy law,” he said. “You can see it's going into effect in Europe with GDPR. That means in Europe your data belongs to you, but in the United States, your data belongs to all these companies that are collecting it, and they can do with it basically whatever they want. That's a shift we have to make. You can see that's about to happen in California where I am from. There is a statewide privacy law that is moving its way to voters. But what we need is a national privacy law. It's not just going to protect the tech industry, it's going to protect all the consumers, and ultimately our kids, which is really what this is all about. We know that all these companies are looking to bring kids into their social networks as well.”&lt;/p&gt;
&lt;p&gt;Benioff also called for regulations to address advances in artificial intelligence. &quot;I see huge advancements that are happening every day in our industry. You could see that last week with Google Duplex. It was the most amazing AI technology I've seen. It's indistinguishable from a human being when you are talking to it. Many people in the computer industry feel that it passed the Turing Test—that means 'is that a human or a computer.' If we are at that point we have to have better regulations and controls. The Europeans understand that. It's time for Americans to understand that too.”&lt;/p&gt;
&lt;p&gt;A national privacy law would require that companies disclose how they collect your information, use your information, and offer a right-to-be-forgotten, Benioff explained. “If you want to delete your information, you could hit that button and be sure your data is gone forever.”&lt;/p&gt;
&lt;p&gt;Benioff also discussed gender equality and the role of chief executives. “CEOs need to be held accountable for making sure that they are equally paying men and women,” he said. “It's also about preventing sexual harassment. You can't have gender equality without knowing you have psychological safety in your workplace.”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Related Stories:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.salesforce.com/company/news-press/stories/2018/5/051418/&quot;&gt;Salesforce's GDPR Journey&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.salesforce.com/company/news-press/stories/2018/5/051418-a/&quot;&gt;Q&amp;amp;A: Salesforce's Data Protection Officer on Trust, GDPR and How Privacy Found Her&lt;/a&gt;&lt;/p&gt;


</description>
<pubDate>Wed, 16 May 2018 16:28:19 +0000</pubDate>
<dc:creator>jeffthechimp</dc:creator>
<og:url>https://www.salesforce.com/company/news-press/stories/2018/5/051618/</og:url>
<og:type>website</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.salesforce.com/company/news-press/stories/2018/5/051618/</dc:identifier>
</item>
<item>
<title>The sad state of sysadmin in the age of containers (2015)</title>
<link>https://www.vitavonni.de/blog/201503/2015031201-the-sad-state-of-sysadmin-in-the-age-of-containers.html</link>
<guid isPermaLink="true" >https://www.vitavonni.de/blog/201503/2015031201-the-sad-state-of-sysadmin-in-the-age-of-containers.html</guid>
<description>&lt;p&gt;System administration is in a sad state. It in a mess.&lt;/p&gt;
&lt;p&gt;I’m not complaining about old-school sysadmins. They know how to keep systems running, manage update and upgrade paths.&lt;/p&gt;
&lt;p&gt;This rant is about containers, prebuilt VMs, and the incredible mess they cause because their concept lacks notions of “trust” and “upgrades”.&lt;/p&gt;
&lt;p&gt;Consider for example Hadoop. &lt;strong&gt;Nobody seems to know how to build Hadoop from scratch.&lt;/strong&gt; It’s an incredible mess of dependencies, version requirements and build tools.&lt;/p&gt;
&lt;p&gt;None of these “fancy” tools still builds by a traditional &lt;code class=&quot;highlighter-rouge&quot;&gt;make&lt;/code&gt; command. Every tool has to come up with their own, incomptaible, and non-portable “method of the day” of building.&lt;/p&gt;
&lt;p&gt;And since nobody is still able to compile things from scratch, &lt;strong&gt;everybody just downloads precompiled binaries from random websites&lt;/strong&gt;. Often &lt;strong&gt;without any authentication or signature&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;NSA and virus heaven. &lt;strong&gt;You don’t need to exploit any security hole anymore.&lt;/strong&gt; Just make an “app” or “VM” or “Docker” image, and have people load your malicious binary to their network.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://wiki.debian.org/Hadoop&quot;&gt;Hadoop Wiki Page&lt;/a&gt; of Debian is a typical example. Essentially, people have given up in 2010 to be able build Hadoop from source for Debian and offer nice packages.&lt;/p&gt;
&lt;p&gt;To build Apache Bigtop, you apparently first have to install puppet3. Let it download magic data from the internet. Then it tries to run &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo puppet&lt;/code&gt; to enable the NSA backdoors (for example, it will download and install an outdated precompiled JDK, because it considers you too stupid to install Java.) And then hope the gradle build doesn’t throw a 200 line useless backtrace.&lt;/p&gt;
&lt;p&gt;I am not joking. It will try to execute commands such as e.g.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;/bin/bash -c &quot;wget http://www.scala-lang.org/files/archive/scala-2.10.3.deb ; dpkg -x ./scala-2.10.3.deb /&quot;&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note that it doesn’t even &lt;em&gt;install&lt;/em&gt; the package properly, but extracts it to your root directory. The download does not check any signature, not even SSL certificates. (Source: &lt;a href=&quot;https://github.com/apache/bigtop/blob/master/bigtop_toolchain/manifests/scala.pp&quot;&gt;Bigtop puppet manifests&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Even if your build would work, it will involve Maven downloading unsigned binary code from the internet, and use that for building.&lt;/p&gt;
&lt;p&gt;Instead of writing clean, modular architecture, everything these days morphs into a huge mess of interlocked dependencies. Last I checked, the Hadoop classpath was already over 100 jars. I bet it is now 150, without even using any of the HBaseGiraphFlumeCrunchPigHiveMahoutSolrSparkElasticsearch (or any other of the Apache chaos) mess yet.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stack&lt;/strong&gt; is the new term for “I have no idea what I’m actually using”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Maven&lt;/strong&gt;, &lt;strong&gt;ivy&lt;/strong&gt; and &lt;strong&gt;sbt&lt;/strong&gt; are the go-to tools for having your system download unsigned binary data from the internet and run it on your computer.&lt;/p&gt;
&lt;p&gt;And with containers, this mess gets even worse.&lt;/p&gt;
&lt;p&gt;Ever tried to &lt;strong&gt;security update&lt;/strong&gt; a container?&lt;/p&gt;
&lt;p&gt;Essentially, the Docker approach boils down to downloading an unsigned binary, running it, and hoping it doesn’t contain any backdoor into your companies network.&lt;/p&gt;
&lt;p&gt;Feels like downloading Windows shareware in the 90s to me.&lt;/p&gt;
&lt;p&gt;When will the first docker image appear which contains the Ask toolbar? The first internet worm spreading via flawed docker images?&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;Back then, years ago, Linux distributions were trying to provide you with a safe operating system. With signed packages, built from a web of trust. Some even work on reproducible builds.&lt;/p&gt;
&lt;p&gt;But then, everything got Windows-ized. “Apps” were the rage, which you download and run, without being concerned about security, or the ability to upgrade the application to the next version. Because “you only live once”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; it was pointed out that this started way before Docker: »&lt;em&gt;Docker is the new ‘&lt;code class=&quot;highlighter-rouge&quot;&gt;curl | sudo bash&lt;/code&gt;‘&lt;/em&gt;«. That’s right, but it’s now pretty much mainstream to download and run untrusted software in your “datacenter”. That is bad, really bad. Before, admins would try hard to prevent security holes, now they call themselves “devops” and happily introduce them to the network themselves!&lt;/p&gt;
</description>
<pubDate>Wed, 16 May 2018 16:00:57 +0000</pubDate>
<dc:creator>xg15</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.vitavonni.de/blog/201503/2015031201-the-sad-state-of-sysadmin-in-the-age-of-containers.html</dc:identifier>
</item>
<item>
<title>How a domain registrar can kill your business</title>
<link>https://www.uptimechecker.io/blog/how-domain-registrar-can-kill-your-business</link>
<guid isPermaLink="true" >https://www.uptimechecker.io/blog/how-domain-registrar-can-kill-your-business</guid>
<description>&lt;p&gt;In this post we will write about recent domain incident we suffered, who is responsible and what we did to minimize the damage.&lt;/p&gt;&lt;div readability=&quot;216.25326909842&quot;&gt;
&lt;p&gt;So, in the morning of May 3rd, I noticed &lt;strong&gt;uptimechecker.io&lt;/strong&gt; becomes unreachable at moments, then available again, then down again, and so on. It all reminded of DNS problems, but I didn't have idea how big the problem will become.&lt;/p&gt;
&lt;h3&gt;Horror started when we contacted our Registrar support&lt;/h3&gt;
&lt;p&gt;I knew our domain should be renewed about these dates, and we were already &lt;strong&gt;billed&lt;/strong&gt; for this renewal. Our domain registrar is &lt;a href=&quot;https://www.domain.com&quot;&gt;domain.com&lt;/a&gt;, so I logged in to our &lt;strong&gt;domain.com&lt;/strong&gt; dashboard to check if everything is OK, but domain &lt;strong&gt;was not renewed&lt;/strong&gt; and domain details were reading &lt;strong&gt;2 days&lt;/strong&gt; until expiry.&lt;/p&gt;
&lt;p&gt;I contacted &lt;strong&gt;domain.com&lt;/strong&gt; support immediately, their chat agent responded immediately, &lt;strong&gt;confirmed we were billed&lt;/strong&gt; for domain and that domain is &lt;strong&gt;not renewed&lt;/strong&gt;, and he created support ticket for the issue. This is the start of &lt;strong&gt;horror story&lt;/strong&gt; with domain.com support.&lt;/p&gt;
&lt;p&gt;I was regularly checking the ticket, but nothing happened, and after 4 hours I noticed nameservers for uptimechecker.io were reverted to default values (not serving anything). UptimeChecker.io was &lt;strong&gt;completely down&lt;/strong&gt; for all users.&lt;/p&gt;
&lt;p&gt;I tried to set nameservers to correct values, but Control panel returned error: &lt;strong&gt;uptimechekcer.io is not managed here&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;I updated the ticket, said it is &lt;strong&gt;urgent&lt;/strong&gt; because our &lt;strong&gt;users cannot access the service&lt;/strong&gt;. I didn't got response for the next 8 hours, and after that, this was response:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hello,&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Thank you for contacting Support.&lt;/p&gt;&lt;p&gt;I apologize for any inconvenience this has caused you. We can understand your concern about the domain renewal issue. I will need to hand this ticket over to one of our senior registrar specialists to further assist you on 'uptimechecker.io' domain renewal issue. You should be hearing from them within 24-48 hours.&lt;/p&gt;&lt;p&gt;If you have any further questions, please chat with us at https://www1.domain.com/chat/ .&lt;/p&gt;&lt;p&gt;Sincerely,&lt;/p&gt;&lt;p&gt;Sachin K&lt;br/&gt;Domain Registrar Specialist&lt;/p&gt;
&lt;p&gt;So, our &lt;strong&gt;domain is not working&lt;/strong&gt;, no one can access the service, obviously &lt;strong&gt;domain.com&lt;/strong&gt; is responsible for the incident because they failed to renew the domain, but they said please wait &lt;strong&gt;24-48 hours&lt;/strong&gt;!? That is the time we started panicking, because it looked like no one at &lt;strong&gt;domain.com&lt;/strong&gt; is really caring about the issue, and we were not able to do anything to recover the service by ourselves, since we were &lt;strong&gt;completely cut off&lt;/strong&gt; of our domain because nameservers were set to incorrect values and there were no way we can update them.&lt;/p&gt;
&lt;p&gt;It turned to be even worse than this, they didn't answer anything for next &lt;strong&gt;four days&lt;/strong&gt;. I was contacting live chat support &lt;strong&gt;every day&lt;/strong&gt;, in first days two times per day, and every time &lt;strong&gt;chat agent assured me&lt;/strong&gt; someone is working on the ticket, and that ticket will be updated shortly, they sometimes said in next &lt;strong&gt;6 hours&lt;/strong&gt;, or next &lt;strong&gt;24 hours&lt;/strong&gt;. So yeah, it may sound crazy, but that is true, and they lied to us constantly every day. And we were completely blind what was happening and when we can expect the resolution.&lt;/p&gt;
&lt;h3&gt;Recovery plan&lt;/h3&gt;
&lt;p&gt;Since we were &lt;strong&gt;completely locked out of control&lt;/strong&gt; of our primary domain, only way to do something on our side was to purchase another domain, change all of our systems to work with the new domain, and to inform our users about it. This is pretty radical, so I wanted to avoid this if possible, but no one from &lt;strong&gt;domain.com&lt;/strong&gt; gave us an answer when we can expect the service to be online again.&lt;/p&gt;
&lt;p&gt;Finally, on Sunday (four days after incident started) I concluded that we really cannot rely on them, and I purchased the new domain: &lt;strong&gt;uptimechecker.org&lt;/strong&gt; and started the migration process. Everything was ready for the switch on Monday, so I contacted live chat support one more time, and got the same answer: someone will update your ticket today. They really updated the ticket with this bizarre answer:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hello,&lt;/em&gt;&lt;/p&gt;&lt;p&gt;Thank you for your recent contact and concerns in regards toward your domain name. We are currently reaching out to our registrar members, to ensure your domain name is renewed. Unlike common domain names like .com or .nets. .IO's are managed by a specific organization, that manages only .IO domain names. They must receive notification to renew the domain name prior to three days before the domain name expires. Unfortunately, our automatic renewal system is not compatible to renew the .IO domain names. Therefor, we are reaching out to our registrar members to reach to the registry to redeem the domain name. We do apologize for the delay in your resolution. Once I have received a confirmation on your domain renewal, I will notify you. If you have any questions or concerns please feel free to contact us.&lt;/p&gt;&lt;p&gt;Regards,&lt;br/&gt;Danny G.&lt;br/&gt;Domain Registrar Specialist.&lt;/p&gt;
&lt;p&gt;So, basically, they admitted this is their fault, and their systems cannot do it properly, but they &lt;strong&gt;can create invoice and take money&lt;/strong&gt;, and not even inform the user about any problems!? Also, in that update, &lt;strong&gt;they didn't&lt;/strong&gt; give us &lt;strong&gt;any time estimate&lt;/strong&gt; when we can expect the resolution. I asked for that information each day over live chat, always getting response: your ticket will be updated in the next 6 hours, or sometimes, in next 24 hours. It is &lt;strong&gt;still not updated&lt;/strong&gt;, it is now &lt;strong&gt;8 days&lt;/strong&gt; after the last update.&lt;/p&gt;
&lt;p&gt;After that update, I decided it is time to inform all our users about migration to the new domain: &lt;strong&gt;uptimechecker.org&lt;/strong&gt;. This was all we could do, and we would do that even earlier, but we hoped urgent problem like this will be handled earlier by &lt;strong&gt;domain.com&lt;/strong&gt;, however, we were terribly wrong.&lt;/p&gt;
&lt;p&gt;One week after the migration, &lt;strong&gt;uptimechecker.io&lt;/strong&gt; started working again (we still don't have control over our domain, but somehow DNS servers are reverted to correct values). They still didn't updated the ticket, we don't know how they fixed this, is it going to work permanently, or this is only temporary fix - again, &lt;strong&gt;zero information&lt;/strong&gt; from them. I contacted live support again, they said it works now because &quot;they are working on the issue&quot;, and that specialist will update ticket - and guess what, they didn't.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;I know this all sounds unbelievable, and if someone told me such story probably I would not believe also, so, at the end of this post, you can find &lt;strong&gt;full text of this ticket&lt;/strong&gt;. Sadly, all said in this post is &lt;strong&gt;true&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;While we are still waiting to regain control of our domain, UptimeChecker will continue to work both on uptimechecker.org and uptimechecker.io. To avoid situations like this to happen in the future, we will transfer all our domains out of &lt;strong&gt;domain.com&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So, to conclude, be aware of your Domain registrar, and be careful who you work with, because domain problem like this is one of the worst thing can happen to your business:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;You can't do anything to fix the issue&lt;/li&gt;
&lt;li&gt;Your users cannot access the service at all&lt;/li&gt;
&lt;li&gt;They can't event send you email to ask about the problem: (email address cannot be resolved also)&lt;/li&gt;
&lt;li&gt;Your ranks on Google will be destroyed (uptimechecker.io was completely removed from Google search results)&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;We value our users, and we are really sorry this happened, but it was out of our control. We apologize one more time to all of you, we did only thing we could to minimize the impact of the problem, and we hope you will stay with us :)&lt;/p&gt;
&lt;p&gt;If you are new to UptimeChecker, you can &lt;a href=&quot;https://www.uptimechecker.io/accounts/create-account&quot;&gt;try our service&lt;/a&gt; with all features totally free for 14 days! And if you become our user, we promise we will never let you down like our registrar did with us!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Here you can find whole ticket, without any modifications:&lt;/strong&gt; &lt;img src=&quot;https://www.uptimechecker.io/Content/images/blog/domain_com_ticket.png&quot;/&gt;&lt;/p&gt;
&lt;/div&gt;</description>
<pubDate>Wed, 16 May 2018 15:47:54 +0000</pubDate>
<dc:creator>richeyrw</dc:creator>
<og:title>Be aware: How domain registrar can kill your business</og:title>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.uptimechecker.io/blog/how-domain-registrar-can-kill-your-business</dc:identifier>
</item>
<item>
<title>Browser extension that strips Google Analytics tokens from URL query strings</title>
<link>https://github.com/jparise/chrome-utm-stripper</link>
<guid isPermaLink="true" >https://github.com/jparise/chrome-utm-stripper</guid>
<description>&lt;h3&gt;README.md&lt;/h3&gt;
&lt;article class=&quot;markdown-body entry-content&quot; itemprop=&quot;text&quot;&gt;
&lt;p&gt;This is a Chrome and Firefox browser extension that strips Google Analytics (i.e. &lt;a href=&quot;https://support.google.com/urchin/answer/28307?hl=en&quot; rel=&quot;nofollow&quot;&gt;Urchin Tracking Monitor&lt;/a&gt;) tokens from URL query strings. This is done &lt;em&gt;before&lt;/em&gt; the web request is made and result in both more private browsing as well as more aestethicly-pleasing URLs.&lt;/p&gt;
&lt;p&gt;Install from the &lt;a href=&quot;https://chrome.google.com/webstore/detail/kcpnkledgcbobhkgimpbmejgockkplob&quot; rel=&quot;nofollow&quot;&gt;Chrome Web Store&lt;/a&gt; or &lt;a href=&quot;https://addons.mozilla.org/addon/utm-tracking-token-stripper/&quot; rel=&quot;nofollow&quot;&gt;Firefox Add-ons&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The following &lt;a href=&quot;http://www.google.com/support/analytics/bin/answer.py?answer=55578&quot; rel=&quot;nofollow&quot;&gt;Google Analytics query string parameters&lt;/a&gt; are stripped:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;utm_source&lt;/li&gt;
&lt;li&gt;utm_medium&lt;/li&gt;
&lt;li&gt;utm_term&lt;/li&gt;
&lt;li&gt;utm_campaign&lt;/li&gt;
&lt;li&gt;utm_content&lt;/li&gt;
&lt;li&gt;utm_cid&lt;/li&gt;
&lt;li&gt;utm_reader&lt;/li&gt;
&lt;li&gt;utm_name&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;This extension requires these &lt;a href=&quot;https://developer.chrome.com/extensions/declare_permissions&quot; rel=&quot;nofollow&quot;&gt;permissions&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;code&gt;webRequest&lt;/code&gt;, to use the &lt;a href=&quot;https://developer.chrome.com/extensions/webRequest&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;chrome.webRequest&lt;/code&gt; API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;webRequestBlocking&lt;/code&gt;, to use &lt;code&gt;chrome.webRequest&lt;/code&gt; in a blocking fashion&lt;/li&gt;
&lt;li&gt;&lt;code&gt;http://*/*?*&lt;/code&gt;, to filter http URLs&lt;/li&gt;
&lt;li&gt;&lt;code&gt;https://*/*?*&lt;/code&gt;, to filter https URLs&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;http://www.openclipart.org/detail/69997&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://github.com/jparise/chrome-utm-stripper/raw/master/icon-128.png&quot; alt=&quot;Urchin Logo&quot; title=&quot;Urchin Logo&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.openclipart.org/detail/69997&quot; rel=&quot;nofollow&quot;&gt;Urchin Logo&lt;/a&gt; by Jordan Irwin / Deluge.&lt;/p&gt;
&lt;/article&gt;</description>
<pubDate>Wed, 16 May 2018 15:35:29 +0000</pubDate>
<dc:creator>falcon620</dc:creator>
<og:image>https://avatars2.githubusercontent.com/u/10311?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>jparise/chrome-utm-stripper</og:title>
<og:url>https://github.com/jparise/chrome-utm-stripper</og:url>
<og:description>chrome-utm-stripper - Browser extension that strips Google Analytics (UTM) tokens from URL query strings</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/jparise/chrome-utm-stripper</dc:identifier>
</item>
<item>
<title>The fifth hyperfactorial: 5⁵×4⁴×3³×2²×1¹=86400000 milliseconds is exactly 1 day</title>
<link>https://twitter.com/fermatslibrary/status/996736533511266304</link>
<guid isPermaLink="true" >https://twitter.com/fermatslibrary/status/996736533511266304</guid>
<description>&lt;div class=&quot;UIWalkthrough-step UIWalkthrough-step--welcome&quot; readability=&quot;9&quot;&gt;
&lt;h3 class=&quot;UIWalkthrough-title&quot;&gt;Welcome home!&lt;/h3&gt;
&lt;p class=&quot;UIWalkthrough-message&quot;&gt;This timeline is where you’ll spend most of your time, getting instant updates about what matters to you.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;UIWalkthrough-step UIWalkthrough-step--unfollow&quot; readability=&quot;7&quot;&gt;
&lt;h3 class=&quot;UIWalkthrough-title&quot;&gt;Tweets not working for you?&lt;/h3&gt;
&lt;p class=&quot;UIWalkthrough-message&quot;&gt;Hover over the profile pic and click the Following button to unfollow any account.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;UIWalkthrough-step UIWalkthrough-step--like&quot; readability=&quot;9&quot;&gt;
&lt;h3 class=&quot;UIWalkthrough-title&quot;&gt;Say a lot with a little&lt;/h3&gt;
&lt;p class=&quot;UIWalkthrough-message&quot;&gt;When you see a Tweet you love, tap the heart — it lets the person who wrote it know you shared the love.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;UIWalkthrough-step UIWalkthrough-step--retweet&quot; readability=&quot;8&quot;&gt;
&lt;h3 class=&quot;UIWalkthrough-title&quot;&gt;Spread the word&lt;/h3&gt;
&lt;p class=&quot;UIWalkthrough-message&quot;&gt;The fastest way to share someone else’s Tweet with your followers is with a Retweet. Tap the icon to send it instantly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;UIWalkthrough-step UIWalkthrough-step--reply&quot; readability=&quot;9&quot;&gt;
&lt;h3 class=&quot;UIWalkthrough-title&quot;&gt;Join the conversation&lt;/h3&gt;
&lt;p class=&quot;UIWalkthrough-message&quot;&gt;Add your thoughts about any Tweet with a Reply. Find a topic you’re passionate about, and jump right in.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;UIWalkthrough-step UIWalkthrough-step--trends&quot; readability=&quot;7&quot;&gt;
&lt;h3 class=&quot;UIWalkthrough-title&quot;&gt;Learn the latest&lt;/h3&gt;
&lt;p class=&quot;UIWalkthrough-message&quot;&gt;Get instant insight into what people are talking about now.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;UIWalkthrough-step UIWalkthrough-step--wtf&quot; readability=&quot;7&quot;&gt;
&lt;h3 class=&quot;UIWalkthrough-title&quot;&gt;Get more of what you love&lt;/h3&gt;
&lt;p class=&quot;UIWalkthrough-message&quot;&gt;Follow more accounts to get instant updates about topics you care about.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;UIWalkthrough-step UIWalkthrough-step--search&quot; readability=&quot;7&quot;&gt;
&lt;h3 class=&quot;UIWalkthrough-title&quot;&gt;Find what's happening&lt;/h3&gt;
&lt;p class=&quot;UIWalkthrough-message&quot;&gt;See the latest conversations about any topic instantly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;UIWalkthrough-step UIWalkthrough-step--moments&quot; readability=&quot;7&quot;&gt;
&lt;h3 class=&quot;UIWalkthrough-title&quot;&gt;Never miss a Moment&lt;/h3&gt;
&lt;p class=&quot;UIWalkthrough-message&quot;&gt;Catch up instantly on the best stories happening as they unfold.&lt;/p&gt;
&lt;/div&gt;
</description>
<pubDate>Wed, 16 May 2018 14:28:18 +0000</pubDate>
<dc:creator>slbenfica</dc:creator>
<og:type>article</og:type>
<og:url>https://twitter.com/fermatslibrary/status/996736533511266304</og:url>
<og:title>Fermat's Library on Twitter</og:title>
<og:image>https://pbs.twimg.com/media/DdUemCnU0AEPvKF.jpg:large</og:image>
<og:description>“The fifth hyperfactorial: 5⁵ × 4⁴ × 3³ × 2² × 1¹ = 86400000 milliseconds is exactly 1 day!”</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://twitter.com/fermatslibrary/status/996736533511266304</dc:identifier>
</item>
<item>
<title>Tarballs, the ultimate container image format</title>
<link>https://www.gnu.org/software/guix/blog/2018/tarballs-the-ultimate-container-image-format/</link>
<guid isPermaLink="true" >https://www.gnu.org/software/guix/blog/2018/tarballs-the-ultimate-container-image-format/</guid>
<description>&lt;h2&gt;Tarballs, the ultimate container image format&lt;/h2&gt;&lt;p class=&quot;post-metadata centered-text&quot;&gt;Ludovic Courtès — May 16, 2018&lt;/p&gt;
&lt;p&gt;A year ago &lt;a href=&quot;https://www.gnu.org/software/guix/blog/2017/creating-bundles-with-guix-pack/&quot;&gt;we introduced &lt;code&gt;guix pack&lt;/code&gt;&lt;/a&gt;, a tool that allows you to create “application bundles” from a set of Guix package definitions. On your Guix machine, you run:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-sh&quot;&gt;guix pack -S /opt/gnu/bin=bin guile gnutls guile-json&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;and you get a tarball containing your favorite programming language implementation and a couple of libraries, where &lt;code&gt;/opt/gnu/bin&lt;/code&gt; is a symlink to the &lt;code&gt;bin&lt;/code&gt; directory containing, in this case, the &lt;code&gt;guile&lt;/code&gt; command. Add &lt;code&gt;-f docker&lt;/code&gt; and, instead of a tarball, you get an image in the Docker format that you can pass to &lt;code&gt;docker load&lt;/code&gt; on any machine where Docker is installed. Overall that’s a relatively easy way to share software stacks with machines that do not run Guix.&lt;/p&gt;
&lt;p&gt;The tarball format is plain and simple, it’s the one we know and love, and it’s been there “forever” &lt;a href=&quot;https://www.gnu.org/software/tar/manual/html_node/Introduction.html&quot;&gt;as its name suggests&lt;/a&gt;. The tarball that &lt;code&gt;guix pack&lt;/code&gt; produces can be readily extracted on another machine, one that doesn’t run Guix, and you’re done. The problem though, is that you’ll need to either unpack the tarball in the root file system or to play tricks with the &lt;code&gt;unshare&lt;/code&gt; command, as we saw &lt;a href=&quot;https://www.gnu.org/software/guix/blog/2017/creating-bundles-with-guix-pack/&quot;&gt;in the previous post&lt;/a&gt;. Why can’t we just extract such a tarball in our home directory and directly run &lt;code&gt;./opt/gnu/bin/guile&lt;/code&gt; for instance?&lt;/p&gt;

&lt;p&gt;The main issue is that, except in the uncommon case where developers went to great lengths to make it possible (as with &lt;a href=&quot;http://lilypond.org/gub/&quot;&gt;GUB&lt;/a&gt;, see the &lt;a href=&quot;https://github.com/gperciva/gub/tree/master/patches&quot;&gt;&lt;code&gt;*-reloc*.patch&lt;/code&gt; files&lt;/a&gt;), packages built for GNU/Linux are not relocatable. ELF files embed things like the absolute file name of the dynamic linker, directories where libraries are to be search for (they can be relative file names with &lt;code&gt;$ORIGIN&lt;/code&gt; but usually aren’t), and so on; furthermore, it’s very common to embed things like the name of the directory that contains locale data or other application-specific data. For Guix-built software, all these are absolute file names under &lt;code&gt;/gnu/store&lt;/code&gt; so Guix-built binaries won’t run unless those &lt;code&gt;/gnu/store&lt;/code&gt; files exist.&lt;/p&gt;
&lt;p&gt;On machines where support for &lt;a href=&quot;http://man7.org/linux/man-pages/man7/user_namespaces.7.html&quot;&gt;“user namespaces”&lt;/a&gt; is enabled, we can easily “map” the directory where users unpacked the tarball that &lt;code&gt;guix pack&lt;/code&gt; produced to &lt;code&gt;/gnu/store&lt;/code&gt;, as shown in the previous post:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-sh&quot;&gt;$ tar xf /path/to/pack.tar.gz
$ unshare -mrf chroot . /opt/gnu/bin/guile --version
guile (GNU Guile) 2.2.0&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It does the job but remains quite tedious. Can’t we automate that?&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;--relocatable&lt;/code&gt; (or &lt;code&gt;-R&lt;/code&gt;) option of &lt;code&gt;guix pack&lt;/code&gt;, which landed &lt;a href=&quot;https://bugs.gnu.org/31360&quot;&gt;a few days ago&lt;/a&gt;, produces tarballs with automatically relocatable binaries. Back to our earlier example, let’s say you produce a tarball with this new option:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-sh&quot;&gt;guix pack --relocatable -S /bin=bin -S /etc=etc guile gnutls guile-json&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;You can send the resulting tarball to any machine that runs the kernel Linux (it &lt;a href=&quot;https://www.gnu.org/software/guix/blog/2018/guix-on-android/&quot;&gt;doesn’t even have to be GNU/Linux&lt;/a&gt;) with user namespace support—which, unfortunately, is disabled by default on some distros. There, as a regular user, you can run:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-sh&quot;&gt;$ tar xf /path/to/pack.tar.gz
$ source ./etc/profile    # define ’GUILE_LOAD_PATH’, etc.
$ ./bin/guile
guile: warning: failed to install locale
GNU Guile 2.2.3
Copyright (C) 1995-2017 Free Software Foundation, Inc.

Guile comes with ABSOLUTELY NO WARRANTY; for details type `,show w'.
This program is free software, and you are welcome to redistribute it
under certain conditions; type `,show c' for details.

Enter `,help' for help.
scheme@(guile-user)&amp;gt; ,use(json)
scheme@(guile-user)&amp;gt; ,use(gnutls)&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;We were able to run Guile and to use our Guile libraries since sourcing &lt;code&gt;./etc/profile&lt;/code&gt; augmented the &lt;code&gt;GUILE_LOAD_PATH&lt;/code&gt; environment variable that tells Guile where to look for libraries. Indeed we can see it by inspecting the value of &lt;code&gt;%load-path&lt;/code&gt; at the Guile prompt:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;&lt;span class=&quot;syntax-symbol&quot;&gt;scheme@&lt;/span&gt;&lt;span class=&quot;syntax-open&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;syntax-symbol&quot;&gt;guile-user&lt;/span&gt;&lt;span class=&quot;syntax-close&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;syntax-symbol&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;syntax-symbol&quot;&gt;%load-path&lt;/span&gt;
&lt;span class=&quot;syntax-symbol&quot;&gt;$1&lt;/span&gt; &lt;span class=&quot;syntax-symbol&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;syntax-open&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;syntax-string&quot;&gt;&quot;/gnu/store/w9xd291967cvmdp3m0s7739icjzgs8ns-profile/share/guile/site/2.2&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;/gnu/store/b90y3swxlx3vw2yyacs8cz59b8cbpbw5-guile-2.2.3/share/guile/2.2&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;/gnu/store/b90y3swxlx3vw2yyacs8cz59b8cbpbw5-guile-2.2.3/share/guile/site/2.2&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;/gnu/store/b90y3swxlx3vw2yyacs8cz59b8cbpbw5-guile-2.2.3/share/guile/site&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;/gnu/store/b90y3swxlx3vw2yyacs8cz59b8cbpbw5-guile-2.2.3/share/guile&quot;&lt;/span&gt;&lt;span class=&quot;syntax-close&quot;&gt;)&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Wait, it’s all &lt;code&gt;/gnu/store&lt;/code&gt;! As it turns out, &lt;code&gt;guix pack --relocatable&lt;/code&gt; created a wrapper around &lt;code&gt;guile&lt;/code&gt; that populates &lt;code&gt;/gnu/store&lt;/code&gt; in the mount namespace of the process. Even though &lt;code&gt;/gnu/store&lt;/code&gt; does not exist on that machine, our &lt;code&gt;guile&lt;/code&gt; process “sees” our packages under &lt;code&gt;/gnu/store&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;&lt;span class=&quot;syntax-symbol&quot;&gt;scheme@&lt;/span&gt;&lt;span class=&quot;syntax-open&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;syntax-symbol&quot;&gt;guile-user&lt;/span&gt;&lt;span class=&quot;syntax-close&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;syntax-symbol&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;syntax-symbol&quot;&gt;,use&lt;/span&gt;&lt;span class=&quot;syntax-open&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;syntax-symbol&quot;&gt;ice-9&lt;/span&gt; &lt;span class=&quot;syntax-symbol&quot;&gt;ftw&lt;/span&gt;&lt;span class=&quot;syntax-close&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;syntax-symbol&quot;&gt;scheme@&lt;/span&gt;&lt;span class=&quot;syntax-open&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;syntax-symbol&quot;&gt;guile-user&lt;/span&gt;&lt;span class=&quot;syntax-close&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;syntax-symbol&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;syntax-open&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;syntax-symbol&quot;&gt;scandir&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;/gnu/store&quot;&lt;/span&gt;&lt;span class=&quot;syntax-close&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;syntax-symbol&quot;&gt;$2&lt;/span&gt; &lt;span class=&quot;syntax-symbol&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;syntax-open&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;syntax-string&quot;&gt;&quot;.&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;..&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;0249nw8c7z626fw1fayacm160fpd543k-guile-json-0.6.0R&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;05dvazr5wfh7lxx4zi54zfqnx6ha8vxr-bash-static-4.4.12&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;0jawbsyafm93nxf4rcmkf1rsk7z03qfa-libltdl-2.4.6&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;0z1r7ai6syi2qnf5z8w8n25b1yv8gdr4-info-dir&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;1n59wjm6dbvc38b320iiwrxra3dg7yv8-libunistring-0.9.8&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;2fg01r58vv9w41kw6drl1wnvqg7rkv9d-libtasn1-4.12&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;2ifmksc425qcysl5rkxkbv6yrgc1w9cs-gcc-5.5.0-lib&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;2vxvd3vls7c8i9ngs881dy1p5brc7p85-gmp-6.1.2&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;4sqaib7c2dfjv62ivrg9b8wa7bh226la-glibc-2.26.105-g0890d5379c&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;5kih0kxmipzjw10c53hhckfzkcs7c8mm-gnutls-3.5.13&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;8hxm8am4ll05sa8wlwgdq2lj4ddag464-zlib-1.2.11&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;90vz0r78bww7dxhpa7vsiynr1rcqhyh4-nettle-3.4&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;b90y3swxlx3vw2yyacs8cz59b8cbpbw5-guile-2.2.3&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;c4jrwbv7qckvnqa7f3h7bd1hh8rbg72y-libgc-7.6.0&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;f5lw5w4nxs6p5gq0c2nb3jsrxc6mmxbi-libgc-7.6.0&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;hjxic0k4as384vn2qp0l964isfkb0blb-guile-json-0.6.0&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;ksyja5lbwy0mpskvn4rfi5klc00c092d-libidn2-2.0.4&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;l15mx9lrwdflyvmb4a05va05v5yqizg5-libffi-3.2.1&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;mm0zclrzj3y7rj74hzyd0f224xly04fh-bash-minimal-4.4.12&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;vgmln3b639r68vvy75xhcbi7d2w31mx1-pkg-config-0.29.2&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;vz3zfmphvv4w4y7nffwr4jkk7k4s0rfs-guile-2.2.3&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;w9xd291967cvmdp3m0s7739icjzgs8ns-profile&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;x0jf9ckd30k3nhs6bbhkrxsjmqz8phqd-nettle-3.4&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;x8z6cr7jggs8vbyh0xzfmxbid63z6y83-guile-2.2.3R&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;xbkl3nx0fqgpw2ba8jsjy0bk3nw4q3i4-gnutls-3.5.13R&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;xh4k91vl0i8nlyrmvsh01x0mz629w5a9-gmp-6.1.2&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;yx12x8v4ny9f6fipk8285jgfzqavii83-manual-database&quot;&lt;/span&gt; &lt;span class=&quot;syntax-string&quot;&gt;&quot;zksh1n0p9x903kqbvswgwy2vsk2b7255-libatomic-ops-7.4.8&quot;&lt;/span&gt;&lt;span class=&quot;syntax-close&quot;&gt;)&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The wrapper is a small statically-linked &lt;a href=&quot;https://git.savannah.gnu.org/cgit/guix.git/tree/gnu/packages/aux-files/run-in-namespace.c&quot;&gt;C program&lt;/a&gt;. (Scheme would be nice and would allow us to reuse &lt;a href=&quot;https://www.gnu.org/software/guix/blog/2015/container-provisioning-with-guix/&quot;&gt;&lt;code&gt;call-with-container&lt;/code&gt;&lt;/a&gt;, but it would also take up more space.) All it does is create a child process with separate mount and user namespaces, which in turn mounts the tarball’s &lt;code&gt;/gnu/store&lt;/code&gt; to &lt;code&gt;/gnu/store&lt;/code&gt;, bind-mounts other entries from the host root file system, and &lt;code&gt;chroot&lt;/code&gt;s into that. The result is a binary that sees everything a “normal” program sees on the host, but with the addition of &lt;code&gt;/gnu/store&lt;/code&gt;, with minimal startup overhead.&lt;/p&gt;
&lt;p&gt;In a way, it’s a bit of a hack: for example, what gets bind-mounted in the mount namespace of the wrapped program is hard-coded, which is OK, but some flexibility would be welcome (things like Flatpak’s &lt;a href=&quot;http://docs.flatpak.org/en/latest/sandbox-permissions.html&quot;&gt;sandbox permissions&lt;/a&gt;, for instance). Still, that it Just Works is a pretty cool feature.&lt;/p&gt;

&lt;p&gt;Come to think of it: if you’re a developer, &lt;code&gt;guix pack&lt;/code&gt; is probably one of the easiest ways to create an “application bundle” to share with your users; and as a user, these relocatable tarballs are about the simplest thing you can deal with since you don’t need anything but &lt;code&gt;tar&lt;/code&gt;—well, and user namespace support. Plus, since they are &lt;a href=&quot;https://www.gnu.org/software/guix/blog/tags/reproducible-builds/&quot;&gt;bit-reproducible&lt;/a&gt;, anyone can rebuild them to ensure they &lt;a href=&quot;https://www.omgubuntu.co.uk/2018/05/ubuntu-snap-malware&quot;&gt;do not contain malware&lt;/a&gt; or to &lt;a href=&quot;https://lwn.net/Articles/752982/&quot;&gt;check the provenance and licensing of its contents&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Application bundles cannot replace full-blown package management, which allows users to upgrade, get security updates, use storage and memory efficiently, and so on. For the purposes of quickly sharing packages with users or with Guix-less machines, though, you might find Guix packs to be more convenient than Snap, Flatplak, or Docker. Give it a spin and &lt;a href=&quot;https://www.gnu.org/software/guix/contact/&quot;&gt;let us know&lt;/a&gt;!&lt;/p&gt;
&lt;h4&gt;About GNU Guix&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://www.gnu.org/software/guix&quot;&gt;GNU Guix&lt;/a&gt; is a transactional package manager for the GNU system. The Guix System Distribution or GuixSD is an advanced distribution of the GNU system that relies on GNU Guix and &lt;a href=&quot;https://www.gnu.org/distros/free-system-distribution-guidelines.html&quot;&gt;respects the user's freedom&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In addition to standard package management features, Guix supports transactional upgrades and roll-backs, unprivileged package management, per-user profiles, and garbage collection. Guix uses low-level mechanisms from the Nix package manager, except that packages are defined as native &lt;a href=&quot;https://www.gnu.org/software/guile&quot;&gt;Guile&lt;/a&gt; modules, using extensions to the &lt;a href=&quot;http://schemers.org&quot;&gt;Scheme&lt;/a&gt; language. GuixSD offers a declarative approach to operating system configuration management, and is highly customizable and hackable.&lt;/p&gt;
&lt;p&gt;GuixSD can be used on an i686, x86_64 and armv7 machines. It is also possible to use Guix on top of an already installed GNU/Linux system, including on mips64el and aarch64.&lt;/p&gt;

</description>
<pubDate>Wed, 16 May 2018 13:36:51 +0000</pubDate>
<dc:creator>severus_snape</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.gnu.org/software/guix/blog/2018/tarballs-the-ultimate-container-image-format/</dc:identifier>
</item>
</channel>
</rss>