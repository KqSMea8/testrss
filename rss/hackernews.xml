<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Surviving 75 hours alone in the ocean</title>
<link>https://www.outsideonline.com/2275116/how-survive-75-hours-alone-ocean?utm_medium=social&amp;utm_source=twitter&amp;utm_campaign=onsiteshare</link>
<guid isPermaLink="true" >https://www.outsideonline.com/2275116/how-survive-75-hours-alone-ocean?utm_medium=social&amp;utm_source=twitter&amp;utm_campaign=onsiteshare</guid>
<description>&lt;p&gt;In February 2006, Robert Hewitt was scuba diving near Mana Island, off the coast of New Zealand’s North Island. Hewitt was an experienced navy diving instructor with 20 years in the service, and he told his dive buddy that he would swim back to shore himself. Instead, when he next surfaced, he had been pulled several hundred meters away by a strong current. The dive boat had moved on, and Hewitt was left alone, the tide pushing him farther and farther from shore.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/29241234&quot; target=&quot;_blank&quot;&gt;a recent issue&lt;/a&gt; of the journal &lt;em&gt;Diving and Hyperbaric Medicine&lt;/em&gt;, a team of researchers led by physiologist Heather Massey of the University of Portsmouth in the UK take a closer look at what happened next: Hewitt’s progressive deterioration over the next four days and three nights, how he survived, and what took place after his eventual rescue. It’s an interesting glimpse at a branch of extreme physiology that most of us hope we’ll never encounter.&lt;/p&gt;
&lt;p&gt;(Massey’s interest isn’t purely theoretical. She’s currently training to swim across the English Channel, which will require prolonged immersion in cool water. She also took home a &lt;a href=&quot;https://twitter.com/i/web/status/818866624417689601&quot; target=&quot;_blank&quot;&gt;gold medal&lt;/a&gt; from the World Ice Swimming Championships last year, in temperatures just a few degrees above freezing, and &lt;a href=&quot;http://uopnews.port.ac.uk/2016/08/16/olympic-swimmers-trained-at-portsmouth/&quot; target=&quot;_blank&quot;&gt;helped British open-water swimmers&lt;/a&gt; prepare for the Rio Olympics.)&lt;/p&gt;
&lt;p&gt;The most pressing challenge facing Hewitt was the water temperature of 61 to 63 degrees Fahrenheit (16 to 17 degrees Celsius), well below body temperature. According to physiological models, when water is 59 degrees Fahrenheit (15 degrees Celsius), the median survival time is between 4.8 and 7.7 hours. Amazingly, Hewitt spent the next 75 hours in the water, drifting back and forth over a distance of nearly 40 miles before he was spotted by Navy diving friends and rescued.&lt;/p&gt;
&lt;p&gt;In general, immersion in cold water produces a four-stage response. First is the “cold shock response” that triggers “an inspiratory gasp, uncontrollable hyperventilation, hypertension, and increased cardiac workload.” If you’re not ready for it, this shock response can cause you to inhale water and drown and can set off heart arrhythmias. Hewitt had two key defenses against the cold shock: a five-millimeter custom-fit wetsuit and habituation from more than 1,000 previous dives, which eventually blunts the initial shock response.&lt;/p&gt;
&lt;p&gt;After the cold shock, which peaks within 30 seconds and diminishes after a few minutes, the next stage of immersion is peripheral muscle cooling. For every 1.8 degrees Fahrenheit (1 degree Celsius) that your muscles cool, your maximum muscle power drops by about 3 percent. That means you can lose the ability to swim before your core actually gets hypothermic. Hewitt did indeed lose the ability to swim at some points during his ordeal—sometimes because he lost consciousness—but he had a buoyancy compensator that kept him floating with his head above water.&lt;/p&gt;
&lt;p&gt;The third stage is deep body cooling, which affects both physical and mental function and eventually results in loss of consciousness and then death. No one took Hewitt’s temperature until he had been wrapped in blankets and received warm drinks after his rescue. At that point, it was 96.3 degrees Fahrenheit (35.7 degrees Celsius), which isn’t particularly low. He did have some episodes of confusion and disorientation that suggest he was on the border of hypothermia, but it’s hard to be sure.&lt;/p&gt;

&lt;p&gt;One key factor that helped stave off hypothermia was the fact that Hewitt is (in the words of the researchers) “a large, muscular male”; at 5'11&quot; and 220 pounds, he clearly had a decent amount of insulation. In fact, for every 1 percent increase in body fat, you slow your rate of heat loss by 0.18 degrees Fahrenheit (0.1 degree Celsius) per hour—a big deal when you extrapolate to 75 hours. Hewitt also tried to maintain the fetal position, which minimizes heat loss and extends survival time in cool water.&lt;/p&gt;
&lt;p&gt;The fourth and final stage of immersion, if you make it that far, is the “circum-rescue” phase. It’s apparently quite common for people to collapse during rescue, thanks in part to the change in pressure when you leave the water and the strong nervous system reaction to the idea of being rescued. With this in mind, Hewitt’s rescuers kept him horizontal to maintain blood flow to the brain and gave him “verbal encouragement” to keep fighting for his life.&lt;/p&gt;
&lt;p&gt;Of course, cold water wasn’t the only challenge in play. Despite water, water, everywhere, dehydration is a serious problem—in fact, the squeeze of the wetsuit and water pressure shunt blood to your core, which stimulates urination, even when you’re already dehydrated. The practical guidance in situations like this is that you should avoid drinking for the first day; this will trigger hormone changes that make your body start conserving water. After that, aim to scrounge up half a liter per day. Hewitt used his mask and wetsuit jacket to collect rainwater, but this was far below his needs. When he was rescued, he drank a liter and a half of water, then received another six liters intravenously.&lt;/p&gt;
&lt;p&gt;Prolonged soaking in seawater, along with the friction from his wetsuit and fins, damaged Hewitt’s skin pretty badly. When found (put your spoon down for a moment), “his body was covered with sea lice feeding on his macerated skin.” And then there’s the psychological challenge, both during and after the ordeal. By the third day, he was contemplating (and half-heartedly attempting) suicide, but he managed to keep fighting.&lt;/p&gt;
&lt;p&gt;Can we extract any lessons from Hewitt’s ordeal? Well, wearing a wetsuit and weighing more than 200 pounds obviously helped, but those aren’t particularly useful takeaways. Staying in the fetal position—sometimes known as the “heat escape lessening posture,” or HELP—was a good idea. Ultimately, the most pointed lesson, and the one Hewitt himself now spreads as a water safety advocate, is that he shouldn’t have been in that situation in the first place. Rather than diving alone, he should have aborted the dive and joined another group or, at the very least, used a surface-marker buoy to flag his position. “In some ways, Rob almost contributed to his own demise,” the police search team leader said bluntly. “He took some shortcuts.”&lt;/p&gt;
&lt;p&gt;Still, shit sometimes happens. And if it does, the other big lesson to keep in mind is that in defiance of all the physiological models, Rob Hewitt survived for an astounding 75 hours alone in the cold water. If you find yourself out there, don’t give up.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;em&gt;Discuss this post on&lt;/em&gt; &lt;a href=&quot;https://twitter.com/sweatscience&quot; target=&quot;_blank&quot;&gt;&lt;em&gt;Twitter&lt;/em&gt;&lt;/a&gt; &lt;em&gt;or&lt;/em&gt; &lt;a href=&quot;https://www.facebook.com/sweatscience1&quot; target=&quot;_blank&quot;&gt;&lt;em&gt;Facebook&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, sign up for the Sweat Science&lt;/em&gt; &lt;a href=&quot;http://alexhutchinson.us10.list-manage1.com/subscribe?u=16b257be614b5f18187d3b50a&amp;amp;id=4111e620a3&quot; target=&quot;_blank&quot;&gt;&lt;em&gt;email newsletter&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, and check out my forthcoming book,&lt;/em&gt; &lt;a href=&quot;https://amzn.to/2g7dSKS&quot; target=&quot;_blank&quot;&gt;Endure: Mind, Body, and the Curiously Elastic Limits of Human Performance&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;

</description>
<pubDate>Thu, 25 Jan 2018 00:19:56 +0000</pubDate>
<dc:creator>DanBC</dc:creator>
<og:type>article</og:type>
<og:title>How to Survive 75 Hours Alone in the Ocean</og:title>
<og:url>https://www.outsideonline.com/2275116/how-survive-75-hours-alone-ocean</og:url>
<og:description>A new case study digs into the medical records of a lost diver's incredible survival story</og:description>
<og:image>https://www.outsideonline.com/sites/default/files/styles/full-page/public/2018/01/17/survival-at-sea-sweat-science_h.jpg?itok=VPP2RKx_</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.outsideonline.com/2275116/how-survive-75-hours-alone-ocean</dc:identifier>
</item>
<item>
<title>Zstandard – Real-time data compression algorithm</title>
<link>http://facebook.github.io/zstd/</link>
<guid isPermaLink="true" >http://facebook.github.io/zstd/</guid>
<description>&lt;p&gt;Zstandard is a real-time compression algorithm, providing high compression ratios. It offers a very wide range of compression / speed trade-off, while being backed by a very fast decoder (see &lt;a href=&quot;http://facebook.github.io/zstd/#benchmarks&quot;&gt;benchmarks&lt;/a&gt; below). It also offers a special mode for small data, called &lt;a href=&quot;http://facebook.github.io/zstd/#small-data&quot;&gt;dictionary compression&lt;/a&gt;, and can create dictionaries from any sample set. Zstandard library is provided as open source software using a BSD license.&lt;/p&gt;
</description>
<pubDate>Wed, 24 Jan 2018 22:25:31 +0000</pubDate>
<dc:creator>josephscott</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://facebook.github.io/zstd/</dc:identifier>
</item>
<item>
<title>Burger King Has an Opinion on Net Neutrality</title>
<link>https://www.bloomberg.com/news/articles/2018-01-24/net-neutrality-movement-finds-a-fast-food-friend-in-burger-king</link>
<guid isPermaLink="true" >https://www.bloomberg.com/news/articles/2018-01-24/net-neutrality-movement-finds-a-fast-food-friend-in-burger-king</guid>
<description>&lt;p&gt;An unlikely voice has joined the passionate debate over internet regulation known as net neutrality: Burger King.&lt;/p&gt;


&lt;p&gt;The fast-food chain released a nearly 3 minute &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.youtube.com/watch?v=ltzy5vRmN8Q&quot; title=&quot;Whopper net neutrality ad&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;video&lt;/a&gt; on YouTube that portrays the downside of being asked to pay more for speedier service. In it, customers at a Los Angeles restaurant impatiently wait at the counter for Whoppers while others who, they are told, had paid more, received faster service.&lt;/p&gt;


&lt;div class=&quot;thirdparty-embed&quot;&gt;
&lt;div class=&quot;thirdparty-embed__container&quot;&gt;
&lt;p&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/ltzy5vRmN8Q?rel=0&amp;amp;showinfo=0&quot; allowfullscreen=&quot;&quot; scrolling=&quot;no&quot; data-embed-id=&quot;https://www.youtube.com/watch?v=ltzy5vRmN8Q&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The customers are baffled, such as the hungry diner waiting for what turns out to be a slow-access Whopper. &quot;Wait. What?&quot; he exclaims.&lt;/p&gt;



&lt;p&gt;&quot;Whopper neutrality was repealed,” a counter worker explains.&lt;/p&gt;



&lt;p&gt;“We believe the internet should be like Burger King restaurants, a place that doesn’t prioritize and welcomes everyone,” Fernando Machado, Burger King’s global chief marketing officer, said in a statement. &quot;That is why we created this experiment, to call attention to the potential effects of net neutrality.”&lt;/p&gt;
&lt;p&gt;Burger King restaurants are independent franchisees but the brand is owned by &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.bloomberg.com/quote/QSR:US&quot; title=&quot;Company Overview&quot; rel=&quot;nofollow noopener&quot;&gt;Restaurant Brands International&lt;/a&gt;, based in Ontario, Canada.&lt;/p&gt;
&lt;p&gt;Wonks and politicians perked up.&lt;/p&gt;
&lt;p&gt;“Must watch,” &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://twitter.com/brianschatz/status/956220724573700096&quot; title=&quot;Schatz tweet&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;tweeted&lt;/a&gt; Hawaii Democratic Senator Brian Schatz, who has criticized &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.bloomberg.com/quote/0244544Z:US&quot; title=&quot;Company Overview&quot; rel=&quot;nofollow noopener&quot;&gt;Federal Communications Commission&lt;/a&gt; Republicans for their December vote to eliminate rules forbidding broadband providers such as AT&amp;amp;T Inc. and Comcast Corp. from blocking or slowing web traffic.&lt;/p&gt;

&lt;p&gt;“When it comes to the internet, consumers and innovators deserve to have it their way -- not big corporations,” Connecticut’s Richard Blumenthal, another Democratic senator, said in &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://twitter.com/SenBlumenthal/status/956220708593553410&quot; title=&quot;Blumenthal tweet&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;a tweet&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some panned the ad.&lt;/p&gt;
&lt;p&gt;&lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://twitter.com/BurgerKing&quot; title=&quot;Click to view webpage.&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;“Burger King&lt;/a&gt; understands net neutrality even less than their people-on-the-street, who at least know they are ignorant,” tweeted Phil Kerpen, president of the group American Commitment, which backed the Republican proposal to roll back Obama-era rules.&lt;/p&gt;
&lt;p&gt;At the end of the video, the chain’s King mascot sips from a giant mug bearing the logo of Reese’s candy. It’s an unmistakable reference to U.S. Federal Communications Commission chairman Ajit Pai, who wields a large mug emblazoned with the Reese’s image at agency meetings.&lt;/p&gt;
&lt;p&gt;Pai steered the agency in December to a party line vote that eliminated Obama-era rules on net neutrality.&lt;/p&gt;
</description>
<pubDate>Wed, 24 Jan 2018 22:10:55 +0000</pubDate>
<dc:creator>Dangeranger</dc:creator>
<og:description>An unlikely voice has joined the passionate debate over internet regulation known as net neutrality: Burger King.</og:description>
<og:image>https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iOfOTgc6Yx0g/v0/1200x798.jpg</og:image>
<og:title>Burger King Has an Opinion on Net Neutrality</og:title>
<og:type>article</og:type>
<og:url>https://www.bloomberg.com/news/articles/2018-01-24/net-neutrality-movement-finds-a-fast-food-friend-in-burger-king</og:url>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bloomberg.com/news/articles/2018-01-24/net-neutrality-movement-finds-a-fast-food-friend-in-burger-king</dc:identifier>
</item>
<item>
<title>Fake celebrity porn is blowing up on Reddit, thanks to artificial intelligence</title>
<link>https://www.theverge.com/2018/1/24/16929148/fake-celebrity-porn-ai-deepfake-face-swapping-artificial-intelligence-reddit</link>
<guid isPermaLink="true" >https://www.theverge.com/2018/1/24/16929148/fake-celebrity-porn-ai-deepfake-face-swapping-artificial-intelligence-reddit</guid>
<description>&lt;p id=&quot;cwCJZ8&quot;&gt;Back in December, the unsavory hobby of a Reddit user by the name of deepfakes became a new centerpiece of artificial intelligence debate, specifically around the newfound ability to &lt;a href=&quot;https://www.theverge.com/2017/12/12/16766596/ai-fake-porn-celebrities-machine-learning&quot;&gt;face-swap celebrities and porn stars&lt;/a&gt;. Using software, deepfakes was able to take the face of famous actresses and swap them with those of porn actresses, letting him live out a fantasy of watching famous people have sex. Now, just two months later, easy-to-use applications have sprouted up with the ability to perform this real-time editing with even more ease, &lt;a href=&quot;https://motherboard.vice.com/en_us/article/bjye8a/reddit-fake-porn-app-daisy-ridley&quot;&gt;according to &lt;em&gt;Motherboard&lt;/em&gt;&lt;/a&gt;, which also &lt;a href=&quot;https://motherboard.vice.com/en_us/article/gydydm/gal-gadot-fake-ai-porn&quot;&gt;first reported about deepfakes&lt;/a&gt; late last year.&lt;/p&gt;
&lt;p id=&quot;JBg6Ai&quot;&gt;Thanks to AI training techniques like machine learning, scores of photographs can be fed into an algorithm that creates convincing human masks to replace the faces of anyone on video, all by using lookalike data and letting the software train itself to improve over time. In this case, users are putting famous actresses into existing adult films. According to deepfakes, this required some extensive computer science know-how. But &lt;em&gt;Motherboard&lt;/em&gt; reports that one user in the burgeoning community of pornographic celebrity face swapping has created a user-friendly app that basically anyone can use.&lt;/p&gt;
&lt;div class=&quot;c-float-right&quot;&gt;
&lt;aside id=&quot;ERjNfr&quot;&gt;&lt;q&gt;“Deepfake” has become a catch-all term for AI-assisted face swapping&lt;/q&gt;&lt;/aside&gt;&lt;/div&gt;
&lt;p id=&quot;WVZUpE&quot;&gt;These anonymous internet users are gathering on a &lt;a href=&quot;https://www.reddit.com/r/deepfakes/&quot;&gt;dedicated subreddit deepfakes created back in December&lt;/a&gt;, which has now amassed more than 15,000 subscribers. (It is very NSFW.) The term “deepfakes” has also become a stand-in for these AI-assisted face swaps. As for the new user-friendly app, it’s known as FakeApp, and it was created by using deepfakes’ original software and improving upon it. An exhaustive tutorial with download links posted 16 days ago is now pinned to the top of the subreddit. “This app is intended to allow users to move through the full deepfake creation pipeline — creating training data, training a model, and creating fakes with that model — without the need to install Python and other dependencies or parse code,” explains the creator, who goes by the online handle “deepfakeapp.”&lt;/p&gt;
&lt;p id=&quot;icg8E8&quot;&gt;“I think the current version of the app is a good start, but I hope to streamline it even more in the coming days and weeks,” the Redditor behind FakeApp tells &lt;em&gt;Motherboard&lt;/em&gt;. “Eventually, I want to improve it to the point where prospective users can simply select a video on their computer, download a neural network correlated to a certain face from a publicly available library, and swap the video with a different face with the press of one button.”&lt;/p&gt;
&lt;p id=&quot;xn6KI8&quot;&gt;It’s easy to point the finger at deepfakes’ growing community as outliers with repulsive intentions, but these users are &lt;a href=&quot;https://www.cbsnews.com/news/digital-doubles-bringing-actors-back-to-life/&quot;&gt;taking their inspirations from Hollywood&lt;/a&gt;. Increasingly, films are using software to digitally render faces at various stages of aging, regressing older actors’ faces by decades or even bringing people back to life on the silver screen. Another member of the community &lt;a href=&quot;https://www.reddit.com/r/deepfakes/comments/7sjkw5/ilm_fixed_that_for_you_edition/&quot;&gt;used the technology to try and replicate an instance of this&lt;/a&gt;: the scene from &lt;em&gt;Rogue One&lt;/em&gt; in which Carry Fisher’s Princess Leia shows up as her younger self. And the results are staggering, showing a near-identical result when comparing Hollywood CGI to a deepfake.&lt;/p&gt;
&lt;span class=&quot;e-image__inner&quot;&gt;&lt;span class=&quot;e-image__image&quot; data-original=&quot;https://cdn.vox-cdn.com/uploads/chorus_asset/file/10091569/leiadeepfake.gif&quot;&gt; &lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;e-image__meta&quot;&gt;&lt;em&gt;The top scene features a digitally regressed Princess Leia from Rogue One, while the face in the bottom is a deepfake created using AI-assisted algorithms and a “visually similar actress.”&lt;/em&gt; &lt;cite&gt;&lt;a href=&quot;https://www.reddit.com/r/deepfakes/comments/7sjkw5/ilm_fixed_that_for_you_edition/&quot;&gt;Image: Derpfakes&lt;/a&gt;&lt;/cite&gt;&lt;/span&gt;
&lt;p id=&quot;Hgee0Z&quot;&gt;The implications of this technology are immense. It’s not just the effects this could have on celebrities and other public figures who already are subjected to all manners of manipulative forgeries and hoaxes. It’s also what AI-assisted face swapping could mean for politics, journalism, and crime. We assume, too, that face swapping is the end game, but it’s clearly just the beginning. AI advancements have &lt;a href=&quot;https://www.theverge.com/2016/12/20/14022958/ai-image-manipulation-creation-fakes-audio-video&quot;&gt;effectively ushered in a new, more powerful, and sophisticated form of Adobe Photoshop&lt;/a&gt; for any type of media, be it photo or video.&lt;/p&gt;
&lt;p id=&quot;bPlJ8Z&quot;&gt;There’s no telling what objects, actions, or scenes might be fabricated with enough photographic and video data to go on. The line between real and fake will only get more blurry. Right now, there are &lt;a href=&quot;https://twitter.com/smilevector?lang=en&quot;&gt;apps that can make anyone smile in a photo&lt;/a&gt;, and neural networks are already capable of generating organic photos of landscapes, animals, and architecture. It just so happens that right now, the internet being what is, AI-assisted face swapping is being used to a particularly disturbing end.&lt;/p&gt;
</description>
<pubDate>Wed, 24 Jan 2018 21:28:11 +0000</pubDate>
<dc:creator>LearnerHerzog</dc:creator>
<og:description>Easy-to-use applications are letting people face-swap celebrities and porn stars</og:description>
<og:image>https://cdn.vox-cdn.com/thumbor/HjHzDVXNq4Y8yKIiPBRRgGD09fw=/0x412:5760x3428/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/10091827/shutterstock_295933367.0.jpg</og:image>
<og:title>Fake celebrity porn is blowing up on Reddit, thanks to artificial intelligence</og:title>
<og:type>article</og:type>
<og:url>https://www.theverge.com/2018/1/24/16929148/fake-celebrity-porn-ai-deepfake-face-swapping-artificial-intelligence-reddit</og:url>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.theverge.com/2018/1/24/16929148/fake-celebrity-porn-ai-deepfake-face-swapping-artificial-intelligence-reddit</dc:identifier>
</item>
<item>
<title>Scientists In Alaska Find Mammoth Amounts Of Carbon In The Warming Permafrost</title>
<link>https://www.npr.org/sections/goatsandsoda/2018/01/24/575220206/is-there-a-ticking-time-bomb-under-the-arctic</link>
<guid isPermaLink="true" >https://www.npr.org/sections/goatsandsoda/2018/01/24/575220206/is-there-a-ticking-time-bomb-under-the-arctic</guid>
<description>&lt;div id=&quot;res580099004&quot; class=&quot;bucketwrap image large&quot;&gt;
&lt;div class=&quot;imagewrap&quot; data-crop-type=&quot;&quot;&gt;&lt;img src=&quot;https://media.npr.org/assets/img/2018/01/23/permafrost_custom-cfbc8ea7ebc5f88959f2892c2d688a9dfa46af74-s1100-c15.jpg&quot; data-original=&quot;https://media.npr.org/assets/img/2018/01/23/permafrost_custom-cfbc8ea7ebc5f88959f2892c2d688a9dfa46af74-s1100.jpg&quot; class=&quot;img lazyOnLoad&quot; alt=&quot;&quot;/&gt;
&lt;/div&gt;
&lt;div class=&quot;credit-caption&quot;&gt;
&lt;div class=&quot;caption-wrap&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;caption&quot; aria-label=&quot;Image caption&quot; readability=&quot;10&quot;&gt;
&lt;p&gt;The Permafrost Tunnel Research Facility, dug in the mid-1960s, allows scientists a three-dimensional look at frozen ground. &lt;strong class=&quot;credit&quot; aria-label=&quot;Image credit&quot;&gt;Kate Ramsayer/NASA&lt;/strong&gt; &lt;strong class=&quot;hide-caption&quot;&gt;hide caption&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;strong class=&quot;toggle-caption&quot;&gt;toggle caption&lt;/strong&gt;&lt;/div&gt;
&lt;span class=&quot;credit&quot; aria-label=&quot;Image credit&quot;&gt;Kate Ramsayer/NASA&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;enlarge_measure&quot;&gt;
&lt;div class=&quot;img_wrap&quot;&gt;&lt;img data-original=&quot;https://media.npr.org/assets/img/2018/01/23/permafrost_custom-cfbc8ea7ebc5f88959f2892c2d688a9dfa46af74-s1200.jpg&quot; alt=&quot;&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;enlarge_html&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;image_data&quot; readability=&quot;10&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;The Permafrost Tunnel Research Facility, dug in the mid-1960s, allows scientists a three-dimensional look at frozen ground.&lt;/p&gt;
&lt;span class=&quot;credit&quot; aria-label=&quot;Image credit&quot;&gt;Kate Ramsayer/NASA&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;A short drive north of Fairbanks, Alaska, there's a red shed stuck right up against a hillside. The shed looks unremarkable, except for the door. It looks like a door to a walk-in freezer, with thick insulation and a heavy latch. Whatever is behind that door needs to stay very cold.&lt;/p&gt;
&lt;p&gt;&quot;Are you ready to go inside?&quot; asks Dr. Thomas Douglas, a geochemist at the U.S. Army Corps of Engineers.&lt;/p&gt;
&lt;p&gt;Behind the door is a geological time bomb, scientists say. No one knows exactly how big the bomb is. It may even be a dud that barely detonates. But the fallout could be so large that it's felt all around the world. Now there's evidence that, in the past few years, the bomb's timer has started ticking.&lt;/p&gt;
&lt;p&gt;Douglas opens the shed door, and we step inside. Immediately, we're standing 40 feet below ground, inside a tunnel carved into the hillside.&lt;/p&gt;
&lt;p&gt;&quot;That's a mammoth leg right there,&quot; Douglas says as he points to a giant femur protruding from the tunnel wall.&lt;/p&gt;
&lt;aside id=&quot;ad-backstage-wrap&quot; aria-label=&quot;advertisement&quot;&gt;
&lt;/aside&gt;&lt;aside id=&quot;ad-mobilebackfill-wrap&quot; aria-label=&quot;advertisement&quot;&gt;
&lt;/aside&gt;&lt;p&gt;All around are signs of extinct creatures. Tusks poke out of the ceiling and skulls stick up from the floor. But it's the material between the bones that interests Douglas the most: the permafrost.&lt;/p&gt;
&lt;p&gt;In the 1960s, the Army dug the tunnel so it could study this unique surface, which covers about a quarter of the Northern Hemisphere. In some places, the frozen soil extends downward more than 1,000 feet, or about the height of the Empire State Building.&lt;/p&gt;
&lt;div id=&quot;res580099343&quot; class=&quot;bucketwrap image large&quot;&gt;
&lt;div class=&quot;imagewrap&quot; data-crop-type=&quot;&quot;&gt;&lt;img src=&quot;https://media.npr.org/assets/img/2018/01/23/bonewall_custom-a96d6398fb16b237959f7758d37be7ab20831fda-s1100-c15.jpg&quot; data-original=&quot;https://media.npr.org/assets/img/2018/01/23/bonewall_custom-a96d6398fb16b237959f7758d37be7ab20831fda-s1100.jpg&quot; class=&quot;img lazyOnLoad&quot; alt=&quot;&quot;/&gt;
&lt;/div&gt;
&lt;div class=&quot;credit-caption&quot;&gt;
&lt;div class=&quot;caption-wrap&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;caption&quot; aria-label=&quot;Image caption&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;A mammoth bone sticks out of the wall of the tunnel in the permafrost. &lt;strong class=&quot;credit&quot; aria-label=&quot;Image credit&quot;&gt;Kate Ramsayer/NASA&lt;/strong&gt; &lt;strong class=&quot;hide-caption&quot;&gt;hide caption&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;strong class=&quot;toggle-caption&quot;&gt;toggle caption&lt;/strong&gt;&lt;/div&gt;
&lt;span class=&quot;credit&quot; aria-label=&quot;Image credit&quot;&gt;Kate Ramsayer/NASA&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;enlarge_measure&quot;&gt;
&lt;div class=&quot;img_wrap&quot;&gt;&lt;img data-original=&quot;https://media.npr.org/assets/img/2018/01/23/bonewall_custom-a96d6398fb16b237959f7758d37be7ab20831fda-s1200.jpg&quot; alt=&quot;&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;enlarge_html&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;image_data&quot; readability=&quot;7&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;A mammoth bone sticks out of the wall of the tunnel in the permafrost.&lt;/p&gt;
&lt;span class=&quot;credit&quot; aria-label=&quot;Image credit&quot;&gt;Kate Ramsayer/NASA&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Technically, permafrost is frozen soil. But it's helpful to think of it in terms of chocolate cake. Typically, cake is soft, moist and spongy. Now if you take that cake, dip it into water and freeze it, the cake becomes hard or stiff. That's exactly what happens to soil when you freeze it: Moist, soft soil turns hard and stiff. That's permafrost.&lt;/p&gt;
&lt;p&gt;For the first time in centuries, the Arctic permafrost is beginning to change — rapidly. It's warming up. Some places are softening like a stick of butter left out on the kitchen counter.&lt;/p&gt;
&lt;p&gt;In northern Alaska, the temperature at some permafrost sites has risen by more than 4 degrees Fahrenheit since the 1980s, the National Oceanic and Atmospheric Administration &lt;a href=&quot;http://www.arctic.noaa.gov/Report-Card/Report-Card-2017/ArtMID/7798/ArticleID/694/Terrestrial-Permafrost&quot;&gt;reported&lt;/a&gt; in November. And in recent years, many spots have reached record temperatures.&lt;/p&gt;
&lt;p&gt;&quot;Arctic shows no sign of returning to reliably frozen region of recent past decades,&quot; NOAA &lt;a href=&quot;http://www.arctic.noaa.gov/Report-Card/Report-Card-2017&quot;&gt;wrote&lt;/a&gt; in its annual Arctic Report Card last year.&lt;/p&gt;
&lt;p&gt;The consequences of this warming could have ripple effects around the world. To explain why, Douglas takes me deeper down into the tunnel.&lt;/p&gt;
&lt;p&gt;&quot;This is really an amazing feature,&quot; he says, shining his flashlight up to the ceiling. Crispy grass is dangling upside-down above our heads.&lt;/p&gt;
&lt;p&gt;&quot;It's green grass — from 25,000 years ago,&quot; he exclaims. &quot;It has been preserved that way for 25,000 years.&quot;&lt;/p&gt;
&lt;p&gt;The permafrost is packed with the remains of ancient life. From prehistoric grass and trees to woolly mammoths and woolly rhinoceroses, just about every creature that lived on the tundra over the past 100,000 years is buried and preserved down in the permafrost.&lt;/p&gt;
&lt;p&gt;And all this life is made of carbon. So there's a massive amount of carbon buried down here. &quot;The permafrost contains twice as much carbon as is currently in Earth's atmosphere,&quot; Douglas says. &quot;That's 1,600 billion metric tons.&quot;&lt;/p&gt;
&lt;p&gt;In fact, there's more carbon in the permafrost, Douglas says, than all the carbon humans have spewed into the atmosphere since the Industrial Revolution — first with steam trains, then with coal plants, cars and planes.&lt;/p&gt;
&lt;div id=&quot;res580100066&quot; class=&quot;bucketwrap image large&quot;&gt;
&lt;div class=&quot;imagewrap&quot; data-crop-type=&quot;&quot;&gt;&lt;img src=&quot;https://media.npr.org/assets/img/2018/01/23/wedge-36ab4ee5f692d79c7328db3f37274da547c3951c-s1100-c15.jpg&quot; data-original=&quot;https://media.npr.org/assets/img/2018/01/23/wedge-36ab4ee5f692d79c7328db3f37274da547c3951c-s1100.jpg&quot; class=&quot;img lazyOnLoad&quot; alt=&quot;&quot;/&gt;
&lt;/div&gt;
&lt;div class=&quot;credit-caption&quot;&gt;
&lt;div class=&quot;caption-wrap&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;caption&quot; aria-label=&quot;Image caption&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;Ice wedges form over centuries, creating polygonal patterns in the permafrost. &lt;strong class=&quot;credit&quot; aria-label=&quot;Image credit&quot;&gt;Kate Ramsayer/NASA&lt;/strong&gt; &lt;strong class=&quot;hide-caption&quot;&gt;hide caption&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;strong class=&quot;toggle-caption&quot;&gt;toggle caption&lt;/strong&gt;&lt;/div&gt;
&lt;span class=&quot;credit&quot; aria-label=&quot;Image credit&quot;&gt;Kate Ramsayer/NASA&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;enlarge_measure&quot;&gt;
&lt;div class=&quot;img_wrap&quot;&gt;&lt;img data-original=&quot;https://media.npr.org/assets/img/2018/01/23/wedge-36ab4ee5f692d79c7328db3f37274da547c3951c-s1200.jpg&quot; alt=&quot;&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;enlarge_html&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;image_data&quot; readability=&quot;8&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;Ice wedges form over centuries, creating polygonal patterns in the permafrost.&lt;/p&gt;
&lt;span class=&quot;credit&quot; aria-label=&quot;Image credit&quot;&gt;Kate Ramsayer/NASA&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Right now the permafrost carbon is inert and trapped in the frozen soil. But what happens when the soil thaws? That's the question Douglas and his colleagues are trying to figure out.&lt;/p&gt;
&lt;p&gt;A few years ago, they ran a simple experiment. They brought big drills into the tunnel and cut out chunks of ice. &quot;We collected pieces about the size of Coca-Cola cans,&quot; he says, as he points out holes in the tunnel's wall.&lt;/p&gt;
&lt;p&gt;They took the ice back to the lab and let it slowly come up to room temperature. Then they looked for signs of life. A few days later, something started growing — slowly at first, but then like gangbusters.&lt;/p&gt;
&lt;div id=&quot;res580099765&quot; class=&quot;bucketwrap image large&quot;&gt;
&lt;div class=&quot;imagewrap&quot; data-crop-type=&quot;&quot;&gt;&lt;img src=&quot;https://media.npr.org/assets/img/2018/01/23/bones-2158991f0c8d58c3072ab18f721db87717e29e8e-s1100-c15.jpg&quot; data-original=&quot;https://media.npr.org/assets/img/2018/01/23/bones-2158991f0c8d58c3072ab18f721db87717e29e8e-s1100.jpg&quot; class=&quot;img lazyOnLoad&quot; alt=&quot;&quot;/&gt;
&lt;/div&gt;
&lt;div class=&quot;credit-caption&quot;&gt;
&lt;div class=&quot;caption-wrap&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;caption&quot; aria-label=&quot;Image caption&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;The tunnel turned up a variety of ice age mammal bones — including the giant leg bone of a mammoth. &lt;strong class=&quot;credit&quot; aria-label=&quot;Image credit&quot;&gt;Kate Ramsayer/NASA&lt;/strong&gt; &lt;strong class=&quot;hide-caption&quot;&gt;hide caption&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;strong class=&quot;toggle-caption&quot;&gt;toggle caption&lt;/strong&gt;&lt;/div&gt;
&lt;span class=&quot;credit&quot; aria-label=&quot;Image credit&quot;&gt;Kate Ramsayer/NASA&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;enlarge_measure&quot;&gt;
&lt;div class=&quot;img_wrap&quot;&gt;&lt;img data-original=&quot;https://media.npr.org/assets/img/2018/01/23/bones-2158991f0c8d58c3072ab18f721db87717e29e8e-s1200.jpg&quot; alt=&quot;&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;enlarge_html&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;image_data&quot; readability=&quot;8&quot;&gt;
&lt;p class=&quot;caption&quot;&gt;The tunnel turned up a variety of ice age mammal bones — including the giant leg bone of a mammoth.&lt;/p&gt;
&lt;span class=&quot;credit&quot; aria-label=&quot;Image credit&quot;&gt;Kate Ramsayer/NASA&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&quot;This is material that stayed frozen for 25,000 years,&quot; Douglas says. &quot;And given the right environmental conditions, it came back alive again vigorously.&quot;&lt;/p&gt;
&lt;p&gt;They were ancient bacteria. And once they warmed up, they were hungry. The bacteria started converting the carbon that's in dead plants and animals into gases that cause climate change: carbon dioxide and methane.&lt;/p&gt;
&lt;p&gt;That experiment was in the lab. But imagine these bacteria waking up, all around the Arctic, across Canada, Greenland and Russia. Last year, scientists &lt;a href=&quot;http://www.pnas.org/content/114/21/5361&quot;&gt;started&lt;/a&gt; seeing signs of this happening in northern Alaska.&lt;/p&gt;
&lt;p&gt;&quot;We have evidence that Alaska has changed from being a net absorber of carbon dioxide out of the atmosphere to a net exporter of the gas back to the atmosphere,&quot; says Charles Miller, a chemist at NASA's Jet Propulsion Laboratory who measures gas emissions from Arctic permafrost.&lt;/p&gt;
&lt;p&gt;Scientists don't know yet how much carbon will get released from thawing permafrost or how fast it will happen. Some of the carbon — maybe a big percentage of it — will get washed into the ocean by erosion. Some of the carbon will also get sucked back into the ground by new trees and plants popping up across the warming tundra.&lt;/p&gt;
&lt;p&gt;But once carbon begins to percolate up through the thawing soil, it could form a feedback loop &quot;over which we would have zero control,&quot; Miller says. The gas, coming from the ground, warms the Earth, which in turn causes more gas to be released and more warming to occur.&lt;/p&gt;
&lt;p&gt;Thawing permafrost is a big wild card of climate change.&lt;/p&gt;
</description>
<pubDate>Wed, 24 Jan 2018 20:59:34 +0000</pubDate>
<dc:creator>marchenko</dc:creator>
<og:title>Is There A Ticking Time Bomb Under The Arctic?</og:title>
<og:url>https://www.npr.org/sections/goatsandsoda/2018/01/24/575220206/is-there-a-ticking-time-bomb-under-the-arctic</og:url>
<og:type>article</og:type>
<og:description>Just what exactly is permafrost? And what is happening now that it's warming up? To find out, we enter the Arctic Circle's secret world of ice and frozen history.</og:description>
<og:image>https://media.npr.org/assets/img/2018/01/23/permafrost_wide-55001b7cd09c0567b7d8feeb389af4fa3bda2da7.jpg?s=1400</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.npr.org/sections/goatsandsoda/2018/01/24/575220206/is-there-a-ticking-time-bomb-under-the-arctic</dc:identifier>
</item>
<item>
<title>Curry spice turmeric boosts memory by nearly 30%, eases depression, study finds</title>
<link>https://ac.els-cdn.com/S1064748117305110/1-s2.0-S1064748117305110-main.pdf?_tid=55b0e2e0-013c-11e8-b35b-00000aab0f01&amp;acdnat=1516822117_7ded3d2cd7f5c27fedf6a8992e549857</link>
<guid isPermaLink="true" >https://ac.els-cdn.com/S1064748117305110/1-s2.0-S1064748117305110-main.pdf?_tid=55b0e2e0-013c-11e8-b35b-00000aab0f01&amp;acdnat=1516822117_7ded3d2cd7f5c27fedf6a8992e549857</guid>
<description>&lt;article class=&quot;cl-m-3-3 cl-t-6-9 cl-l-6-12 pad-left pad-right&quot; role=&quot;main&quot; lang=&quot;en&quot; readability=&quot;9.1636844758065&quot;&gt;&lt;noscript readability=&quot;3&quot;&gt;
&lt;p&gt;JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page.&lt;/p&gt;
&lt;/noscript&gt;
&lt;div class=&quot;Publication&quot; readability=&quot;1.4890510948905&quot;&gt;
&lt;div class=&quot;publication-brand&quot;&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/journal/10647481&quot;&gt;&lt;img class=&quot;publication-brand-image hide-m&quot; src=&quot;https://sdfestaticassets-us-west-2.sciencedirectassets.com/prod/50d0e01a991ff414103f3f298b179063b99289c6/image/elsevier-non-solus.png&quot; alt=&quot;Elsevier&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;publication-volume&quot; readability=&quot;1.7372262773723&quot;&gt;

&lt;p&gt;&lt;span class=&quot;size-m&quot;&gt;Available online 27 October 2017&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;div class=&quot;publication-cover&quot;&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/journal/10647481&quot;&gt;&lt;img class=&quot;publication-cover-image hide-m&quot; src=&quot;https://ars.els-cdn.com/content/image/S10647481.gif&quot; alt=&quot;The American Journal of Geriatric Psychiatry&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;



&lt;div class=&quot;LicenseInfo&quot;&gt;

&lt;p&gt;open access&lt;/p&gt;
&lt;/div&gt;
&lt;section class=&quot;ReferencedArticles&quot;/&gt;&lt;section class=&quot;ReferencedArticles&quot;/&gt;
&lt;div class=&quot;Abstracts&quot;&gt;
&lt;div class=&quot;abstract author-highlights&quot; id=&quot;ab0010&quot; lang=&quot;en&quot;&gt;
&lt;h2 class=&quot;section-title&quot;&gt;Highlights&lt;/h2&gt;
&lt;div id=&quot;abs0010&quot;&gt;

&lt;dl class=&quot;list&quot; readability=&quot;3.5&quot;&gt;&lt;dt class=&quot;list-label&quot;&gt;•&lt;/dt&gt;
&lt;dd class=&quot;list-description&quot; readability=&quot;1&quot;&gt;
&lt;p id=&quot;p0010&quot;&gt;This is the first long-term (18 months) double-blind, placebo controlled trial of a bioavailable form of curcumin (Theracurmin® containing 90 mg of curcumin twice daily) in non-demented adults.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&quot;list-label&quot;&gt;•&lt;/dt&gt;
&lt;dd class=&quot;list-description&quot; readability=&quot;-1&quot;&gt;
&lt;p id=&quot;p0015&quot;&gt;We found that daily oral Theracurmin led to significant memory and attention benefits.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&quot;list-label&quot;&gt;•&lt;/dt&gt;
&lt;dd class=&quot;list-description&quot; readability=&quot;1&quot;&gt;
&lt;p id=&quot;p0020&quot;&gt;FDDNP-PET scans performed pre- and post-treatment suggested that behavioral and cognitive benefits are associated with decreases in plaque and tangle accumulation in brain regions modulating mood and memory.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&quot;list-label&quot;&gt;•&lt;/dt&gt;
&lt;dd class=&quot;list-description&quot; readability=&quot;0&quot;&gt;
&lt;p id=&quot;p8015&quot;&gt;Curcumin's cognitive benefits may stem from its anti-inflammatory and/or anti-amyloid brain effects.&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;abstract author&quot; id=&quot;ab0015&quot; lang=&quot;en&quot; readability=&quot;29&quot;&gt;
&lt;div id=&quot;abs0015&quot; readability=&quot;11&quot;&gt;
&lt;h3 id=&quot;st0015&quot;&gt;Objective&lt;/h3&gt;
&lt;p id=&quot;sp0015&quot;&gt;Because curcumin's anti-inflammatory properties may protect the brain from neurodegeneration, we studied its effect on memory in non-demented adults and explored its impact on brain amyloid and tau accumulation using 2-(1-{6-[(2-[F-18]fluoroethyl)(methyl)amino]-2-naphthyl}ethylidene)malononitrile positron emission tomography (FDDNP-PET).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;abs0020&quot; readability=&quot;19&quot;&gt;
&lt;h3 id=&quot;st0020&quot;&gt;Methods&lt;/h3&gt;
&lt;p id=&quot;sp0020&quot;&gt;Forty subjects (age 51–84 years) were randomized to a bioavailable form of curcumin (Theracurmin® containing 90 mg of curcumin twice daily [N = 21]) or placebo (N = 19) for 18 months. Primary outcomes were verbal (Buschke Selective Reminding Test [SRT]) and visual (Brief Visual Memory Test-Revised [BVMT-R]) memory, and attention (Trail Making A) was a secondary outcome. FDDNP-PET signals (15 curcumin, 15 placebo) were determined in amygdala, hypothalamus, medial and lateral temporal, posterior cingulate, parietal, frontal, and motor (reference) regions. Mixed effects general linear models controlling for age and education, and effect sizes (ES; Cohen's d) were estimated.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;abs0025&quot; readability=&quot;29&quot;&gt;
&lt;h3 id=&quot;st0025&quot;&gt;Results&lt;/h3&gt;
&lt;p id=&quot;sp0025&quot;&gt;SRT Consistent Long-Term Retrieval improved with curcumin (ES = 0.63, p = 0.002) but not with placebo (ES = 0.06, p = 0.8; between-group: ES = 0.68, p = 0.05). Curcumin also improved SRT Total (ES = 0.53, p = 0.002), visual memory (BVMT-R Recall: ES = 0.50, p = 0.01; BVMT-R Delay: ES = 0.51, p = 0.006), and attention (ES = 0.96, p &amp;lt; 0.0001) compared with placebo (ES = 0.28, p = 0.1; between-group: ES = 0.67, p = 0.04). FDDNP binding decreased significantly in the amygdala with curcumin (ES = −0.41, p = 0.04) compared with placebo (ES = 0.08, p = 0.6; between-group: ES = 0.48, p = 0.07). In the hypothalamus, FDDNP binding did not change with curcumin (ES = −0.30, p = 0.2), but increased with placebo (ES = 0.26, p = 0.05; between-group: ES = 0.55, p = 0.02).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;abs0030&quot; readability=&quot;9&quot;&gt;
&lt;h3 id=&quot;st0030&quot;&gt;Conclusions&lt;/h3&gt;
&lt;p id=&quot;sp0030&quot;&gt;Daily oral Theracurmin may lead to improved memory and attention in non-demented adults. The FDDNP-PET findings suggest that symptom benefits are associated with decreases in amyloid and tau accumulation in brain regions modulating mood and memory.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;Keywords&quot; readability=&quot;6&quot;&gt;
&lt;div id=&quot;kwd0010&quot; class=&quot;keywords-section&quot; readability=&quot;7&quot;&gt;
&lt;h2 class=&quot;section-title&quot;&gt;Key Words&lt;/h2&gt;
&lt;p&gt;&lt;span id=&quot;te0015&quot;&gt;Bioavailable curcumin&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;te0020&quot;&gt;normal aging&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;te0025&quot;&gt;memory&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;te0030&quot;&gt;cognition&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span id=&quot;te0035&quot;&gt;positron emission tomography&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;button class=&quot;button button-anchor&quot; disabled=&quot;disabled&quot; type=&quot;button&quot;&gt;&lt;span class=&quot;button-text&quot;&gt;Recommended articles&lt;/span&gt;&lt;/button&gt;&lt;button class=&quot;button button-anchor&quot; disabled=&quot;disabled&quot; type=&quot;button&quot;&gt;&lt;span class=&quot;button-text&quot;&gt;Citing articles (0)&lt;/span&gt;&lt;/button&gt;&lt;/p&gt;


&lt;p&gt;&lt;span class=&quot;copyright-line&quot;&gt;© 2017 The Authors. Published by Elsevier Inc. on behalf of American Association for Geriatric Psychiatry.&lt;/span&gt;&lt;/p&gt;
&lt;/article&gt;&lt;div class=&quot;hide-m hide-t-s cl-t-3-9 cl-l-3-12 pad-right&quot;&gt;
&lt;aside class=&quot;RelatedContent&quot; aria-label=&quot;Related content&quot;&gt;&lt;section class=&quot;SidePanel&quot;&gt;&lt;header class=&quot;side-panel-header&quot; id=&quot;recommended-articles-header&quot;&gt;&lt;h2 class=&quot;section-title&quot;&gt;Recommended articles&lt;/h2&gt;
&lt;/header&gt;
&lt;/section&gt;&lt;section class=&quot;SidePanel&quot;&gt;&lt;header class=&quot;side-panel-header&quot; id=&quot;citing-articles-header&quot;&gt;&lt;h2 class=&quot;section-title&quot;&gt;Citing articles&lt;/h2&gt;
&lt;/header&gt;

&lt;/section&gt;&lt;section class=&quot;SidePanel hidden&quot;&gt;

&lt;/section&gt;&lt;/aside&gt;&lt;/div&gt;
</description>
<pubDate>Wed, 24 Jan 2018 19:27:01 +0000</pubDate>
<dc:creator>bcaulfield</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.sciencedirect.com/science/article/pii/S1064748117305110</dc:identifier>
</item>
<item>
<title>Apple, in Sign of Health Ambitions, Adds Medical Records Feature for iPhone</title>
<link>https://www.nytimes.com/2018/01/24/technology/Apple-iPhone-medical-records.html</link>
<guid isPermaLink="true" >https://www.nytimes.com/2018/01/24/technology/Apple-iPhone-medical-records.html</guid>
<description>&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;299&quot; data-total-count=&quot;1901&quot; id=&quot;story-continues-3&quot;&gt;Apple, more than the others, has been reticent to publicize its long-term vision for health technology. But recent product introductions, like the new health records feature, highlight how focused Apple is on using its iPhone, Apple Watch and apps to give people more control over their health care.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;457&quot; data-total-count=&quot;2358&quot;&gt;In addition to the iPhone Health app, Apple has developed ResearchKit, software to help researchers develop iPhone apps to conduct health studies, and HealthKit, a platform that allows consumers to share health data on their iPhone or Apple Watch with health and fitness apps. Apple is also sponsoring clinical research, called the &lt;a href=&quot;https://clinicaltrials.gov/ct2/show/NCT03335800?term=apple+heart&amp;amp;rank=1&quot; title=&quot;Information about the study&quot;&gt;Apple Heart Study&lt;/a&gt;, at Stanford University to determine whether an app for the Apple Watch can detect irregular heart rhythms.&lt;/p&gt;
&lt;span class=&quot;visually-hidden&quot;&gt;Photo&lt;/span&gt;
&lt;div class=&quot;image&quot;&gt;&lt;img src=&quot;https://static01.nyt.com/images/2018/01/24/business/24applehealth-2/24applehealth-2-blog427.jpg&quot; alt=&quot;&quot; class=&quot;media-viewer-candidate&quot; data-mediaviewer-src=&quot;https://static01.nyt.com/images/2018/01/24/business/24applehealth-2/24applehealth-2-superJumbo.jpg&quot; data-mediaviewer-caption=&quot;Part of the Apple Health app, the new health records feature can transfer medical data like immunization records and prescriptions.&quot; data-mediaviewer-credit=&quot;Apple&quot; itemprop=&quot;url&quot; itemid=&quot;https://static01.nyt.com/images/2018/01/24/business/24applehealth-2/24applehealth-2-blog427.jpg&quot;/&gt;&lt;/div&gt;
&lt;span class=&quot;caption-text&quot;&gt;Part of the Apple Health app, the new health records feature can transfer medical data like immunization records and prescriptions.&lt;/span&gt; &lt;span class=&quot;credit&quot; itemprop=&quot;copyrightHolder&quot;&gt;&lt;span class=&quot;visually-hidden&quot;&gt;Credit&lt;/span&gt; Apple&lt;/span&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;124&quot; data-total-count=&quot;2482&quot;&gt;A review of Apple’s current job openings also gives clues about the company’s wider ambitions in the health care sector.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;547&quot; data-total-count=&quot;3029&quot;&gt;According to the company’s site, Apple is seeking a hardware engineer to develop “next-generation” health sensors for products like the iPhone and iPad; software engineers for the company’s “health special projects team” to join “an exciting new project at an early stage”; an engineering manager for the company’s motion technologies team “to help shape the next set of groundbreaking features” in fitness and health; and a biomedical scientist to help design studies for health, wellness and physiological measurement apps.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;255&quot; data-total-count=&quot;3284&quot;&gt;“We will empower you to engage with a variety of internal teams and external partners to continually question the limitations of technology implemented in health products,” says an Apple job description for a health tech hardware development engineer.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;268&quot; data-total-count=&quot;3552&quot;&gt;Apple’s personal medical record feature is hardly a new idea. With much fanfare about a decade ago, both Google and Microsoft introduced free services — called &lt;a href=&quot;http://www.nytimes.com/2008/05/20/technology/20google.html&quot; title=&quot;Times story on the topic.&quot;&gt;Google Health&lt;/a&gt; and &lt;a href=&quot;http://www.nytimes.com/2007/10/05/technology/05soft.html&quot; title=&quot;Times' story on the topic.&quot;&gt;Microsoft HealthVault&lt;/a&gt; — that helped consumers centralize their personal health data.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;249&quot; data-total-count=&quot;3801&quot;&gt;But the concept of the personal medical record did not generate widespread adoption in that era, which predated the popularization of the iPhone and mobile apps. Google &lt;a href=&quot;http://www.nytimes.com/2011/06/25/technology/25health.html&quot; title=&quot;Times' story on the topic.&quot;&gt;shut down Google Health&lt;/a&gt; in 2011. Microsoft still offers its &lt;a href=&quot;https://www.healthvault.com/en-us/healthvault-for-consumers/&quot; title=&quot;The Microsoft service&quot;&gt;HealthVault service&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&quot;addenda&quot; class=&quot;addenda&quot; readability=&quot;8&quot;&gt;
&lt;div class=&quot;story-addendum story-content theme-correction&quot; readability=&quot;36&quot;&gt;&lt;strong&gt;Correction: January 24, 2018&lt;/strong&gt;&lt;p&gt;An earlier version of this article misstated when Apple plans to allow consumers to try testing the new Health app feature that allows users to automatically download and see parts of their medical records on their iPhones. Consumers will be allowed to test a beta version of the feature on Thursday, not Friday.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;a class=&quot;visually-hidden skip-to-text-link&quot; href=&quot;https://www.nytimes.com/2018/01/24/technology/Apple-iPhone-medical-records.html#whats-next&quot;&gt;Continue reading the main story&lt;/a&gt;</description>
<pubDate>Wed, 24 Jan 2018 17:25:44 +0000</pubDate>
<dc:creator>alwillis</dc:creator>
<og:url>https://www.nytimes.com/2018/01/24/technology/Apple-iPhone-medical-records.html</og:url>
<og:type>article</og:type>
<og:title>Apple, in Sign of Health Ambitions, Adds Medical Records Feature for iPhone</og:title>
<og:description>A new feature on the iPhone Health app will allow users to automatically download blood test results and other data from their health care providers.</og:description>
<og:image>https://static01.nyt.com/images/2018/01/24/business/24applehealth-1/24applehealth-1-facebookJumbo.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nytimes.com/2018/01/24/technology/Apple-iPhone-medical-records.html</dc:identifier>
</item>
<item>
<title>How to solve most NLP problems: a step-by-step guide</title>
<link>https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e</link>
<guid isPermaLink="true" >https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e</guid>
<description>&lt;p name=&quot;e097&quot; id=&quot;e097&quot; class=&quot;graf graf--p graf--leading&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;This post is accompanied by&lt;/em&gt;&lt;/strong&gt; &lt;a href=&quot;https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb&quot; data-href=&quot;https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;an interactive notebook&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;demonstrating and applying all these techniques. Feel free to run the code and follow along!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h3 name=&quot;3693&quot; id=&quot;3693&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Step 1: Gather your data&lt;/h3&gt;
&lt;h4 name=&quot;faf5&quot; id=&quot;faf5&quot; class=&quot;graf graf--h4 graf-after--h3&quot;&gt;Example data sources&lt;/h4&gt;
&lt;p name=&quot;f3e1&quot; id=&quot;f3e1&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Every Machine Learning problem starts with data, such as a list of emails, posts, or tweets. Common sources of textual information include:&lt;/p&gt;
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;8e91&quot; id=&quot;8e91&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;Product reviews (on Amazon, Yelp, and various App Stores)&lt;/li&gt;
&lt;li name=&quot;cadd&quot; id=&quot;cadd&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;User-generated content (Tweets, Facebook posts, StackOverflow questions)&lt;/li&gt;
&lt;li name=&quot;e017&quot; id=&quot;e017&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Troubleshooting (customer requests, support tickets, chat logs)&lt;/li&gt;
&lt;/ul&gt;&lt;p name=&quot;bb40&quot; id=&quot;bb40&quot; class=&quot;graf graf--p graf--startsWithDoubleQuote graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;“Disasters on Social Media” dataset&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;851c&quot; id=&quot;851c&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;For this post, we will use a dataset generously provided by &lt;a href=&quot;https://www.crowdflower.com/data-for-everyone/&quot; data-href=&quot;https://www.crowdflower.com/data-for-everyone/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;CrowdFlower&lt;/a&gt;, called “Disasters on Social Media”, where:&lt;/p&gt;
&lt;blockquote name=&quot;eca3&quot; id=&quot;eca3&quot; class=&quot;graf graf--blockquote graf-after--p&quot; readability=&quot;11&quot;&gt;
&lt;p&gt;Contributors looked at over 10,000 tweets culled with a variety of searches like “ablaze”, “quarantine”, and “pandemonium”, then noted whether the tweet referred to a disaster event (as opposed to a joke with the word or a movie review or something non-disastrous).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;82a2&quot; id=&quot;82a2&quot; class=&quot;graf graf--p graf-after--blockquote&quot;&gt;Our task will be to detect which tweets are about a &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;disastrous event&lt;/strong&gt; as opposed to an &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;irrelevant topic&lt;/strong&gt; such as a movie. Why? A potential application would be to exclusively notify law enforcement officials about urgent emergencies while ignoring reviews of the most recent Adam Sandler film. A particular challenge with this task is that both classes contain the same search terms used to find the tweets, so we will have to use subtler differences to distinguish between them.&lt;/p&gt;
&lt;p name=&quot;d7ec&quot; id=&quot;d7ec&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In the rest of this post, we will refer to tweets that are about disasters as “&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;disaster&lt;/strong&gt;”, and tweets about anything else as “&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;irrelevant&lt;/strong&gt;”.&lt;/p&gt;
&lt;h4 name=&quot;7fb7&quot; id=&quot;7fb7&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Labels&lt;/h4&gt;
&lt;p name=&quot;e2d8&quot; id=&quot;e2d8&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;We have labeled data and so we know which tweets belong to which categories. As Richard Socher outlines below, it is usually faster, simpler, and cheaper to &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;find and label enough data&lt;/strong&gt; to train a model on, rather than trying to optimize a complex unsupervised method.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*CdnxyA_fMXxEcEQ1kUTFRg.png&quot; data-width=&quot;1232&quot; data-height=&quot;634&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*CdnxyA_fMXxEcEQ1kUTFRg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*CdnxyA_fMXxEcEQ1kUTFRg.png&quot;/&gt;&lt;/div&gt;
Richard Socher’s pro-tip
&lt;h3 name=&quot;2a98&quot; id=&quot;2a98&quot; class=&quot;graf graf--h3 graf-after--figure&quot;&gt;Step 2: Clean your data&lt;/h3&gt;
&lt;blockquote name=&quot;4a39&quot; id=&quot;4a39&quot; class=&quot;graf graf--blockquote graf-after--h3&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;The number one rule we follow is: “Your model will only ever be as good as your data.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;05a8&quot; id=&quot;05a8&quot; class=&quot;graf graf--p graf-after--blockquote&quot;&gt;One of the key skills of a data scientist is knowing whether the next step should be working on the model or the data. A good rule of thumb is to look at the data first and then clean it up. &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;A clean dataset will allow a model to learn meaningful features and not overfit on irrelevant noise.&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;ed00&quot; id=&quot;ed00&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Here is a checklist to use to clean your data: (see the &lt;a href=&quot;https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb&quot; data-href=&quot;https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;code&lt;/a&gt; for more details):&lt;/p&gt;
&lt;ol class=&quot;postList&quot;&gt;&lt;li name=&quot;4f9b&quot; id=&quot;4f9b&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;Remove all irrelevant characters such as any non alphanumeric characters&lt;/li&gt;
&lt;li name=&quot;9b14&quot; id=&quot;9b14&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html&quot; data-href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Tokenize&lt;/a&gt; your text by separating it into individual words&lt;/li&gt;
&lt;li name=&quot;e441&quot; id=&quot;e441&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Remove words that are not relevant, such as “@” twitter mentions or urls&lt;/li&gt;
&lt;li name=&quot;3b94&quot; id=&quot;3b94&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Convert all characters to lowercase, in order to treat words such as “hello”, “Hello”, and “HELLO” the same&lt;/li&gt;
&lt;li name=&quot;61d0&quot; id=&quot;61d0&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Consider combining misspelled or alternately spelled words to a single representation (e.g. “cool”/”kewl”/”cooool”)&lt;/li&gt;
&lt;li name=&quot;5ebc&quot; id=&quot;5ebc&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Consider &lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html&quot; data-href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;lemmatization&lt;/a&gt; (reduce words such as “am”, “are”, and “is” to a common form such as “be”)&lt;/li&gt;
&lt;/ol&gt;&lt;p name=&quot;6c30&quot; id=&quot;6c30&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;After following these steps and checking for additional errors, we can start using the clean, labelled data to train models!&lt;/p&gt;
&lt;h3 name=&quot;4808&quot; id=&quot;4808&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Step 3: Find a good data representation&lt;/h3&gt;
&lt;p name=&quot;bf8a&quot; id=&quot;bf8a&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Machine Learning models take numerical values as input. Models working on images, for example, take in a matrix representing the intensity of each pixel in each color channel.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*6pW5mPAxYhYBZxkc-hKf0A.png&quot; data-width=&quot;1050&quot; data-height=&quot;410&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*6pW5mPAxYhYBZxkc-hKf0A.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*6pW5mPAxYhYBZxkc-hKf0A.png&quot;/&gt;&lt;/div&gt;
A smiling face represented as a matrix of numbers.
&lt;p name=&quot;a538&quot; id=&quot;a538&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Our dataset is a list of sentences, so in order for our algorithm to extract patterns from the data, we first need to find a way to represent it in a way that our algorithm can understand, i.e. as a list of numbers.&lt;/p&gt;
&lt;h4 name=&quot;8d9f&quot; id=&quot;8d9f&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;One-hot encoding (Bag of Words)&lt;/h4&gt;
&lt;p name=&quot;09e0&quot; id=&quot;09e0&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;A natural way to represent text for computers is to encode each character individually as a number (&lt;a href=&quot;https://en.wikipedia.org/wiki/ASCII&quot; data-href=&quot;https://en.wikipedia.org/wiki/ASCII&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;ASCII&lt;/a&gt; for example). If we were to feed this simple representation into a classifier, it would have to learn the structure of words from scratch based only on our data, which is impossible for most datasets. We need to use a higher level approach.&lt;/p&gt;
&lt;p name=&quot;4c10&quot; id=&quot;4c10&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;For example, we can build a &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;vocabulary&lt;/strong&gt; of all the unique words in our dataset, and associate a unique index to each word in the vocabulary. Each sentence is then represented as a list that is as long as the number of distinct words in our vocabulary. At each index in this list, we mark how many times the given word appears in our sentence. This is called a &lt;a href=&quot;https://en.wikipedia.org/wiki/Bag-of-words_model&quot; data-href=&quot;https://en.wikipedia.org/wiki/Bag-of-words_model&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Bag of Words&lt;/strong&gt;&lt;/a&gt; &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;model&lt;/strong&gt;, since it is a representation that completely ignores the order of words in our sentence. This is illustrated below.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*oQ3suk0Ayc8z8i1QIl5Big.png&quot; data-width=&quot;2356&quot; data-height=&quot;566&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*oQ3suk0Ayc8z8i1QIl5Big.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*oQ3suk0Ayc8z8i1QIl5Big.png&quot;/&gt;&lt;/div&gt;
Representing sentences as a Bag of Words. Sentences on the left, representation on the right. Each index in the vectors represent one particular word.
&lt;h4 name=&quot;ae55&quot; id=&quot;ae55&quot; class=&quot;graf graf--h4 graf-after--figure&quot;&gt;Visualizing the embeddings&lt;/h4&gt;
&lt;p name=&quot;2ec8&quot; id=&quot;2ec8&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;We have around 20,000 words in our vocabulary in the “Disasters of Social Media” example, which means that every sentence will be represented as a vector of length 20,000. The vector will contain &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;mostly 0s&lt;/strong&gt; because each sentence contains only a very small subset of our vocabulary.&lt;/p&gt;
&lt;p name=&quot;6551&quot; id=&quot;6551&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In order to see whether our embeddings are capturing information that is &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;relevant to our problem&lt;/strong&gt; (i.e. whether the tweets are about disasters or not), it is a good idea to visualize them and see if the classes look well separated. Since vocabularies are usually very large and visualizing data in 20,000 dimensions is impossible, techniques like &lt;a href=&quot;https://en.wikipedia.org/wiki/Principal_component_analysis&quot; data-href=&quot;https://en.wikipedia.org/wiki/Principal_component_analysis&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;PCA&lt;/a&gt; will help project the data down to two dimensions. This is plotted below.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*ikis3EFujlrmVk_JEQMvzQ.png&quot; data-width=&quot;1686&quot; data-height=&quot;1618&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*ikis3EFujlrmVk_JEQMvzQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*ikis3EFujlrmVk_JEQMvzQ.png&quot;/&gt;&lt;/div&gt;
Visualizing Bag of Words embeddings.
&lt;p name=&quot;72b1&quot; id=&quot;72b1&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The two classes do not look very well separated, which could be a feature of our embeddings or simply of our dimensionality reduction. In order to see whether the Bag of Words features are of any use, we can train a classifier based on them.&lt;/p&gt;
&lt;h3 name=&quot;671a&quot; id=&quot;671a&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Step 4: Classification&lt;/h3&gt;
&lt;p name=&quot;92e2&quot; id=&quot;92e2&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;When first approaching a problem, a general best practice is to start with the simplest tool that could solve the job. Whenever it comes to classifying data, a common favorite for its versatility and explainability is &lt;a href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot; data-href=&quot;https://en.wikipedia.org/wiki/Logistic_regression&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Logistic Regression&lt;/a&gt;. It is very simple to train and the results are interpretable as you can easily extract the most important coefficients from the model.&lt;/p&gt;
&lt;p name=&quot;0386&quot; id=&quot;0386&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;We split our data in to a training set used to fit our model and a test set to see how well it generalizes to unseen data. After training, we get an &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;accuracy of 75.4%.&lt;/strong&gt; Not too shabby! Guessing the most frequent class (“irrelevant”) would give us only 57%. However, even if 75% precision was good enough for our needs, &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;we should never ship a model without trying to understand it.&lt;/strong&gt;&lt;/p&gt;
&lt;h3 name=&quot;2dc1&quot; id=&quot;2dc1&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Step 5: Inspection&lt;/h3&gt;
&lt;h4 name=&quot;5405&quot; id=&quot;5405&quot; class=&quot;graf graf--h4 graf-after--h3&quot;&gt;Confusion Matrix&lt;/h4&gt;
&lt;p name=&quot;8cef&quot; id=&quot;8cef&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;A first step is to understand the types of errors our model makes, and which kind of errors are least desirable. In our example, &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;false positives&lt;/strong&gt; are classifying an irrelevant tweet as a disaster, and &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;false negatives&lt;/strong&gt; are classifying a disaster as an irrelevant tweet. If the priority is to react to every potential event, we would want to lower our false negatives. If we are constrained in resources however, we might prioritize a lower false positive rate to reduce false alarms. A good way to visualize this information is using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Confusion_matrix&quot; data-href=&quot;https://en.wikipedia.org/wiki/Confusion_matrix&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Confusion Matrix&lt;/a&gt;, which compares the predictions our model makes with the true label. Ideally, the matrix would be a diagonal line from top left to bottom right (our predictions match the truth perfectly).&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*DicbwUOoqFezDWROfsZ-CQ.png&quot; data-width=&quot;1388&quot; data-height=&quot;1288&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*DicbwUOoqFezDWROfsZ-CQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*DicbwUOoqFezDWROfsZ-CQ.png&quot;/&gt;&lt;/div&gt;
Confusion Matrix (Green is a high proportion, blue is low)
&lt;p name=&quot;e3f3&quot; id=&quot;e3f3&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Our classifier creates more false negatives than false positives (proportionally). In other words, our model’s most common error is inaccurately classifying disasters as irrelevant. If false positives represent a high cost for law enforcement, this could be a good bias for our classifier to have.&lt;/p&gt;
&lt;h4 name=&quot;23d1&quot; id=&quot;23d1&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Explaining and interpreting our model&lt;/h4&gt;
&lt;p name=&quot;be7c&quot; id=&quot;be7c&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;To validate our model and interpret its predictions, it is important to look at which words it is using to make decisions. If our data is biased, our classifier will make make accurate predictions in the sample data, but the model would not generalize well in the real world. Here we plot the &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;most important words&lt;/strong&gt; for both the disaster and irrelevant class. Plotting word importance is simple with Bag of Words and Logistic Regression, since we can just extract and rank the coefficients that the model used for its predictions.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*UicK4XWFmF8mh0LL_fUJcA.png&quot; data-width=&quot;1136&quot; data-height=&quot;1140&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*UicK4XWFmF8mh0LL_fUJcA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*UicK4XWFmF8mh0LL_fUJcA.png&quot;/&gt;&lt;/div&gt;
Bag of Words: Word importance
&lt;p name=&quot;dc4f&quot; id=&quot;dc4f&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Our classifier correctly picks up on some patterns (hiroshima, massacre), but clearly seems to be overfitting on some meaningless terms (heyoo, x1392). Right now, our Bag of Words model is dealing with a huge vocabulary of different words and &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;treating all words equally&lt;/strong&gt;. However, some of these words are very frequent, and are only contributing noise to our predictions. Next, we will try a way to represent sentences that can account for the frequency of words, to see if we can pick up more signal from our data.&lt;/p&gt;
&lt;h3 name=&quot;425f&quot; id=&quot;425f&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Step 6: Accounting for vocabulary structure&lt;/h3&gt;
&lt;h4 name=&quot;0c9f&quot; id=&quot;0c9f&quot; class=&quot;graf graf--h4 graf-after--h3&quot;&gt;TF-IDF&lt;/h4&gt;
&lt;p name=&quot;952b&quot; id=&quot;952b&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;In order to help our model focus more on meaningful words, we can use a &lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot; data-href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;TF-IDF score&lt;/a&gt; (Term Frequency, Inverse Document Frequency) on top of our Bag of Words model. TF-IDF weighs words by how rare they are in our dataset, discounting words that are too frequent and just add to the noise. Here is the PCA projection of our new embeddings.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*wjD8-Cq009lMkRvfcVaW0w.png&quot; data-width=&quot;1698&quot; data-height=&quot;1630&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*wjD8-Cq009lMkRvfcVaW0w.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*wjD8-Cq009lMkRvfcVaW0w.png&quot;/&gt;&lt;/div&gt;
Visualizing TF-IDF embeddings.
&lt;p name=&quot;4dd4&quot; id=&quot;4dd4&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;We can see above that there is a clearer distinction between the two colors. This should make it easier for our classifier to separate both groups. Let’s see if this leads to better performance. Training another Logistic Regression on our new embeddings, we get &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;an accuracy of 76.2%.&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;c970&quot; id=&quot;c970&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;A very slight improvement. Has our model has started picking up on more important words? If we are getting a better result while preventing our model from “cheating” then we can truly consider this model an upgrade.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*bdCuSrONh3dI6r8Xmzurcg.png&quot; data-width=&quot;1130&quot; data-height=&quot;1102&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*bdCuSrONh3dI6r8Xmzurcg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*bdCuSrONh3dI6r8Xmzurcg.png&quot;/&gt;&lt;/div&gt;
TF-IDF: Word importance
&lt;p name=&quot;dea5&quot; id=&quot;dea5&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The words it picked up look much more relevant! Although our metrics on our test set only increased slightly, we have much more confidence in the terms our model is using, and thus would feel more comfortable deploying it in a system that would interact with customers.&lt;/p&gt;
&lt;h3 name=&quot;ca70&quot; id=&quot;ca70&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Step 7: Leveraging semantics&lt;/h3&gt;
&lt;h4 name=&quot;f445&quot; id=&quot;f445&quot; class=&quot;graf graf--h4 graf-after--h3&quot;&gt;Word2Vec&lt;/h4&gt;
&lt;p name=&quot;599f&quot; id=&quot;599f&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Our latest model managed to pick up on high signal words. However, it is very likely that if we deploy this model, we will encounter words that we have not seen in our training set before. The previous model will not be able to accurately classify these tweets, &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;even if it has seen very similar words during training&lt;/strong&gt;.&lt;/p&gt;
&lt;p name=&quot;b23e&quot; id=&quot;b23e&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;To solve this problem, we need to capture the &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;semantic meaning of words&lt;/strong&gt;, meaning we need to understand that words like ‘good’ and ‘positive’ are closer than ‘apricot’ and ‘continent.’ The tool we will use to help us capture meaning is called Word2Vec.&lt;/p&gt;
&lt;p name=&quot;0171&quot; id=&quot;0171&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Using pre-trained words&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;5180&quot; id=&quot;5180&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1301.3781&quot; data-href=&quot;https://arxiv.org/abs/1301.3781&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Word2Vec&lt;/a&gt; is a technique to find continuous embeddings for words. It learns from reading massive amounts of text and memorizing which words tend to appear in similar contexts. After being trained on enough data, it generates a 300-dimension vector for each word in a vocabulary, with words of similar meaning being closer to each other.&lt;/p&gt;
&lt;p name=&quot;6c76&quot; id=&quot;6c76&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The authors of the &lt;a href=&quot;https://arxiv.org/abs/1301.3781&quot; data-href=&quot;https://arxiv.org/abs/1301.3781&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;paper&lt;/a&gt; open sourced a model that was pre-trained on a very large corpus which we can leverage to include some knowledge of semantic meaning into our model. The pre-trained vectors can be found in the &lt;a href=&quot;https://github.com/hundredblocks/concrete_NLP_tutorial&quot; data-href=&quot;https://github.com/hundredblocks/concrete_NLP_tutorial&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;repository&lt;/a&gt; associated with this post.&lt;/p&gt;
&lt;h4 name=&quot;86ed&quot; id=&quot;86ed&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Sentence level representation&lt;/h4&gt;
&lt;p name=&quot;9d62&quot; id=&quot;9d62&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;A quick way to get a sentence embedding for our classifier is to average Word2Vec scores of all words in our sentence. This is a Bag of Words approach just like before, but this time &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;we only lose the syntax of our sentence, while keeping some semantic information.&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*THo9NKchWkCAOILvs1eHuQ.png&quot; data-width=&quot;2306&quot; data-height=&quot;910&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*THo9NKchWkCAOILvs1eHuQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*THo9NKchWkCAOILvs1eHuQ.png&quot;/&gt;&lt;/div&gt;
Word2Vec sentence embedding
&lt;p name=&quot;82e9&quot; id=&quot;82e9&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Here is a visualization of our new embeddings using previous techniques:&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*Dt66MaGpumPGxHfAwooieA.png&quot; data-width=&quot;1712&quot; data-height=&quot;1630&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*Dt66MaGpumPGxHfAwooieA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*Dt66MaGpumPGxHfAwooieA.png&quot;/&gt;&lt;/div&gt;
Visualizing Word2Vec embeddings.
&lt;p name=&quot;8b9b&quot; id=&quot;8b9b&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The two groups of colors look even more separated here, our new embeddings should help our classifier find the separation between both classes. After training the same model a third time (a Logistic Regression), we get &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;an accuracy score of 77.7%&lt;/strong&gt;, our best result yet! Time to inspect our model.&lt;/p&gt;
&lt;h4 name=&quot;3f73&quot; id=&quot;3f73&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;The Complexity/Explainability trade-off&lt;/h4&gt;
&lt;p name=&quot;ff12&quot; id=&quot;ff12&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Since our embeddings are not represented as a vector with one dimension per word as in our previous models, it’s harder to see which words are the most relevant to our classification. While we still have access to the coefficients of our Logistic Regression, they relate to the 300 dimensions of our embeddings rather than the indices of words.&lt;/p&gt;
&lt;p name=&quot;8eb6&quot; id=&quot;8eb6&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;For such a low gain in accuracy, losing all explainability seems like a harsh trade-off. However, with more complex models we can we can leverage &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;black box explainers&lt;/strong&gt; such as &lt;a href=&quot;https://arxiv.org/abs/1602.04938&quot; data-href=&quot;https://arxiv.org/abs/1602.04938&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;LIME&lt;/a&gt; in order to get some insight into how our classifier works.&lt;/p&gt;
&lt;p name=&quot;3234&quot; id=&quot;3234&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;LIME&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;7914&quot; id=&quot;7914&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;LIME is &lt;a href=&quot;https://github.com/marcotcr/lime&quot; data-href=&quot;https://github.com/marcotcr/lime&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;available on Github&lt;/a&gt; through an open-sourced package. A a black-box explainer allows users to explain the decisions of any classifier &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;on one particular example&lt;/strong&gt; by perturbing the input (in our case removing words from the sentence) and seeing how the prediction changes.&lt;/p&gt;
&lt;p name=&quot;cc44&quot; id=&quot;cc44&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Let’s see a couple explanations for sentences from our dataset.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*w8zlZbHildSWpEWW_bBcAw.png&quot; data-width=&quot;1772&quot; data-height=&quot;394&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*w8zlZbHildSWpEWW_bBcAw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*w8zlZbHildSWpEWW_bBcAw.png&quot;/&gt;&lt;/div&gt;
Correct disaster words are picked up to classify as “relevant”.
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*44zlmsnb-hENy0BXSsmzfg.png&quot; data-width=&quot;1540&quot; data-height=&quot;386&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*44zlmsnb-hENy0BXSsmzfg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*44zlmsnb-hENy0BXSsmzfg.png&quot;/&gt;&lt;/div&gt;
Here, the contribution of the words to the classification seems less obvious.
&lt;p name=&quot;d8ed&quot; id=&quot;d8ed&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;However, we do not have time to explore the thousands of examples in our dataset. What we’ll do instead is run LIME on a representative sample of test cases and see which words keep coming up as strong contributors. Using this approach we can get word importance scores like we had for previous models and validate our model’s predictions.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*YxFOFx_kxovglm_OLZD9Sw.png&quot; data-width=&quot;1220&quot; data-height=&quot;1152&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*YxFOFx_kxovglm_OLZD9Sw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*YxFOFx_kxovglm_OLZD9Sw.png&quot;/&gt;&lt;/div&gt;
Word2Vec: Word importance
&lt;p name=&quot;5c17&quot; id=&quot;5c17&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Looks like the model picks up highly relevant words implying that it appears to make understandable decisions. These seem like the most relevant words out of all previous models and therefore we’re more comfortable deploying in to production.&lt;/p&gt;
&lt;h3 name=&quot;f99a&quot; id=&quot;f99a&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Step 8: Leveraging syntax using end-to-end approaches&lt;/h3&gt;
&lt;p name=&quot;9971&quot; id=&quot;9971&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;We’ve covered quick and efficient approaches to generate compact sentence embeddings. However, by omitting the order of words, we are discarding all of the syntactic information of our sentences. If these methods do not provide sufficient results, you can utilize more complex model that take in whole sentences as input and predict labels without the need to build an intermediate representation. A common way to do that is to treat a sentence as &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;a sequence of individual word vectors&lt;/strong&gt; using either Word2Vec or more recent approaches such as &lt;a href=&quot;https://nlp.stanford.edu/projects/glove/&quot; data-href=&quot;https://nlp.stanford.edu/projects/glove/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;GloVe&lt;/a&gt; or &lt;a href=&quot;https://arxiv.org/abs/1708.00107&quot; data-href=&quot;https://arxiv.org/abs/1708.00107&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;CoVe&lt;/a&gt;. This is what we will do below.&lt;/p&gt;
</description>
<pubDate>Wed, 24 Jan 2018 17:17:39 +0000</pubDate>
<dc:creator>e_ameisen</dc:creator>
<og:title>How to solve 90% of NLP problems: a step-by-step guide</og:title>
<og:url>https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*J8-5VgoWtMsIJhyHAuljSw.png</og:image>
<og:description>Using Machine Learning to understand and leverage text.</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e?gi=dc85a569912c</dc:identifier>
</item>
<item>
<title>Show HN: RetroClip – Instant Replay for Your Mac</title>
<link>https://www.realartists.com/blog/retroclip-instant-replay-for-your-mac.html</link>
<guid isPermaLink="true" >https://www.realartists.com/blog/retroclip-instant-replay-for-your-mac.html</guid>
<description>&lt;header&gt;
&lt;/header&gt;&lt;footer class=&quot;post-info&quot;&gt;&lt;abbr class=&quot;published&quot; title=&quot;2018-01-24T09:00:00-08:00&quot;&gt;Wed 24 January 2018&lt;/abbr&gt;
&lt;address class=&quot;vcard author&quot;&gt;By &lt;a class=&quot;url fn&quot; href=&quot;https://www.realartists.com/blog/authors/james-howard.html&quot;&gt;James Howard&lt;/a&gt;&lt;/address&gt;
&lt;/footer&gt;&lt;p&gt;We have a new app &lt;a href=&quot;https://itunes.apple.com/us/app/retroclip/id1332064978?ls=1&amp;amp;mt=12&quot;&gt;out on the Mac App Store.&lt;/a&gt; It's called RetroClip, and it makes taking instant replay video captures of your Mac's screen as easy as taking a screenshot. There is &lt;a href=&quot;https://www.realartists.com/retroclip/&quot;&gt;a browser based demo&lt;/a&gt; you can try if you're on a Mac or a PC (and you really should try it because it's super cool).&lt;/p&gt;
&lt;p&gt;I got the idea to write RetroClip after playing the game &lt;a href=&quot;https://www.epicgames.com/fortnite&quot;&gt;Fortnite Battle Royale&lt;/a&gt; and winning, and then having nothing to show for it besides a static screenshot.&lt;sup id=&quot;fnref-1&quot;/&gt; Current generation video game consoles all have a feature where you can press a button and capture the last minute or so of gameplay and I wanted this for my Mac. The key idea is that you don't know when you want to save a video clip until after something exciting has happened, so it needs to work retroactively — it's no good if you have to press a button to start the recording because it's already too late.&lt;/p&gt;
&lt;p&gt;Unlike other screen capture utilities for macOS, RetroClip does not save to an ever growing scratch file on disk as it is recording. Instead, it saves video frames to a fixed size circular buffer in main memory. The neat thing is, we don't even need that much memory to do a good job. We can fit 30 seconds of HD video in around 40MB of memory.&lt;sup id=&quot;fnref-2&quot;/&gt; Once you see something you want to save, just press the RetroClip keyboard shortcut, and RetroClip writes the memory region with the video data out to disk, essentially instantaneously.&lt;/p&gt;
&lt;p&gt;Anyway, you should &lt;a href=&quot;https://itunes.apple.com/us/app/retroclip/id1332064978?ls=1&amp;amp;mt=12&quot;&gt;download it&lt;/a&gt; and give it a try. Even if you don't use it for games, you may find it useful for capturing those pesky bug reports where you can't seem to reproduce it a second time. Please let us know what you think, either by &lt;a href=&quot;mailto:support@realartists.com&quot;&gt;email&lt;/a&gt; or on &lt;a href=&quot;https://twitter.com/ShipRealArtists&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I've been code golfing RetroClip in my spare time for the past couple of months. It actually only took me a few hours to get an initial version of the app up and running, but eking out the maximum performance became something of an obsession. It's not just for bragging rights, though, the idea is to be able to leave RetroClip running all the time, even while playing games, watching videos, compiling code, writing blog posts, and so on. The more efficient RetroClip can be, the more is left over for doing important things, and for saving battery life.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;If you're interested in the inner workings of RetroClip, read on because this section is for you. If not, that's fine, there's no reason to know this stuff anyway.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&quot;screen-capture&quot;&gt;&lt;a class=&quot;toclink&quot; href=&quot;https://www.realartists.com/blog/retroclip-instant-replay-for-your-mac.html#screen-capture&quot;&gt;Screen Capture&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Screen Capture in macOS inevitably starts with the Window Server. The Window Server is the process in macOS responsible for keeping track of windows and their contents and for compositing them together to make the image you see on your display.&lt;sup id=&quot;fnref-3&quot;/&gt; As an application, we can ask the Window Server to send us the screen contents as they're displayed via the &lt;a href=&quot;https://developer.apple.com/documentation/coregraphics/quartz_display_services?language=objc&quot;&gt;CGDisplayStream APIs&lt;/a&gt;. When we do this, the Window Server will feed us, via IPC, &lt;a href=&quot;https://developer.apple.com/documentation/iosurface&quot;&gt;IOSurfaces&lt;/a&gt;, which are pointers to shared graphics texture memory, as fast as either it can create them or as fast as we can consume them, whichever is slower.&lt;/p&gt;
&lt;p&gt;Once we get the pointers to the texture data, we can turn around and feed this into the hardware H.264 video encoder which is present in most Macs via the &lt;a href=&quot;https://developer.apple.com/documentation/videotoolbox/vtcompressionsession-7bn&quot;&gt;VideoToolbox APIs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Everything described so far all works great, and it is a highly optimized area of macOS as it's used in important built-in features such as AirPlay Mirroring. For convenience, it's even wrapped up in a &lt;a href=&quot;https://developer.apple.com/documentation/avfoundation/avcapturescreeninput&quot;&gt;higher level AVFoundation API&lt;/a&gt; that works pretty well (and this is either the basis of QuickTime Player's screen recording functionality or their implementations are substantially similar).&lt;/p&gt;
&lt;p&gt;With a bit of effort, however, it is possible to optimize things further and save some CPU time along the way. We can't get around asking the Window Server to do some work on our behalf to get the display data in the first place, and we can't really get around having to H.264 encode the images.&lt;sup id=&quot;fnref-4&quot;/&gt; But, there is still a place to save time: the mouse cursor.&lt;/p&gt;
&lt;p&gt;The mouse cursor is special. It lives in an overlay that is composited on top of everything else by the GPU at the last moment before an image is shown on screen. This actually makes a difference in how responsive your computer feels. If, when you move the mouse, the Window Server had to re-composite the entire scene, and the mouse cursor was locked to vsync, you can actually, subtilely, feel it as being laggy. You may have experienced this effect when playing some full screen video games that attempt to implement their own cursor rendering. You can also notice this effect when turning on AirPlay Mirroring, as this apparently forces the Window Server into a mode where it will composite the cursor along with everything else, as opposed to using the hardware cursor overlay.&lt;/p&gt;
&lt;p&gt;So going back to the &lt;a href=&quot;https://developer.apple.com/documentation/coregraphics/kcgdisplaystreamshowcursor&quot;&gt;CGDisplayStream API&lt;/a&gt;, notice that we can ask the Window Server to composite the cursor for us. This works just fine and produces the correct picture, but disables the hardware cursor overlay, which will slowly drive you crazy. So, to avoid a slow descent into madness, the best approach is to instruct the Window Server to not include the mouse cursor in the display stream and just composite it myself on my own time. And, in fact, this is the approach that QuickTime Player and AVCaptureScreenInput also employ. However, I know a way to do it faster.&lt;/p&gt;
&lt;p&gt;My trick is to observe that the IOSurfaces sent over to my application from the Window Server are writeable. The Window Server apparently rotates through a small handful of them, so I can't hang on to them for more than a few frames or I risk a backlog, but what I can do is quickly modify them, and this is a lot faster than trying to copy them. So, I do a quick &lt;a href=&quot;https://developer.apple.com/documentation/metal/mtlblitcommandencoder&quot;&gt;blit&lt;/a&gt; to a small scratch texture to save aside the pixels in the area where the cursor is, then I composite the cursor into the original full screen texture I got from the Window Server. If the mouse cursor moves without a screen update (which can and should happen when the Window Server is using the hardware cursor overlay), then I can use that scratch texture I saved aside to undo the cursor compositing I did earlier, and then I have a clean canvas on which to re-composite the cursor at its new location.&lt;/p&gt;
&lt;p&gt;I think the mouse cursor trick is the main source of my performance advantage over QuickTime Player/AVCaptureScreenInput, along with generally careful programming to avoid unnecessary copies, allocations, and indirections. On my machine, RetroClip typically uses about one-fifth of the CPU time to capture the same content at 60 frames per second as AVCaptureScreenInput does. Finally, if there is no mouse cursor, because you're playing a game without one or you're watching a video, then we can go down an even faster code path and avoid all of the cursor compositing magic.&lt;/p&gt;
&lt;h3 id=&quot;media-buffering&quot;&gt;&lt;a class=&quot;toclink&quot; href=&quot;https://www.realartists.com/blog/retroclip-instant-replay-for-your-mac.html#media-buffering&quot;&gt;Media Buffering&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;RetroClip stores the encoded H.264 video frames in memory, rather than immediately writing them out to disk like QuickTime Player does. The goal in RetroClip for buffering this video is to hold it in memory as efficiently as possible, and to have stable memory use over days or weeks of use. Additionally, when the user requests to save a clip, we want to do this as quickly as possible.&lt;/p&gt;
&lt;p&gt;My first pass was to just put &lt;a href=&quot;https://developer.apple.com/documentation/coremedia/cmsamplebuffer-u71&quot;&gt;the reference counted pointers to the frames&lt;/a&gt; output from the video encoder into &lt;a href=&quot;https://developer.apple.com/documentation/coremedia/cmbufferqueue-u8p&quot;&gt;a queue&lt;/a&gt;. This works OK, but it has two downsides: first, we have to lock whenever doing a save operation until we've had a chance to duplicate the queue and increment the retain counts on all of the sample buffers within it, and second, keeping thousands of these variably sized things scattered around turns the heap into swiss cheese after a while.&lt;/p&gt;
&lt;p&gt;The solution to both of these problems is to use a &lt;a href=&quot;https://en.wikipedia.org/wiki/Circular_buffer&quot;&gt;circular queue&lt;/a&gt; to store the encoded video data end to end without any fragmentation. As new frames are generated, we write them at the head, and we can forget about old frames at the tail simply by incrementing a pointer.&lt;/p&gt;
&lt;p&gt;Mike Ash wrote a very interesting &lt;a href=&quot;https://www.mikeash.com/pyblog/friday-qa-2012-02-03-ring-buffers-and-mirrored-memory-part-i.html&quot;&gt;series of articles&lt;/a&gt; about using mach virtual memory tricks to avoid having to expose segmented objects that are split across the edge of a circular queue, and you should definitely read it if you haven't already, but I actually don't need to use this trick myself in RetroClip. This is because the code that writes out the mp4 file &lt;a href=&quot;https://developer.apple.com/documentation/coremedia/cmblockbuffer-u9i&quot;&gt;already can easily handle&lt;/a&gt; a split frame if necessary.&lt;/p&gt;
&lt;p&gt;Fortunately, there is still an opportunity to do a mach virtual memory trick. When the user requests a clip to be saved, we can make use of the rarely used &lt;a href=&quot;https://developer.apple.com/documentation/kernel/1585336-vm_remap?language=objc&quot;&gt;vm_remap&lt;/a&gt; function to make a quick copy of the video data. An I/O thread gets the copy and writes it out to disk, and the media buffer thread keeps on going with the original.&lt;/p&gt;
&lt;p&gt;What's neat is, the copy isn't really a copy, it's just some new virtual memory page table entries pointing to the same physical memory containing the video data as the original. Only as new frames are received and written to the original do the contents of the circular buffers begin to diverge, and the kernel handles finding new memory to store the data for us transparently. And because doing a contiguous write of 40 or 80 MB from memory to disk is incredibly fast these days, much faster than the incoming data rate from the H.264 encoder, there really isn't much copy on write that the kernel needs to do for us, at the most maybe a couple of megabytes.&lt;/p&gt;

&lt;p&gt;As purveyors of &lt;a href=&quot;https://www.realartists.com/&quot;&gt;real native Mac software&lt;/a&gt;, Nick and I occasionally worry that we are ignoring the advancing capabilities of web browsers as application platforms. Regarding our first product a couple years ago, a Hacker News reader confidently wrote:&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;Also all the stated reasons for using native are actually wrong and just as possible in web app, or will be by end of 2016.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As it's already 2018, we knew we had to get with the times. While at first, I just &lt;a href=&quot;https://www.realartists.com/retroclip/static/&quot;&gt;wanted to make a video, write a blurb, and have a Mac App Store link&lt;/a&gt; for our website, Nick quickly convinced me that I could and should &lt;a href=&quot;https://www.realartists.com/retroclip/&quot;&gt;do better&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So, last week, armed with nothing else besides the latest version of Safari and my trusty &lt;a href=&quot;https://www.barebones.com/products/bbedit/&quot;&gt;text editor&lt;/a&gt;, I set off to port RetroClip to the web.&lt;/p&gt;
&lt;p&gt;My first challenge was to figure out how to do H.264 encoding in a web browser. Using the hardware encoder was obviously off limits, but I figured doing software encoding on a &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API&quot;&gt;background thread&lt;/a&gt; would have to suffice. I found a &lt;a href=&quot;https://github.com/Kagami/ffmpeg.js&quot;&gt;couple&lt;/a&gt; of &lt;a href=&quot;https://bgrins.github.io/videoconverter.js/&quot;&gt;projects&lt;/a&gt; that aimed at doing it, but I was hoping to do better than 25MB of uncompressed javascript.&lt;/p&gt;
&lt;p&gt;I wasn't about to write my own H.264 encoder, but I thought maybe if I got &lt;a href=&quot;https://www.videolan.org/developers/x264.html&quot;&gt;x264&lt;/a&gt; built as &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/WebAssembly&quot;&gt;web assembly&lt;/a&gt; with as little extra glue code as needed to get RGB data from the browser into H.264 video in an MP4 container and back, that would be good enough. And, it turns out, &lt;a href=&quot;https://github.com/realartists/raw2mp4&quot;&gt;it is&lt;/a&gt;. It works out to about 850KB of uncompressed web assembly, and it will encode scaled image data to H.264 much faster than real time (at least on decent computers).&lt;/p&gt;
&lt;p&gt;Once the video encoding piece of the puzzle was solved, I was confident that RetroClip for Web could become a reality. All I needed now was to create for the web a Window Server, a menu bar, some Cocoa view hierarchy, window management, event handling code, &lt;a href=&quot;https://developer.apple.com/documentation/appkit/nsvisualeffectview&quot;&gt;NSVisualEffectView&lt;/a&gt;, &lt;a href=&quot;https://developer.apple.com/documentation/foundation/nsusernotificationcenter&quot;&gt;NSUserNotificationCenter&lt;/a&gt;, and the &lt;a href=&quot;https://support.apple.com/en-us/HT206997&quot;&gt;picture in picture&lt;/a&gt; media player introduced in macOS Sierra, and then I'd have everything I needed to port and run RetroClip on the web!&lt;/p&gt;
&lt;p&gt;In the end, it wasn't too hard. The HTML &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D&quot;&gt;Canvas&lt;/a&gt; element is basically the same thing as &lt;a href=&quot;https://developer.apple.com/documentation/coregraphics/cgcontext&quot;&gt;CGContext&lt;/a&gt;, so reimplementing a macOS UI on the web proceeds somewhat naturally.&lt;/p&gt;
&lt;p&gt;Granted, an environment in which one can only run RetroClip is a bit too self-referential to be of any practical use beyond marketing &lt;a href=&quot;https://itunes.apple.com/us/app/retroclip/id1332064978?ls=1&amp;amp;mt=12&quot;&gt;RetroClip for Mac&lt;/a&gt;, but I figure, with the ground work I've already done, I will have a leg up on porting other Cocoa apps to the web. Hey, it worked for &lt;a href=&quot;https://en.wikipedia.org/wiki/280_North,_Inc.&quot;&gt;280 North&lt;/a&gt; a few years ago, and now with the advent of exciting new web technologies it might be time to revisit the idea.&lt;sup id=&quot;fnref-5&quot;/&gt;&lt;/p&gt;

&lt;p&gt;As you can tell from this way too long blog post, a lot of work went in to RetroClip. If you're a &lt;a href=&quot;https://www.realartists.com/&quot;&gt;Ship&lt;/a&gt; customer, you're probably wondering how this affects Ship. The answer is, we're still working hard on Ship, and I wrote RetroClip in my spare time and decided to release it because it's neat.&lt;/p&gt;
&lt;p&gt;If you're not already a Ship user, and you use GitHub and a Mac, you should &lt;a href=&quot;https://www.realartists.com/&quot;&gt;check it out&lt;/a&gt;.&lt;/p&gt;

</description>
<pubDate>Wed, 24 Jan 2018 16:24:50 +0000</pubDate>
<dc:creator>kogir</dc:creator>
<og:type>website</og:type>
<og:title>Ship</og:title>
<og:description>Fast, native, comprehensive issue tracking for GitHub</og:description>
<og:url>https://www.realartists.com/</og:url>
<og:image>https://www.realartists.com/image/big-icon-2x.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.realartists.com/blog/retroclip-instant-replay-for-your-mac.html</dc:identifier>
</item>
<item>
<title>Why we transitioned from Medium back to our own blog</title>
<link>https://baremetrics.com/blog/medium-back-to-blog</link>
<guid isPermaLink="true" >https://baremetrics.com/blog/medium-back-to-blog</guid>
<description>&lt;p&gt;&lt;em&gt;Where&lt;/em&gt; to publish something has become a difficult decision for a lot of businesses. You read so many stories about using various channels to distribute content and grow traffic, it’s hard to know what does and doesn’t work. Medium, in particular, has become a major player in the world of startup content, but is it really that great?&lt;/p&gt;
&lt;p&gt;I’ve certainly been tempted by the prospect of loads of traffic and tons of likes (or “fans” as Medium calls them now). But how does that translate in to something meaningful for the business?&lt;/p&gt;
&lt;p&gt;This is our story of how we’ve used Medium, what worked, what didn’t and what our strategy is now, after kicking the proverbial tires for a couple of years.&lt;/p&gt;
&lt;h2&gt;Growing with content marketing&lt;/h2&gt;
&lt;p&gt;Before Baremetrics, at previous companies I’d started, I’d dabbled in content marketing. But not the quality stuff…the mass-produced, pay-somebody-$20-to-knock-out-500-words-around-a-random-SEO-keyword kind of stuff. As you can imagine, I got nothing out of it. Totally worthless, across the board.&lt;/p&gt;
&lt;p&gt;So when I started Baremetrics, I knew I wanted to do things differently.&lt;/p&gt;
&lt;p&gt;My first step towards this actually started before I even launched Baremetrics. I &lt;a href=&quot;https://twitter.com/i/moments/788809454565396480&quot;&gt;live-tweeted a lot of the process&lt;/a&gt; of getting Baremetrics launched and was very open from the start with how the business was doing.&lt;/p&gt;
&lt;p&gt;Early on I decided to make “transparency” a core part of the business, both in how we operated and also our strategy around marketing. Three months after launching, I &lt;a href=&quot;https://baremetrics.com/blog/inside-our-financials-baremetrics-demo&quot;&gt;made all of our numbers public&lt;/a&gt;, and from then on just started sharing everything I could think of.&lt;/p&gt;
&lt;p&gt;That was my entire content marketing strategy early on: &lt;strong&gt;share it all&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;That stance really helped grow the blog pretty quickly. I was publishing weekly about all the things I was trying and it got quite a bit of momentum, making the blog a place people checked regularly and came to expect new content from each week.&lt;/p&gt;
&lt;h2&gt;Then came Medium&lt;/h2&gt;
&lt;p&gt;About two years in, I found myself reading a ton of content on Medium and various businesses were moving their publications to Medium (the most prominent of which, in my mind, was &lt;a href=&quot;https://m.signalvnoise.com/signal-v-noise-moves-to-medium-c8083ce19686&quot;&gt;Basecamp’s Signal v. Noise&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I’d pumped out dozens and dozens of articles on our own blog to pretty great success, but Medium started looking like some really green grass on the other side. Not just from the “Ooooo, traffic!” prospect, but Medium also has a pretty active community, and I selfishly wanted the endorphin rush.&lt;/p&gt;
&lt;p&gt;I wasn’t really sure how to approach it. Should we wholesale move over every article from our blog to Medium? Do we post new content to Medium and repost to our own blog later? How would we even technically handle that from an SEO duplicate-content standpoint? Would it cannibalize newsletter signups?&lt;/p&gt;
&lt;p&gt;Lots of questions, but very little data out there. Not because people hadn’t done it, but because so much of it is based on your goals and what you actually want to get out of it.&lt;/p&gt;
&lt;h2&gt;The switch&lt;/h2&gt;
&lt;p&gt;What originally sealed the deal on making the switch to Medium was when I posted &lt;a href=&quot;https://medium.com/baremetrics-founders-journey/is-your-company-really-only-doing-45-000-per-month-a2098bf29bf5&quot;&gt;an email I’d received from an investor&lt;/a&gt;. That gave me my first taste of success on Medium.&lt;/p&gt;
&lt;p&gt;Our game plan was to post all new content to our Medium publication and set it up on a subdomain (blog.baremetrics.com). We’d keep the old blog, and then just post links to the new content as it was created. We’d also slowly transition past content over to the Medium publication as well.&lt;/p&gt;
&lt;p&gt;For most of 2017, all new blog posts went directly to Medium. Honestly, it was really enjoyable. Publishing on Medium is so effortless. Formatting content and uploading images takes basically zero time. It’s really the best web-based publishing experience I’ve had to date.&lt;/p&gt;
&lt;p&gt;I didn’t pay a whole lot of attention to stats. I just knew we were getting lots of likes and highlights and it was generating meaningful discussion, so it felt like a win.&lt;/p&gt;
&lt;h2&gt;Second guessing the switch&lt;/h2&gt;
&lt;p&gt;After about a year of posting on Medium, traffic on our own marketing site was (rightfully) taking a hit. Newsletter signups were also slumping and there was just palpably less “buzz” around Baremetrics content in general.&lt;/p&gt;
&lt;p&gt;I realized Medium is really great about surfacing content, but it removes the &lt;em&gt;face&lt;/em&gt; of it. It neutralizes all content to basically be author-agnostic. It’s like Walmart or Amazon in that you can buy from thousands of different brands, but you rarely actually know what brand you’re buying…you just know “I got it from Amazon.”&lt;/p&gt;
&lt;p&gt;Same with content on Medium. Sure, you can see who the author is or what publication it’s on, but ultimately your takeaway is “I read this article on Medium”, and that’s not what I wanted. I wanted to get back to people saying “I read this article on Baremetrics”.&lt;/p&gt;
&lt;p&gt;On top of that, the amount of views a given article would get ultimately weren’t that impressive. At least not much/any more than our own blog (given we’d already built up a nice readership).&lt;/p&gt;
&lt;p&gt;As an example, our top posts on Medium had total Views in the 8-10k range (with our best ever getting around 40k, though that’s an outlier).&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-full wp-image-10946&quot; src=&quot;https://baremetrics-wordpress.imgix.net/20180123191319/Screen-Shot-2018-01-23-at-12.38.28-PM.png&quot; alt=&quot;&quot; width=&quot;1061&quot; height=&quot;486&quot; srcset=&quot;https://baremetrics-wordpress.imgix.net/20180123191319/Screen-Shot-2018-01-23-at-12.38.28-PM.png 1061w, https://baremetrics-wordpress.imgix.net/20180123191319/Screen-Shot-2018-01-23-at-12.38.28-PM-300x137.png 300w, https://baremetrics-wordpress.imgix.net/20180123191319/Screen-Shot-2018-01-23-at-12.38.28-PM-768x352.png 768w, https://baremetrics-wordpress.imgix.net/20180123191319/Screen-Shot-2018-01-23-at-12.38.28-PM-1024x469.png 1024w&quot; sizes=&quot;(max-width: 1061px) 100vw, 1061px&quot;/&gt;&lt;/p&gt;
&lt;p&gt;After those top 5, the Views dropped off pretty drastically to just 500-1k per article.&lt;/p&gt;
&lt;p&gt;Whereas our own blog, beforehand, we were getting multiples of that on basically every article.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-full wp-image-10947&quot; src=&quot;https://baremetrics-wordpress.imgix.net/20180123191340/Screen-Shot-2018-01-23-at-12.41.11-PM.png&quot; alt=&quot;&quot; width=&quot;769&quot; height=&quot;976&quot; srcset=&quot;https://baremetrics-wordpress.imgix.net/20180123191340/Screen-Shot-2018-01-23-at-12.41.11-PM.png 769w, https://baremetrics-wordpress.imgix.net/20180123191340/Screen-Shot-2018-01-23-at-12.41.11-PM-236x300.png 236w, https://baremetrics-wordpress.imgix.net/20180123191340/Screen-Shot-2018-01-23-at-12.41.11-PM-768x975.png 768w&quot; sizes=&quot;(max-width: 769px) 100vw, 769px&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The numbers just didn’t make sense. Yes, I could put &lt;em&gt;more&lt;/em&gt; into Medium and try to build up readership even more. The guys at Basecamp regularly get 250k+ views on their content. But doing that helps &lt;em&gt;Medium&lt;/em&gt; the most in the long run. They’ve been fumbling left and right trying to figure out how to make Medium sustainable, and I’m just not convinced they’ll always do what’s best for us and our business.&lt;/p&gt;
&lt;h2&gt;Switching back&lt;/h2&gt;
&lt;p&gt;Now I didn’t want to throw out distribution on Medium entirely. There can definitely be some benefit to &lt;em&gt;syndicating&lt;/em&gt; content there. It’s essentially another distribution channel to expose people to our content.&lt;/p&gt;
&lt;p&gt;So we needed a game plan on how we could still make use of Medium as a distribution channel without cannibalizing our own readership or SEO work.&lt;/p&gt;
&lt;p&gt;That game plan consisted of two main things.&lt;/p&gt;
&lt;h3&gt;Removing the custom domain&lt;/h3&gt;
&lt;p&gt;We decided to stop using “blog.baremetrics.com” on Medium and redirected all of those URLs back to the same content on “baremetrics.com/blog”. This process was relatively painless.&lt;/p&gt;
&lt;p&gt;Ahead of time I added all the missing content from Medium to our own blog, then I setup 301 Redirects. I pointed “blog.baremetrics.com” to our marketing site, which runs on WordPress, and using the fantastic &lt;a href=&quot;https://redirection.me&quot;&gt;Redirection plugin&lt;/a&gt; was able to monitor any 404 errors from those Medium redirects and immediately create the necessary 301 Redirects from that.&lt;/p&gt;
&lt;p&gt;The only hiccup here is that you have to email Medium support to remove the custom domain (something I didn’t realize beforehand), so there was about a 12-hour period where all the articles on Medium weren’t accessible due to Medium forcing the incorrect URL.&lt;/p&gt;
&lt;h3&gt;Republishing new content to Medium&lt;/h3&gt;
&lt;p&gt;Going forward, we are still going to publish to Medium, but with two big caveats.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;We’ll publish new content two weeks later to Medium (so the initial publishing of the content is able to get solidified as the primary source from an SEO standpoint).&lt;/li&gt;
&lt;li&gt;We’ll use Medium’s &lt;a href=&quot;https://medium.com/p/import&quot;&gt;Import tool&lt;/a&gt; to publish the content. Medium buries this thing, but what it does is lets us republish on Medium and have them set the &lt;a href=&quot;https://support.google.com/webmasters/answer/139066?hl=en&quot;&gt;canonical URL&lt;/a&gt; to the original post on your own website. That’s a big kick from the SEO side of things as it tells Google that your original post is the main one and should be given preference in search results.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;With this method we still own the original content, get the benefits of being the primary source and get the distribution benefits of Medium!&lt;/p&gt;
&lt;p&gt;By making ourselves the original, authoritative source, we’re able to control the whole experience &lt;em&gt;for the long term&lt;/em&gt; and &lt;em&gt;to our benefit&lt;/em&gt; instead of potential &lt;em&gt;short term wins&lt;/em&gt; to &lt;em&gt;Medium’s benefit&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;What about you?&lt;/h2&gt;
&lt;p&gt;So what about you? What’s worked for you as you build up content and readership?&lt;/p&gt;
</description>
<pubDate>Wed, 24 Jan 2018 15:46:28 +0000</pubDate>
<dc:creator>uptown</dc:creator>
<og:type>article</og:type>
<og:title>Why we transitioned from Medium back to our own blog - Baremetrics</og:title>
<og:description>Where to publish something has becoming a difficult decision for a lot of businesses. You read so many stories about using various channels to distribute content and grow traffic, it’s hard to know what does and doesn’t work. Medium, in particular, has become a major player in the world of startup content, but is it really that great?</og:description>
<og:url>https://baremetrics.com/blog/medium-back-to-blog</og:url>
<og:image>https://baremetrics-wordpress.imgix.net/20180123191834/photo-1499750310107-5fef28a66643.jpeg</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://baremetrics.com/blog/medium-back-to-blog</dc:identifier>
</item>
</channel>
</rss>