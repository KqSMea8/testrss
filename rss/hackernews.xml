<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>YC China and Qi Lu</title>
<link>https://blog.ycombinator.com/yc-china-qi-lu/</link>
<guid isPermaLink="true" >https://blog.ycombinator.com/yc-china-qi-lu/</guid>
<description>&lt;div class=&quot;post-content&quot; readability=&quot;70.818414322251&quot;&gt;
&lt;p&gt;I am delighted to announce that &lt;a href=&quot;https://www.youtube.com/watch?v=WSydk0XzxEE&quot;&gt;Qi Lu&lt;/a&gt; is joining YC to launch and run YC China. I have been trying to recruit Qi for many years—he is one of the most impressive technologists I know.&lt;/p&gt;
&lt;p&gt;Our mission at YC is to enable more innovation than any other company in the world, and to ensure that the benefits of that are fairly spread throughout humanity. We believe that technology drives innovation, that startups can do very ambitious things if they think on a long enough time horizon, and that hackers can change the world.&lt;/p&gt;
&lt;p&gt;Qi embodies all of these values, and China has been an important missing piece of our puzzle—the entrepreneurial energy and talent there is an amazing force. Qi will be able to take what makes YC work and adapt it for China. We are excited for Qi to come onboard as the Founding CEO of YC China and to build a long-term local organization that will combine the best of Silicon Valley and China and create a lot of innovation.&lt;/p&gt;
&lt;p&gt;Qi will also take over as the Head of YC Research, YC’s non-profit research lab. More investment in R&amp;amp;D is critical to a good future for the world, and we plan to establish a new YC Research location in Seattle to drive more research.&lt;/p&gt;
&lt;p&gt;YC has been very successful in the world by believing that hackers – not businesspeople – can build the biggest companies. We are confident that with Qi at the helm, YC China will do the same.&lt;/p&gt;
&lt;p&gt;We think that a significant percentage of the largest technology companies that are founded in the next decade—companies at the scale of Google, Microsoft, Apple, Amazon, and Facebook—will be based in the US and China. YC’s greatest strength is our founder community and with the launch of YC China we believe we have a special opportunity to include many more Chinese founders in our global community.&lt;/p&gt;
&lt;p&gt;This has taken a lot of work from a lot of people, but I’d especially like to thank Eric Migicovsky, who has done a huge amount of work to establish YC in China!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;mc_embed_signup&quot;&gt;
&lt;h3 id=&quot;mc_copy&quot;&gt;Sign up for weekly updates from Y Combinator.&lt;/h3&gt;

&lt;/div&gt;
</description>
<pubDate>Wed, 15 Aug 2018 02:46:48 +0000</pubDate>
<dc:creator>sama</dc:creator>
<og:title>YC China + Qi Lu</og:title>
<og:url>https://blog.ycombinator.com/yc-china-qi-lu/</og:url>
<og:type>article</og:type>
<og:description>I am delighted to announce that Qi Lu is joining YC to launch and run YC China. We are excited for Qi to come onboard as the Founding CEO of YC China and to build a long-term local organization that will combine the best of Silicon Valley and China and create a lot of innovation.</og:description>
<og:image>https://blog.ycombinator.com/wp-content/uploads/2018/08/YC-China-Qi-Lu.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.ycombinator.com/yc-china-qi-lu/</dc:identifier>
</item>
<item>
<title>Tinder founders sue parent companies Match and IAC for at least $2B</title>
<link>https://techcrunch.com/2018/08/14/tinder-match-lawsuit/</link>
<guid isPermaLink="true" >https://techcrunch.com/2018/08/14/tinder-match-lawsuit/</guid>
<description>&lt;p&gt;A group of &lt;a class=&quot;crunchbase-link&quot; href=&quot;https://www.crunchbase.com/organization/tinder/&quot; target=&quot;_blank&quot; data-type=&quot;organization&quot; data-entity=&quot;tinder&quot;&gt;Tinder&lt;/a&gt; founders and executives has filed a lawsuit against parent company Match Group and its controlling shareholder IAC.&lt;/p&gt;
&lt;p&gt;The plaintiffs in the suit include Tinder co-founders &lt;a class=&quot;crunchbase-link&quot; href=&quot;https://www.crunchbase.com/person/sean-rad/&quot; target=&quot;_blank&quot; data-type=&quot;person&quot; data-entity=&quot;sean-rad&quot;&gt;Sean Rad,&lt;/a&gt; Justin Mateen and Jonathan Badeen — Badeen still works at Tinder, as do plaintiffs James Kim (the company’s vice president of finance) and Rosette Pambakian (its vice president of marketing and communications).&lt;/p&gt;
&lt;p&gt;We’ve reached out to &lt;a class=&quot;crunchbase-link&quot; href=&quot;https://www.crunchbase.com/organization/iac/&quot; target=&quot;_blank&quot; data-type=&quot;organization&quot; data-entity=&quot;iac&quot;&gt;IAC&lt;/a&gt; for comment, as well as Pambakian, who’s served as our main contact at Tinder. We’ll update the post if we hear back.&lt;/p&gt;
&lt;p&gt;The suit alleges that IAC and &lt;a class=&quot;crunchbase-link&quot; href=&quot;https://www.crunchbase.com/organization/match-group/&quot; target=&quot;_blank&quot; data-type=&quot;organization&quot; data-entity=&quot;match-group&quot;&gt;Match Group&lt;/a&gt; manipulated financial data in order to create “a fake lowball valuation” (to quote &lt;a href=&quot;http://www.tindercase.com&quot;&gt;the plaintiffs’ press release&lt;/a&gt;), then stripped Rad, Mateen, Badeen and others of their stock options. It points to &lt;a href=&quot;https://techcrunch.com/2016/12/08/sean-rad-steps-away-from-ceo-role-again-to-run-tinders-new-swipe-ventures/&quot;&gt;the removal of Rad as CEO&lt;/a&gt;, as well as other management changes, as moves designed “to allow Defendants to control the valuation of Tinder and deprive Tinder optionholders of their right to participate in the company’s future success.”&lt;/p&gt;
&lt;p&gt;The lawsuit also alleges that Greg Blatt, the Match CEO who became CEO of Tinder, groped and sexually harassed Pambakian at the company’s 2016 holiday party, supposedly leading the company to “whitewash” his actions long enough for him to complete the valuation of Tinder and its merger with Match Group, and then to &lt;a href=&quot;https://www.prnewswire.com/news-releases/match-group-names-mandy-ginsberg-to-succeed-greg-blatt-as-ceo-300497940.html&quot;&gt;announce his departure&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In response, the plaintiffs are asking for “compensatory damages in an amount to be determined at trial, but not less than $2,000,000,000.”&lt;/p&gt;
&lt;p&gt;“We were always concerned about IAC’s reputation for ignoring their contractual commitments and acting like the rules don’t apply to them,” Rad said in the release. “But we never imagined the lengths they would go to cheat all the people who built Tinder. The Tinder team — especially the plaintiffs who are currently senior leaders at the company — have shown tremendous strength in exposing IAC/Match’s systematic violation of employees’ rights.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; We’ve just received the following joint statement from IAC and Match Group.&lt;/p&gt;
&lt;blockquote readability=&quot;18&quot;&gt;
&lt;p&gt;The allegations in the complaint are meritless, and IAC and Match Group intend to vigorously defend against them.&lt;/p&gt;
&lt;p&gt;Since Tinder’s inception, Match Group has paid out in excess of a billion dollars in equity compensation to Tinder’s founders and employees. With respect to the matters alleged in the complaint, the facts are simple: Match Group and the plaintiffs went through a rigorous, contractually – defined valuation process involving two independent global investment banks, and Mr. Rad and his merry band of plaintiffs did not like the outcome. Mr. Rad (who was dismissed from the Company a year ago) and Mr. Mateen (who has not been with the Company in years) may not like the fact that Tinder has experienced enormous success following their respective departures, but sour grapes alone do not a lawsuit make. Mr. Rad has a rich history of outlandish public statements, and this lawsuit contains just another series of them. We look forward to defending our position in court.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a title=&quot;View As-filed complaint.pdf on Scribd&quot; href=&quot;https://www.scribd.com/document/386197360/As-filed-complaint-pdf#from_embed&quot;&gt;As-filed complaint.pdf&lt;/a&gt; by &lt;a title=&quot;View TechCrunch's profile on Scribd&quot; href=&quot;https://www.scribd.com/publisher/17551504/TechCrunch#from_embed&quot;&gt;TechCrunch&lt;/a&gt; on Scribd&lt;/p&gt;

</description>
<pubDate>Tue, 14 Aug 2018 18:57:59 +0000</pubDate>
<dc:creator>hvo</dc:creator>
<og:title>Tinder founders sue parent companies Match and IAC for at least $2B</og:title>
<og:description>A group of Tinder founders and executives has filed a lawsuit against parent company Match Group and its controlling shareholder IAC. The plaintiffs in the suit include Tinder co-founders Sean Rad, Justin Mateen and Jonathan Badeen — Badeen still works at Tinder, as do plaintiffs James Kim (the com…</og:description>
<og:image>https://techcrunch.com/wp-content/uploads/2015/05/sean-rad81.jpg?w=600</og:image>
<og:url>http://social.techcrunch.com/2018/08/14/tinder-match-lawsuit/</og:url>
<og:type>article</og:type>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://techcrunch.com/2018/08/14/tinder-match-lawsuit/</dc:identifier>
</item>
<item>
<title>Understanding L1 Terminal Fault aka Foreshadow: What You Need to Know</title>
<link>https://www.redhat.com/en/blog/understanding-l1-terminal-fault-aka-foreshadow-what-you-need-know</link>
<guid isPermaLink="true" >https://www.redhat.com/en/blog/understanding-l1-terminal-fault-aka-foreshadow-what-you-need-know</guid>
<description>&lt;h4&gt;L1 Terminal Fault/Foreshadow explained in ~three minutes&lt;/h4&gt;
&lt;p&gt;&lt;iframe allowfullscreen=&quot;allowfullscreen&quot; frameborder=&quot;0&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/kBOsVt0iXE4&quot; width=&quot;560&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For a more detailed technical view of L1 Terminal Fault, please see this &lt;a href=&quot;https://www.redhat.com/en/blog/deeper-look-l1-terminal-fault-aka-foreshadow&quot;&gt;deeper dive&lt;/a&gt; with Jon Masters.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;How we got here: a brief history of modern microprocessor caches&lt;/h4&gt;
&lt;p&gt;Modern computer microprocessors are complex machines, filled with many optimizations designed to squeeze out every last ounce of performance available. The earliest stored program computers were, by contrast, relatively simple machines. They executed the programs provided by the user, following each of the binary encoded instructions precisely, in the order given, delivering predictable program flow and results. This worked well in the early days of computing in large part because different parts of the computer operated at similar speeds. The processor (CPU) wasn’t much faster than the memory chips attached to it, for example. As a result, the processor could simply wait to load data from memory as a program needed it.&lt;/p&gt;
&lt;p&gt;Over time, the relative performance of the different components of computers changed dramatically. Microprocessors continued to increase in terms of frequency from a few thousands of instructions per second to millions, and eventually billions of instructions per second. At the same time, transistor counts (area) that could fit onto a single chip also increased by many orders of magnitude, again from thousands to the many billions of transistors found on today’s high-end processors. This allowed for increasingly complex designs. Yet, while the other parts of the computer also advanced (memory chips went from a few megabytes to many gigabytes), relative processor performance became much faster. As a direct consequence of this disparity in performance, processors began to become bottlenecked on slower external memory, often waiting hundreds, thousands, or millions of “cycles” for data.&lt;/p&gt;
&lt;p&gt;Academia came to the rescue with the innovation of computer caches. Caches store copies of recently used data much closer to the processor’s functional units (the parts that perform calculations). The data values are stored in slower external memory and as they are loaded by the program, they are also stored in the processor caches. In fact, modern processors (almost) never operate directly on data stored in external memory. Instead, they are optimized to work on data contained within the caches. These caches form a hierarchy in which the highest level (L1) is closest to the “core” of the processor, while lower levels (L2, L3, etc.) are conceptually further away. Each level has different characteristics.&lt;/p&gt;
&lt;p&gt;In digital electronics, it is often said that you can have “small and fast” or “big and slow”, but rarely can you have both. Thus, the L1 data cache within a modern processor is only on the order of 32 kilobytes (barely enough for a single cat photo) while the L3 (sometimes called LLC - Last Level Cache) closer to the external memory might be 32 megabytes or even larger in today’s highest-end servers. The manner in which caches are designed (“organized”) varies from one design to another, but in a classical inclusive design, copies of the same data will be contained within multiple levels of the cache depending upon when it was last used. A replacement algorithm within the processor will “evict” less recently used entries from the L1 data cache in order to load new data while keeping copies within the L2 or L3. Thus, it is rare that recently used data must be loaded from memory, but it may well come from one of the larger, slower, lower level caches into the L1 when needed.&lt;/p&gt;
&lt;p&gt;Caches are a shared resource between multiple individual cores, threads, and programs. Modern processor chips contain multiple cores. Each of these behaves just as a traditional single processor computer. Each may execute individual programs or run multiple threads of the same program and operate with shared memory. Each core has its own L1 cache and may have its own L2 cache as well, while the larger L3 is usually shared by all of the cores in a processor. A technique known as cache “coherency” is then employed by the processor to keep each core’s internal copy of a memory location in sync with any copy stored in the cache of another core. This is achieved by the cores tracking ownership of memory and sending small messages to one another behind the scenes each time memory is updated.&lt;/p&gt;
&lt;p&gt;The shared nature of caches is both a benefit, as well as a potential source of exploitation. We learned from the “Spectre” and “Meltdown” vulnerabilities that caches can serve as a potential “side-channel” through which information may be leaked. In side-channel analysis, data is not directly accessed but is instead inferred due to some property of the system under observation. In the case of caches, the relative difference in performance between cache memory, and the much slower main memory external to a processor is the whole point of using caches to begin with. Unfortunately, this same difference can be exploited by measuring the relative difference in access times to determine whether one memory location is in the cache or not. If it is contained within the cache, it has been recently accessed as a result of some other processor activity. As an example, data loaded during speculative execution by a processor will alter the cache state.&lt;/p&gt;
&lt;h4&gt;Virtual and physical memory addressing&lt;/h4&gt;
&lt;p&gt;Modern processor caches typically use a combination of virtual and physical memory addresses to reference data. Physical addresses are used to access main memory. Conceptually, you can think of the external memory DIMM or DDR chips in your computer as being a giant array of values beginning at address zero and continuing upward until the memory is exhausted. The amount of physical memory varies from one machine to another, from 8GB in a typical laptop to hundreds of gigabytes or even more in contemporary high-end server machines. Programmers once had to worry about physical memory when writing programs. In the earlier days, it was necessary for the programmer to explicitly track what was contained in each physical memory location and to avoid potential conflict with other applications that use memory.&lt;/p&gt;
&lt;p&gt;Today’s machines use virtual memory. Virtual memory means that the operating system is able to present each application with its own conceptually isolated view of the world. Programs see memory as a nearly infinite range within which they can do whatever they like. Every time the program accesses a memory location, the address is translated by special hardware within the processor known as the Memory Management Unit (MMU). The MMU works in consort with the operating system (OS), which creates and manages a set of page tables that translate virtual memory addresses into physical ones. Physical memory is divided into tiny chunks known as pages that are typically 4KB in size. Page tables contain translations for these pages such that one 4KB size range of virtual addresses will translate into a 4KB range of physical ones. As a further optimization, page tables are hierarchical in nature with a single address being decoded through a sequence of “walks” through several layers of tables until it has been fully translated.&lt;/p&gt;
&lt;p&gt;MMUs contain special hardware that can read and even update the OS managed page tables. These include page table “walkers” that go through the process of a table walk, as well as additional hardware that can update page table entries to indicate recently accessed data. The latter is used by the OS for tracking what data can be temporarily “paged” or “swapped” out to disk if it has not recently been used. A page table entry can be marked as “not present”, meaning that any attempt to access the associated address will generate a special condition known as a “page fault” that signals the OS to take action. Thus, the OS can intercept attempts to access data that has been previously swapped out to disk, pulling it back into memory, and resuming an application without it noticing this has happened. As a result, paging is used to create the illusion of having more physical memory than actually exists.&lt;/p&gt;
&lt;p&gt;As you might imagine, page table walks can be expensive from a performance perspective. OS managed page tables live in real physical memory which must be loaded into the processor when read. Walking a page table can take quite a few such memory accesses, which would be prohibitively slow were it to happen every time. So instead of doing this walk on each memory access by a program, the processor will cache the result of these table walks in a separate processor structure, known as a Translation Lookaside Buffer (or TLB). Recently used translations are thus much faster to resolve into physical addresses because the processor need only search the TLB. If an entry does not exist, then the processor will perform the much more expensive page walk and populate a TLB entry, possibly evicting another entry in the process. Incidentally, we saw another unrelated attack against TLBs recently in the form of the &lt;a href=&quot;https://www.redhat.com/en/blog/temporal-side-channels-and-you-understanding-tlbleed&quot;&gt;TLBleed vulnerability&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When programs read or write to memory, these accesses go through the highest level (L1) data cache, which is (in most modern implementations) known as Virtually Indexed, Physically Tagged (VIPT). This means that the cache uses part virtual and part physical addresses to look up a memory location. As the virtual address for a load is examined, the processor will perform a simultaneous search of the TLB for the virtual to physical page translation, while beginning to search the cache for a possible matching entry by using the offset within a single page. Thus, the process of reading from a virtual memory location is extremely complicated in almost every modern processor. For the curious, this common design optimization explains why L1 data caches are typically 32KB on processors with a 4KB page size, since they are limited by the available number of offset bits within a single page to begin the cache search.&lt;/p&gt;
&lt;p&gt;Intel processors contain a further optimization in how they handle the process of a page table walk and “terminal faults” (not present). We will dive into this further after first reviewing the speculative nature of modern processors. You can skip over the next section if you’re already familiar with &lt;a href=&quot;https://www.redhat.com/en/blog/what-are-meltdown-and-spectre-heres-what-you-need-know&quot;&gt;vulnerabilities such as Meltdown&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Out-of-Order and Speculative execution&lt;/h4&gt;
&lt;p&gt;The adoption of caches enabled faster microprocessor performance improvements as compared with other parts of a modern computer platform. Academia and industry working together created foundational innovations - such as Out-of-Order (OoO) and speculative execution - that serve as the underpinning for the subsequent consistent gains in throughput and performance seen in recent years. As transistor counts increase and processors become more complex, the possible optimizations further advance, but they are all still built upon the the key insight of an OoO design.&lt;/p&gt;
&lt;p&gt;In OoO, the processor is conceptually split into an “in-order” front-end and an “out-of-order” back-end. The front-end takes as input the user program. This program is sequential in nature, formed from blocks of code and occasional branches (such as “if” statements) to other blocks based upon the conditional evaluation of data values. The in-order front end dispatches instructions contained within the program to the out-of-order backend. As each of these instructions is dispatched, an entry is allocated within a processor structure known as the Re-Order Buffer (ROB). The ROB enables data dependence tracking, realizing the key innovation of OoO that instructions can execute in any order just so long as the programmer is unable to tell the difference in the end. They only see the same effect as in a sequential execution model.&lt;/p&gt;
&lt;p&gt;The ROB effectively serves to convert an in-order machine into what is known as a “dataflow” machine, in which dependent instructions are forced to wait to execute until their input values are ready. Whenever an entry in the ROB containing a program instruction has all of its dependent data values available (e.g. loaded from memory), it will be issued to the processor functional units and the result stored back in the ROB for use by following instructions. As entries in the ROB age out (become the “oldest instruction in the machine”) they are known as retired and are made available to the programmer visible view of the machine. This is known as the “architecturally visible” state and it is identical to that obtained from a sequential execution of the program. Re-Ordering instructions in this manner provides significant speedups.&lt;/p&gt;
&lt;p&gt;Consider this example pseudocode:&lt;/p&gt;
&lt;p&gt;LOAD R1&lt;/p&gt;
&lt;p&gt;LOAD R2&lt;/p&gt;
&lt;p&gt;R3 = R1 + R2&lt;/p&gt;
&lt;p&gt;R4 = 2&lt;/p&gt;
&lt;p&gt;R5 = R4 + 1&lt;/p&gt;
&lt;p&gt;R6 = R3 + 1&lt;/p&gt;
&lt;p&gt;The example uses the letter R to designate small processor internal memory locations known as registers, or more broadly as GPRs (General Purpose Registers). There are typically only a small handful of registers, 16 in the case of Intel x86-64 machines. For convenience here, we number them R1, R2, etc. while in reality, they have other names, such as RAX, RBX, etc.&lt;/p&gt;
&lt;p&gt;In a classical sequential model of execution, the first two instructions could cause the machine to wait (“stall”) while slow external memory locations were accessed. Caches speed this up, but even if the two values are contained within some level of the processor cache, there may still be a small delay while the load instructions are completed. Rather than wait for these, an OoO machine will skip ahead, noticing that while instruction number 3 depends upon the first two (has a “data dependency”), instructions 4, and 5 are independent. In fact, those two instructions have no dependency at all upon the earlier instructions. They can be executed at any time and don’t even depend upon external memory. Their results will be stored in the ROB until such time as the earlier instructions have also completed, at which time they too will retire.&lt;/p&gt;
&lt;p&gt;Instruction number 6 in the previous example is known as a data dependency. Just like how instruction 3 depends upon the results of loading memory locations A and B, instruction 6 depends upon the result of adding those two memory locations. A real-world program will have many such dependencies, all tracked in the ROB, which might be quite large in size. As a result of the large size of this structure, it is possible to layer another innovation upon OoO.&lt;/p&gt;
&lt;p&gt;Speculation builds upon OoO execution. In speculative execution, the processor again performs instructions in a sequence different from that in which the program is written, but it also speculates beyond branches in the program code.&lt;/p&gt;
&lt;p&gt;Consider a program statement such as:&lt;/p&gt;
&lt;p&gt;if (it_is_raining)&lt;/p&gt;
&lt;p&gt;pack_umbrella();&lt;/p&gt;
&lt;p&gt;The value “it_is_raining” might be contained in (slower) external memory. As a consequence, it would be useful for the processor to be able to continue to perform useful work while waiting for the branch condition to be “resolved”. Rather than stalling (as in a classical, simpler design), a speculative processor will guess (predict) the direction of a branch based upon history. The processor will continue to execute instructions following the branch, but it will tag the results in the ROB to indicate that they are speculative and may need to be thrown away. A notion of checkpointing within the processor allows speculation to be quickly undone without becoming visible to the programmer, but some artifacts of speculative activity may still be visible.&lt;/p&gt;
&lt;p&gt;We learned from the Spectre vulnerabilities that processor branch predictors can be tricked (trained) into predicting a certain way. Then, code can be executed speculatively and have an observable effect upon the processor caches. If we can find suitable “gadgets” in existing code, we can cause deliberate speculative execution of code that ordinarily should not form part of a program flow (e.g. exceeding the bounds of an array (in Spectre-v1), and then cause dependent instructions to execute that will alter locations in the cache through which we can infer the value of data to which we should not have access). You can read more about the Spectre vulnerabilities by referring to our earlier blog.&lt;/p&gt;
&lt;h4&gt;Increasing speculation in Intel processors&lt;/h4&gt;
&lt;p&gt;Modern processors take speculation much further than simply running ahead in a program. Since the speculative apparatus has been created and is already in use, microprocessor vendors like Intel have further extended it in order to speculate upon all manner of additional possible states within the processor. This includes speculating upon the result of a page table walk by the MMU page walker during the translation of a virtual to physical memory address.&lt;/p&gt;
&lt;p&gt;Intel defines the term “terminal fault” to mean the condition that arises whenever a Page Table Entry (PTE) is found to be “not present” during a page table walk. This typically happens because an operating system (OS) has swapped a page out to disk (or not yet demand loaded it) and marked the page as not present in order to trigger a later fault on access. As explained earlier, the illusion of swapping allows an OS to provide much more virtual memory than physical memory within the machine. Page faults will occur in the case of a not present page, and the OS can then determine what memory location needs to be swapped back in from disk.&lt;/p&gt;
&lt;p&gt;The OS does this by using the bits of a “not present” PTE to store various housekeeping data, such as the physical location on disk containing the content of the page. The Intel Software Developer’s Manual (SDM) states that pages marked as not present in this fashion will have all of the remaining bits (other than the present bit) ignored such that they are available to the OS. Both Linux, Windows, and other operating systems make heavy use of these PTE bits for “not present” pages in order to support swapping, as well as for various other permitted purposes.&lt;/p&gt;
&lt;p&gt;As mentioned previously, processors such as Intel’s use a common optimization in which virtual address translation is performed in parallel with cache access to the Virtually Indexed, Physically Tagged (VIPT) L1 data cache. As a further optimization, Intel realized that in the optimal case (critical logic path) a data value loaded from memory is present in the cache, and there is a valid page table translation for it. Put another way, it is less likely that the page table has an entry that is marked “not present”. As a result, modern Intel processors delay handling the present bit check slightly and forward the content of Page Table Entries (PTEs) directly to the cache control logic while simultaneously performing all of the other checks, including whether the entry is valid.A new set of speculation vulnerabilities&lt;/p&gt;
&lt;p&gt;As in Meltdown before it,  the ROB is tagged to indicate if a “not present” fault should be raised, but meanwhile, the processor will continue to speculate slightly further in the program until this fault takes effect. During this small window, any data values present in the L1 data cache for the “not present” page will nonetheless be forwarded to dependent instructions. An attack similar in concept to Meltdown can be used to read data from physical addresses if a “not present” page table entry can be created (or cause to be created) for the address, and if that physical address is currently present in the L1 data cache. This is known as an L1 Terminal Fault attack.&lt;/p&gt;
&lt;p&gt;On Linux, an attacker could exploit this vulnerability and attempt to compromise the kernel or another application through a malicious use of the mprotect() system call to cause a “not present” page table entry for a physical address of interest that might be in the cache. If they can then trick the other application (or the kernel) into loading a secret of interest - such as a cryptographic key, password, or other sensitive data - then they can extract it using an attack similar in nature to the Meltdown exploit code. This attack may be mitigated by changing how Linux generates “not present” PTEs such that certain physical address bits are always set in the PTE (using an inexpensive masking operation), thus the processor will still forward a physical address in the “not present” case, but it will appear to be a large physical address that is outside of the range of populated physical memory in all but the most extreme cases.&lt;/p&gt;
&lt;h4&gt;Beyond bare metal&lt;/h4&gt;
&lt;p&gt;The L1TF attack against bare metal machines is trivial to mitigate through a few lines of kernel code (that is available in all of our errata releases, and has also been submitted for inclusion in upstream Linux). This mitigation has no measurable performance impact and requires systems be promptly patched . If that were the end of it, this blog post might not be necessary, and nor would the inevitable attention that will be paid to this latest vulnerability.&lt;/p&gt;
&lt;p&gt;Unfortunately, there are several other components to this vulnerability.&lt;/p&gt;
&lt;p&gt;One relates to Software Guard Extensions (SGX). SGX is an Intel technology, also referred to as a “secure enclave” in which users can provide secure software code that will run in a special protected “enclave” that will keep that software from being observed by even the operating system. The typical use case of SGX is to provide tampering protection for rights management, encryption, and other software. Red Hat does not ship SGX software, which is typically owned and managed directly by third-parties. Intel has protected SGX by issuing processor microcode updates designed to prevent it from being compromised through the “Foreshadow” SGX specific variant of the L1TF vulnerability. Red Hat is providing access to courtesy microcode updates to assist in deploying this mitigation.&lt;/p&gt;
&lt;p&gt;Another variant of L1TF concerns virtualization use cases. In virtualized deployments, Intel processors implement a technology known as EPT (Extended Page Tables) in which page tables are jointly managed by both a hypervisor, a guest operating system running under that hypervisor, and the hardware. EPT replaces an older software-only approach in which the hypervisor was forced to use shadow page tables. In the older design, each time a guest operating system wanted to update its own page tables, the hypervisor would have to trap (stop the guest), update its own shadow tables (as used by the real hardware), and resume the guest. This was necessary to ensure that a guest could never try to create page tables accessing memory disallowed by the hypervisor.&lt;/p&gt;
&lt;p&gt;EPT significantly improve performance because a guest operating system can manage its own page tables just as it would on bare metal. Under EPT, each memory access is translated multiple times, first using the guest page tables from a guest virtual address to a guest physical address, and then by the Hypervisor page tables from a guest physical address to a host physical address. This process allows all of the benefit of native bare metal hardware assisted page translation while still allowing the hypervisor to retain control since it can arrange for various traps to occur, and manage which guest physical memory is available.&lt;/p&gt;
&lt;p&gt;When Intel implemented EPT, it was an extension of the existing paging infrastructure. It seems likely that the extra stage of translation was simply added beyond the traditional one. This worked well, with the exception of one small problem. A terminal fault condition arising in the guest virtual to physical address translation can result in an untranslated guest physical address being treated as a host physical address, and forwarded on to the L1 data cache. Thus, it is possible for a malicious guest to create an EPT page table entry that is marked as both “not present” and also contains a host physical address from which it would like to read. If that host physical address is in the L1 data cache, it can read it.&lt;/p&gt;
&lt;p&gt;As a result, if a malicious guest can cause a Hypervisor (or another guest) to load a secret into the L1 data cache (e.g. just by using a data value during its normal operation), it can extract that data using an attack that is similar to Meltdown. When we first reproduced this attack within Red Hat earlier this year, we used a modified version of the TU Graz Meltdown code to extract data from known physical addresses in which we had stored interesting strings. While we should have seen an innocuous string we stashed in the guest physical memory, once the malicious L1TF PTE was created for the same location in the host, we read its memory instead. There are a few additional pieces required to reproduce the vulnerability that are omitted.&lt;/p&gt;
&lt;p&gt;L1TF is a significant threat to virtualized environments, especially those that contain a mixture of trusted and untrusted virtual machines. Fortunately, L1TF can be mitigated with a modest cost to system performance. Since a successful exploit requires that data be contained within the L1 data cache on a vulnerable machine, it is possible to arrange for the L1 to be flushed before returning to a guest virtual machine in cases where secrets or other data of interest to a malicious party might have been loaded. Performing this flush is not without cost, but a refill from the L2 to the L1 cache is fast (only a few hundreds of cycles) and uses a high bandwidth internal bus that exists in these processors. Thus, the overhead is on the order of a few percent. We have both a software (fallback) cache flush, as well as an (optimized) hardware-assisted flush that is available through microcode updates.&lt;/p&gt;
&lt;p&gt;The L1 data cache flush mitigation will be automatically enabled whenever virtualization is in use on impacted machines.&lt;/p&gt;
&lt;p&gt;There is one further complexity to the L1TF vulnerability concerning Intel Hyper-Threading.&lt;/p&gt;
&lt;h4&gt;Simultaneous Multi-Threading&lt;/h4&gt;
&lt;p&gt;Processors may implement an optimization known as Simultaneous Multi-Threading (SMT). SMT was invented by Susan Eggers, who earlier this year received the prestigious Eckert-Mauchly Award for her contributions to the field. Eggers realized in the 1990s that greater program thread-level parallelism could be realized by splitting a single physical processor core into several lighter-weight threads. Rather than duplicating all of the resources of a full core, SMT duplicates only the more essential resources needed to have two separate threads of execution running at the same time. The idea is that expensive (in terms of transistor count) resources like caches can be shared tightly between two threads of the same program because they are often operating upon the same data. Rather than destructively competing, these threads actually serve to pull useful data into their shared caches, for example in the case of a “producer-consumer” situation in which one thread generates data used by another thread.&lt;/p&gt;
&lt;p&gt;Intel was one of the early commercial adopters of SMT. Their implementation, known as “Hyper-Threading” has been effective, leading to average computer users referring to the number of cores and threads in their machines as a key characteristic. In Hyper-Threading, two peer threads (siblings) exist within a single core. Each is dynamically scheduled to use the available resources of the core in such a way that an overall gain in throughput of up to 30% can be realized for truly threaded applications that are operating on shared data. At the same time, effort is made to reduce the impact of unintentionally disruptive interference (for example in terms of cache footprint) between two threads that aren’t closely sharing resources.&lt;/p&gt;
&lt;p&gt;Indeed, so good is the general implementation of Intel Hyper-Threading that in many cases it can be hard for end users to distinguish between Hyper-Threading threads and additional physical cores. Under Linux, these threaded “logical processors” are reported (in /proc/cpuinfo) almost identically to full processor cores. The only real way to tell the difference is to look at the associated topology, as described in the “core id”, “cpu cores”, and “siblings” fields of the output, or in the more structured output of commands that parse this topology, such as “lscpu”. The Linux scheduler knows the difference, of course, and it will try not to schedule unrelated threads onto the same core. Nonetheless, there are many occasions (such as in HPC applications) where the potential for interference between unrelated threads outweighs the benefit. In these cases, some users have long disabled Hyper-Threading using BIOS settings in their computer firmware.&lt;/p&gt;
&lt;p&gt;The concept of not splitting Hyper-Threading threads across different workloads has long extended into the realm of virtualization as well. For a number of reasons, it has long made sense to assign only full cores (so-called “core scheduling”) to virtual machine instances. Two different virtual machines sharing a single core can otherwise interfere with one another’s performance as they split the underlying caches and other resources. Yet for all the potential problems that can arise, it has long been tempting to treat threads as cheap extra cores. Thus, it is common in today’s deployments to split VMs across Hyper-Threads of the same core, and technologies like OpenStack will often do this by default. This has never been a great idea, but the impact to overall security is far more significant in the presence of the L1TF vulnerability.&lt;/p&gt;
&lt;p&gt;Hyper-Threads run simultaneously (the “S” in “SMT”), and as a result, it is possible that one thread is running Hypervisor code or another virtual machine instance, while simultaneously the peer thread is executing a malicious guest. When entering the malicious guest on the peer thread, the L1 data cache will be flushed, but unfortunately, it is not possible to prevent it from subsequently observing the cache loads performed by its peer thread. Thus, if two different virtual machines are running on the same core at the same time, it is difficult to guarantee that they cannot perform an L1TF exploit against one another and steal secrets.&lt;/p&gt;
&lt;p&gt;The precise impact of L1TF to Hyper-Threading depends upon the specific use case and the virtualization environment being used. In some cases, it may be possible for public cloud vendors (who have often built special purpose hardware to assist in isolation) to take steps to render Hyper-Threading safe. In other cases, such as in a traditional enterprise environment featuring untrusted guest virtual machines, it may be necessary to disable Intel Hyper-Threading. Since this varies from one use case to another, and from one environment to another, Red Hat and our peers are not disabling Intel Hyper-Threading by default. Customers should instead consult our Knowledge Base article and make the appropriate determination for their own situation.&lt;/p&gt;
&lt;p&gt;To help facilitate control over Intel Hyper-Threading, Red Hat is shipping updated kernels that include a new interface through which customers can disable Hyper-Threading at boot time. Consult the updated kernel documentation and Knowledgebase for further information.&lt;/p&gt;
&lt;h4&gt;Wrapping up&lt;/h4&gt;
&lt;p&gt;The L1TF (L1 Terminal Fault) Intel processor vulnerability is complex and in some cases requires specific actions by customers to effect a complete mitigation. Red Hat and our partners have been working to prepare for the public coordinated disclosure, and to prepare patches, documentation, training, and other materials necessary to help keep our customers and their data safe. We recommend that you always follow best security practices, including deploying the updates for the L1TF vulnerability as quickly as possible.&lt;/p&gt;
&lt;p&gt;For further information, please consult the &lt;a href=&quot;https://access.redhat.com/security/vulnerabilities/L1TF&quot;&gt;Red Hat Knowledgebase article on L1TF&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Jon Masters is chief ARM architect at Red Hat.&lt;/em&gt;&lt;/p&gt;
</description>
<pubDate>Tue, 14 Aug 2018 17:10:27 +0000</pubDate>
<dc:creator>jterrill</dc:creator>
<og:description>L1 Terminal Fault/Foreshadow explained in ~three minutes For a more detailed technical view of L1 Terminal Fault, please see this deeper dive with Jon Masters.</og:description>
<og:url>https://www.redhat.com/en/blog/understanding-l1-terminal-fault-aka-foreshadow-what-you-need-know</og:url>
<og:image>https://www.redhat.com/profiles/rh/themes/redhatdotcom/img/logo-rh-og-image.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.redhat.com/en/blog/understanding-l1-terminal-fault-aka-foreshadow-what-you-need-know</dc:identifier>
</item>
<item>
<title>Serverless Docker Beta</title>
<link>https://zeit.co/blog/serverless-docker</link>
<guid isPermaLink="true" >https://zeit.co/blog/serverless-docker</guid>
<description>&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;6.3450292397661&quot;&gt;
&lt;div class=&quot;jsx-2412941416&quot; readability=&quot;8.1578947368421&quot;&gt;The focus of our &lt;a href=&quot;https://www.youtube.com/watch?v=yqACl3tRHNI&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; class=&quot;jsx-457212854&quot;&gt;ZEIT Day Keynote&lt;/a&gt; this year was on the new capabilities of the Now cloud platform. In particular, we emphasized our focus on &lt;span class=&quot;jsx-219205317&quot;&gt;Serverless Docker Deployments&lt;/span&gt;.&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;Today, we are announcing their availability as a public beta, which features:&lt;/p&gt;
&lt;/div&gt;
&lt;ul class=&quot;jsx-2171813335&quot;&gt;&lt;li class=&quot;jsx-852904543&quot; readability=&quot;-1.5&quot;&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;A &lt;span class=&quot;jsx-219205317&quot;&gt;10x-20x improvement in cold boot performance&lt;/span&gt;, based on data from 1.5M+ deployments&lt;/p&gt;
&lt;/div&gt;
&lt;ul class=&quot;jsx-2171813335&quot;&gt;&lt;li class=&quot;jsx-852904543&quot;&gt;This translates into a &lt;span class=&quot;jsx-219205317&quot;&gt;sub-second&lt;/span&gt; cold boot (full round trip) for most workloads&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot; readability=&quot;-1&quot;&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;A new &lt;span class=&quot;jsx-219205317&quot;&gt;&lt;code class=&quot;jsx-515620400&quot;&gt;slot&lt;/code&gt;&lt;/span&gt; configuration property which defines the resource allocation in terms of CPU and Memory, defaulting to &lt;code class=&quot;jsx-515620400&quot;&gt;c.125-m512&lt;/code&gt; (.125 of a vCPU and 512MB of memory)&lt;/p&gt;
&lt;/div&gt;
&lt;ul class=&quot;jsx-2171813335&quot;&gt;&lt;li class=&quot;jsx-852904543&quot;&gt;This enables &lt;span class=&quot;jsx-219205317&quot;&gt;fitting your application&lt;/span&gt; into the most appropriate set of constraints, paving the road to special CPU features, GPU cores, etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot; readability=&quot;-2&quot;&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Strictly specified tunable &lt;span class=&quot;jsx-219205317&quot;&gt;limits&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;ul class=&quot;jsx-2171813335&quot;&gt;&lt;li class=&quot;jsx-852904543&quot;&gt;&lt;span class=&quot;jsx-219205317&quot;&gt;Maximum execution time&lt;/span&gt; (defaulting to &lt;span class=&quot;jsx-219205317&quot;&gt;5 minutes&lt;/span&gt;, with a maximum of &lt;span class=&quot;jsx-219205317&quot;&gt;30 minutes&lt;/span&gt;)&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;A &lt;span class=&quot;jsx-219205317&quot;&gt;shutdown timeout&lt;/span&gt; after the last request (defaulting to &lt;span class=&quot;jsx-219205317&quot;&gt;1 minute&lt;/span&gt;, max &lt;span class=&quot;jsx-219205317&quot;&gt;5&lt;/span&gt;)&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;&lt;span class=&quot;jsx-219205317&quot;&gt;Maximum request concurrency&lt;/span&gt; before &lt;span class=&quot;jsx-219205317&quot;&gt;automatically scaling&lt;/span&gt; (defaulting to &lt;span class=&quot;jsx-219205317&quot;&gt;10&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot; readability=&quot;-1.7042253521127&quot;&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Support for &lt;span class=&quot;jsx-219205317&quot;&gt;HTTP/2.0 and WebSocket&lt;/span&gt; connections to the deployments&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;&lt;span class=&quot;jsx-219205317&quot;&gt;Automatic port discovery&lt;/span&gt;. We no longer rely on the &lt;code class=&quot;jsx-515620400&quot;&gt;EXPOSE&lt;/code&gt; instruction. We automatically forward traffic to the port of the process started by &lt;code class=&quot;jsx-515620400&quot;&gt;CMD&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;5.2380952380952&quot;&gt;
&lt;div class=&quot;jsx-2412941416&quot; readability=&quot;6.1111111111111&quot;&gt;Read on to learn how it works or head directly to our &lt;a href=&quot;https://github.com/zeit/now-examples&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; class=&quot;jsx-457212854&quot;&gt;examples&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;


&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Let's deploy a simple function exposed as an HTTP service using &lt;code class=&quot;jsx-515620400&quot;&gt;micro&lt;/code&gt;:&lt;/p&gt;
&lt;/div&gt;

&lt;p class=&quot;jsx-81588296&quot;&gt;A simple function accessible via &lt;a href=&quot;https://node-function.now.sh&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; class=&quot;jsx-457212854&quot;&gt;node-function.now.sh&lt;/a&gt;, built with &lt;code class=&quot;jsx-2550463271&quot;&gt;npm ci&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot;&gt;
&lt;p&gt;Here is what happened:&lt;/p&gt;
&lt;/div&gt;
&lt;ul class=&quot;jsx-2171813335&quot;&gt;&lt;li class=&quot;jsx-852904543&quot;&gt;A deployment is created, diffing our local file system with the cloud&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;A simple &lt;code class=&quot;jsx-515620400&quot;&gt;Dockerfile&lt;/code&gt; is used to hold the &lt;span class=&quot;jsx-219205317&quot;&gt;instructions to build the project&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;We build with &lt;span class=&quot;jsx-219205317&quot;&gt;our own preference&lt;/span&gt; of Node.js version (&lt;code class=&quot;jsx-515620400&quot;&gt;10&lt;/code&gt;) and package manager (&lt;code class=&quot;jsx-515620400&quot;&gt;npm ci&lt;/code&gt;)&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;The &lt;code class=&quot;jsx-515620400&quot;&gt;index.js&lt;/code&gt; file contains &lt;span class=&quot;jsx-219205317&quot;&gt;our main function&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;The serverless container is limited to &lt;span class=&quot;jsx-219205317&quot;&gt;just .125 CPUs and 512MB of memory&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;DNS lookup + TLS handshake + Cold Boot + Full roundtrip happens in &lt;span class=&quot;jsx-219205317&quot;&gt;600ms~&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;Once the deployment instance is warm, subsequent requests take &lt;span class=&quot;jsx-219205317&quot;&gt;100ms~&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;The only requirement to make this work is for your &lt;code class=&quot;jsx-515620400&quot;&gt;now.json&lt;/code&gt; to activate the beta via a feature flag:&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&quot;jsx-951606128&quot;&gt;
&lt;code class=&quot;jsx-951606128&quot;&gt;{
  &quot;type&quot;: &quot;docker&quot;,
  &quot;features&quot;: {
    &quot;cloud&quot;: &quot;v2&quot;
  }
}
&lt;/code&gt;
&lt;/pre&gt;

&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;7.4013157894737&quot;&gt;
&lt;div class=&quot;jsx-2412941416&quot; readability=&quot;9.8684210526316&quot;&gt;Let's go a bit deeper into the power of this technology. The next example will take an image &lt;em&gt;right from the Docker registry&lt;/em&gt;, of a program written in &lt;a href=&quot;https://golang.org&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; class=&quot;jsx-457212854&quot;&gt;Go&lt;/a&gt;.&lt;/div&gt;
&lt;/div&gt;

&lt;p class=&quot;jsx-2261775688&quot;&gt;A serverless shell, powered by HTTP/2.0 and WebSockets, available at &lt;a href=&quot;https://terminal.now.sh&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; class=&quot;jsx-457212854&quot;&gt;terminal.now.sh&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot;&gt;
&lt;p&gt;This demo highlights:&lt;/p&gt;
&lt;/div&gt;
&lt;ul class=&quot;jsx-2171813335&quot;&gt;&lt;li class=&quot;jsx-852904543&quot;&gt;The usage of an unmodified &lt;code class=&quot;jsx-515620400&quot;&gt;Dockerfile&lt;/code&gt; from the &lt;span class=&quot;jsx-219205317&quot;&gt;public Docker registry&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;A different programming language and runtime: &lt;span class=&quot;jsx-219205317&quot;&gt;Go&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;&lt;span class=&quot;jsx-219205317&quot;&gt;Transient statefulness&lt;/span&gt;, as evidenced by our ability to inspect the filesystem - After 5 minutes (the default duration), the state will recycle&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;&lt;span class=&quot;jsx-219205317&quot;&gt;Sub-500ms cold roundtrip&lt;/span&gt;. Go exhibits better startup performance, even though this is a larger application (usually 400ms-500ms for this example)&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;The service responds to HTTP requests to serve the initial HTML and then a &lt;span class=&quot;jsx-219205317&quot;&gt;WebSocket connection&lt;/span&gt; to exchange the PTY data&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;7.0283018867925&quot;&gt;
&lt;div class=&quot;jsx-2412941416&quot; readability=&quot;9.3710691823899&quot;&gt;This infrastructure works remarkably well in combination with &lt;a class=&quot;jsx-14359869&quot; href=&quot;https://zeit.co/blog/global-now&quot;&gt;Global Now&lt;/a&gt;. In other words, it takes &lt;span class=&quot;jsx-219205317&quot;&gt;one flag to deploy serverlessly to all our global locations&lt;/span&gt;.&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;5.3333333333333&quot;&gt;
&lt;div class=&quot;jsx-2412941416&quot; readability=&quot;6.2222222222222&quot;&gt;Here is an example of deploying Rust + &lt;a href=&quot;https://hyper.rs&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; class=&quot;jsx-457212854&quot;&gt;Hyper&lt;/a&gt;:&lt;/div&gt;
&lt;/div&gt;

&lt;p class=&quot;jsx-2599953480&quot;&gt;A Rust microservice instantly available in every region at &lt;a href=&quot;https://rust-http-microservice-v2.now.sh&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; class=&quot;jsx-457212854&quot;&gt;rust-http-microservice-v2.now.sh&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;This is equivalent to the rest of the examples, but we scaled right from the get-go to &lt;em&gt;all regions&lt;/em&gt; by running &lt;code class=&quot;jsx-515620400&quot;&gt;now --regions all&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;It's also possible to scale &lt;em&gt;after you have already deployed&lt;/em&gt;, by running:&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&quot;jsx-951606128&quot;&gt;
&lt;code class=&quot;jsx-951606128&quot;&gt;// scale to sfo
now scale rust-http-microservice-v2.now.sh sfo

// scale to all regions
now scale rust-http-microservice-v2.now.sh all

// disable everywhere
now scale rust-http-microservice-v2.now.sh 0
&lt;/code&gt;
&lt;/pre&gt;

&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;12&quot;&gt;
&lt;p&gt;To underline the ability of this system to automatically scale with the parameters you define, within the boundaries that you define, here is an example that stress tests with &lt;code class=&quot;jsx-515620400&quot;&gt;wrk&lt;/code&gt;, a load-testing tool:&lt;/p&gt;
&lt;/div&gt;

&lt;p class=&quot;jsx-2061338184&quot;&gt;Instant and predictable horizontal scalability&lt;/p&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;12&quot;&gt;
&lt;p&gt;This is, in our opinion, the most important defining characteristic of &lt;span class=&quot;jsx-219205317&quot;&gt;Serverless Deployments&lt;/span&gt;. However, it's not the only one, as we will see next.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;10&quot;&gt;
&lt;p&gt;We selected these demos in particular to underline a very important point. We think Serverless can be a very general computing model. &lt;span class=&quot;jsx-219205317&quot;&gt;One that does not require new protocols, new APIs and can support every programming language and framework without large rewrites&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Here are three of the underlying ideas behind this new architecture.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;14&quot;&gt;
&lt;p&gt;Serverless enables engineers to focus on code rather than managing servers, VMs, registries, clusters, load balancers, availability zones, and so on.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;8.9338235294118&quot;&gt;
&lt;div class=&quot;jsx-2412941416&quot; readability=&quot;13.400735294118&quot;&gt;This, in turn, allows you to define your engineering workflow solely around &lt;span class=&quot;jsx-219205317&quot;&gt;source control&lt;/span&gt; and its associated tools (like pull requests). &lt;a class=&quot;jsx-14359869&quot; href=&quot;https://zeit.co/github&quot;&gt;Our recent GitHub integration&lt;/a&gt;, therefore, makes it possible to deploy a Docker container in the cloud solely by creating a &lt;code class=&quot;jsx-515620400&quot;&gt;Dockerfile&lt;/code&gt;.&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;14&quot;&gt;
&lt;p&gt;It is not sufficient to &lt;em&gt;ignore that the infrastructure is there&lt;/em&gt;, or forget about it. The &lt;span class=&quot;jsx-219205317&quot;&gt;execution model&lt;/span&gt; must make it so that manual intervention, inspection, replication, and monitoring or alert-based uptime assurance is completely unnecessary, which takes us to our next two points.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;When we deployed the examples above, we didn't have to deal with:&lt;/p&gt;
&lt;/div&gt;
&lt;ul class=&quot;jsx-2171813335&quot;&gt;&lt;li class=&quot;jsx-852904543&quot;&gt;Clusters or federations of clusters&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;Build nodes or build farms&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;Container registries and authentication&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;Container image storage, garbage collection and distributed caching&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;10&quot;&gt;
&lt;p&gt;A very common category of failure of software applications is associated with failures that occur after programs get into states that the developers didn't anticipate, usually arising after many &lt;em&gt;cycles&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;10&quot;&gt;
&lt;p&gt;In other words, programs can fail unexpectedly from accumulating state over a long lifespan of operation. Perhaps the most common example of this is a memory leak: the unanticipated growth of irreclaimable memory that ultimately concludes in a faulty application.&lt;/p&gt;
&lt;/div&gt;

&lt;p class=&quot;jsx-3718170408&quot;&gt;Serverless means never having to &quot;try turning it off and back on again&quot;&lt;/p&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;12&quot;&gt;
&lt;p&gt;Serverless models completely remove this category of issues, ensuring that no request goes unserviced during the recycling, upgrading or scaling of an application, even when it encounters runtime errors.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;11&quot;&gt;
&lt;p&gt;Your deployment instances are constantly &lt;span class=&quot;jsx-219205317&quot;&gt;recycling and rotating&lt;/span&gt;. Because of the request-driven nature of scheduling execution, combined with &lt;span class=&quot;jsx-219205317&quot;&gt;limits&lt;/span&gt; such as maximum execution length, you avoid many common operational errors completely.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;Perhaps the most important or appealing aspect of the serverless paradigm is the promise of &lt;em&gt;automatic scalability&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;10&quot;&gt;
&lt;p&gt;In its most basic form, a function automatically scales with a 1:1 mapping of requests to resource allocations. A request comes in, a new function is provisioned or an existing one is re-used.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;We have taken this a step further, by allowing you to customize the concurrency your process can handle.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;This new infrastructure is already available to Docker deployments made in the free tier, or for paying subscriptions that opt-into the feature via &lt;code class=&quot;jsx-515620400&quot;&gt;now.json&lt;/code&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&quot;jsx-951606128&quot;&gt;
&lt;code class=&quot;jsx-951606128&quot;&gt;{
  &quot;type&quot;: &quot;docker&quot;,
  &quot;features&quot;: {
    &quot;cloud&quot;: &quot;v2&quot;
  }
}
&lt;/code&gt;
&lt;/pre&gt;



&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;These limits are fixed. They are subject to change once the feature goes into General Availability.&lt;/p&gt;
&lt;/div&gt;
&lt;ul class=&quot;jsx-2171813335&quot;&gt;&lt;li class=&quot;jsx-852904543&quot;&gt;A maximum of &lt;code class=&quot;jsx-515620400&quot;&gt;3&lt;/code&gt; concurrent deployment instances for OSS&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;A maximum of &lt;code class=&quot;jsx-515620400&quot;&gt;10&lt;/code&gt; concurrent deployment instances per subscription&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;A maximum of &lt;code class=&quot;jsx-515620400&quot;&gt;500&lt;/code&gt; concurrent requests/connections across deployments per subscription&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;These limits are configurable in &lt;code class=&quot;jsx-515620400&quot;&gt;now.json&lt;/code&gt; as part of a &lt;code class=&quot;jsx-515620400&quot;&gt;limits&lt;/code&gt; object.&lt;/p&gt;
&lt;/div&gt;
&lt;ul class=&quot;jsx-2171813335&quot;&gt;&lt;li class=&quot;jsx-852904543&quot;&gt;&lt;code class=&quot;jsx-515620400&quot;&gt;maxConcurrentReqs&lt;/code&gt; max concurrency of each process (min &lt;code class=&quot;jsx-515620400&quot;&gt;1&lt;/code&gt;, max &lt;code class=&quot;jsx-515620400&quot;&gt;256&lt;/code&gt;, default &lt;code class=&quot;jsx-515620400&quot;&gt;10&lt;/code&gt;)&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;&lt;code class=&quot;jsx-515620400&quot;&gt;duration&lt;/code&gt; max amount of time in &lt;span class=&quot;jsx-219205317&quot;&gt;ms&lt;/span&gt; your process can run (min/default 5 minutes, max 30 minutes)&lt;/li&gt;
&lt;li class=&quot;jsx-852904543&quot;&gt;&lt;code class=&quot;jsx-515620400&quot;&gt;timeout&lt;/code&gt; how long in &lt;em&gt;ms&lt;/em&gt; to wait after the last request to downscale (min/default 1 minute, max 30 minutes)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;While in beta, we require a paid subscription to be able to go over the maximum of &lt;code class=&quot;jsx-515620400&quot;&gt;3&lt;/code&gt; concurrent deployment instances. Current rates apply and are subject to change.&lt;/p&gt;
&lt;/div&gt;


&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;Despite having so dramatically sped up instantiation times, we still have very significant room for improvement.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;We are excited about unveiling some of these over the coming weeks before the new infrastructure goes into General Availability.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;We will introduce new &lt;code class=&quot;jsx-515620400&quot;&gt;slot&lt;/code&gt; identifiers so that you can fit your applications into other CPU/memory combinations.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;This is important for resource-intensive applications.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;When your code is built, we post-process the resulting snapshot and let you know what the total size is.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;We are confident that in its present form, our system can fit the vast majority of our customers' workloads without any issues.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;10&quot;&gt;
&lt;p&gt;However, we are currently developing improvements to optimize this dimension further, without you having to make any changes.&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;10&quot;&gt;
&lt;p&gt;This beta contains the lessons and the experiences of a massively distributed and diverse user base, that has completed millions of deployments, over the past two years.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;6.4473684210526&quot;&gt;
&lt;div class=&quot;jsx-2412941416&quot; readability=&quot;8.5964912280702&quot;&gt;To get started, we suggest you take a look at the comprehensive &lt;a href=&quot;https://github.com/zeit/now-examples&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot; class=&quot;jsx-457212854&quot;&gt;list of examples&lt;/a&gt; we put together for this release.&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;Over the coming weeks, we will share more in-depth articles and documentation about our new offering.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;jsx-2819943309 wrapper&quot; readability=&quot;6.1444444444444&quot;&gt;
&lt;div class=&quot;jsx-2412941416&quot; readability=&quot;7.9&quot;&gt;Your feedback is crucial during this period. Please &lt;a class=&quot;jsx-14359869&quot; href=&quot;https://zeit.co/chat&quot;&gt;let us know&lt;/a&gt; how well it works for you.&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Tue, 14 Aug 2018 16:47:27 +0000</pubDate>
<dc:creator>Rauchg</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://zeit.co/blog/serverless-docker</dc:identifier>
</item>
<item>
<title>What Happens to #MeToo When a Feminist Is the Accused?</title>
<link>https://www.nytimes.com/2018/08/13/nyregion/sexual-harassment-nyu-female-professor.html</link>
<guid isPermaLink="true" >https://www.nytimes.com/2018/08/13/nyregion/sexual-harassment-nyu-female-professor.html</guid>
<description>&lt;p class=&quot;css-1i0edl6 e2kc3sl0&quot;&gt;Professor Ronell, 66, denied any harassment. “Our communications — which Reitman now claims constituted sexual harassment — were between two adults, a gay man and a queer woman, who share an Israeli heritage, as well as a penchant for florid and campy communications arising from our common academic backgrounds and sensibilities,” she wrote in a statement to The New York Times. “These communications were repeatedly invited, responded to and encouraged by him over a period of three years.”&lt;/p&gt;
&lt;p class=&quot;css-1i0edl6 e2kc3sl0&quot;&gt;Two years after graduating from N.Y.U. with a Ph.D., Mr. Reitman filed a Title IX complaint against his former adviser, alleging sexual harassment, sexual assault, stalking and retaliation. In May, the university found Professor Ronell responsible for sexual harassment and cleared her of the other allegations.&lt;/p&gt;
&lt;p class=&quot;css-1i0edl6 e2kc3sl0&quot;&gt;Mr. Reitman’s lawyer, Donald Kravet, said he and his client have drafted a lawsuit against N.Y.U. and Professor Ronell and are now considering their options.&lt;/p&gt;
&lt;p class=&quot;css-1i0edl6 e2kc3sl0&quot;&gt;Both Mr. Reitman and Professor Ronell’s descriptions of their experiences echo other &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.nytimes.com/interactive/2018/06/28/arts/metoo-movement-stories.html&quot; title=&quot;&quot;&gt;#MeToo stories&lt;/a&gt;: In Mr. Reitman’s recollection, he was afraid of his professor and the power she wielded over him, and often went along with behavior that left him feeling violated. Professor Ronell said that Mr. Reitman desperately sought her attention and guidance in interviews she submitted to the Title IX office at N.Y.U., which The New York Times obtained.&lt;/p&gt;
&lt;p class=&quot;css-1i0edl6 e2kc3sl0&quot;&gt;The problems began, according to Mr. Reitman, in the spring of 2012, before he officially started school. Professor Ronell invited him to stay with her in Paris for a few days. The day he arrived, she asked him to read poetry to her in her bedroom while she took an afternoon nap, he said.&lt;/p&gt;
&lt;p class=&quot;css-1i0edl6 e2kc3sl0&quot;&gt;“That was already a red flag to me,” said Mr. Reitman. “But I also thought, O.K., you’re here. Better not make a scene.”&lt;/p&gt;
&lt;p class=&quot;css-1i0edl6 e2kc3sl0&quot;&gt;Then, he said, she pulled him into her bed.&lt;/p&gt;
&lt;p class=&quot;css-1i0edl6 e2kc3sl0&quot;&gt;“She put my hands onto her breasts, and was pressing herself — her buttocks — onto my crotch,” he said. “She was kissing me, kissing my hands, kissing my torso.” That evening, a similar scene played out again, he said.&lt;/p&gt;
</description>
<pubDate>Tue, 14 Aug 2018 14:15:44 +0000</pubDate>
<dc:creator>Tomte</dc:creator>
<og:url>https://www.nytimes.com/2018/08/13/nyregion/sexual-harassment-nyu-female-professor.html</og:url>
<og:type>article</og:type>
<og:title>What Happens to #MeToo When a Feminist Is the Accused?</og:title>
<og:image>https://static01.nyt.com/images/2018/08/14/nyregion/14nyu/00nyu-facebookJumbo.jpg</og:image>
<og:description>Avital Ronell, a superstar professor, was found by N.Y.U. to have sexually harassed a male grad student. But his charges have met disbelief from some feminist scholars.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nytimes.com/2018/08/13/nyregion/sexual-harassment-nyu-female-professor.html</dc:identifier>
</item>
<item>
<title>Google tracks users who turn off location history</title>
<link>https://www.bbc.co.uk/news/technology-45183041</link>
<guid isPermaLink="true" >https://www.bbc.co.uk/news/technology-45183041</guid>
<description>&lt;figure class=&quot;media-landscape has-caption full-width lead&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                &lt;img class=&quot;js-image-replace&quot; alt=&quot;Crowd of people&quot; src=&quot;https://ichef.bbci.co.uk/news/320/cpsprodpb/6A82/production/_102966272_crowdscene2.gif&quot; width=&quot;976&quot; height=&quot;549&quot;/&gt;&lt;span class=&quot;off-screen&quot;&gt;Image copyright&lt;/span&gt;
                 &lt;span class=&quot;story-image-copyright&quot;&gt;Getty Images&lt;/span&gt;
                
            &lt;/span&gt;
            
            &lt;figcaption class=&quot;media-caption&quot;&gt;&lt;span class=&quot;off-screen&quot;&gt;Image caption&lt;/span&gt;
                &lt;span class=&quot;media-caption__text&quot;&gt;
                    The study found that users had to turn off another setting in order to disable location being recorded
                &lt;/span&gt;
            &lt;/figcaption&gt;&lt;/figure&gt;&lt;p class=&quot;story-body__introduction&quot;&gt;Google records users' locations even when they have asked it not to, a report from the Associated Press has suggested.&lt;/p&gt;&lt;p&gt;The issue could affect up to two billion Android and Apple devices which use Google for maps or search.&lt;/p&gt;&lt;p&gt;The study, verified by researchers at Princeton University, has angered US law-makers.&lt;/p&gt;&lt;p&gt;Google said in response that it provides clear descriptions of its tools and how to turn them off.&lt;/p&gt;&lt;p&gt;The study found that users' whereabouts are recorded even when location history has been disabled. &lt;/p&gt;&lt;p&gt;For example:&lt;/p&gt;&lt;ul class=&quot;story-body__unordered-list&quot;&gt;&lt;li class=&quot;story-body__list-item&quot;&gt;Google stores a snapshot of where you are when you open the Maps app&lt;/li&gt;
&lt;li class=&quot;story-body__list-item&quot;&gt; Automatic weather updates on Android phones pinpoint roughly where a user is&lt;/li&gt;
&lt;li class=&quot;story-body__list-item&quot;&gt;Searches that have nothing to do with location pinpoint precise longitude and latitude of users&lt;/li&gt;
&lt;/ul&gt;&lt;h2 class=&quot;story-body__crosshead&quot;&gt;'Pretty sneaky'&lt;/h2&gt;&lt;p&gt;To illustrate the effect of these location markers, AP created a visual map showing the movements of Princeton researcher Gunes Acar who was using an Android phone with location history turned off.&lt;/p&gt;&lt;p&gt;The map showed his train commute around New York as well as visits to The High Line park, Chelsea Market, Hell's Kitchen, Central Park and Harlem. It also revealed his home address.&lt;/p&gt;&lt;p&gt;To stop Google saving these location markers, users have to turn off another setting called Web and App Activity, which is enabled by default and which does not mention location data. &lt;/p&gt;&lt;p&gt;Disabling this prevents Google storing information generated by searches and other activities which can limit the effectiveness of its digital assistant.&lt;/p&gt;&lt;p&gt;&quot;You would think that telling Google that you didn't want your location to be tracked by disabling an option called &quot;Location History&quot; would stop the internet giant from storing data about your location,&quot;  &lt;a href=&quot;https://www.grahamcluley.com/pausing-google-location-history-doesnt-stop-your-location-data-from-being-collected-and-stored/&quot; class=&quot;story-body__link-external&quot;&gt;writes security researcher Graham Cluley on his blog.&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&quot;It seems pretty sneaky to me that Google continues to store location data, unless you both disable &quot;Location history&quot; and &quot;Web &amp;amp; App Activity.&quot;&quot;&lt;/p&gt;&lt;p&gt;In response, Google told AP: &quot;There are a number of different ways that Google may use location to improve people's experience, including: Location History, Web and App Activity, and through device-level Location Services.&lt;/p&gt;&lt;p&gt;&quot;We provide clear descriptions of these tools, and robust controls so people can turn them on or off, and delete their histories at any time.&quot;&lt;/p&gt;&lt;h2 class=&quot;story-body__crosshead&quot;&gt;Corporate practices&lt;/h2&gt;&lt;p&gt;Following its research, &lt;a href=&quot;https://www.apnews.com/b031ee35d4534f548e43b7575f4ab494/How-to-find-and-delete-where-Google-knows-you've-been&quot; class=&quot;story-body__link-external&quot;&gt;AP created a guide&lt;/a&gt; to show users how to delete location data.&lt;/p&gt;&lt;p&gt;Presented with the evidence of the AP study, Democratic senator Mark Warner accused technology companies of having &quot;corporate practices that diverge wildly from the totally reasonable expectation of their users&quot;.&lt;/p&gt;&lt;p&gt;Democratic congressman Frank Pallone called for &quot;comprehensive consumer privacy and data security legislation&quot;.&lt;/p&gt;&lt;p&gt;In the UK, a spokesman for the Information Commissioner's Office told the BBC: &quot;Under the GDPR and the Data Protection Act 2018, organisations have a legal duty to be open, transparent and fair with the public about how their personal data is used.&lt;/p&gt;&lt;p&gt;&quot;Anybody who has concerns about how an organisation is handling their personal information can contact the ICO.&quot;&lt;/p&gt;&lt;p&gt;Technology firms are under fire for not being clear about privacy settings and how to use them. In June, a report from the Norwegian Consumer Council found evidence that privacy-friendly options are hidden away or obscured.&lt;/p&gt;&lt;p&gt;Location-based advertising offers big opportunities to marketers. According to research firm BIA/Kelsey, US brands are poised to spend up to $20.6bn (£16.3bn) on targeted mobile ads in 2018.&lt;/p&gt;&lt;p&gt;Since 2014, Google has let advertisers track the effectiveness of online adverts with a feature based on footfall data, which relies on location history.&lt;/p&gt;
            </description>
<pubDate>Tue, 14 Aug 2018 14:00:02 +0000</pubDate>
<dc:creator>oinkgrr</dc:creator>
<og:title>Google tracks users who turn off location</og:title>
<og:type>article</og:type>
<og:description>A study from Associated Press suggests that users are still tracked even if they turn off location history.</og:description>
<og:url>https://www.bbc.com/news/technology-45183041</og:url>
<og:image>https://ichef.bbci.co.uk/news/1024/branded_news/6A82/production/_102966272_crowdscene2.gif</og:image>
<dc:language>en-GB</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bbc.co.uk/news/technology-45183041</dc:identifier>
</item>
<item>
<title>Why Use an FPGA Instead of a CPU or GPU?</title>
<link>https://blog.esciencecenter.nl/why-use-an-fpga-instead-of-a-cpu-or-gpu-b234cd4f309c</link>
<guid isPermaLink="true" >https://blog.esciencecenter.nl/why-use-an-fpga-instead-of-a-cpu-or-gpu-b234cd4f309c</guid>
<description>&lt;h2 name=&quot;2a51&quot; id=&quot;2a51&quot; class=&quot;graf graf--h4 graf-after--h3 graf--subtitle&quot;&gt;The (dis)advantages of Field Programmable Gate Arrays&lt;/h2&gt;
&lt;p name=&quot;b5a4&quot; id=&quot;b5a4&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Recently, &lt;a href=&quot;https://newsroom.intel.com/news-releases/intel-completes-acquisition-of-altera/&quot; data-href=&quot;https://newsroom.intel.com/news-releases/intel-completes-acquisition-of-altera/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Intel bought Altera&lt;/a&gt;, one of the largest producers of FPGAs. Intel paid a whopping $16.7 billion, making it their largest acquisition ever. In other news, &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2014/06/HC26.12.520-Recon-Fabric-Pulnam-Microsoft-Catapult.pdf&quot; data-href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2014/06/HC26.12.520-Recon-Fabric-Pulnam-Microsoft-Catapult.pdf&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Microsoft is using FPGAs in its data centers&lt;/a&gt;, and &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/f1/&quot; data-href=&quot;https://aws.amazon.com/ec2/instance-types/f1/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Amazon is offering&lt;/a&gt; them on their cloud services. Previously, these FPGAs were mainly used in &lt;em class=&quot;markup--em markup--p-em&quot;&gt;electronics&lt;/em&gt; engineering, but not so much in &lt;em class=&quot;markup--em markup--p-em&quot;&gt;software&lt;/em&gt; engineering. Are FPGAs about to take off and become serious alternatives to CPUs and GPUs?&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*c54zfRSRF7i6MOSy04Pa6Q.jpeg&quot; data-width=&quot;640&quot; data-height=&quot;251&quot; data-is-featured=&quot;true&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*c54zfRSRF7i6MOSy04Pa6Q.jpeg&quot;/&gt;&lt;/div&gt;
&lt;h3 name=&quot;fd3a&quot; id=&quot;fd3a&quot; class=&quot;graf graf--h3 graf-after--figure&quot;&gt;What is an FPGA?&lt;/h3&gt;
&lt;p name=&quot;6e5b&quot; id=&quot;6e5b&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;If you want to compute something, the common approach is to write some software for an instruction based architecture, such as a CPU or GPU. Another, more arduous, route one could take is to design a special circuit for this specific computation — as opposed to writing instructions for a general purpose circuit such as a CPU or GPU.&lt;/p&gt;
&lt;p name=&quot;92f8&quot; id=&quot;92f8&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Once you have designed this circuit, you need some way to implement the design so that you can actually compute something. One way, which requires quite deep pockets, is to actually produce a circuit that implements this design (this is called an Application Specific Integrated Circuit or ASIC).&lt;/p&gt;
&lt;p name=&quot;9ac2&quot; id=&quot;9ac2&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;An easier way, and the main topic of this blog, is to implement your circuit design is to use a &lt;em class=&quot;markup--em markup--p-em&quot;&gt;Field Programmable Gate Array&lt;/em&gt; (FPGA), a &lt;em class=&quot;markup--em markup--p-em&quot;&gt;reconfigurable integrated circuit&lt;/em&gt;. You can configure the FPGA to become any circuit you want to (as long as it fits on the FPGA). This is quite a bit different than the &lt;em class=&quot;markup--em markup--p-em&quot;&gt;instruction-based hardware&lt;/em&gt; most programmers are used to, such as CPUs and GPUs. Instruction-based hardware is configured via &lt;em class=&quot;markup--em markup--p-em&quot;&gt;software&lt;/em&gt;, whereas FPGAs are instead configured by specifying a &lt;em class=&quot;markup--em markup--p-em&quot;&gt;hardware&lt;/em&gt; circuit.&lt;/p&gt;
&lt;h3 name=&quot;bb62&quot; id=&quot;bb62&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;(Dis)Advantages of FPGAs&lt;/h3&gt;
&lt;p name=&quot;6657&quot; id=&quot;6657&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Why would you prefer to use an FPGA for your computation over the more common CPU or GPU? The differences with GPUs and CPUs are in the following areas:&lt;/p&gt;
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;dfea&quot; id=&quot;dfea&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Latency&lt;/strong&gt;: How long does it take to compute something?&lt;br/&gt;→ FPGAs are good at this.&lt;/li&gt;
&lt;li name=&quot;e250&quot; id=&quot;e250&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Connectivity&lt;/strong&gt;: What input/output can we connect and with which bandwidth?&lt;br/&gt;→ FPGAs can directly be connected to inputs and can offer very high bandwith.&lt;/li&gt;
&lt;li name=&quot;29a5&quot; id=&quot;29a5&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Engineering cost&lt;/strong&gt;: How much effort does it cost to express the computation?&lt;br/&gt;→ The engineering cost is typically much higher than for instruction based architectures, so the advantages must really be worth it.&lt;/li&gt;
&lt;li name=&quot;bc24&quot; id=&quot;bc24&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Energy efficiency&lt;/strong&gt;: How much energy does it cost to compute something?&lt;br/&gt;→ This is often listed as a large benefit of FPGAs, but whether FPGAs are better than CPUs or GPUs really depends on the application.&lt;/li&gt;
&lt;/ul&gt;&lt;p name=&quot;3a8e&quot; id=&quot;3a8e&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;Let’s discuss each of these in more detail.&lt;/p&gt;
&lt;h3 name=&quot;4054&quot; id=&quot;4054&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Low latency&lt;/h3&gt;
&lt;p name=&quot;5da7&quot; id=&quot;5da7&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Low latency is what you need if you are programming the autopilot of a jet fighter or a high-frequency algorithmic trading engine: the time between an input and its response as short as possible. This is where FPGAs are much better than CPUs (or GPUs, which have to communicate via the CPU).&lt;/p&gt;
&lt;p name=&quot;1ff6&quot; id=&quot;1ff6&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;With an FPGA it is feasible to get a latency around or below 1 microsecond, whereas with a CPU a latency smaller than 50 microseconds is already very good. Moreover, the latency of an FPGA is much more deterministic. One of the main reasons for this low latency is that FPGAs can be much more specialized: they do not depend on the generic operating system, and communication does not have to go via generic buses (such as USB or PCIe).&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*rIAJPMywpoe5q_RDLpV1uw.jpeg&quot; data-width=&quot;2000&quot; data-height=&quot;1000&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*rIAJPMywpoe5q_RDLpV1uw.jpeg&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*rIAJPMywpoe5q_RDLpV1uw.jpeg&quot;/&gt;&lt;/div&gt;
A couple of FPGAs in mid-air (probably)
&lt;h3 name=&quot;d0c3&quot; id=&quot;d0c3&quot; class=&quot;graf graf--h3 graf-after--figure&quot;&gt;Connectivity&lt;/h3&gt;
&lt;p name=&quot;6395&quot; id=&quot;6395&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;On an FPGA, you can hook up any data source, such as a network interface or sensor, directly to the &lt;em class=&quot;markup--em markup--p-em&quot;&gt;pins of the chip&lt;/em&gt;. This in sharp contrast to GPUs and CPUs, where you have to connect your source via the standardized buses (such as USB or PCIe) — and depend on the operating system to deliver the data to your application. A direct connection to the pins of the chip gives very high bandwidth (as well as low latency).&lt;/p&gt;
&lt;p name=&quot;f14d&quot; id=&quot;f14d&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;This high bandwidth is needed, for example, in radio-astronomy applications, such as &lt;a href=&quot;http://www.lofar.org/about-lofar/about-lofar&quot; data-href=&quot;http://www.lofar.org/about-lofar/about-lofar&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;LOFAR&lt;/a&gt; and &lt;a href=&quot;https://www.skatelescope.org/&quot; data-href=&quot;https://www.skatelescope.org/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;SKA&lt;/a&gt;. In these applications there are a lot of specialized sensors in the field, which generate an enormous amount of data. The volume of data needs to be reduced before being sent off, to make it more manageable. For this purpose, the Netherlands Institute for Radio Astronomy, &lt;a href=&quot;https://www.astron.nl/&quot; data-href=&quot;https://www.astron.nl/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;ASTRON&lt;/a&gt;, designed the &lt;a href=&quot;https://www.astron.nl/r-d-laboratory/uniboard/uniboard-i-and-ii&quot; data-href=&quot;https://www.astron.nl/r-d-laboratory/uniboard/uniboard-i-and-ii&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;Uniboard²&lt;/em&gt;&lt;/a&gt;, a board with four FPGAs which can handle more data per second than the Amsterdam internet exchange.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*Bomy9uvjcoI5F8rALbwUFg.png&quot; data-width=&quot;800&quot; data-height=&quot;447&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*Bomy9uvjcoI5F8rALbwUFg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*Bomy9uvjcoI5F8rALbwUFg.png&quot;/&gt;&lt;/div&gt;
These small radio astronomy antennas generate &lt;strong class=&quot;markup--strong markup--figure-strong&quot;&gt;a lot&lt;/strong&gt; of data (image credit: &lt;a href=&quot;https://en.wikipedia.org/wiki/User:Svenlafe&quot; data-href=&quot;https://en.wikipedia.org/wiki/User:Svenlafe&quot; class=&quot;markup--anchor markup--figure-anchor&quot; title=&quot;en:User:Svenlafe&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Svenlafe&lt;/a&gt; at &lt;a href=&quot;http://en.wikipedia.org/&quot; data-href=&quot;http://en.wikipedia.org/&quot; class=&quot;markup--anchor markup--figure-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;en.wikipedia&lt;/a&gt;)
&lt;h3 name=&quot;1b74&quot; id=&quot;1b74&quot; class=&quot;graf graf--h3 graf-after--figure&quot;&gt;Engineering cost&lt;/h3&gt;
&lt;p name=&quot;c6ce&quot; id=&quot;c6ce&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Before turning to the subtle issue of energy efficiency, let’s discuss the main disadvantage of FPGAs: they are really &lt;em class=&quot;markup--em markup--p-em&quot;&gt;much&lt;/em&gt; harder to program/configure than instruction based architectures (i.e. CPUs and GPUs). Traditionally, these hardware circuits are described via &lt;em class=&quot;markup--em markup--p-em&quot;&gt;Hardware Description Languages&lt;/em&gt; (HDL), such as VHDL and Verilog, whereas software is programmed via one of a plethora of programming languages, such as Java, C and Python.&lt;/p&gt;
&lt;p name=&quot;eb52&quot; id=&quot;eb52&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;From a theoretical perspective, both hardware description languages and programming languages can be use to express any computation (both are Turing complete), but the difference in engineering details is vast.&lt;/p&gt;
&lt;p name=&quot;670e&quot; id=&quot;670e&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;An upcoming trend is High Level Synthesis (HLS): programming FPGAs using regular programming languages such as OpenCL or C++, allowing for a much higher level of abstraction. However, even when using such languages, programming FPGAs is still an order of magnitude more difficult than programming instruction based systems.&lt;/p&gt;
&lt;p name=&quot;1c75&quot; id=&quot;1c75&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;A large part of the difficulty of programming FPGAs are the &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;long&lt;/strong&gt; compilation times. For example, when using Intel’s OpenCL compiler, it takes somewhere between 4 and 12 &lt;em class=&quot;markup--em markup--p-em&quot;&gt;hours&lt;/em&gt; to compile a typical program for the FPGA. This is due to the &lt;em class=&quot;markup--em markup--p-em&quot;&gt;place-and-route phase&lt;/em&gt;: the custom circuit that we &lt;em class=&quot;markup--em markup--p-em&quot;&gt;want&lt;/em&gt; needs to be mapped to the FPGA resources that we &lt;em class=&quot;markup--em markup--p-em&quot;&gt;have&lt;/em&gt;, with paths as short as possible. This is a complex optimization problem which requires significant computation. Intel does offer an emulator, so testing for correctness does not require this long step, but determining and optimizing performance does require these overnight compile phases.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*J-1MC3QGbIuwq4tb-yr-iA.png&quot; data-width=&quot;413&quot; data-height=&quot;360&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*J-1MC3QGbIuwq4tb-yr-iA.png&quot;/&gt;&lt;/div&gt;
Programming FPGAs gives you &lt;strong class=&quot;markup--strong markup--figure-strong&quot;&gt;a lot&lt;/strong&gt; of time to slack off (image credit: &lt;a href=&quot;https://xkcd.com/303/&quot; data-href=&quot;https://xkcd.com/303/&quot; class=&quot;markup--anchor markup--figure-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;XKCD&lt;/a&gt;)
&lt;h3 name=&quot;789c&quot; id=&quot;789c&quot; class=&quot;graf graf--h3 graf-after--figure&quot;&gt;Energy efficiency&lt;/h3&gt;
&lt;p name=&quot;057b&quot; id=&quot;057b&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;In their communications, Intel is always touting energy efficiency as a clear benefit of FPGAs. However, the situation is really not that clear cut, especially when it comes to floating point computations, but let us first consider situations where FPGAs are clearly more energy efficient than a CPU or GPU.&lt;/p&gt;
&lt;p name=&quot;8298&quot; id=&quot;8298&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Where FPGAs shine in terms of energy efficiency is at logic and fixed precision (as opposed to floating point) computations. In crypto-currency (such as bitcoin) mining, it is exactly this property that makes FPGAs advantageous. In fact, everyone used to mine bitcoin on FPGAs.&lt;/p&gt;
&lt;p name=&quot;9df3&quot; id=&quot;9df3&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;By the way, nowadays everybody is using ASICs (Application Specific Integrated Circuit) for bitcoin mining. Which are special integrated circuits built for just one purpose. ASICs are an even more energy efficient solution but require a very large upfront investment for the design and large number of chips produced to be cost effective. But back to FPGAs.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*1oa05oiK6chWPsItd-QX_A.jpeg&quot; data-width=&quot;4232&quot; data-height=&quot;1636&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*1oa05oiK6chWPsItd-QX_A.jpeg&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*1oa05oiK6chWPsItd-QX_A.jpeg&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;2cdd&quot; id=&quot;2cdd&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Another benefit of FPGAs in terms of energy efficiency is that FPGA boards &lt;em class=&quot;markup--em markup--p-em&quot;&gt;do not require a host computer to run,&lt;/em&gt; since they have their own input/output — we can save energy and money on the host. This in contrast to GPUs, which communicate with a host system using PCIe or NVLink, and hence require a host to run. (An exception to the rule that GPUs require a host is the &lt;a href=&quot;https://developer.nvidia.com/embedded-computing&quot; data-href=&quot;https://developer.nvidia.com/embedded-computing&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;NVidia Jetson&lt;/a&gt;, but this is not a high-end GPU.)&lt;/p&gt;
&lt;h3 name=&quot;3bf9&quot; id=&quot;3bf9&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Energy efficiency for floating point — FPGA vs GPU&lt;/h3&gt;
&lt;p name=&quot;a846&quot; id=&quot;a846&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;A lot of high performance computing use cases, such as deep learning, often depend on floating point arithmetic — something GPUs are very good at. In the past, FPGAs were pretty inefficient for floating point computations because a floating point unit had to be assembled from logic blocks, costing a lot of resources.&lt;/p&gt;
&lt;p name=&quot;93c7&quot; id=&quot;93c7&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Newer FPGAs such as the &lt;a href=&quot;https://www.altera.com/products/fpga/arria-series/arria-10/overview.html&quot; data-href=&quot;https://www.altera.com/products/fpga/arria-series/arria-10/overview.html&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Arria 10&lt;/a&gt; and &lt;a href=&quot;https://www.altera.com/products/fpga/stratix-series/stratix-10/overview.html&quot; data-href=&quot;https://www.altera.com/products/fpga/stratix-series/stratix-10/overview.html&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Stratix 10&lt;/a&gt; have &lt;em class=&quot;markup--em markup--p-em&quot;&gt;built-in&lt;/em&gt; floating point units on the FPGA fabric, making them much better at floating point computations. Does the addition of floating point units make FPGAs interesting for floating point computations in terms of energy efficiency? Are they more energy efficient than a GPU?&lt;/p&gt;
&lt;p name=&quot;c3f2&quot; id=&quot;c3f2&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Let’s compare a state-of-the-art GPU to a state-of-the-art FPGA. The fastest professional GPU that is available now is the &lt;a href=&quot;https://images.nvidia.com/content/technologies/volta/pdf/tesla-volta-v100-datasheet-letter-fnl-web.pdf&quot; data-href=&quot;https://images.nvidia.com/content/technologies/volta/pdf/tesla-volta-v100-datasheet-letter-fnl-web.pdf&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Tesla V100&lt;/a&gt;, which has a theoretical maximum of 15 TFLOPS (Tera-floating-point-operations per second, a standard means of measuring floating point performance) and uses about 250 Watts of power. One of the best available FPGA boards now is the &lt;a href=&quot;http://www.nallatech.com/wp-content/uploads/Nallatech-520C-Product-Brief-V9.pdf&quot; data-href=&quot;http://www.nallatech.com/wp-content/uploads/Nallatech-520C-Product-Brief-V9.pdf&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Nallatech 520C&lt;/a&gt;, which uses the new &lt;a href=&quot;https://www.altera.com/products/fpga/stratix-series/stratix-10/overview.html#family-table&quot; data-href=&quot;https://www.altera.com/products/fpga/stratix-series/stratix-10/overview.html#family-table&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Statix 10 Chip&lt;/a&gt; by Altera/Intel. This card has a theoretical maximum of 9.2 TFLOPS and uses about 225 Watts of power.&lt;/p&gt;
&lt;p name=&quot;27a0&quot; id=&quot;27a0&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;If we compare these two devices on energy efficiency, the GPU appears to be more energy efficient, achieving 56 GFLOP/W (Giga-floating-point-operation per Watt, a standard means of measuring energy efficiency of float point performance) in theory, while the FPGA achieves only 40.9 GFLOP/W. So if you’re going to buy new floating point hardware right now, and you need a host computer, then it seems you’re better off with the GPU, at least in this crude comparison.&lt;/p&gt;
&lt;p name=&quot;b322&quot; id=&quot;b322&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;However, the difference is small, and it is very possible that a new FPGA card, such as &lt;a href=&quot;https://www.bittware.com/fpga/intel/boards/s10vm4/&quot; data-href=&quot;https://www.bittware.com/fpga/intel/boards/s10vm4/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;this upcoming card&lt;/a&gt; based on the Stratix 10 FPGA, is more energy efficient than the Volta on floating point computations. Moreover, the above comparison is between apples and oranges in the sense that the Tesla V100 is produced at a12 nanometer process, whereas the Stratix 10 is produced at the older 14 nanometer process.&lt;/p&gt;
&lt;p name=&quot;e2bd&quot; id=&quot;e2bd&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;While the comparison does show that if you want energy efficient floating point computations &lt;em class=&quot;markup--em markup--p-em&quot;&gt;now&lt;/em&gt; that it is better to stick with GPUs, it does &lt;em class=&quot;markup--em markup--p-em&quot;&gt;not&lt;/em&gt; show that GPUs are inherently more energy efficient for floating point computations. The battle for floating point energy efficiency is currently won by GPUs, but this may change in the near future.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*0jN3eROhFPE_g28k1lqg4Q.png&quot; data-width=&quot;505&quot; data-height=&quot;304&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*0jN3eROhFPE_g28k1lqg4Q.png&quot;/&gt;&lt;/div&gt;
Energy label for FPGAs: depends on the application (Image copyright: European Union)
&lt;p name=&quot;62e2&quot; id=&quot;62e2&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;If the host is not required, then a comparison between a high end &lt;em class=&quot;markup--em markup--p-em&quot;&gt;GPU with a host&lt;/em&gt; and an high end &lt;em class=&quot;markup--em markup--p-em&quot;&gt;FPGA without a host&lt;/em&gt; is in order. If we use the same numbers as in the above comparison, then a GPU with a host and an FPGA without a host are exactly as energy efficient if the host takes 116.7 Watts (per GPU in the case of a multi-GPU setup). A modern host consumes somewhere in the 50–250 Watt range, making the FPGA much more competitive.&lt;/p&gt;
&lt;h3 name=&quot;0e78&quot; id=&quot;0e78&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Overview and outlook&lt;/h3&gt;
&lt;p name=&quot;4ca0&quot; id=&quot;4ca0&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;In some areas, it is hard to get around FPGAs. In military applications, such as missile guidance systems, FPGAs are used for their low latency. In radio-astronomy applications, the specialized input/output of FPGAs is essential to process the huge amount of data. In crypto-currency mining the energy efficiency on fixed precision and logic operations of FPGAs can be advantageous.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*BvBH7nToLZJ155CfsOey3Q.jpeg&quot; data-width=&quot;2000&quot; data-height=&quot;1000&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*BvBH7nToLZJ155CfsOey3Q.jpeg&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*BvBH7nToLZJ155CfsOey3Q.jpeg&quot;/&gt;&lt;/div&gt;
Artist’s impression of the yet to be built SKA radio telescope (image credit: SKA Organisation/Swinburne Astronomy Productions)
&lt;p name=&quot;e959&quot; id=&quot;e959&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;However, Intel did not spend $16.7 billion on Altera just for these somewhat niche-markets — they have bigger plans for it. The two markets they want to infiltrate are, as far as I can tell, high performance computing and cloud computing (i.e. use in amazon-like centers).&lt;/p&gt;
&lt;h4 name=&quot;d793&quot; id=&quot;d793&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;FPGAs for High Performance Computing&lt;/h4&gt;
&lt;p name=&quot;e4fa&quot; id=&quot;e4fa&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Personally, I do not think FPGAs will make a big splash in the high performance computing market in the coming years. Even if they become slightly more energy efficient than GPUs, the development of software for FPGAs is still a lot more difficult than for GPUs. The HPC community is already used to GPUs — getting people to switch from GPUs to FPGAs requires larger benefits. In the longer run, i.e. more than 5 years, it might turn out that FPGAs do offer such large benefits, which is what Intel seems to be hoping.&lt;/p&gt;
&lt;h4 name=&quot;7213&quot; id=&quot;7213&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;FPGAs for Cloud providers&lt;/h4&gt;
&lt;p name=&quot;65f9&quot; id=&quot;65f9&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;The other market is cloud providers. Intel envisions cloud servers to have an FPGA or to run on an CPU-FPGA hybrid. The idea is that certain parts of the computation can be offloaded to the FPGA and/or that FPGAs can be used to customize the network topology.&lt;/p&gt;
&lt;p name=&quot;62e7&quot; id=&quot;62e7&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Microsoft, no doubt in cooperation with Intel, &lt;a href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2014/06/HC26.12.520-Recon-Fabric-Pulnam-Microsoft-Catapult.pdf&quot; data-href=&quot;https://www.microsoft.com/en-us/research/wp-content/uploads/2014/06/HC26.12.520-Recon-Fabric-Pulnam-Microsoft-Catapult.pdf&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;has implemented using FPGAs in its datacenters&lt;/a&gt; and has a network of 100.000 FPGAs. Microsoft is touting big benefits in terms of performance of Bing search, which now is computed partially by FPGAs, and flexibility. Amazon is also offering FPGA nodes on its popular EC2 platform. Whether this trend continues remains to be seen.&lt;/p&gt;
&lt;h4 name=&quot;b1d0&quot; id=&quot;b1d0&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Outlook&lt;/h4&gt;
&lt;p name=&quot;02fd&quot; id=&quot;02fd&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;So are these previously esoteric FPGAs about to go mainstream? Personally, I’m skeptical. I think that for FPGAs to really take off two things are needed:&lt;/p&gt;
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;357b&quot; id=&quot;357b&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;They should be much easier to &lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;program&lt;/strong&gt;, especially by bringing down compile times.&lt;/li&gt;
&lt;li name=&quot;7b6b&quot; id=&quot;7b6b&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;They should be more &lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;energy efficient&lt;/strong&gt; on floating point computations.&lt;/li&gt;
&lt;/ul&gt;&lt;p name=&quot;6f57&quot; id=&quot;6f57&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;Intel is working hard on these issues, but these are very large hurdles to take.&lt;/p&gt;
&lt;p name=&quot;640a&quot; id=&quot;640a&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Do you have comments or more information? Leave a note in the comments!&lt;/p&gt;
&lt;p name=&quot;6fae&quot; id=&quot;6fae&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Want to know more? A lot of information can be found here:&lt;/p&gt;
&lt;p name=&quot;d91a&quot; id=&quot;d91a&quot; class=&quot;graf graf--p graf-after--p graf--trailing&quot;&gt;&lt;a href=&quot;https://www.nextplatform.com/tag/fpga/&quot; data-href=&quot;https://www.nextplatform.com/tag/fpga/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;https://www.nextplatform.com/tag/fpga/&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Tue, 14 Aug 2018 13:22:11 +0000</pubDate>
<dc:creator>ScottWRobinson</dc:creator>
<og:title>Why use an FPGA instead of a CPU or GPU? – Netherlands eScience Center</og:title>
<og:url>https://blog.esciencecenter.nl/why-use-an-fpga-instead-of-a-cpu-or-gpu-b234cd4f309c</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*c54zfRSRF7i6MOSy04Pa6Q.jpeg</og:image>
<og:description>The (dis)advantages of Field Programmable Gate Arrays</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.esciencecenter.nl/why-use-an-fpga-instead-of-a-cpu-or-gpu-b234cd4f309c?gi=bb2b2c2ff2e5</dc:identifier>
</item>
<item>
<title>Why you should, and shouldn’t, join a startup</title>
<link>https://www.atrium.co/blog/work-at-a-startup/</link>
<guid isPermaLink="true" >https://www.atrium.co/blog/work-at-a-startup/</guid>
<description>&lt;div class=&quot;entry-content&quot;&gt;
&lt;div class=&quot;addtoany_share_save_container addtoany_content addtoany_content_top&quot;&gt;
&lt;div class=&quot;addtoany_header&quot;&gt;Share this article!&lt;/div&gt;

&lt;/div&gt;
&lt;p&gt;I’ve spent my entire professional career in the startup space. After a couple successes (and more than a couple failures), I’ve determined that I’m passionate about startups and it’s how I want to spend my time. It might therefore seem obvious that I’d recommend working at (or starting) startups to everyone. That’s definitely &lt;strong&gt;not&lt;/strong&gt; the case!&lt;/p&gt;
&lt;p&gt;There are many reasons to join a startup irrespective of your long-term goals or outlook. But even as a serial entrepreneur and startup lifer, I can see a few reasons for some people to steer clear. If you’re seriously considering both options — a traditional company and a startup — here are some points to consider.&lt;/p&gt;
&lt;p&gt;This post is extracted from a talk I gave at Y Combinator’s &lt;em&gt;Work at a Startup&lt;/em&gt; Expo. You can watch the video here:&lt;/p&gt;
&lt;div class=&quot;responsive-embed widescreen&quot;&gt;&lt;iframe width=&quot;500&quot; height=&quot;281&quot; src=&quot;https://www.youtube.com/embed/YzyatiQrQlQ?feature=oembed&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/div&gt;
&lt;h2&gt;Why not to join a startup&lt;/h2&gt;
&lt;p&gt;My goal here is just to give you the most unfiltered, raw information that I have in order to help you make an informed decision.&lt;/p&gt;
&lt;h3&gt;1. Management&lt;/h3&gt;
&lt;p&gt;The management at startups generally sucks. I wish I was joking, but sadly it is very true.&lt;/p&gt;
&lt;p&gt;I used to joke that there were two types of startups: rocket ships with bad management, and other companies with bad management. And as kind of a corollary to that, it is likely that if you join a startup, especially in early stage one, you won’t necessarily get enough mentorship or direction on what you’re doing unless you really actively force people to give it to you.&lt;/p&gt;
&lt;p&gt;There’s a positive side to lack of direction, which I’ll get to in the next part, but having to force your leaders and managers to give you guidance can be a very strong negative to people who are not self-directed.&lt;/p&gt;
&lt;p&gt;I’m running a startup now. I think our managers do a great job (and hope they’d say the same about me). But when you compare it to the defined structure of an established company, it just doesn’t compare. Big companies have employee onboarding, management training, goal setting they’ve been doing for years — all of the things that give people guidance and mentorship on what they need to do to be successful.&lt;/p&gt;
&lt;h3&gt;2. Riches&lt;/h3&gt;
&lt;p&gt;You are not likely to actually get rich joining a startup. It’s statistically improbable. It’s the most glamorized — and worst — reason to put up with the chaos.&lt;/p&gt;
&lt;p&gt;People hear the story of the masseuse at Google or the wall painter at Facebook receiving substantial equity checks and think that it will happen to them.&lt;/p&gt;
&lt;p&gt;If you think you’re going to join a startup and then be set for life, you are setting yourself up to be disappointed. There is certainly higher upside than any normal 9-5 job. But you have to assess a) your appetite for risk, and b) the opportunity cost of giving up stability and a healthier work/life balance.&lt;/p&gt;
&lt;h3&gt;3. Stability&lt;/h3&gt;
&lt;p&gt;Silicon Valley has matured in the last 10 years since I got here. Originally, people came here because they just wanted to work on interesting frontier projects, and that was a lot fewer people.&lt;/p&gt;
&lt;p&gt;Today, I think that there are a lot of people who come to Silicon Valley because they want a good career trajectory and high pay, and there are a lot of big tech companies that can offer that stability.&lt;/p&gt;
&lt;p&gt;If you want those things, you should not join a startup. And I’ve noticed more and more people, even people I’ve recruited more recently, coming in and asking, “What’s the career pathing here? What’s the five-year plan?”&lt;/p&gt;
&lt;p&gt;And the honest answer is, “We don’t have five years of money,” so anything I told you about a five-year plan would be complete bullshit right now. We certainly plan to be here, figure it out, and build a company that lasts decades (or more!). But if you want a guarantee of stability today, I think you should go join Google.&lt;/p&gt;
&lt;h2&gt;Why join a startup&lt;/h2&gt;
&lt;p&gt;Now on to what you really came for: the reasons you &lt;strong&gt;should&lt;/strong&gt; join a startup. For each reason I give below, I’ll share a story from my experience as an entrepreneur.&lt;/p&gt;
&lt;h3&gt;1. Access to jobs you are not qualified for&lt;/h3&gt;
&lt;p&gt;At a startup, you will get access to jobs that you are completely unqualified for and you might not be able to do well (yet). I have a good example from my time at Justin.tv: Eight years ago, I was on stage at the same YC Work at a Startup Expo talking about our startup and we actually ended up recruiting someone from that event.&lt;/p&gt;
&lt;p&gt;He was a programmer from France named Guillaume with only a couple years of practical experience. Within a year of joining the team, he was running our Rails application backend. Justin.tv was a Top 150 global site at the time, and top 20 Rails (it was 2010, so the bar was a lot lower).&lt;/p&gt;
&lt;p&gt;And that was a job he was completely unqualified for, and he would never have gotten the opportunity to it if he didn’t join a startup where we didn’t really have anyone else to do it. He went on to be co-founder of another Justin.tv spinoff, Socialcam.&lt;/p&gt;
&lt;p&gt;They went through YC and got an even greater scale challenge when they scaled like from zero to 128 million users in about two months. Just the rate of learning for him was pretty incredible. Now he’s a co-founder at a company called &lt;a href=&quot;http://triplebyte.com&quot;&gt;Triplebyte&lt;/a&gt; that helps technology companies recruit great engineers.&lt;/p&gt;
&lt;h3&gt;2. Gateway to starting your own company&lt;/h3&gt;
&lt;p&gt;Joining a startup is a really good gateway to starting your own company.&lt;/p&gt;
&lt;p&gt;Another example – in 2012, I started a company called Exec and I recruited Finbarr, a very talented engineer. He was an engineer at Groupon at the time. He wanted to break into the startup space and knew that he wanted to try building his own company at some point.&lt;/p&gt;
&lt;p&gt;I think it is really important is to put yourself in positions where you’re around people who will help you get to where you want to be. One of my co-founders of Twitch, Emmett Shear, always told me that “you are the average of your five closest friends”; if you want to be a founder, make friends with people who are in startups. I completely agree.&lt;/p&gt;
&lt;p&gt;After we sold Exec, Finbarr launched his own startup with another employee he ended up meeting at the company. That one didn’t work out, but it got him into startups, and eventually he founded another startup, &lt;a href=&quot;https://getshogun.com/&quot;&gt;Shogun&lt;/a&gt;, that just went through YC and is growing extremely well.&lt;/p&gt;
&lt;h3&gt;3. Optimize for rate of learning&lt;/h3&gt;
&lt;p&gt;I think this is actually the most important reason why you should join a startup. I have two examples of people who did that while working with me — the two co-founders of Cruise. I think that they are great examples because one is maximizing learning on the way up and the other on the way down.&lt;/p&gt;
&lt;p&gt;One of the cofounders of Cruise, Kyle Vogt, was a student recruited from MIT to Justin.tv. He was a hardware guy, and we thought we were building a hardware solution. Kyle ended up becoming a co-founder and VP of Engineering. He’s an amazing hacker and tinkerer who always figures out a way to build whatever you need. But at the time he wasn’t experienced in scaling systems or building scalable technology architectures.&lt;/p&gt;
&lt;p&gt;Kyle ended up building this scalable, dynamic, live video system that he engineered and architected from scratch. At first, it would go down all the time and we were calling him every 36-48 hours to bring it back to life. One time he finally took a vacation and the only way to get in touch with him was to order a pizza to his hotel and give the pizza guy a message to pass along!&lt;/p&gt;
&lt;p&gt;There weren’t many technical precedents and he had to learn fast. Kyle eventually got our live video system to the point that, by the time Twitch sold to Amazon in 2014, was the fourth largest bandwidth consumer in North America. Fifteen points of presence around the world. Ninety petabytes of data transfer every month. So his rate of learning was incredible as a software architect. And obviously, went on and took a lot of that learning to build a self-driving vehicle at Cruise.&lt;/p&gt;
&lt;p&gt;The other co-founder of Cruise was my brother Daniel, who met Kyle actually as an intern at Justin.tv. He also had a crash course in startups over the next couple years. As part of his job, he recruited this then unknown band called the Jonas Brothers to broadcast on the site.&lt;/p&gt;
&lt;p&gt;They ended up crashing Justin.tv’s infrastructure with all the traffic they brought in (and gave Kyle plenty of reasons to hate his life as a result). Daniel joined as an intern and he got to interact and make a deal with what basically become the number one teen band at the time in 2007. That’s not the kind of opportunity you get outside of a startup.&lt;/p&gt;
&lt;p&gt;Daniel is also a perfect example of how you will also maximize your learning if the startup is failing miserably (ie, on the way down). Daniel, our friend Amir and I cofounder Exec in 2012, and by the end of 2013 we realized that the home cleaning business is just not a great business. We were trying to sell it and ended up negotiating a deal with similar company in NYC: Handy.&lt;/p&gt;
&lt;p&gt;The deal was taking forever. There were tons of lawyers hours wasted negotiating trivial points that kept dragging and dragging and dragging. I was very burned out. I finally said, “I’m going on vacation, Daniel. You have to deal with it.”&lt;/p&gt;
&lt;p&gt;Admittedly, it wasn’t a very responsible thing to for me to do. But Daniel ended up having to be the one who would close this deal over the next month while I was in Thailand. I was in touch by phone, but he was mostly running this deal. He later told me how it was a horrible experience – but he learned all the minutiae of negotiating a deal from a point of low leverage. And he learned it on this very small, horrible deal which we were mostly just trying to offload because we were so burned out we wanted to get out of business.&lt;/p&gt;
&lt;p&gt;Fast forward two years and he was now a co-founder of Cruise, which ended up selling to GM for a billion dollars. The big takeaway is that Daniel applied all those horrible lessons he learned negotiating this awful, piddling deal for our company to his next company — which obviously turned out much better.&lt;/p&gt;
&lt;h2&gt;Conclusion: Join a startup if learning trumps stability&lt;/h2&gt;
&lt;p&gt;No matter what, when you join a startup, you’re going to learn something. Whether the company succeeds or fails, you’ll walk away with something valuable.&lt;/p&gt;
&lt;p&gt;The way I think about growing and maximizing your speed of learning is a quote that YC partner Paul Buchheit often says to exiting batches: &lt;em&gt;It’s not your Y intercept, but your slope that’s important.&lt;/em&gt; The way I’ve always thought about it is to figure out ways that I can put myself in the position to maximize my own person rate of growth and rate of learning. And I suggest that you do the same, regardless of whether that’s at a startup or not.&lt;/p&gt;
&lt;div class=&quot;addtoany_share_save_container addtoany_content addtoany_content_bottom&quot;&gt;
&lt;div class=&quot;addtoany_header&quot;&gt;Share this article!&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;&lt;div class=&quot;entry-content excerpt&quot;&gt;
&lt;p&gt;Hiring your first engineer at a startup is incredibly hard. As a founder, you’re already stretched dangerously thin on time.…&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.atrium.co/blog/hire-first-engineer/&quot; class=&quot;read-on&quot;&gt;Read On&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description>
<pubDate>Tue, 14 Aug 2018 13:01:14 +0000</pubDate>
<dc:creator>madmax108</dc:creator>
<og:type>article</og:type>
<og:title>Why You Should (And Shouldn’t) Join a Startup - atrium</og:title>
<og:description>I’ve spent my entire professional career in the startup space. After a couple successes (and more than a couple failures),… Read On</og:description>
<og:url>https://www.atrium.co/blog/work-at-a-startup/</og:url>
<og:image>https://www.atrium.co/blog/wp-content/uploads/2018/08/Screen-Shot-2018-08-09-at-5.16.26-PM.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.atrium.co/blog/work-at-a-startup/</dc:identifier>
</item>
<item>
<title>Americans Own Less Stuff, and That’s Reason to Be Nervous</title>
<link>https://www.bloomberg.com/view/articles/2018-08-12/american-ownership-society-is-changing-thanks-to-technology</link>
<guid isPermaLink="true" >https://www.bloomberg.com/view/articles/2018-08-12/american-ownership-society-is-changing-thanks-to-technology</guid>
<description>&lt;head&gt;&lt;title&gt;Terms of Service Violation&lt;/title&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;24&quot;&gt;
&lt;div class=&quot;container&quot; readability=&quot;9.4285714285714&quot;&gt;&lt;img src=&quot;https://www.bloomberg.com/graphics/assets/img/BB-Logo-2line.svg&quot; width=&quot;310&quot;/&gt;
&lt;p&gt;Your usage has been flagged as a violation of our &lt;a href=&quot;http://www.bloomberg.com/tos&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;terms of service&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For inquiries related to this message please &lt;a href=&quot;http://www.bloomberg.com/feedback&quot;&gt;contact support&lt;/a&gt;. For sales inquiries, please visit &lt;a href=&quot;http://www.bloomberg.com/professional/request-demo&quot;&gt;http://www.bloomberg.com/professional/request-demo&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;If you believe this to be in error, please confirm below that you are not a robot by clicking &quot;I'm not a robot&quot; below.&lt;/h3&gt;
&lt;br/&gt;&lt;h3&gt;Please make sure your browser supports JavaScript and cookies and that you are not blocking them from loading. For more information you can review the Terms of Service and Cookie Policy.&lt;/h3&gt;
&lt;br/&gt;&lt;h3 id=&quot;block_uuid&quot;&gt;Block reference ID:&lt;/h3&gt;
&lt;/div&gt;
&lt;/body&gt;</description>
<pubDate>Tue, 14 Aug 2018 11:49:26 +0000</pubDate>
<dc:creator>dustinupdyke</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bloomberg.com/tosv2.html?vid=&amp;uuid=76e26320-a061-11e8-8188-c152b5520376&amp;url=L3ZpZXcvYXJ0aWNsZXMvMjAxOC0wOC0xMi9hbWVyaWNhbi1vd25lcnNoaXAtc29jaWV0eS1pcy1jaGFuZ2luZy10aGFua3MtdG8tdGVjaG5vbG9neQ==</dc:identifier>
</item>
<item>
<title>Italy bridge: Genoa motorway collapse kills at least 22</title>
<link>https://www.bbc.com/news/world-europe-45183624</link>
<guid isPermaLink="true" >https://www.bbc.com/news/world-europe-45183624</guid>
<description>&lt;figure class=&quot;media-with-caption&quot; readability=&quot;-24&quot;&gt;&lt;div class=&quot;player-with-placeholder&quot; readability=&quot;7&quot;&gt;
            &lt;img class=&quot;media-placeholder player-with-placeholder__image lead-video-placeholder&quot; src=&quot;https://ichef.bbci.co.uk/images/ic/720x405/p06hgbfy.jpg&quot;/&gt;&lt;p&gt;Media playback is unsupported on your device&lt;/p&gt;
      
    &lt;/div&gt;    &lt;figcaption class=&quot;media-with-caption__caption&quot;&gt;&lt;span class=&quot;off-screen&quot;&gt;Media caption&lt;/span&gt;Parts of the bridge can be seen collapsing&lt;/figcaption&gt;&lt;/figure&gt;&lt;p class=&quot;story-body__introduction&quot;&gt;A motorway bridge has collapsed in the northwest Italian city of Genoa, killing 26 people and badly injuring 15, police told the BBC. &lt;/p&gt;&lt;p&gt;Dramatic video footage captured the moment of the disaster when one of the huge supporting towers crashed down during torrential rain.&lt;/p&gt;&lt;p&gt;Cars and trucks plummeted 45m (148ft) on to rail tracks, buildings and a river along with slabs of concrete.&lt;/p&gt;&lt;p&gt;Searches for people trapped in rubble are expected to go into the night.&lt;/p&gt;&lt;p&gt;Fears that other parts of the bridge might fall have prompted the evacuation of buildings in the area, a rescuer told Italy's Ansa news agency.  &lt;/p&gt;&lt;figure class=&quot;media-landscape has-caption full-width&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                
                
                
                
                 &lt;span class=&quot;off-screen&quot;&gt;Image copyright&lt;/span&gt;
                 &lt;span class=&quot;story-image-copyright&quot;&gt;EPA&lt;/span&gt;
                
            &lt;/span&gt;
            
            &lt;figcaption class=&quot;media-caption&quot;&gt;&lt;span class=&quot;off-screen&quot;&gt;Image caption&lt;/span&gt;
                &lt;span class=&quot;media-caption__text&quot;&gt;
                    Injured people were winched to safety
                &lt;/span&gt;
            &lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Interior Minister Matteo Salvini promised that anyone found to be responsible for the bridge collapse would be held to account.&lt;/p&gt;&lt;p&gt;&quot;I have crossed that bridge hundreds of times,&quot; he said. &quot;Now, as an Italian citizen, I will do everything to get the names and surnames of the managers responsible, past and present, because it is unacceptable to die like that in Italy.&quot;&lt;/p&gt;&lt;p&gt;A representative of the motorway's operator, Autostrade, told Reuters news agency there had been &quot;no reason to consider the bridge was dangerous&quot;.    &lt;/p&gt;&lt;p&gt;Shares in Atlantia, Autostrade's parent company which runs much of the country's motorways, fell 6.3% after news of the collapse.&lt;/p&gt;&lt;h2 class=&quot;story-body__crosshead&quot;&gt;How did the structure collapse?&lt;/h2&gt;&lt;p&gt;It fell around 11:30 local time (09:30 GMT) during heavy rain. Police reported a violent cloudburst. &lt;/p&gt;&lt;p&gt;&quot;We saw lightning strike the bridge,&quot; eyewitness Pietro M all'Asa was quoted as saying by Ansa. &quot;And we saw the bridge going down.&quot;&lt;/p&gt;&lt;p&gt;Engineers say it is too early to determine the cause of the collapse but that lightning is unlikely to be the reason. &lt;/p&gt;&lt;figure class=&quot;media-landscape no-caption full-width&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                
                
                
                
                 &lt;span class=&quot;off-screen&quot;&gt;Image copyright&lt;/span&gt;
                 &lt;span class=&quot;story-image-copyright&quot;&gt;AFP&lt;/span&gt;
                
            &lt;/span&gt;
            
        &lt;/figure&gt;&lt;p&gt;Another witness, unnamed, recalled: &quot;We heard an incredible roar and first we thought it was thunder very close by.&lt;/p&gt;&lt;p&gt;&quot;We live about 5km [three miles] from the bridge but we heard a crazy bang... We were very scared... Traffic went completely haywire and the city was paralysed.&quot;&lt;/p&gt;&lt;figure class=&quot;media-landscape no-caption full-width&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                
                
                
                
            &lt;/span&gt;
            
        &lt;/figure&gt;&lt;p&gt;One image posted by the regional emergency services shows a truck perched at the end of the surviving bridge section immediately before the drop.&lt;/p&gt;&lt;figure class=&quot;media-landscape no-caption full-width&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                
                
                
                
            &lt;/span&gt;
            
        &lt;/figure&gt;&lt;h2 class=&quot;story-body__crosshead&quot;&gt;What do we know of the victims?&lt;/h2&gt;&lt;p&gt;Earlier, fire brigade sources told Ansa unofficially that 35 people were dead and 12 missing. &lt;/p&gt;&lt;p&gt;A child is among the dead, said the head of the civil defence agency, Angelo Borrelli. &lt;/p&gt;&lt;p&gt;Between 30 and 35 cars and three heavy vehicles were on the bridge at the time of the collapse, he said.&lt;/p&gt;&lt;p&gt;&quot;We are continuing with the rescue operations because we think there are other people alive under the rubble,&quot; Genoa police spokesperson Alessandra Bucci told Reuters.&lt;/p&gt;&lt;p&gt;&quot;We have extracted people from the rubble and now we are focusing on assisting the people, and later on we will understand what caused the collapse of the bridge.&quot;&lt;/p&gt;&lt;p&gt;The full horror of the collapse could be seen in aerial video of the scene.&lt;/p&gt;&lt;figure class=&quot;media-landscape no-caption full-width&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                
                
                
                
            &lt;/span&gt;
            
        &lt;/figure&gt;&lt;p&gt;Patrick Villardry, a French firefighter who came from Nice to help the rescue effort, told AFP news agency the task was huge.&lt;/p&gt;&lt;p&gt;&quot;The first victims have been evacuated and now we have to search under the wreckage of buildings, but there are thousands of tonnes of concrete,&quot; he said.&lt;/p&gt;&lt;figure class=&quot;media-landscape no-caption full-width&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                
                
                
                
                 &lt;span class=&quot;off-screen&quot;&gt;Image copyright&lt;/span&gt;
                 &lt;span class=&quot;story-image-copyright&quot;&gt;Reuters&lt;/span&gt;
                
            &lt;/span&gt;
            
        &lt;/figure&gt;&lt;h2 class=&quot;story-body__crosshead&quot;&gt;How important is the bridge?&lt;/h2&gt;&lt;p&gt;The Morandi Bridge, built in the 1960s, stands on the A10 toll motorway, which serves the Italian Riviera and southern coast of France.&lt;/p&gt;&lt;p&gt;The missing section was dozens of metres in length, and ran across the span of the Polcevera river.&lt;/p&gt;&lt;figure class=&quot;media-landscape no-caption full-width&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                
                
                
                
            &lt;/span&gt;
            
        &lt;/figure&gt;&lt;p&gt;The collapse of the bridge was an &quot;incident of vast proportions on a vital arterial road, not just for Genoa, but for the whole country&quot;, said Mr Toti.&lt;/p&gt;&lt;p&gt;&quot;The Morandi bridge connects three major ports in our country, used by tens, even hundreds of thousands of people. They depart from these ports on holiday. These docks receive most of our country's imported goods. It damages the very structure of the Italian logistics system. We are expecting a very fast response from the government.&quot; &lt;/p&gt;&lt;figure class=&quot;media-landscape no-caption full-width&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                
                
                
                
            &lt;/span&gt;
            
        &lt;/figure&gt;&lt;figure class=&quot;media-landscape no-caption full-width&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                
                
                
                
            &lt;/span&gt;
            
        &lt;/figure&gt;&lt;p&gt;Mr Borrelli said the authorities were trying to arrange help for those affected by the disaster, as well as setting up diversions for traffic.&lt;/p&gt;&lt;p&gt;&quot;What we are carrying out at the moment is a search and rescue operation for the victims and the injured, to get the victims out and recover the injured,&quot; he said.&lt;/p&gt;&lt;p&gt;&quot;Then we are obviously also trying to work out how to set up a viable route that is an alternative to the motorway, and also for entry and exit from the port.&quot;&lt;/p&gt;&lt;h2 class=&quot;story-body__crosshead&quot;&gt;Were there any concerns about the bridge? &lt;/h2&gt;&lt;p&gt;&quot;It's not acceptable that such an important bridge... was not built to avoid this kind of collapse,&quot; Mr Rixi was quoted as saying by Reuters.&lt;/p&gt;&lt;p&gt;However, Stefano Marigliani, the Autostrade official responsible for the Genoa area, told the agency that the bridge had been &quot;constantly monitored and supervised well beyond what the law required&quot;. &lt;/p&gt;&lt;p&gt;The highway operator said work to shore up its foundation was being carried out at the time of the collapse.&lt;/p&gt;&lt;p&gt;Repair work on the bridge was carried out in 2016, Reuters reports. Major repairs also took place in the 1990s. &lt;/p&gt;&lt;figure class=&quot;media-landscape no-caption full-width&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                
                
                
                
                 &lt;span class=&quot;off-screen&quot;&gt;Image copyright&lt;/span&gt;
                 &lt;span class=&quot;story-image-copyright&quot;&gt;EPA&lt;/span&gt;
                
            &lt;/span&gt;
            
        &lt;/figure&gt;&lt;p&gt;A structural engineer who lectures at Genoa University, Antonio Brencich, warned in 2016 that there were problems with the bridge, Italian media report. &lt;/p&gt;&lt;p&gt;The bridge's designer, Riccardo Morandi (1902-1989) had miscalculated the &quot;viscous deformation&quot; - an ageing effect on reinforced concrete, Mr Brencich said.&lt;/p&gt;&lt;p&gt;&quot;He was an engineer with great insight but lacking in practical calculations,&quot; the lecturer said.&lt;/p&gt;&lt;p&gt;Italy's recently installed government has pledged to increase public investment in infrastructure. &lt;/p&gt;&lt;p&gt;The country spent more than €14bn (£12.5bn; $16bn) on its roads in 2006 but that had dropped to less than €4bn by 2010, according to data from the Organisation for Economic Co-operation and Development.&lt;/p&gt;&lt;figure class=&quot;media-landscape no-caption full-width&quot;&gt;&lt;span class=&quot;image-and-copyright-container&quot;&gt;
                
                
                
                
                
            &lt;/span&gt;
            
        &lt;/figure&gt;&lt;p&gt;The figures cover spending on new transport construction and the improvement of the existing networks.&lt;/p&gt;&lt;p&gt;Spending started to increase in 2013, when total spend was less than Spain, Germany, France and the UK.&lt;/p&gt;&lt;h2 class=&quot;story-body__crosshead&quot;&gt;How has the world reacted?&lt;/h2&gt;&lt;p&gt;French President Emmanuel Macron tweeted a message of sympathy to the people of Italy, writing in both Italian and French. He said France was ready to offer any necessary aid.&lt;/p&gt;&lt;p&gt;European Commission chief Jean-Claude Juncker voiced his &quot;deepest sympathy and sincere condolences to the families and friends of those  who have died, and to the Italian people&quot;.&lt;/p&gt;&lt;hr class=&quot;story-body__line&quot;/&gt;&lt;p&gt;&lt;strong&gt;Did you witness the collapse of the bridge? Are you in the area? If safe to do so, please email &lt;/strong&gt;&lt;a href=&quot;mailto:haveyoursay@bbc.co.uk?subject=Genoa45183624&quot; class=&quot;story-body__link-email&quot;&gt;&lt;span class=&quot;icon email&quot;/&gt;&lt;span class=&quot;story-body__link-email-text&quot;&gt;haveyoursay@bbc.co.uk&lt;/span&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Please include a contact number if you are willing to speak to a BBC journalist. You can also contact us in the following ways:&lt;/p&gt;&lt;ul class=&quot;story-body__unordered-list&quot;&gt;&lt;li class=&quot;story-body__list-item&quot;&gt;WhatsApp: &lt;strong&gt;+44 7555 173285&lt;/strong&gt;
&lt;/li&gt;
&lt;li class=&quot;story-body__list-item&quot;&gt;Send pictures/video to &lt;a href=&quot;mailto:yourpics@bbc.co.uk&quot; class=&quot;story-body__link-email&quot;&gt;&lt;span class=&quot;icon email&quot;/&gt;&lt;span class=&quot;story-body__link-email-text&quot;&gt;yourpics@bbc.co.uk&lt;/span&gt;&lt;/a&gt;
&lt;/li&gt;
&lt;li class=&quot;story-body__list-item&quot;&gt;Or &lt;a href=&quot;https://bbcnewsupload.streamuk.com/&quot; class=&quot;story-body__link-external&quot;&gt;Upload your pictures/video here&lt;/a&gt;
&lt;/li&gt;
&lt;li class=&quot;story-body__list-item&quot;&gt;Tweet: &lt;a href=&quot;http://twitter.com/BBC_HaveYourSay&quot; class=&quot;story-body__link-external&quot;&gt;@BBC_HaveYourSay&lt;/a&gt;
&lt;/li&gt;
&lt;li class=&quot;story-body__list-item&quot;&gt;Send an SMS or MMS to +44 7624 800 100&lt;/li&gt;
&lt;li class=&quot;story-body__list-item&quot;&gt;Please read our &lt;a href=&quot;http://www.bbc.co.uk/usingthebbc/terms/&quot; class=&quot;story-body__link&quot;&gt;terms &amp;amp; conditions&lt;/a&gt; and &lt;a href=&quot;http://www.bbc.co.uk/usingthebbc/privacy-policy/&quot; class=&quot;story-body__link&quot;&gt;privacy policy&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
<pubDate>Tue, 14 Aug 2018 11:29:54 +0000</pubDate>
<dc:creator>rubenbe</dc:creator>
<og:title>Dozens feared dead in Italy bridge collapse</og:title>
<og:type>article</og:type>
<og:description>A big section of motorway plunges to the ground in Genoa taking many vehicles with it.</og:description>
<og:url>https://www.bbc.co.uk/news/world-europe-45183624</og:url>
<og:image>https://ichef.bbci.co.uk/images/ic/1024x576/p06hgbfy.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bbc.com/news/world-europe-45183624</dc:identifier>
</item>
</channel>
</rss>