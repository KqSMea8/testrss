<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Police Say Uber Is Likely Not at Fault for Self-Driving Car Fatality in Arizona</title>
<link>http://fortune.com/2018/03/19/uber-self-driving-car-crash/</link>
<guid isPermaLink="true" >http://fortune.com/2018/03/19/uber-self-driving-car-crash/</guid>
<description>&lt;div class=&quot;padded&quot; readability=&quot;44.16&quot;&gt;
&lt;p&gt;The police chief of Tempe, Arizona, where a woman was &lt;a href=&quot;http://fortune.com/2018/03/19/uber-halts-self-driving-car-testing-fatal-accident-tempe-arizona/&quot;&gt;struck and killed by one of Uber’s self-driving cars&lt;/a&gt; Sunday, says the ride-sharing company is likely not at fault for the accident, following a preliminary investigation.&lt;/p&gt;
&lt;p&gt;Chief of Police Sylvia Moir &lt;a href=&quot;https://www.sfchronicle.com/business/article/Exclusive-Tempe-police-chief-says-early-probe-12765481.php?utm_campaign=twitter-premium&amp;amp;utm_source=CMS%20Sharing%20Button&amp;amp;utm_medium=social&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;told the San Francisco &lt;em&gt;Chronicle&lt;/em&gt;&lt;/a&gt; on Monday that video footage taken from cameras equipped to the autonomous Volvo SUV potentially shift the blame to the victim herself, 49-year-old Elaine Herzberg, rather than the vehicle.&lt;/p&gt;
&lt;p&gt;“It’s very clear it would have been difficult to avoid this collision in any kind of mode [autonomous or human-driven] based on how she came from the shadows right into the roadway,” Moir told the paper, adding that the incident occurred roughly 100 yards from a crosswalk. “It is dangerous to cross roadways in the evening hour when well-illuminated managed crosswalks are available,” she said.&lt;/p&gt;


&lt;p&gt;Though the vehicle was operating in autonomous mode, a driver was present in the front seat. But Moir said there appears to be little he could have done to intervene before the crash.&lt;/p&gt;
&lt;p&gt;“The driver said it was like a flash, the person walked out in front of them,” Moir said. “His first alert to the collision was the sound of the collision.”&lt;/p&gt;
&lt;p&gt;According to the &lt;em&gt;Chronicle&lt;/em&gt;, the preliminary investigation found the Uber car was driving at 38 mph in a 35 mph zone and did not attempt to brake. Herzberg is said to have abruptly walked from a center median into a lane with traffic. Police believe she may have been homeless.&lt;/p&gt;
&lt;p&gt;Tempe police plan to work with investigators from the National Transportation Safety Board and the National Highway Traffic Safety Administration to reach a conclusion about what happened, according to the &lt;em&gt;Chronicle&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Sunday’s incident appears to be the &lt;a href=&quot;http://time.com/5205767/uber-autonomous-car-crash-arizona/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;first-ever self-driving car fatality involving a pedestrian.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
<pubDate>Tue, 20 Mar 2018 00:28:54 +0000</pubDate>
<dc:creator>tlrobinson</dc:creator>
<og:title>Police Say Uber Is Likely Not at Fault for Its Self-Driving Car Fatality in Arizona</og:title>
<og:type>article</og:type>
<og:url>http://fortune.com/2018/03/19/uber-self-driving-car-crash/</og:url>
<og:description>A preliminary investigation has shifted the blame more towards the victim.</og:description>
<og:image>https://fortunedotcom.files.wordpress.com/2016/05/uatc-car.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://fortune.com/2018/03/19/uber-self-driving-car-crash/</dc:identifier>
</item>
<item>
<title>Facebook Security Chief Said to Leave After Clashes Over Disinformation</title>
<link>https://www.nytimes.com/2018/03/19/technology/facebook-alex-stamos.html</link>
<guid isPermaLink="true" >https://www.nytimes.com/2018/03/19/technology/facebook-alex-stamos.html</guid>
<description>&lt;p itemprop=&quot;articleBody&quot;&gt;Despite the rumors, I'm still fully engaged with my work at Facebook. It's true that my role did change. I'm currently spending more time exploring emerging security risks and working on election security.&lt;/p&gt;</description>
<pubDate>Mon, 19 Mar 2018 22:23:41 +0000</pubDate>
<dc:creator>aaronbrethorst</dc:creator>
<og:url>https://www.nytimes.com/2018/03/19/technology/facebook-alex-stamos.html</og:url>
<og:type>article</og:type>
<og:title>Facebook Executive Planning to Leave Company Amid Disinformation Backlash</og:title>
<og:description>Facebook has reached a deal with its chief information security officer to depart after disagreements over how to address its role in spreading disinformation, said people briefed on the matter.</og:description>
<og:image>https://static01.nyt.com/images/2018/03/20/business/20STAMOS/20STAMOS-facebookJumbo-v2.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nytimes.com/2018/03/19/technology/facebook-alex-stamos.html</dc:identifier>
</item>
<item>
<title>At full speed with Python: a book for self-learners</title>
<link>https://github.com/joaoventura/full-speed-python</link>
<guid isPermaLink="true" >https://github.com/joaoventura/full-speed-python</guid>
<description>&lt;h3&gt;README.md&lt;/h3&gt;
&lt;article class=&quot;markdown-body entry-content&quot; itemprop=&quot;text&quot;&gt;
&lt;p&gt;This book aims to teach the basics of the python programming language using a practical approach. Its method is quite basic though: after a very simple introduction to each topic, the reader is invited to learn by solving the proposed exercises.&lt;/p&gt;
&lt;p&gt;These exercises have been used extensively in my web development and distributed computing classes at the Superior School of Technology of Setúbal. With these exercises, most students are at full speed with Python in less than a month. In fact, students of the distributed computing course, taught in the second year of the software engineering degree, become familiar with Python's syntax in two weeks and are able to implement a distributed client-server application with sockets in the third week.&lt;/p&gt;
&lt;p&gt;This book is made available in github (&lt;a href=&quot;https://github.com/joaoventura/full-speed-python&quot;&gt;https://github.com/joaoventura/full-speed-python&lt;/a&gt;) so I appreciate any pull requests to correct misspellings or to suggest new exercises or clarification of the current content.&lt;/p&gt;
&lt;p&gt;The generated files (pdf and epub) can be found at &lt;a href=&quot;https://github.com/joaoventura/full-speed-python/releases/&quot;&gt;https://github.com/joaoventura/full-speed-python/releases/&lt;/a&gt;.&lt;/p&gt;
&lt;/article&gt;</description>
<pubDate>Mon, 19 Mar 2018 19:12:07 +0000</pubDate>
<dc:creator>jventura</dc:creator>
<og:image>https://avatars2.githubusercontent.com/u/1718359?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>joaoventura/full-speed-python</og:title>
<og:url>https://github.com/joaoventura/full-speed-python</og:url>
<og:description>full-speed-python - At full speed with Python: a book for self-learners</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/joaoventura/full-speed-python</dc:identifier>
</item>
<item>
<title>The Cambridge Analytica scandal isn’t a scandal: this is how Facebook works</title>
<link>https://www.independent.co.uk/voices/camridge-analytica-scandal-how-facebook-works-harvesting-data-politics-trump-brexit-a8264051.html</link>
<guid isPermaLink="true" >https://www.independent.co.uk/voices/camridge-analytica-scandal-how-facebook-works-harvesting-data-politics-trump-brexit-a8264051.html</guid>
<description>&lt;p&gt;It is easy to be misled into believing that the &lt;a href=&quot;https://www.independent.co.uk/news/world/americas/facebook-cambridge-analytica-millions-users-data-massachusetts-investigation-maura-healy-trump-a8261526.html&quot;&gt;Cambridge Analytica&lt;/a&gt; story is about rogue data miners taking advantage of an innocent Facebook. Facebook’s decision to suspend Cambridge Analytica’s access, the use of terms like “data breach”, and a good deal of coverage in the media seems to follow these lines. That, however, misses the key point. This is not a data breach by any means – and nor is it something that could not have been predicted or could easily have been avoided. This is, in many ways, Cambridge Analytica using Facebook exactly as the social media platform was designed to be used. This is how Facebook works.&lt;/p&gt;
&lt;p&gt;Three key parts of &lt;a href=&quot;https://www.independent.co.uk/topic/Facebook&quot;&gt;Facebook’s&lt;/a&gt; model come into play: gathering data from people in order to profile them, both &lt;em&gt;en masse&lt;/em&gt; and individually, designing systems that allow that data to be used to target people for advertising and content, then allowing third parties (generally advertisers) to use the data and those targeting systems for their own purposes. The power of these systems is often underestimated, but Facebook themselves know it, and have tested it in a number of ways.&lt;/p&gt;
&lt;p&gt;They have demonstrated, through their “emotional contagion” experiment in 2014, that they can make people happier or sadder, simply by manipulating the order things appear in people’s timelines. They have demonstrated that they can make people more likely to vote, testing it in the 2010 US congressional elections. They can profile people based on the most mundane of information – the sheer scale of Facebook’s user-base and the amount of information given to them means that “big data” analysis can make connections that might seem bizarre, revealing insights into intelligence, politics, ethnicity and religion without people actually discussing any of those things directly.&lt;/p&gt;
&lt;p&gt;They allow advertisements to be targeted to particular “racial affinity” groups – or tailored according to “racial affinity”. Not actual race, because that might conflict with various laws, but the race that your profile suggests you have the most “affinity” towards. Racial profiling without the name.&lt;/p&gt;
&lt;div class=&quot;dnd-widget-wrapper context-sdl_editor_representation type-video&quot;&gt;

&lt;div class=&quot;dnd-caption-wrapper&quot;&gt;Cambridge &lt;span class=&quot;scayt-misspell-word&quot; data-scayt-lang=&quot;en_GB&quot; data-scayt-word=&quot;Analytica&quot;&gt;Analytica&lt;/span&gt;: Chris Wylie tells Channel 4 News data for 50 million Facebook profiles was obtained&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This all seems relatively harmless when it is just restricted to advertising for products – it might be a bit creepy to find carefully targeted advertisements for holidays in places you like or musicians you admire – but a few changes in parameters for targeting change things completely. The benefits of this profiling for electoral purposes are huge. Profiling the electorate has long been a part of political campaigning, but this makes it much more detailed, much more surreptitious, and much more effective.&lt;/p&gt;
&lt;p&gt;Parties can target and influence voters directly; make their own supporters happier and opponents’ supporters sadder; make their own more likely to vote. They can spread stories tailored to individuals’ views, focussing on the aspects of a campaign that they know the individual cares about – both positively and negatively. When you add “fake news” to this, the situation becomes even worse.&lt;/p&gt;
&lt;p&gt;That is the real point here. When thought about in terms of profiling and micro-targeting advertising for products, this just sounds efficient and appropriate, and harmless. It is a tiny shift, however to take this into politics – and a shift that groups like Cambridge Analytica found easy to do. All they did was understand how Facebook works, and use it. On a big scale, and in a big way, but this is how Facebook works. Profiling, targeting, persuasive manipulation are the tools of the advertiser on steroids, provably effective and available to those with the money and intent to use them. Unless Facebook changes its entire business model, it will be used in ways that interfere with our lives – and in particular that interferes with our politics.&lt;/p&gt;
&lt;p&gt;What is more, it is only going to get &lt;em&gt;more&lt;/em&gt; effective. Facebook is gathering more data all the time – including through its associated operations in Instagram, WhatsApp and more. Its analyses are being refined and becoming more effective all the time – and more people like Cambridge Analytica are becoming aware of the possibilities and how they might be used.&lt;/p&gt;
&lt;p&gt;How it might be stopped, or at least slowed down, is another matter. This is all based on the fundamental operations of Facebook, so while we rely on Facebook, it is hard to see a way out. By choosing to let ourselves become dependent, we have built this trap for ourselves. Until we find our way out of Facebook, more of this is inevitable.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Paul Bernal is Senior Lecturer in IT, IP and Media Law, UEA Law School&lt;/em&gt;&lt;/p&gt;

&lt;a href=&quot;/syndication/reuse-permission-form?url=http://www.independent.co.uk/voices/camridge-analytica-scandal-how-facebook-works-harvesting-data-politics-trump-brexit-a8264051.html&quot; target=&quot;_blank&quot; class=&quot;syndication-btn&quot;&gt;&lt;img src=&quot;https://www.independent.co.uk/sites/all/themes/ines_themes/independent_theme/img/reuse.png&quot; width=&quot;25&quot;/&gt;Reuse content&lt;/a&gt;</description>
<pubDate>Mon, 19 Mar 2018 18:58:55 +0000</pubDate>
<dc:creator>auxbuss</dc:creator>
<og:type>article</og:type>
<og:title>Cambridge Analytica: This isn't a scandal, it's how Facebook works</og:title>
<og:url>http://www.independent.co.uk/voices/camridge-analytica-scandal-how-facebook-works-harvesting-data-politics-trump-brexit-a8264051.html</og:url>
<og:description>It is easy to be misled into believing that the Cambridge Analytica story is about rogue data miners taking advantage of an innocent Facebook. Facebook’s decision to suspend Cambridge Analytica’s access, the use of terms like “data breach”, and a good deal of coverage in the media seems to follow these lines. That, however, misses the key point.</og:description>
<og:image>https://static.independent.co.uk/s3fs-public/thumbnails/image/2018/02/28/11/facebook-democracy.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.independent.co.uk/voices/camridge-analytica-scandal-how-facebook-works-harvesting-data-politics-trump-brexit-a8264051.html</dc:identifier>
</item>
<item>
<title>LG launches open-source version of webOS</title>
<link>https://sdtimes.com/os/lg-launches-open-source-version-webos/</link>
<guid isPermaLink="true" >https://sdtimes.com/os/lg-launches-open-source-version-webos/</guid>
<description>&lt;img width=&quot;250&quot; height=&quot;250&quot; src=&quot;https://sdtimes.com/wp-content/uploads/2018/03/A.jpg&quot; class=&quot;alignright size-medium wp-post-image&quot; alt=&quot;&quot; srcset=&quot;https://sdtimes.com/wp-content/uploads/2018/03/A.jpg 250w, https://sdtimes.com/wp-content/uploads/2018/03/A-150x150.jpg 150w, https://sdtimes.com/wp-content/uploads/2018/03/A-80x80.jpg 80w, https://sdtimes.com/wp-content/uploads/2018/03/A-180x180.jpg 180w, https://sdtimes.com/wp-content/uploads/2018/03/A-50x50.jpg 50w, https://sdtimes.com/wp-content/uploads/2018/03/A-32x32.jpg 32w, https://sdtimes.com/wp-content/uploads/2018/03/A-64x64.jpg 64w, https://sdtimes.com/wp-content/uploads/2018/03/A-96x96.jpg 96w, https://sdtimes.com/wp-content/uploads/2018/03/A-128x128.jpg 128w&quot; sizes=&quot;(max-width: 250px) 100vw, 250px&quot;/&gt;&lt;p&gt;&lt;span&gt;LG Electronics is moving webOS beyond TVs with the release of webOS Open Source Edition. WebOS is a multitasking operating system that was designed for smart devices and smart TVs.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Before coming to LG, webOS was launched as Palm OS in 2009. It was acquired by HP in 2010, and then licensed to LG in 2013. Since then, the company has been using the technology for its smart TVs and refrigerators.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;“WebOS has come a long way since then and is now a mature and stable platform ready to move beyond TVs to join the very exclusive group of operating systems that have been successfully commercialization at such a mass level. As we move from an app-based environment to a web-based one, we believe the true potential of webOS has yet to be seen,” said I.P. Park, chief technology officer at LG Electronics.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;With the webOS Open Source Edition, developers can download the source code and take advantage of related tools, guides and forums. According to the company, new developers will be able to easily pick up on webOS because it is a Linux-based multitasking OS with support for both HTML5 and CSS3.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;“Web developers can easily build apps and services for webOS OSE. With the initial release, you can port webOS OSE to Raspberry Pi 3 for use in your own projects. It will be extended in the coming future to be used for other kinds of devices with various form factors,” the company&lt;/span&gt; &lt;a href=&quot;http://webosose.org/discover/webos-ose-overview/&quot;&gt;&lt;span&gt;wrote&lt;/span&gt;&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Other features include ability to develop and test apps without a specific IDE with the CLI, the LSP2 API for sending and receiving data in a single device, and a developer SDK.   &lt;/span&gt;&lt;/p&gt;

</description>
<pubDate>Mon, 19 Mar 2018 18:25:12 +0000</pubDate>
<dc:creator>spacemanspiffy</dc:creator>
<og:type>article</og:type>
<og:title>LG launches open source version of webOS - SD Times</og:title>
<og:description>LG launches webOS Open Source Edition to assist with its goals of extending the webOS global footprint</og:description>
<og:url>https://sdtimes.com/os/lg-launches-open-source-version-webos/</og:url>
<og:image>https://sdtimes.com/wp-content/uploads/2018/03/A.jpg</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://sdtimes.com/os/lg-launches-open-source-version-webos/</dc:identifier>
</item>
<item>
<title>Announcing Microsoft DirectX Raytracing</title>
<link>https://blogs.msdn.microsoft.com/directx/2018/03/19/announcing-microsoft-directx-raytracing/</link>
<guid isPermaLink="true" >https://blogs.msdn.microsoft.com/directx/2018/03/19/announcing-microsoft-directx-raytracing/</guid>
<description>&lt;h3&gt;&lt;strong&gt;3D Graphics is a Lie&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;For the last thirty years, almost all games have used the same general technique—rasterization—to render images on screen.  While the internal representation of the game world is maintained as three dimensions, rasterization ultimately operates in two dimensions (the plane of the screen), with 3D primitives mapped onto it through transformation matrices.  Through approaches like z-buffering and occlusion culling, games have historically strived to minimize the number of spurious pixels rendered, as normally they do not contribute to the final frame.  And in a perfect world, the pixels rendered would be exactly those that are directly visible from the camera:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig1a.png&quot;&gt;&lt;img width=&quot;362&quot; height=&quot;636&quot; class=&quot;size-full wp-image-1065 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig1a.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1a: a top-down illustration of various pixel reduction techniques. Top to bottom: no culling, view frustum culling, viewport clipping&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig1b.png&quot;&gt;&lt;img width=&quot;382&quot; height=&quot;452&quot; class=&quot;size-full wp-image-1115 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig1b.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1b: back-face culling, z-buffering&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Through the first few years of the new millennium, this approach was sufficient.  Normal and parallax mapping continued to add layers of realism to 3D games, and GPUs provided the ongoing improvements to bandwidth and processing power needed to deliver them.  It wasn’t long, however, until games began using techniques that were incompatible with these optimizations.  Shadow mapping allowed off-screen objects to contribute to on-screen pixels, and environment mapping required a complete spherical representation of the world.  Today, techniques such as screen-space reflection and global illumination are pushing rasterization to its limits, with SSR, for example, being solved with level design tricks, and GI being solved in some cases by processing a full 3D representation of the world using async compute.  In the future, the utilization of full-world 3D data for rendering techniques will only increase.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig-2.png&quot;&gt;&lt;img width=&quot;422&quot; height=&quot;358&quot; class=&quot;size-full wp-image-1095 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig-2.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 2: a top-down view showing how shadow mapping can allow even culled geometry to contribute to on-screen shadows in a scene&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Today, we are introducing a feature to DirectX 12 that will bridge the gap between the rasterization techniques employed by games today, and the full 3D effects of tomorrow.  This feature is DirectX Raytracing.  By allowing traversal of a full 3D representation of the game world, DirectX Raytracing allows current rendering techniques such as SSR to naturally and efficiently fill the gaps left by rasterization, and opens the door to an entirely new class of techniques that have never been achieved in a real-time game. Readers unfamiliar with rasterization and raytracing will find more information about the basics of these concepts in the appendix below.&lt;/p&gt;

&lt;h4&gt;&lt;strong&gt;What is DirectX Raytracing?&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;At the highest level, DirectX Raytracing (DXR) introduces four, new concepts to the DirectX 12 API:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;The &lt;em&gt;acceleration structure&lt;/em&gt; is an object that represents a full 3D environment in a format optimal for traversal by the GPU.  Represented as a two-level hierarchy, the structure affords both optimized ray traversal by the GPU, as well as efficient modification by the application for dynamic objects.&lt;/li&gt;
&lt;li&gt;A new command list method, &lt;em&gt;DispatchRays&lt;/em&gt;, which is the starting point for tracing rays into the scene.  This is how the game actually submits DXR workloads to the GPU.&lt;/li&gt;
&lt;li&gt;A set of new HLSL shader types including &lt;em&gt;ray-generation&lt;/em&gt;, &lt;em&gt;closest-hit&lt;/em&gt;, &lt;em&gt;any-hit&lt;/em&gt;, and &lt;em&gt;miss&lt;/em&gt; shaders.  These specify what the DXR workload actually does computationally.  When DispatchRays is called, the ray-generation shader runs.  Using the new &lt;em&gt;TraceRay&lt;/em&gt; intrinsic function in HLSL, the ray generation shader causes rays to be traced into the scene.  Depending on where the ray goes in the scene, one of several hit or miss shaders may be invoked at the point of intersection.  This allows a game to assign each object its own set of shaders and textures, resulting in a unique material.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;raytracing pipeline state&lt;/em&gt;, a companion in spirit to today’s Graphics and Compute pipeline state objects, encapsulates the raytracing shaders and other state relevant to raytracing workloads.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You may have noticed that DXR does not introduce a new GPU engine to go alongside DX12’s existing Graphics and Compute engines.  This is intentional – DXR workloads can be run on either of DX12’s existing engines.  The primary reason for this is that, fundamentally, DXR is a compute-like workload. It does not require complex state such as output merger blend modes or input assembler vertex layouts.  A secondary reason, however, is that representing DXR as a compute-like workload is aligned to what we see as the future of graphics, namely that hardware will be increasingly general-purpose, and eventually most fixed-function units will be replaced by HLSL code.  The design of the raytracing pipeline state exemplifies this shift through its name and design in the API. With DX12, the traditional approach would have been to create a new CreateRaytracingPipelineState method.  Instead, we decided to go with a much more generic and flexible &lt;em&gt;CreateStateObject&lt;/em&gt; method.  It is designed to be adaptable so that in addition to Raytracing, it can eventually be used to create Graphics and Compute pipeline states, as well as any future pipeline designs.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Anatomy of a DXR Frame&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The first step in rendering any content using DXR is to build the acceleration structures, which operate in a two-level hierarchy.  At the bottom level of the structure, the application specifies a set of &lt;em&gt;geometries&lt;/em&gt;, essentially vertex and index buffers representing distinct objects in the world.  At the top level of the structure, the application specifies a list of instance descriptions containing references to a particular geometry, and some additional per-instance data such as transformation matrices, that can be updated from frame to frame in ways similar to how games perform dynamic object updates today.  Together, these allow for efficient traversal of multiple complex geometries.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig3.png&quot;&gt;&lt;img width=&quot;404&quot; height=&quot;276&quot; class=&quot;size-full wp-image-1125 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig3.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 3: Instances of 2 geometries, each with its own transformation matrix&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The second step in using DXR is to create the raytracing pipeline state.  Today, most games batch their draw calls together for efficiency, for example rendering all metallic objects first, and all plastic objects second.  But because it’s impossible to predict exactly what material a particular ray will hit, batching like this isn’t possible with raytracing.  Instead, the raytracing pipeline state allows specification of multiple sets of raytracing shaders and texture resources.  Ultimately, this allows an application to specify, for example, that any ray intersections with object A should use shader P and texture X, while intersections with object B should use shader Q and texture Y.  This allows applications to have ray intersections run the correct shader code with the correct textures for the materials they hit.&lt;/p&gt;
&lt;p&gt;The third and final step in using DXR is to call DispatchRays, which invokes the ray generation shader.  Within this shader, the application makes calls to the TraceRay intrinsic, which triggers traversal of the acceleration structure, and eventual execution of the appropriate hit or miss shader.  In addition, TraceRay can also be called from within hit and miss shaders, allowing for ray recursion or “multi-bounce” effects.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig-4.png&quot;&gt;&lt;br/&gt;&lt;img width=&quot;434&quot; height=&quot;298&quot; class=&quot;size-full wp-image-1105 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig-4.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4: an illustration of ray recursion in a scene&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Note that because the raytracing pipeline omits many of the fixed-function units of the graphics pipeline such as the input assembler and output merger, it is up to the application to specify how geometry is interpreted.  Shaders are given the minimum set of attributes required to do this, namely the intersection point’s &lt;a href=&quot;https://en.wikipedia.org/wiki/Barycentric_coordinate_system&quot;&gt;barycentric coordinates&lt;/a&gt; within the primitive.  Ultimately, this flexibility is a significant benefit of DXR; the design allows for a huge variety of techniques without the overhead of mandating particular formats or constructs.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;PIX for Windows Support Available on Day 1&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;As new graphics features put an increasing array of options at the disposal of game developers, the need for great tools becomes increasingly important.  The great news is that PIX for Windows will support the DirectX Raytracing API from day 1 of the API’s release.  PIX on Windows supports capturing and analyzing frames built using DXR to help developers understand how DXR interacts with the hardware. Developers can inspect API calls, view pipeline resources that contribute to the raytracing work, see contents of state objects, and visualize acceleration structures. This provides the information developers need to build great experiences using DXR.&lt;/p&gt;


&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/Multithreading.png&quot;&gt;&lt;img width=&quot;1024&quot; height=&quot;771&quot; class=&quot;size-large wp-image-1235 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/Multithreading-1024x771.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;&lt;strong&gt;What Does This Mean for Games?&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;DXR will initially be used to supplement current rendering techniques such as screen space reflections, for example, to fill in data from geometry that’s either occluded or off-screen.  This will lead to a material increase in visual quality for these effects in the near future.  Over the next several years, however, we expect an increase in utilization of DXR for techniques that are simply impractical for rasterization, such as true global illumination.  Eventually, raytracing may completely replace rasterization as the standard algorithm for rendering 3D scenes.  That said, until everyone has a &lt;a href=&quot;https://en.wikipedia.org/wiki/Light_field&quot;&gt;light-field&lt;/a&gt; display on their desk, rasterization will continue to be an excellent match for the common case of rendering content to a flat grid of square pixels, supplemented by raytracing for true 3D effects.&lt;/p&gt;
&lt;p&gt;Thanks to our friends at &lt;a href=&quot;https://www.ea.com/seed&quot;&gt;SEED, Electronic Arts&lt;/a&gt;, we can show you a &lt;a href=&quot;https://www.youtube.com/watch?v=LXo0WdlELJk&quot;&gt;glimpse of what future gaming scenes could look like&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/SEED-screenshot.png&quot;&gt;&lt;img width=&quot;1024&quot; height=&quot;576&quot; class=&quot;size-large wp-image-1045 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/SEED-screenshot-1024x576.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Project PICA PICA from SEED, Electronic Arts&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In addition, while today marks the first public announcement of DirectX Raytracing, we have been working closely with hardware vendors and industry developers for nearly a year to design and tune the API.  In fact, a significant number of studios and engines are already planning to integrate DXR support into their games and engines, including:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/frostbite.png&quot;&gt;&lt;img width=&quot;652&quot; height=&quot;367&quot; class=&quot;size-full wp-image-1245 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/frostbite.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Electronic Arts, Frostbite&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/seed-working.jpg&quot;&gt;&lt;img width=&quot;381&quot; height=&quot;381&quot; class=&quot; wp-image-1285 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/seed-working.jpg&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Electronic Arts,  SEED&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/Unreal_Engine_Black.png&quot;&gt;&lt;img width=&quot;341&quot; height=&quot;388&quot; class=&quot;wp-image-1265 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/Unreal_Engine_Black-900x1024.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Epic Games, Unreal Engine&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/3dmarkworking.png&quot;&gt;&lt;img width=&quot;696&quot; height=&quot;145&quot; class=&quot;wp-image-1275 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/3dmarkworking.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Futuremark, 3DMark&lt;/em&gt;&lt;/p&gt;


&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/unitylogo.jpg&quot;&gt;&lt;img width=&quot;448&quot; height=&quot;182&quot; class=&quot; wp-image-1155 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/unitylogo.jpg&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Unity Technologies, Unity Engine&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;And more will be coming soon.&lt;/p&gt;

&lt;h4&gt;&lt;strong&gt;What Hardware Will DXR Run On?&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Developers can use currently in-market hardware to get started on DirectX Raytracing.  There is also a fallback layer which will allow developers to start experimenting with DirectX Raytracing that does not require any specific hardware support.  For hardware roadmap support for DirectX Raytracing, please contact hardware vendors directly for further details.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Available now for experimentation!&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Want to be one of the first to bring real-time raytracing to your game?  Start by attending our &lt;a href=&quot;http://schedule.gdconf.com/session/directx-evolving-microsofts-graphics-platform-presented-by-microsoft/856594&quot;&gt;Game Developer Conference Session on DirectX Raytracing &lt;/a&gt;for all the technical details you need to begin, then download the &lt;a href=&quot;http://aka.ms/dxrsdk&quot;&gt;Experimental DXR SDK&lt;/a&gt; and start coding!  Not attending GDC?  No problem!  We'll be posting the slides on this blog after the session.&lt;/p&gt;






&lt;h4&gt;&lt;strong&gt;Appendix – Primers on rasterization, raytracing and DirectX Raytracing&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intro to Rasterization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Of all the rendering algorithms out there, by far the most widely used is &lt;strong&gt;rasterization&lt;/strong&gt;. Rasterization has been around since the 90s and has since become the dominant rendering technique in video games. This is with good reason: it’s incredibly efficient and can produce high levels of visual realism.&lt;/p&gt;

&lt;p&gt;Rasterization is an algorithm that in a sense doesn’t do all its work in 3D. This is because rasterization has a step where 3D objects get projected onto your 2D monitor, before they are colored in. This work can be done efficiently by GPUs because it’s work that can be done in parallel: the work needed to color in one pixel on the 2D screen can be done independently of the work needed to color one the pixel next to it.&lt;/p&gt;

&lt;p&gt;There’s a problem with this: in the real world the color of one object will have an impact on the objects around it, because of the complicated interplay of light.  This means that developers must resort to a wide variety of clever techniques to simulate the visual effects that are normally caused by light scattering, reflecting and refracting off objects in the real world. The shadows, reflections and indirect lighting in games are made with these techniques.&lt;/p&gt;

&lt;p&gt;Games rendered with rasterization can look and feel incredibly lifelike, because developers have gotten extremely good at making it look as if their worlds have light that acts in convincing way. Having said that, it takes an incredible deal of technical expertise to do this well and there’s also an upper limit to how realistic a rasterized game can get, since information about 3D objects gets lost every time they get projected onto your 2D screen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intro to Raytracing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Raytracing calculates the color of pixels by tracing the path of light that would have created it and simulates this ray of light’s interactions with objects in the virtual world. Raytracing therefore calculates what a pixel would look like if a virtual world had real light. The beauty of raytracing is that it preserves the 3D world and visual effects like shadows, reflections and indirect lighting are a natural consequence of the raytracing algorithm, not special effects.&lt;/p&gt;

&lt;p&gt;Raytracing can be used to calculate the color of every single pixel on your screen, or it can be used for only some pixels, such as those on reflective surfaces.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How does it work?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A ray gets sent out for each pixel in question. The algorithm works out which object gets hit first by the ray and the exact point at which the ray hits the object. This point is called the first point of intersection and the algorithm does two things here: 1) it estimates the incoming light at the point of intersection and 2) combines this information about the incoming light with information about the object that was hit.&lt;/p&gt;

&lt;p&gt;1)      To estimate what the incoming light looked like at the first point of intersection, the algorithm needs to consider where this light was reflected or refracted from.&lt;/p&gt;
&lt;p&gt;2)      Specific information about each object is important because objects don’t all have the same properties: they absorb, reflect and refract light in different ways:&lt;/p&gt;
&lt;p&gt;-          different ways of absorption are what cause objects to have different colors (for example, a leaf is green because it absorbs all but green light)&lt;/p&gt;
&lt;p&gt;-          different rates of reflection are what cause some objects to give off mirror-like reflections and other objects to scatter rays in all directions&lt;/p&gt;
&lt;p&gt;-          different rates of refraction are what cause some objects (like water) to distort light more than other objects.&lt;/p&gt;
&lt;p&gt;Often to estimate the incoming light at the first point of intersection, the algorithm must trace that light to a second point of intersection (because the light hitting an object might have been reflected off another object), or even further back.&lt;/p&gt;

&lt;p&gt;Savvy readers with some programming knowledge might notice some edge cases here.&lt;/p&gt;

&lt;p&gt;Sometimes light rays that get sent out never hit anything. Don’t worry, this is an edge case we can cover easily by measuring for how far a ray has travelled so that we can do additional work on rays that have travelled for too far.&lt;/p&gt;

&lt;p&gt;The second edge case covers the opposite situation: light might bounce around so much that it’ll slow down the algorithm, or an infinite number of times, causing an infinite loop. The algorithm keeps track of how many times a ray gets traced after every step and gets terminated after a certain number of reflections. We can justify doing this because every object in the real world absorbs some light, even mirrors. This means that a light ray loses energy (becomes fainter) every time it’s reflected, until it becomes too faint to notice. So even if we could, tracing a ray an arbitrary number of times doesn’t make sense.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is the state of raytracing today?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Raytracing a technique that’s been around for decades. It’s used quite often to do CGI in films and several games already use forms of raytracing. For example, developers might use offline raytracing to do things like pre-calculating the brightness of virtual objects before shipping their games.&lt;/p&gt;

&lt;p&gt;No games currently use real-time raytracing, but we think that this will change soon: over the past few years, computer hardware has become more and more flexible: even with the same TFLOPs, a GPU can do more.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How does this fit into DirectX?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We believe that DirectX Raytracing will bring raytracing within reach of real-time use cases, since it comes with dedicated hardware acceleration and can be integrated seamlessly with existing DirectX 12 content.&lt;/p&gt;

&lt;p&gt;This means that it’s now possible for developers to build games that use rasterization for some of its rendering and raytracing to be used for the rest. For example, developers can build a game where much of the content is generated with rasterization, but DirectX Raytracing calculates the shadows or reflections, helping out in areas where rasterization is lacking.&lt;/p&gt;

&lt;p&gt;This is the power of DirectX Raytracing: it lets developers have their cake and eat it.&lt;/p&gt;
</description>
<pubDate>Mon, 19 Mar 2018 17:26:02 +0000</pubDate>
<dc:creator>nnx</dc:creator>
<og:type>article</og:type>
<og:title>Announcing Microsoft DirectX Raytracing!</og:title>
<og:url>https://blogs.msdn.microsoft.com/directx/?p=975</og:url>
<og:description>3D Graphics is a Lie For the last thirty years, almost all games have used the same general technique—rasterization—to render images on screen.  While the internal representation of the game world is maintained as three dimensions, rasterization ultimately operates in two dimensions (the plane of the screen), with 3D primitives mapped onto it through transformation...</og:description>
<og:image>https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig1a.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://blogs.msdn.microsoft.com/directx/2018/03/19/announcing-microsoft-directx-raytracing/</dc:identifier>
</item>
<item>
<title>Facebook suspended the account of whistleblower who exposed Cambridge Analytica</title>
<link>https://www.yahoo.com/news/facebook-suspended-account-whistleblower-exposed-221429183.html</link>
<guid isPermaLink="true" >https://www.yahoo.com/news/facebook-suspended-account-whistleblower-exposed-221429183.html</guid>
<description>&lt;div class=&quot;Maw(100%) Pos(r) H(0)&quot; data-reactid=&quot;7&quot;&gt;&lt;img alt=&quot;Facebook has suspended the account of the whistleblower who exposed Cambridge Analytica&quot; class=&quot; StretchedBox W(100%) H(100%) ie-7_H(a)&quot; itemprop=&quot;url&quot; src=&quot;https://s.yimg.com/ny/api/res/1.2/3ECjwE_6jNJoiUi.fOWcGg--/YXBwaWQ9aGlnaGxhbmRlcjtzbT0xO3c9ODAw/http://media.zenfs.com/en-US/homerun/techcrunch_350/4445a4efdd593f5d4b2c45e8ef00abfa&quot; data-reactid=&quot;8&quot;/&gt;&lt;noscript data-reactid=&quot;9&quot;&gt;
&lt;p&gt;&lt;img alt=&quot;Facebook has suspended the account of the whistleblower who exposed Cambridge Analytica&quot; class=&quot;StretchedBox W(100%) H(100%) ie-7_H(a)&quot; src=&quot;https://s.yimg.com/ny/api/res/1.2/3ECjwE_6jNJoiUi.fOWcGg--/YXBwaWQ9aGlnaGxhbmRlcjtzbT0xO3c9ODAw/http://media.zenfs.com/en-US/homerun/techcrunch_350/4445a4efdd593f5d4b2c45e8ef00abfa&quot; itemprop=&quot;url&quot;/&gt;&lt;/p&gt;
&lt;/noscript&gt;&lt;/div&gt;&lt;div itemprop=&quot;caption description&quot; class=&quot;Ov(h) Pos(r) Ff(ss) Mah(80px)&quot; data-reactid=&quot;10&quot;&gt;
&lt;div class=&quot;figure-caption&quot; data-reactid=&quot;12&quot;&gt;Tech hath no fury like a multi-billion dollar social media giant scorned.&lt;/div&gt;
&lt;button class=&quot;C($c-fuji-blue-1-b) Cur(p) W(100%) T(63px) Bgc(#fff) Ta(start) Fz(13px) P(0) Bd(0) O(0) Lh(1.5) Pos(a)&quot; data-reactid=&quot;13&quot;&gt;&lt;span data-reactid=&quot;14&quot;&gt;More&lt;/span&gt;&lt;/button&gt;&lt;/div&gt;
&lt;div class=&quot;canvas-body Wow(bw) Cl(start) Mb(20px) Lh(30px) Fz(18px) Ff(s) C(#000) D(i)&quot; data-reactid=&quot;15&quot;&gt;
&lt;p class=&quot;canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm&quot; type=&quot;text&quot; content=&quot;Tech hath no fury like a multi-billion dollar social media giant scorned.&quot; data-reactid=&quot;16&quot;&gt;Tech hath no fury like a multi-billion dollar social media giant scorned.&lt;/p&gt;
&lt;p class=&quot;canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm&quot; type=&quot;text&quot; content=&quot;&amp;lt;a href=&amp;quot;https://techcrunch.com/2018/03/17/trump-campaign-linked-data-firm-cambridge-analytica-reportedly-collected-info-on-50m-facebook-profiles/&amp;quot; rel=&amp;quot;nofollow noopener&amp;quot; target=&amp;quot;_blank&amp;quot;&amp;gt;In the latest turn of the developing scandal&amp;lt;/a&amp;gt; around how &amp;lt;a class=&amp;quot;crunchbase-link&amp;quot; href=&amp;quot;https://www.crunchbase.com/organization/facebook/&amp;quot; target=&amp;quot;_blank&amp;quot; rel=&amp;quot;nofollow noopener&amp;quot;&amp;gt;Facebook's &amp;lt;/a&amp;gt; user data wound up in the hands of Cambridge Analytica -- for use in the in development in psychographic profiles that may or may not have played a part in the election victory of Donald Trump -- the company has taken the unusual step of suspending the account of the whistleblower who helped expose the issues.&quot; data-reactid=&quot;17&quot;&gt;&lt;a href=&quot;https://techcrunch.com/2018/03/17/trump-campaign-linked-data-firm-cambridge-analytica-reportedly-collected-info-on-50m-facebook-profiles/&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;In the latest turn of the developing scandal&lt;/a&gt; around how &lt;a class=&quot;crunchbase-link&quot; href=&quot;https://www.crunchbase.com/organization/facebook/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Facebook's&lt;/a&gt; user data wound up in the hands of Cambridge Analytica -- for use in the in development in psychographic profiles that may or may not have played a part in the election victory of Donald Trump -- the company has taken the unusual step of suspending the account of the whistleblower who helped expose the issues.&lt;/p&gt;

&lt;p class=&quot;canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm&quot; type=&quot;text&quot; content=&quot;&amp;lt;a href=&amp;quot;https://platform.twitter.com/widgets.js&amp;quot; rel=&amp;quot;nofollow noopener&amp;quot; target=&amp;quot;_blank&amp;quot;&amp;gt;https://platform.twitter.com/widgets.js&amp;lt;/a&amp;gt;&quot; data-reactid=&quot;19&quot;&gt;&lt;a href=&quot;https://platform.twitter.com/widgets.js&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;https://platform.twitter.com/widgets.js&lt;/a&gt;&lt;/p&gt;
&lt;p class=&quot;canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm&quot; type=&quot;text&quot; content=&quot;In &amp;lt;a href=&amp;quot;https://www.theguardian.com/news/2018/mar/17/data-war-whistleblower-christopher-wylie-faceook-nix-bannon-trump&amp;quot; rel=&amp;quot;nofollow noopener&amp;quot; target=&amp;quot;_blank&amp;quot;&amp;gt;a fantastic profile in&amp;amp;nbsp;&amp;lt;em&amp;gt;The Guardian&amp;lt;/em&amp;gt;&amp;lt;/a&amp;gt;, Wylie revealed himself to be the architect of the technology that Cambridge Analytica used to develop targeted advertising strategies that arguably helped sway the U.S. presidential election.&quot; data-reactid=&quot;20&quot;&gt;In &lt;a href=&quot;https://www.theguardian.com/news/2018/mar/17/data-war-whistleblower-christopher-wylie-faceook-nix-bannon-trump&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;a fantastic profile in &lt;em&gt;The Guardian&lt;/em&gt;&lt;/a&gt;, Wylie revealed himself to be the architect of the technology that Cambridge Analytica used to develop targeted advertising strategies that arguably helped sway the U.S. presidential election.&lt;/p&gt;
&lt;p class=&quot;canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm&quot; type=&quot;text&quot; content=&quot;A self-described gay, Canadian vegan, Wylie eventually became -- as he told The Guardian -- the developer of “&amp;lt;a class=&amp;quot;u-underline in-body-link--immersive&amp;quot; href=&amp;quot;https://www.theguardian.com/us-news/steve-bannon&amp;quot; rel=&amp;quot;nofollow noopener&amp;quot; target=&amp;quot;_blank&amp;quot;&amp;gt;Steve Bannon&amp;lt;/a&amp;gt;’s psychological warfare mindfuck tool.&amp;quot;&quot; data-reactid=&quot;21&quot;&gt;A self-described gay, Canadian vegan, Wylie eventually became -- as he told The Guardian -- the developer of “&lt;a class=&quot;u-underline in-body-link--immersive&quot; href=&quot;https://www.theguardian.com/us-news/steve-bannon&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Steve Bannon&lt;/a&gt;’s psychological warfare mindfuck tool.&quot;&lt;/p&gt;
&lt;p class=&quot;canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm&quot; type=&quot;text&quot; content=&quot;The goal, as&amp;amp;nbsp;&amp;lt;em&amp;gt;The Guardian&amp;amp;nbsp;&amp;lt;/em&amp;gt;reported, was to combine social media's reach with big data analytical tools to create psychographic profiles that could then be manipulated in what Bannon and &amp;lt;a class=&amp;quot;crunchbase-link&amp;quot; href=&amp;quot;https://www.crunchbase.com/organization/cambridge-analytica/&amp;quot; target=&amp;quot;_blank&amp;quot; rel=&amp;quot;nofollow noopener&amp;quot;&amp;gt;Cambridge Analytica &amp;lt;/a&amp;gt; investor Robert Mercer allegedly referred to as a military-style psychological operations campaign -- targeting U.S. voters.&quot; data-reactid=&quot;22&quot;&gt;The goal, as &lt;em&gt;The Guardian &lt;/em&gt;reported, was to combine social media's reach with big data analytical tools to create psychographic profiles that could then be manipulated in what Bannon and &lt;a class=&quot;crunchbase-link&quot; href=&quot;https://www.crunchbase.com/organization/cambridge-analytica/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Cambridge Analytica&lt;/a&gt; investor Robert Mercer allegedly referred to as a military-style psychological operations campaign -- targeting U.S. voters.&lt;/p&gt;
&lt;p class=&quot;canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm&quot; type=&quot;text&quot; content=&quot;In a series of Tweets late Saturday, Wylie's former employer, Cambridge Analytica, took issue with Wylie's characterization of events (and much of the reporting around the stories from&amp;amp;nbsp;&amp;lt;em&amp;gt;The Times&amp;lt;/em&amp;gt; and &amp;lt;em&amp;gt;The Guardian).&amp;amp;nbsp;&amp;lt;/em&amp;gt;&quot; data-reactid=&quot;23&quot;&gt;In a series of Tweets late Saturday, Wylie's former employer, Cambridge Analytica, took issue with Wylie's characterization of events (and much of the reporting around the stories from &lt;em&gt;The Times&lt;/em&gt; and &lt;em&gt;The Guardian). &lt;/em&gt;&lt;/p&gt;

&lt;p class=&quot;canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm&quot; type=&quot;text&quot; content=&quot;&amp;lt;a href=&amp;quot;https://platform.twitter.com/widgets.js&amp;quot; rel=&amp;quot;nofollow noopener&amp;quot; target=&amp;quot;_blank&amp;quot;&amp;gt;https://platform.twitter.com/widgets.js&amp;lt;/a&amp;gt;&quot; data-reactid=&quot;25&quot;&gt;&lt;a href=&quot;https://platform.twitter.com/widgets.js&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;https://platform.twitter.com/widgets.js&lt;/a&gt;&lt;/p&gt;
&lt;p class=&quot;canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm&quot; type=&quot;text&quot; content=&quot;Meanwhile, Cadwalldr noted on Twitter earlier today she'd received a phone call from the aggrieved whistleblower.&quot; data-reactid=&quot;26&quot;&gt;Meanwhile, Cadwalldr noted on Twitter earlier today she'd received a phone call from the aggrieved whistleblower.&lt;/p&gt;
&lt;div class=&quot;canvas-tweet canvas-atom Ov(a) W(100%)&quot; data-type=&quot;tweet&quot; data-reactid=&quot;27&quot;&gt;
&lt;blockquote class=&quot;twitter-tweet&quot;&gt;
&lt;p dir=&quot;ltr&quot; lang=&quot;en&quot;&gt;Plaintive phone call from Chris: he's also banned from WhatsApp.&lt;br/&gt;And - outraged voice! - Instagram.&lt;br/&gt;&quot;But how am I going to curate my online identity?&quot; he says.&lt;br/&gt;The Millennials' first great whistleblower? And &lt;a href=&quot;https://twitter.com/facebook?ref_src=twsrc%5Etfw&quot;&gt;@facebook&lt;/a&gt; hitting him where it hurts &lt;a href=&quot;https://t.co/abjfh4td4g&quot;&gt;https://t.co/abjfh4td4g&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;— Carole Cadwalladr (@carolecadwalla) &lt;a href=&quot;https://twitter.com/carolecadwalla/status/975429619317923840?ref_src=twsrc%5Etfw&quot;&gt;March 18, 2018&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p class=&quot;canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm&quot; type=&quot;text&quot; content=&quot;&amp;lt;a href=&amp;quot;https://platform.twitter.com/widgets.js&amp;quot; rel=&amp;quot;nofollow noopener&amp;quot; target=&amp;quot;_blank&amp;quot;&amp;gt;https://platform.twitter.com/widgets.js&amp;lt;/a&amp;gt;&quot; data-reactid=&quot;28&quot;&gt;&lt;a href=&quot;https://platform.twitter.com/widgets.js&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;https://platform.twitter.com/widgets.js&lt;/a&gt;&lt;/p&gt;
&lt;p class=&quot;canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm&quot; type=&quot;text&quot; content=&quot;Not cool, Facebook. Not cool at all.&quot; data-reactid=&quot;29&quot;&gt;Not cool, Facebook. Not cool at all.&lt;/p&gt;

&lt;p class=&quot;canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm&quot; type=&quot;text&quot; content=&quot;&amp;lt;span&amp;gt;&amp;lt;em&amp;gt;For more &amp;lt;/em&amp;gt;&amp;lt;a href=&amp;quot;https://view.yahoo.com/browse/tv/genre/news-and-information?utm_campaign=tout&amp;amp;amp;utm_medium=referral&amp;amp;amp;utm_source=yahoo/&amp;quot;&amp;gt;&amp;lt;em&amp;gt;news videos&amp;lt;/em&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;em&amp;gt; visit &amp;lt;/em&amp;gt;&amp;lt;a href=&amp;quot;https://view.yahoo.com/?utm_campaign=tout&amp;amp;amp;utm_medium=referral&amp;amp;amp;utm_source=yahoo&amp;quot;&amp;gt;&amp;lt;em&amp;gt;Yahoo View&amp;lt;/em&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;em&amp;gt;. &amp;lt;/em&amp;gt;&amp;lt;/span&amp;gt;&quot; data-reactid=&quot;32&quot;&gt;&lt;span&gt;&lt;em&gt;For more&lt;/em&gt; &lt;a href=&quot;https://view.yahoo.com/browse/tv/genre/news-and-information?utm_campaign=tout&amp;amp;utm_medium=referral&amp;amp;utm_source=yahoo/&quot;&gt;&lt;em&gt;news videos&lt;/em&gt;&lt;/a&gt; &lt;em&gt;visit&lt;/em&gt; &lt;a href=&quot;https://view.yahoo.com/?utm_campaign=tout&amp;amp;utm_medium=referral&amp;amp;utm_source=yahoo&quot;&gt;&lt;em&gt;Yahoo View&lt;/em&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm&quot; type=&quot;text&quot; content=&quot;&amp;amp;nbsp;&quot; data-reactid=&quot;33&quot;&gt; &lt;/p&gt;

&lt;/div&gt;
</description>
<pubDate>Mon, 19 Mar 2018 17:23:09 +0000</pubDate>
<dc:creator>rock57</dc:creator>
<og:type>article</og:type>
<og:description>Tech hath no fury like a multi-billion dollar social media giant scorned.</og:description>
<og:image>https://s.yimg.com/uu/api/res/1.2/PxPlOKD9plJkdNvEEpivPA--~B/aD0xNDQwO3c9MjU2MDtzbT0xO2FwcGlkPXl0YWNoeW9u/http://media.zenfs.com/en-US/homerun/techcrunch_350/4445a4efdd593f5d4b2c45e8ef00abfa</og:image>
<og:title>Facebook has suspended the account of the whistleblower who exposed Cambridge Analytica</og:title>
<og:url>https://www.yahoo.com/news/facebook-suspended-account-whistleblower-exposed-221429183.html</og:url>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.yahoo.com/news/facebook-suspended-account-whistleblower-exposed-221429183.html</dc:identifier>
</item>
<item>
<title>Announcing Microsoft DirectX Raytracing</title>
<link>https://blogs.msdn.microsoft.com/directx/2018/03/19/announcing-microsoft-directx-raytracing/</link>
<guid isPermaLink="true" >https://blogs.msdn.microsoft.com/directx/2018/03/19/announcing-microsoft-directx-raytracing/</guid>
<description>&lt;h3&gt;&lt;strong&gt;3D Graphics is a Lie&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;For the last thirty years, almost all games have used the same general technique—rasterization—to render images on screen.  While the internal representation of the game world is maintained as three dimensions, rasterization ultimately operates in two dimensions (the plane of the screen), with 3D primitives mapped onto it through transformation matrices.  Through approaches like z-buffering and occlusion culling, games have historically strived to minimize the number of spurious pixels rendered, as normally they do not contribute to the final frame.  And in a perfect world, the pixels rendered would be exactly those that are directly visible from the camera:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig1a.png&quot;&gt;&lt;img width=&quot;362&quot; height=&quot;636&quot; class=&quot;size-full wp-image-1065 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig1a.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 1a: a top-down illustration of various pixel reduction techniques. Top to bottom: no culling, view frustum culling, viewport clipping&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig1b.png&quot;&gt;&lt;img width=&quot;382&quot; height=&quot;452&quot; class=&quot;size-full wp-image-1115 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig1b.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1b: back-face culling, z-buffering&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Through the first few years of the new millennium, this approach was sufficient.  Normal and parallax mapping continued to add layers of realism to 3D games, and GPUs provided the ongoing improvements to bandwidth and processing power needed to deliver them.  It wasn’t long, however, until games began using techniques that were incompatible with these optimizations.  Shadow mapping allowed off-screen objects to contribute to on-screen pixels, and environment mapping required a complete spherical representation of the world.  Today, techniques such as screen-space reflection and global illumination are pushing rasterization to its limits, with SSR, for example, being solved with level design tricks, and GI being solved in some cases by processing a full 3D representation of the world using async compute.  In the future, the utilization of full-world 3D data for rendering techniques will only increase.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig-2.png&quot;&gt;&lt;img width=&quot;422&quot; height=&quot;358&quot; class=&quot;size-full wp-image-1095 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig-2.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 2: a top-down view showing how shadow mapping can allow even culled geometry to contribute to on-screen shadows in a scene&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Today, we are introducing a feature to DirectX 12 that will bridge the gap between the rasterization techniques employed by games today, and the full 3D effects of tomorrow.  This feature is DirectX Raytracing.  By allowing traversal of a full 3D representation of the game world, DirectX Raytracing allows current rendering techniques such as SSR to naturally and efficiently fill the gaps left by rasterization, and opens the door to an entirely new class of techniques that have never been achieved in a real-time game. Readers unfamiliar with rasterization and raytracing will find more information about the basics of these concepts in the appendix below.&lt;/p&gt;

&lt;h4&gt;&lt;strong&gt;What is DirectX Raytracing?&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;At the highest level, DirectX Raytracing (DXR) introduces four, new concepts to the DirectX 12 API:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;The &lt;em&gt;acceleration structure&lt;/em&gt; is an object that represents a full 3D environment in a format optimal for traversal by the GPU.  Represented as a two-level hierarchy, the structure affords both optimized ray traversal by the GPU, as well as efficient modification by the application for dynamic objects.&lt;/li&gt;
&lt;li&gt;A new command list method, &lt;em&gt;DispatchRays&lt;/em&gt;, which is the starting point for tracing rays into the scene.  This is how the game actually submits DXR workloads to the GPU.&lt;/li&gt;
&lt;li&gt;A set of new HLSL shader types including &lt;em&gt;ray-generation&lt;/em&gt;, &lt;em&gt;closest-hit&lt;/em&gt;, &lt;em&gt;any-hit&lt;/em&gt;, and &lt;em&gt;miss&lt;/em&gt; shaders.  These specify what the DXR workload actually does computationally.  When DispatchRays is called, the ray-generation shader runs.  Using the new &lt;em&gt;TraceRay&lt;/em&gt; intrinsic function in HLSL, the ray generation shader causes rays to be traced into the scene.  Depending on where the ray goes in the scene, one of several hit or miss shaders may be invoked at the point of intersection.  This allows a game to assign each object its own set of shaders and textures, resulting in a unique material.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;raytracing pipeline state&lt;/em&gt;, a companion in spirit to today’s Graphics and Compute pipeline state objects, encapsulates the raytracing shaders and other state relevant to raytracing workloads.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You may have noticed that DXR does not introduce a new GPU engine to go alongside DX12’s existing Graphics and Compute engines.  This is intentional – DXR workloads can be run on either of DX12’s existing engines.  The primary reason for this is that, fundamentally, DXR is a compute-like workload. It does not require complex state such as output merger blend modes or input assembler vertex layouts.  A secondary reason, however, is that representing DXR as a compute-like workload is aligned to what we see as the future of graphics, namely that hardware will be increasingly general-purpose, and eventually most fixed-function units will be replaced by HLSL code.  The design of the raytracing pipeline state exemplifies this shift through its name and design in the API. With DX12, the traditional approach would have been to create a new CreateRaytracingPipelineState method.  Instead, we decided to go with a much more generic and flexible &lt;em&gt;CreateStateObject&lt;/em&gt; method.  It is designed to be adaptable so that in addition to Raytracing, it can eventually be used to create Graphics and Compute pipeline states, as well as any future pipeline designs.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Anatomy of a DXR Frame&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The first step in rendering any content using DXR is to build the acceleration structures, which operate in a two-level hierarchy.  At the bottom level of the structure, the application specifies a set of &lt;em&gt;geometries&lt;/em&gt;, essentially vertex and index buffers representing distinct objects in the world.  At the top level of the structure, the application specifies a list of instance descriptions containing references to a particular geometry, and some additional per-instance data such as transformation matrices, that can be updated from frame to frame in ways similar to how games perform dynamic object updates today.  Together, these allow for efficient traversal of multiple complex geometries.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig3.png&quot;&gt;&lt;img width=&quot;404&quot; height=&quot;276&quot; class=&quot;size-full wp-image-1125 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig3.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Figure 3: Instances of 2 geometries, each with its own transformation matrix&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The second step in using DXR is to create the raytracing pipeline state.  Today, most games batch their draw calls together for efficiency, for example rendering all metallic objects first, and all plastic objects second.  But because it’s impossible to predict exactly what material a particular ray will hit, batching like this isn’t possible with raytracing.  Instead, the raytracing pipeline state allows specification of multiple sets of raytracing shaders and texture resources.  Ultimately, this allows an application to specify, for example, that any ray intersections with object A should use shader P and texture X, while intersections with object B should use shader Q and texture Y.  This allows applications to have ray intersections run the correct shader code with the correct textures for the materials they hit.&lt;/p&gt;
&lt;p&gt;The third and final step in using DXR is to call DispatchRays, which invokes the ray generation shader.  Within this shader, the application makes calls to the TraceRay intrinsic, which triggers traversal of the acceleration structure, and eventual execution of the appropriate hit or miss shader.  In addition, TraceRay can also be called from within hit and miss shaders, allowing for ray recursion or “multi-bounce” effects.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig-4.png&quot;&gt;&lt;br/&gt;&lt;img width=&quot;434&quot; height=&quot;298&quot; class=&quot;size-full wp-image-1105 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig-4.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4: an illustration of ray recursion in a scene&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Note that because the raytracing pipeline omits many of the fixed-function units of the graphics pipeline such as the input assembler and output merger, it is up to the application to specify how geometry is interpreted.  Shaders are given the minimum set of attributes required to do this, namely the intersection point’s &lt;a href=&quot;https://en.wikipedia.org/wiki/Barycentric_coordinate_system&quot;&gt;barycentric coordinates&lt;/a&gt; within the primitive.  Ultimately, this flexibility is a significant benefit of DXR; the design allows for a huge variety of techniques without the overhead of mandating particular formats or constructs.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;PIX for Windows Support Available on Day 1&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;As new graphics features put an increasing array of options at the disposal of game developers, the need for great tools becomes increasingly important.  The great news is that PIX for Windows will support the DirectX Raytracing API from day 1 of the API’s release.  PIX on Windows supports capturing and analyzing frames built using DXR to help developers understand how DXR interacts with the hardware. Developers can inspect API calls, view pipeline resources that contribute to the raytracing work, see contents of state objects, and visualize acceleration structures. This provides the information developers need to build great experiences using DXR.&lt;/p&gt;


&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/Multithreading.png&quot;&gt;&lt;img width=&quot;1024&quot; height=&quot;771&quot; class=&quot;size-large wp-image-1235 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/Multithreading-1024x771.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;&lt;strong&gt;What Does This Mean for Games?&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;DXR will initially be used to supplement current rendering techniques such as screen space reflections, for example, to fill in data from geometry that’s either occluded or off-screen.  This will lead to a material increase in visual quality for these effects in the near future.  Over the next several years, however, we expect an increase in utilization of DXR for techniques that are simply impractical for rasterization, such as true global illumination.  Eventually, raytracing may completely replace rasterization as the standard algorithm for rendering 3D scenes.  That said, until everyone has a &lt;a href=&quot;https://en.wikipedia.org/wiki/Light_field&quot;&gt;light-field&lt;/a&gt; display on their desk, rasterization will continue to be an excellent match for the common case of rendering content to a flat grid of square pixels, supplemented by raytracing for true 3D effects.&lt;/p&gt;
&lt;p&gt;Thanks to our friends at &lt;a href=&quot;https://www.ea.com/seed&quot;&gt;SEED, Electronic Arts&lt;/a&gt;, we can show you a &lt;a href=&quot;https://www.youtube.com/watch?v=LXo0WdlELJk&quot;&gt;glimpse of what future gaming scenes could look like&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/SEED-screenshot.png&quot;&gt;&lt;img width=&quot;1024&quot; height=&quot;576&quot; class=&quot;size-large wp-image-1045 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/SEED-screenshot-1024x576.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Project PICA PICA from SEED, Electronic Arts&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In addition, while today marks the first public announcement of DirectX Raytracing, we have been working closely with hardware vendors and industry developers for nearly a year to design and tune the API.  In fact, a significant number of studios and engines are already planning to integrate DXR support into their games and engines, including:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/frostbite.png&quot;&gt;&lt;img width=&quot;652&quot; height=&quot;367&quot; class=&quot;size-full wp-image-1245 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/frostbite.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Electronic Arts, Frostbite&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/seed-working.jpg&quot;&gt;&lt;img width=&quot;381&quot; height=&quot;381&quot; class=&quot; wp-image-1285 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/seed-working.jpg&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Electronic Arts,  SEED&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/Unreal_Engine_Black.png&quot;&gt;&lt;img width=&quot;341&quot; height=&quot;388&quot; class=&quot;wp-image-1265 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/Unreal_Engine_Black-900x1024.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Epic Games, Unreal Engine&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/3dmarkworking.png&quot;&gt;&lt;img width=&quot;696&quot; height=&quot;145&quot; class=&quot;wp-image-1275 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/3dmarkworking.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Futuremark, 3DMark&lt;/em&gt;&lt;/p&gt;


&lt;p&gt;&lt;a href=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/unitylogo.jpg&quot;&gt;&lt;img width=&quot;448&quot; height=&quot;182&quot; class=&quot; wp-image-1155 aligncenter&quot; alt=&quot;&quot; src=&quot;https://msdnshared.blob.core.windows.net/media/2018/03/unitylogo.jpg&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Unity Technologies, Unity Engine&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;And more will be coming soon.&lt;/p&gt;

&lt;h4&gt;&lt;strong&gt;What Hardware Will DXR Run On?&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Developers can use currently in-market hardware to get started on DirectX Raytracing.  There is also a fallback layer which will allow developers to start experimenting with DirectX Raytracing that does not require any specific hardware support.  For hardware roadmap support for DirectX Raytracing, please contact hardware vendors directly for further details.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Available now for experimentation!&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Want to be one of the first to bring real-time raytracing to your game?  Start by attending our &lt;a href=&quot;http://schedule.gdconf.com/session/directx-evolving-microsofts-graphics-platform-presented-by-microsoft/856594&quot;&gt;Game Developer Conference Session on DirectX Raytracing &lt;/a&gt;for all the technical details you need to begin, then download the &lt;a href=&quot;http://aka.ms/dxrsdk&quot;&gt;Experimental DXR SDK&lt;/a&gt; and start coding!  Not attending GDC?  No problem!  We'll be posting the slides on this blog after the session.&lt;/p&gt;






&lt;h4&gt;&lt;strong&gt;Appendix – Primers on rasterization, raytracing and DirectX Raytracing&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intro to Rasterization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Of all the rendering algorithms out there, by far the most widely used is &lt;strong&gt;rasterization&lt;/strong&gt;. Rasterization has been around since the 90s and has since become the dominant rendering technique in video games. This is with good reason: it’s incredibly efficient and can produce high levels of visual realism.&lt;/p&gt;

&lt;p&gt;Rasterization is an algorithm that in a sense doesn’t do all its work in 3D. This is because rasterization has a step where 3D objects get projected onto your 2D monitor, before they are colored in. This work can be done efficiently by GPUs because it’s work that can be done in parallel: the work needed to color in one pixel on the 2D screen can be done independently of the work needed to color one the pixel next to it.&lt;/p&gt;

&lt;p&gt;There’s a problem with this: in the real world the color of one object will have an impact on the objects around it, because of the complicated interplay of light.  This means that developers must resort to a wide variety of clever techniques to simulate the visual effects that are normally caused by light scattering, reflecting and refracting off objects in the real world. The shadows, reflections and indirect lighting in games are made with these techniques.&lt;/p&gt;

&lt;p&gt;Games rendered with rasterization can look and feel incredibly lifelike, because developers have gotten extremely good at making it look as if their worlds have light that acts in convincing way. Having said that, it takes an incredible deal of technical expertise to do this well and there’s also an upper limit to how realistic a rasterized game can get, since information about 3D objects gets lost every time they get projected onto your 2D screen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intro to Raytracing&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Raytracing calculates the color of pixels by tracing the path of light that would have created it and simulates this ray of light’s interactions with objects in the virtual world. Raytracing therefore calculates what a pixel would look like if a virtual world had real light. The beauty of raytracing is that it preserves the 3D world and visual effects like shadows, reflections and indirect lighting are a natural consequence of the raytracing algorithm, not special effects.&lt;/p&gt;

&lt;p&gt;Raytracing can be used to calculate the color of every single pixel on your screen, or it can be used for only some pixels, such as those on reflective surfaces.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How does it work?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A ray gets sent out for each pixel in question. The algorithm works out which object gets hit first by the ray and the exact point at which the ray hits the object. This point is called the first point of intersection and the algorithm does two things here: 1) it estimates the incoming light at the point of intersection and 2) combines this information about the incoming light with information about the object that was hit.&lt;/p&gt;

&lt;p&gt;1)      To estimate what the incoming light looked like at the first point of intersection, the algorithm needs to consider where this light was reflected or refracted from.&lt;/p&gt;
&lt;p&gt;2)      Specific information about each object is important because objects don’t all have the same properties: they absorb, reflect and refract light in different ways:&lt;/p&gt;
&lt;p&gt;-          different ways of absorption are what cause objects to have different colors (for example, a leaf is green because it absorbs all but green light)&lt;/p&gt;
&lt;p&gt;-          different rates of reflection are what cause some objects to give off mirror-like reflections and other objects to scatter rays in all directions&lt;/p&gt;
&lt;p&gt;-          different rates of refraction are what cause some objects (like water) to distort light more than other objects.&lt;/p&gt;
&lt;p&gt;Often to estimate the incoming light at the first point of intersection, the algorithm must trace that light to a second point of intersection (because the light hitting an object might have been reflected off another object), or even further back.&lt;/p&gt;

&lt;p&gt;Savvy readers with some programming knowledge might notice some edge cases here.&lt;/p&gt;

&lt;p&gt;Sometimes light rays that get sent out never hit anything. Don’t worry, this is an edge case we can cover easily by measuring for how far a ray has travelled so that we can do additional work on rays that have travelled for too far.&lt;/p&gt;

&lt;p&gt;The second edge case covers the opposite situation: light might bounce around so much that it’ll slow down the algorithm, or an infinite number of times, causing an infinite loop. The algorithm keeps track of how many times a ray gets traced after every step and gets terminated after a certain number of reflections. We can justify doing this because every object in the real world absorbs some light, even mirrors. This means that a light ray loses energy (becomes fainter) every time it’s reflected, until it becomes too faint to notice. So even if we could, tracing a ray an arbitrary number of times doesn’t make sense.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What is the state of raytracing today?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Raytracing a technique that’s been around for decades. It’s used quite often to do CGI in films and several games already use forms of raytracing. For example, developers might use offline raytracing to do things like pre-calculating the brightness of virtual objects before shipping their games.&lt;/p&gt;

&lt;p&gt;No games currently use real-time raytracing, but we think that this will change soon: over the past few years, computer hardware has become more and more flexible: even with the same TFLOPs, a GPU can do more.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How does this fit into DirectX?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We believe that DirectX Raytracing will bring raytracing within reach of real-time use cases, since it comes with dedicated hardware acceleration and can be integrated seamlessly with existing DirectX 12 content.&lt;/p&gt;

&lt;p&gt;This means that it’s now possible for developers to build games that use rasterization for some of its rendering and raytracing to be used for the rest. For example, developers can build a game where much of the content is generated with rasterization, but DirectX Raytracing calculates the shadows or reflections, helping out in areas where rasterization is lacking.&lt;/p&gt;

&lt;p&gt;This is the power of DirectX Raytracing: it lets developers have their cake and eat it.&lt;/p&gt;
</description>
<pubDate>Mon, 19 Mar 2018 17:21:18 +0000</pubDate>
<dc:creator>mxfh</dc:creator>
<og:type>article</og:type>
<og:title>Announcing Microsoft DirectX Raytracing!</og:title>
<og:url>https://blogs.msdn.microsoft.com/directx/?p=975</og:url>
<og:description>3D Graphics is a Lie For the last thirty years, almost all games have used the same general technique—rasterization—to render images on screen.  While the internal representation of the game world is maintained as three dimensions, rasterization ultimately operates in two dimensions (the plane of the screen), with 3D primitives mapped onto it through transformation...</og:description>
<og:image>https://msdnshared.blob.core.windows.net/media/2018/03/raytracing-fig1a.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://blogs.msdn.microsoft.com/directx/2018/03/19/announcing-microsoft-directx-raytracing/</dc:identifier>
</item>
<item>
<title>Startup founders throughout the Midwest are doing something new: staying</title>
<link>https://story.californiasunday.com/indianapolis-tech</link>
<guid isPermaLink="true" >https://story.californiasunday.com/indianapolis-tech</guid>
<description>&lt;section class=&quot;atavist-section story-cover story-data-container&quot; id=&quot;story-cover&quot; data-story=&quot;{&amp;quot;title&amp;quot;:&amp;quot;Does Tech Need Silicon Valley?&amp;quot;,&amp;quot;subtitle&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;byline&amp;quot;:&amp;quot;The California Sunday Magazine&amp;quot;,&amp;quot;byline_picker&amp;quot;:&amp;quot;{\&amp;quot;authors\&amp;quot;:[{\&amp;quot;id\&amp;quot;:\&amp;quot;custom\&amp;quot;,\&amp;quot;profile_name\&amp;quot;:\&amp;quot;By Matthew Shaer\&amp;quot;,\&amp;quot;has_avatar\&amp;quot;:false,\&amp;quot;is_atavist\&amp;quot;:false,\&amp;quot;url\&amp;quot;:null,\&amp;quot;type\&amp;quot;:\&amp;quot;custom\&amp;quot;}],\&amp;quot;publication\&amp;quot;:{\&amp;quot;id\&amp;quot;:\&amp;quot;publication-24442\&amp;quot;,\&amp;quot;profile_name\&amp;quot;:\&amp;quot;The California Sunday Magazine\&amp;quot;,\&amp;quot;avatar\&amp;quot;:\&amp;quot;https:\/\/atavist.com\/data\/files\/organization\/24442\/image\/derivative\/cropandscale~150x150~favicon196-1437196149-59.png\&amp;quot;,\&amp;quot;is_publication\&amp;quot;:true,\&amp;quot;type\&amp;quot;:\&amp;quot;publication\&amp;quot;,\&amp;quot;is_atavist\&amp;quot;:true,\&amp;quot;url\&amp;quot;:\&amp;quot;http:\/\/story.californiasunday.com\&amp;quot;,\&amp;quot;is_profile\&amp;quot;:false,\&amp;quot;has_avatar\&amp;quot;:true}}&amp;quot;,&amp;quot;title_page&amp;quot;:&amp;quot;0401midwest-1521050557-70.jpg&amp;quot;,&amp;quot;description&amp;quot;:&amp;quot;Startup founders throughout the Midwest are doing something new. Staying. &amp;quot;,&amp;quot;pub_date&amp;quot;:&amp;quot;1521000000&amp;quot;}&quot;&gt;&lt;div class=&quot;story-cover&quot; id=&quot;story-cover&quot; style-scope=&quot;custom-cover-564f914a04c15&quot; data-cover-layout=&quot;full&quot; readability=&quot;56&quot;&gt;
&lt;div class=&quot;cover-text atavist-cover-column-width-and-alignment&quot; style-scope=&quot;custom-cover-564f914a04c15&quot; readability=&quot;33&quot;&gt;
&lt;p&gt;1521000000&lt;/p&gt;

&lt;h2 class=&quot;cover-subtitle atavist-cover-h2&quot; style-scope=&quot;custom-cover-564f914a04c15&quot;&gt;Startup founders throughout the Midwest are doing something new. Staying.&lt;/h2&gt;
&lt;p&gt;The California Sunday Magazine&lt;/p&gt;
&lt;div class=&quot;cover-credit atavist-cover-byline&quot; style-scope=&quot;custom-cover-564f914a04c15&quot; readability=&quot;7&quot;&gt;
&lt;p is=&quot;atavist-caption&quot; key=&quot;other_credit_1&quot; placeholder=&quot;Write additional credit…&quot; style-scope=&quot;custom-cover-564f914a04c15&quot; class=&quot;atavist-caption atavist-shared atavist-subcomponent&quot; contenteditable=&quot;true&quot; data-editables=&quot;bold,italic,link&quot; data-disable-return=&quot;true&quot;&gt;Photographs by Jason Henry&lt;/p&gt;
&lt;/div&gt;


&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;&lt;section is=&quot;atavist-section&quot; class=&quot;chapter atavist-section chapter-type-0 web chapter-3586337 chapter-type-text&quot; id=&quot;chapter-3586337&quot; data-chapter_type=&quot;0&quot; data-chapter_id=&quot;3586337&quot; data-chapter_url=&quot;https://californiasunday.atavist.com/indianapolis-tech#chapter-3586337&quot; data-title=&quot;&quot;&gt;
&lt;/section&gt;&lt;section is=&quot;atavist-section&quot; class=&quot;chapter atavist-section chapter-type-0 web chapter-3580438 chapter-type-text&quot; id=&quot;chapter-3580438&quot; data-chapter_type=&quot;0&quot; data-chapter_id=&quot;3580438&quot; data-chapter_url=&quot;https://californiasunday.atavist.com/indianapolis-tech#chapter-3580438&quot; data-title=&quot;&quot; readability=&quot;153&quot;&gt;
&lt;p&gt;A couple of years ago, Megan Glover was on a coffee date with a friend when the conversation turned to the water crisis in Flint. Glover found herself nodding along. Like a lot of residents of Indianapolis, she was familiar with the topic — Indiana abuts Michigan, and for weeks, it seemed, local media had reported on little else. And yet, as Glover confessed to her friend, a fellow entrepreneur named Chris Baggott, she hadn’t gotten around to testing her own water supply. “I’m not sure where to start,” she said.&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;“Hey!” Baggott exclaimed. “I think you just stumbled on your next business.”&lt;/p&gt;
&lt;p&gt;That evening, Glover opened her laptop and searched water-testing services. Plenty of results came back, although most of the kits were either prohibitively expensive (up to $1,500 each) or required a high degree of scientific skill to operate. An idea began to cement in Glover’s mind: Her kit would be cheap and bare-bones — a beaker inside a box. It would be shipped to a consumer’s house with a return label inside. Users would fill up a beaker and drop the box back into the mail. An outside laboratory would do the actual testing, and the results would be emailed back to the consumer via in-house, proprietary software.&lt;/p&gt;
&lt;p&gt;By the summer of 2016, Glover had a name for her project — 120WaterAudit — and $130,000 in seed money, part of it provided by Baggott, who signed on as co-founder, and part of it pulled from Glover’s own savings. “When I say ‘bootstrapped,’ I mean ‘bootstrapped,’ ” Glover recalled. “Still, this was a company that I thought could be both profitable and have a real social impact, and I was determined to make it work.”&lt;/p&gt;
&lt;p&gt;If Glover had been based in Silicon Valley — or if she had relocated to California, like so many entrepreneurs before her — her path forward would have been all but preordained: a seat at an incubator with connections to big venture capital firms, the angel investment that would pay for a few part-time programmers, the multiple VC-backed financing rounds. Finally, if she were lucky, an acquisition by Google or another tech giant — the kind of lucrative exit startup founders like to dream about.&lt;/p&gt;
&lt;p&gt;But she was hesitant to move. She’d grown up in Northern Indiana and studied communications at DePauw University. She and her husband, a commercial real estate developer, had built a life for themselves and their two kids near the capital city.&lt;/p&gt;
&lt;p&gt;Glover was also inspired by a book she’d started reading that spring. The book was called &lt;em&gt;The Third Wave&lt;/em&gt;; the author was Steve Case, a co-founder of America Online. As Case saw it, the internet had evolved in three distinct stages. Case himself helped inaugurate the first wave of basic connectivity. In the second wave, companies had shifted from “building the internet to building on top of the internet,” with search engines and apps. Now, in the third wave, digital “disruptors” would conquer industries like energy, government services, and health care. Crucially, Case theorized, many of the disruptors would not live in Silicon Valley. Instead, they’d be based in manufacturing hubs like Pittsburgh or Columbus, Ohio, where startup scenes were already springing to life. If the second wave had been dominated by young app developers — “entrepreneurial carpetbaggers,” Case called them — then the third wave would be dominated by 30-somethings in flyover country.&lt;/p&gt;
&lt;p&gt;Glover, who is 36, recognized herself in Case’s book. And she recognized her city. When she’d arrived in Indianapolis in 2005, the place was best known as the site of the headquarters of the pharmaceutical company Eli Lilly. The few startups, such as the marketing software company Aprimo or Angie’s List, where Glover worked for a while, tended to be disconnected from one another. But in the mid-2000s, the state had started offering more incentives to entrepreneurs, and existing businesses like Lilly had invested in some of the startups.&lt;/p&gt;
&lt;p&gt;In late 2013, Larry Ellison’s company, Oracle, acquired Com­pendium, a local cloud-based content-marketing provider; a year prior, San Francisco–based Salesforce had bought another software company, ExactTarget, for $2.5 billion. Cash cascaded over the city as residents like Chris Baggott, who was a co-founder of ExactTarget, began to invest heavily in other Indianapolis startups. “Chris and others, they really doubled down,” Glover said. “It got to the point where it wasn’t uncommon for two separate software co-founders to be in on the same deal, mostly because people really want to see the ecosystem in Indianapolis crush it.”&lt;/p&gt;
&lt;p&gt;A crop of co-working facilities and incubators and accelerators sprang up downtown and in nearby cities like Fishers, the site of the warehouse-size Launch Fishers, which offered space for 500 entrepreneurs. Real estate was cheap. Founders could sink more of their cash into development. And in contrast to the frequently cutthroat nature of Silicon Valley, the emerging scene in Indianapolis was, for the most part, mutually supportive, infused with Midwestern politeness.&lt;/p&gt;
&lt;p&gt;“In the end, I didn’t deliberate long,” Glover remembered. She officially launched 120WaterAudit from a desk in the offices of another downtown Indy startup; in early 2017, she moved to the back room of a co-working space nearer to her home, in the northern suburb of Zionsville. “It came down to this: There weren’t a lot of compelling reasons to leave for a place like Silicon Valley,” she said. “And there were a ton of reasons to stay — family, a solid community, potential. So I stayed.”&lt;/p&gt;
&lt;/section&gt;&lt;section is=&quot;atavist-section&quot; class=&quot;chapter atavist-section chapter-type-0 web chapter-3580640 chapter-type-text&quot; id=&quot;chapter-3580640&quot; data-chapter_type=&quot;0&quot; data-chapter_id=&quot;3580640&quot; data-chapter_url=&quot;https://californiasunday.atavist.com/indianapolis-tech#chapter-3580640&quot; data-title=&quot;&quot;&gt;
&lt;/section&gt;&lt;section is=&quot;atavist-section&quot; class=&quot;chapter atavist-section chapter-type-0 web chapter-3580445 chapter-type-text&quot; id=&quot;chapter-3580445&quot; data-chapter_type=&quot;0&quot; data-chapter_id=&quot;3580445&quot; data-chapter_url=&quot;https://californiasunday.atavist.com/indianapolis-tech#chapter-3580445&quot; data-title=&quot;&quot; readability=&quot;138&quot;&gt;
&lt;p&gt;&lt;strong&gt;Steve Case likes&lt;/strong&gt; to remind people that Silicon Valley wasn’t always the epicenter of tech culture. In his book and in speeches, he’ll point out that AOL was originally headquartered near Washington, D.C., and Sprint, in Kansas. But after Google went public in 2004, with a valuation of $27 billion, an army of entrepreneurs and developers headed west in an attempt to duplicate — or, just as often, piggyback on — Google’s success. Many did: In the past decade and a half, 136 U.S. software startups have reached a valuation of $1 billion or higher, according to Atomico, an investment firm. Eighty of those companies were founded in the Bay Area.&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;In 2010, Case was tapped to co-chair a working group on American entrepreneurship for then-President Barack Obama’s Department of Commerce. Part of the gig was looking into ways to create tech jobs. “That’s when I really had my eyes opened,” Case told me recently. Poring over the available data, Case discovered that plenty of Midwestern and Southwestern cities were leveraging tax incentives to stanch local brain drain, and a few, such as Pittsburgh and Indianapolis, were cultivating their own robust startup scenes. The talent wasn’t all on the coasts. Nor, for that matter, were the jobs. Metro areas like Detroit and Atlanta were adding jobs in tech services and software at surprisingly steady rates. But the requisite venture capital, that ultimate driver of any startup economy, was missing. The vast majority — 80 percent — of VC investment goes to the coasts, Case pointed out, referring to a report from the Martin Prosperity Institute in Toronto. The other states have to fight for the scraps. “It was pretty clear there was a problem here and an opportunity.”&lt;/p&gt;
&lt;p&gt;Working through his investment fund, Revolution, Case embarked on a four-city bus tour he called Rise of the Rest. At each stop — in Detroit, Pittsburgh, Cincinnati, and Nashville — he organized a pitch competition; the winners took home $100K. In distributing the money, Case and a panel of local judges tended to gravitate toward projects that disrupted local industries in intriguing ways — in other words, third-wave startups. In Nashville, the winner of the competition was a digital-instrument startup called Artiphon. In Pittsburgh, a Rust Belt city once wholly dominated by coal and steel, it was SolePower, a maker of “smart boots” that self-charge with every step. “If you really want to make these cities rise fast,” Case told me, “I think you do it partly by encouraging the residents to completely reimagine the fields they’ve grown up around.”&lt;/p&gt;
&lt;p&gt;As Case saw it, there was value in telling the story of communities that had been historically overlooked by investors — in drawing attention to them and “making them magnets for capital.” Because he was Steve Case, media followed: In 2016, when the tour expanded to five cities, including Raleigh-Durham, North Carolina, there were reporters waiting outside the bus.&lt;/p&gt;
&lt;p&gt;The more Case traveled, the more encouraged he became. Yes, Silicon Valley was likely to remain the capital of tech culture for years to come. As he put it, “the rise of the rest doesn’t mean the fall of Silicon Valley.” Gradually, he believed, a national balancing would occur, with talent and cash being dispersed more evenly among the cities between the coasts. Startup founders would increasingly take advantage of the fact that they no longer needed to leave their hometown — as Case, a native Hawaiian, had done — to achieve their goals. “Out west, there’s a depth of talent, and there’s a mercenary culture where people jump around from startup to startup,” Case said. “In Rise cities, it’s harder to scale, but once you get there, it’s easier to build and sustain a real community.” &lt;/p&gt;
&lt;p&gt;Last fall, Rise of the Rest hosted its first pitch competition in downtown Indianapolis at The Union 525, a co-working space not far from Lucas Oil Stadium. Among the finalists was 120WaterAudit. Megan Glover submitted her application without expectation — her company was a little over a year old. Still, it was growing fast. Several months earlier, she’d signed a $300,000 contract with the city of Pittsburgh to test the water supply at schools. The proceeds from the contract had gone into hiring developers to create an in-house software suite. Similar deals followed: with the state of Indiana, with the city of Loveland in Colorado. The orders were coming in faster than she could keep up with. She was running into the same issue that Case had observed in other cities. It had been easy enough to scrounge up seed money and desk space in Indianapolis, but the venture capital she’d need to bring her company to scale was much harder to find locally.&lt;/p&gt;
&lt;p&gt;Tickets for the pitch competition had sold out well in advance, and The Union 525 was packed. Glover had heard of most of the other finalists. Many were marketing software startups of one type or another. She went close to last, using a four-minute pitch to highlight the social good her product could do and the ways in which 120WaterAudit could change what had long been a manual, time-consuming process.&lt;/p&gt;
&lt;p&gt;Case was immediately taken by the pitch. “You travel around enough, and you see a lot of similar ideas,” he told me. “This was something that we hadn’t seen before, and it had regional appeal: It was directed at other nearby Midwestern cities, like Flint.”&lt;/p&gt;
&lt;p&gt;That evening, he presented Glover with a check for $100,000.&lt;/p&gt;
&lt;/section&gt;&lt;section is=&quot;atavist-section&quot; class=&quot;chapter atavist-section chapter-type-0 web chapter-3580654 chapter-type-text&quot; id=&quot;chapter-3580654&quot; data-chapter_type=&quot;0&quot; data-chapter_id=&quot;3580654&quot; data-chapter_url=&quot;https://californiasunday.atavist.com/indianapolis-tech#chapter-3580654&quot; data-title=&quot;&quot;&gt;
&lt;/section&gt;&lt;section is=&quot;atavist-section&quot; class=&quot;chapter atavist-section chapter-type-0 web chapter-3580543 chapter-type-text&quot; id=&quot;chapter-3580543&quot; data-chapter_type=&quot;0&quot; data-chapter_id=&quot;3580543&quot; data-chapter_url=&quot;https://californiasunday.atavist.com/indianapolis-tech#chapter-3580543&quot; data-title=&quot;&quot; readability=&quot;179&quot;&gt;
&lt;p&gt;&lt;strong&gt;Shortly after the last&lt;/strong&gt; presidential election, Steve Case had drinks with the venture capitalist J.D. Vance at Riggsby, a bar off Dupont Circle in downtown Washington, D.C. Case lives in the capital; Vance, the author of the bestselling &lt;em&gt;Hillbilly Elegy&lt;/em&gt;, about the poor, white communities of the Midwest, was in town for an appearance on one of the Sunday talk shows. “I said to J.D., ‘Look, I loved your book. I was inspired by your book,’ ” Case remembered. “ ‘But I was also troubled that it brought up all these problems without offering any real solutions.’ J.D. turns to me and goes, ‘Well, that’s interesting because I’m actually thinking of leaving Silicon Valley and moving back home to Ohio.’ ”&lt;/p&gt;
&lt;p&gt;For months, Case had been batting around the idea of raising cash for the logical next step in his Rise initiative — a massive seed fund for early-stage startups outside of Boston, New York, or California. Vance, then at Peter Thiel’s Mithril Capital Management, would be a natural fit for such a project. Vance had Midwestern roots and VC experience; at Mithril, he’d led investments in non–Silicon Valley companies like the New Hampshire–based biomedical device maker Avitide and the Kansas software startup C2FO. “Basically,” Vance said, “I’d already bought into this idea that there was something to be found where other people weren’t looking. But building a big fund isn’t something you do overnight. It took us some time to plan.”&lt;/p&gt;
&lt;p&gt;Vance and Case started by reaching out to friends they thought might be interested in participating. “We deliberately wanted individuals and not institutions,” Case told me, “and, specifically, we wanted the kind of individuals who would send a signal with their participation.” Early backers included Amazon’s Jeff Bezos; former New York mayor Michael Bloomberg; Eric Schmidt of Alphabet, Google’s parent company; Howard Schultz of Starbucks; and John Doerr, the chairman of Kleiner Perkins Caufield &amp;amp; Byers, one of the most influential venture capital groups in the world. “It got to the point,” Case said, “that the requests were coming to us rather than the other way around. There was excitement and people saying, ‘Hey, can you bring us along?’ ”&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Last December, the Rise of the Rest Fund officially launched, with $150 million in cash. A few months after that, Case identified the recipients of the first seed round. Two were software startups headquartered in Ohio. One was an outdoor-apparel company from Utah. And one was an Indianapolis cloud-computing platform.&lt;/p&gt;
&lt;p&gt;The size of the investments has varied, but generally hovers in the range of $100,000 to close to $1 million. Not an insignificant chunk of capital, but not enough to drive even a relatively small startup in the long term. So in recent months, Case, Vance, and a third partner, Mary Grove, a former Google executive, have been busy cultivating a network of local investors in the hope of amplifying the effects of the fund’s investments.&lt;/p&gt;
&lt;p&gt;This winter, 80 members of that network convened for a two-day summit in the Watergate Hotel. They were joined by roughly 30 representatives from accelerators and consultancies headquartered in cities the Rise of the Rest tour had visited. A handful of entrepreneurs were on hand, too, including Megan Glover, of 120WaterAudit, who the previous day had been one of three Rise winners to spend a couple of hours at the offices of former President Barack Obama, giving short presentations on their startups.&lt;/p&gt;
&lt;p&gt;Outside one of the hotel’s conference rooms, a small crowd milled around a 20-foot-wide map of emerging startup scenes throughout the country; comic-book-style speech bubbles testified to the potential of certain locales. In Raleigh-Durham, one bubble explained, there’d been a 47 percent year-over-year increase in seed and series A funding for local startups. In Dallas, a group called the Texas Entrepreneurship Center calculated that its members had made a $130 million impact on the city’s economy. Nearby were sets of headphones for people to listen to what amounted to pretaped testimonials for tech hubs in Hawaii and Utah. In the hallways, the discussion was often of Amazon and which city it would choose as its new headquarters (Indianapolis is on the shortlist).&lt;/p&gt;
&lt;p&gt;The optimistic mood was mirrored on the summit’s main stage. In a TED talk–ish introduction, Case reminded the audience that “America itself was a startup” and recalled the days when major technology companies — from IBM to AOL — had sprouted up far from Silicon Valley. Later, a handful of attendees were invited to tell their own stories. The CEO of a Columbus trade organization discussed the recent $1 billion acquisition of CoverMyMeds, a pharmaceutical software startup, and the chief innovation officer for the state of Colorado said that venture investment in Denver had recently hit $514 million. “We continue to hustle. But we do so with a deeply Western heart. We are a magnet for the humbly brilliant,” he roared into the mic.&lt;/p&gt;
&lt;p&gt;But as Case and Vance acknowledged to me, not every Rise city had risen at the same pace. There were setbacks and stumbles, too, and working through them was part of the reason the summit had been organized. During a coffee break, Brandon Clark, a founder of StartupAZ, an accelerator based in Phoenix, explained, “We’re a state with 115,000 millionaires. On paper, that’s a lot of potential investors, right? But I’d guess we’ve only had 300 to 400 who’ve done any angel investing. That doesn’t mean that these folks don’t like making money.” Phoenix, he said, needed to continue to “build the narrative that putting cash into real estate isn’t the only game in town.”&lt;/p&gt;
&lt;p&gt;Case acknowledged that the investment culture of Silicon Valley is unique. It “encourages fearlessness,” he said, “and that’s something to be admired.” It can also be difficult to replicate elsewhere.&lt;/p&gt;
&lt;p&gt;That same afternoon, John Murdock of the Nashville Entrepreneur Center, a Tennessee nonprofit, took the stage. Murdock is in his early 30s; he grew up around Nashville and had watched as the city, in recent years, filled with enough investment cash — and enough incubators and accelerators — to make it feel like a genuine tech hub.&lt;/p&gt;
&lt;p&gt;But beginning in 2016, he’d noticed a troubling trend. The number of new startups was declining. “The biggest issue,” he later told me in the summit’s green room, “is that back in 2013 or 2014, you had a major spike in early-stage investments, with people saying, ‘I can make money, and I can give back to Nashville.’ ” There was only so much capital to go around, and a lot of it was still tied up in the early startups, at least until the companies folded or were sold. When new investors do emerge, Murdock said, “they want to see the results of the initial investments before they act.”&lt;/p&gt;
&lt;p&gt;Murdock believed a similar slowdown could eventually occur in other burgeoning tech scenes. “All these cities,” he said, “we’re going to grow. But we’ll grow in waves.”&lt;/p&gt;
&lt;/section&gt;&lt;section is=&quot;atavist-section&quot; class=&quot;chapter atavist-section chapter-type-0 web chapter-3580886 chapter-type-text&quot; id=&quot;chapter-3580886&quot; data-chapter_type=&quot;0&quot; data-chapter_id=&quot;3580886&quot; data-chapter_url=&quot;https://californiasunday.atavist.com/indianapolis-tech#chapter-3580886&quot; data-title=&quot;&quot;&gt;
&lt;/section&gt;&lt;section is=&quot;atavist-section&quot; class=&quot;chapter atavist-section chapter-type-0 web chapter-3580547 chapter-type-text&quot; id=&quot;chapter-3580547&quot; data-chapter_type=&quot;0&quot; data-chapter_id=&quot;3580547&quot; data-chapter_url=&quot;https://californiasunday.atavist.com/indianapolis-tech#chapter-3580547&quot; data-title=&quot;&quot; readability=&quot;144&quot;&gt;
&lt;p&gt;&lt;strong&gt;The Salesforce Tower&lt;/strong&gt; looms high over the Indianapolis skyline — a 48-story, 800-foot-plus glass and steel reminder of the central role the technology sector will play in the city for years to come. “It’s been amazing how quickly the area around here has been populated by tech folks,” Mike Langellier, the 36-year-old CEO of the local nonprofit accelerator TechPoint, said from his corner office on the 18th floor of the tower. “In that building right down there,” he explained, tapping at the glass, “you’ve got High Alpha” — a VC fund founded by Scott Dorsey, formerly of ExactTarget. Here, a couple of incubators. There, The Union 525, ground zero for more than a dozen startups, including the software company Springbuk, which recently locked down $20 million in series B funding.&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;“I think we’ve got all the right ingredients,” Langellier continued. “We’ve got the innovators and the talent, we’ve got the community, and now we’re getting the capital.” Salesforce had opened up a VC firm in the city, and in December, the state of Indiana had hired a Chicago-based asset management company to oversee its new Next Level Fund — a $250 million pot pooled from government coffers that will soon be available to local startups. Langellier speculated that in coming years, the city’s population could swell with developers and entrepreneurs. “We have a lot of quality-of-life advantages: not a lot of congestion, good restaurants, low home prices,” he said. “It’s a place where a young family won’t bankrupt themselves buying a house with a yard.”  &lt;/p&gt;
&lt;p&gt;Down the road from the Salesforce Tower, in the rapidly gentrifying neighborhood of Broad Ripple, is the Speak Easy, a startup incubator housed in an old carpet warehouse. Of the 300 paying members, many were transplants from elsewhere in the Midwest; some came from as far as Boston. “I think the sense is that Indianapolis is friendly and affordable and accessible,” Danielle McDowell, the executive director of Speak Easy, told me. “In Silicon Valley, you’d have to really struggle to get feedback from a top investor or founder. Here, the scene is smaller and more giving. You can get to the people you want to get to without much trouble.”&lt;/p&gt;
&lt;p&gt;Still, she said, “I don’t want to sugarcoat the fact that there are real issues.” For one thing, the majority of investors in Indianapolis were male and white, and so too were a lot of the entrepreneurs. (As a whole, the city is about 60 percent white.) “I did my own study and found that about 3.5 percent of venture capital here in Indiana goes into female-founded businesses,” McDowell said. “That’s a woeful number” — if marginally better than the national rate of approximately 2 percent.&lt;/p&gt;
&lt;p&gt;Then there was the passage, in 2015, of the Religious Freedom Restoration Act, or RFRA, which essentially made it legal for employers to discriminate — on the basis of religion — against gay, bisexual, and transgender employees. Nearly every developer and entrepreneur I met in Indianapolis mentioned the bill, which was signed into law by then-Governor Mike Pence. Some had gay friends who had decided against moving to the city because they feared the impact it would have on their lives.&lt;/p&gt;
&lt;p&gt;“The tech community here is really open-minded,” McDowell said, pointing out that the capital had tacked mostly blue in the presidential election. “But I do worry that Indiana as a whole now has something of a marred reputation.”&lt;/p&gt;
&lt;p&gt;McDowell took me around to meet a couple of the Speak Easy members. Ronald Pulliam was developing a sensor for internet-connected kitchen appliances. Kate Nolan ran a mountain-bike and hiking-guide business that catered primarily to women. And Erin Edds had helped develop an alcoholic-soda line called Garden Party. Progress, Edds admitted, had been a little slower than she would have liked — she’d raised enough seed money to get her product off the ground, but she and her husband, her co-founder, had struggled to drum up real interest among local investors.&lt;/p&gt;
&lt;p&gt;“It can be frustrating,” Edds told me, “but we’re hopeful.”&lt;/p&gt;
&lt;p&gt;Among the most successful transplants is Anderson Schoenrock, a self-described “serial entrepreneur” and the head of a photo-digitization company called Memory Ventures. Schoenrock started his company in Los Angeles in 2007, but by 2014, he had more than 30 employees and needed a bigger space. But a bigger space was going to be really expensive. So Schoenrock and his wife, a Michigander by birth, started looking at the Midwest: Grand Rapids, Columbus, Greater Indianapolis. After a meeting with the mayor and John Wechsler, the head of Launch Fishers, they settled on Indiana. “It was appealing because at that point, and I think this is still true, there was a lot of momentum, but the real story of the tech scene had yet to be written,” Schoenrock said. “It was a place I could get involved and make a difference in shaping that story.”&lt;/p&gt;
&lt;p&gt;Schoenrock was able to persuade 12 employees to make the move with him. In the year and a half since, he’s hired around 35 additional staffers, upgraded office spaces, and seen his revenue climb. “The awareness of what’s happening has grown,” Schoenrock told me. “I’ll tell you this: As I network with my entrepreneur friends who are outside Indianapolis, the reaction has gone from ‘What the hell are you doing, going to Indiana?’ to ‘OK, wow. That was probably a smart move.’ ”  &lt;/p&gt;
&lt;/section&gt;</description>
<pubDate>Mon, 19 Mar 2018 17:16:35 +0000</pubDate>
<dc:creator>prostoalex</dc:creator>
<og:type>article</og:type>
<og:title>Does Tech Need Silicon Valley?</og:title>
<og:description>Startup founders throughout the Midwest are doing something new. Staying.</og:description>
<og:url>https://story.californiasunday.com/indianapolis-tech</og:url>
<og:image>https://story.californiasunday.com/data/files/organization/24442/image/derivative/scale~1200x1200~0401midwest-1520973354-15.jpg?1521136926</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://story.californiasunday.com/indianapolis-tech</dc:identifier>
</item>
<item>
<title>SEC Announces Its Largest-Ever Whistleblower Awards</title>
<link>https://www.sec.gov/news/press-release/2018-44</link>
<guid isPermaLink="true" >https://www.sec.gov/news/press-release/2018-44</guid>
<description>&lt;p&gt;The Securities and Exchange Commission today announced its highest-ever Dodd-Frank whistleblower awards, with two whistleblowers sharing a nearly $50 million award and a third whistleblower receiving more than $33 million.  The previous high was a $30 million award in 2014.&lt;/p&gt;
&lt;p&gt;“These awards demonstrate that whistleblowers can provide the SEC with incredibly significant information that enables us to pursue and remedy serious violations that might otherwise go unnoticed,” said Jane Norberg, Chief of the SEC’s Office of the Whistleblower.  “We hope that these awards encourage others with specific, high-quality information regarding securities laws violations to step forward and report it to the SEC.”&lt;/p&gt;
&lt;p&gt;The SEC has awarded more than $262 million to 53 whistleblowers since issuing its first award in 2012.  All payments are made out of an investor protection fund established by Congress that is financed entirely through monetary sanctions paid to the SEC by securities law violators.  No money has been taken or withheld from harmed investors to pay whistleblower awards.&lt;/p&gt;
&lt;p&gt;Whistleblowers may be eligible for an award when they voluntarily provide the SEC with original, timely, and credible information that leads to a successful enforcement action. &lt;/p&gt;
&lt;p&gt;Whistleblower awards can range from 10 percent to 30 percent of the money collected when the monetary sanctions exceed $1 million. As with this case, whistleblowers can report jointly under the program and share an award.&lt;/p&gt;
&lt;p&gt;By law, the SEC protects the confidentiality of whistleblowers and does not disclose information that might directly or indirectly reveal a whistleblower’s identity.&lt;/p&gt;
&lt;p&gt;For more information about the whistleblower program and how to report a tip, visit &lt;a href=&quot;http://www.sec.gov/whistleblower&quot;&gt;www.sec.gov/whistleblower&lt;/a&gt;.&lt;/p&gt;
</description>
<pubDate>Mon, 19 Mar 2018 17:08:22 +0000</pubDate>
<dc:creator>ColinFCodeChef</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.sec.gov/news/press-release/2018-44</dc:identifier>
</item>
</channel>
</rss>