<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Digital Exile: How I Got Banned for Life from AirBnB</title>
<link>https://medium.com/@jacksoncunningham/digital-exile-how-i-got-banned-for-life-from-airbnb-615434c6eeba</link>
<guid isPermaLink="true" >https://medium.com/@jacksoncunningham/digital-exile-how-i-got-banned-for-life-from-airbnb-615434c6eeba</guid>
<description>&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;0*lfFsZKCyj5HSvBY5&quot; data-width=&quot;1241&quot; data-height=&quot;667&quot; data-is-featured=&quot;true&quot; data-action=&quot;zoom&quot; data-action-value=&quot;0*lfFsZKCyj5HSvBY5&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/0*lfFsZKCyj5HSvBY5&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;0ffc&quot; id=&quot;0ffc&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;A few months ago, I received a cryptic message from AirBnB that sounded like something straight out that Black Mirror episode with Jon Hamm.&lt;/p&gt;
&lt;blockquote name=&quot;6a49&quot; id=&quot;6a49&quot; class=&quot;graf graf--blockquote graf-after--p&quot;&gt;
&lt;p&gt;Dear Jackson,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;d2e5&quot; id=&quot;d2e5&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;We regret to inform you that we’ll be unable to support your account moving forward, and have exercised our discretion under our Terms of Service to disable your account(s). &lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;This decision is irreversible&lt;/strong&gt; and will affect any duplicated or future accounts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;f24b&quot; id=&quot;f24b&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;10&quot;&gt;
&lt;p&gt;Please understand that &lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;we are not obligated to provide an explanation&lt;/strong&gt; for the action taken against your account. Furthermore, we are not liable to you in any way with respect to disabling or canceling your account. Airbnb reserves the right to make the final determination with respect to such matters, and &lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;this decision will not be reversed&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;0*EjgzHee-A0Tjrgxi&quot; data-width=&quot;268&quot; data-height=&quot;225&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/0*EjgzHee-A0Tjrgxi&quot;/&gt;&lt;/div&gt;
blocked
&lt;p name=&quot;f92e&quot; id=&quot;f92e&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;At first, I wasn’t concerned. Surely there must be a misunderstanding. After all, I’ve been a loyal AirBnB evangelist from the early days. I referred dozens of friends when it first launched. I even convinced my parents to list their vacation properties.&lt;/p&gt;
&lt;p name=&quot;0f40&quot; id=&quot;0f40&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;After reaching out to support, I received the following unsettling email.&lt;/p&gt;
&lt;blockquote name=&quot;6409&quot; id=&quot;6409&quot; class=&quot;graf graf--blockquote graf-after--p&quot;&gt;
&lt;p&gt;Hi Jackson,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;32aa&quot; id=&quot;32aa&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;Please understand that &lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;we are not obligated to provide an explanation&lt;/strong&gt; for the action taken against your account. Additionally, &lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;we consider this matter closed and will no longer reply to any inquiries regarding your account.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;24d4&quot; id=&quot;24d4&quot; class=&quot;graf graf--p graf-after--blockquote&quot;&gt;Okay, this is a tough spot. Somehow I’ve violated the terms of service, but they won’t tell me which ones. And I can’t communicate with them anymore.&lt;/p&gt;
&lt;p name=&quot;8c37&quot; id=&quot;8c37&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Seems a little harsh.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*aOoLD6WiK6QhqL5_63eZrA.jpeg&quot; data-width=&quot;620&quot; data-height=&quot;343&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*aOoLD6WiK6QhqL5_63eZrA.jpeg&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;4107&quot; id=&quot;4107&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;At this point, I was pretty shocked but also very curious about what I could have possibly done. After carefully reading the &lt;a href=&quot;https://www.airbnb.com/terms&quot; data-href=&quot;https://www.airbnb.com/terms&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener nofollow&quot; target=&quot;_blank&quot;&gt;AirBnB terms of service&lt;/a&gt; and reading about the most common ways people get banned from AirBnB, I went through each of my bookings. My first thought was that I must have inadvertently paid one of the hosts in cash because this is the #1 reason why people get banned. But I confirmed that all my bookings were paid through AirBnB. No foul play there.&lt;/p&gt;
&lt;p name=&quot;3321&quot; id=&quot;3321&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;After discussing with my girlfriend, the only thing we could think of was that we had recently had a very uncomfortable AirBnB experience with a rude host.&lt;/p&gt;
&lt;h4 name=&quot;e7ab&quot; id=&quot;e7ab&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Here’s a brief summary of the incident that likely got me banned.&lt;/h4&gt;
&lt;p name=&quot;31a7&quot; id=&quot;31a7&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;After booking the weekend, we were told that we’d need to vacate the premises from 12–4pm because the room was located in a spa retreat.&lt;/p&gt;
&lt;p name=&quot;c65e&quot; id=&quot;c65e&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The conflict arose when the host entered the room an hour early, unannounced. She forgot to let us know she’d be coming an hour early with spa guests. We weren’t dressed but she continued setting up while we eventually scurried outside in front of the spa guests.&lt;/p&gt;
&lt;p name=&quot;e29a&quot; id=&quot;e29a&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;When the trip was over, we decided not to leave a review after giving her the benefit of the doubt that it was just a bad day. The property itself was great.&lt;/p&gt;
&lt;p name=&quot;49d9&quot; id=&quot;49d9&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;A few weeks later, I saw that the host left me a critical review with completely distorted details from what had actually happened.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;0*vF4YuO3dYyOUNklH&quot; data-width=&quot;770&quot; data-height=&quot;331&quot; data-action=&quot;zoom&quot; data-action-value=&quot;0*vF4YuO3dYyOUNklH&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/0*vF4YuO3dYyOUNklH&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;5225&quot; id=&quot;5225&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;I emailed AirBnB to report that the host had fabricated details in her review — details which could be proven within the AirBnB platform. But I was told it’s against their policy to censor reviews, even if they’re dishonest.&lt;/p&gt;
&lt;p name=&quot;805c&quot; id=&quot;805c&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Because the review period had passed on AirBnB and I felt the need to do some justice to my side of the story, I left a review on Google instead. The host has received a number of similar complaints so it definitely wasn’t an isolated incident.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;0*ZiRvqmzxuAljq3Fy&quot; data-width=&quot;652&quot; data-height=&quot;487&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/0*ZiRvqmzxuAljq3Fy&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;0*3E5byfiW0CmVWyKZ&quot; data-width=&quot;768&quot; data-height=&quot;306&quot; data-action=&quot;zoom&quot; data-action-value=&quot;0*3E5byfiW0CmVWyKZ&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/0*3E5byfiW0CmVWyKZ&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;2a94&quot; id=&quot;2a94&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;I still can’t believe that leaving an offsite review was a bannable offense, but even more disturbing to me is the way AirBnB handled the situation with a one-sided, permanent, irreversible, closed book suspension.&lt;/p&gt;
&lt;p name=&quot;3fb4&quot; id=&quot;3fb4&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The part that’s especially poetic to me is that AirBnB touts a firm brand message of community and connectedness with their “Belong Anywhere” campaigns but the frightening reality is that any individual user is completely disposable, without a shred of appeal to due process. I’m really thankful that I wasn’t reliant on AirBnB income like so many of my friends.&lt;/p&gt;
&lt;p name=&quot;1079&quot; id=&quot;1079&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;But let’s call it like it is&lt;/strong&gt;. This policy leverages the company’s power over the individual user to a cruel and unprecedented extent. And it’s in laughable contradiction to the &lt;a href=&quot;https://press.atairbnb.com/about-us/&quot; data-href=&quot;https://press.atairbnb.com/about-us/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener nofollow&quot; target=&quot;_blank&quot;&gt;brand’s inflated idealism&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*5cYYNlSvybxCrBF1kcIADA.jpeg&quot; data-width=&quot;818&quot; data-height=&quot;460&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*5cYYNlSvybxCrBF1kcIADA.jpeg&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*5cYYNlSvybxCrBF1kcIADA.jpeg&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;3fd8&quot; id=&quot;3fd8&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;After emailing AirBnB support and its founders multiple times, I’ve finally given up.&lt;/p&gt;
&lt;p name=&quot;c698&quot; id=&quot;c698&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Moving forward, I question whether these types of suspensions should be allowed from the tech giants without any oversight or regulation. At what point does a company become pervasive enough in everyday life that they owe users an explanation or warning before dropping the guillotine? Or is this all part of an ongoing trend, toward something like the &lt;a href=&quot;https://www.buzzfeed.com/mattstoller2/as-democracy-suffers-digital-dictators-are-seizing-power&quot; data-href=&quot;https://www.buzzfeed.com/mattstoller2/as-democracy-suffers-digital-dictators-are-seizing-power&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener nofollow&quot; target=&quot;_blank&quot;&gt;Chinese Social Credit Score system&lt;/a&gt;, where the consequences of not maintaining a high rating are socially crippling?&lt;/p&gt;
&lt;p name=&quot;f957&quot; id=&quot;f957&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;We’re becoming increasingly dependent on a handful of major tech giants to get through our basic daily routine. Imagine waking up one day and no longer being able to check your Gmail, buy things on Amazon, or book an Uber.&lt;/p&gt;
&lt;p name=&quot;9a17&quot; id=&quot;9a17&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;It all feels very 1984. Or Black Mirror. The one with Jon Hamm.&lt;/p&gt;
&lt;p name=&quot;3b02&quot; id=&quot;3b02&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;— —&lt;/p&gt;
&lt;h4 name=&quot;421b&quot; id=&quot;421b&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Let me know if you enjoyed this article by hitting the applause button below. It would mean a lot.&lt;/h4&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*cSZooi744b7FeELMIPBOjg.gif&quot; data-width=&quot;245&quot; data-height=&quot;150&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*cSZooi744b7FeELMIPBOjg.gif&quot;/&gt;&lt;/div&gt;
</description>
<pubDate>Fri, 13 Jul 2018 14:04:37 +0000</pubDate>
<dc:creator>ancarda</dc:creator>
<og:title>Digital Exile: How I Got Banned for Life from AirBnB</og:title>
<og:url>https://medium.com/@jacksoncunningham/digital-exile-how-i-got-banned-for-life-from-airbnb-615434c6eeba</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/0*lfFsZKCyj5HSvBY5</og:image>
<og:description>A few months ago, I received a cryptic message from AirBnB that sounded like something straight out that Black Mirror episode with Jon…</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://medium.com/@jacksoncunningham/digital-exile-how-i-got-banned-for-life-from-airbnb-615434c6eeba</dc:identifier>
</item>
<item>
<title>Learn how to design large-scale systems</title>
<link>https://github.com/donnemartin/system-design-primer</link>
<guid isPermaLink="true" >https://github.com/donnemartin/system-design-primer</guid>
<description>&lt;h3&gt;README.md&lt;/h3&gt;
&lt;article class=&quot;markdown-body entry-content&quot; itemprop=&quot;text&quot;&gt;&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/README.md&quot;&gt;English&lt;/a&gt; ∙ &lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/README-ja.md&quot;&gt;日本語&lt;/a&gt; ∙ &lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/README-zh-Hans.md&quot;&gt;简体中文&lt;/a&gt; ∙ &lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/README-zh-TW.md&quot;&gt;繁體中文&lt;/a&gt; | &lt;a href=&quot;https://github.com/donnemartin/system-design-primer/issues/40&quot;&gt;Brazilian Portuguese&lt;/a&gt; ∙ &lt;a href=&quot;https://github.com/donnemartin/system-design-primer/issues/130&quot;&gt;Greek&lt;/a&gt; ∙ &lt;a href=&quot;https://github.com/donnemartin/system-design-primer/issues/104&quot;&gt;Italian&lt;/a&gt; ∙ &lt;a href=&quot;https://github.com/donnemartin/system-design-primer/issues/102&quot;&gt;Korean&lt;/a&gt; ∙ &lt;a href=&quot;https://github.com/donnemartin/system-design-primer/issues/110&quot;&gt;Persian&lt;/a&gt; ∙ &lt;a href=&quot;https://github.com/donnemartin/system-design-primer/issues/68&quot;&gt;Polish&lt;/a&gt; ∙ &lt;a href=&quot;https://github.com/donnemartin/system-design-primer/issues/87&quot;&gt;Russian&lt;/a&gt; ∙ &lt;a href=&quot;https://github.com/donnemartin/system-design-primer/issues/136&quot;&gt;Spanish&lt;/a&gt; ∙ &lt;a href=&quot;https://github.com/donnemartin/system-design-primer/issues/39&quot;&gt;Turkish&lt;/a&gt; ∙ &lt;a href=&quot;https://github.com/donnemartin/system-design-primer/issues/127&quot;&gt;Vietnamese&lt;/a&gt; | &lt;a href=&quot;https://github.com/donnemartin/system-design-primer/issues/28&quot;&gt;Add Translation&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/e45e39c36eebcc4c66e1aecd4e4145112d8e88e3/687474703a2f2f692e696d6775722e636f6d2f6a6a3341354e382e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/e45e39c36eebcc4c66e1aecd4e4145112d8e88e3/687474703a2f2f692e696d6775722e636f6d2f6a6a3341354e382e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/jj3A5N8.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Learn how to design large-scale systems.&lt;/p&gt;
&lt;p&gt;Prep for the system design interview.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Learn how to design large-scale systems&lt;/h3&gt;
&lt;p&gt;Learning how to design scalable systems will help you become a better engineer.&lt;/p&gt;
&lt;p&gt;System design is a broad topic. There is a &lt;strong&gt;vast amount of resources scattered throughout the web&lt;/strong&gt; on system design principles.&lt;/p&gt;
&lt;p&gt;This repo is an &lt;strong&gt;organized collection&lt;/strong&gt; of resources to help you learn how to build systems at scale.&lt;/p&gt;
&lt;h3&gt;Learn from the open source community&lt;/h3&gt;
&lt;p&gt;This is an early draft of a continually updated, open source project.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer#contributing&quot;&gt;Contributions&lt;/a&gt; are welcome!&lt;/p&gt;
&lt;h3&gt;Prep for the system design interview&lt;/h3&gt;
&lt;p&gt;In addition to coding interviews, system design is a &lt;strong&gt;required component&lt;/strong&gt; of the &lt;strong&gt;technical interview process&lt;/strong&gt; at many tech companies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Practice common system design interview questions&lt;/strong&gt; and &lt;strong&gt;compare&lt;/strong&gt; your results with &lt;strong&gt;sample solutions&lt;/strong&gt;: discussions, code, and diagrams.&lt;/p&gt;
&lt;p&gt;Additional topics for interview prep:&lt;/p&gt;
&lt;h2&gt;Anki flashcards&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/75b5cf737556050871218226ea211256f19f3a40/687474703a2f2f692e696d6775722e636f6d2f7a6443416b42332e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/75b5cf737556050871218226ea211256f19f3a40/687474703a2f2f692e696d6775722e636f6d2f7a6443416b42332e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/zdCAkB3.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;The provided &lt;a href=&quot;https://apps.ankiweb.net/&quot; rel=&quot;nofollow&quot;&gt;Anki flashcard decks&lt;/a&gt; use spaced repetition to help you retain key system design concepts.&lt;/p&gt;
&lt;p&gt;Great for use while on-the-go.&lt;/p&gt;
&lt;h3&gt;Coding Resource: Interactive Coding Challenges&lt;/h3&gt;
&lt;p&gt;Looking for resources to help you prep for the &lt;a href=&quot;https://github.com/donnemartin/interactive-coding-challenges&quot;&gt;&lt;strong&gt;Coding Interview&lt;/strong&gt;&lt;/a&gt;?&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/473700c20356af5875155f24d3a26b57ae940bdc/687474703a2f2f692e696d6775722e636f6d2f6234597441454e2e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/473700c20356af5875155f24d3a26b57ae940bdc/687474703a2f2f692e696d6775722e636f6d2f6234597441454e2e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/b4YtAEN.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Check out the sister repo &lt;a href=&quot;https://github.com/donnemartin/interactive-coding-challenges&quot;&gt;&lt;strong&gt;Interactive Coding Challenges&lt;/strong&gt;&lt;/a&gt;, which contains an additional Anki deck:&lt;/p&gt;
&lt;h2&gt;Contributing&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Learn from the community.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Feel free to submit pull requests to help:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Fix errors&lt;/li&gt;
&lt;li&gt;Improve sections&lt;/li&gt;
&lt;li&gt;Add new sections&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/issues/28&quot;&gt;Translate&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Content that needs some polishing is placed &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#under-development&quot;&gt;under development&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Review the &lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/CONTRIBUTING.md&quot;&gt;Contributing Guidelines&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Index of system design topics&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Summaries of various system design topics, including pros and cons. &lt;strong&gt;Everything is a trade-off&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Each section contains links to more in-depth resources.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/14f76dab28dfbfa12ea6b02c6bd0ec726fc17306/687474703a2f2f692e696d6775722e636f6d2f6a7255424146372e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/14f76dab28dfbfa12ea6b02c6bd0ec726fc17306/687474703a2f2f692e696d6775722e636f6d2f6a7255424146372e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/jrUBAF7.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;/p&gt;
&lt;h2&gt;Study guide&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Suggested topics to review based on your interview timeline (short, medium, long).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/eb92600aa3bb1314b33edd0204da8428d4d3a493/687474703a2f2f692e696d6775722e636f6d2f4f66566c6c65782e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/eb92600aa3bb1314b33edd0204da8428d4d3a493/687474703a2f2f692e696d6775722e636f6d2f4f66566c6c65782e706e67&quot; alt=&quot;Imgur&quot; data-canonical-src=&quot;http://i.imgur.com/OfVllex.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q: For interviews, do I need to know everything here?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A: No, you don't need to know everything here to prepare for the interview&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;What you are asked in an interview depends on variables such as:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;How much experience you have&lt;/li&gt;
&lt;li&gt;What your technical background is&lt;/li&gt;
&lt;li&gt;What positions you are interviewing for&lt;/li&gt;
&lt;li&gt;Which companies you are interviewing with&lt;/li&gt;
&lt;li&gt;Luck&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;More experienced candidates are generally expected to know more about system design. Architects or team leads might be expected to know more than individual contributors. Top tech companies are likely to have one or more design interview rounds.&lt;/p&gt;
&lt;p&gt;Start broad and go deeper in a few areas. It helps to know a little about various key system design topics. Adjust the following guide based on your timeline, experience, what positions you are interviewing for, and which companies you are interviewing with.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Short timeline&lt;/strong&gt; - Aim for &lt;strong&gt;breadth&lt;/strong&gt; with system design topics. Practice by solving &lt;strong&gt;some&lt;/strong&gt; interview questions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Medium timeline&lt;/strong&gt; - Aim for &lt;strong&gt;breadth&lt;/strong&gt; and &lt;strong&gt;some depth&lt;/strong&gt; with system design topics. Practice by solving &lt;strong&gt;many&lt;/strong&gt; interview questions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Long timeline&lt;/strong&gt; - Aim for &lt;strong&gt;breadth&lt;/strong&gt; and &lt;strong&gt;more depth&lt;/strong&gt; with system design topics. Practice by solving &lt;strong&gt;most&lt;/strong&gt; interview questions.&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;How to approach a system design interview question&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;How to tackle a system design interview question.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The system design interview is an &lt;strong&gt;open-ended conversation&lt;/strong&gt;. You are expected to lead it.&lt;/p&gt;
&lt;p&gt;You can use the following steps to guide the discussion. To help solidify this process, work through the &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#system-design-interview-questions-with-solutions&quot;&gt;System design interview questions with solutions&lt;/a&gt; section using the following steps.&lt;/p&gt;
&lt;h3&gt;Step 1: Outline use cases, constraints, and assumptions&lt;/h3&gt;
&lt;p&gt;Gather requirements and scope the problem. Ask questions to clarify use cases and constraints. Discuss assumptions.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Who is going to use it?&lt;/li&gt;
&lt;li&gt;How are they going to use it?&lt;/li&gt;
&lt;li&gt;How many users are there?&lt;/li&gt;
&lt;li&gt;What does the system do?&lt;/li&gt;
&lt;li&gt;What are the inputs and outputs of the system?&lt;/li&gt;
&lt;li&gt;How much data do we expect to handle?&lt;/li&gt;
&lt;li&gt;How many requests per second do we expect?&lt;/li&gt;
&lt;li&gt;What is the expected read to write ratio?&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Step 2: Create a high level design&lt;/h3&gt;
&lt;p&gt;Outline a high level design with all important components.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Sketch the main components and connections&lt;/li&gt;
&lt;li&gt;Justify your ideas&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Step 3: Design core components&lt;/h3&gt;
&lt;p&gt;Dive into details for each core component. For example, if you were asked to &lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/pastebin/README.md&quot;&gt;design a url shortening service&lt;/a&gt;, discuss:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Generating and storing a hash of the full url
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/pastebin/README.md&quot;&gt;MD5&lt;/a&gt; and &lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/pastebin/README.md&quot;&gt;Base62&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hash collisions&lt;/li&gt;
&lt;li&gt;SQL or NoSQL&lt;/li&gt;
&lt;li&gt;Database schema&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Translating a hashed url to the full url
&lt;ul&gt;&lt;li&gt;Database lookup&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;API and object-oriented design&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Step 4: Scale the design&lt;/h3&gt;
&lt;p&gt;Identify and address bottlenecks, given the constraints. For example, do you need the following to address scalability issues?&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Load balancer&lt;/li&gt;
&lt;li&gt;Horizontal scaling&lt;/li&gt;
&lt;li&gt;Caching&lt;/li&gt;
&lt;li&gt;Database sharding&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Discuss potential solutions and trade-offs. Everything is a trade-off. Address bottlenecks using &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#index-of-system-design-topics&quot;&gt;principles of scalable system design&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Back-of-the-envelope calculations&lt;/h3&gt;
&lt;p&gt;You might be asked to do some estimates by hand. Refer to the &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#appendix&quot;&gt;Appendix&lt;/a&gt; for the following resources:&lt;/p&gt;
&lt;h3&gt;Source(s) and further reading&lt;/h3&gt;
&lt;p&gt;Check out the following links to get a better idea of what to expect:&lt;/p&gt;
&lt;h2&gt;System design interview questions with solutions&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Common system design interview questions with sample discussions, code, and diagrams.&lt;/p&gt;
&lt;p&gt;Solutions linked to content in the &lt;code&gt;solutions/&lt;/code&gt; folder.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Question&lt;/th&gt;
&lt;th/&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Design Pastebin.com (or Bit.ly)&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/pastebin/README.md&quot;&gt;Solution&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Design the Twitter timeline (or Facebook feed)&lt;br/&gt;Design Twitter search (or Facebook search)&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/twitter/README.md&quot;&gt;Solution&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Design a web crawler&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/web_crawler/README.md&quot;&gt;Solution&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Design Mint.com&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/mint/README.md&quot;&gt;Solution&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Design the data structures for a social network&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/social_graph/README.md&quot;&gt;Solution&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Design a key-value store for a search engine&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/query_cache/README.md&quot;&gt;Solution&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Design Amazon's sales ranking by category feature&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/sales_rank/README.md&quot;&gt;Solution&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Design a system that scales to millions of users on AWS&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/scaling_aws/README.md&quot;&gt;Solution&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Add a system design question&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer#contributing&quot;&gt;Contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3&gt;Design Pastebin.com (or Bit.ly)&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/pastebin/README.md&quot;&gt;View exercise and solution&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/4aee2d26ebedc20e7fa07a2c30780e332fa29f2c/687474703a2f2f692e696d6775722e636f6d2f346564584730542e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/4aee2d26ebedc20e7fa07a2c30780e332fa29f2c/687474703a2f2f692e696d6775722e636f6d2f346564584730542e706e67&quot; alt=&quot;Imgur&quot; data-canonical-src=&quot;http://i.imgur.com/4edXG0T.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Design the Twitter timeline and search (or Facebook feed and search)&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/twitter/README.md&quot;&gt;View exercise and solution&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/14f76dab28dfbfa12ea6b02c6bd0ec726fc17306/687474703a2f2f692e696d6775722e636f6d2f6a7255424146372e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/14f76dab28dfbfa12ea6b02c6bd0ec726fc17306/687474703a2f2f692e696d6775722e636f6d2f6a7255424146372e706e67&quot; alt=&quot;Imgur&quot; data-canonical-src=&quot;http://i.imgur.com/jrUBAF7.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Design a web crawler&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/web_crawler/README.md&quot;&gt;View exercise and solution&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/ba21a95852d1cf7bb64c8c4622a79d1d5a20d344/687474703a2f2f692e696d6775722e636f6d2f625778507451412e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/ba21a95852d1cf7bb64c8c4622a79d1d5a20d344/687474703a2f2f692e696d6775722e636f6d2f625778507451412e706e67&quot; alt=&quot;Imgur&quot; data-canonical-src=&quot;http://i.imgur.com/bWxPtQA.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Design Mint.com&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/mint/README.md&quot;&gt;View exercise and solution&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/12fea5f9324f74189a9cd983b02239c68615b67e/687474703a2f2f692e696d6775722e636f6d2f563571353776552e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/12fea5f9324f74189a9cd983b02239c68615b67e/687474703a2f2f692e696d6775722e636f6d2f563571353776552e706e67&quot; alt=&quot;Imgur&quot; data-canonical-src=&quot;http://i.imgur.com/V5q57vU.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Design the data structures for a social network&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/social_graph/README.md&quot;&gt;View exercise and solution&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/16d78e51c2e2949e23122f4c26afe5886f82a96f/687474703a2f2f692e696d6775722e636f6d2f636443763567372e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/16d78e51c2e2949e23122f4c26afe5886f82a96f/687474703a2f2f692e696d6775722e636f6d2f636443763567372e706e67&quot; alt=&quot;Imgur&quot; data-canonical-src=&quot;http://i.imgur.com/cdCv5g7.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Design a key-value store for a search engine&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/query_cache/README.md&quot;&gt;View exercise and solution&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/b6439861687b9a0fc62d0149a364082643ebaf86/687474703a2f2f692e696d6775722e636f6d2f346a39396d68652e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/b6439861687b9a0fc62d0149a364082643ebaf86/687474703a2f2f692e696d6775722e636f6d2f346a39396d68652e706e67&quot; alt=&quot;Imgur&quot; data-canonical-src=&quot;http://i.imgur.com/4j99mhe.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Design Amazon's sales ranking by category feature&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/sales_rank/README.md&quot;&gt;View exercise and solution&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/a56f5600f7ae29dc0c2e436b8e4e4b55c44d6894/687474703a2f2f692e696d6775722e636f6d2f4d7a45785030362e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/a56f5600f7ae29dc0c2e436b8e4e4b55c44d6894/687474703a2f2f692e696d6775722e636f6d2f4d7a45785030362e706e67&quot; alt=&quot;Imgur&quot; data-canonical-src=&quot;http://i.imgur.com/MzExP06.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Design a system that scales to millions of users on AWS&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer/blob/master/solutions/system_design/scaling_aws/README.md&quot;&gt;View exercise and solution&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/e45e39c36eebcc4c66e1aecd4e4145112d8e88e3/687474703a2f2f692e696d6775722e636f6d2f6a6a3341354e382e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/e45e39c36eebcc4c66e1aecd4e4145112d8e88e3/687474703a2f2f692e696d6775722e636f6d2f6a6a3341354e382e706e67&quot; alt=&quot;Imgur&quot; data-canonical-src=&quot;http://i.imgur.com/jj3A5N8.png&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Object-oriented design interview questions with solutions&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Common object-oriented design interview questions with sample discussions, code, and diagrams.&lt;/p&gt;
&lt;p&gt;Solutions linked to content in the &lt;code&gt;solutions/&lt;/code&gt; folder.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note: This section is under development&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;System design topics: start here&lt;/h2&gt;
&lt;p&gt;New to system design?&lt;/p&gt;
&lt;p&gt;First, you'll need a basic understanding of common principles, learning about what they are, how they are used, and their pros and cons.&lt;/p&gt;
&lt;h3&gt;Step 1: Review the scalability video lecture&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=-W9F__D3oY4&quot; rel=&quot;nofollow&quot;&gt;Scalability Lecture at Harvard&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Topics covered:
&lt;ul&gt;&lt;li&gt;Vertical scaling&lt;/li&gt;
&lt;li&gt;Horizontal scaling&lt;/li&gt;
&lt;li&gt;Caching&lt;/li&gt;
&lt;li&gt;Load balancing&lt;/li&gt;
&lt;li&gt;Database replication&lt;/li&gt;
&lt;li&gt;Database partitioning&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Step 2: Review the scalability article&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://www.lecloud.net/tagged/scalability&quot; rel=&quot;nofollow&quot;&gt;Scalability&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Topics covered:
&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Next steps&lt;/h3&gt;
&lt;p&gt;Next, we'll look at high-level trade-offs:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Performance&lt;/strong&gt; vs &lt;strong&gt;scalability&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Latency&lt;/strong&gt; vs &lt;strong&gt;throughput&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Availability&lt;/strong&gt; vs &lt;strong&gt;consistency&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Keep in mind that &lt;strong&gt;everything is a trade-off&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Then we'll dive into more specific topics such as DNS, CDNs, and load balancers.&lt;/p&gt;
&lt;h2&gt;Performance vs scalability&lt;/h2&gt;
&lt;p&gt;A service is &lt;strong&gt;scalable&lt;/strong&gt; if it results in increased &lt;strong&gt;performance&lt;/strong&gt; in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.&lt;sup&gt;&lt;a href=&quot;http://www.allthingsdistributed.com/2006/03/a_word_on_scalability.html&quot; rel=&quot;nofollow&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Another way to look at performance vs scalability:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;If you have a &lt;strong&gt;performance&lt;/strong&gt; problem, your system is slow for a single user.&lt;/li&gt;
&lt;li&gt;If you have a &lt;strong&gt;scalability&lt;/strong&gt; problem, your system is fast for a single user but slow under heavy load.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Source(s) and further reading&lt;/h3&gt;
&lt;h2&gt;Latency vs throughput&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Latency&lt;/strong&gt; is the time to perform some action or to produce some result.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Throughput&lt;/strong&gt; is the number of such actions or results per unit of time.&lt;/p&gt;
&lt;p&gt;Generally, you should aim for &lt;strong&gt;maximal throughput&lt;/strong&gt; with &lt;strong&gt;acceptable latency&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Source(s) and further reading&lt;/h3&gt;
&lt;h2&gt;Availability vs consistency&lt;/h2&gt;
&lt;h3&gt;CAP theorem&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/13719354da7dcd34cd79ff5f8b6306a67bc18261/687474703a2f2f692e696d6775722e636f6d2f62674c4d4932752e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/13719354da7dcd34cd79ff5f8b6306a67bc18261/687474703a2f2f692e696d6775722e636f6d2f62674c4d4932752e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/bgLMI2u.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://robertgreiner.com/2014/08/cap-theorem-revisited&quot; rel=&quot;nofollow&quot;&gt;Source: CAP theorem revisited&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In a distributed computer system, you can only support two of the following guarantees:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt; - Every read receives the most recent write or an error&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Availability&lt;/strong&gt; - Every request receives a response, without guarantee that it contains the most recent version of the information&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Partition Tolerance&lt;/strong&gt; - The system continues to operate despite arbitrary partitioning due to network failures&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;em&gt;Networks aren't reliable, so you'll need to support partition tolerance. You'll need to make a software tradeoff between consistency and availability.&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;CP - consistency and partition tolerance&lt;/h4&gt;
&lt;p&gt;Waiting for a response from the partitioned node might result in a timeout error. CP is a good choice if your business needs require atomic reads and writes.&lt;/p&gt;
&lt;h4&gt;AP - availability and partition tolerance&lt;/h4&gt;
&lt;p&gt;Responses return the most recent version of the data available on the a node, which might not be the latest. Writes might take some time to propagate when the partition is resolved.&lt;/p&gt;
&lt;p&gt;AP is a good choice if the business needs allow for &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#eventual-consistency&quot;&gt;eventual consistency&lt;/a&gt; or when the system needs to continue working despite external errors.&lt;/p&gt;
&lt;h3&gt;Source(s) and further reading&lt;/h3&gt;
&lt;h2&gt;Consistency patterns&lt;/h2&gt;
&lt;p&gt;With multiple copies of the same data, we are faced with options on how to synchronize them so clients have a consistent view of the data. Recall the definition of consistency from the &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#cap-theorem&quot;&gt;CAP theorem&lt;/a&gt; - Every read receives the most recent write or an error.&lt;/p&gt;
&lt;h3&gt;Weak consistency&lt;/h3&gt;
&lt;p&gt;After a write, reads may or may not see it. A best effort approach is taken.&lt;/p&gt;
&lt;p&gt;This approach is seen in systems such as memcached. Weak consistency works well in real time use cases such as VoIP, video chat, and realtime multiplayer games. For example, if you are on a phone call and lose reception for a few seconds, when you regain connection you do not hear what was spoken during connection loss.&lt;/p&gt;
&lt;h3&gt;Eventual consistency&lt;/h3&gt;
&lt;p&gt;After a write, reads will eventually see it (typically within milliseconds). Data is replicated asynchronously.&lt;/p&gt;
&lt;p&gt;This approach is seen in systems such as DNS and email. Eventual consistency works well in highly available systems.&lt;/p&gt;
&lt;h3&gt;Strong consistency&lt;/h3&gt;
&lt;p&gt;After a write, reads will see it. Data is replicated synchronously.&lt;/p&gt;
&lt;p&gt;This approach is seen in file systems and RDBMSes. Strong consistency works well in systems that need transactions.&lt;/p&gt;
&lt;h3&gt;Source(s) and further reading&lt;/h3&gt;
&lt;h2&gt;Availability patterns&lt;/h2&gt;
&lt;p&gt;There are two main patterns to support high availability: &lt;strong&gt;fail-over&lt;/strong&gt; and &lt;strong&gt;replication&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Fail-over&lt;/h3&gt;
&lt;h4&gt;Active-passive&lt;/h4&gt;
&lt;p&gt;With active-passive fail-over, heartbeats are sent between the active and the passive server on standby. If the heartbeat is interrupted, the passive server takes over the active's IP address and resumes service.&lt;/p&gt;
&lt;p&gt;The length of downtime is determined by whether the passive server is already running in 'hot' standby or whether it needs to start up from 'cold' standby. Only the active server handles traffic.&lt;/p&gt;
&lt;p&gt;Active-passive failover can also be referred to as master-slave failover.&lt;/p&gt;
&lt;h4&gt;Active-active&lt;/h4&gt;
&lt;p&gt;In active-active, both servers are managing traffic, spreading the load between them.&lt;/p&gt;
&lt;p&gt;If the servers are public-facing, the DNS would need to know about the public IPs of both servers. If the servers are internal-facing, application logic would need to know about both servers.&lt;/p&gt;
&lt;p&gt;Active-active failover can also be referred to as master-master failover.&lt;/p&gt;
&lt;h3&gt;Disadvantage(s): failover&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;Fail-over adds more hardware and additional complexity.&lt;/li&gt;
&lt;li&gt;There is a potential for loss of data if the active system fails before any newly written data can be replicated to the passive.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Replication&lt;/h3&gt;
&lt;h4&gt;Master-slave and master-master&lt;/h4&gt;
&lt;p&gt;This topic is further discussed in the &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#database&quot;&gt;Database&lt;/a&gt; section:&lt;/p&gt;
&lt;h2&gt;Domain name system&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/fae27d1291ed38dd120595d692eacd2505cd3a9c/687474703a2f2f692e696d6775722e636f6d2f494f794c6a34692e6a7067&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/fae27d1291ed38dd120595d692eacd2505cd3a9c/687474703a2f2f692e696d6775722e636f6d2f494f794c6a34692e6a7067&quot; data-canonical-src=&quot;http://i.imgur.com/IOyLj4i.jpg&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://www.slideshare.net/srikrupa5/dns-security-presentation-issa&quot; rel=&quot;nofollow&quot;&gt;Source: DNS security presentation&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A Domain Name System (DNS) translates a domain name such as &lt;a href=&quot;http://www.example.com&quot; rel=&quot;nofollow&quot;&gt;www.example.com&lt;/a&gt; to an IP address.&lt;/p&gt;
&lt;p&gt;DNS is hierarchical, with a few authoritative servers at the top level. Your router or ISP provides information about which DNS server(s) to contact when doing a lookup. Lower level DNS servers cache mappings, which could become stale due to DNS propagation delays. DNS results can also be cached by your browser or OS for a certain period of time, determined by the &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_to_live&quot; rel=&quot;nofollow&quot;&gt;time to live (TTL)&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;NS record (name server)&lt;/strong&gt; - Specifies the DNS servers for your domain/subdomain.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MX record (mail exchange)&lt;/strong&gt; - Specifies the mail servers for accepting messages.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A record (address)&lt;/strong&gt; - Points a name to an IP address.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CNAME (canonical)&lt;/strong&gt; - Points a name to another name or &lt;code&gt;CNAME&lt;/code&gt; (example.com to &lt;a href=&quot;http://www.example.com&quot; rel=&quot;nofollow&quot;&gt;www.example.com&lt;/a&gt;) or to an &lt;code&gt;A&lt;/code&gt; record.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Services such as &lt;a href=&quot;https://www.cloudflare.com/dns/&quot; rel=&quot;nofollow&quot;&gt;CloudFlare&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/route53/&quot; rel=&quot;nofollow&quot;&gt;Route 53&lt;/a&gt; provide managed DNS services. Some DNS services can route traffic through various methods:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;http://g33kinfo.com/info/archives/2657&quot; rel=&quot;nofollow&quot;&gt;Weighted round robin&lt;/a&gt;
&lt;ul&gt;&lt;li&gt;Prevent traffic from going to servers under maintenance&lt;/li&gt;
&lt;li&gt;Balance between varying cluster sizes&lt;/li&gt;
&lt;li&gt;A/B testing&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Latency-based&lt;/li&gt;
&lt;li&gt;Geolocation-based&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Disadvantage(s): DNS&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;Accessing a DNS server introduces a slight delay, although mitigated by caching described above.&lt;/li&gt;
&lt;li&gt;DNS server management could be complex and is generally managed by &lt;a href=&quot;http://superuser.com/questions/472695/who-controls-the-dns-servers/472729&quot; rel=&quot;nofollow&quot;&gt;governments, ISPs, and large companies&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;DNS services have recently come under &lt;a href=&quot;http://dyn.com/blog/dyn-analysis-summary-of-friday-october-21-attack/&quot; rel=&quot;nofollow&quot;&gt;DDoS attack&lt;/a&gt;, preventing users from accessing websites such as Twitter without knowing Twitter's IP address(es).&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Source(s) and further reading&lt;/h3&gt;
&lt;h2&gt;Content delivery network&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/853a8603651149c686bf3c504769fc594ff08849/687474703a2f2f692e696d6775722e636f6d2f683954417547492e6a7067&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/853a8603651149c686bf3c504769fc594ff08849/687474703a2f2f692e696d6775722e636f6d2f683954417547492e6a7067&quot; data-canonical-src=&quot;http://i.imgur.com/h9TAuGI.jpg&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;https://www.creative-artworks.eu/why-use-a-content-delivery-network-cdn/&quot; rel=&quot;nofollow&quot;&gt;Source: Why use a CDN&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A content delivery network (CDN) is a globally distributed network of proxy servers, serving content from locations closer to the user. Generally, static files such as HTML/CSS/JS, photos, and videos are served from CDN, although some CDNs such as Amazon's CloudFront support dynamic content. The site's DNS resolution will tell clients which server to contact.&lt;/p&gt;
&lt;p&gt;Serving content from CDNs can significantly improve performance in two ways:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Users receive content at data centers close to them&lt;/li&gt;
&lt;li&gt;Your servers do not have to serve requests that the CDN fulfills&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Push CDNs&lt;/h3&gt;
&lt;p&gt;Push CDNs receive new content whenever changes occur on your server. You take full responsibility for providing content, uploading directly to the CDN and rewriting URLs to point to the CDN. You can configure when content expires and when it is updated. Content is uploaded only when it is new or changed, minimizing traffic, but maximizing storage.&lt;/p&gt;
&lt;p&gt;Sites with a small amount of traffic or sites with content that isn't often updated work well with push CDNs. Content is placed on the CDNs once, instead of being re-pulled at regular intervals.&lt;/p&gt;
&lt;h3&gt;Pull CDNs&lt;/h3&gt;
&lt;p&gt;Pull CDNs grab new content from your server when the first user requests the content. You leave the content on your server and rewrite URLs to point to the CDN. This results in a slower request until the content is cached on the CDN.&lt;/p&gt;
&lt;p&gt;A &lt;a href=&quot;https://en.wikipedia.org/wiki/Time_to_live&quot; rel=&quot;nofollow&quot;&gt;time-to-live (TTL)&lt;/a&gt; determines how long content is cached. Pull CDNs minimize storage space on the CDN, but can create redundant traffic if files expire and are pulled before they have actually changed.&lt;/p&gt;
&lt;p&gt;Sites with heavy traffic work well with pull CDNs, as traffic is spread out more evenly with only recently-requested content remaining on the CDN.&lt;/p&gt;
&lt;h3&gt;Disadvantage(s): CDN&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;CDN costs could be significant depending on traffic, although this should be weighed with additional costs you would incur not using a CDN.&lt;/li&gt;
&lt;li&gt;Content might be stale if it is updated before the TTL expires it.&lt;/li&gt;
&lt;li&gt;CDNs require changing URLs for static content to point to the CDN.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Source(s) and further reading&lt;/h3&gt;
&lt;h2&gt;Load balancer&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/21caea3d7f67f451630012f657ae59a56709365c/687474703a2f2f692e696d6775722e636f6d2f6838316e39694b2e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/21caea3d7f67f451630012f657ae59a56709365c/687474703a2f2f692e696d6775722e636f6d2f6838316e39694b2e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/h81n9iK.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://horicky.blogspot.com/2010/10/scalable-system-design-patterns.html&quot; rel=&quot;nofollow&quot;&gt;Source: Scalable system design patterns&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Load balancers distribute incoming client requests to computing resources such as application servers and databases. In each case, the load balancer returns the response from the computing resource to the appropriate client. Load balancers are effective at:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Preventing requests from going to unhealthy servers&lt;/li&gt;
&lt;li&gt;Preventing overloading resources&lt;/li&gt;
&lt;li&gt;Helping eliminate single points of failure&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Load balancers can be implemented with hardware (expensive) or with software such as HAProxy.&lt;/p&gt;
&lt;p&gt;Additional benefits include:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;SSL termination&lt;/strong&gt; - Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Session persistence&lt;/strong&gt; - Issue cookies and route a specific client's requests to same instance if the web apps do not keep track of sessions&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;To protect against failures, it's common to set up multiple load balancers, either in &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#active-passive&quot;&gt;active-passive&lt;/a&gt; or &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#active-active&quot;&gt;active-active&lt;/a&gt; mode.&lt;/p&gt;
&lt;p&gt;Load balancers can route traffic based on various metrics, including:&lt;/p&gt;
&lt;h3&gt;Layer 4 load balancing&lt;/h3&gt;
&lt;p&gt;Layer 4 load balancers look at info at the &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#communication&quot;&gt;transport layer&lt;/a&gt; to decide how to distribute requests. Generally, this involves the source, destination IP addresses, and ports in the header, but not the contents of the packet. Layer 4 load balancers forward network packets to and from the upstream server, performing &lt;a href=&quot;https://www.nginx.com/resources/glossary/layer-4-load-balancing/&quot; rel=&quot;nofollow&quot;&gt;Network Address Translation (NAT)&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Layer 7 load balancing&lt;/h3&gt;
&lt;p&gt;Layer 7 load balancers look at the &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#communication&quot;&gt;application layer&lt;/a&gt; to decide how to distribute requests. This can involve contents of the header, message, and cookies. Layer 7 load balancers terminates network traffic, reads the message, makes a load-balancing decision, then opens a connection to the selected server. For example, a layer 7 load balancer can direct video traffic to servers that host videos while directing more sensitive user billing traffic to security-hardened servers.&lt;/p&gt;
&lt;p&gt;At the cost of flexibility, layer 4 load balancing requires less time and computing resources than Layer 7, although the performance impact can be minimal on modern commodity hardware.&lt;/p&gt;
&lt;h3&gt;Horizontal scaling&lt;/h3&gt;
&lt;p&gt;Load balancers can also help with horizontal scaling, improving performance and availability. Scaling out using commodity machines is more cost efficient and results in higher availability than scaling up a single server on more expensive hardware, called &lt;strong&gt;Vertical Scaling&lt;/strong&gt;. It is also easier to hire for talent working on commodity hardware than it is for specialized enterprise systems.&lt;/p&gt;
&lt;h4&gt;Disadvantage(s): horizontal scaling&lt;/h4&gt;
&lt;ul&gt;&lt;li&gt;Scaling horizontally introduces complexity and involves cloning servers
&lt;ul&gt;&lt;li&gt;Servers should be stateless: they should not contain any user-related data like sessions or profile pictures&lt;/li&gt;
&lt;li&gt;Sessions can be stored in a centralized data store such as a &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#database&quot;&gt;database&lt;/a&gt; (SQL, NoSQL) or a persistent &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#cache&quot;&gt;cache&lt;/a&gt; (Redis, Memcached)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Downstream servers such as caches and databases need to handle more simultaneous connections as upstream servers scale out&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Disadvantage(s): load balancer&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;The load balancer can become a performance bottleneck if it does not have enough resources or if it is not configured properly.&lt;/li&gt;
&lt;li&gt;Introducing a load balancer to help eliminate single points of failure results in increased complexity.&lt;/li&gt;
&lt;li&gt;A single load balancer is a single point of failure, configuring multiple load balancers further increases complexity.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Source(s) and further reading&lt;/h3&gt;
&lt;h2&gt;Reverse proxy (web server)&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/e88216d0999853426f72b28e41223f43977d22b7/687474703a2f2f692e696d6775722e636f6d2f6e3431417a66662e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/e88216d0999853426f72b28e41223f43977d22b7/687474703a2f2f692e696d6775722e636f6d2f6e3431417a66662e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/n41Azff.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;https://upload.wikimedia.org/wikipedia/commons/6/67/Reverse_proxy_h2g2bob.svg&quot; rel=&quot;nofollow&quot;&gt;Source: Wikipedia&lt;/a&gt;&lt;/em&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;A reverse proxy is a web server that centralizes internal services and provides unified interfaces to the public. Requests from clients are forwarded to a server that can fulfill it before the reverse proxy returns the server's response to the client.&lt;/p&gt;
&lt;p&gt;Additional benefits include:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Increased security&lt;/strong&gt; - Hide information about backend servers, blacklist IPs, limit number of connections per client&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Increased scalability and flexibility&lt;/strong&gt; - Clients only see the reverse proxy's IP, allowing you to scale servers or change their configuration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SSL termination&lt;/strong&gt; - Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compression&lt;/strong&gt; - Compress server responses&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Caching&lt;/strong&gt; - Return the response for cached requests&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Static content&lt;/strong&gt; - Serve static content directly
&lt;ul&gt;&lt;li&gt;HTML/CSS/JS&lt;/li&gt;
&lt;li&gt;Photos&lt;/li&gt;
&lt;li&gt;Videos&lt;/li&gt;
&lt;li&gt;Etc&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Load balancer vs reverse proxy&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;Deploying a load balancer is useful when you have multiple servers. Often, load balancers route traffic to a set of servers serving the same function.&lt;/li&gt;
&lt;li&gt;Reverse proxies can be useful even with just one web server or application server, opening up the benefits described in the previous section.&lt;/li&gt;
&lt;li&gt;Solutions such as NGINX and HAProxy can support both layer 7 reverse proxying and load balancing.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Disadvantage(s): reverse proxy&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;Introducing a reverse proxy results in increased complexity.&lt;/li&gt;
&lt;li&gt;A single reverse proxy is a single point of failure, configuring multiple reverse proxies (ie a &lt;a href=&quot;https://en.wikipedia.org/wiki/Failover&quot; rel=&quot;nofollow&quot;&gt;failover&lt;/a&gt;) further increases complexity.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Source(s) and further reading&lt;/h3&gt;
&lt;h2&gt;Application layer&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/feeb549c5b6e94f65c613635f7166dc26e0c7de7/687474703a2f2f692e696d6775722e636f6d2f7942355359776d2e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/feeb549c5b6e94f65c613635f7166dc26e0c7de7/687474703a2f2f692e696d6775722e636f6d2f7942355359776d2e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/yB5SYwm.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://lethain.com/introduction-to-architecting-systems-for-scale/#platform_layer&quot; rel=&quot;nofollow&quot;&gt;Source: Intro to architecting systems for scale&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Separating out the web layer from the application layer (also known as platform layer) allows you to scale and configure both layers independently. Adding a new API results in adding application servers without necessarily adding additional web servers. The &lt;strong&gt;single responsibility principle&lt;/strong&gt; advocates for small and autonomous services that work together. Small teams with small services can plan more aggressively for rapid growth.&lt;/p&gt;
&lt;p&gt;Workers in the application layer also help enable &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#asynchronism&quot;&gt;asynchronism&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Microservices&lt;/h3&gt;
&lt;p&gt;Related to this discussion are &lt;a href=&quot;https://en.wikipedia.org/wiki/Microservices&quot; rel=&quot;nofollow&quot;&gt;microservices&lt;/a&gt;, which can be described as a suite of independently deployable, small, modular services. Each service runs a unique process and communicates through a well-defined, lightweight mechanism to serve a business goal. &lt;sup&gt;&lt;a href=&quot;https://smartbear.com/learn/api-design/what-are-microservices&quot; rel=&quot;nofollow&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Pinterest, for example, could have the following microservices: user profile, follower, feed, search, photo upload, etc.&lt;/p&gt;
&lt;h3&gt;Service Discovery&lt;/h3&gt;
&lt;p&gt;Systems such as &lt;a href=&quot;https://www.consul.io/docs/index.html&quot; rel=&quot;nofollow&quot;&gt;Consul&lt;/a&gt;, &lt;a href=&quot;https://coreos.com/etcd/docs/latest&quot; rel=&quot;nofollow&quot;&gt;Etcd&lt;/a&gt;, and &lt;a href=&quot;http://www.slideshare.net/sauravhaloi/introduction-to-apache-zookeeper&quot; rel=&quot;nofollow&quot;&gt;Zookeeper&lt;/a&gt; can help services find each other by keeping track of registered names, addresses, and ports. &lt;a href=&quot;https://www.consul.io/intro/getting-started/checks.html&quot; rel=&quot;nofollow&quot;&gt;Health checks&lt;/a&gt; help verify service integrity and are often done using an &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#hypertext-transfer-protocol-http&quot;&gt;HTTP&lt;/a&gt; endpoint. Both Consul and Etcd have a built in &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#key-value-store&quot;&gt;key-value store&lt;/a&gt; that can be useful for storing config values and other shared data.&lt;/p&gt;
&lt;h3&gt;Disadvantage(s): application layer&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;Adding an application layer with loosely coupled services requires a different approach from an architectural, operations, and process viewpoint (vs a monolithic system).&lt;/li&gt;
&lt;li&gt;Microservices can add complexity in terms of deployments and operations.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Source(s) and further reading&lt;/h3&gt;
&lt;h2&gt;Database&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/15a7553727e6da98d0de5e9ca3792f6d2b5e92d4/687474703a2f2f692e696d6775722e636f6d2f586b6d3543587a2e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/15a7553727e6da98d0de5e9ca3792f6d2b5e92d4/687474703a2f2f692e696d6775722e636f6d2f586b6d3543587a2e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/Xkm5CXz.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=w95murBkYmU&quot; rel=&quot;nofollow&quot;&gt;Source: Scaling up to your first 10 million users&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Relational database management system (RDBMS)&lt;/h3&gt;
&lt;p&gt;A relational database like SQL is a collection of data items organized in tables.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ACID&lt;/strong&gt; is a set of properties of relational database &lt;a href=&quot;https://en.wikipedia.org/wiki/Database_transaction&quot; rel=&quot;nofollow&quot;&gt;transactions&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Atomicity&lt;/strong&gt; - Each transaction is all or nothing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt; - Any transaction will bring the database from one valid state to another&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Isolation&lt;/strong&gt; - Executing transactions concurrently has the same results as if the transactions were executed serially&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Durability&lt;/strong&gt; - Once a transaction has been committed, it will remain so&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;There are many techniques to scale a relational database: &lt;strong&gt;master-slave replication&lt;/strong&gt;, &lt;strong&gt;master-master replication&lt;/strong&gt;, &lt;strong&gt;federation&lt;/strong&gt;, &lt;strong&gt;sharding&lt;/strong&gt;, &lt;strong&gt;denormalization&lt;/strong&gt;, and &lt;strong&gt;SQL tuning&lt;/strong&gt;.&lt;/p&gt;
&lt;h4&gt;Master-slave replication&lt;/h4&gt;
&lt;p&gt;The master serves reads and writes, replicating writes to one or more slaves, which serve only reads. Slaves can also replicate to additional slaves in a tree-like fashion. If the master goes offline, the system can continue to operate in read-only mode until a slave is promoted to a master or a new master is provisioned.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/6a097809b9690236258747d969b1d3e0d93bb8ca/687474703a2f2f692e696d6775722e636f6d2f4339696f47746e2e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/6a097809b9690236258747d969b1d3e0d93bb8ca/687474703a2f2f692e696d6775722e636f6d2f4339696f47746e2e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/C9ioGtn.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://www.slideshare.net/jboner/scalability-availability-stability-patterns/&quot; rel=&quot;nofollow&quot;&gt;Source: Scalability, availability, stability, patterns&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h5&gt;Disadvantage(s): master-slave replication&lt;/h5&gt;
&lt;ul&gt;&lt;li&gt;Additional logic is needed to promote a slave to a master.&lt;/li&gt;
&lt;li&gt;See &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#disadvantages-replication&quot;&gt;Disadvantage(s): replication&lt;/a&gt; for points related to &lt;strong&gt;both&lt;/strong&gt; master-slave and master-master.&lt;/li&gt;
&lt;/ul&gt;&lt;h4&gt;Master-master replication&lt;/h4&gt;
&lt;p&gt;Both masters serve reads and writes and coordinate with each other on writes. If either master goes down, the system can continue to operate with both reads and writes.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/5862604b102ee97d85f86f89edda44bde85a5b7f/687474703a2f2f692e696d6775722e636f6d2f6b7241484c47672e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/5862604b102ee97d85f86f89edda44bde85a5b7f/687474703a2f2f692e696d6775722e636f6d2f6b7241484c47672e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/krAHLGg.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://www.slideshare.net/jboner/scalability-availability-stability-patterns/&quot; rel=&quot;nofollow&quot;&gt;Source: Scalability, availability, stability, patterns&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h5&gt;Disadvantage(s): master-master replication&lt;/h5&gt;
&lt;ul&gt;&lt;li&gt;You'll need a load balancer or you'll need to make changes to your application logic to determine where to write.&lt;/li&gt;
&lt;li&gt;Most master-master systems are either loosely consistent (violating ACID) or have increased write latency due to synchronization.&lt;/li&gt;
&lt;li&gt;Conflict resolution comes more into play as more write nodes are added and as latency increases.&lt;/li&gt;
&lt;li&gt;See &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#disadvantages-replication&quot;&gt;Disadvantage(s): replication&lt;/a&gt; for points related to &lt;strong&gt;both&lt;/strong&gt; master-slave and master-master.&lt;/li&gt;
&lt;/ul&gt;&lt;h5&gt;Disadvantage(s): replication&lt;/h5&gt;
&lt;ul&gt;&lt;li&gt;There is a potential for loss of data if the master fails before any newly written data can be replicated to other nodes.&lt;/li&gt;
&lt;li&gt;Writes are replayed to the read replicas. If there are a lot of writes, the read replicas can get bogged down with replaying writes and can't do as many reads.&lt;/li&gt;
&lt;li&gt;The more read slaves, the more you have to replicate, which leads to greater replication lag.&lt;/li&gt;
&lt;li&gt;On some systems, writing to the master can spawn multiple threads to write in parallel, whereas read replicas only support writing sequentially with a single thread.&lt;/li&gt;
&lt;li&gt;Replication adds more hardware and additional complexity.&lt;/li&gt;
&lt;/ul&gt;&lt;h5&gt;Source(s) and further reading: replication&lt;/h5&gt;
&lt;h4&gt;Federation&lt;/h4&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/6eb6570a8b6b4e1d52e3d7cc07e7959ea5dac75f/687474703a2f2f692e696d6775722e636f6d2f553371563333652e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/6eb6570a8b6b4e1d52e3d7cc07e7959ea5dac75f/687474703a2f2f692e696d6775722e636f6d2f553371563333652e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/U3qV33e.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=w95murBkYmU&quot; rel=&quot;nofollow&quot;&gt;Source: Scaling up to your first 10 million users&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Federation (or functional partitioning) splits up databases by function. For example, instead of a single, monolithic database, you could have three databases: &lt;strong&gt;forums&lt;/strong&gt;, &lt;strong&gt;users&lt;/strong&gt;, and &lt;strong&gt;products&lt;/strong&gt;, resulting in less read and write traffic to each database and therefore less replication lag. Smaller databases result in more data that can fit in memory, which in turn results in more cache hits due to improved cache locality. With no single central master serializing writes you can write in parallel, increasing throughput.&lt;/p&gt;
&lt;h5&gt;Disadvantage(s): federation&lt;/h5&gt;
&lt;ul&gt;&lt;li&gt;Federation is not effective if your schema requires huge functions or tables.&lt;/li&gt;
&lt;li&gt;You'll need to update your application logic to determine which database to read and write.&lt;/li&gt;
&lt;li&gt;Joining data from two databases is more complex with a &lt;a href=&quot;http://stackoverflow.com/questions/5145637/querying-data-by-joining-two-tables-in-two-database-on-different-servers&quot; rel=&quot;nofollow&quot;&gt;server link&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Federation adds more hardware and additional complexity.&lt;/li&gt;
&lt;/ul&gt;&lt;h5&gt;Source(s) and further reading: federation&lt;/h5&gt;
&lt;h4&gt;Sharding&lt;/h4&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/1df78be67b749171569a0e11a51aa76b3b678d4f/687474703a2f2f692e696d6775722e636f6d2f775538783549642e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/1df78be67b749171569a0e11a51aa76b3b678d4f/687474703a2f2f692e696d6775722e636f6d2f775538783549642e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/wU8x5Id.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://www.slideshare.net/jboner/scalability-availability-stability-patterns/&quot; rel=&quot;nofollow&quot;&gt;Source: Scalability, availability, stability, patterns&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Sharding distributes data across different databases such that each database can only manage a subset of the data. Taking a users database as an example, as the number of users increases, more shards are added to the cluster.&lt;/p&gt;
&lt;p&gt;Similar to the advantages of &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#federation&quot;&gt;federation&lt;/a&gt;, sharding results in less read and write traffic, less replication, and more cache hits. Index size is also reduced, which generally improves performance with faster queries. If one shard goes down, the other shards are still operational, although you'll want to add some form of replication to avoid data loss. Like federation, there is no single central master serializing writes, allowing you to write in parallel with increased throughput.&lt;/p&gt;
&lt;p&gt;Common ways to shard a table of users is either through the user's last name initial or the user's geographic location.&lt;/p&gt;
&lt;h5&gt;Disadvantage(s): sharding&lt;/h5&gt;
&lt;ul&gt;&lt;li&gt;You'll need to update your application logic to work with shards, which could result in complex SQL queries.&lt;/li&gt;
&lt;li&gt;Data distribution can become lopsided in a shard. For example, a set of power users on a shard could result in increased load to that shard compared to others.
&lt;ul&gt;&lt;li&gt;Rebalancing adds additional complexity. A sharding function based on &lt;a href=&quot;http://www.paperplanes.de/2011/12/9/the-magic-of-consistent-hashing.html&quot; rel=&quot;nofollow&quot;&gt;consistent hashing&lt;/a&gt; can reduce the amount of transferred data.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Joining data from multiple shards is more complex.&lt;/li&gt;
&lt;li&gt;Sharding adds more hardware and additional complexity.&lt;/li&gt;
&lt;/ul&gt;&lt;h5&gt;Source(s) and further reading: sharding&lt;/h5&gt;
&lt;h4&gt;Denormalization&lt;/h4&gt;
&lt;p&gt;Denormalization attempts to improve read performance at the expense of some write performance. Redundant copies of the data are written in multiple tables to avoid expensive joins. Some RDBMS such as &lt;a href=&quot;https://en.wikipedia.org/wiki/PostgreSQL&quot; rel=&quot;nofollow&quot;&gt;PostgreSQL&lt;/a&gt; and Oracle support &lt;a href=&quot;https://en.wikipedia.org/wiki/Materialized_view&quot; rel=&quot;nofollow&quot;&gt;materialized views&lt;/a&gt; which handle the work of storing redundant information and keeping redundant copies consistent.&lt;/p&gt;
&lt;p&gt;Once data becomes distributed with techniques such as &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#federation&quot;&gt;federation&lt;/a&gt; and &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#sharding&quot;&gt;sharding&lt;/a&gt;, managing joins across data centers further increases complexity. Denormalization might circumvent the need for such complex joins.&lt;/p&gt;
&lt;p&gt;In most systems, reads can heavily outnumber writes 100:1 or even 1000:1. A read resulting in a complex database join can be very expensive, spending a significant amount of time on disk operations.&lt;/p&gt;
&lt;h5&gt;Disadvantage(s): denormalization&lt;/h5&gt;
&lt;ul&gt;&lt;li&gt;Data is duplicated.&lt;/li&gt;
&lt;li&gt;Constraints can help redundant copies of information stay in sync, which increases complexity of the database design.&lt;/li&gt;
&lt;li&gt;A denormalized database under heavy write load might perform worse than its normalized counterpart.&lt;/li&gt;
&lt;/ul&gt;&lt;h6&gt;Source(s) and further reading: denormalization&lt;/h6&gt;
&lt;h4&gt;SQL tuning&lt;/h4&gt;
&lt;p&gt;SQL tuning is a broad topic and many &lt;a href=&quot;https://www.amazon.com/s/ref=nb_sb_noss_2?url=search-alias%3Daps&amp;amp;field-keywords=sql+tuning&quot; rel=&quot;nofollow&quot;&gt;books&lt;/a&gt; have been written as reference.&lt;/p&gt;
&lt;p&gt;It's important to &lt;strong&gt;benchmark&lt;/strong&gt; and &lt;strong&gt;profile&lt;/strong&gt; to simulate and uncover bottlenecks.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Benchmark&lt;/strong&gt; - Simulate high-load situations with tools such as &lt;a href=&quot;http://httpd.apache.org/docs/2.2/programs/ab.html&quot; rel=&quot;nofollow&quot;&gt;ab&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Profile&lt;/strong&gt; - Enable tools such as the &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.7/en/slow-query-log.html&quot; rel=&quot;nofollow&quot;&gt;slow query log&lt;/a&gt; to help track performance issues.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Benchmarking and profiling might point you to the following optimizations.&lt;/p&gt;
&lt;h5&gt;Tighten up the schema&lt;/h5&gt;
&lt;ul&gt;&lt;li&gt;MySQL dumps to disk in contiguous blocks for fast access.&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;CHAR&lt;/code&gt; instead of &lt;code&gt;VARCHAR&lt;/code&gt; for fixed-length fields.
&lt;ul&gt;&lt;li&gt;&lt;code&gt;CHAR&lt;/code&gt; effectively allows for fast, random access, whereas with &lt;code&gt;VARCHAR&lt;/code&gt;, you must find the end of a string before moving onto the next one.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;TEXT&lt;/code&gt; for large blocks of text such as blog posts. &lt;code&gt;TEXT&lt;/code&gt; also allows for boolean searches. Using a &lt;code&gt;TEXT&lt;/code&gt; field results in storing a pointer on disk that is used to locate the text block.&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;INT&lt;/code&gt; for larger numbers up to 2^32 or 4 billion.&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;DECIMAL&lt;/code&gt; for currency to avoid floating point representation errors.&lt;/li&gt;
&lt;li&gt;Avoid storing large &lt;code&gt;BLOBS&lt;/code&gt;, store the location of where to get the object instead.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VARCHAR(255)&lt;/code&gt; is the largest number of characters that can be counted in an 8 bit number, often maximizing the use of a byte in some RDBMS.&lt;/li&gt;
&lt;li&gt;Set the &lt;code&gt;NOT NULL&lt;/code&gt; constraint where applicable to &lt;a href=&quot;http://stackoverflow.com/questions/1017239/how-do-null-values-affect-performance-in-a-database-search&quot; rel=&quot;nofollow&quot;&gt;improve search performance&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;h5&gt;Use good indices&lt;/h5&gt;
&lt;ul&gt;&lt;li&gt;Columns that you are querying (&lt;code&gt;SELECT&lt;/code&gt;, &lt;code&gt;GROUP BY&lt;/code&gt;, &lt;code&gt;ORDER BY&lt;/code&gt;, &lt;code&gt;JOIN&lt;/code&gt;) could be faster with indices.&lt;/li&gt;
&lt;li&gt;Indices are usually represented as self-balancing &lt;a href=&quot;https://en.wikipedia.org/wiki/B-tree&quot; rel=&quot;nofollow&quot;&gt;B-tree&lt;/a&gt; that keeps data sorted and allows searches, sequential access, insertions, and deletions in logarithmic time.&lt;/li&gt;
&lt;li&gt;Placing an index can keep the data in memory, requiring more space.&lt;/li&gt;
&lt;li&gt;Writes could also be slower since the index also needs to be updated.&lt;/li&gt;
&lt;li&gt;When loading large amounts of data, it might be faster to disable indices, load the data, then rebuild the indices.&lt;/li&gt;
&lt;/ul&gt;&lt;h5&gt;Avoid expensive joins&lt;/h5&gt;
&lt;h5&gt;Partition tables&lt;/h5&gt;
&lt;ul&gt;&lt;li&gt;Break up a table by putting hot spots in a separate table to help keep it in memory.&lt;/li&gt;
&lt;/ul&gt;&lt;h5&gt;Tune the query cache&lt;/h5&gt;
&lt;h5&gt;Source(s) and further reading: SQL tuning&lt;/h5&gt;
&lt;h3&gt;NoSQL&lt;/h3&gt;
&lt;p&gt;NoSQL is a collection of data items represented in a &lt;strong&gt;key-value store&lt;/strong&gt;, &lt;strong&gt;document-store&lt;/strong&gt;, &lt;strong&gt;wide column store&lt;/strong&gt;, or a &lt;strong&gt;graph database&lt;/strong&gt;. Data is denormalized, and joins are generally done in the application code. Most NoSQL stores lack true ACID transactions and favor &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#eventual-consistency&quot;&gt;eventual consistency&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;BASE&lt;/strong&gt; is often used to describe the properties of NoSQL databases. In comparison with the &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#cap-theorem&quot;&gt;CAP Theorem&lt;/a&gt;, BASE chooses availability over consistency.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Basically available&lt;/strong&gt; - the system guarantees availability.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Soft state&lt;/strong&gt; - the state of the system may change over time, even without input.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Eventual consistency&lt;/strong&gt; - the system will become consistent over a period of time, given that the system doesn't receive input during that period.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;In addition to choosing between &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#sql-or-nosql&quot;&gt;SQL or NoSQL&lt;/a&gt;, it is helpful to understand which type of NoSQL database best fits your use case(s). We'll review &lt;strong&gt;key-value stores&lt;/strong&gt;, &lt;strong&gt;document-stores&lt;/strong&gt;, &lt;strong&gt;wide column stores&lt;/strong&gt;, and &lt;strong&gt;graph databases&lt;/strong&gt; in the next section.&lt;/p&gt;
&lt;h4&gt;Key-value store&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Abstraction: hash table&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A key-value store generally allows for O(1) reads and writes and is often backed by memory or SSD. Data stores can maintain keys in &lt;a href=&quot;https://en.wikipedia.org/wiki/Lexicographical_order&quot; rel=&quot;nofollow&quot;&gt;lexicographic order&lt;/a&gt;, allowing efficient retrieval of key ranges. Key-value stores can allow for storing of metadata with a value.&lt;/p&gt;
&lt;p&gt;Key-value stores provide high performance and are often used for simple data models or for rapidly-changing data, such as an in-memory cache layer. Since they offer only a limited set of operations, complexity is shifted to the application layer if additional operations are needed.&lt;/p&gt;
&lt;p&gt;A key-value store is the basis for more complex systems such as a document store, and in some cases, a graph database.&lt;/p&gt;
&lt;h5&gt;Source(s) and further reading: key-value store&lt;/h5&gt;
&lt;h4&gt;Document store&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Abstraction: key-value store with documents stored as values&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A document store is centered around documents (XML, JSON, binary, etc), where a document stores all information for a given object. Document stores provide APIs or a query language to query based on the internal structure of the document itself. &lt;em&gt;Note, many key-value stores include features for working with a value's metadata, blurring the lines between these two storage types.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Based on the underlying implementation, documents are organized in either collections, tags, metadata, or directories. Although documents can be organized or grouped together, documents may have fields that are completely different from each other.&lt;/p&gt;
&lt;p&gt;Some document stores like &lt;a href=&quot;https://www.mongodb.com/mongodb-architecture&quot; rel=&quot;nofollow&quot;&gt;MongoDB&lt;/a&gt; and &lt;a href=&quot;https://blog.couchdb.org/2016/08/01/couchdb-2-0-architecture/&quot; rel=&quot;nofollow&quot;&gt;CouchDB&lt;/a&gt; also provide a SQL-like language to perform complex queries. &lt;a href=&quot;http://www.read.seas.harvard.edu/%7Ekohler/class/cs239-w08/decandia07dynamo.pdf&quot; rel=&quot;nofollow&quot;&gt;DynamoDB&lt;/a&gt; supports both key-values and documents.&lt;/p&gt;
&lt;p&gt;Document stores provide high flexibility and are often used for working with occasionally changing data.&lt;/p&gt;
&lt;h5&gt;Source(s) and further reading: document store&lt;/h5&gt;
&lt;h4&gt;Wide column store&lt;/h4&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/823668b07b4bff50574e934273c9244e4e5017d6/687474703a2f2f692e696d6775722e636f6d2f6e3136694f476b2e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/823668b07b4bff50574e934273c9244e4e5017d6/687474703a2f2f692e696d6775722e636f6d2f6e3136694f476b2e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/n16iOGk.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://blog.grio.com/2015/11/sql-nosql-a-brief-history.html&quot; rel=&quot;nofollow&quot;&gt;Source: SQL &amp;amp; NoSQL, a brief history&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Abstraction: nested map &lt;code&gt;ColumnFamily&amp;lt;RowKey, Columns&amp;lt;ColKey, Value, Timestamp&amp;gt;&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A wide column store's basic unit of data is a column (name/value pair). A column can be grouped in column families (analogous to a SQL table). Super column families further group column families. You can access each column independently with a row key, and columns with the same row key form a row. Each value contains a timestamp for versioning and for conflict resolution.&lt;/p&gt;
&lt;p&gt;Google introduced &lt;a href=&quot;http://www.read.seas.harvard.edu/%7Ekohler/class/cs239-w08/chang06bigtable.pdf&quot; rel=&quot;nofollow&quot;&gt;Bigtable&lt;/a&gt; as the first wide column store, which influenced the open-source &lt;a href=&quot;https://www.mapr.com/blog/in-depth-look-hbase-architecture&quot; rel=&quot;nofollow&quot;&gt;HBase&lt;/a&gt; often-used in the Hadoop ecosystem, and &lt;a href=&quot;http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/architecture/architectureIntro_c.html&quot; rel=&quot;nofollow&quot;&gt;Cassandra&lt;/a&gt; from Facebook. Stores such as BigTable, HBase, and Cassandra maintain keys in lexicographic order, allowing efficient retrieval of selective key ranges.&lt;/p&gt;
&lt;p&gt;Wide column stores offer high availability and high scalability. They are often used for very large data sets.&lt;/p&gt;
&lt;h5&gt;Source(s) and further reading: wide column store&lt;/h5&gt;
&lt;h4&gt;Graph database&lt;/h4&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/bf6508b65e98a7210d9861515833afa0d9434436/687474703a2f2f692e696d6775722e636f6d2f664e636c3635672e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/bf6508b65e98a7210d9861515833afa0d9434436/687474703a2f2f692e696d6775722e636f6d2f664e636c3635672e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/fNcl65g.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/File:GraphDatabase_PropertyGraph.png&quot; rel=&quot;nofollow&quot;&gt;Source: Graph database&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Abstraction: graph&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In a graph database, each node is a record and each arc is a relationship between two nodes. Graph databases are optimized to represent complex relationships with many foreign keys or many-to-many relationships.&lt;/p&gt;
&lt;p&gt;Graphs databases offer high performance for data models with complex relationships, such as a social network. They are relatively new and are not yet widely-used; it might be more difficult to find development tools and resources. Many graphs can only be accessed with &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#representational-state-transfer-rest&quot;&gt;REST APIs&lt;/a&gt;.&lt;/p&gt;
&lt;h5&gt;Source(s) and further reading: graph&lt;/h5&gt;
&lt;h4&gt;Source(s) and further reading: NoSQL&lt;/h4&gt;
&lt;h3&gt;SQL or NoSQL&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/a6e2e844765c9d5382d9c9b64ef7693977981646/687474703a2f2f692e696d6775722e636f6d2f775847714735662e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/a6e2e844765c9d5382d9c9b64ef7693977981646/687474703a2f2f692e696d6775722e636f6d2f775847714735662e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/wXGqG5f.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;https://www.infoq.com/articles/Transition-RDBMS-NoSQL/&quot; rel=&quot;nofollow&quot;&gt;Source: Transitioning from RDBMS to NoSQL&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Reasons for &lt;strong&gt;SQL&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Structured data&lt;/li&gt;
&lt;li&gt;Strict schema&lt;/li&gt;
&lt;li&gt;Relational data&lt;/li&gt;
&lt;li&gt;Need for complex joins&lt;/li&gt;
&lt;li&gt;Transactions&lt;/li&gt;
&lt;li&gt;Clear patterns for scaling&lt;/li&gt;
&lt;li&gt;More established: developers, community, code, tools, etc&lt;/li&gt;
&lt;li&gt;Lookups by index are very fast&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Reasons for &lt;strong&gt;NoSQL&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Semi-structured data&lt;/li&gt;
&lt;li&gt;Dynamic or flexible schema&lt;/li&gt;
&lt;li&gt;Non-relational data&lt;/li&gt;
&lt;li&gt;No need for complex joins&lt;/li&gt;
&lt;li&gt;Store many TB (or PB) of data&lt;/li&gt;
&lt;li&gt;Very data intensive workload&lt;/li&gt;
&lt;li&gt;Very high throughput for IOPS&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Sample data well-suited for NoSQL:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Rapid ingest of clickstream and log data&lt;/li&gt;
&lt;li&gt;Leaderboard or scoring data&lt;/li&gt;
&lt;li&gt;Temporary data, such as a shopping cart&lt;/li&gt;
&lt;li&gt;Frequently accessed ('hot') tables&lt;/li&gt;
&lt;li&gt;Metadata/lookup tables&lt;/li&gt;
&lt;/ul&gt;&lt;h5&gt;Source(s) and further reading: SQL or NoSQL&lt;/h5&gt;
&lt;h2&gt;Cache&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/7acedde6aa7853baf2eb4a53f88e2595ebe43756/687474703a2f2f692e696d6775722e636f6d2f51367a32344c612e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/7acedde6aa7853baf2eb4a53f88e2595ebe43756/687474703a2f2f692e696d6775722e636f6d2f51367a32344c612e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/Q6z24La.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://horicky.blogspot.com/2010/10/scalable-system-design-patterns.html&quot; rel=&quot;nofollow&quot;&gt;Source: Scalable system design patterns&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Caching improves page load times and can reduce the load on your servers and databases. In this model, the dispatcher will first lookup if the request has been made before and try to find the previous result to return, in order to save the actual execution.&lt;/p&gt;
&lt;p&gt;Databases often benefit from a uniform distribution of reads and writes across its partitions. Popular items can skew the distribution, causing bottlenecks. Putting a cache in front of a database can help absorb uneven loads and spikes in traffic.&lt;/p&gt;
&lt;h3&gt;Client caching&lt;/h3&gt;
&lt;p&gt;Caches can be located on the client side (OS or browser), &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#reverse-proxy&quot;&gt;server side&lt;/a&gt;, or in a distinct cache layer.&lt;/p&gt;
&lt;h3&gt;CDN caching&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer#content-delivery-network&quot;&gt;CDNs&lt;/a&gt; are considered a type of cache.&lt;/p&gt;
&lt;h3&gt;Web server caching&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer#reverse-proxy-web-server&quot;&gt;Reverse proxies&lt;/a&gt; and caches such as &lt;a href=&quot;https://www.varnish-cache.org/&quot; rel=&quot;nofollow&quot;&gt;Varnish&lt;/a&gt; can serve static and dynamic content directly. Web servers can also cache requests, returning responses without having to contact application servers.&lt;/p&gt;
&lt;h3&gt;Database caching&lt;/h3&gt;
&lt;p&gt;Your database usually includes some level of caching in a default configuration, optimized for a generic use case. Tweaking these settings for specific usage patterns can further boost performance.&lt;/p&gt;
&lt;h3&gt;Application caching&lt;/h3&gt;
&lt;p&gt;In-memory caches such as Memcached and Redis are key-value stores between your application and your data storage. Since the data is held in RAM, it is much faster than typical databases where data is stored on disk. RAM is more limited than disk, so &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_algorithms&quot; rel=&quot;nofollow&quot;&gt;cache invalidation&lt;/a&gt; algorithms such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_algorithms#Least_Recently_Used&quot; rel=&quot;nofollow&quot;&gt;least recently used (LRU)&lt;/a&gt; can help invalidate 'cold' entries and keep 'hot' data in RAM.&lt;/p&gt;
&lt;p&gt;Redis has the following additional features:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Persistence option&lt;/li&gt;
&lt;li&gt;Built-in data structures such as sorted sets and lists&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;There are multiple levels you can cache that fall into two general categories: &lt;strong&gt;database queries&lt;/strong&gt; and &lt;strong&gt;objects&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Row level&lt;/li&gt;
&lt;li&gt;Query-level&lt;/li&gt;
&lt;li&gt;Fully-formed serializable objects&lt;/li&gt;
&lt;li&gt;Fully-rendered HTML&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Generally, you should try to avoid file-based caching, as it makes cloning and auto-scaling more difficult.&lt;/p&gt;
&lt;h3&gt;Caching at the database query level&lt;/h3&gt;
&lt;p&gt;Whenever you query the database, hash the query as a key and store the result to the cache. This approach suffers from expiration issues:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Hard to delete a cached result with complex queries&lt;/li&gt;
&lt;li&gt;If one piece of data changes such as a table cell, you need to delete all cached queries that might include the changed cell&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Caching at the object level&lt;/h3&gt;
&lt;p&gt;See your data as an object, similar to what you do with your application code. Have your application assemble the dataset from the database into a class instance or a data structure(s):&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Remove the object from cache if its underlying data has changed&lt;/li&gt;
&lt;li&gt;Allows for asynchronous processing: workers assemble objects by consuming the latest cached object&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Suggestions of what to cache:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;User sessions&lt;/li&gt;
&lt;li&gt;Fully rendered web pages&lt;/li&gt;
&lt;li&gt;Activity streams&lt;/li&gt;
&lt;li&gt;User graph data&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;When to update the cache&lt;/h3&gt;
&lt;p&gt;Since you can only store a limited amount of data in cache, you'll need to determine which cache update strategy works best for your use case.&lt;/p&gt;
&lt;h4&gt;Cache-aside&lt;/h4&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/7f5934e49a678b67f65e5ed53134bc258b007ebb/687474703a2f2f692e696d6775722e636f6d2f4f4e6a4f52716b2e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/7f5934e49a678b67f65e5ed53134bc258b007ebb/687474703a2f2f692e696d6775722e636f6d2f4f4e6a4f52716b2e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/ONjORqk.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://www.slideshare.net/tmatyashovsky/from-cache-to-in-memory-data-grid-introduction-to-hazelcast&quot; rel=&quot;nofollow&quot;&gt;Source: From cache to in-memory data grid&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The application is responsible for reading and writing from storage. The cache does not interact with storage directly. The application does the following:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Look for entry in cache, resulting in a cache miss&lt;/li&gt;
&lt;li&gt;Load entry from the database&lt;/li&gt;
&lt;li&gt;Add entry to cache&lt;/li&gt;
&lt;li&gt;Return entry&lt;/li&gt;
&lt;/ul&gt;&lt;pre&gt;
&lt;code&gt;def get_user(self, user_id):
    user = cache.get(&quot;user.{0}&quot;, user_id)
    if user is None:
        user = db.query(&quot;SELECT * FROM users WHERE user_id = {0}&quot;, user_id)
        if user is not None:
            key = &quot;user.{0}&quot;.format(user_id)
            cache.set(key, json.dumps(user))
    return user
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;a href=&quot;https://memcached.org/&quot; rel=&quot;nofollow&quot;&gt;Memcached&lt;/a&gt; is generally used in this manner.&lt;/p&gt;
&lt;p&gt;Subsequent reads of data added to cache are fast. Cache-aside is also referred to as lazy loading. Only requested data is cached, which avoids filling up the cache with data that isn't requested.&lt;/p&gt;
&lt;h5&gt;Disadvantage(s): cache-aside&lt;/h5&gt;
&lt;ul&gt;&lt;li&gt;Each cache miss results in three trips, which can cause a noticeable delay.&lt;/li&gt;
&lt;li&gt;Data can become stale if it is updated in the database. This issue is mitigated by setting a time-to-live (TTL) which forces an update of the cache entry, or by using write-through.&lt;/li&gt;
&lt;li&gt;When a node fails, it is replaced by a new, empty node, increasing latency.&lt;/li&gt;
&lt;/ul&gt;&lt;h4&gt;Write-through&lt;/h4&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/56b870f4d199335ccdbc98b989ef6511ed14f0e2/687474703a2f2f692e696d6775722e636f6d2f3076426330684e2e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/56b870f4d199335ccdbc98b989ef6511ed14f0e2/687474703a2f2f692e696d6775722e636f6d2f3076426330684e2e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/0vBc0hN.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://www.slideshare.net/jboner/scalability-availability-stability-patterns/&quot; rel=&quot;nofollow&quot;&gt;Source: Scalability, availability, stability, patterns&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The application uses the cache as the main data store, reading and writing data to it, while the cache is responsible for reading and writing to the database:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Application adds/updates entry in cache&lt;/li&gt;
&lt;li&gt;Cache synchronously writes entry to data store&lt;/li&gt;
&lt;li&gt;Return&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Application code:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;set_user(12345, {&quot;foo&quot;:&quot;bar&quot;})
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Cache code:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;def set_user(user_id, values):
    user = db.query(&quot;UPDATE Users WHERE id = {0}&quot;, user_id, values)
    cache.set(user_id, user)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Write-through is a slow overall operation due to the write operation, but subsequent reads of just written data are fast. Users are generally more tolerant of latency when updating data than reading data. Data in the cache is not stale.&lt;/p&gt;
&lt;h5&gt;Disadvantage(s): write through&lt;/h5&gt;
&lt;ul&gt;&lt;li&gt;When a new node is created due to failure or scaling, the new node will not cache entries until the entry is updated in the database. Cache-aside in conjunction with write through can mitigate this issue.&lt;/li&gt;
&lt;li&gt;Most data written might never read, which can be minimized with a TTL.&lt;/li&gt;
&lt;/ul&gt;&lt;h4&gt;Write-behind (write-back)&lt;/h4&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/8aa9f1a2f050c1422898bb5e82f1f01773334e22/687474703a2f2f692e696d6775722e636f6d2f72675372766a472e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/8aa9f1a2f050c1422898bb5e82f1f01773334e22/687474703a2f2f692e696d6775722e636f6d2f72675372766a472e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/rgSrvjG.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://www.slideshare.net/jboner/scalability-availability-stability-patterns/&quot; rel=&quot;nofollow&quot;&gt;Source: Scalability, availability, stability, patterns&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In write-behind, the application does the following:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Add/update entry in cache&lt;/li&gt;
&lt;li&gt;Asynchronously write entry to the data store, improving write performance&lt;/li&gt;
&lt;/ul&gt;&lt;h5&gt;Disadvantage(s): write-behind&lt;/h5&gt;
&lt;ul&gt;&lt;li&gt;There could be data loss if the cache goes down prior to its contents hitting the data store.&lt;/li&gt;
&lt;li&gt;It is more complex to implement write-behind than it is to implement cache-aside or write-through.&lt;/li&gt;
&lt;/ul&gt;&lt;h4&gt;Refresh-ahead&lt;/h4&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/49dcb54307763b4f56d61a4a1369826e2e7d52e4/687474703a2f2f692e696d6775722e636f6d2f6b78746a7167452e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/49dcb54307763b4f56d61a4a1369826e2e7d52e4/687474703a2f2f692e696d6775722e636f6d2f6b78746a7167452e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/kxtjqgE.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://www.slideshare.net/tmatyashovsky/from-cache-to-in-memory-data-grid-introduction-to-hazelcast&quot; rel=&quot;nofollow&quot;&gt;Source: From cache to in-memory data grid&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You can configure the cache to automatically refresh any recently accessed cache entry prior to its expiration.&lt;/p&gt;
&lt;p&gt;Refresh-ahead can result in reduced latency vs read-through if the cache can accurately predict which items are likely to be needed in the future.&lt;/p&gt;
&lt;h5&gt;Disadvantage(s): refresh-ahead&lt;/h5&gt;
&lt;ul&gt;&lt;li&gt;Not accurately predicting which items are likely to be needed in the future can result in reduced performance than without refresh-ahead.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Disadvantage(s): cache&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;Need to maintain consistency between caches and the source of truth such as the database through &lt;a href=&quot;https://en.wikipedia.org/wiki/Cache_algorithms&quot; rel=&quot;nofollow&quot;&gt;cache invalidation&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Cache invalidation is a difficult problem, there is additional complexity associated with when to update the cache.&lt;/li&gt;
&lt;li&gt;Need to make application changes such as adding Redis or memcached.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Source(s) and further reading&lt;/h3&gt;
&lt;h2&gt;Asynchronism&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/c01ec137453216bbc188e3a8f16da39ec9131234/687474703a2f2f692e696d6775722e636f6d2f353447597353782e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/c01ec137453216bbc188e3a8f16da39ec9131234/687474703a2f2f692e696d6775722e636f6d2f353447597353782e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/54GYsSx.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://lethain.com/introduction-to-architecting-systems-for-scale/#platform_layer&quot; rel=&quot;nofollow&quot;&gt;Source: Intro to architecting systems for scale&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Asynchronous workflows help reduce request times for expensive operations that would otherwise be performed in-line. They can also help by doing time-consuming work in advance, such as periodic aggregation of data.&lt;/p&gt;
&lt;h3&gt;Message queues&lt;/h3&gt;
&lt;p&gt;Message queues receive, hold, and deliver messages. If an operation is too slow to perform inline, you can use a message queue with the following workflow:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;An application publishes a job to the queue, then notifies the user of job status&lt;/li&gt;
&lt;li&gt;A worker picks up the job from the queue, processes it, then signals the job is complete&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The user is not blocked and the job is processed in the background. During this time, the client might optionally do a small amount of processing to make it seem like the task has completed. For example, if posting a tweet, the tweet could be instantly posted to your timeline, but it could take some time before your tweet is actually delivered to all of your followers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Redis&lt;/strong&gt; is useful as a simple message broker but messages can be lost.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RabbitMQ&lt;/strong&gt; is popular but requires you to adapt to the 'AMQP' protocol and manage your own nodes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Amazon SQS&lt;/strong&gt;, is hosted but can have high latency and has the possibility of messages being delivered twice.&lt;/p&gt;
&lt;h3&gt;Task queues&lt;/h3&gt;
&lt;p&gt;Tasks queues receive tasks and their related data, runs them, then delivers their results. They can support scheduling and can be used to run computationally-intensive jobs in the background.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Celery&lt;/strong&gt; has support for scheduling and primarily has python support.&lt;/p&gt;
&lt;h3&gt;Back pressure&lt;/h3&gt;
&lt;p&gt;If queues start to grow significantly, the queue size can become larger than memory, resulting in cache misses, disk reads, and even slower performance. &lt;a href=&quot;http://mechanical-sympathy.blogspot.com/2012/05/apply-back-pressure-when-overloaded.html&quot; rel=&quot;nofollow&quot;&gt;Back pressure&lt;/a&gt; can help by limiting the queue size, thereby maintaining a high throughput rate and good response times for jobs already in the queue. Once the queue fills up, clients get a server busy or HTTP 503 status code to try again later. Clients can retry the request at a later time, perhaps with &lt;a href=&quot;https://en.wikipedia.org/wiki/Exponential_backoff&quot; rel=&quot;nofollow&quot;&gt;exponential backoff&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Disadvantage(s): asynchronism&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;Use cases such as inexpensive calculations and realtime workflows might be better suited for synchronous operations, as introducing queues can add delays and complexity.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Source(s) and further reading&lt;/h3&gt;
&lt;h2&gt;Communication&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/1d761d5688d28ce1fb12a0f1c8191bca96eece4c/687474703a2f2f692e696d6775722e636f6d2f354b656f6351732e6a7067&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/1d761d5688d28ce1fb12a0f1c8191bca96eece4c/687474703a2f2f692e696d6775722e636f6d2f354b656f6351732e6a7067&quot; data-canonical-src=&quot;http://i.imgur.com/5KeocQs.jpg&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://www.escotal.com/osilayer.html&quot; rel=&quot;nofollow&quot;&gt;Source: OSI 7 layer model&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Hypertext transfer protocol (HTTP)&lt;/h3&gt;
&lt;p&gt;HTTP is a method for encoding and transporting data between a client and a server. It is a request/response protocol: clients issue requests and servers issue responses with relevant content and completion status info about the request. HTTP is self-contained, allowing requests and responses to flow through many intermediate routers and servers that perform load balancing, caching, encryption, and compression.&lt;/p&gt;
&lt;p&gt;A basic HTTP request consists of a verb (method) and a resource (endpoint). Below are common HTTP verbs:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Verb&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Idempotent*&lt;/th&gt;
&lt;th&gt;Safe&lt;/th&gt;
&lt;th&gt;Cacheable&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;GET&lt;/td&gt;
&lt;td&gt;Reads a resource&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;POST&lt;/td&gt;
&lt;td&gt;Creates a resource or trigger a process that handles data&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes if response contains freshness info&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PUT&lt;/td&gt;
&lt;td&gt;Creates or replace a resource&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PATCH&lt;/td&gt;
&lt;td&gt;Partially updates a resource&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes if response contains freshness info&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DELETE&lt;/td&gt;
&lt;td&gt;Deletes a resource&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;*Can be called many times without different outcomes.&lt;/p&gt;
&lt;p&gt;HTTP is an application layer protocol relying on lower-level protocols such as &lt;strong&gt;TCP&lt;/strong&gt; and &lt;strong&gt;UDP&lt;/strong&gt;.&lt;/p&gt;
&lt;h4&gt;Source(s) and further reading: HTTP&lt;/h4&gt;
&lt;h3&gt;Transmission control protocol (TCP)&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/821620cf6aa83566f4def561e754e5991480ca8d/687474703a2f2f692e696d6775722e636f6d2f4a6441736476472e6a7067&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/821620cf6aa83566f4def561e754e5991480ca8d/687474703a2f2f692e696d6775722e636f6d2f4a6441736476472e6a7067&quot; data-canonical-src=&quot;http://i.imgur.com/JdAsdvG.jpg&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://www.wildbunny.co.uk/blog/2012/10/09/how-to-make-a-multi-player-game-part-1/&quot; rel=&quot;nofollow&quot;&gt;Source: How to make a multiplayer game&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;TCP is a connection-oriented protocol over an &lt;a href=&quot;https://en.wikipedia.org/wiki/Internet_Protocol&quot; rel=&quot;nofollow&quot;&gt;IP network&lt;/a&gt;. Connection is established and terminated using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Handshaking&quot; rel=&quot;nofollow&quot;&gt;handshake&lt;/a&gt;. All packets sent are guaranteed to reach the destination in the original order and without corruption through:&lt;/p&gt;
&lt;p&gt;If the sender does not receive a correct response, it will resend the packets. If there are multiple timeouts, the connection is dropped. TCP also implements &lt;a href=&quot;https://en.wikipedia.org/wiki/Flow_control_(data)&quot; rel=&quot;nofollow&quot;&gt;flow control&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Network_congestion#Congestion_control&quot; rel=&quot;nofollow&quot;&gt;congestion control&lt;/a&gt;. These guarantees cause delays and generally result in less efficient transmission than UDP.&lt;/p&gt;
&lt;p&gt;To ensure high throughput, web servers can keep a large number of TCP connections open, resulting in high memory usage. It can be expensive to have a large number of open connections between web server threads and say, a &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#memcached&quot;&gt;memcached&lt;/a&gt; server. &lt;a href=&quot;https://en.wikipedia.org/wiki/Connection_pool&quot; rel=&quot;nofollow&quot;&gt;Connection pooling&lt;/a&gt; can help in addition to switching to UDP where applicable.&lt;/p&gt;
&lt;p&gt;TCP is useful for applications that require high reliability but are less time critical. Some examples include web servers, database info, SMTP, FTP, and SSH.&lt;/p&gt;
&lt;p&gt;Use TCP over UDP when:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;You need all of the data to arrive intact&lt;/li&gt;
&lt;li&gt;You want to automatically make a best estimate use of the network throughput&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;User datagram protocol (UDP)&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/47eb14c0a2dff2166f8781a6ce8c7f33d4c33da8/687474703a2f2f692e696d6775722e636f6d2f797a44724a74412e6a7067&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/47eb14c0a2dff2166f8781a6ce8c7f33d4c33da8/687474703a2f2f692e696d6775722e636f6d2f797a44724a74412e6a7067&quot; data-canonical-src=&quot;http://i.imgur.com/yzDrJtA.jpg&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://www.wildbunny.co.uk/blog/2012/10/09/how-to-make-a-multi-player-game-part-1/&quot; rel=&quot;nofollow&quot;&gt;Source: How to make a multiplayer game&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;UDP is connectionless. Datagrams (analogous to packets) are guaranteed only at the datagram level. Datagrams might reach their destination out of order or not at all. UDP does not support congestion control. Without the guarantees that TCP support, UDP is generally more efficient.&lt;/p&gt;
&lt;p&gt;UDP can broadcast, sending datagrams to all devices on the subnet. This is useful with &lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_Host_Configuration_Protocol&quot; rel=&quot;nofollow&quot;&gt;DHCP&lt;/a&gt; because the client has not yet received an IP address, thus preventing a way for TCP to stream without the IP address.&lt;/p&gt;
&lt;p&gt;UDP is less reliable but works well in real time use cases such as VoIP, video chat, streaming, and realtime multiplayer games.&lt;/p&gt;
&lt;p&gt;Use UDP over TCP when:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;You need the lowest latency&lt;/li&gt;
&lt;li&gt;Late data is worse than loss of data&lt;/li&gt;
&lt;li&gt;You want to implement your own error correction&lt;/li&gt;
&lt;/ul&gt;&lt;h4&gt;Source(s) and further reading: TCP and UDP&lt;/h4&gt;
&lt;h3&gt;Remote procedure call (RPC)&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/1a3d7771c0b0a7816d0533fffeb6eeeb442d9945/687474703a2f2f692e696d6775722e636f6d2f6946344d6b62352e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/1a3d7771c0b0a7816d0533fffeb6eeeb442d9945/687474703a2f2f692e696d6775722e636f6d2f6946344d6b62352e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/iF4Mkb5.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;http://www.puncsky.com/blog/2016/02/14/crack-the-system-design-interview/&quot; rel=&quot;nofollow&quot;&gt;Source: Crack the system design interview&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In an RPC, a client causes a procedure to execute on a different address space, usually a remote server. The procedure is coded as if it were a local procedure call, abstracting away the details of how to communicate with the server from the client program. Remote calls are usually slower and less reliable than local calls so it is helpful to distinguish RPC calls from local calls. Popular RPC frameworks include &lt;a href=&quot;https://developers.google.com/protocol-buffers/&quot; rel=&quot;nofollow&quot;&gt;Protobuf&lt;/a&gt;, &lt;a href=&quot;https://thrift.apache.org/&quot; rel=&quot;nofollow&quot;&gt;Thrift&lt;/a&gt;, and &lt;a href=&quot;https://avro.apache.org/docs/current/&quot; rel=&quot;nofollow&quot;&gt;Avro&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;RPC is a request-response protocol:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Client program&lt;/strong&gt; - Calls the client stub procedure. The parameters are pushed onto the stack like a local procedure call.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Client stub procedure&lt;/strong&gt; - Marshals (packs) procedure id and arguments into a request message.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Client communication module&lt;/strong&gt; - OS sends the message from the client to the server.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Server communication module&lt;/strong&gt; - OS passes the incoming packets to the server stub procedure.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Server stub procedure&lt;/strong&gt; - Unmarshalls the results, calls the server procedure matching the procedure id and passes the given arguments.&lt;/li&gt;
&lt;li&gt;The server response repeats the steps above in reverse order.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Sample RPC calls:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;GET /someoperation?data=anId

POST /anotheroperation
{
  &quot;data&quot;:&quot;anId&quot;;
  &quot;anotherdata&quot;: &quot;another value&quot;
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;RPC is focused on exposing behaviors. RPCs are often used for performance reasons with internal communications, as you can hand-craft native calls to better fit your use cases.&lt;/p&gt;
&lt;p&gt;Choose a native library (aka SDK) when:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;You know your target platform.&lt;/li&gt;
&lt;li&gt;You want to control how your &quot;logic&quot; is accessed.&lt;/li&gt;
&lt;li&gt;You want to control how error control happens off your library.&lt;/li&gt;
&lt;li&gt;Performance and end user experience is your primary concern.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;HTTP APIs following &lt;strong&gt;REST&lt;/strong&gt; tend to be used more often for public APIs.&lt;/p&gt;
&lt;h4&gt;Disadvantage(s): RPC&lt;/h4&gt;
&lt;ul&gt;&lt;li&gt;RPC clients become tightly coupled to the service implementation.&lt;/li&gt;
&lt;li&gt;A new API must be defined for every new operation or use case.&lt;/li&gt;
&lt;li&gt;It can be difficult to debug RPC.&lt;/li&gt;
&lt;li&gt;You might not be able to leverage existing technologies out of the box. For example, it might require additional effort to ensure &lt;a href=&quot;http://etherealbits.com/2012/12/debunking-the-myths-of-rpc-rest/&quot; rel=&quot;nofollow&quot;&gt;RPC calls are properly cached&lt;/a&gt; on caching servers such as &lt;a href=&quot;http://www.squid-cache.org/&quot; rel=&quot;nofollow&quot;&gt;Squid&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Representational state transfer (REST)&lt;/h3&gt;
&lt;p&gt;REST is an architectural style enforcing a client/server model where the client acts on a set of resources managed by the server. The server provides a representation of resources and actions that can either manipulate or get a new representation of resources. All communication must be stateless and cacheable.&lt;/p&gt;
&lt;p&gt;There are four qualities of a RESTful interface:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Identify resources (URI in HTTP)&lt;/strong&gt; - use the same URI regardless of any operation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Change with representations (Verbs in HTTP)&lt;/strong&gt; - use verbs, headers, and body.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Self-descriptive error message (status response in HTTP)&lt;/strong&gt; - Use status codes, don't reinvent the wheel.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;http://restcookbook.com/Basics/hateoas/&quot; rel=&quot;nofollow&quot;&gt;HATEOAS&lt;/a&gt; (HTML interface for HTTP)&lt;/strong&gt; - your web service should be fully accessible in a browser.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Sample REST calls:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;GET /someresources/anId

PUT /someresources/anId
{&quot;anotherdata&quot;: &quot;another value&quot;}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;REST is focused on exposing data. It minimizes the coupling between client/server and is often used for public HTTP APIs. REST uses a more generic and uniform method of exposing resources through URIs, &lt;a href=&quot;https://github.com/for-GET/know-your-http-well/blob/master/headers.md&quot;&gt;representation through headers&lt;/a&gt;, and actions through verbs such as GET, POST, PUT, DELETE, and PATCH. Being stateless, REST is great for horizontal scaling and partitioning.&lt;/p&gt;
&lt;h4&gt;Disadvantage(s): REST&lt;/h4&gt;
&lt;ul&gt;&lt;li&gt;With REST being focused on exposing data, it might not be a good fit if resources are not naturally organized or accessed in a simple hierarchy. For example, returning all updated records from the past hour matching a particular set of events is not easily expressed as a path. With REST, it is likely to be implemented with a combination of URI path, query parameters, and possibly the request body.&lt;/li&gt;
&lt;li&gt;REST typically relies on a few verbs (GET, POST, PUT, DELETE, and PATCH) which sometimes doesn't fit your use case. For example, moving expired documents to the archive folder might not cleanly fit within these verbs.&lt;/li&gt;
&lt;li&gt;Fetching complicated resources with nested hierarchies requires multiple round trips between the client and server to render single views, e.g. fetching content of a blog entry and the comments on that entry. For mobile applications operating in variable network conditions, these multiple roundtrips are highly undesirable.&lt;/li&gt;
&lt;li&gt;Over time, more fields might be added to an API response and older clients will receive all new data fields, even those that they do not need, as a result, it bloats the payload size and leads to larger latencies.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;RPC and REST calls comparison&lt;/h3&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Operation&lt;/th&gt;
&lt;th&gt;RPC&lt;/th&gt;
&lt;th&gt;REST&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Signup&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /signup&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /persons&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Resign&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /resign&lt;br/&gt;{&lt;br/&gt;&quot;personid&quot;: &quot;1234&quot;&lt;br/&gt;}&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;DELETE&lt;/strong&gt; /persons/1234&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Read a person&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /readPerson?personid=1234&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /persons/1234&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Read a person’s items list&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /readUsersItemsList?personid=1234&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /persons/1234/items&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Add an item to a person’s items&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /addItemToUsersItemsList&lt;br/&gt;{&lt;br/&gt;&quot;personid&quot;: &quot;1234&quot;;&lt;br/&gt;&quot;itemid&quot;: &quot;456&quot;&lt;br/&gt;}&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /persons/1234/items&lt;br/&gt;{&lt;br/&gt;&quot;itemid&quot;: &quot;456&quot;&lt;br/&gt;}&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Update an item&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /modifyItem&lt;br/&gt;{&lt;br/&gt;&quot;itemid&quot;: &quot;456&quot;;&lt;br/&gt;&quot;key&quot;: &quot;value&quot;&lt;br/&gt;}&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;PUT&lt;/strong&gt; /items/456&lt;br/&gt;{&lt;br/&gt;&quot;key&quot;: &quot;value&quot;&lt;br/&gt;}&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Delete an item&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /removeItem&lt;br/&gt;{&lt;br/&gt;&quot;itemid&quot;: &quot;456&quot;&lt;br/&gt;}&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;DELETE&lt;/strong&gt; /items/456&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p align=&quot;center&quot;&gt;&lt;em&gt;&lt;a href=&quot;https://apihandyman.io/do-you-really-know-why-you-prefer-rest-over-rpc/&quot; rel=&quot;nofollow&quot;&gt;Source: Do you really know why you prefer REST over RPC&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;Source(s) and further reading: REST and RPC&lt;/h4&gt;
&lt;h2&gt;Security&lt;/h2&gt;
&lt;p&gt;This section could use some updates. Consider &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#contributing&quot;&gt;contributing&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Security is a broad topic. Unless you have considerable experience, a security background, or are applying for a position that requires knowledge of security, you probably won't need to know more than the basics:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Encrypt in transit and at rest.&lt;/li&gt;
&lt;li&gt;Sanitize all user inputs or any input parameters exposed to user to prevent &lt;a href=&quot;https://en.wikipedia.org/wiki/Cross-site_scripting&quot; rel=&quot;nofollow&quot;&gt;XSS&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/SQL_injection&quot; rel=&quot;nofollow&quot;&gt;SQL injection&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Use parameterized queries to prevent SQL injection.&lt;/li&gt;
&lt;li&gt;Use the principle of &lt;a href=&quot;https://en.wikipedia.org/wiki/Principle_of_least_privilege&quot; rel=&quot;nofollow&quot;&gt;least privilege&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;Source(s) and further reading&lt;/h3&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;p&gt;You'll sometimes be asked to do 'back-of-the-envelope' estimates. For example, you might need to determine how long it will take to generate 100 image thumbnails from disk or how much memory a data structure will take. The &lt;strong&gt;Powers of two table&lt;/strong&gt; and &lt;strong&gt;Latency numbers every programmer should know&lt;/strong&gt; are handy references.&lt;/p&gt;
&lt;h3&gt;Powers of two table&lt;/h3&gt;
&lt;pre&gt;
&lt;code&gt;Power           Exact Value         Approx Value        Bytes
---------------------------------------------------------------
7                             128
8                             256
10                           1024   1 thousand           1 KB
16                         65,536                       64 KB
20                      1,048,576   1 million            1 MB
30                  1,073,741,824   1 billion            1 GB
32                  4,294,967,296                        4 GB
40              1,099,511,627,776   1 trillion           1 TB
&lt;/code&gt;
&lt;/pre&gt;
&lt;h4&gt;Source(s) and further reading&lt;/h4&gt;
&lt;h3&gt;Latency numbers every programmer should know&lt;/h3&gt;
&lt;pre&gt;
&lt;code&gt;Latency Comparison Numbers
--------------------------
L1 cache reference                           0.5 ns
Branch mispredict                            5   ns
L2 cache reference                           7   ns                      14x L1 cache
Mutex lock/unlock                          100   ns
Main memory reference                      100   ns                      20x L2 cache, 200x L1 cache
Compress 1K bytes with Zippy            10,000   ns       10 us
Send 1 KB bytes over 1 Gbps network     10,000   ns       10 us
Read 4 KB randomly from SSD*           150,000   ns      150 us          ~1GB/sec SSD
Read 1 MB sequentially from memory     250,000   ns      250 us
Round trip within same datacenter      500,000   ns      500 us
Read 1 MB sequentially from SSD*     1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory
Disk seek                           10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip
Read 1 MB sequentially from 1 Gbps  10,000,000   ns   10,000 us   10 ms  40x memory, 10X SSD
Read 1 MB sequentially from disk    30,000,000   ns   30,000 us   30 ms 120x memory, 30X SSD
Send packet CA-&amp;gt;Netherlands-&amp;gt;CA    150,000,000   ns  150,000 us  150 ms

Notes
-----
1 ns = 10^-9 seconds
1 us = 10^-6 seconds = 1,000 ns
1 ms = 10^-3 seconds = 1,000 us = 1,000,000 ns
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Handy metrics based on numbers above:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Read sequentially from disk at 30 MB/s&lt;/li&gt;
&lt;li&gt;Read sequentially from 1 Gbps Ethernet at 100 MB/s&lt;/li&gt;
&lt;li&gt;Read sequentially from SSD at 1 GB/s&lt;/li&gt;
&lt;li&gt;Read sequentially from main memory at 4 GB/s&lt;/li&gt;
&lt;li&gt;6-7 world-wide round trips per second&lt;/li&gt;
&lt;li&gt;2,000 round trips per second within a data center&lt;/li&gt;
&lt;/ul&gt;&lt;h4&gt;Latency numbers visualized&lt;/h4&gt;
&lt;p&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/77f72259e1eb58596b564d1ad823af1853bc60a3/687474703a2f2f692e696d6775722e636f6d2f6b307431652e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/77f72259e1eb58596b564d1ad823af1853bc60a3/687474703a2f2f692e696d6775722e636f6d2f6b307431652e706e67&quot; alt=&quot;&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Source(s) and further reading&lt;/h4&gt;
&lt;h3&gt;Additional system design interview questions&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Common system design interview questions, with links to resources on how to solve each.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Real world architectures&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Articles on how real world systems are designed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://camo.githubusercontent.com/b7c71be73fb466344c2d773178ae74e3fbb1dcc6/687474703a2f2f692e696d6775722e636f6d2f5463556f3266772e706e67&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/b7c71be73fb466344c2d773178ae74e3fbb1dcc6/687474703a2f2f692e696d6775722e636f6d2f5463556f3266772e706e67&quot; data-canonical-src=&quot;http://i.imgur.com/TcUo2fw.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;&lt;em&gt;&lt;a href=&quot;https://www.infoq.com/presentations/Twitter-Timeline-Scalability&quot; rel=&quot;nofollow&quot;&gt;Source: Twitter timelines at scale&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Don't focus on nitty gritty details for the following articles, instead:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Identify shared principles, common technologies, and patterns within these articles&lt;/li&gt;
&lt;li&gt;Study what problems are solved by each component, where it works, where it doesn't&lt;/li&gt;
&lt;li&gt;Review the lessons learned&lt;/li&gt;
&lt;/ul&gt;&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;System&lt;/th&gt;
&lt;th&gt;Reference(s)&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Data processing&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;MapReduce&lt;/strong&gt; - Distributed data processing from Google&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://static.googleusercontent.com/media/research.google.com/zh-CN/us/archive/mapreduce-osdi04.pdf&quot; rel=&quot;nofollow&quot;&gt;research.google.com&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Data processing&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Spark&lt;/strong&gt; - Distributed data processing from Databricks&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://www.slideshare.net/AGrishchenko/apache-spark-architecture&quot; rel=&quot;nofollow&quot;&gt;slideshare.net&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Data processing&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Storm&lt;/strong&gt; - Distributed data processing from Twitter&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://www.slideshare.net/previa/storm-16094009&quot; rel=&quot;nofollow&quot;&gt;slideshare.net&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;
&lt;td/&gt;
&lt;td/&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Data store&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Bigtable&lt;/strong&gt; - Distributed column-oriented database from Google&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://www.read.seas.harvard.edu/%7Ekohler/class/cs239-w08/chang06bigtable.pdf&quot; rel=&quot;nofollow&quot;&gt;harvard.edu&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Data store&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;HBase&lt;/strong&gt; - Open source implementation of Bigtable&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://www.slideshare.net/alexbaranau/intro-to-hbase&quot; rel=&quot;nofollow&quot;&gt;slideshare.net&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Data store&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Cassandra&lt;/strong&gt; - Distributed column-oriented database from Facebook&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://www.slideshare.net/planetcassandra/cassandra-introduction-features-30103666&quot; rel=&quot;nofollow&quot;&gt;slideshare.net&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Data store&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;DynamoDB&lt;/strong&gt; - Document-oriented database from Amazon&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://www.read.seas.harvard.edu/%7Ekohler/class/cs239-w08/decandia07dynamo.pdf&quot; rel=&quot;nofollow&quot;&gt;harvard.edu&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Data store&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;MongoDB&lt;/strong&gt; - Document-oriented database&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://www.slideshare.net/mdirolf/introduction-to-mongodb&quot; rel=&quot;nofollow&quot;&gt;slideshare.net&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Data store&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Spanner&lt;/strong&gt; - Globally-distributed database from Google&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://research.google.com/archive/spanner-osdi2012.pdf&quot; rel=&quot;nofollow&quot;&gt;research.google.com&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Data store&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Memcached&lt;/strong&gt; - Distributed memory caching system&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://www.slideshare.net/oemebamo/introduction-to-memcached&quot; rel=&quot;nofollow&quot;&gt;slideshare.net&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Data store&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Redis&lt;/strong&gt; - Distributed memory caching system with persistence and value types&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://www.slideshare.net/dvirsky/introduction-to-redis&quot; rel=&quot;nofollow&quot;&gt;slideshare.net&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;
&lt;td/&gt;
&lt;td/&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;File system&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Google File System (GFS)&lt;/strong&gt; - Distributed file system&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://static.googleusercontent.com/media/research.google.com/zh-CN/us/archive/gfs-sosp2003.pdf&quot; rel=&quot;nofollow&quot;&gt;research.google.com&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;File system&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Hadoop File System (HDFS)&lt;/strong&gt; - Open source implementation of GFS&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html&quot; rel=&quot;nofollow&quot;&gt;apache.org&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;
&lt;td/&gt;
&lt;td/&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Misc&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Chubby&lt;/strong&gt; - Lock service for loosely-coupled distributed systems from Google&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/chubby-osdi06.pdf&quot; rel=&quot;nofollow&quot;&gt;research.google.com&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Misc&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Dapper&lt;/strong&gt; - Distributed systems tracing infrastructure&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36356.pdf&quot; rel=&quot;nofollow&quot;&gt;research.google.com&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Misc&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Kafka&lt;/strong&gt; - Pub/sub message queue from LinkedIn&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://www.slideshare.net/mumrah/kafka-talk-tri-hug&quot; rel=&quot;nofollow&quot;&gt;slideshare.net&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Misc&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Zookeeper&lt;/strong&gt; - Centralized infrastructure and services enabling synchronization&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://www.slideshare.net/sauravhaloi/introduction-to-apache-zookeeper&quot; rel=&quot;nofollow&quot;&gt;slideshare.net&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;
&lt;td&gt;Add an architecture&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer#contributing&quot;&gt;Contribute&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3&gt;Company architectures&lt;/h3&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Company&lt;/th&gt;
&lt;th&gt;Reference(s)&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Amazon&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/amazon-architecture&quot; rel=&quot;nofollow&quot;&gt;Amazon architecture&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Cinchcast&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/blog/2012/7/16/cinchcast-architecture-producing-1500-hours-of-audio-every-d.html&quot; rel=&quot;nofollow&quot;&gt;Producing 1,500 hours of audio every day&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DataSift&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/blog/2011/11/29/datasift-architecture-realtime-datamining-at-120000-tweets-p.html&quot; rel=&quot;nofollow&quot;&gt;Realtime datamining At 120,000 tweets per second&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;DropBox&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=PE4gwstWhmc&quot; rel=&quot;nofollow&quot;&gt;How we've scaled Dropbox&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ESPN&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/blog/2013/11/4/espns-architecture-at-scale-operating-at-100000-duh-nuh-nuhs.html&quot; rel=&quot;nofollow&quot;&gt;Operating At 100,000 duh nuh nuhs per second&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Google&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/google-architecture&quot; rel=&quot;nofollow&quot;&gt;Google architecture&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Instagram&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/blog/2011/12/6/instagram-architecture-14-million-users-terabytes-of-photos.html&quot; rel=&quot;nofollow&quot;&gt;14 million users, terabytes of photos&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;http://instagram-engineering.tumblr.com/post/13649370142/what-powers-instagram-hundreds-of-instances&quot; rel=&quot;nofollow&quot;&gt;What powers Instagram&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Justin.tv&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/blog/2010/3/16/justintvs-live-video-broadcasting-architecture.html&quot; rel=&quot;nofollow&quot;&gt;Justin.Tv's live video broadcasting architecture&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Facebook&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://cs.uwaterloo.ca/%7Ebrecht/courses/854-Emerging-2014/readings/key-value/fb-memcached-nsdi-2013.pdf&quot; rel=&quot;nofollow&quot;&gt;Scaling memcached at Facebook&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;https://cs.uwaterloo.ca/%7Ebrecht/courses/854-Emerging-2014/readings/data-store/tao-facebook-distributed-datastore-atc-2013.pdf&quot; rel=&quot;nofollow&quot;&gt;TAO: Facebook’s distributed data store for the social graph&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Beaver.pdf&quot; rel=&quot;nofollow&quot;&gt;Facebook’s photo storage&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;http://highscalability.com/blog/2016/6/27/how-facebook-live-streams-to-800000-simultaneous-viewers.html&quot; rel=&quot;nofollow&quot;&gt;How Facebook Live Streams To 800,000 Simultaneous Viewers&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Flickr&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/flickr-architecture&quot; rel=&quot;nofollow&quot;&gt;Flickr architecture&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Mailbox&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/blog/2013/6/18/scaling-mailbox-from-0-to-one-million-users-in-6-weeks-and-1.html&quot; rel=&quot;nofollow&quot;&gt;From 0 to one million users in 6 weeks&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Netflix&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/blog/2017/12/11/netflix-what-happens-when-you-press-play.html&quot; rel=&quot;nofollow&quot;&gt;Netflix: What Happens When You Press Play?&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Pinterest&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/blog/2013/4/15/scaling-pinterest-from-0-to-10s-of-billions-of-page-views-a.html&quot; rel=&quot;nofollow&quot;&gt;From 0 To 10s of billions of page views a month&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;http://highscalability.com/blog/2012/5/21/pinterest-architecture-update-18-million-visitors-10x-growth.html&quot; rel=&quot;nofollow&quot;&gt;18 million visitors, 10x growth, 12 employees&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Playfish&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/blog/2010/9/21/playfishs-social-gaming-architecture-50-million-monthly-user.html&quot; rel=&quot;nofollow&quot;&gt;50 million monthly users and growing&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;PlentyOfFish&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/plentyoffish-architecture&quot; rel=&quot;nofollow&quot;&gt;PlentyOfFish architecture&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Salesforce&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/blog/2013/9/23/salesforce-architecture-how-they-handle-13-billion-transacti.html&quot; rel=&quot;nofollow&quot;&gt;How they handle 1.3 billion transactions a day&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Stack Overflow&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/blog/2009/8/5/stack-overflow-architecture.html&quot; rel=&quot;nofollow&quot;&gt;Stack Overflow architecture&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;TripAdvisor&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/blog/2011/6/27/tripadvisor-architecture-40m-visitors-200m-dynamic-page-view.html&quot; rel=&quot;nofollow&quot;&gt;40M visitors, 200M dynamic page views, 30TB data&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Tumblr&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/blog/2012/2/13/tumblr-architecture-15-billion-page-views-a-month-and-harder.html&quot; rel=&quot;nofollow&quot;&gt;15 billion page views a month&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Twitter&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/scaling-twitter-making-twitter-10000-percent-faster&quot; rel=&quot;nofollow&quot;&gt;Making Twitter 10000 percent faster&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;http://highscalability.com/blog/2011/12/19/how-twitter-stores-250-million-tweets-a-day-using-mysql.html&quot; rel=&quot;nofollow&quot;&gt;Storing 250 million tweets a day using MySQL&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;http://highscalability.com/blog/2013/7/8/the-architecture-twitter-uses-to-deal-with-150m-active-users.html&quot; rel=&quot;nofollow&quot;&gt;150M active users, 300K QPS, a 22 MB/S firehose&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;https://www.infoq.com/presentations/Twitter-Timeline-Scalability&quot; rel=&quot;nofollow&quot;&gt;Timelines at scale&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=5cKTP36HVgI&quot; rel=&quot;nofollow&quot;&gt;Big and small data at Twitter&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=z8LU0Cj6BOU&quot; rel=&quot;nofollow&quot;&gt;Operations at Twitter: scaling beyond 100 million users&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Uber&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/blog/2015/9/14/how-uber-scales-their-real-time-market-platform.html&quot; rel=&quot;nofollow&quot;&gt;How Uber scales their real-time market platform&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;http://highscalability.com/blog/2016/10/12/lessons-learned-from-scaling-uber-to-2000-engineers-1000-ser.html&quot; rel=&quot;nofollow&quot;&gt;Lessons Learned From Scaling Uber To 2000 Engineers, 1000 Services, And 8000 Git Repositories&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;WhatsApp&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;http://highscalability.com/blog/2014/2/26/the-whatsapp-architecture-facebook-bought-for-19-billion.html&quot; rel=&quot;nofollow&quot;&gt;The WhatsApp architecture Facebook bought for $19 billion&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;YouTube&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=w5WVu624fY8&quot; rel=&quot;nofollow&quot;&gt;YouTube scalability&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;http://highscalability.com/youtube-architecture&quot; rel=&quot;nofollow&quot;&gt;YouTube architecture&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3&gt;Company engineering blogs&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Architectures for companies you are interviewing with.&lt;/p&gt;
&lt;p&gt;Questions you encounter might be from the same domain.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Source(s) and further reading&lt;/h4&gt;
&lt;p&gt;Looking to add a blog? To avoid duplicating work, consider adding your company blog to the following repo:&lt;/p&gt;
&lt;h2&gt;Under development&lt;/h2&gt;
&lt;p&gt;Interested in adding a section or helping complete one in-progress? &lt;a href=&quot;https://github.com/donnemartin/system-design-primer#contributing&quot;&gt;Contribute&lt;/a&gt;!&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Distributed computing with MapReduce&lt;/li&gt;
&lt;li&gt;Consistent hashing&lt;/li&gt;
&lt;li&gt;Scatter gather&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/donnemartin/system-design-primer#contributing&quot;&gt;Contribute&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Credits&lt;/h2&gt;
&lt;p&gt;Credits and sources are provided throughout this repo.&lt;/p&gt;
&lt;p&gt;Special thanks to:&lt;/p&gt;
&lt;h2&gt;Contact info&lt;/h2&gt;
&lt;p&gt;Feel free to contact me to discuss any issues, questions, or comments.&lt;/p&gt;
&lt;p&gt;My contact info can be found on my &lt;a href=&quot;https://github.com/donnemartin&quot;&gt;GitHub page&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;License&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;I am providing code and resources in this repository to you under an open source license. Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Copyright 2017 Donne Martin

Creative Commons Attribution 4.0 International License (CC BY 4.0)

http://creativecommons.org/licenses/by/4.0/
&lt;/code&gt;
&lt;/pre&gt;&lt;/article&gt;</description>
<pubDate>Fri, 13 Jul 2018 12:34:59 +0000</pubDate>
<dc:creator>donnemartin</dc:creator>
<og:image>https://avatars1.githubusercontent.com/u/5458997?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>donnemartin/system-design-primer</og:title>
<og:url>https://github.com/donnemartin/system-design-primer</og:url>
<og:description>system-design-primer - Learn how to design large-scale systems. Prep for the system design interview. Includes Anki flashcards.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/donnemartin/system-design-primer</dc:identifier>
</item>
<item>
<title>Blackmagic eGPU – Thunderbolt 3 external graphics processor</title>
<link>https://www.blackmagicdesign.com/products/blackmagicegpu/</link>
<guid isPermaLink="true" >https://www.blackmagicdesign.com/products/blackmagicegpu/</guid>
<description>&lt;div readability=&quot;42&quot;&gt;
&lt;p&gt;In addition to graphics and computational acceleration, &lt;span class=&quot;nb-lg&quot;&gt;the Blackmagic eGPU&lt;/span&gt; is also the perfect hub for connecting devices such as keyboards, mice, Thunderbolt monitors, big screen HDMI televisions, high speed storage and more. You get two Thunderbolt 3 connections, a built in 4 port USB hub and HDMI which supports 4K output. Plus, the connections are ergonomically spaced which makes it easy to connect and disconnect your peripherals quickly!&lt;/p&gt;
&lt;/div&gt;&lt;p&gt;&lt;span class=&quot;label&quot;&gt;Built in USB hub supports 4 USB devices&lt;/span&gt; &lt;span class=&quot;label&quot;&gt;Two high speed 40 Gb/s Thunderbolt 3 ports&lt;/span&gt; &lt;span class=&quot;label&quot;&gt;HDMI 2.0 for monitoring up to 4K&lt;/span&gt; &lt;span class=&quot;label&quot;&gt;85W power for charging your computer&lt;/span&gt;&lt;/p&gt;</description>
<pubDate>Fri, 13 Jul 2018 02:23:39 +0000</pubDate>
<dc:creator>gregpower</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.blackmagicdesign.com/products/blackmagicegpu/</dc:identifier>
</item>
<item>
<title>Foundations Machine Learning</title>
<link>https://bloomberg.github.io/foml/</link>
<guid isPermaLink="true" >https://bloomberg.github.io/foml/</guid>
<description>&lt;td class=&quot;label&quot; colspan=&quot;2&quot; readability=&quot;29.789144865287&quot;&gt;

&lt;p&gt;With linear methods, we may need a whole lot of features to get a hypothesis space that's expressive enough to fit our data -- there can be orders of magnitude more features than training examples. While regularization can control overfitting, having a huge number of features can make things computationally very difficult, if handled naively. For objective functions of a particular general form, which includes ridge regression and SVMs but not lasso regression, we can &quot;kernelize&quot;, which can allow significant speedups in certain situations. In fact, with the &quot;kernel trick&quot;, we can even use an infinite-dimensional feature space at a computational cost that depends primarily on the training set size.&lt;/p&gt;
&lt;details readability=&quot;28&quot;&gt;&lt;p&gt;More...In more detail, it turns out that even when the optimal parameter vector we're searching for lives in a very high-dimensional vector space (dimension being the number of features), a basic linear algebra argument shows that for certain objective functions, the optimal parameter vector lives in a subspace spanned by the training input vectors. Thus, when we have more features than training points, we may be better off restricting our search to the lower-dimensional subspace spanned by training inputs. We can do this by an easy reparameterization of the objective function. This result is referred to as the &quot;representer theorem&quot;, and its proof can be given on one slide.&lt;/p&gt;
&lt;p&gt;After reparameterization, we'll find that the objective function depends on the data only through the Gram matrix, or &quot;kernel matrix&quot;, which contains the dot products between all pairs of training feature vectors. This is where things get interesting a second time: Suppose f is our featurization function. Sometimes the dot product between two feature vectors f(x) and f(x') can be computed much more efficiently than multiplying together corresponding features and summing. In such a situation, we write the dot products in terms of the &quot;kernel function&quot;: k(x,x')=〈f(x),f(x')〉, which we hope to compute much more quickly than O(d), where d is the dimension of the feature space. Using this &quot;kernel trick&quot;, together with the reparameterization described above, is the essence of a &quot;kernel method&quot;, and it allows one to use huge (even infinite-dimensional) feature spaces with a computational burden that depends primarily on the size of your training set. In practice, it's useful for small and medium-sized datasets for which computing the kernel matrix is tractable. Scaling kernel methods to large data sets is still an active area of research.&lt;/p&gt;
&lt;/details&gt;&lt;/td&gt;&lt;td colspan=&quot;2&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/m1otj-SdwYw&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/td&gt;
</description>
<pubDate>Thu, 12 Jul 2018 23:59:44 +0000</pubDate>
<dc:creator>gohwell</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://bloomberg.github.io/foml/</dc:identifier>
</item>
<item>
<title>The Spiral Language</title>
<link>https://github.com/mrakgr/The-Spiral-Language</link>
<guid isPermaLink="true" >https://github.com/mrakgr/The-Spiral-Language</guid>
<description>&lt;h3&gt;readme.md&lt;/h3&gt;
&lt;article class=&quot;markdown-body entry-content&quot; itemprop=&quot;text&quot;&gt;

&lt;h2&gt;Overview&lt;/h2&gt;
&lt;h3&gt;Intro&lt;/h3&gt;
&lt;p&gt;As the world inexorably hurls towards the black maw of tomorrow, the power to face it is needed.&lt;/p&gt;
&lt;p&gt;Throughout the history of programming languages, the choice was between fast or expressive; the two traditions are crystallized by the C and the Lisp family of languages. There has been a lot of effort into this, but always as languages developed and moved forward they stepped away from the bare metal and in turn lost some of that core vitality that is needed for performance.&lt;/p&gt;
&lt;p&gt;The culprit for this is the heap allocation by default dogma introduced by Lisp decades ago. It is a crutch for languages with weak type systems.&lt;/p&gt;
&lt;p&gt;Abstraction by heap allocation is a dead end. It works moderately well on the current generation of computers where CPU is still the dominant driver of computation.&lt;/p&gt;
&lt;p&gt;It cannot work for devices like GPUs and the rest coming down the line. Many of the most important computational devices of the future won't support heap allocation so an alternative is needed to draw out their full power. It is of absolute importance that a language for that task have excellent control over inlining. Inlining therefore must comes as guarantee in the language and be a part of the type system.&lt;/p&gt;
&lt;p&gt;Inlining is a trade-off that expresses the exchange of memory for computation. It should be the default instead of heap allocating.&lt;/p&gt;
&lt;p&gt;A language good enough at propagating information so as to be capable of expressing inlining guarantees is also powerful enough for expressing a lot of other things well - without any abstraction overhead.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;First class types.&lt;/li&gt;
&lt;li&gt;Structural reflection through pattern matching.&lt;/li&gt;
&lt;li&gt;Interoperability between different languages (such as F# and Cuda.)&lt;/li&gt;
&lt;li&gt;First class functions.&lt;/li&gt;
&lt;li&gt;Tuples as heterogeneous lists.&lt;/li&gt;
&lt;li&gt;First class modules.&lt;/li&gt;
&lt;li&gt;First class layouts of data structures.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Spiral is such a language.&lt;/p&gt;
&lt;p&gt;Statically typed and with a lightweight, very powerful type system giving it expressiveness of dynamic languages and the speed of C, Spiral is the crystallization of staged functional programming. It boasts of having intensional polymorphism and first class staging. It was made for the sake of making a deep learning library which was too difficult to do in F# itself for its author.&lt;/p&gt;
&lt;h3&gt;Design Philosophy&lt;/h3&gt;
&lt;p&gt;Automatically doing type inference, inlining and other optimizations requires restrictions and heuristics in order to ensure termination. Languages that make the choice of automating the important parts of their internals invariably hamstring their expressiveness. Even if they end up doing well on some low level benchmarks, they perform poorly when high level abstractions are required.&lt;/p&gt;
&lt;p&gt;Spiral is different. The power of Spiral lies in its novel kind of language design, not compiler smartness. Spiral is a static language without any restrictions on either type inference or optimizations.&lt;/p&gt;
&lt;p&gt;It introduces novel constructs such as inlineables and join points, exposes optimizations as polymorphism in its type system, and then ties them together and with the rest of the features in such a manner so as to allow programs to be written so they terminate.&lt;/p&gt;
&lt;p&gt;The halting problem is the primary obstacle preventing the bridging of expressiveness and performance. Ultimately, C is a competing style more than it is a competing language. It is something other higher level languages regress to once they start worrying about performance.&lt;/p&gt;
&lt;p&gt;In Spiral, inlining by hand will never be necessary.&lt;/p&gt;
&lt;p&gt;In Spiral, the most abstract way of writing a program is also the optimal one from a performance standpoint.&lt;/p&gt;
&lt;p&gt;Spiral exists to abstract away optimization.&lt;/p&gt;
&lt;h2&gt;Dependencies&lt;/h2&gt;
&lt;h5&gt;For the compiler:&lt;/h5&gt;
&lt;p&gt;FParsec&lt;/p&gt;
&lt;p&gt;Visual Studio 2017 F# template (.NET desktop development)&lt;/p&gt;
&lt;h5&gt;For the Cuda using Spiral libraries:&lt;/h5&gt;
&lt;p&gt;ManagedCuda 8.0 + (CUBLAS,CURAND)&lt;/p&gt;
&lt;p&gt;Cuda SDK 8.0 + 9.0 (8.0 for the libraries and 9.0 for the NVCC compiler)&lt;/p&gt;
&lt;p&gt;The Cuda Unbound library&lt;/p&gt;
&lt;p&gt;Visual C++ version 15.4 v14.11 toolset&lt;/p&gt;
&lt;h2&gt;Tutorials: Introduction to Spiral&lt;/h2&gt;
&lt;h3&gt;0: The way to use the language&lt;/h3&gt;
&lt;p&gt;The easiest way to do it right now would be to clone this repo. In the Testing project, look at &lt;code&gt;run.fs&lt;/code&gt;. It has the latest example used for the tutorial. Select the &lt;code&gt;Testing&lt;/code&gt; project as the starter one and point the output to the &lt;code&gt;output.fs&lt;/code&gt; file in the &lt;code&gt;Temporary&lt;/code&gt; project. No need to worry about getting it wrong - at worst an exception will be raised.&lt;/p&gt;
&lt;p&gt;Modifying the Cuda configuration options in the &lt;code&gt;run.fs&lt;/code&gt; file unless usage of libraries related to that is required.&lt;/p&gt;
&lt;h3&gt;1: Inlineables, Methods and Join Points&lt;/h3&gt;
&lt;p&gt;Spiral has great many similarities to other languages of the ML family, most notably F# with whom it shares the most similarity and a great deal of syntax, but in terms of semantics, it is different at its core.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl x = 2 // Define a 64-bit integer in Spiral.
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let x = 2 // Define a 32-bit integer in F#.
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Unlike in F#, statements and function definitions in Spiral are preceded by &lt;code&gt;inl&lt;/code&gt; instead of &lt;code&gt;let&lt;/code&gt; which is short for &lt;code&gt;inl&lt;/code&gt;ineable.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let x = 2
()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;If a program like the above was disassembled, &lt;code&gt;x&lt;/code&gt; would compile down to a public method in F#. In Spiral in contrast, there would be nothing.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;module SpiralExample.Main
let cuda_kernels = &quot;&quot;&quot;

extern &quot;C&quot; {
    
}
&quot;&quot;&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It is not absolutely nothing though. If the program used Cuda kernels in it, they would gathered at the top of the file inside the &lt;code&gt;cuda_kernels&lt;/code&gt; variable. For interests of brevity, the unremarkable top part will be cut out during the tutorials.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl x = dyn 2
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let (var_0: int64) = 2L // Generated F# code.
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The reason Spiral is generating nothing is because &lt;code&gt;2&lt;/code&gt; as defined is a literal and gets tracked as such by the partial evaluator inside the environment. In order to have it appear in the generated code, it is necessary to cast it from the type to the term level using &lt;code&gt;dyn&lt;/code&gt;amize function. From here on out, the literal will be bound to a variable and the binding &lt;code&gt;x&lt;/code&gt; will track &lt;code&gt;var_0&lt;/code&gt; instead.&lt;/p&gt;
&lt;p&gt;Being able to do this is useful for various reasons. For example, without it constructs such as runtime loops would be impossible to write in Spiral because the partial evaluator would diverge. Despite its static typing features, the language would essentially be constrained to being an interpreter for a pure dynamic functional language.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl x = dyn 2
inl y = 3
(x + y) * (x + y)
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let (var_0: int64) = 2L
let (var_1: int64) = (var_0 + 3L)
(var_1 * var_1)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;To get a sense of how &lt;code&gt;dyn&lt;/code&gt; works, here is a slightly more complex example. The evaluator does common subexpression elimination in the local scope and so it is smart enough to optimize the &lt;code&gt;x+y&lt;/code&gt; into a single addition and a multiplication.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl x = 2
inl y = 3
(x + y) * (x + y)
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;25L
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Without &lt;code&gt;dyn&lt;/code&gt;, all the arithmetic operations get evaluated at compile time. This is due to the simple fact that a variable added to a literal is a variable. In general if an operation has a variable as one of its inputs, then its output will also be a variable. The evaluator term casts the literals when necessary. For an operation to be evaluated at compile time, the partial evaluator must have support for it internally.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;inl&lt;/code&gt; can also be used to define functions.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl add a b = a + b
add 1 2, add 3 4
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: int64
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
Tuple0(3L, 7L)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Tuples are used in Spiral much in the same way as in other functional languages. As per their name, &lt;code&gt;inl&lt;/code&gt;ineables are always inlined. There are no heuristics at play here in the evaluator. Spiral's staged functions are tracked at the type level and both their exact body and environments are known at every point of compilation. This is an absolute guarantee and there is no point at which they can be automatically converted into heap allocated closures.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl add a b = a + b
inl f = add 1
f 2, f 3
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: int64
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
Tuple0(3L, 4L)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Of course, they can be partially applied.&lt;/p&gt;
&lt;p&gt;Besides being staged, Spiral's functions allow more powerful forms of polymorphism than F#'s.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl mult a b = a * b
inl f g = g 1 2, g 3.0 4.0 // Would give a type error in F#.
f mult
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: float
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
Tuple0(2L, 12.000000)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Haskell could manage something like that using typeclasses and higher ranked types, but it would be nowhere as simple. Having to declare new types and then putting in the necessary annotations into the function would be needed.&lt;/p&gt;
&lt;p&gt;On the other hand, being able to do this makes type inference for Spiral undecidable.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl f _ = 1 + &quot;qwe&quot; // Does not give a type error
()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Undecidability manifests in Spiral like so - the body of the function is not evaluated until it is applied. That means that type errors can lurk in functions that are unused.&lt;/p&gt;
&lt;p&gt;That having said, Spiral is a statically typed language and any type errors for code on the execution path will get reported at compile time along with the trace for it.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl f _ = 1 + &quot;qwe&quot; // Does not give a type error
inl _ = 1
inl _ = 2
inl _ = 3
f ()
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;Error trace on line: 3, column: 5 in file &quot;example1&quot;.
inl _ = 1
    ^
Error trace on line: 4, column: 5 in file &quot;example1&quot;.
inl _ = 2
    ^
Error trace on line: 5, column: 5 in file &quot;example1&quot;.
inl _ = 3
    ^
Error trace on line: 6, column: 1 in file &quot;example1&quot;.
f ()
^
Error trace on line: 2, column: 7 in file &quot;example1&quot;.
inl f _ = 1 + &quot;qwe&quot; // Does not give a type error
      ^
Error trace on line: 51, column: 11 in file &quot;Core&quot;.
inl (+) a b = !Add(a,b)
          ^
`is_numeric a &amp;amp;&amp;amp; get_type a = get_type b` is false.
a=lit 1i64, b=lit qwe
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Since in order to achieve code reuse methods are necessary, Spiral makes it possible to make use of them with the &lt;code&gt;met&lt;/code&gt; keyword. It works much like &lt;code&gt;inl&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl mult a b = a * b
met f g = g 1 2, g 3.0 4.0
f mult
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: float
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let rec method_0(): Tuple0 =
    Tuple0(2L, 12.000000)
method_0()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;mult&lt;/code&gt; can also be defined using &lt;code&gt;met&lt;/code&gt; and passed into the other function.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;met mult a b = a * b
met f g = g 1 2, g 3.0 4.0
f mult
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: float
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let rec method_0(): Tuple0 =
    let (var_0: int64) = method_1()
    let (var_1: float) = method_2()
    Tuple0(var_0, var_1)
and method_1(): int64 =
    2L
and method_2(): float =
    12.000000
method_0()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The result would not be as one might expect since the methods would get specialized to the literal arguments passed to them. Instead it would be better to use &lt;code&gt;dyn&lt;/code&gt; here. But rather than letting the caller &lt;code&gt;f&lt;/code&gt; do the term casting operation, it would be better if the callee &lt;code&gt;mult&lt;/code&gt; did it.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;met mult (!dyn a) (!dyn b) = a * b
met f g = g 1 2, g 3.0 4.0
f mult
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: float
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let rec method_0(): Tuple0 =
    let (var_0: int64) = 1L
    let (var_1: int64) = 2L
    let (var_2: int64) = method_1((var_0: int64), (var_1: int64))
    let (var_3: float) = 3.000000
    let (var_4: float) = 4.000000
    let (var_5: float) = method_2((var_3: float), (var_4: float))
    Tuple0(var_2, var_5)
and method_1((var_0: int64), (var_1: int64)): int64 =
    (var_0 * var_1)
and method_2((var_0: float), (var_1: float)): float =
    (var_0 * var_1)
method_0()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;!&lt;/code&gt; on the left (the pattern) side of the expression is the active pattern unary operator. It takes a function as its first argument, applies the input to it and rebinds the result to second argument of the pattern (in this case &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; respectively) before the body is evaluated.&lt;/p&gt;
&lt;h4&gt;Recursion, Destructuring and Pattern Matching&lt;/h4&gt;
&lt;p&gt;Much like in F#, recursive functions can be defined using &lt;code&gt;rec&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl rec foldl f s = function
    | x :: xs -&amp;gt; foldl f (f s x) xs
    | () -&amp;gt; s

inl sum = foldl (+) 0

sum (1,2,3)
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;6L
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In the ML family of languages, &lt;code&gt;::&lt;/code&gt; is the list cons pattern. In Spiral it is the the tuple cons pattern. Tuple are fully fledged heterogeneous lists in Spiral and can be treated as such.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl a = 2,3
1 :: a
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: int64
    val mem_2: int64
    new(arg_mem_0, arg_mem_1, arg_mem_2) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1; mem_2 = arg_mem_2}
    end
Tuple0(1L, 2L, 3L)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;There are some interesting applications of this.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl rec foldl f s = function
    | x :: xs -&amp;gt; foldl f (f s x) xs
    | () -&amp;gt; s

met sum (!dyn l) = foldl (+) 0 l

sum (1,2,3), sum (1,2,3,4)
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: int64
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let rec method_0((var_0: int64), (var_1: int64), (var_2: int64)): int64 =
    let (var_3: int64) = (var_0 + var_1)
    (var_3 + var_2)
and method_1((var_0: int64), (var_1: int64), (var_2: int64), (var_3: int64)): int64 =
    let (var_4: int64) = (var_0 + var_1)
    let (var_5: int64) = (var_4 + var_2)
    (var_5 + var_3)
let (var_0: int64) = 1L
let (var_1: int64) = 2L
let (var_2: int64) = 3L
let (var_3: int64) = method_0((var_0: int64), (var_1: int64), (var_2: int64))
let (var_4: int64) = 1L
let (var_5: int64) = 2L
let (var_6: int64) = 3L
let (var_7: int64) = 4L
let (var_8: int64) = method_1((var_4: int64), (var_5: int64), (var_6: int64), (var_7: int64))
Tuple0(var_3, var_8)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;While the language allows variant arguments, Spiral has the ability to specialize methods to their exact arguments and in combination with destructuring to implement variant arguments in a typesafe manner.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl rec foldl f s = function
    | x :: xs -&amp;gt; foldl f (f s x) xs
    | () -&amp;gt; s

met sum l = foldl (+) 0 l

sum (1,2,3,dyn 4), sum (2,2,3,dyn 4)
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: int64
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let rec method_0((var_0: int64)): int64 =
    (6L + var_0)
and method_1((var_0: int64)): int64 =
    (7L + var_0)
let (var_0: int64) = 4L
let (var_1: int64) = method_0((var_0: int64))
let (var_2: int64) = 4L
let (var_3: int64) = method_1((var_2: int64))
Tuple0(var_1, var_3)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;By default, in Spiral tuples and modules are flattened and have their variables tracked individually. As can be seen in the generated code, when a tuple is passed into a function it is not the actual tuple that is being passed into it, but its arguments instead.&lt;/p&gt;
&lt;p&gt;The specialization is exact to the structure, not just the types. If literals are being passed through the method call, the method will get specialized to them.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;met f _ = 1,2,3
inl x = f ()
0
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: int64
    val mem_2: int64
    new(arg_mem_0, arg_mem_1, arg_mem_2) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1; mem_2 = arg_mem_2}
    end
let rec method_0(): Tuple0 =
    Tuple0(1L, 2L, 3L)
let (var_0: Tuple0) = method_0()
let (var_1: int64) = var_0.mem_0
let (var_2: int64) = var_0.mem_1
let (var_3: int64) = var_0.mem_2
0L
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Tuples themselves are only manifested in the generated code on join point and branch returns. They get destructured right away and tracked by their individual variables on binding and function application. Had the method return not been bound to &lt;code&gt;x&lt;/code&gt;, it would not have been destructured. This is the desired behavior because otherwise destructuring might block tail call optimizations.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;met f _ = 1
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;inl f _ = join 1
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above two code fragments are identical in Spiral. &lt;code&gt;met&lt;/code&gt; is just syntax sugar for a function with a join point around its body.&lt;/p&gt;
&lt;p&gt;Being able to do this is quite powerful as it allows more fine grained control over inlining.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl rec foldl f s = function
    | x :: xs -&amp;gt; foldl f (f s x) xs
    | () -&amp;gt; s

inl rec forall f = function
    | x :: xs -&amp;gt; f x &amp;amp;&amp;amp; forall f xs
    | () -&amp;gt; true

inl sum l = 
    if forall lit_is l then foldl (+) 0 l
    else join foldl (+) 0 (dyn l)

sum (1,2,3,4), sum (1,2,3,dyn 4)
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: int64
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let rec method_0((var_0: int64)): int64 =
    let (var_1: int64) = 1L
    let (var_2: int64) = 2L
    let (var_3: int64) = 3L
    let (var_4: int64) = (var_1 + var_2)
    let (var_5: int64) = (var_4 + var_3)
    (var_5 + var_0)
let (var_0: int64) = 4L
let (var_1: int64) = method_0((var_0: int64))
Tuple0(10L, var_1)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;lit_is&lt;/code&gt; always resolves at compile time to either &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt; just like other structure testing functions. In combination with &lt;code&gt;forall&lt;/code&gt; that allows for testing of whether all the arguments of &lt;code&gt;l&lt;/code&gt; are known at compile time. Then using a static if, the two branches amount to either summing them all at compile time, or term casting them and pushing the work to runtime.&lt;/p&gt;
&lt;p&gt;This ensures that the sum function does not get specialized to every arbitrary literal passed into it.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;if true then 1 else &quot;qwe&quot; // Not a type error.
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;if dyn true then 1 else &quot;qwe&quot; // A type error.
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;If statements in Spiral default to evaluating only one branch if their conditional is known at compile time meaning they are static by default. This is the bedrock of its intensional (structural) polymorphism. Under the hood, the patterns get compiled to static if statements which allow it to branch on structures and types.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;if&lt;/code&gt; goes hand in hand with join point specialization.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl default_of = function
    | _: int64 -&amp;gt; 0
    | _: float64 -&amp;gt; 0.0
    | _: string -&amp;gt; &quot;&quot;

default_of 1, default_of 1.0, default_of &quot;qwe&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: float
    val mem_2: string
    new(arg_mem_0, arg_mem_1, arg_mem_2) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1; mem_2 = arg_mem_2}
    end
Tuple0(0L, 0.000000, &quot;&quot;)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Unlike ML languages which use Hindley-Milner global type inference, Spiral does not infer as much as propagate. A consequence of that besides undecidability is that it knows the exact structure and type of everything at all times. When done on non-union types as done up to now, this sort of branching has no runtime overhead whatsoever and can be readily seen by looking into the generated code.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/1839016/f-explicit-match-vs-function-syntax&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;function&lt;/code&gt;&lt;/a&gt; is just shorthand for matching on the immediate argument like in F#.&lt;/p&gt;
&lt;p&gt;One other thing that is different from F# is that &lt;code&gt;int64&lt;/code&gt;,&lt;code&gt;float64&lt;/code&gt; and &lt;code&gt;string&lt;/code&gt; on the right side of the &lt;code&gt;:&lt;/code&gt; operators are not type annotations, but standard variables. The types in Spiral are first class much like everything else.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl int64_type = type 1
inl float64_type = type 1.0
inl string_type = type &quot;qwe&quot;

inl default_of = function
    | _: int64_type -&amp;gt; 0
    | _: float64_type -&amp;gt; 0.0
    | _: string_type -&amp;gt; &quot;&quot;

default_of 1, default_of 1.0, default_of &quot;qwe&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: float
    val mem_2: string
    new(arg_mem_0, arg_mem_1, arg_mem_2) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1; mem_2 = arg_mem_2}
    end
Tuple0(0L, 0.000000, &quot;&quot;)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As can be seen, the two generated code fragments are identical. &lt;code&gt;:&lt;/code&gt; on the pattern side is the type equality operator. It can be invoked outside the pattern using the &lt;code&gt;eq_type&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;type&lt;/code&gt; is a keyword and like &lt;code&gt;join&lt;/code&gt; it enters a new scope.&lt;/p&gt;
&lt;p&gt;The types themselves can do more than be passed around or be matched on.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl int64_type = type 1
inl float64_type = type 1.0
inl string_type = type &quot;qwe&quot;

inl default_of = function
    | _: int64_type -&amp;gt; 0
    | _: float64_type -&amp;gt; 0.0
    | _: string_type -&amp;gt; &quot;&quot;

inl a = type int64_type + 1
inl b = type float64_type + 1.0
inl c = type string_format &quot;{0}, {1}&quot; (string_type, &quot;rty&quot;)

default_of a, default_of b, default_of c
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: float
    val mem_2: string
    new(arg_mem_0, arg_mem_1, arg_mem_2) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1; mem_2 = arg_mem_2}
    end
Tuple0(0L, 0.000000, &quot;&quot;)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above is just to illustrate that inside the evaluator naked types are treated just like variables of the same type. If they should happen to slip on the term level that would cause an error.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;int64 + 3
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;(naked_type (*int64*) + 3L)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;These kinds of errors are easier to locate when they are shown in generated code. When they happen it is usually because of a missed argument to a curried function which causes its environment to spill into the generated code. This makes the usual error messages unhelpful, but looking at the error code gives a good indication of what is happening.&lt;/p&gt;
&lt;h5&gt;Intensional Recursion&lt;/h5&gt;
&lt;p&gt;Spiral in general does not need type annotations. The only exceptions are recursive functions when used in tandem with join points.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;met rec fact (!dyn x) = if x &amp;gt; 1 then x * fact (x-1) else 1
fact 3
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;Process is terminated due to StackOverflowException.
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The correct way to write the above would be.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;met rec fact (!dyn x) = 
    if x &amp;gt; 1 then x * fact (x-1) else 1
    : int64 // or alternatively `: x`
fact 3
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let rec method_0((var_0: int64)): int64 =
    let (var_1: bool) = (var_0 &amp;gt; 1L)
    if var_1 then
        let (var_2: int64) = (var_0 - 1L)
        let (var_3: int64) = method_0((var_2: int64))
        (var_0 * var_3)
    else
        1L
let (var_0: int64) = 3L
method_0((var_0: int64))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;:&lt;/code&gt; has the lowest precedence of all Spiral's constructs so it will get applied before any of the statements. It does not necessarily have to be put directly into the function. As reminder, on the pattern side &lt;code&gt;:&lt;/code&gt; is not a type annotation, but a type equality test.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl rec fact x =
    inl body x = if x &amp;gt; 1 then x * fact (x-1) else 1
    if lit_is x then body x
    else join (body x : int64)
fact 3, fact (dyn 3)
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: int64
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let rec method_0((var_0: int64)): int64 =
    let (var_1: bool) = (var_0 &amp;gt; 1L)
    if var_1 then
        let (var_2: int64) = (var_0 - 1L)
        let (var_3: int64) = method_0((var_2: int64))
        (var_0 * var_3)
    else
        1L
let (var_0: int64) = 3L
let (var_1: int64) = method_0((var_0: int64))
Tuple0(6L, var_1)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It takes some work, but it is not difficult to make functions stage polymorphic in Spiral.&lt;/p&gt;
&lt;p&gt;Mutual recursion can also be done using join points.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;// https://en.wikipedia.org/wiki/Hofstadter_sequence#Hofstadter_Female_and_Male_sequences
inl rec hof x = 
    inl male n = if n &amp;gt; 0 then n - hof.female (hof.male (n-1)) else 0
    inl female n = if n &amp;gt; 0 then n - hof.male (hof.female (n-1)) else 1
    match x with
    | .male (!dyn n) -&amp;gt; join male n : int64
    | .female (!dyn n) -&amp;gt; join female n : int64
hof.male 3
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let rec method_0((var_0: int64)): int64 =
    let (var_1: bool) = (var_0 &amp;gt; 0L)
    if var_1 then
        let (var_2: int64) = (var_0 - 1L)
        let (var_3: int64) = method_0((var_2: int64))
        let (var_4: int64) = method_1((var_3: int64))
        (var_0 - var_4)
    else
        0L
and method_1((var_0: int64)): int64 =
    let (var_1: bool) = (var_0 &amp;gt; 0L)
    if var_1 then
        let (var_2: int64) = (var_0 - 1L)
        let (var_3: int64) = method_1((var_2: int64))
        let (var_4: int64) = method_0((var_3: int64))
        (var_0 - var_4)
    else
        1L
let (var_0: int64) = 3L
method_0((var_0: int64))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;.&lt;/code&gt; here is the type literal lift operator. It has special syntax for strings and when used directly next to an expression, it binds more tightly than application similar to how F#'s method access works. It also has its own dedicated pattern as shown above.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl f x = .(x)
inl a = f &quot;asd&quot;
inl b = .asd
eq_type a b
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;true
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It works on any kind of literal, not just strings. Type literals can be converted to ordinary literals as well.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl a = .1
inl b = .2
match a,b with
| .(a), .(b) -&amp;gt; a + b
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;3L
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The difference between type literals and ordinary literals is that type literals will always be erased in generated code and it is impossible to push them at runtime by &lt;code&gt;dyn&lt;/code&gt;ing them.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;dyn (.a,&quot;b&quot;,.false,true)
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: string
    val mem_1: bool
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let (var_0: string) = &quot;b&quot;
let (var_1: bool) = true
Tuple0(var_0, var_1)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Using &lt;code&gt;print_static&lt;/code&gt; can be used to inspect what the evaluator sees at compile time.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;print_static (dyn (.a,&quot;b&quot;,.false,true))
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;[type (type_lit (a)), var (string), type (type_lit (false)), var (bool)]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;All the information in type literals is preserved at all times.&lt;/p&gt;
&lt;h5&gt;Term Casting of Functions&lt;/h5&gt;
&lt;p&gt;Spiral's functions as flexible as they are have the notable weakness of not being able to emulate recursive datatypes. For that they need to be cast to the term level.&lt;/p&gt;
&lt;p&gt;Consider a silly example like the following where the function is used as a counter.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;met rec loop f (!dyn i) =
    if i &amp;lt; 10 then loop (inl _ -&amp;gt; f() + 1) (i + 1)
    else f()
    : int64
loop (inl _ -&amp;gt; 0) 0
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This will never compile for the reason that &lt;code&gt;f&lt;/code&gt; continually expands its environment.&lt;/p&gt;
&lt;p&gt;At first it tries to specialize the function for just &lt;code&gt;inl _ -&amp;gt; 0&lt;/code&gt; -&amp;gt; &lt;code&gt;int64&lt;/code&gt; -&amp;gt; &lt;code&gt;int64&lt;/code&gt;. The second specialization it tries is &lt;code&gt;[inl _ -&amp;gt; 0; inl _ -&amp;gt; f() + 1]&lt;/code&gt; -&amp;gt; &lt;code&gt;int64&lt;/code&gt; -&amp;gt; &lt;code&gt;int64&lt;/code&gt;. During the third it is trying to specialize it for &lt;code&gt;[inl _ -&amp;gt; 0; inl _ -&amp;gt; f() + 1; inl _ -&amp;gt; f() + 1]&lt;/code&gt; -&amp;gt; &lt;code&gt;int64&lt;/code&gt; -&amp;gt; &lt;code&gt;int64&lt;/code&gt;. The syntax used here is just for the sake of description. The problem is the it is impossible for the compiler to ever terminate on the above program. The only way to do it would be to cast the function to the term level and track it as a variable.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl rec loop f i =
    inl f, i = term_cast f (), dyn i
    join 
        if i &amp;lt; 10 then loop (inl _ -&amp;gt; f() + 1) (i + 1) else f()
        : int64

loop (inl _ -&amp;gt; 0) 0
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let rec method_0 (): int64 =
    0L
and method_1((var_0: (unit -&amp;gt; int64)), (var_1: int64)): int64 =
    let (var_2: bool) = (var_1 &amp;lt; 10L)
    if var_2 then
        let (var_3: int64) = (var_1 + 1L)
        let (var_4: (unit -&amp;gt; int64)) = method_2((var_0: (unit -&amp;gt; int64)))
        method_1((var_4: (unit -&amp;gt; int64)), (var_3: int64))
    else
        var_0()
and method_2 ((var_0: (unit -&amp;gt; int64))) (): int64 =
    let (var_1: int64) = var_0()
    (var_1 + 1L)
let (var_0: (unit -&amp;gt; int64)) = method_0
let (var_1: int64) = 0L
method_1((var_0: (unit -&amp;gt; int64)), (var_1: int64))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;term_cast&lt;/code&gt; works by taking a function as its first argument and a type as its second. It emulates a function call, gets the return type of the term function from the result of that, and set the input type to the first argument. In the generated code, it flattens the arguments a single tuple level.&lt;/p&gt;
&lt;p&gt;Term level functions have their environments hidden and the only information available to the evaluator is its type.&lt;/p&gt;
&lt;p&gt;On the Cuda side, term functions are also allowed with the restriction that their environments be empty. Meaning, they cannot capture variables in their lexical scope and can only be used as function pointers. Despite that restriction, they are useful for interop with Cuda libraries.&lt;/p&gt;
&lt;p&gt;All the features of Spiral with the exception of heap allocated modules and closures can be used on the Cuda side.&lt;/p&gt;
&lt;h5&gt;&lt;code&gt;&amp;lt;function&amp;gt;&lt;/code&gt; error message&lt;/h5&gt;
&lt;p&gt;Don't be fooled by the &lt;code&gt;&amp;lt;function&amp;gt;&lt;/code&gt; during type errors. As was repeatedly stated, functions are not at all opaque - they are fully transparent to the evaluator. The reason why they get printed like that is simply because they have a tendency to suck everything into the environment. And except for very small examples, trying to print out the raw AST of its body is worthless even for debugging as it is so convoluted.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;if dyn true then
    inl a,b = dyn (1,2)
    inl _ -&amp;gt; a + b
else
    inl a,b = dyn (3.0,4.0)
    inl _ -&amp;gt; a + b
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;if dyn true then
^
Types in branches of If do not match.
Got: &amp;lt;function&amp;gt; and &amp;lt;function&amp;gt;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;If functions have the same bodies, they can be returned from branches of a dynamic if statement if they also have the same environments.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;if dyn true then
    inl a,b = dyn (1,2)
    inl _ -&amp;gt; a + b
else
    inl a,b = dyn (3,4)
    inl _ -&amp;gt; a + b
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Env0 =
    struct
    val mem_0: int64
    val mem_1: int64
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let (var_0: bool) = true
if var_0 then
    let (var_1: int64) = 1L
    let (var_2: int64) = 2L
    (Env0(var_1, var_2))
else
    let (var_3: int64) = 3L
    let (var_4: int64) = 4L
    (Env0(var_3, var_4))
&lt;/code&gt;
&lt;/pre&gt;
&lt;h3&gt;2: Modules, Macros and Interop&lt;/h3&gt;
&lt;h4&gt;Modules&lt;/h4&gt;
&lt;p&gt;Owing to Spiral's relatively dynamic nature, modules work much like &lt;a href=&quot;https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/records&quot; rel=&quot;nofollow&quot;&gt;records&lt;/a&gt; do in F# albeit with greatly expanded functionality. Unlike in F#, they do not need to be predefined.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl f m =
    open m
    q + w + e
inl m1 = {q=1; w=2; e=3}
inl m2 = {q=1.0; w=2.0; e=3.0}
f m1, f m2
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: float
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
Tuple0(6L, 6.000000)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As per their namesake, they can be opened and passed as arguments. They have their own dedicated patterns.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl f {q w e} = q + w + e
inl q = 1
inl w = 2
inl e = 3
inl m = {q w e}
f m
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;6L
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;They allow functional lens updates. Note that in the generated code their fields are ordered by their names.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl f d = {d.data with a = self + 10}
inl a = 1
inl b = 2
inl c = 3
inl m = {data={a b c}}
f m
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Env0 =
    struct
    val mem_0: int64
    val mem_1: int64
    val mem_2: int64
    new(arg_mem_0, arg_mem_1, arg_mem_2) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1; mem_2 = arg_mem_2}
    end
and Env1 =
    struct
    val mem_0: Env0
    new(arg_mem_0) = {mem_0 = arg_mem_0}
    end
(Env1((Env0(11L, 2L, 3L))))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Fields can be added to them and removed arbitrarily in an immutable fashion. Using &lt;code&gt;without&lt;/code&gt; on a non-existing field will not do anything.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl a = 1
inl b = 2
inl c = 3
inl m = {a b c}
{m with d = 4; without a}
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Env0 =
    struct
    val mem_0: int64
    val mem_1: int64
    val mem_2: int64
    new(arg_mem_0, arg_mem_1, arg_mem_2) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1; mem_2 = arg_mem_2}
    end
(Env0(2L, 3L, 4L))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Like with tuples which are represented by immutable lists in Spiral, the modules in Spiral allow anything immutable maps might do. For example, they can be mapped over(&lt;code&gt;module_map&lt;/code&gt;), folded(&lt;code&gt;module_foldl&lt;/code&gt;,&lt;code&gt;module_fold&lt;/code&gt;), and filtered(&lt;code&gt;module_filter&lt;/code&gt;). Here is the fold example.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl m = {a=1; b=2; c=3}
module_foldl (inl key state value -&amp;gt; state + value) 0 m
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;6L
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;They support more powerful patterns than F# allows on records like not(&lt;code&gt;!&lt;/code&gt;) and xor(&lt;code&gt;^&lt;/code&gt;).&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl f {!nope (a ^ b)=s} = s
// f {nope=()} // Would trigger a type error
inl m = {a=1; b=2}
// f m // Without trigger a type error
f {m without a}, f {m without b}
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: int64
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
Tuple0(2L, 1L)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Last, but not least, Spiral's modules and functions support several kinds of layouts. By default, like tuples they have a transparent structure whose variables are tracked on an individual basis. Here is the heap layout.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;{a=1; b=2; c=3} |&amp;gt; dyn |&amp;gt; heap
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type EnvHeap0 =
    {
    mem_0: int64
    mem_1: int64
    mem_2: int64
    }
let (var_0: int64) = 1L
let (var_1: int64) = 2L
let (var_2: int64) = 3L
({mem_0 = (var_0: int64); mem_1 = (var_1: int64); mem_2 = (var_2: int64)} : EnvHeap0)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here are the 5 layouts in order: &lt;code&gt;indiv&lt;/code&gt;,&lt;code&gt;heap&lt;/code&gt;,&lt;code&gt;heapm&lt;/code&gt;,&lt;code&gt;stack&lt;/code&gt;,&lt;code&gt;packed_stack&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;{a=1; b=2; c=3} |&amp;gt; dyn |&amp;gt; heap |&amp;gt; heapm |&amp;gt; stack |&amp;gt; packed_stack
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type EnvHeap0 =
    {
    mem_0: int64
    mem_1: int64
    mem_2: int64
    }
and EnvHeapMutable1 =
    {
    mutable mem_0: int64
    mutable mem_1: int64
    mutable mem_2: int64
    }
and EnvStack2 =
    struct
    val mem_0: int64
    val mem_1: int64
    val mem_2: int64
    new(arg_mem_0, arg_mem_1, arg_mem_2) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1; mem_2 = arg_mem_2}
    end
[&amp;lt;System.Runtime.InteropServices.StructLayout(System.Runtime.InteropServices.LayoutKind.Sequential,Pack=1)&amp;gt;]
and EnvPackedStack3 =
    struct
    val mem_0: int64
    val mem_1: int64
    val mem_2: int64
    new(arg_mem_0, arg_mem_1, arg_mem_2) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1; mem_2 = arg_mem_2}
    end
let (var_0: int64) = 1L
let (var_1: int64) = 2L
let (var_2: int64) = 3L
let (var_3: EnvHeap0) = ({mem_0 = (var_0: int64); mem_1 = (var_1: int64); mem_2 = (var_2: int64)} : EnvHeap0)
let (var_4: int64) = var_3.mem_0
let (var_5: int64) = var_3.mem_1
let (var_6: int64) = var_3.mem_2
let (var_7: EnvHeapMutable1) = ({mem_0 = (var_4: int64); mem_1 = (var_5: int64); mem_2 = (var_6: int64)} : EnvHeapMutable1)
let (var_8: int64) = var_7.mem_0
let (var_9: int64) = var_7.mem_1
let (var_10: int64) = var_7.mem_2
let (var_11: EnvStack2) = EnvStack2((var_8: int64), (var_9: int64), (var_10: int64))
let (var_12: int64) = var_11.mem_0
let (var_13: int64) = var_11.mem_1
let (var_14: int64) = var_11.mem_2
EnvPackedStack3((var_12: int64), (var_13: int64), (var_14: int64))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;That is the tour of them, but it does not yet demonstrate their true power. The essence of modules converted to layout types is that they capture scope.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl npc = 
    {
    health = dyn 0
    mana = dyn 0
    max_health = 40
    max_mana = 30
    }

inl ar = array_create (type stack npc) 3
ar 0 &amp;lt;- stack {npc with health = dyn 10; mana = dyn 20}
ar 1 &amp;lt;- stack {npc with health = dyn 20; mana = dyn 10}
//ar 2 &amp;lt;- {npc with health = dyn 10; mana = dyn 20} // Gives a type error as the module here is not a layout type.
//ar 2 &amp;lt;- stack {npc with health = dyn 10; mana = dyn 20; max_health = 50} // Gives a type error as max health is an incorrect literal.
()
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type EnvStack0 =
    struct
    val mem_0: int64
    val mem_1: int64
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let (var_0: int64) = 0L
let (var_1: int64) = 0L
let (var_3: (EnvStack0 [])) = Array.zeroCreate&amp;lt;EnvStack0&amp;gt; (System.Convert.ToInt32(3L))
let (var_4: int64) = 10L
let (var_5: int64) = 20L
let (var_6: EnvStack0) = EnvStack0((var_4: int64), (var_5: int64))
var_3.[int32 0L] &amp;lt;- var_6
let (var_7: int64) = 20L
let (var_8: int64) = 10L
let (var_9: EnvStack0) = EnvStack0((var_7: int64), (var_8: int64))
var_3.[int32 1L] &amp;lt;- var_9
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In layout types, literals and naked types become a part of the bigger type and are tracked at the type level. The individual variables are flattened and the intermediate structures are erased in the generated code, very similarly to how the arguments are handled at join points.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;packed_stack&lt;/code&gt; layout is just there in case it might be necessary to pass a tuple over to the Cuda side. In most cases though, it makes more sense to use the default (non)layout and pass them as individual arguments.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;heapm&lt;/code&gt; layout is useful for mutably updating individual fields of a heap allocated module.&lt;/p&gt;
&lt;p&gt;Layout types are there in order to allow finer control of the boxed representations of modules and functions. Without &lt;code&gt;heap&lt;/code&gt; it would be impossible to heap allocate modules directly for example.&lt;/p&gt;
&lt;h4&gt;Macros&lt;/h4&gt;
&lt;h5&gt;Solve Me&lt;/h5&gt;
&lt;p&gt;Modules are beautiful and elegant part of Spiral. Macros are definitely ugly, but they are the only way for Spiral to interop with other languages' libraries and are as such indispensable.&lt;/p&gt;
&lt;p&gt;In Spiral they have the interesting property of also acting as types.&lt;/p&gt;
&lt;p&gt;So far all the examples given in the tutorial were relatively unmotivated. Macros make it is possible to do IO among other thing which allow the language to be applied to real world problems.&lt;/p&gt;
&lt;p&gt;As a very basic demonstration of them, let us start with this HackerRank problem.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;// https://www.hackerrank.com/challenges/solve-me-first/problem
// The entire code is given, you can just review and submit!
open System

[&amp;lt;EntryPoint&amp;gt;]
let main argv = 
    let a = Console.ReadLine() |&amp;gt; int
    let b = Console.ReadLine() |&amp;gt; int
    printfn &quot;%d&quot; (a+b)
    0 // return an integer exit code
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above is the F# solution given directly. It just reads two ints from input, sums them and returns the sum. Doing it in Spiral without the IDE support or even direct language support for .NET constructs make it more complicated.&lt;/p&gt;
&lt;p&gt;First the &lt;code&gt;System.Console&lt;/code&gt; type needs to be defined.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;//inl console = fs ((.text, &quot;System.Console&quot;) :: ())
inl console = fs [text: &quot;System.Console&quot;]
print_static console
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type (dotnet_type (System.Console))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;What this has done is create the &lt;code&gt;[text: &quot;System.Console&quot;]&lt;/code&gt; naked type. The type shown in the output is just how it gets printed - the actual type is determined by its body, namely &lt;code&gt;[text: &quot;System.Console&quot;]&lt;/code&gt;. This is equivalent to &lt;code&gt;(.text, &quot;System.Console&quot;) :: ()&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl a = (.text, &quot;System.Console&quot;) :: () |&amp;gt; fs
inl b = [text: &quot;System.Console&quot;] |&amp;gt; fs
eq_type a b
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;true
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;[]&lt;/code&gt; is just syntax sugar for named tuples. It has no extra functionality apart from what is provided by the standard constructs.&lt;/p&gt;
&lt;p&gt;Since types are just macros it is possible to make nonsensical types.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;print_static (fs [text: &quot;1 + 2 + 3&quot;])
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type (dotnet_type (1 + 2 + 3))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Hence mistakes with macros will have to be responsibility of the downwards languages. But in the worst they will just lead to a type error.&lt;/p&gt;
&lt;p&gt;Unlike in other languages where they are used for abstraction, macros in Spiral are only to be used for interop. They would not be good at all for that anyway given that at most they can print text.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl console = fs [text: &quot;System.Console&quot;]
inl static_method static_type method_name args return_type = 
    macro.fs return_type [
        type: static_type
        text: &quot;.&quot;
        text: method_name
        args: args
        ]
inl readline() = static_method console .ReadLine() string
inl a, b = readline(), readline()
()
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let (var_0: string) = System.Console.ReadLine()
let (var_1: string) = System.Console.ReadLine()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above program succinctly captures Spiral's approach to language interop. The facilities used for defining macro-based types and printing them are interwoven with one another. One extra ingredient macros evaluation require over macro type definitions is the return type.&lt;/p&gt;
&lt;p&gt;What the &lt;code&gt;macro.fs&lt;/code&gt; function is doing is printing the macro based on the second argument and returning a variable of the type in the first argument.&lt;/p&gt;
&lt;p&gt;Now all the pieces are in place to finish the exercise.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl console = fs [text: &quot;System.Console&quot;]
inl static_method static_type method_name args return_type = 
    macro.fs return_type [
        type: static_type
        text: &quot;.&quot;
        text: method_name
        args: args
        ]
inl unop name arg return_type = 
    macro.fs return_type [
        text: name
        text: &quot; &quot;
        arg: arg
        ]
inl readline() = static_method console .ReadLine() string
inl writeline x = static_method console .WriteLine x string
inl int x = unop &quot;int&quot; x int32
inl a, b = readline(), readline()
writeline (int a + int b)
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let (var_0: string) = System.Console.ReadLine()
let (var_1: string) = System.Console.ReadLine()
let (var_2: int32) = int var_0
let (var_3: int32) = int var_1
let (var_4: int32) = (var_2 + var_3)
System.Console.WriteLine(var_4)
&lt;/code&gt;
&lt;/pre&gt;
&lt;h5&gt;Simple Array Sum (macro version)&lt;/h5&gt;
&lt;p&gt;This example is to demonstrate how macros can be used to interop with F# libraries which often take in functions as arguments.&lt;/p&gt;
&lt;p&gt;The code fragments will be split into two. The first part loads the numbers into a Spiral array, splits them based on the whitespace char and convert them to ints.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;//https://www.hackerrank.com/challenges/simple-array-sum/problem
inl console = fs [text: &quot;System.Console&quot;]
inl static_method static_type method_name args return_type = 
    macro.fs return_type [
        type: static_type
        text: &quot;.&quot;
        text: method_name
        args: args
        ]

inl readline() = static_method console .ReadLine() string
inl writeline x = static_method console .WriteLine x string

inl array t = type (array_create t 0)
inl _, ar = readline(), macro.fs (array int32) [arg: readline(); text: &quot;.Split [|' '|] |&amp;gt; Array.map int&quot;]
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let (var_0: string) = System.Console.ReadLine()
let (var_2: string) = System.Console.ReadLine()
let (var_3: (int32 [])) = var_2.Split [|' '|] |&amp;gt; Array.map int
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The next part could also be done using macros, but is here to demonstrate an aspect of Spiral's intensional polymorphism.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;// Converts a type level function to a term level function based on a type.
inl rec closure_of f tys = 
    match tys with
    | x =&amp;gt; xs -&amp;gt; term_cast (inl x -&amp;gt; closure_of (f x) xs) x
    | x: f -&amp;gt; f
    | _ -&amp;gt; error_type &quot;The tail of the closure does not correspond to the one being casted to.&quot;

inl add a b = a + b
inl add_closure = closure_of add (int32 =&amp;gt; int32 =&amp;gt; int32)

macro.fs int32 [text: &quot;Array.fold &quot;; arg: add_closure; text: &quot; 0 &quot;; arg: ar]
|&amp;gt; writeline
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let (var_5: (int32 -&amp;gt; (int32 -&amp;gt; int32))) = method_0
let (var_6: int32) = Array.fold var_5 0 var_3
System.Console.WriteLine(var_6)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;closure_of&lt;/code&gt; is an expanded version of &lt;code&gt;term_cast&lt;/code&gt; that instead of converting by applying using the input argument converts a (staged) type level function to a term level using a target type thereby unstaging it. Term level functions have their own dedicated pattern for destructuring their types.&lt;/p&gt;
&lt;p&gt;Naked types for them can be constructed with the &lt;code&gt;=&amp;gt;&lt;/code&gt; operator. The &lt;code&gt;error_type&lt;/code&gt; raises a type error with the specified message whenever it is evaluated.&lt;/p&gt;
&lt;p&gt;What the &lt;code&gt;closure_of&lt;/code&gt; function does can be better understood by rewriting it to a specific instance with two arguments.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl closure_of_2 f (a' =&amp;gt; b' =&amp;gt; c') = 
    term_cast (inl a -&amp;gt; term_cast (inl b -&amp;gt; f a b : c') b') a'
closure_of_2 (+) (int32 =&amp;gt; int32 =&amp;gt; int32)
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let rec method_0 ((var_0: int32)): (int32 -&amp;gt; int32) =
    method_1((var_0: int32))
and method_1 ((var_1: int32)) ((var_0: int32)): int32 =
    (var_1 + var_0)
method_0
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The original version is just a more generic version of &lt;code&gt;closure_of_2&lt;/code&gt; that loops over the arguments while both accumulating the results of the application of the closure and term casting it.&lt;/p&gt;
&lt;p&gt;That is roughly it with regards to interop. Spiral of course does have its own libraries.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;closure_of&lt;/code&gt; and other macro related functions can be found in the &lt;code&gt;Extern&lt;/code&gt; module.&lt;/p&gt;
&lt;h4&gt;Spiral libraries&lt;/h4&gt;
&lt;pre&gt;
&lt;code&gt;let example1 = 
    &quot;example1&quot;,[array;console],&quot;Module description.&quot;,
    &quot;&quot;&quot;
open Console
inl _, b = readline(), macro.fs (array int32) [arg: readline(); text: &quot;.Split [|' '|] |&amp;gt; Array.map int&quot;]
Array.foldl (+) (dyn 0i32) b |&amp;gt; writeline
    &quot;&quot;&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The way Spiral is currently meant to be used is as a scripting language inside F#. The module argument is the list in the middle and the &lt;code&gt;array&lt;/code&gt; and &lt;code&gt;console&lt;/code&gt; are the modules of the same name respectively.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl Array = ...
inl Console = ...
open Console
inl _, b = readline(), macro.fs (array int32) [arg: readline(); text: &quot;.Split [|' '|] |&amp;gt; Array.map int&quot;]
Array.foldl (+) (dyn 0i32) b |&amp;gt; writeline
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above is roughly how the program would be unfolded after parsing, but before typing and partial evaluation. Modules are unfolded in a flattened manner in the sequence they are input. Duplicate modules are ignored.&lt;/p&gt;
&lt;p&gt;Much like F#, Spiral imposes a top down ordering of the program and modules cannot refer to each other recursively. If that functionality is required, it can be achieved using join points, but in general it should not be necessary.&lt;/p&gt;
&lt;p&gt;This kind of constrained architecture cuts down on circular referencing and encourages purposeful laying out of programs.&lt;/p&gt;
&lt;p&gt;Spiral libraries are (to be) covered in depth in the user guide and the reference.&lt;/p&gt;
&lt;h3&gt;3: Loops and Arrays&lt;/h3&gt;
&lt;h4&gt;Loops&lt;/h4&gt;
&lt;p&gt;Most languages make it trivial to write loops and the user does not have to worry about them diverging except at runtime.&lt;/p&gt;
&lt;p&gt;Spiral's staging abilities introduce new complexities into the mix. In Spiral, for every loop one writes, it is necessary to keep in mind whether it is intended to run at compile or at runtime.&lt;/p&gt;
&lt;p&gt;Making functions stage polymorphic takes even more effort. Furthermore for recursive runtime functions it is easy to forget to put in the type annotation and to dynamize the counter.&lt;/p&gt;
&lt;p&gt;For that reason, the bog standard &lt;code&gt;for&lt;/code&gt; and &lt;code&gt;while&lt;/code&gt; loops exist as a part of the &lt;code&gt;Loops&lt;/code&gt; module in Spiral.&lt;/p&gt;
&lt;p&gt;This chapter will be on building up the basic loop and then using it to implement the array library functions from first principles.&lt;/p&gt;
&lt;p&gt;At this point, apart from union types and the Cuda backend, all the main language features have been introduced albeit shallowly.&lt;/p&gt;
&lt;p&gt;This makes it possible to demonstrate how the architecture of a Spiral program differs from those in other languages.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let example = 
    &quot;example&quot;,[console],&quot;Module description.&quot;,
    &quot;&quot;&quot;
open Console
met rec for {d with from=(!dyn from) to by body} =
    if from &amp;lt;= to then body from; for {d with from=from+by}
    else ()
    : ()

for {from=0; to=5; by=1; body=inl i -&amp;gt;
    string_format &quot;The loop is on iteration {0}&quot; i |&amp;gt; writeline
    }
    &quot;&quot;&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let rec method_0((var_0: int64)): unit =
    let (var_1: bool) = (var_0 &amp;lt;= 5L)
    if var_1 then
        let (var_2: string) = System.String.Format(&quot;The loop is on iteration {0}&quot;,var_0)
        let (var_3: string) = System.String.Format(&quot;{0}&quot;,var_2)
        System.Console.WriteLine(var_3)
        let (var_4: int64) = (var_0 + 1L)
        method_1((var_4: int64))
    else
        ()
and method_1((var_0: int64)): unit =
    let (var_1: bool) = (var_0 &amp;lt;= 5L)
    if var_1 then
        let (var_2: string) = System.String.Format(&quot;The loop is on iteration {0}&quot;,var_0)
        let (var_3: string) = System.String.Format(&quot;{0}&quot;,var_2)
        System.Console.WriteLine(var_3)
        let (var_4: int64) = (var_0 + 1L)
        method_1((var_4: int64))
    else
        ()
let (var_0: int64) = 0L
method_0((var_0: int64))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Somewhat inadvertently, the first example become a good lesson in why loops would be desirable as a part of the library. The first example was careful to &lt;code&gt;dyn&lt;/code&gt; the counter and did not forget the annotation, but for some reason the loop got specialized to two functions one which only got called once.&lt;/p&gt;
&lt;p&gt;It is not a compiler bug.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;met rec for {d with from=(!dyn from) to by body} =
    if from &amp;lt;= to then body from; for {d with from=from+by}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The way join points work is that they specialize the call by their arguments. By rewriting the fragment highlighted above to an equivalent form it will be easy to demonstrate what is happening.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl rec for d =
    inl from = dyn d.from
    inl {to by body} = d
    join
        if from &amp;lt;= to then body from; for {d with from=from+by}
        else ()
        : ()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;What is going on is that &lt;code&gt;d&lt;/code&gt; - the old one with the &lt;code&gt;from&lt;/code&gt; field still as literal is getting passed through the join point and causes the redundant specialization to happen.&lt;/p&gt;
&lt;p&gt;Here is the way to write the &lt;code&gt;for&lt;/code&gt; function correctly.&lt;/p&gt;
&lt;p&gt;Out of all the mistakes to make in Spiral, accidentally passing old state through the join point is the easiest one to make. With missed return type annotations and such the compiler will diverge and warn the user that way, but but this one has a way of preying on laziness.&lt;/p&gt;
&lt;p&gt;In fact, this kind of error can happen in any language that supports records with mutable updates, not just Spiral. Spiral in particular just makes it obvious by looking at the argument count in the generated code.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;met rec for {from=(!dyn from) to by body} =
    if from &amp;lt;= to then body from; for {from=from+by; to by body}
    else ()
    : ()
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let rec method_0((var_0: int64)): unit =
    let (var_1: bool) = (var_0 &amp;lt;= 5L)
    if var_1 then
        let (var_2: string) = System.String.Format(&quot;The loop is on iteration {0}&quot;,var_0)
        let (var_3: string) = System.String.Format(&quot;{0}&quot;,var_2)
        System.Console.WriteLine(var_3)
        let (var_4: int64) = (var_0 + 1L)
        method_0((var_4: int64))
    else
        ()
let (var_0: int64) = 0L
method_0((var_0: int64))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above output is the ideal for this kind of loop. Only the &lt;code&gt;var_0&lt;/code&gt; varies, the other literals all get passed through the boundary and specialized along with the body.&lt;/p&gt;
&lt;p&gt;This is kind of specialization important to do with Cuda kernels as using too many variables in place of literals can cause register spillage into global memory and cause drastic degradations of performance. Spiral makes it easy to keep such data static and propagate it through the program.&lt;/p&gt;
&lt;p&gt;In addition, Spiral makes it trivial to this kind of specialization even across language boundaries. Partial evaluation is commonly refereed to as specialization. Staging makes it user directed. And being able to use staging constructs through the the type system as the basis of abstraction rather than being restricted to a second class macro inspired system is what makes Spiral's staging first class. That is a desirable trait as it increases uniformity of the language and with it, its power. It also simplifies its implementation greatly, so it is a good design principle to follow at all times.&lt;/p&gt;
&lt;p&gt;More concretely, one of the main motivations for writing Spiral for its author is avoiding having to write unending litanies of wrappers for simple Cuda kernels.&lt;/p&gt;
&lt;p&gt;Moving on, here is the static version of the loop.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl rec for {from to by body} =
    if from &amp;lt;= to then body from; for {from=from+by; to by body}
    else ()

for {from=0; to=5; by=1; body=inl i -&amp;gt;
    string_format &quot;The loop is on iteration {0}&quot; i |&amp;gt; writeline
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;System.Console.WriteLine(&quot;The loop is on iteration 0&quot;)
System.Console.WriteLine(&quot;The loop is on iteration 1&quot;)
System.Console.WriteLine(&quot;The loop is on iteration 2&quot;)
System.Console.WriteLine(&quot;The loop is on iteration 3&quot;)
System.Console.WriteLine(&quot;The loop is on iteration 4&quot;)
System.Console.WriteLine(&quot;The loop is on iteration 5&quot;)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Now what remains is to make the function stage polymorphic.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let example = 
    &quot;example&quot;,[tuple;console],&quot;Module description.&quot;,
    &quot;&quot;&quot;
open Console
inl rec for {from to by body} =
    inl body from = 
        if from &amp;lt;= to then body from; for {from=from+by; to by body}
        else ()
    if Tuple.forall lit_is (from,to,by) then body from
    else 
        inl from = dyn from
        join body from : ()

for {from=0; to=5; by=1; body=inl i -&amp;gt;
    string_format &quot;The loop is on iteration {0}&quot; i |&amp;gt; writeline
    }

for {from=dyn 0; to=5; by=1; body=inl i -&amp;gt;
    string_format &quot;The loop is on iteration {0}&quot; i |&amp;gt; writeline
    }
    &quot;&quot;&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let rec method_0((var_0: int64)): unit =
    let (var_1: bool) = (var_0 &amp;lt;= 5L)
    if var_1 then
        let (var_2: string) = System.String.Format(&quot;The loop is on iteration {0}&quot;,var_0)
        let (var_3: string) = System.String.Format(&quot;{0}&quot;,var_2)
        System.Console.WriteLine(var_3)
        let (var_4: int64) = (var_0 + 1L)
        method_0((var_4: int64))
    else
        ()
System.Console.WriteLine(&quot;The loop is on iteration 0&quot;)
System.Console.WriteLine(&quot;The loop is on iteration 1&quot;)
System.Console.WriteLine(&quot;The loop is on iteration 2&quot;)
System.Console.WriteLine(&quot;The loop is on iteration 3&quot;)
System.Console.WriteLine(&quot;The loop is on iteration 4&quot;)
System.Console.WriteLine(&quot;The loop is on iteration 5&quot;)
let (var_0: int64) = 0L
method_0((var_0: int64))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above loop can further improved in terms of functionality. Notice that its body has a type &lt;code&gt;unit&lt;/code&gt; which is represented by an empty tuple in Spiral. That is a throwback to C that has no place in modern language such as Spiral.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;open Console
inl rec for {from to by state body} =
    inl body from = 
        if from &amp;lt;= to then for {to by body from=from+by; state=body {state i=from}}
        else state
    if Tuple.forall lit_is (from,to,by) then body from
    else 
        inl from = dyn from
        join body from : state

inl power a to = for {from=2; to by=1; state=a; body=inl {state} -&amp;gt; state * a}

power 2 3
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;8L
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above works, but various criticisms of the program could be made. For one, is it really necessary to give &lt;code&gt;by&lt;/code&gt; every time? Vast majority of loops will in fact have it as &lt;code&gt;1&lt;/code&gt; so if it is not given it makes sense to use that default instead of giving a type error.&lt;/p&gt;
&lt;p&gt;Speaking of defaults, a decent guess would be that most loops are not intended to be unrolled and that a user is more likely to just forget to &lt;code&gt;dyn&lt;/code&gt; the &lt;code&gt;from&lt;/code&gt; field by accident.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;open Console
inl rec for {d with to state body} =
    inl body {from by} = 
        if from &amp;lt;= to then for {to by body from=from+by; state=body {state i=from}}
        else state

    inl from =
        match d with
        | {from} -&amp;gt; dyn from
        | {static_from} -&amp;gt; static_from

    inl by =
        match d with
        | {by} -&amp;gt; by
        | _ -&amp;gt; 1

    if Tuple.forall lit_is (from,to,by) then body {from}
    else 
        inl from = dyn from
        join body {from by} : state

inl power a to = for {from=2; to state=a; body=inl {state} -&amp;gt; state * a}

power 2 3
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let rec method_0((var_0: int64)): int64 =
    let (var_1: bool) = (var_0 &amp;lt;= 3L)
    if var_1 then
        let (var_2: int64) = (var_0 + 1L)
        method_1((var_2: int64))
    else
        2L
and method_1((var_0: int64)): int64 =
    let (var_1: bool) = (var_0 &amp;lt;= 3L)
    if var_1 then
        let (var_2: int64) = (var_0 + 1L)
        method_2((var_2: int64))
    else
        4L
and method_2((var_0: int64)): int64 =
    let (var_1: bool) = (var_0 &amp;lt;= 3L)
    if var_1 then
        let (var_2: int64) = (var_0 + 1L)
        method_3((var_2: int64))
    else
        8L
// ...and so on up to method_63
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Not quite as planned. An error made now is that the state gets specialized for every different power of 2.&lt;/p&gt;
&lt;p&gt;With a single added &lt;code&gt;dyn&lt;/code&gt; that can be fixed.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl power a to = for {from=2; to state=dyn a; body=inl {state} -&amp;gt; state * a}
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let rec method_0((var_0: int64), (var_1: int64)): int64 =
    let (var_2: bool) = (var_1 &amp;lt;= 3L)
    if var_2 then
        let (var_3: int64) = (var_1 + 1L)
        let (var_4: int64) = (var_0 * 2L)
        method_0((var_4: int64), (var_3: int64))
    else
        var_0
let (var_0: int64) = 2L
let (var_1: int64) = 2L
method_0((var_0: int64), (var_1: int64))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As a matter of convention, Spiral library functions that take in &lt;code&gt;state&lt;/code&gt; never &lt;code&gt;dyn&lt;/code&gt; it directly. That responsibility should fall onto the user.&lt;/p&gt;
&lt;p&gt;As an example of the reason for that, the state might be an option type so it might be better to specialize it for both of its states without instantiating it directly. Or it might be a tuple with some fields which would be desirable to remain as literals.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;for&lt;/code&gt; is intended to be used as a primitive and so requires some flexibility; it would not do to block functionality.&lt;/p&gt;
&lt;p&gt;Looking over the function now it seems fine, but it is a bit uncomfortable how from has to start from &lt;code&gt;2&lt;/code&gt;. It is not like the loop has to use the &lt;code&gt;&amp;lt;=&lt;/code&gt; operator for comparison in the conditional. In a lot of cases &lt;code&gt;&amp;lt;&lt;/code&gt; make a lot more sense.&lt;/p&gt;
&lt;p&gt;Furthermore, an user might want to iterate downwards. That can be accommodated.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;open Console
inl rec for {d with state body} =
    inl check =
        match d with
        | {near_to} from -&amp;gt; from &amp;lt; near_to 
        | {to} from -&amp;gt; from &amp;lt;= to
        | {down_to} from -&amp;gt; from &amp;gt;= down_to
        | {near_down_to} from -&amp;gt; from &amp;gt; near_down_to

    inl from =
        match d with
        | {from} -&amp;gt; dyn from
        | {static_from} -&amp;gt; static_from

    inl {(to ^ near_to ^ down_to ^ near_down_to)=to} = d

    inl by =
        match d with
        | {by} -&amp;gt; by
        | _ -&amp;gt; 1

    inl rec loop {from state} =
        inl body {from} = 
            if check from then loop {from=from+by; state=body {state i=from}}
            else state

        if Tuple.forall lit_is (from,to,by) then body {from}
        else 
            inl from = dyn from
            join body {from} : state

    loop {from state}

inl power a near_to = for {from=1; near_to state=dyn a; body=inl {state} -&amp;gt; state * a}

power 2 3
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let rec method_0((var_0: int64), (var_1: int64)): int64 =
    let (var_2: bool) = (var_1 &amp;lt; 3L)
    if var_2 then
        let (var_3: int64) = (var_1 + 1L)
        let (var_4: int64) = (var_0 * 2L)
        method_0((var_4: int64), (var_3: int64))
    else
        var_0
let (var_0: int64) = 2L
let (var_1: int64) = 1L
method_0((var_0: int64), (var_1: int64))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The module member queries are all done statically and so maximum polymorphism is attained. The above program also demonstrates why lexical scope is so great.&lt;/p&gt;
&lt;p&gt;The above is starting to near the functionality of the &lt;code&gt;for&lt;/code&gt; function in the actual library. To make it more professional, rather than returning a pattern miss error on when a field is missed, it would be better to tell the user what the problem is.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;open Console
inl for {d with state body} =
    inl check =
        match d with
        | {near_to} from -&amp;gt; from &amp;lt; near_to 
        | {to} from -&amp;gt; from &amp;lt;= to
        | {down_to} from -&amp;gt; from &amp;gt;= down_to
        | {near_down_to} from -&amp;gt; from &amp;gt; near_down_to
        | _ -&amp;gt; error_type &quot;One of `to`,`near_to`,`down_to`,`near_down_to` needs be present.&quot;


    inl from =
        match d with
        | {from=(!dyn from) ^ static_from=from} -&amp;gt; from
        | _ -&amp;gt; error_type &quot;Only one of `from` and `static_from` field to loop needs to be present.&quot;

    inl to =
        match d with
        | {(to ^ near_to ^ down_to ^ near_down_to)=to} -&amp;gt; to
        | _ -&amp;gt; error_type &quot;Only one of `to`,`near_to`,`down_to`,`near_down_to` is allowed.&quot;

    inl by =
        match d with
        | {by} -&amp;gt; by
        | _ -&amp;gt; 1

    inl rec loop {from state} =
        inl body {from} = 
            if check from then loop {from=from+by; state=body {state i=from}}
            else state

        if Tuple.forall lit_is (from,to,by) then body {from}
        else 
            inl from = dyn from
            join body {from} : state

    loop {from state}

inl power a near_to = for {static_from=1; near_to state=a; body=inl {state} -&amp;gt; state * a}

power 2 3
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;8L
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above design is in fact superior to what is currently in the standard library. A lot of features of the language were developed along with the library and some parts of it did not keep up. The author also got to fancy with the design of it. At the time that was useful for pushing the language, but not so much from a design perspective.&lt;/p&gt;
&lt;p&gt;Another issue with the standard library as it stands is that in fact its author did not know how to program in the language he was making and had to learn it as he going along.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl from =
        match d with
        | {from=(!dyn from) ^ static_from=from} -&amp;gt; from
        | _ -&amp;gt; error_type &quot;Only one of `from` and `static_from` field to loop needs to be present.&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This part here is highlighted in order to show the xor (&lt;code&gt;^&lt;/code&gt;) pattern might be used in tandem with active patterns.&lt;/p&gt;
&lt;p&gt;All the features in the making of the loop so far have been covered in the previous chapters and now it can be seen how they come together.&lt;/p&gt;
&lt;p&gt;It is not done yet.&lt;/p&gt;
&lt;p&gt;In order to attain the full functionality of C style loops, Spiral's loops also need the ability to break out. Strictly speaking, this cannot be done in a functional language and having &lt;code&gt;return&lt;/code&gt; would make even less sense in Spiral than it does in ML variants, but the same functionality can be achieved instead by writing the loop body and calling the continuation for the next iteration in tail position.&lt;/p&gt;
&lt;p&gt;As motivating example, imagine trying to iterate over nested arrays trying to find a specific item before breaking out. With the loop as was written above, there is no way to stop before reaching the end.&lt;/p&gt;
&lt;p&gt;To start things off, first the nested arrays need to be created.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;open Console
inl for {d with body} =
    inl state = 
        match d with
        | {state} -&amp;gt; state
        | _ -&amp;gt; ()
/// ...
inl array_init near_to f =
    assert (near_to &amp;gt;= 0) &quot;The input to init needs to be greater or equal to 0.&quot;
    // Somewhat of an ugly practice in order to infer the type in a language that doesn't support inference. 
    // For large functions, it is recommended to put them in a join point otherwise compile times could 
    // become exponential if the function contains branches.
    // For a simple map for an array like here, it does not matter.
    inl typ = type (f 0) 
    inl ar = array_create typ near_to
    for {from=0; near_to; body=inl {i} -&amp;gt; ar i &amp;lt;- f i}
    ar

inl rec zeroes = function
    | x :: x' -&amp;gt; array_init x (inl _ -&amp;gt; zeroes x')
    | () -&amp;gt; &quot;&quot;

inl ar = zeroes (4,4,4,4)
ar 0 0 0 2 &amp;lt;- &quot;princess&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let rec method_3((var_0: ((((string []) []) []) [])), (var_1: int64)): unit =
    let (var_2: bool) = (var_1 &amp;lt; 4L)
    if var_2 then
        let (var_3: int64) = (var_1 + 1L)
        let (var_8: (((string []) []) [])) = Array.zeroCreate&amp;lt;((string []) [])&amp;gt; (System.Convert.ToInt32(4L))
        let (var_9: int64) = 0L
        method_2((var_8: (((string []) []) [])), (var_9: int64))
        var_0.[int32 var_1] &amp;lt;- var_8
        method_3((var_0: ((((string []) []) []) [])), (var_3: int64))
    else
        ()
and method_2((var_0: (((string []) []) [])), (var_1: int64)): unit =
    let (var_2: bool) = (var_1 &amp;lt; 4L)
    if var_2 then
        let (var_3: int64) = (var_1 + 1L)
        let (var_6: ((string []) [])) = Array.zeroCreate&amp;lt;(string [])&amp;gt; (System.Convert.ToInt32(4L))
        let (var_7: int64) = 0L
        method_1((var_6: ((string []) [])), (var_7: int64))
        var_0.[int32 var_1] &amp;lt;- var_6
        method_2((var_0: (((string []) []) [])), (var_3: int64))
    else
        ()
and method_1((var_0: ((string []) [])), (var_1: int64)): unit =
    let (var_2: bool) = (var_1 &amp;lt; 4L)
    if var_2 then
        let (var_3: int64) = (var_1 + 1L)
        let (var_4: (string [])) = Array.zeroCreate&amp;lt;string&amp;gt; (System.Convert.ToInt32(4L))
        let (var_5: int64) = 0L
        method_0((var_4: (string [])), (var_5: int64))
        var_0.[int32 var_1] &amp;lt;- var_4
        method_1((var_0: ((string []) [])), (var_3: int64))
    else
        ()
and method_0((var_0: (string [])), (var_1: int64)): unit =
    let (var_2: bool) = (var_1 &amp;lt; 4L)
    if var_2 then
        let (var_3: int64) = (var_1 + 1L)
        var_0.[int32 var_1] &amp;lt;- &quot;&quot;
        method_0((var_0: (string [])), (var_3: int64))
    else
        ()
let (var_6: ((((string []) []) []) [])) = Array.zeroCreate&amp;lt;(((string []) []) [])&amp;gt; (System.Convert.ToInt32(4L))
let (var_7: int64) = 0L
method_3((var_6: ((((string []) []) []) [])), (var_7: int64))
let (var_8: (((string []) []) [])) = var_6.[int32 0L]
let (var_9: ((string []) [])) = var_8.[int32 0L]
let (var_10: (string [])) = var_9.[int32 0L]
var_10.[int32 2L] &amp;lt;- &quot;princess&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;First off, the case for &lt;code&gt;state&lt;/code&gt; that was forgotten is added at the top of the &lt;code&gt;for&lt;/code&gt; function. As the big comment states inferring the type returned by &lt;code&gt;f&lt;/code&gt; involves evaluating it twice which might not be a good idea depending on what it is.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Lazy&lt;/code&gt; module's only function &lt;code&gt;lazy&lt;/code&gt; for example puts a join point before evaluating the function because it might otherwise repeat long evaluations and those long evaluation in combination with branching (such as when nesting lazy values) might make compilation time take exponential time.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;assert&lt;/code&gt; function if its conditional can be statically determined and is true gives a type error at compile time instead of triggering at runtime. Spiral has support for throwing exceptions, but not catching or cleaning up after them, so they are intended to be used only for unrecoverable errors.&lt;/p&gt;
&lt;p&gt;As the example shows, nesting loops is straightforward in Spiral. It is a decent bit more elegant than doing it with macros which are the only choice in languages with weaker type systems. In Spiral, type inference and partial evaluation are one.&lt;/p&gt;
&lt;p&gt;Its type system is extremely powerful, and yet it does not have parametric polymorphism. Adding parametric polymorphism would significantly increase the complexity of both the language and its implementation, would not make the language any more expressive and would make it a lot harder to integrate the partial evaluator with the type system. This would make the language quite a bit slower.&lt;/p&gt;
&lt;p&gt;It is interesting to consider the implication of this - in Lisp languages, its raw AST flavored syntax is there for the reason of supporting its macro meta-programming feature. Maybe a really powerful type system does require the absence of parametricity?&lt;/p&gt;
&lt;p&gt;In light of what Spiral can do, it might be worth considering whether the programming language community at large collectively missed a whole evolutionary branch of languages with static typing, but without parametric polymorphism.&lt;/p&gt;
&lt;p&gt;If that is not convincing yet, maybe it will be after the tutorials are through.&lt;/p&gt;
&lt;p&gt;Here is how to write a breakable version of the &lt;code&gt;for&lt;/code&gt; function in to take advantage of the continuation passing style.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl for' {d with body} =
    inl finally =
        match d with
        | {finally} -&amp;gt; finally
        | _ -&amp;gt; id

    inl state = 
        match d with
        | {state} -&amp;gt; state
        | _ -&amp;gt; ()

    inl check =
        match d with
        | {near_to} from -&amp;gt; from &amp;lt; near_to 
        | {to} from -&amp;gt; from &amp;lt;= to
        | {down_to} from -&amp;gt; from &amp;gt;= down_to
        | {near_down_to} from -&amp;gt; from &amp;gt; near_down_to
        | _ -&amp;gt; error_type &quot;Only one of `to`,`near_to`,`down_to`,`near_down_to` needs be present.&quot;

    inl from =
        match d with
        | {from=(!dyn from) ^ static_from=from} -&amp;gt; from
        | _ -&amp;gt; error_type &quot;Only one of `from` and `static_from` field to loop needs to be present.&quot;

    inl to =
        match d with
        | {(to ^ near_to ^ down_to ^ near_down_to)=to} -&amp;gt; to
        | _ -&amp;gt; error_type &quot;Only one of `to`,`near_to`,`down_to`,`near_down_to` is allowed.&quot;

    inl by =
        match d with
        | {by} -&amp;gt; by
        | _ -&amp;gt; 1

    inl rec loop {from state} =
        inl body {from} = 
            if check from then 
                inl next state = loop {state from=from+by}
                body {next state i=from}
            else finally state

        if Tuple.forall lit_is (from,to,by) then body {from}
        else 
            inl from = dyn from
            join body {from} : finally state

    loop {from state}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;There is a significant amount of duplication now that will need to be eliminated. The highlights are the addition of the &lt;code&gt;finally&lt;/code&gt; field and the parts inside &lt;code&gt;loop&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;            if check from then 
                inl next state = loop {state from=from+by}
                body {next state i=from}
            else finally state
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Instead of the loop calling itself, it instead passes a function to the body and lets it do it instead.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;finally&lt;/code&gt; field is useful for resuming the outer loop. It can also be used to set the state to &lt;code&gt;unit&lt;/code&gt;, which would allow the loop to change states without having to resort to union types.&lt;/p&gt;
&lt;p&gt;First a utility function for reversing a tuple is needed. This is standard fare for functional programmers and closely mirrors how one would reverse a list in ML styled languages.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;// Reverses a tuple
inl tuple_rev = 
    inl rec loop state = function
        | x :: xs -&amp;gt; loop (x :: state) xs
        | () -&amp;gt; state
    loop ()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is how to apply the breakable for loop function. The goal is to find the coordinates of &quot;princess&quot;. The method is generalized to an arbitrary of number of nested arrays.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;// Correct version
inl rec find_index {next state} = function
    | ar &amp;amp; @array_is _ -&amp;gt; 
        inl body {next i} = find_index {next state=i::state} (ar i)
        for' {from=0; near_to=array_length ar; finally=next; body}
    | &quot;princess&quot; -&amp;gt; tuple_rev state
    | _ -&amp;gt; next ()

find_index {state=(); next = inl _ -&amp;gt; failwith (type (dim)) &quot;The princess is in another castle.&quot;} ar
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;failwith&lt;/code&gt; unlike in F#, requires the return type in Spiral but otherwise functions the same.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;@&lt;/code&gt; operator on the pattern side is a partial active pattern. Unlike F#'s which expect an option type, what &lt;code&gt;@&lt;/code&gt; takes in is a function with three arguments in the &lt;code&gt;inl arg on_fail on_succ -&amp;gt; ...&lt;/code&gt; form. &lt;code&gt;on_fail&lt;/code&gt; and &lt;code&gt;on_succ&lt;/code&gt; are to be called in tail position and possibly with join points around them when done so multiple times. They represent pattern failure and pattern success respectively.&lt;/p&gt;
&lt;p&gt;Here is a small example.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl f pat = function
    | @pat x -&amp;gt; x
    | _ -&amp;gt; error_type &quot;The pattern failed to trigger.&quot;

inl pat x on_fail on_succ =
    match x with
    | x: string | x: int64 -&amp;gt; join on_succ (string_format &quot;{0} joined&quot; x)
    | _ -&amp;gt; on_fail()

f pat &quot;qwe&quot;, f pat 123
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: string
    val mem_1: string
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let rec method_0(): string =
    &quot;qwe joined&quot;
and method_1(): string =
    &quot;123 joined&quot;
let (var_0: string) = method_0()
let (var_1: string) = method_1()
Tuple0(var_0, var_1)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Anything in Spiral can be passed as an argument, and since that includes functions it also applies to partial active patterns.&lt;/p&gt;
&lt;p&gt;The output of the compiled program is rather large, but it will be reproduced in bulk as an example this time to show that all the loops are being unfolded correctly into tail recursive functions.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: int64
    val mem_2: int64
    val mem_3: int64
    new(arg_mem_0, arg_mem_1, arg_mem_2, arg_mem_3) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1; mem_2 = arg_mem_2; mem_3 = arg_mem_3}
    end
let rec method_3((var_0: ((((string []) []) []) [])), (var_1: int64)): unit =
    let (var_2: bool) = (var_1 &amp;lt; 4L)
    if var_2 then
        let (var_3: int64) = (var_1 + 1L)
        let (var_8: (((string []) []) [])) = Array.zeroCreate&amp;lt;((string []) [])&amp;gt; (System.Convert.ToInt32(4L))
        let (var_9: int64) = 0L
        method_2((var_8: (((string []) []) [])), (var_9: int64))
        var_0.[int32 var_1] &amp;lt;- var_8
        method_3((var_0: ((((string []) []) []) [])), (var_3: int64))
    else
        ()
and method_4((var_0: ((((string []) []) []) [])), (var_1: int64), (var_2: int64)): Tuple0 =
    let (var_3: bool) = (var_2 &amp;lt; var_1)
    if var_3 then
        let (var_4: (((string []) []) [])) = var_0.[int32 var_2]
        let (var_5: int64) = var_4.LongLength
        let (var_6: int64) = 0L
        method_5((var_4: (((string []) []) [])), (var_2: int64), (var_5: int64), (var_0: ((((string []) []) []) [])), (var_1: int64), (var_6: int64))
    else
        (failwith &quot;The princess is in another castle.&quot;)
and method_2((var_0: (((string []) []) [])), (var_1: int64)): unit =
    let (var_2: bool) = (var_1 &amp;lt; 4L)
    if var_2 then
        let (var_3: int64) = (var_1 + 1L)
        let (var_6: ((string []) [])) = Array.zeroCreate&amp;lt;(string [])&amp;gt; (System.Convert.ToInt32(4L))
        let (var_7: int64) = 0L
        method_1((var_6: ((string []) [])), (var_7: int64))
        var_0.[int32 var_1] &amp;lt;- var_6
        method_2((var_0: (((string []) []) [])), (var_3: int64))
    else
        ()
and method_5((var_0: (((string []) []) [])), (var_1: int64), (var_2: int64), (var_3: ((((string []) []) []) [])), (var_4: int64), (var_5: int64)): Tuple0 =
    let (var_6: bool) = (var_5 &amp;lt; var_2)
    if var_6 then
        let (var_7: ((string []) [])) = var_0.[int32 var_5]
        let (var_8: int64) = var_7.LongLength
        let (var_9: int64) = 0L
        method_6((var_7: ((string []) [])), (var_5: int64), (var_1: int64), (var_8: int64), (var_0: (((string []) []) [])), (var_2: int64), (var_3: ((((string []) []) []) [])), (var_4: int64), (var_9: int64))
    else
        let (var_11: int64) = (var_1 + 1L)
        method_4((var_3: ((((string []) []) []) [])), (var_4: int64), (var_11: int64))
and method_1((var_0: ((string []) [])), (var_1: int64)): unit =
    let (var_2: bool) = (var_1 &amp;lt; 4L)
    if var_2 then
        let (var_3: int64) = (var_1 + 1L)
        let (var_4: (string [])) = Array.zeroCreate&amp;lt;string&amp;gt; (System.Convert.ToInt32(4L))
        let (var_5: int64) = 0L
        method_0((var_4: (string [])), (var_5: int64))
        var_0.[int32 var_1] &amp;lt;- var_4
        method_1((var_0: ((string []) [])), (var_3: int64))
    else
        ()
and method_6((var_0: ((string []) [])), (var_1: int64), (var_2: int64), (var_3: int64), (var_4: (((string []) []) [])), (var_5: int64), (var_6: ((((string []) []) []) [])), (var_7: int64), (var_8: int64)): Tuple0 =
    let (var_9: bool) = (var_8 &amp;lt; var_3)
    if var_9 then
        let (var_10: (string [])) = var_0.[int32 var_8]
        let (var_11: int64) = var_10.LongLength
        let (var_12: int64) = 0L
        method_7((var_10: (string [])), (var_8: int64), (var_1: int64), (var_2: int64), (var_11: int64), (var_0: ((string []) [])), (var_3: int64), (var_4: (((string []) []) [])), (var_5: int64), (var_6: ((((string []) []) []) [])), (var_7: int64), (var_12: int64))
    else
        let (var_14: int64) = (var_1 + 1L)
        method_5((var_4: (((string []) []) [])), (var_2: int64), (var_5: int64), (var_6: ((((string []) []) []) [])), (var_7: int64), (var_14: int64))
and method_0((var_0: (string [])), (var_1: int64)): unit =
    let (var_2: bool) = (var_1 &amp;lt; 4L)
    if var_2 then
        let (var_3: int64) = (var_1 + 1L)
        var_0.[int32 var_1] &amp;lt;- &quot;&quot;
        method_0((var_0: (string [])), (var_3: int64))
    else
        ()
and method_7((var_0: (string [])), (var_1: int64), (var_2: int64), (var_3: int64), (var_4: int64), (var_5: ((string []) [])), (var_6: int64), (var_7: (((string []) []) [])), (var_8: int64), (var_9: ((((string []) []) []) [])), (var_10: int64), (var_11: int64)): Tuple0 =
    let (var_12: bool) = (var_11 &amp;lt; var_4)
    if var_12 then
        let (var_13: string) = var_0.[int32 var_11]
        let (var_14: bool) = (var_13 = &quot;princess&quot;)
        if var_14 then
            Tuple0(var_3, var_2, var_1, var_11)
        else
            let (var_15: int64) = (var_11 + 1L)
            method_7((var_0: (string [])), (var_1: int64), (var_2: int64), (var_3: int64), (var_4: int64), (var_5: ((string []) [])), (var_6: int64), (var_7: (((string []) []) [])), (var_8: int64), (var_9: ((((string []) []) []) [])), (var_10: int64), (var_15: int64))
    else
        let (var_18: int64) = (var_1 + 1L)
        method_6((var_5: ((string []) [])), (var_2: int64), (var_3: int64), (var_6: int64), (var_7: (((string []) []) [])), (var_8: int64), (var_9: ((((string []) []) []) [])), (var_10: int64), (var_18: int64))
let (var_6: ((((string []) []) []) [])) = Array.zeroCreate&amp;lt;(((string []) []) [])&amp;gt; (System.Convert.ToInt32(4L))
let (var_7: int64) = 0L
method_3((var_6: ((((string []) []) []) [])), (var_7: int64))
let (var_8: (((string []) []) [])) = var_6.[int32 0L]
let (var_9: ((string []) [])) = var_8.[int32 0L]
let (var_10: (string [])) = var_9.[int32 0L]
var_10.[int32 2L] &amp;lt;- &quot;princess&quot;
let (var_11: int64) = var_6.LongLength
let (var_12: int64) = 0L
method_4((var_6: ((((string []) []) []) [])), (var_11: int64), (var_12: int64))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The continuation passing style is the key to a significant amount of abstractive power. It is difficult to understand in terms of what the program does, instead what is needed is to focus on what the program is.&lt;/p&gt;
&lt;p&gt;There are numerous ways of writing &lt;code&gt;find_index&lt;/code&gt; incorrectly that would not get immediately caught by the type system.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Forgetting to pass in the array.&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code&gt;...
find_index {state=(); next = inl _ -&amp;gt; failwith (type (dim)) &quot;The princess is in another castle.&quot;}
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;...
method_3((var_6: ((((string []) []) []) [])), (var_7: int64))
let (var_8: (((string []) []) [])) = var_6.[int32 0L]
let (var_9: ((string []) [])) = var_8.[int32 0L]
let (var_10: (string [])) = var_9.[int32 0L]
var_10.[int32 2L] &amp;lt;- &quot;princess&quot;
(Env7((Env4((Env3((Env2((Env1((Env0(naked_type (*bool*))))))))))), (Env3((Env2((Env1((Env0(naked_type (*bool*))))))))), (Env6(Tuple5(4L, 4L, 4L, 4L)))))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Seeing a dozen nested &lt;code&gt;Env&lt;/code&gt;s along with a naked type in the generated code is almost always a sign of forgetting to apply an argument somewhere.&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;Passing the state in incorrectly in the else branch.&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code&gt;...
| _ -&amp;gt; next state
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This would still have it compile and run correctly, but the code would have 40 lines more of useless specializations. It won't be shown here.&lt;/p&gt;
&lt;ol start=&quot;3&quot;&gt;&lt;li&gt;Passing the state even more incorrectly.&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code&gt;inl rec find_index {next state} = function
    | ar &amp;amp; @array_is _ -&amp;gt; 
        inl body {next state i} = find_index {next state=i::state} (ar i)
        for' {from=0; near_to=array_length ar; state finally=next; body}
    | &quot;princess&quot; -&amp;gt; tuple_rev state
    | _ -&amp;gt; next state
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This would cause it to diverge as it would continually append to &lt;code&gt;state&lt;/code&gt; inside the loop.&lt;/p&gt;
&lt;p&gt;In general though, programs written in a higher order style tend to work well after they typecheck much like in F# despite the language feeling more dynamic. And it is not necessarily the case that Spiral is less typesafe than F#.&lt;/p&gt;
&lt;p&gt;In fact it is the opposite for tasks that require union types due to F# having insufficient polymorphism in its type system. Many tasks that would otherwise require writing an interpreter in other languages can be done at compile time in Spiral.&lt;/p&gt;
&lt;p&gt;Loop unrolling is just one of those examples.&lt;/p&gt;
&lt;p&gt;Before the section on Loops can be finished there is just one bit of cleaning up left to do. That would be to merge &lt;code&gt;for&lt;/code&gt; and &lt;code&gt;for'&lt;/code&gt; into one function. Here is the full example in its completed form with a last minute change to &lt;code&gt;by&lt;/code&gt;. The new loops are going to go into the standard library.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl for_template kind {d with body} =
    inl finally =
        match d with
        | {finally} -&amp;gt; finally
        | _ -&amp;gt; id

    inl state = 
        match d with
        | {state} -&amp;gt; state
        | _ -&amp;gt; ()

    inl check =
        match d with
        | {near_to} from -&amp;gt; from &amp;lt; near_to 
        | {to} from -&amp;gt; from &amp;lt;= to
        | {down_to} from -&amp;gt; from &amp;gt;= down_to
        | {near_down_to} from -&amp;gt; from &amp;gt; near_down_to
        | _ -&amp;gt; error_type &quot;Only one of `to`,`near_to`,`down_to`,`near_down_to` needs be present.&quot;

    inl from =
        match d with
        | {from=(!dyn from) ^ static_from=from} -&amp;gt; from
        | _ -&amp;gt; error_type &quot;Only one of `from` and `static_from` field to loop needs to be present.&quot;

    inl to =
        match d with
        | {(to ^ near_to ^ down_to ^ near_down_to)=to} -&amp;gt; to
        | _ -&amp;gt; error_type &quot;Only one of `to`,`near_to`,`down_to`,`near_down_to` is allowed.&quot;

    inl by =
        match d with
        | {by} -&amp;gt; by
        | {to | near_to} -&amp;gt; 1
        | {down_to | near_down_to} -&amp;gt; -1

    inl rec loop {from state} =
        inl body {from} = 
            if check from then 
                match kind with
                | .Standard -&amp;gt;
                    loop {from=from+by; state=body {state i=from}}
                | .CPSd -&amp;gt;
                    inl next state = loop {state from=from+by}
                    body {next state i=from}
            else finally state

        if Tuple.forall lit_is (from,to,by) then body {from}
        else 
            inl from = dyn from
            join (body {from} : finally state)

    loop {from state}

inl for' = for_template .CPSd
inl for = for_template .Standard

inl array_init near_to f =
    assert (near_to &amp;gt;= 0) &quot;The input to init needs to be greater or equal to 0.&quot;
    // Somewhat of an ugly practice in order to infer the type in a language that doesn't support inference. 
    // For large functions, it is recomended to put them in a join point otherwise compile times could 
    // become exponential if the function contains branches.
    // For a simple map for an array like here, it does not matter.
    inl typ = type (f 0) 
    inl ar = array_create typ near_to
    for {from=0; near_to; body=inl {i} -&amp;gt; ar i &amp;lt;- f i}
    ar

inl rec zeroes = function
    | x :: x' -&amp;gt; array_init x (inl _ -&amp;gt; zeroes x')
    | () -&amp;gt; &quot;&quot;

inl dim = (4,4,4,4)
inl ar = zeroes dim
ar 0 0 0 2 &amp;lt;- &quot;princess&quot;

// Reverses a tuple
inl tuple_rev = 
    inl rec loop state = function
        | x :: xs -&amp;gt; loop (x :: state) xs
        | () -&amp;gt; state
    loop ()

// Correct version
inl rec find_index {next state} = function
    | ar &amp;amp; @array_is _ -&amp;gt; 
        inl body {next i} = find_index {next state=i::state} (ar i)
        for' {from=0; near_to=array_length ar; finally=next; body}
    | &quot;princess&quot; -&amp;gt; tuple_rev state
    | _ -&amp;gt; next ()

find_index {state=(); next = inl _ -&amp;gt; failwith (type (dim)) &quot;The princess is in another castle.&quot;} ar
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;There was a lot of material covered here. The logic of &lt;code&gt;find_index&lt;/code&gt; as well as the other loop unrolling functions might seem confusing to the uninitiated, and would no doubt be to the author had he encountered this over a year ago. But ultimately the function is just 5 lines long and there is nothing particular magical about it; the function is fully explicit. Thinking about it for a long time will help and so will mentally rehearsing the motions until the pieces fall into place.&lt;/p&gt;
&lt;p&gt;One useful tool in gaining understanding is trying to manually expand the loop. Here is what happens if &lt;code&gt;find_index&lt;/code&gt; is expanded a single step of recursion.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl rec find_index {next state} = function
    | ar &amp;amp; @array_is _ -&amp;gt; 
        inl body {next i} = 
            inl state = i :: state
            match ar i with
            | ar &amp;amp; @array_is _ -&amp;gt; 
                inl body {next i} = find_index {next state=i::state} (ar i)
                for' {from=0; near_to=array_length ar; finally=next; body}
            | &quot;princess&quot; -&amp;gt; tuple_rev state
            | _ -&amp;gt; next ()
        for' {from=0; near_to=array_length ar; finally=next; body}
    | &quot;princess&quot; -&amp;gt; tuple_rev state
    | _ -&amp;gt; next ()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Supposing the input is one dimensional, that is if the type of the array was &lt;code&gt;string []&lt;/code&gt; it become possible to do more partial evaluation by hand.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl rec find_index {next state} = function
    | ar &amp;amp; @array_is _ -&amp;gt; 
        inl body {next i} = 
            inl state = i :: state
            match ar i with
            | &quot;princess&quot; -&amp;gt; tuple_rev state
            | _ -&amp;gt; next ()
        for' {from=0; near_to=array_length ar; finally=next; body}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This program corresponds to a single loop and is in fact what the program would get specialized to had it been given only a string array as input.&lt;/p&gt;
&lt;p&gt;Seeing similar examples of this pattern will no doubt help and there will be a significant number of them throughout these tutorials.&lt;/p&gt;
&lt;h4&gt;Arrays&lt;/h4&gt;
&lt;p&gt;Compared to the intensity of the previous section, this one should be a breeze in comparison. The most important of the array functions &lt;code&gt;init&lt;/code&gt; was already introduced.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let array =
    (
    &quot;Array&quot;,[tuple;loops],&quot;The array module&quot;,
    &quot;&quot;&quot;
open Loops

/// Creates an empty array with the given type.
/// t -&amp;gt; t array
inl empty t = array_create t 0

/// Creates a singleton array with the given element.
/// x -&amp;gt; t array
inl singleton x =
    inl ar = array_create x 1
    ar 0 &amp;lt;- x
    ar

/// Applies a function to each elements of the collection, threading an accumulator argument through the computation.
/// If the input function is f and the elements are i0..iN then computes f..(f i0 s)..iN.
/// (s -&amp;gt; a -&amp;gt; s) -&amp;gt; s -&amp;gt; a array -&amp;gt; s
inl foldl f state ar = for {from=0; near_to=array_length ar; state; body=inl {state i} -&amp;gt; f state (ar i)}

/// Applies a function to each element of the array, threading an accumulator argument through the computation. 
/// If the input function is f and the elements are i0...iN then computes f i0 (...(f iN s)).
/// (a -&amp;gt; s -&amp;gt; a) -&amp;gt; a array -&amp;gt; s -&amp;gt; s
inl foldr f ar state = for {from=array_length ar-1; down_to=0; state; body=inl {state i} -&amp;gt; f (ar i) state}
...
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here are some of the basic ones. Having a loop as a part of the standard library makes it really easy to implement the two &lt;code&gt;fold&lt;/code&gt; functions. Unlike in F# where &lt;code&gt;foldl&lt;/code&gt; and &lt;code&gt;foldr&lt;/code&gt; are &lt;code&gt;fold&lt;/code&gt; and &lt;code&gt;foldBack&lt;/code&gt;, here the Haskell naming convention has been followed for no special reason apart from &lt;code&gt;foldr&lt;/code&gt; being more elegant to write than &lt;code&gt;foldBack&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;// Creates an array given a dimension and a generator function to compute the elements.
// ?(.is_static) -&amp;gt; int -&amp;gt; (int -&amp;gt; a) -&amp;gt; a array
inl init = 
    inl body is_static n f =
        assert (n &amp;gt;= 0) &quot;The input to init needs to be greater or equal to 0.&quot;
        inl typ = type (f 0)
        inl ar = array_create typ n
        inl d = 
            inl d = {near_to=n; body=inl {i} -&amp;gt; ar i &amp;lt;- f i}
            if is_static then {d with from = 0} else {d with static_from = 0}
        for d
        ar
    function
    | .static -&amp;gt; body true
    | n -&amp;gt; body false n
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;init&lt;/code&gt; here is a example how the architecture of a Spiral function differs from that in F#. The interesting part is that &lt;code&gt;.static&lt;/code&gt; can be passed into it as an optional argument that will allow it to be run statically. Otherwise this is the same as the &lt;code&gt;init&lt;/code&gt; from the previous section so no demonstration of it should be necessary.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Builds a new array that contains elements of a given array.
/// a array -&amp;gt; a array
met copy ar = init (array_length ar) ar

/// Builds a new array whose elements are the result of applying a given function to each of the elements of the array.
/// (a -&amp;gt; b) -&amp;gt; a array -&amp;gt; a array
inl map f ar = init (array_length ar) (ar &amp;gt;&amp;gt; f)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;init&lt;/code&gt; is useful as it is easy to derive other functions from it. These two function exactly as in F# and other functional languages.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Returns a new array containing only elements of the array for which the predicate function returns `true`.
/// (a -&amp;gt; bool) -&amp;gt; a array -&amp;gt; a array
inl filter f ar =
    inl ar' = array_create ar.elem_type (array_length ar)
    inl count = foldl (inl s x -&amp;gt; if f x then ar' s &amp;lt;- x; s+1 else s) (dyn 0) ar
    init count ar'
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Being able to apply arrays directly instead of having to index them allows them to be used more like functions. Also worthy of note is the &lt;code&gt;ar.elem_type&lt;/code&gt;. In Spiral there is no inference, only propagation so the type of an array must be extracted directly. In Spiral, types are first class and can be used as values. This can be exploited to get around the lack of inference in most cases.&lt;/p&gt;
&lt;p&gt;Arrays are a simple examples of how types might be held in structures.&lt;/p&gt;
&lt;p&gt;Unlike other .NET types, arrays are built into the language directly.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Merges all the arrays in a tuple into a single one.
/// a array tuple -&amp;gt; a array
inl append l =
    inl ar' = array_create ((fst l).elem_type) (Tuple.foldl (inl s l -&amp;gt; s + array_length l) 0 l)
    inl ap s ar = foldl (inl i x -&amp;gt; ar' i &amp;lt;- x; i+1) s ar
    Tuple.foldl ap (dyn 0) l |&amp;gt; ignore
    ar'
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Like how &lt;code&gt;init&lt;/code&gt; can match on its first arguments before deciding whether to run statically or not, being able to iterate over tuples in order to merge the arrays is a standard use case for intensional polymorphism.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Flattens an array of arrays into a single one.
/// a array array -&amp;gt; a array
inl concat ar =
    inl count = foldl (inl s ar -&amp;gt; s + array_length ar) (dyn 0) ar
    inl ar' = array_create ar.elem_type.elem_type count
    (foldl &amp;lt;&amp;lt; foldl) (inl i x -&amp;gt; ar' i &amp;lt;- x; i+1) (dyn 0) ar |&amp;gt; ignore
    ar'
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;foldl &amp;lt;&amp;lt; foldl&lt;/code&gt; is a good way to compose folds for nested arrays.&lt;/p&gt;
&lt;p&gt;Writing functions in this higher order style is the optimal way to program in Spiral. For contrast, here is how &lt;code&gt;concat&lt;/code&gt; is implemented in F#'s source.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let concatArrays (arrs : 'T[][]) : 'T[] =
    let mutable acc = 0    
    for h in arrs do
        acc &amp;lt;- acc + h.Length        
        
    let res = Microsoft.FSharp.Primitives.Basics.Array.zeroCreateUnchecked acc  
        
    let mutable j = 0
    for i = 0 to arrs.Length-1 do     
        let h = arrs.[i]
        let len = h.Length
        Array.Copy(h,0,res,j,len)        
        j &amp;lt;- j + len
    res               
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It is not quite C, but it is the same style inherited from it. All fast languages tend to regress to that particular kind of programming when performance or just the guarantee of it becomes a necessity.&lt;/p&gt;
&lt;p&gt;Even in a pure and lazy language like Haskell, looking under the hood of some of its fast libraries will reveal this and other kinds of regressions.&lt;/p&gt;
&lt;p&gt;On the strength of its inlining guarantees, the goal of Spiral is to liberate programmers from that gravitic impulse towards C.&lt;/p&gt;
&lt;p&gt;During the last 45 years there have been numerous attempts at bridging the expressiveness of dynamic languages with the performance of C, none of which have borne fruit.&lt;/p&gt;
&lt;p&gt;Assuming Spiral can be scaled, it or some other language of similar design with powerful first class types and staging features will finally break beyond the atmosphere to bring light of civilization into the cold, dead space that lies beyond.&lt;/p&gt;
&lt;p&gt;The above example is not that bad actually. It is only 12 lines in F# vs 5 in Spiral. It is hardly a reason to create a new language and propose the jettison of parametric polymoprhism.&lt;/p&gt;
&lt;p&gt;In the following chapters there will be examples of programs, most notably of Spiral's tensors, whose functionalities have such requirements that would pretty much break any existing language.&lt;/p&gt;
&lt;p&gt;The next two functions are all that remains of the module.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Tests if all the elements of the array satisfy the given predicate.
/// (a -&amp;gt; bool) -&amp;gt; a array -&amp;gt; bool
inl forall f ar = for' {from=0; near_to=array_length ar; state=true; body = inl {next state i} -&amp;gt; f (ar i) &amp;amp;&amp;amp; next state}

/// Tests if any the element of the array satisfies the given predicate.
/// (a -&amp;gt; bool) -&amp;gt; a array -&amp;gt; bool
inl exists f ar = for' {from=0; near_to=array_length ar; state=false; body = inl {next state i} -&amp;gt; f (ar i) || next state}

{empty singleton foldl foldr init copy map filter append concat forall exists} 
|&amp;gt; stack
    &quot;&quot;&quot;) |&amp;gt; module_
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;On the F# side it is necessary to wrap the module in a type using the &lt;code&gt;module_&lt;/code&gt; function. That &lt;code&gt;|&amp;gt; stack&lt;/code&gt; at the end is not necessary and only has something to do with the way the language is currently implemented. Omitting the conversion of the module to a layout type would not break anything, at most there might be a minor compilation slowdown. More details are (to be) provided in the user guide.&lt;/p&gt;
&lt;p&gt;Modules with no free variables such as the &lt;code&gt;Array&lt;/code&gt; module whose fields are entirely made of combinators always get converted into naked types rather than variables and hence have no overhead.&lt;/p&gt;
&lt;h3&gt;3: Union Types and Lists&lt;/h3&gt;
&lt;p&gt;Discriminated union types in Spiral take direct inspiration from F#'s own. Having said that, the lack of type inference and the aggressive unboxing of them by the Spiral evaluator makes them less convenient to work with. Nonetheless, union types capture the essence of dynamism and are absolutely essential in a modern language.&lt;/p&gt;
&lt;p&gt;Since Spiral has first class types, type string literals take the place of case names. Furthermore, types can be defined anywhere in the program rather than only at the top level like in F#.&lt;/p&gt;
&lt;p&gt;A non-recursive union type like the Option can be defined like the following. &lt;code&gt;\/&lt;/code&gt; is the type union keyword operator. It has a lower precedence than tuples.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl Option x = .Some, x \/ .None

// constructors
inl some x = box (Option x) (.Some, x)
inl none x = box (Option x) (.None)

none int64
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Union0 =
    | Union0Case0 of Tuple1
    | Union0Case1
and Tuple1 =
    struct
    val mem_0: int64
    new(arg_mem_0) = {mem_0 = arg_mem_0}
    end
Union0Case1
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Commentary on the quality of the generated code will be left for the user guide. Pattern matching on the boxed union values can be done the same way as in F#.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;match none int64 with
| .Some, x -&amp;gt; x
| .None -&amp;gt; -11
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;11L
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The word 'staging' means 'defering for later'. Just like literals, the creation of union types is deferred for as long as possible in Spiral.&lt;/p&gt;
&lt;p&gt;In order to actually instantiate the type, it is necessary to &lt;code&gt;dyn&lt;/code&gt; it or return it from a join point or an if branch. The end of the entire program also qualifies for instantiation.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;match none int64 |&amp;gt; dyn with
| .Some, x -&amp;gt; x
| .None -&amp;gt; -11
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Union0 =
    | Union0Case0 of Tuple1
    | Union0Case1
and Tuple1 =
    struct
    val mem_0: int64
    new(arg_mem_0) = {mem_0 = arg_mem_0}
    end
let (var_0: Union0) = Union0Case1
match var_0 with
| Union0Case0(var_1) -&amp;gt;
    var_1.mem_0
| Union0Case1 -&amp;gt;
    -11L
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above is roughly what would be expect to in F# or the MLs. Spiral's pattern matching is more flexible though.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl TypeA = .A \/ .B
inl TypeB = .B \/ .C

inl f = function
    | .A -&amp;gt; 1
    | .B -&amp;gt; 2
    | .C -&amp;gt; 3

box TypeA .A |&amp;gt; dyn |&amp;gt; f |&amp;gt; ignore
box TypeB .C |&amp;gt; dyn |&amp;gt; f |&amp;gt; ignore
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Union0 =
    | Union0Case0
    | Union0Case1
and Union1 =
    | Union1Case0
    | Union1Case1
let (var_0: Union0) = Union0Case0
let (var_1: int64) =
    match var_0 with
    | Union0Case0 -&amp;gt;
        1L
    | Union0Case1 -&amp;gt;
        2L
let (var_2: Union1) = Union1Case1
let (var_3: int64) =
    match var_2 with
    | Union1Case0 -&amp;gt;
        2L
    | Union1Case1 -&amp;gt;
        3L
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Despite this added flexibility, it is in fact exhaustive. Unlike in F#, this is not a warning, but an error as Spiral's union types are intended to be used on devices which have no capabilities for raising exceptions.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl TypeA = .A \/ .B

inl f = function
    | .A -&amp;gt; 1
    | .C -&amp;gt; 3

box TypeA .A |&amp;gt; dyn |&amp;gt; f |&amp;gt; ignore
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;...
Error trace on line: 35, column: 7 in file &quot;example&quot;.
    | .A -&amp;gt; 1
      ^
Error trace on line: 36, column: 7 in file &quot;example&quot;.
    | .C -&amp;gt; 3
      ^
Pattern miss error. The argument is type (type_lit (B))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As it never matches &lt;code&gt;.B&lt;/code&gt; it goes over the edge and returns a type error.&lt;/p&gt;
&lt;p&gt;Here is how recursive datatypes like lists might be defined.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let example = 
    &quot;example&quot;,[option;tuple;loops],&quot;Module description.&quot;,
    &quot;&quot;&quot;
open Loops
inl rec List x = join_type () \/ x, List x

/// Creates an empty list with the given type.
/// t -&amp;gt; List t
inl empty x = box (List x) ()

/// Creates a single element list with the given type.
/// x -&amp;gt; List x
inl singleton x = box (List x) (x, empty x)

/// Immutable appends an element to the head of the list.
/// x -&amp;gt; List x -&amp;gt; List x
inl cons a b = 
    inl t = List a
    box t (a, box t b)

singleton 3 |&amp;gt; cons 2 |&amp;gt; cons 1
    &quot;&quot;&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Rec0 =
    | Rec0Case0
    | Rec0Case1 of Tuple1
and Tuple1 =
    struct
    val mem_0: int64
    val mem_1: Rec0
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
(Rec0Case1(Tuple1(1L, (Rec0Case1(Tuple1(2L, (Rec0Case1(Tuple1(3L, Rec0Case0)))))))))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;join_type&lt;/code&gt; is similar to the standard &lt;code&gt;join&lt;/code&gt; except it is used to define types. It always returns a naked type and on entry converts everything in the environment to their types and that includes literals. That means that passing literals through the type join point requires doing it on the type level or inside a layout type.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Creates a list by calling the given generator on each index.
/// ?(.static) -&amp;gt; int -&amp;gt; (int -&amp;gt; a) -&amp;gt; List a
inl init =
    inl body is_static n f =
        inl t = type (f 0)
        inl d = {near_to=n; state=empty t; body=inl {next i state} -&amp;gt; cons (f i) (next state)}
        if is_static then for' {d with static_from=0}
        else for' {d with from=0}

    function
    | .static -&amp;gt; body true
    | x -&amp;gt; body false x
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above function resembles the &lt;code&gt;init&lt;/code&gt; in the &lt;code&gt;Array&lt;/code&gt; module in structure. There is an interesting usage of the breakable &lt;code&gt;for'&lt;/code&gt; here. Usually the &lt;code&gt;next&lt;/code&gt; is intended to be called in tail position, but here it is not. Instead the &lt;code&gt;state&lt;/code&gt; is used merely to ship the empty list to the end of it.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl x = init.static 3 id |&amp;gt; dyn
()
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Rec0 =
    | Rec0Case0
    | Rec0Case1 of Tuple1
and Tuple1 =
    struct
    val mem_0: int64
    val mem_1: Rec0
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let (var_0: Rec0) = (Rec0Case1(Tuple1(0L, (Rec0Case1(Tuple1(1L, (Rec0Case1(Tuple1(2L, Rec0Case0)))))))))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above is nearly identical to the &lt;code&gt;singleton 3 |&amp;gt; cons 2 |&amp;gt; cons 1&lt;/code&gt; example.&lt;/p&gt;
&lt;p&gt;The next function on the list would be the &lt;code&gt;map&lt;/code&gt;. This is where things start to get tricky. Here is an example of it that does not work.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl rec map f l = 
    inl loop l =
        match l with
        | x,xs -&amp;gt; cons (f x) (map f xs)
        | () -&amp;gt; l // Error #1
        : ??? // Error #2
    if box_is l then loop l
    else join loop l
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Error #2 should be obvious - there is no return type. Error #1 is more subtle - and is related to the way pattern matching is compiled.&lt;/p&gt;
&lt;p&gt;Backtracking to the earlier example.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl TypeA = .A \/ .B

inl f = function
    | x -&amp;gt; x

box TypeA .A |&amp;gt; dyn |&amp;gt; f |&amp;gt; ignore
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Union0 =
    | Union0Case0
    | Union0Case1
let (var_0: Union0) = Union0Case0
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above is as one would expect.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl TypeA = .A \/ .B

inl f = function
    | &quot;qwe&quot; -&amp;gt; ()
    | x -&amp;gt; x

box TypeA .A |&amp;gt; dyn |&amp;gt; f |&amp;gt; ignore
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;...
Error trace on line: 35, column: 7 in file &quot;example&quot;.
    | &quot;qwe&quot; -&amp;gt; ()
      ^
All the cases in pattern matching clause with dynamic data must have the same type.
Got: [type_lit (A), type_lit (B)]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The error message gives an indication of what is wrong. In Spiral, the match case is not what triggers unboxing - the operations that actually need to unbox the union type are what do it. That means literal, type literal, type equality, tuple and module patterns. This applies even to those patterns that have nothing to do with the variable's type and would have been expected to be skipped.&lt;/p&gt;
&lt;p&gt;It gets worse. Spiral is really aggressive at rewriting the terms it is unboxing even if they are outside its intended scope.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl x = box TypeA .A |&amp;gt; dyn
print_static x // var (union {type_lit (A) | type_lit (B)})

match x with
| &quot;qwe&quot; -&amp;gt; ()
| _ -&amp;gt; 
    // prints twice
    // type (type_lit (A))
    // type (type_lit (B))
    print_static x 
    x
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;...
All the cases in pattern matching clause with dynamic data must have the same type.
Got: [type_lit (A), type_lit (B)]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The way things are currently is the fault of whoever wrote the pattern matching compiler. Since patterns would be difficult to compile otherwise, internally Spiral uses the same mechanism used to do common subexpression elimination to pass information over multiple branches. There is no issue at all with this when not dealing with union types, but here there is some friction there.&lt;/p&gt;
&lt;p&gt;There is something good about the current arrangement that MLs do not have.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl TypeA = .A \/ .B \/ .C \/ .D

inl f g = function
    | .A -&amp;gt; 1
    | .B -&amp;gt; 2
    | x -&amp;gt; 
        dyn &quot;Just passing through.&quot; |&amp;gt; ignore
        g x

f (function
    | .C -&amp;gt; 3
    | .D -&amp;gt; 4)
    (box TypeA .A |&amp;gt; dyn )
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Union0 =
    | Union0Case0
    | Union0Case1
    | Union0Case2
    | Union0Case3
let (var_0: Union0) = Union0Case0
match var_0 with
| Union0Case0 -&amp;gt;
    1L
| Union0Case1 -&amp;gt;
    2L
| Union0Case2 -&amp;gt;
    let (var_1: string) = &quot;Just passing through.&quot;
    3L
| Union0Case3 -&amp;gt;
    let (var_2: string) = &quot;Just passing through.&quot;
    4L
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;That would be that the exhaustiveness check is not local to the pattern. As long as all the branches of it are properly handled, the pattern does not have to be squeezed all into one place and can be composed. This is one of the safety aspects at compile time that F# does not have.&lt;/p&gt;
&lt;p&gt;Regardless of the merits and demerits of this approach, in order to complete the map function some kind of method for getting what would be the generic parameter of the list in a parametric language is needed.&lt;/p&gt;
&lt;h4&gt;Type Splitting and Generic Parameters&lt;/h4&gt;
&lt;pre&gt;
&lt;code&gt;/// Returns the element type of the list.
/// a List -&amp;gt; a type
inl elem_type l =
    match split l with
    | (), (a,b) when eq_type (List a) l -&amp;gt; a
    | _ -&amp;gt; error_type &quot;Expected a List in elem_type.&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The way the &lt;code&gt;split&lt;/code&gt; function works is that it splits an union type into its individual components and returns them as a tuple. After it has been split, this makes it possible to match on the types of it directly.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl Option x = .Some, x \/ .None
print_static (Option int64) // type (union {[type_lit (Some), int64] | type_lit (None)})
print_static (Option int64 |&amp;gt; split) // [type ([type_lit (Some), int64]), type (type_lit (None))]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is a bit of a hack. Spiral has union and not sum types, meaning they are not ordered. Or better put, they are ordered, just not based on how they were entered.&lt;/p&gt;
&lt;p&gt;The above example works for lists and is how they are implemented in the standard library, but there are alternative ways of implementing the basic list.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl rec List x = join_type 
    inl el = stack {elem_type=x}
    el, () \/ el, (x, List x)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;They all involve sticking the type in directly by using layout types. Since layout types capture the scope by the expression instead of type and since &lt;code&gt;x&lt;/code&gt; can only ever be a naked type once it passes the &lt;code&gt;join_type&lt;/code&gt; point, that assures that it will always be instantiated.&lt;/p&gt;
&lt;p&gt;If adding &lt;code&gt;el&lt;/code&gt; to all the branches of a larger type by hand is tedious, it is possible to automate that. It needs to be done inside the type join point. Here is how it would be done on a tuple.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl rec List x = join_type 
    inl el = stack {elem_type=x}
    inl typ = () \/ x, List x
    split typ
    |&amp;gt; Tuple.map (inl x -&amp;gt; el, x)
    |&amp;gt; Tuple.reducel (inl a b -&amp;gt; a \/ b)

// [type ([layout_stack {elem_type=type (int64)}, []]), type ([layout_stack {elem_type=type (int64)}, [int64, rec_type 0]])]
print_static (split (List int64))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Using first class types Spiral can emulate what would be generic parameters of a container in a language with parametric polymorphism.&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;elem_type&lt;/code&gt; in hand, it becomes possible to implement map.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Builds a new list whose elements are the results of applying the given function to each of the elements of the list.
/// (a List -&amp;gt; b List) -&amp;gt; a List -&amp;gt; List b
inl rec map f l = 
    inl t = elem_type l
    inl loop = function
        | x,xs -&amp;gt; cons (f x) (map f xs)
        | () -&amp;gt; empty t
    if box_is l then loop l
    else join loop l : List t

inl l = init.static 3 id |&amp;gt; map ((*) 2) |&amp;gt; dyn
()
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Rec0 =
    | Rec0Case0
    | Rec0Case1 of Tuple1
and Tuple1 =
    struct
    val mem_0: int64
    val mem_1: Rec0
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let (var_0: Rec0) = (Rec0Case1(Tuple1(0L, (Rec0Case1(Tuple1(2L, (Rec0Case1(Tuple1(4L, Rec0Case0)))))))))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The static version of map works fine now.&lt;/p&gt;
&lt;p&gt;Here is how the non-static version looks like.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;init 3 id |&amp;gt; map ((*) 2) |&amp;gt; dyn
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Rec0 =
    | Rec0Case0
    | Rec0Case1 of Tuple1
and Tuple1 =
    struct
    val mem_0: int64
    val mem_1: Rec0
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let rec method_1((var_0: int64)): Rec0 =
    let (var_1: bool) = (var_0 &amp;lt; 3L)
    if var_1 then
        let (var_2: int64) = (var_0 + 1L)
        let (var_3: Rec0) = method_1((var_2: int64))
        (Rec0Case1(Tuple1(var_0, var_3)))
    else
        Rec0Case0
and method_2((var_0: Rec0)): Rec0 =
    match var_0 with
    | Rec0Case0 -&amp;gt;
        Rec0Case0
    | Rec0Case1(var_1) -&amp;gt;
        let (var_2: int64) = var_1.mem_0
        let (var_3: Rec0) = var_1.mem_1
        let (var_4: int64) = (2L * var_2)
        let (var_5: Rec0) = method_2((var_3: Rec0))
        (Rec0Case1(Tuple1(var_4, var_5)))
let (var_0: int64) = 0L
let (var_1: Rec0) = method_1((var_0: int64))
method_2((var_1: Rec0))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This does not demonstrate Spiral's true power. The function can map over lists that are partially static.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl l = dyn (singleton 3) |&amp;gt; cons 2 |&amp;gt; cons 1 |&amp;gt; cons 0
map ((*) 2) l |&amp;gt; dyn
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Rec0 =
    | Rec0Case0
    | Rec0Case1 of Tuple1
and Tuple1 =
    struct
    val mem_0: int64
    val mem_1: Rec0
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let rec method_1((var_0: Rec0)): Rec0 =
    match var_0 with
    | Rec0Case0 -&amp;gt;
        Rec0Case0
    | Rec0Case1(var_1) -&amp;gt;
        let (var_2: int64) = var_1.mem_0
        let (var_3: Rec0) = var_1.mem_1
        let (var_4: int64) = (2L * var_2)
        let (var_5: Rec0) = method_1((var_3: Rec0))
        (Rec0Case1(Tuple1(var_4, var_5)))
let (var_0: Rec0) = (Rec0Case1(Tuple1(3L, Rec0Case0)))
let (var_1: Rec0) = method_1((var_0: Rec0))
(Rec0Case1(Tuple1(0L, (Rec0Case1(Tuple1(2L, (Rec0Case1(Tuple1(4L, var_1)))))))))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The first 3 elements are done at compile time, and the rest is done at runtime.&lt;/p&gt;
&lt;p&gt;With the map done, &lt;code&gt;foldl&lt;/code&gt; and &lt;code&gt;foldr&lt;/code&gt; are straightforward enough.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Applies a function f to each element of the collection, threading an accumulator argument through the computation. 
/// The fold function takes the second argument, and applies the function f to it and the first element of the list. 
/// Then, it feeds this result into the function f along with the second element, and so on. It returns the final result. 
/// If the input function is f and the elements are i0...iN, then this function computes f (... (f s i0) i1 ...) iN.
/// (s -&amp;gt; a -&amp;gt; s) -&amp;gt; s -&amp;gt; a List -&amp;gt; s
inl rec foldl f s l = 
    inl loop = function
        | x, xs -&amp;gt; foldl f (f s x) xs
        | () -&amp;gt; s
    if box_is l then loop l
    else join loop l : s

inl l = dyn (singleton 3) |&amp;gt; cons 2 |&amp;gt; cons 1 |&amp;gt; cons 0
foldl (+) (dyn 0) l
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Rec0 =
    | Rec0Case0
    | Rec0Case1 of Tuple1
and Tuple1 =
    struct
    val mem_0: int64
    val mem_1: Rec0
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let rec method_1((var_0: Rec0), (var_1: int64)): int64 =
    match var_0 with
    | Rec0Case0 -&amp;gt;
        var_1
    | Rec0Case1(var_2) -&amp;gt;
        let (var_3: int64) = var_2.mem_0
        let (var_4: Rec0) = var_2.mem_1
        let (var_5: int64) = (var_1 + var_3)
        method_1((var_4: Rec0), (var_5: int64))
let (var_0: Rec0) = (Rec0Case1(Tuple1(3L, Rec0Case0)))
let (var_1: int64) = 0L
let (var_2: int64) = (var_1 + 1L)
let (var_3: int64) = (var_2 + 2L)
method_1((var_0: Rec0), (var_3: int64))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;One thing the example above demonstrates is that Spiral does require the user to know whether compile time or runtime execution is being targeted. The above fragment is not ideal since it would be better to sum the static part of the list and then &lt;code&gt;dyn&lt;/code&gt; the state rather than do so at the beginning.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Applies a function to each element of the collection, threading an accumulator argument through the computation. 
/// If the input function is f and the elements are i0...iN, then this function computes f i0 (...(f iN s)).
/// (a -&amp;gt; s -&amp;gt; s) -&amp;gt; a List -&amp;gt; s -&amp;gt; s
inl rec foldr f l s = 
    inl loop = function
        | x, xs -&amp;gt; f x (foldr f xs s)
        | () -&amp;gt; s
    if box_is l then loop l
    else join loop l : s

inl l = dyn (singleton 3) |&amp;gt; cons 2 |&amp;gt; cons 1 |&amp;gt; cons 0
foldr (+) l (dyn 0)
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Rec0 =
    | Rec0Case0
    | Rec0Case1 of Tuple1
and Tuple1 =
    struct
    val mem_0: int64
    val mem_1: Rec0
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let rec method_1((var_0: Rec0), (var_1: int64)): int64 =
    match var_0 with
    | Rec0Case0 -&amp;gt;
        var_1
    | Rec0Case1(var_2) -&amp;gt;
        let (var_3: int64) = var_2.mem_0
        let (var_4: Rec0) = var_2.mem_1
        let (var_5: int64) = method_1((var_4: Rec0), (var_1: int64))
        (var_3 + var_5)
let (var_0: Rec0) = (Rec0Case1(Tuple1(3L, Rec0Case0)))
let (var_1: int64) = 0L
let (var_2: int64) = method_1((var_0: Rec0), (var_1: int64))
let (var_3: int64) = (2L + var_2)
(1L + var_3)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The next are &lt;code&gt;head&lt;/code&gt; and &lt;code&gt;tail&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;open Option

/// Returns the first element of the list.
/// a List -&amp;gt; a Option
inl head l =
    inl t = elem_type l
    match l with
    | x, xs -&amp;gt; some x
    | () -&amp;gt; none t

/// Returns the list without the first element.
/// a List -&amp;gt; a List Option
inl tail l =
    inl t = elem_type l
    match l with
    | x, xs -&amp;gt; some xs
    | () -&amp;gt; none (List t)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As the above are straightforward so there is no need to run them. That having said, it would be interesting to know how it might be possible to implement them in continuation passing style for greater efficiency.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Returns the first element of the list.
/// a List -&amp;gt; {some=(a -&amp;gt; a) none=(a type -&amp;gt; a)} -&amp;gt; a
inl head' l {some none} =
    inl t = elem_type l
    match l with
    | x, xs -&amp;gt; some x
    | () -&amp;gt; none t

/// Returns the list without the first element.
/// a List -&amp;gt; {some=(a List -&amp;gt; a List) none=(a List type -&amp;gt; a List)} -&amp;gt; a List
inl tail' l {some none} =
    inl t = elem_type l
    match l with
    | x, xs -&amp;gt; some xs
    | () -&amp;gt; none (List t)

inl l = dyn (singleton 3)
tail' l {
    some = id
    none = inl x -&amp;gt; failwith x &quot;The list is empty.&quot;
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Rec0 =
    | Rec0Case0
    | Rec0Case1 of Tuple1
and Tuple1 =
    struct
    val mem_0: int64
    val mem_1: Rec0
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let (var_0: Rec0) = (Rec0Case1(Tuple1(3L, Rec0Case0)))
match var_0 with
| Rec0Case0 -&amp;gt;
    (failwith &quot;The list is empty.&quot;)
| Rec0Case1(var_1) -&amp;gt;
    let (var_3: int64) = var_1.mem_0
    var_1.mem_1
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The best is left for &lt;code&gt;last&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Returns the last element of the list.
/// a List -&amp;gt; a Option
inl last l =
    inl t = elem_type l
    foldl (inl _ x -&amp;gt; some x) (none t) l

inl l = dyn (singleton 3) |&amp;gt; cons 2 |&amp;gt; cons 1 |&amp;gt; cons 0
last l 
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Rec0 =
    | Rec0Case0
    | Rec0Case1 of Tuple1
and Tuple1 =
    struct
    val mem_0: int64
    val mem_1: Rec0
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
and Union2 =
    | Union2Case0 of Tuple3
    | Union2Case1
and Tuple3 =
    struct
    val mem_0: int64
    new(arg_mem_0) = {mem_0 = arg_mem_0}
    end
let rec method_1((var_0: Rec0)): Union2 =
    match var_0 with
    | Rec0Case0 -&amp;gt;
        (Union2Case0(Tuple3(2L)))
    | Rec0Case1(var_1) -&amp;gt;
        let (var_2: int64) = var_1.mem_0
        let (var_3: Rec0) = var_1.mem_1
        method_2((var_3: Rec0), (var_2: int64))
and method_2((var_0: Rec0), (var_1: int64)): Union2 =
    match var_0 with
    | Rec0Case0 -&amp;gt;
        (Union2Case0(Tuple3(var_1)))
    | Rec0Case1(var_2) -&amp;gt;
        let (var_3: int64) = var_2.mem_0
        let (var_4: Rec0) = var_2.mem_1
        method_2((var_4: Rec0), (var_3: int64))
let (var_0: Rec0) = (Rec0Case1(Tuple1(3L, Rec0Case0)))
method_1((var_0: Rec0))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above way of specializing it is close to ideal. It would be better had &lt;code&gt;method_1&lt;/code&gt; been inlined, but this is a decent showing. As can be seen, the option type is staged and only the int inside is passed through until it is time to return from the function at which point the instantiation happens. In F#, this way of doing &lt;code&gt;last&lt;/code&gt; would be grossly inefficient as a new option would be instantiated at each step. Very few languages allow passing of literals across call boundaries due to the uncertainty whether the optimizer will diverge. Spiral achieves its efficiency by making dealing with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Halting_problem&quot; rel=&quot;nofollow&quot;&gt;halting problem&lt;/a&gt; the user's responsibility. This is not a bad strategy - as the halting problem is NP Hard, other compilers' optimizers have no choice but to rely on fallible heuristics whereas the user has to determine whether the program will terminate anyway and has no say in what the black box is deciding. It is not so in Spiral.&lt;/p&gt;
&lt;p&gt;The essence of Spiral is to convert the termination proofs implicitly and informally present in the program into polymorphism.&lt;/p&gt;
&lt;p&gt;Here is how it would be done in CPS for that last bit of efficiency.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Returns the last element of the list.
/// a List -&amp;gt; {some=(a -&amp;gt; a) none=(a type -&amp;gt; a)} -&amp;gt; a
inl rec last' l {some none} =
    inl t = elem_type l
    inl loop = function
        | x, xs -&amp;gt; last' xs {some none=some x}
        | () -&amp;gt; none t
    if box_is l then loop l
    else join loop l : none t

inl l = dyn (singleton 3)
last' l {
    some = inl x _ -&amp;gt; x
    none = inl t -&amp;gt; failwith t &quot;The list is empty.&quot;
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Rec0 =
    | Rec0Case0
    | Rec0Case1 of Tuple1
and Tuple1 =
    struct
    val mem_0: int64
    val mem_1: Rec0
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let rec method_1((var_0: Rec0)): int64 =
    match var_0 with
    | Rec0Case0 -&amp;gt;
        (failwith &quot;The list is empty.&quot;)
    | Rec0Case1(var_1) -&amp;gt;
        let (var_3: int64) = var_1.mem_0
        let (var_4: Rec0) = var_1.mem_1
        method_2((var_4: Rec0), (var_3: int64))
and method_2((var_0: Rec0), (var_1: int64)): int64 =
    match var_0 with
    | Rec0Case0 -&amp;gt;
        var_1
    | Rec0Case1(var_2) -&amp;gt;
        let (var_3: int64) = var_2.mem_0
        let (var_4: Rec0) = var_2.mem_1
        method_2((var_4: Rec0), (var_3: int64))
let (var_0: Rec0) = (Rec0Case1(Tuple1(3L, Rec0Case0)))
method_1((var_0: Rec0))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It can no longer be implemented in terms of fold, but otherwise is rather simple.&lt;/p&gt;
&lt;p&gt;Note that &lt;code&gt;head'&lt;/code&gt;, &lt;code&gt;tail'&lt;/code&gt; and &lt;code&gt;last'&lt;/code&gt; are just more generic versions of the non-CPS versions. Assuming the 3 original functions were missing, here is how they might be implemented in terms of CPS'd ones.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Returns the first element of the list.
/// a List -&amp;gt; a Option
inl head l = head' l {some none}

/// Returns the list without the first element.
/// a List -&amp;gt; a List Option
inl tail l = tail' l {some none}

/// Returns the last element of the list.
/// a List -&amp;gt; a Option
inl last l = last' l {some=const &amp;lt;&amp;lt; some; none}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;const&lt;/code&gt; is simply &lt;code&gt;inl x _ -&amp;gt; x&lt;/code&gt; as was used inside &lt;code&gt;some&lt;/code&gt; of the previous example. The continuation passing style is great for writing generic code in Spiral as it meshes well with its typing scheme. The monadic computations that will be shown in the following chapters are just syntax sugar over CPS.&lt;/p&gt;
&lt;p&gt;The capacity to make specialized functions from generic one like the above is an important factor in ensuring code correctness. Eliminating code duplication and ensuring single responsibility is possible without performance impact in Spiral.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Returns a new list that contains the elements of the first list followed by elements of the second.
/// a List -&amp;gt; a List -&amp;gt; a List
inl append = foldr cons

/// Returns a new list that contains the elements of each list in order.
/// a List List -&amp;gt; a List
inl concat l &amp;amp; !elem_type !elem_type t = foldr append l (empty t)

{List empty cons init map foldl foldr singleton head' tail' last' head tail last append concat} |&amp;gt; stack
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;With this, the new List module is done.&lt;/p&gt;
&lt;h4&gt;Warning on combining union types, partial active patterns and join points&lt;/h4&gt;
&lt;p&gt;Union types in Spiral are an example of a well designed feature with some implementation issues. Union types work, partial active patterns work and join points work, but right now they are a pick two out of three kind of deal. The reason for this is related to how Spiral will aggressively rewrite even variables outside of its intended scope.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl ab = box (.A \/ .B)
inl x = dyn (ab .A, ab .A, ab .A)
match x with
| .A, .A, _ -&amp;gt; 1
| .A, .B, .B -&amp;gt; 2
| _, _, .A -&amp;gt; 3
| _ -&amp;gt; 4   
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Union0 =
    | Union0Case0
    | Union0Case1
let (var_0: Union0) = Union0Case0
let (var_1: Union0) = Union0Case0
let (var_2: Union0) = Union0Case0
match var_0 with
| Union0Case0 -&amp;gt;
    match var_1 with
    | Union0Case0 -&amp;gt;
        1L
    | Union0Case1 -&amp;gt;
        match var_2 with
        | Union0Case0 -&amp;gt;
            3L
        | Union0Case1 -&amp;gt;
            2L
| Union0Case1 -&amp;gt;
    match var_2 with
    | Union0Case0 -&amp;gt;
        3L
    | Union0Case1 -&amp;gt;
        4L
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above compiles nicely, but suppose a partial active pattern with a join point is inserted in the middle.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl ab = box (.A \/ .B)
inl x = dyn (ab .A, ab .A, ab .A)
inl pat arg on_fail on_succ = join on_fail ()
match x with
| .A, .A, _ -&amp;gt; 1
| @pat _ -&amp;gt; -1
| .A, .B, .B -&amp;gt; 2
| _, _, .A -&amp;gt; 3
| _ -&amp;gt; 4    
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Union0 =
    | Union0Case0
    | Union0Case1
let rec method_0((var_0: Union0), (var_1: Union0), (var_2: Union0)): int64 =
    match var_0 with
    | Union0Case0 -&amp;gt;
        match var_1 with
        | Union0Case0 -&amp;gt;
            match var_2 with
            | Union0Case0 -&amp;gt;
                3L
            | Union0Case1 -&amp;gt;
                4L
        | Union0Case1 -&amp;gt;
            match var_2 with
            | Union0Case0 -&amp;gt;
                3L
            | Union0Case1 -&amp;gt;
                2L
    | Union0Case1 -&amp;gt;
        match var_2 with
        | Union0Case0 -&amp;gt;
            3L
        | Union0Case1 -&amp;gt;
            4L
let (var_0: Union0) = Union0Case0
let (var_1: Union0) = Union0Case0
let (var_2: Union0) = Union0Case0
match var_0 with
| Union0Case0 -&amp;gt;
    match var_1 with
    | Union0Case0 -&amp;gt;
        1L
    | Union0Case1 -&amp;gt;
        method_0((var_0: Union0), (var_1: Union0), (var_2: Union0))
| Union0Case1 -&amp;gt;
    method_0((var_0: Union0), (var_1: Union0), (var_2: Union0))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;What is going on here is that the evaluator is forgetting that it already tested the variables and starts unboxing them again inside the join point. This is because join points throw away local left to right rewrite information.&lt;/p&gt;
&lt;p&gt;A workaround would be to put join points in the clause bodies.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl ab = box (.A \/ .B)
inl x = dyn (ab .A, ab .A, ab .A)
inl pat arg on_fail on_succ = on_fail ()
match x with
| .A, .A, _ -&amp;gt; join 1
| @pat _ -&amp;gt; join -1
| .A, .B, .B -&amp;gt; join 2
| _, _, .A -&amp;gt; join 3
| _ -&amp;gt; join 4
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Union0 =
    | Union0Case0
    | Union0Case1
let rec method_0(): int64 =
    1L
and method_1(): int64 =
    3L
and method_2(): int64 =
    2L
and method_3(): int64 =
    4L
let (var_0: Union0) = Union0Case0
let (var_1: Union0) = Union0Case0
let (var_2: Union0) = Union0Case0
match var_0 with
| Union0Case0 -&amp;gt;
    match var_1 with
    | Union0Case0 -&amp;gt;
        method_0()
    | Union0Case1 -&amp;gt;
        match var_2 with
        | Union0Case0 -&amp;gt;
            method_1()
        | Union0Case1 -&amp;gt;
            method_2()
| Union0Case1 -&amp;gt;
    match var_2 with
    | Union0Case0 -&amp;gt;
        method_1()
    | Union0Case1 -&amp;gt;
        method_3()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Now the result is what one might want.&lt;/p&gt;
&lt;p&gt;In the future this might not be an issue and in fact, it might just get fixed as a natural process of making the compiler run faster. The way patterns work now is inefficient from a compilation speed perspective and there is room for improvement there. In fact, since pattern matching is so ubiquitous in Spiral, that would be the first thing one would want to optimize in order to speed up compilation.&lt;/p&gt;
&lt;p&gt;The way they work now though has the great combination of them being elegant, highly effective and simple to implement.&lt;/p&gt;
&lt;p&gt;It is difficult to imagine what could be added to Spiral for it to generate better code for runtime at this point. Spiral's one pass is THE optimization pass to optimize these patterns at runtime, but it does not have any capacity to optimize its own compilation.&lt;/p&gt;
&lt;p&gt;Right now Spiral's worst problem is its poor library support. The libraries are always in text and have to be parsed and prepassed from the scratch on every compilation. The way most languages solve that is by inventing an intermediate bytecode format, but without a doubt there exists a language design that would allow both the language to be fused to libraries, and to optimize both itself and the programs it is applied to. Without a doubt, such a language would significantly exceed Spiral in quality.&lt;/p&gt;
&lt;p&gt;One thing is for certain, such a language would be hard to write as a standard compiler in F#. A lot more infrastructure support would be necessary in order to support a fundamentally different approach towards compiler construction. Its own platform and a surgical compiler like &lt;a href=&quot;https://github.com/TiarkRompf/lancet&quot;&gt;Lancet&lt;/a&gt; would be a prerequisite. MLs are more suited towards writing interpreters than language towers.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://racket-lang.org/&quot; rel=&quot;nofollow&quot;&gt;Racket&lt;/a&gt; has a superior ecosystem for writing such a language compared to the .NET, but it is the author's opinion that the parser in particular is not the best place to do compile time evaluation of functions and that syntax should not be used for abstraction - it should be used for ergonomics and should be consistent. Parsing should be a step to get rid of syntax for the rest of the passes.&lt;/p&gt;
&lt;p&gt;Macros do not make sense in dynamic languages as a tool for language creation. They are absolute insanity in static languages. Often when static languages reach the limit of their design they cram macros to do everything else - like performance optimizations for example, and tout them as a feature rather than an admission of failure in language design.&lt;/p&gt;
&lt;p&gt;Wanting macros in order to optimize performance will never happen in Spiral.&lt;/p&gt;
&lt;h3&gt;4: Continuation Passing Style, Monadic Computation and Parsing&lt;/h3&gt;
&lt;p&gt;Now that union types are out of the way, slowly the subject can move towards the more fun stuff that can be done with the language. CPS is a great way of writing highly abstract, generic and very fast code in Spiral and so the language has support for programming in such a style using monadic syntax. Modules are a significant aid as well for programming in CPS.&lt;/p&gt;
&lt;p&gt;This chapter will be short and won't go into depth of how monads work. Neither will it explain how parsers work. As both of those subjects are highly complex, it would take a lot of time to cover them. For parsers combinators in particular, the place place to learn how they work would be to start with the &lt;a href=&quot;http://www.quanttec.com/fparsec/&quot; rel=&quot;nofollow&quot;&gt;FParsec documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For monads in particular, it is best to study their specific instances. There is a large amount of tutorials online regarding them, most often in the context of the Haskell language. More closer to home, the author's understanding of them went through a dramatic improvement once he stopped trying to figure out what the higher kinded types are doing and simply focused on the functions themselves in terms of flow.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl on_succ a = (a,())
inl on_log x = ((),Tuple.singleton x)
inl (&amp;gt;&amp;gt;=) (a,w) f = // The writer monad.
    inl a',w' = f a
    (a',Tuple.append w w')

inl add x y = x + y |&amp;gt; on_succ

inm x = add 1 1
inm _ = on_log x
inm y = add 3 4
inm _ = on_log y
inm z = add 5 6
inm _ = on_log z
on_succ (x+y+z) // Tuple2(20L, Tuple1(2L, 7L, 11L))
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: int64
    val mem_2: int64
    new(arg_mem_0, arg_mem_1, arg_mem_2) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1; mem_2 = arg_mem_2}
    end
and Tuple1 =
    struct
    val mem_0: int64
    val mem_1: Tuple0
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
Tuple1(20L, Tuple0(2L, 7L, 11L))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;What the &lt;code&gt;inm&lt;/code&gt; keyword does is merely rewrite &lt;code&gt;inm x = f&lt;/code&gt; to &lt;code&gt;f &amp;gt;&amp;gt;= inl x -&amp;gt; ...&lt;/code&gt;. Meaning the above example could have been done manually as...&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;add 1 1 &amp;gt;&amp;gt;= inl x -&amp;gt;
on_log x &amp;gt;&amp;gt;= inl _ -&amp;gt;
add 3 4 &amp;gt;&amp;gt;= inl y -&amp;gt;
on_log y &amp;gt;&amp;gt;= inl _ -&amp;gt;
add 5 6 &amp;gt;&amp;gt;= inl z -&amp;gt;
on_log z &amp;gt;&amp;gt;= inl _ -&amp;gt;
on_succ (x+y+z)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The writer monad pattern is notable as it is used to accumulate the backwards trace in the ML library, so it is worth keeping in mind. Despite the power of monads, most of the time they are used to encapsulate state.&lt;/p&gt;
&lt;p&gt;Apart from &lt;code&gt;inm&lt;/code&gt; there is also &lt;code&gt;inb&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl f x ret =
    Console.writeline x
    ret()
    Console.writeline &quot;done&quot;
inb x = f &quot;hello&quot;
Console.writeline &quot;doing work&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;System.Console.WriteLine(&quot;hello&quot;)
System.Console.WriteLine(&quot;doing work&quot;)
System.Console.WriteLine(&quot;done&quot;)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;A pattern similar to the above is used to emulate stack allocation of Cuda memory in the ML library.&lt;/p&gt;
&lt;p&gt;Anyway, here is a simple Spiral parser library written in CPS style just for the sake of making a benchmark. A more advanced version in monadic style can be found in the standard library.&lt;/p&gt;
&lt;h4&gt;Parsing Benchmark&lt;/h4&gt;
&lt;pre&gt;
&lt;code&gt;let example = 
    &quot;example&quot;,[option;tuple;loops;extern_;console],&quot;Module description.&quot;,
    &quot;&quot;&quot;
/// Converts a string to a parser stream.
/// string -&amp;gt; parser_stream
inl string_stream str {idx on_succ on_fail} =
    inl f idx = idx &amp;gt;= 0 &amp;amp;&amp;amp; idx &amp;lt; string_length str
    inl branch cond = if cond then on_succ (str idx) else on_fail &quot;string index out of bounds&quot; 
    match idx with
    | a, b -&amp;gt; branch (f a &amp;amp;&amp;amp; f b)
    | _ -&amp;gt; branch (f idx)

/// Runs a parser given the string and the expected return type.
/// t type -&amp;gt; string -&amp;gt; t parser -&amp;gt; t
inl run ret_type str parser = 
    inl stream = dyn str |&amp;gt; string_stream

    inl d = {
        stream
        on_succ = inl x state -&amp;gt; id x
        on_fail = inl x state -&amp;gt; failwith ret_type x
        on_type = ret_type
        }

    parser d {pos=dyn 0}

/// Reads a char.
/// char parser
inl stream_char {stream on_succ on_fail} {state with pos} =
    stream {
        idx = pos
        on_succ = inl c -&amp;gt; on_succ c {state with pos=pos+1}
        on_fail = inl msg -&amp;gt; on_fail msg state
        }

inl is_digit x = x &amp;gt;= '0' &amp;amp;&amp;amp; x &amp;lt;= '9'
inl is_whitespace x = x = ' '
inl is_newline x = x = '\n' || x = '\r'

/// Reads a digit.
/// char parser
inl digit {stream on_succ on_fail} state =
    stream_char {
        stream on_fail
        on_succ = inl x state' -&amp;gt; 
            if is_digit x then on_succ x state'
            else on_fail &quot;digit&quot; state
        } state

inl convert_type = fs [text: &quot;System.Convert&quot;]
inl to_uint64 x = Extern.FS.StaticMethod convert_type .ToUInt64 x uint64
/// Reads an 64-bit integer parser.
/// uint64 parser
inl puint64 {stream on_succ on_fail on_type} state =
    inl error state = on_fail &quot;puint64&quot; state
    inl rec loop i on_fail state =
        digit {
            stream
            on_fail=inl _ state -&amp;gt; on_fail i state
            on_succ=inl c state -&amp;gt;
                inl max = 1844674407370955161u64 // max of uint64 / 10u64
                if i &amp;lt;= max then
                    inl i' = i * 10u64 + to_uint64 c - to_uint64 '0'
                    if i &amp;lt; i' then join loop i' on_succ state
                    else error state
                else error state
            } state
        : on_type
    loop (dyn 0u64) (inl _ state -&amp;gt; error state) state

/// The skips an all the whitespaces (including newlines) before succeeding.
/// unit parser
met rec spaces {d with stream on_succ on_fail on_type} state =
    stream_char {
        stream
        on_fail = inl _ state -&amp;gt; on_succ () state
        on_succ = inl c state' -&amp;gt; 
            if is_whitespace c || is_newline c then spaces d state'
            else on_succ () state
        } state
    : on_type

/// Runs the first and then the second parser before returning the result of the second parser.
/// a parser -&amp;gt; b parser -&amp;gt; b parser
inl (&amp;gt;&amp;gt;.) a b {d with on_succ} state = a {d with on_succ = inl _ state -&amp;gt; b d state} state
/// Runs the first and then the second parser before returning the result of the first parser.
/// a parser -&amp;gt; b parser -&amp;gt; a parser
inl (.&amp;gt;&amp;gt;) a b {d with on_succ} state = 
    a {d with on_succ = inl a state -&amp;gt; 
        b {d with on_succ = inl _ state -&amp;gt; on_succ a state} state
        } state

/// Runs the tuple of parsers before returning their result.
/// tuple parser
inl rec tuple l {d with on_succ} state =
    match l with
    | x :: xs -&amp;gt;
        x {d with on_succ = inl x state -&amp;gt;
            tuple xs {d with on_succ = inl xs state -&amp;gt;
                on_succ (x :: xs) state
                } state
            } state
    | () -&amp;gt; on_succ () state

/// Parses an unsigned 64-bit integer and returns it after parsing whitespaces.
/// uint64 parser
inl num = puint64 .&amp;gt;&amp;gt; spaces

run (uint64,uint64,uint64) &quot;123 456 789&quot; (tuple (num, num, num))
    &quot;&quot;&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As the output is 376 lines long, it won't be pasted. Here is a straightforward translation of the above to F#.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let example2 = 
    /// Converts a string to a parser stream.
    /// string -&amp;gt; parser_stream
    /// Note: The functionality of this functions needs to be pared down in F# due to lack of intensional polymorphism.
    let string_stream (str: string) (idx, on_succ, on_fail) =
        if idx &amp;gt;= 0 &amp;amp;&amp;amp; idx &amp;lt; str.Length then on_succ str.[idx] else on_fail &quot;string index out of bounds&quot; 

    /// Runs a parser given the string and the expected return type.
    /// string -&amp;gt; t parser -&amp;gt; t
    let run str parser = 
        let stream = string_stream str

        let d = 
            stream
            ,fun x state -&amp;gt; id x
            ,fun x state -&amp;gt; failwith x

        parser d 0

    /// Reads a char.
    /// char parser
    let stream_char (stream, on_succ, on_fail) pos =
        stream 
            (pos
            ,fun c -&amp;gt; on_succ c (pos+1)
            ,fun msg -&amp;gt; on_fail msg pos
            )

    let is_digit x = x &amp;gt;= '0' &amp;amp;&amp;amp; x &amp;lt;= '9'
    let is_whitespace x = x = ' '
    let is_newline x = x = '\n' || x = '\r'

    /// Reads a digit.
    /// char parser
    let digit (stream, on_succ, on_fail) state =
        stream_char ( 
            stream 
            ,fun x state' -&amp;gt; 
                if is_digit x then on_succ x state'
                else on_fail &quot;digit&quot; state
            ,on_fail
            ) state
            

    /// Reads an 64-bit integer parser.
    /// uint64 parser
    let puint64 (stream, on_succ, on_fail) state =
        let error state = on_fail &quot;puint64&quot; state
        let rec loop i on_fail state =
            digit (
                stream
                ,fun c state -&amp;gt;
                    let max = 1844674407370955161UL // max of uint64 / 10u64
                    if i &amp;lt;= max then
                        let i' = i * 10UL + System.Convert.ToUInt64 c - System.Convert.ToUInt64 '0'
                        if i &amp;lt; i' then loop i' on_succ state
                        else error state
                    else error state
                ,fun _ state -&amp;gt; on_fail i state
                ) state
        loop 0UL (fun _ state -&amp;gt; error state) state

    /// The skips an all the whitespaces (including newlines) before succeeding.
    /// unit parser
    let rec spaces (stream, on_succ, on_fail as d) state =
        stream_char (
            stream
            ,fun c state' -&amp;gt; 
                if is_whitespace c || is_newline c then spaces d state'
                else on_succ () state
            ,fun _ state -&amp;gt; on_succ () state
            ) state

    /// Runs the first and then the second parser before returning the result of the second parser.
    /// a parser -&amp;gt; b parser -&amp;gt; b parser
    let (&amp;gt;&amp;gt;.) a b (stream,on_succ,on_fail as d) state = a (stream,(fun _ state -&amp;gt; b d state), on_fail) state
    /// Runs the first and then the second parser before returning the result of the first parser.
    /// a parser -&amp;gt; b parser -&amp;gt; a parser
    let (.&amp;gt;&amp;gt;) a b (stream,on_succ,on_fail) state = 
        a (stream, (fun a state -&amp;gt; b (stream, (fun _ state -&amp;gt; on_succ a state),on_fail) state), on_fail) state

    /// Runs the tuple of parsers before returning their result.
    /// tuple parser
    /// Note: This one is ugly. It is impossible to make a generic tuple without great type hackery in F#. 
    /// Check out FParsec.Pipes library to see how that might be done.
    let tuple3 (a,b,c) (stream,on_succ,on_fail) =
        a (
            stream 
            ,fun a -&amp;gt;
                b (
                    stream
                    ,fun b -&amp;gt;
                        c (
                            stream
                            ,fun c -&amp;gt;
                                on_succ (a,b,c)
                            ,on_fail)
                    ,on_fail)
            ,on_fail)

    /// Parses an unsigned 64-bit integer and returns it after parsing whitespaces.
    /// uint64 parser
    let num = puint64 .&amp;gt;&amp;gt; spaces

    run &quot;123 456 789&quot; (tuple3 (num, num, num))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here are the performance figures from testing the above two programs using the &lt;a href=&quot;https://github.com/dotnet/BenchmarkDotNet&quot;&gt;BenchmarkDotNet&lt;/a&gt; library . What the above program is doing is merely parsing 3 unsigned 64-bit ints and returning them in a tuple.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;           Method |       Mean |     Error |    StdDev |
----------------- |-----------:|----------:|----------:|
 FullySpecialized |   292.8 ns | 0.8448 ns | 0.7902 ns |
           FSharp | 3,616.2 ns | 8.9547 ns | 8.3762 ns |
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The Spiral parser is about 12x times faster. That is quite a nice improvement and roughly what one could expect. The interesting thing not noted in the benchmark is that once the Spiral's output has been compiled to F# code, it runs instantaneously while in F#'s case, there is a noticeable delay while the .NET JIT tries to optimize it. Obviously, in Spiral's case the JIT does not have much work left for it. Apart from register allocation, Spiral already does everything else and does a better job if it finishes compiling.&lt;/p&gt;
&lt;p&gt;If anything, the above understates the advantage that Spiral has over F#. Given how poor F# is at optimizing &lt;a href=&quot;https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/computation-expressions&quot; rel=&quot;nofollow&quot;&gt;monads&lt;/a&gt;, for parsers written in a monadic style the author would not be surprised to see the gap widen by another 10x.&lt;/p&gt;
&lt;p&gt;Nevertheless, one point in F#'s favor are compile times. The parser being tested here is somewhat trivial, but for a more complex parser such as the one for the Spiral compiler, the author would not be surprised to see it go up into 100s of thousands of lines of code. Given that the IDE gets crushed by the weight of a 20k LOC parser, it has him wondering how would he even compile such a monster? As a rule of the thumb, Spiral's evaluator generates around 3k LOC per second currently.&lt;/p&gt;
&lt;p&gt;For that sake, rather than CPSing everything it would be important to do boxing. In the previous section on lists, it was shown how in Spiral, the CPS form of &lt;code&gt;head&lt;/code&gt;,&lt;code&gt;tail&lt;/code&gt; and &lt;code&gt;last&lt;/code&gt; is just a more generic form of the one that uses the option type. Note that this is not the case in other functional languages as they lack Spiral's inlining guarantees.&lt;/p&gt;
&lt;p&gt;There are two choices for doing boxing in Spiral and they are good to know in order to cut down on exceesive specialization and making sure the evaluator does not diverge.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Do term casting.&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code&gt;/// Term casting for parsers
/// a type -&amp;gt; a parser -&amp;gt; a parser
inl term_cast typ p {d with on_succ on_fail on_type} state =
    inl term_cast_uncurried g a b = // This is to make sure only one closure is allocated.
        inl g = term_cast (inl a, b -&amp;gt; g a b) (a,b)
        inl a b -&amp;gt; g (a,b)
    p {d with 
        on_succ = term_cast_uncurried on_succ typ state
        on_fail = term_cast_uncurried on_fail string state
        } state

inl puint64 d state = join puint64 d state // Make sure that the unrolled outer loop is rolled in.

/// Parses an unsigned 64-bit integer and returns it after parsing whitespaces.
/// uint64 parser
inl num = term_cast uint64 (puint64 .&amp;gt;&amp;gt; spaces)

run (uint64,uint64,uint64) &quot;123 456 789&quot; (tuple (num, num, num))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The amount of code generated drops down to 141 LOC with this. Unfortunately it does make the program slower by about 40%.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;           Method |       Mean |     Error |    StdDev |
----------------- |-----------:|----------:|----------:|
       TermCasted |   406.6 ns | 1.2316 ns | 1.1520 ns |
 FullySpecialized |   292.8 ns | 0.8448 ns | 0.7902 ns |
           FSharp | 3,616.2 ns | 8.9547 ns | 8.3762 ns |
&lt;/code&gt;
&lt;/pre&gt;
&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;Box using union types.&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code&gt;inl ParserResult x state = .ParserSucc, x, state \/ .ParserFail, string, state

/// Union boxing for parsers
/// a type -&amp;gt; a parser -&amp;gt; a parser
inl box typ p {d with on_succ on_fail} state = 
    inl on_type = ParserResult typ state
    inl box = box on_type
    p {d with
        on_succ = inl x state -&amp;gt; box (.ParserSucc, x, state)
        on_fail = inl x state -&amp;gt; box (.ParserFail, x, state)
        on_type
        } state
    |&amp;gt; function
        | .ParserSucc, x, state -&amp;gt; on_succ x state
        | .ParserFail, x, state -&amp;gt; on_fail x state

inl puint64 d state = join puint64 d state // Make sure that the unrolled outer loop is rolled in.

/// Parses an unsigned 64-bit integer and returns it after parsing whitespaces.
/// uint64 parser
inl num = box uint64 (puint64 .&amp;gt;&amp;gt; spaces)

run (uint64,uint64,uint64) &quot;123 456 789&quot; (tuple (num, num, num))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This actually improves the running time significantly.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;           Method |       Mean |     Error |    StdDev |
----------------- |-----------:|----------:|----------:|
       TermCasted |   406.6 ns | 1.2316 ns | 1.1520 ns |
             Boxy |   199.4 ns | 0.9976 ns | 0.9332 ns |
 FullySpecialized |   292.8 ns | 0.8448 ns | 0.7902 ns |
           FSharp | 3,616.2 ns | 8.9547 ns | 8.3762 ns |
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The boxy parser is by far the best variant. It comes out to only 170 LOCs and is 45% faster than the fully specialized version and 18x times than the F# version. This also demonstrates that code duplication does have a noticeable performance impact. The lines of code reduced would be much more dramatic for a larger parser thereby making the application of this technique a necessity in a serious library. Currently the &lt;code&gt;Parsing&lt;/code&gt; module in the standard library is lacking in that regard and the above benchmark is actually the first time the author used union type boxing on parsers or did benchmarking of a Spiral program. The &lt;code&gt;Parsing&lt;/code&gt; module was not intented to be a serious parsing library, but to drive the development of the language in a challening area.&lt;/p&gt;
&lt;p&gt;When the first version of it was created Spiral did not have modules nor monadic syntax nor good error messages, but it did have a lot of compiler bugs as if to make up for it. Spiral's modules were created in part because refactoring the parser was simply so painful during those days - it would take the author hours to fix the type errors that in F# would have taken him 10m. It is much better now thankfully.&lt;/p&gt;
&lt;p&gt;As the author has no intention of doing so and wants to do machine learning instead, for those interested in parsing Spiral is a very good language to experiment with in the context of staged functional programming.&lt;/p&gt;
&lt;p&gt;It will no doubt be a very productive language for such a purpose depending on how much weight is put on performance. If full weight is put on it then there is no doubt that Spiral would be orders of magnitute more productive at such a task than other languages.&lt;/p&gt;
&lt;p&gt;The reason for that is that it is not enough to judge merely by how long would it take to write a parser, but how long would it take to get it on par with Spiral in performance. It took the author ~5h to make the parser for this benchmark in Spiral and then maybe 20m to transcribe it to F# by hand. In order to get to the Boxy level of performance, how long would it have taken to specialize all of that by hand and test it? Days?&lt;/p&gt;
&lt;p&gt;Just how hard would such a fast handwritten C-style parser be to modify after that? It would be completely inflexible, so a decent guess is quite hard. It would also take a special kind of masochism to deliberately write code in such a style.&lt;/p&gt;
&lt;p&gt;ML styled languages still have some advantages over Spiral due to having type inference which is a great aid to refactoring and explorability, but C offshots can be completely replaced with no regret.&lt;/p&gt;
&lt;p&gt;The 4 parsers benchmarked in this section can be found in &lt;a href=&quot;https://github.com/mrakgr/The-Spiral-Language/tree/master/Benchmarking&quot;&gt;this folder&lt;/a&gt; of the repo.&lt;/p&gt;
&lt;h3&gt;5: Tensors and Structural Reflection&lt;/h3&gt;
&lt;p&gt;The development of Spiral was driven by the need for a language with great capability for abstraction whose semantics would allow for it to be compiled to very fast code suitable for GPUs and the architectures coming down the line. During the early days of its development when it was intended as a Cuda backend for the ML library Spiral actually had built in arrays that would track variables at on the type level, but that tensors could be designed like the way they currently could be was beyond the imagination of its author and makes him glad that he decided to complete the language instead of leaving Spiral in a half finished state as a crappy ML library backend.&lt;/p&gt;
&lt;p&gt;Tensors in Spiral represent the crystallization of its power; they are the point at which all of its features flow together to create something that cannot be done in any other language.&lt;/p&gt;
&lt;p&gt;Unlike parsers of the previous chapter which were complicated - so complicated that they could not be explained, tensors are actually rather simple and intended for all ages.&lt;/p&gt;
&lt;p&gt;The implementation of &lt;code&gt;HostTensor&lt;/code&gt; in the standard library is somewhat convoluted due to needing to be generic in order to be reusable on the Cuda side in addition to having a lot of functionality, so this chapter will follow the format of giving a few examples of its most salient features and then show how a simpler tensor can be derived from first principles in order to attain understanding of it.&lt;/p&gt;
&lt;p&gt;Tensors in Spiral can have an arbitrary number of dimensions, arbitrary types and arbitrary layouts in addition to supporting views. Indexing into them emulates the partial application of functions. &lt;code&gt;init&lt;/code&gt; for them takes arguments in curried form which supports scope control.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let example = 
    &quot;example&quot;,[option;tuple;loops;extern_;console;host_tensor],&quot;Module description.&quot;,
    &quot;&quot;&quot;
inl ar = array_create string 3
Tuple.foldl (inl i x -&amp;gt; ar i &amp;lt;- x; i+1) 0 (&quot;zero&quot;,&quot;one&quot;,&quot;two&quot;) |&amp;gt; ignore
HostTensor.init (3,5,{from=2; to=5}) (inl a -&amp;gt;
    inl x = ar a
    string_format &quot;x is {0}&quot; x |&amp;gt; Console.writeline
    inl b c -&amp;gt; x, b, c
    )
|&amp;gt; HostTensor.show.all
|&amp;gt; Console.writeline
    &quot;&quot;&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The generated code won't be posted as the loops for printing the tensors and initializing it are long, but here is what comes out when the program has been ran.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;x is zero
x is one
x is two
[|
    [|
        [|[zero, 0, 2]; [zero, 0, 3]; [zero, 0, 4]; [zero, 0, 5]|]
        [|[zero, 1, 2]; [zero, 1, 3]; [zero, 1, 4]; [zero, 1, 5]|]
        [|[zero, 2, 2]; [zero, 2, 3]; [zero, 2, 4]; [zero, 2, 5]|]
        [|[zero, 3, 2]; [zero, 3, 3]; [zero, 3, 4]; [zero, 3, 5]|]
        [|[zero, 4, 2]; [zero, 4, 3]; [zero, 4, 4]; [zero, 4, 5]|]
    |]
    [|
        [|[one, 0, 2]; [one, 0, 3]; [one, 0, 4]; [one, 0, 5]|]
        [|[one, 1, 2]; [one, 1, 3]; [one, 1, 4]; [one, 1, 5]|]
        [|[one, 2, 2]; [one, 2, 3]; [one, 2, 4]; [one, 2, 5]|]
        [|[one, 3, 2]; [one, 3, 3]; [one, 3, 4]; [one, 3, 5]|]
        [|[one, 4, 2]; [one, 4, 3]; [one, 4, 4]; [one, 4, 5]|]
    |]
    [|
        [|[two, 0, 2]; [two, 0, 3]; [two, 0, 4]; [two, 0, 5]|]
        [|[two, 1, 2]; [two, 1, 3]; [two, 1, 4]; [two, 1, 5]|]
        [|[two, 2, 2]; [two, 2, 3]; [two, 2, 4]; [two, 2, 5]|]
        [|[two, 3, 2]; [two, 3, 3]; [two, 3, 4]; [two, 3, 5]|]
        [|[two, 4, 2]; [two, 4, 3]; [two, 4, 4]; [two, 4, 5]|]
    |]
|]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Having the arguments to &lt;code&gt;init&lt;/code&gt; be partially applied rather than given all at once is what allow the outside array to be indexed only in the outer loop. Had it been otherwise, the indexing would have needed to be done inside the innermost loop. &lt;code&gt;init&lt;/code&gt; gives loops for free. Important operations such as map, rotation and reduction can be implemented in terms of init on the host (CPU) side.&lt;/p&gt;
&lt;p&gt;Tensors themselves emulate partial application of functions. Here is how they can be indexed into.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl ar = array_create string 3
Tuple.foldl (inl i x -&amp;gt; ar i &amp;lt;- x; i+1) 0 (&quot;zero&quot;,&quot;one&quot;,&quot;two&quot;) |&amp;gt; ignore
inl tns = HostTensor.init (3,5,{from=2; to=5}) (inl a -&amp;gt;
    inl x = ar a
    inl b c -&amp;gt; x, b, c
    )
inl f x = x 0 2 .get
tns 0 |&amp;gt; f |&amp;gt; Console.writeline
tns 1 |&amp;gt; f |&amp;gt; Console.writeline
tns 2 |&amp;gt; f |&amp;gt; Console.writeline
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;[zero, 0, 2]
[one, 0, 2]
[two, 0, 2]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is convenient for views. Views can take more than a single argument in which case they need to be passed as a tuple. Like application, views work on dimensions from left to right - from the outermost to the innermost.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl ar = array_create string 3
Tuple.foldl (inl i x -&amp;gt; ar i &amp;lt;- x; i+1) 0 (&quot;zero&quot;,&quot;one&quot;,&quot;two&quot;) |&amp;gt; ignore
inl tns = HostTensor.init (3,5,{from=2; to=5}) (inl a -&amp;gt;
    inl x = ar a
    inl b c -&amp;gt; x, b, c
    )

tns.view (1,{from=2;near_to=4})
|&amp;gt; HostTensor.show.all
|&amp;gt; Console.writeline
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;[|
    [|
        [|[zero, 2, 2]; [zero, 2, 3]; [zero, 2, 4]; [zero, 2, 5]|]
        [|[zero, 3, 2]; [zero, 3, 3]; [zero, 3, 4]; [zero, 3, 5]|]
    |]
|]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Both the tensor applications and views are done done immutably. Apart from join points which are memoized, Spiral's metalanguage is pure and there is no way of mutably updating tuples or modules.&lt;/p&gt;
&lt;p&gt;For the sake of demonstration, here is how &lt;code&gt;.set&lt;/code&gt; works. It is very similar to &lt;code&gt;.get&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl tns = HostTensor.init ({from=2; near_to=5}) id
inl modify f t = t .set (t .get |&amp;gt; f)
tns 2 |&amp;gt; modify ((*) 2)
tns 3 |&amp;gt; modify ((+) 22)
tns 4 |&amp;gt; modify (const -11)

HostTensor.show.all tns
|&amp;gt; Console.writeline
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;[|4; 25; -11|]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In addition to &lt;code&gt;.view&lt;/code&gt;, &lt;code&gt;.view_span&lt;/code&gt; is useful.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl tns_a = HostTensor.init ({from=2; near_to=10}) id

HostTensor.show.all tns_a
|&amp;gt; string_format &quot;tns_a = {0}&quot;
|&amp;gt; Console.write

inl tns_b = tns_a.view_span {from=0;near_to=2}
HostTensor.show.all tns_b
|&amp;gt; string_format &quot;tns_b = {0}&quot;
|&amp;gt; Console.write

inl tns_c = tns_a.view_span {from=2;near_to=4}
HostTensor.show.all tns_c
|&amp;gt; string_format &quot;tns_c = {0}&quot;
|&amp;gt; Console.write

tns_c 0 .get |&amp;gt; Console.writeline
tns_c 1 .get |&amp;gt; Console.writeline
//tns_c 2 .get |&amp;gt; Console.writeline // Would trigger the range check assertion at compile time.
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;tns_a = [|2; 3; 4; 5; 6; 7; 8; 9|]
tns_b = [|2; 3|]
tns_c = [|4; 5|]
4
5
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;.view_span&lt;/code&gt; is similar to view except starts the indexing from the beginning of the dimensions of the target tensor and rebases the dimensions so they start from 0. That means that &lt;code&gt;.view_span -1&lt;/code&gt; would always be an out of bounds error as would all negative values of the index. Given how Spiral specializes join points, it is useful for avoiding code bloat as well.&lt;/p&gt;
&lt;p&gt;For the sake of machine learning, it is preferable to keep the tensor sizes as constants to get rid of as many assertion at compile time as possible. Note that in the last line, had it not been commented out the compiler would have raised a type error at compile time.&lt;/p&gt;
&lt;p&gt;Tensors have even more to offer. By default, their layout is that of tuple of arrays. Meaning a tuple of type &lt;code&gt;float32 * int64 * int64&lt;/code&gt; would be represented using 3 arrays internally each for the separate elements of the tuple. The main motivation behind this design is to make it easy to pass them through language boundaries onto the Cuda side as unless the arrays were of primitive types, it would be difficult to align them in memory, but there are performance benefits as well to allowing such a representation.&lt;/p&gt;
&lt;p&gt;That varies from problem to problem, so it would be even better if it was easy to switch between representations at will.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl tns_toa = HostTensor.init (5,5) (inl a b -&amp;gt; a,b)
inl tns_aot = HostTensor.init.aot (5,5) (inl a b -&amp;gt; tns_toa a b .get)
()
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: int64
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
...
let (var_0: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(25L))
let (var_1: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(25L))
let (var_2: int64) = 0L
method_0((var_0: (int64 [])), (var_1: (int64 [])), (var_2: int64))
let (var_5: (Tuple0 [])) = Array.zeroCreate&amp;lt;Tuple0&amp;gt; (System.Convert.ToInt32(25L))
let (var_6: int64) = 0L
method_2((var_0: (int64 [])), (var_1: (int64 [])), (var_5: (Tuple0 [])), (var_6: int64))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The full thing won't be posted as it is 100 LOC, but it should be imaginable that &lt;code&gt;method_0&lt;/code&gt; is &lt;code&gt;init&lt;/code&gt; for &lt;code&gt;tns_toa&lt;/code&gt; and &lt;code&gt;method_2&lt;/code&gt; is &lt;code&gt;init&lt;/code&gt; for &lt;code&gt;tns_aot&lt;/code&gt;. Spiral's tensors are the perfect abstraction where layouts are concerned and it is possible to mix and match &lt;code&gt;.aot&lt;/code&gt; and &lt;code&gt;.toa&lt;/code&gt; layout using the &lt;code&gt;zip&lt;/code&gt; function and it will still work fine. For most usecases, the default tuple of arrays is sensible.&lt;/p&gt;
&lt;p&gt;Saying tuple of arrays does not cover it completely though. The tensors work fine with modules.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl a = HostTensor.init (3,3) (inl a b -&amp;gt; {a b})
a 2 2 .set {a=99}
HostTensor.show a
|&amp;gt; Console.writeline
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;[|
    [|{a = 0; b = 0}; {a = 0; b = 1}; {a = 0; b = 2}|]
    [|{a = 1; b = 0}; {a = 1; b = 1}; {a = 1; b = 2}|]
    [|{a = 2; b = 0}; {a = 2; b = 1}; {a = 99; b = 2}|]
|]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;When tensor are in &lt;code&gt;toa&lt;/code&gt; form they have the added feature of allowing the module fields to be mutated individually. This is not possible in general in Spiral as modules and tuples are immutable; if they were represented as &lt;code&gt;aot&lt;/code&gt; or if the modules were tuples this would not be possible. What is going on is that when they are represented as separate entities this changes their semantics to reflect that and this is the correct behavior even in a functional language.&lt;/p&gt;
&lt;p&gt;This is the short tour of the tensors in Spiral. The next section will be on how they are implemented.&lt;/p&gt;
&lt;h4&gt;Under the Hood&lt;/h4&gt;
&lt;p&gt;As was demonstrated, there are two aspects of tensor polymorphism - one was that they have an arbitrary number of dimensions and the other was that are layout polymorphic. In a language with weaker type systems that would require creating specific tensor instances for both of those concerns, but Spiral can handle them naturally.&lt;/p&gt;
&lt;p&gt;Even better, the dimensionality of the tensor is really a separate concern from its layout and so the two subjects can be taken in as separate pieces.&lt;/p&gt;
&lt;h5&gt;Layout Polymorphism&lt;/h5&gt;
&lt;p&gt;Creating an array of &lt;code&gt;int64 * int64 * int64&lt;/code&gt; can be done like this in Spiral.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;array_create (int64,int64,int64) 8
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: int64
    val mem_2: int64
    new(arg_mem_0, arg_mem_1, arg_mem_2) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1; mem_2 = arg_mem_2}
    end
Array.zeroCreate&amp;lt;Tuple0&amp;gt; (System.Convert.ToInt32(8L))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above is also known as &lt;code&gt;aot&lt;/code&gt; or &lt;code&gt;array of tuples&lt;/code&gt; form. The opposite of it, the &lt;code&gt;toa&lt;/code&gt; or &lt;code&gt;tuple of arrays&lt;/code&gt; form would be this.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl ar = array_create int64 8, array_create int64 8, array_create int64 8
()
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let (var_0: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
let (var_1: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
let (var_2: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Rather than copy pasting like, it would be better if the type were mapped directly.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl ar = Tuple.map (inl x -&amp;gt; array_create x 8) (int64,int64,int64)
()
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let (var_0: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
let (var_1: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
let (var_2: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The same output as in the above example shows up. By now, the general principle of how Spiral's tensors achieve their layout polymorphism should be becoming clearer. Of course, the above is woefully incomplete. For example, if the tuple were nested then the &lt;code&gt;toa&lt;/code&gt; layout would not be achieved.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl ar = Tuple.map (inl x -&amp;gt; array_create x 8) (int64,(int64,int64))
()
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: int64
    new(arg_mem_0, arg_mem_1) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1}
    end
let (var_0: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
let (var_1: (Tuple0 [])) = Array.zeroCreate&amp;lt;Tuple0&amp;gt; (System.Convert.ToInt32(8L))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;What should be done is to write a function capable mapping over nested tuples.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl rec toa_map f = function
    | x :: x' -&amp;gt; toa_map f x :: toa_map f x'
    | () -&amp;gt; ()
    | x -&amp;gt; f x

inl ar = toa_map (inl x -&amp;gt; array_create x 8) (int64,(int64,int64))
()
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let (var_0: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
let (var_1: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
let (var_2: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Note the subtle difference between &lt;code&gt;toa_map&lt;/code&gt; and a regular &lt;code&gt;map&lt;/code&gt;. In &lt;code&gt;| x :: x' -&amp;gt; toa_map f x :: toa_map f x'&lt;/code&gt; the function recurses on &lt;code&gt;x&lt;/code&gt; as well, not just on the tail.&lt;/p&gt;
&lt;p&gt;For modules, a little extra is needed in &lt;code&gt;toa_map&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl toa_map f =
    inl rec loop = function
        | x :: x' -&amp;gt; loop x :: loop x'
        | () -&amp;gt; ()
        | {} as x -&amp;gt; module_map (const loop) x
        | x -&amp;gt; f x
    loop

inl ar = toa_map (inl x -&amp;gt; array_create x 8) {x=int64; y=int64,int64}
()
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let (var_0: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
let (var_1: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
let (var_2: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Before the function is complete there are two more things needed. That would be to stop the function from unboxing every union type and to stop it from recursing on every module. This last one is more of a convenience than necessity.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl toa_map f =
    inl rec loop = function
        | x when caseable_box_is x -&amp;gt; f x // This needs to be in the first position to prevent the unboxing from triggering.
        | x :: x' -&amp;gt; loop x :: loop x'
        | () -&amp;gt; ()
        | {!toa_map_block} &amp;amp; x -&amp;gt; module_map (const loop) x
        | x -&amp;gt; f x
    loop

inl ar = toa_map (inl x -&amp;gt; array_create x 8) {x=int64; y=int64,int64; o=Option.Option float32}
()
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Union0 =
    | Union0Case0 of Tuple1
    | Union0Case1
and Tuple1 =
    struct
    val mem_0: float32
    new(arg_mem_0) = {mem_0 = arg_mem_0}
    end
let (var_0: (Union0 [])) = Array.zeroCreate&amp;lt;Union0&amp;gt; (System.Convert.ToInt32(8L))
let (var_1: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
let (var_2: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
let (var_3: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;With this as the basis, it is easy to make a &lt;code&gt;toa&lt;/code&gt; version of array create.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl toa_array_create typ size = toa_map (inl x -&amp;gt; array_create x size) typ
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Indexing into such an array is quite similar to creating it. It is just a straightforward application of &lt;code&gt;toa_map&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl toa_index ar idx = toa_map (inl ar -&amp;gt; ar idx) ar

inl ar = toa_array_create {x=int64; y=int64,int64; o=Option.Option float32} 8
inl el = toa_index ar 0
()
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Union0 =
    | Union0Case0 of Tuple1
    | Union0Case1
and Tuple1 =
    struct
    val mem_0: float32
    new(arg_mem_0) = {mem_0 = arg_mem_0}
    end
let (var_0: (Union0 [])) = Array.zeroCreate&amp;lt;Union0&amp;gt; (System.Convert.ToInt32(8L))
let (var_1: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
let (var_2: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
let (var_3: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
let (var_4: Union0) = var_0.[int32 0L]
let (var_5: int64) = var_1.[int32 0L]
let (var_6: int64) = var_2.[int32 0L]
let (var_7: int64) = var_3.[int32 0L]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Setting such an array is a bit more difficult. In order to do that, &lt;code&gt;toa_map2&lt;/code&gt; would be needed first.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl toa_map2 f a b =
    inl rec loop = function
        | x, y when caseable_box_is x || caseable_box_is y -&amp;gt; f x y
        | x :: x', y :: y' -&amp;gt; loop (x,y) :: loop (x',y')
        | (), () -&amp;gt; ()
        | (), _ | _, () -&amp;gt; error_type &quot;Tuple dimensions do not match.&quot;
        | {!toa_map_block} &amp;amp; x, {!toa_map_block} &amp;amp; y -&amp;gt; module_map (inl k y -&amp;gt; loop (x k, y)) y
        | x, y -&amp;gt; f x y
    loop (a,b)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;With this it is possible to implement &lt;code&gt;toa_set&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl toa_set ar idx v = toa_map2 (inl ar v -&amp;gt; ar idx &amp;lt;- v) ar v |&amp;gt; ignore

inl ar = toa_array_create {x=int64; y=int64,int64; o=Option.Option float32} 8
toa_set ar 0 {x=2; y=1,1; o=Option.some 2.2f32}
()
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Union0 =
    | Union0Case0 of Tuple1
    | Union0Case1
and Tuple1 =
    struct
    val mem_0: float32
    new(arg_mem_0) = {mem_0 = arg_mem_0}
    end
let (var_0: (Union0 [])) = Array.zeroCreate&amp;lt;Union0&amp;gt; (System.Convert.ToInt32(8L))
let (var_1: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
let (var_2: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
let (var_3: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(8L))
var_0.[int32 0L] &amp;lt;- (Union0Case0(Tuple1(2.200000f)))
var_1.[int32 0L] &amp;lt;- 2L
var_2.[int32 0L] &amp;lt;- 1L
var_3.[int32 0L] &amp;lt;- 1L
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;There is an interesting design decision of whether to allow partial setting of fields in modules in &lt;code&gt;toa_map2&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;| {!toa_map_block} &amp;amp; x, {!toa_map_block} &amp;amp; y -&amp;gt; module_map (inl k y -&amp;gt; loop (x k, y)) y
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above particular line could have also been written like so.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;| {!toa_map_block} &amp;amp; x, {!toa_map_block} &amp;amp; y -&amp;gt; module_map (inl k x -&amp;gt; loop (x, y k)) x
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Doing it like that would have disallowed the partial module sets as the map would have gone through every field of the &lt;code&gt;toa&lt;/code&gt; array instead of every field of the setter.&lt;/p&gt;
&lt;p&gt;In this section the way to make layout polymorphic arrays was described and is how Spiral's tensors attain such a capability. The ability to reflect on the structure of everything in the language at no runtime cost is a powerful one and solves a lot of the issues currently facing persons writing numerical libraries. It also covers an aspect of polymorphism that parametric languages do not touch.&lt;/p&gt;
&lt;p&gt;The next section will be on how to take what has been done so far and make the 1d &lt;code&gt;toa&lt;/code&gt; array, a tensor capable of support arbitrary dimensions.&lt;/p&gt;
&lt;h5&gt;Dimensionality Polymorphism&lt;/h5&gt;
&lt;p&gt;In one of the previous chapters, it was mentioned that Spiral is not necessarily less typesafe than F#, and that in some cases the opposite is in fact the case. Whenever dynamism and therefore union types are needed, a step is made into an entirely different paradigm for languages with weaker type systems due to have to push typechecking to runtime.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;List.take 5 [1;2;3] // An error at runtime.
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;Tuple.take 5 (1,2,3) // An error at compile time.
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This comparison might seem unfair since lists in F# are explicitly runtime constructs, but that is in fact the point. Spiral's parent language as it stands now does not have any substitute for Spiral's tuples at all and has absolutely no way of making tensors arbitrary in their dimensions at compile time. All it can do is support very specific instances of them.&lt;/p&gt;
&lt;p&gt;So back when the author was making the ML library in F#, he had &lt;code&gt;d2M&lt;/code&gt; for a 2D GPU tensor, and &lt;code&gt;d4M&lt;/code&gt; for 4d GPU tensors. Afterwards he found the need for a 3d tensor so he made &lt;code&gt;d3M&lt;/code&gt; too and so on. There was the &lt;code&gt;Df&lt;/code&gt;, a lazy scalar host tensor as well.&lt;/p&gt;
&lt;p&gt;Layout polymorphism? Forget that. The best what was possible was having them be generic in their type.&lt;/p&gt;
&lt;p&gt;Now, there is no doubt that making a very specific instance of a tensor (such as &lt;code&gt;d2M&lt;/code&gt;) is easier than making a fully blown generic tensor, but making such a tensor type is definitely easier than having to make a specific instance for all the endless varieties of tensors. Making specific instances of the more generic type by hand gets tiresome really quickly. It is humiliating to have to use personal effort because the tool one is using is not good enough.&lt;/p&gt;
&lt;h6&gt;Design of the Tensor&lt;/h6&gt;
&lt;pre&gt;
&lt;code&gt;{
    bodies = { ar size offset toa_map_block } structure
    dim
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above is very similar to the type of the Spiral's &lt;code&gt;HostTensor&lt;/code&gt; in pseudo-code. It is actually one of its previous designs that is easier to explain as it has a more uniform structure.&lt;/p&gt;
&lt;p&gt;In it, &lt;code&gt;dim&lt;/code&gt; is just the dimension of the tensor and might be &lt;code&gt;(2,3,4)&lt;/code&gt; for a 3d tensor, &lt;code&gt;(10,20)&lt;/code&gt; for a 2d tensor or &lt;code&gt;()&lt;/code&gt; for a scalar tensor for example.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;size&lt;/code&gt; and &lt;code&gt;offset&lt;/code&gt; inside the &lt;code&gt;bodies&lt;/code&gt; are directly related to it. Elements of &lt;code&gt;offset&lt;/code&gt; are always to be multiples of their related &lt;code&gt;size&lt;/code&gt; element.&lt;/p&gt;
&lt;p&gt;Before the coding can start, some simple examples need to be gone though by hand so that it becomes clear what is trying to be done in the first place.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;On tensor creation.&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code&gt;/// Creating a 1d tensor of type int64 and dim 10
inl dim = 10
inl ar = array_create int64 dim
inl tns = {
    bodies = {
        ar
        size = Tuple.singleton 1
        offset = Tuple.singleton 0
        toa_map_block = ()
        }
    dim = Tuple.singleton dim
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;/// Creating a 2d tensor of type int64 and dim 10, 10
inl dim = 10, 10
inl ar = array_create int64 (10 * 10)
inl tns = {
    bodies = {
        ar
        size = 10, 1
        offset = 0, 0
        toa_map_block = ()
        }
    dim
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;/// Creating a 3d tensor of type int64 and dim 10, 5, 5
inl dim = 10, 5, 5
inl ar = array_create int64 (10 * 5 * 5)
inl tns = {
    bodies = {
        ar
        size = 5*5, 5, 1
        offset = 0, 0, 0
        toa_map_block = ()
        }
    dim
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;By now some patterns should be coming out. &lt;code&gt;ar&lt;/code&gt; is always inserted directly into the tensor body, &lt;code&gt;size&lt;/code&gt; is just the rightwards &lt;a href=&quot;https://stackoverflow.com/questions/23491216/f-cumulative-product-of-an-array&quot; rel=&quot;nofollow&quot;&gt;scan product&lt;/a&gt; of &lt;code&gt;dim&lt;/code&gt;, &lt;code&gt;offset&lt;/code&gt; is always &lt;code&gt;dim&lt;/code&gt; mapped to 0, and &lt;code&gt;dim&lt;/code&gt; is always itself. The only difference is 1d when &lt;code&gt;size&lt;/code&gt;, &lt;code&gt;offset&lt;/code&gt; and &lt;code&gt;dim&lt;/code&gt; are wrapped in a tuple.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Creating a nd tensor in the array of tuples layout.
inl tensor_aot_create typ dim =
    inl dim = match dim with _ :: _ -&amp;gt; dim | x -&amp;gt; x :: ()
    inl array_size :: size = Tuple.scanr (*) dim 1
    inl offset = Tuple.map (const 0) dim
    {
    bodies = {
        ar = array_create typ array_size
        size offset
        toa_map_block=()
        }
    dim
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is the pattern abstracted and codified. The tuple of arrays version is similar to the above.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Creating a nd tensor in the tuple of arrays layout.
inl tensor_toa_create typ dim =
    inl dim = match dim with _ :: _ -&amp;gt; dim | x -&amp;gt; x :: ()
    inl array_size :: size = Tuple.scanr (*) dim 1
    inl offset = Tuple.map (const 0) dim
    inl make_body typ = {
        toa_map_block=()
        ar = array_create typ array_size
        size offset
        }
    {
    bodies = toa_map make_body typ            
    dim
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The two functions have a very similar internal structure. It can be factored out like so.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Creating a nd tensor
inl tensor_create {dsc with typ dim} =
    inl dim = match dim with _ :: _ -&amp;gt; dim | x -&amp;gt; x :: ()
    inl array_size :: size = Tuple.scanr (*) dim 1
    inl offset = Tuple.map (const 0) dim
    inl make_body typ = {
        toa_map_block=()
        ar = array_create typ array_size
        size offset
        }
    {
    bodies = 
        match dsc with
        | {layout=x} -&amp;gt; 
            match x with
            | .aot -&amp;gt; make_body typ
            | .toa -&amp;gt; toa_map make_body typ
        | _ -&amp;gt; toa_map make_body typ
    dim
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The beautiful thing about this is that since all the sizes are known, Spiral can track them at compile time.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl tns = tensor_create {typ=int64,string,float32; dim=10,5,5}
()
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let (var_0: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(250L))
let (var_1: (string [])) = Array.zeroCreate&amp;lt;string&amp;gt; (System.Convert.ToInt32(250L))
let (var_2: (float32 [])) = Array.zeroCreate&amp;lt;float32&amp;gt; (System.Convert.ToInt32(250L))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Unless the tensor is returned from a join point, or a dynamic if statement, or put into an array, those fields will never be instantiated due being tracked at the type level and the tensor will have the very minimal overhead at runtime.&lt;/p&gt;
&lt;p&gt;Spiral's &lt;code&gt;HostTensor&lt;/code&gt; actually tracks lower and upper bounds in &lt;code&gt;dim&lt;/code&gt; as well, but that won't be done for this tutorial.&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;On application.&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code&gt;{
bodies = {
    ar
    size = 25, 5, 1
    offset = 0, 0, 0
    toa_map_block = ()
    }
dim = 10, 5, 5
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;For the purpose of explanation, the 3d tensor from the previous example will be the starting point.&lt;/p&gt;
&lt;p&gt;How should applying 2 to the tensor transform it?&lt;/p&gt;
&lt;p&gt;It should be into this 2d tensor.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;{
bodies = {
    ar
    size = 5, 1
    offset = 25*2 + 0, 0 // 50
    toa_map_block = ()
    }
dim = 5, 5
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Applying 3 to the above should turn it into this.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;{
bodies = {
    ar
    size = 1 :: ()
    offset = 25*2 + 5*3 + 0 :: () // 65
    toa_map_block = ()
    }
dim = 5 :: ()
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Now that it is has been applied twice, the resulting tensor has gone from 3d to 1d. Once more and it will be scalar. Here is simulating the application of 4.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;{
bodies = {
    ar
    size = ()
    offset = 25*2 + 5*3 + 1*4 // 69
    toa_map_block = ()
    }
dim = ()
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Note how now the offset it a scalar and can be used to index into an array. The rules of tensors are quite simple, only 0d (scalar) tensors can be indexed and set and cannot be applied, while the opposite holds for tensors with higher number of dimensions.&lt;/p&gt;
&lt;p&gt;Also note that &lt;code&gt;dim&lt;/code&gt; plays no role in calculating the top of the new offset.&lt;/p&gt;
&lt;p&gt;What was omitted in the above example is the boundary checking. If the value being applied was out of range that would have triggered an error.&lt;/p&gt;
&lt;p&gt;Here is the above intuition in code.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl tensor_apply i {dim=d::dim bodies} =
    assert (i &amp;gt;= 0 &amp;amp;&amp;amp; i &amp;lt; d) &quot;Tensor application out of bounds&quot;

    {
    dim
    bodies = 
        toa_map (inl {d with size=s::size offset=o::offset} -&amp;gt;
            inl o = o + i * s
            inl offset = 
                match offset with
                | o' :: offset -&amp;gt; o + o' :: offset
                | () -&amp;gt; o
            {d with size offset}
            ) bodies
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;| o' :: offset -&amp;gt; o + o' :: offset&lt;/code&gt; might be surprising, but that is needed because the offsets might not be zero and that should not be thrown away. That can happen in tensors whose inner dimensions were viewed.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl tns = tensor_create {typ=int64,string,float32; dim=10,5,5}
tensor_apply 2 tns
|&amp;gt; tensor_apply 3
|&amp;gt; tensor_apply 4
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let (var_0: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(250L))
let (var_1: (string [])) = Array.zeroCreate&amp;lt;string&amp;gt; (System.Convert.ToInt32(250L))
let (var_2: (float32 [])) = Array.zeroCreate&amp;lt;float32&amp;gt; (System.Convert.ToInt32(250L))
(Env4(Tuple3((Env0(var_0, 69L)), (Env1(var_1, 69L)), (Env2(var_2, 69L)))))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The types were cut out in the above generated code. As can be seen since scalar tensors have only their offset and an array that is what gets printed.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl tns = 
    tensor_create {typ=int64,string,float32; dim=10,5,5}
    |&amp;gt; tensor_apply (dyn 2)
    |&amp;gt; tensor_apply 3
    |&amp;gt; tensor_apply 4
join tns
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let rec method_0((var_0: (int64 [])), (var_1: int64), (var_2: (string [])), (var_3: (float32 []))): Env0 =
    (Env0(Tuple4((Env1(var_0, var_1)), (Env2(var_2, var_1)), (Env3(var_3, var_1)))))
let (var_0: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(250L))
let (var_1: (string [])) = Array.zeroCreate&amp;lt;string&amp;gt; (System.Convert.ToInt32(250L))
let (var_2: (float32 [])) = Array.zeroCreate&amp;lt;float32&amp;gt; (System.Convert.ToInt32(250L))
let (var_3: int64) = 2L
let (var_4: bool) = (var_3 &amp;gt;= 0L)
let (var_6: bool) =
    if var_4 then
        (var_3 &amp;lt; 10L)
    else
        false
let (var_7: bool) = (var_6 = false)
if var_7 then
    (failwith &quot;Tensor application out of bounds&quot;)
else
    ()
let (var_8: int64) = (var_3 * 25L) // 50
let (var_9: int64) = (var_8 + 15L) // 65
let (var_10: int64) = (var_9 + 4L) // 69
method_0((var_0: (int64 [])), (var_10: int64), (var_1: (string [])), (var_2: (float32 [])))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In the above example the first application was &lt;code&gt;dyn&lt;/code&gt;ed. This makes the assertion check necessary at runtime, but the line of note is the last one. Due to common subexpression rewriting, in the end all the bodies end up with the same index variable.&lt;/p&gt;
&lt;p&gt;This will be reflected when they are being passed through join points.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;met f (a,b,c) = a+b+c
inl x = dyn 3
f (x,x,x)
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let rec method_0((var_0: int64)): int64 =
    let (var_1: int64) = (var_0 + var_0)
    (var_1 + var_0)
let (var_0: int64) = 3L
method_0((var_0: int64))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The moral of that is - for efficiency tensors should not be returned from anything apart from inlineables and should not be stored into arrays. Spiral's natural style is towards continuation passing and (mostly) functional purity. Tuples and modules should be used to store arguments whenever possible and opaque structures should be avoided.&lt;/p&gt;
&lt;ol start=&quot;3&quot;&gt;&lt;li&gt;On indexing and setting.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;These two are easy since all the hard work has already been done by &lt;code&gt;tensor_apply&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl tensor_index {bodies dim=()} = toa_map (inl {ar offset} -&amp;gt; ar offset) bodies
inl tensor_set {bodies dim=()} v = toa_map2 (inl {ar offset} v -&amp;gt; ar offset &amp;lt;- v) bodies v |&amp;gt; ignore

inl tns = 
    tensor_create {typ=int64,string,float32; dim=10,5,5}
    |&amp;gt; tensor_apply 2
    |&amp;gt; tensor_apply 3
    |&amp;gt; tensor_apply 4

tensor_set tns (1,&quot;asd&quot;,3.3f32)
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let (var_0: (int64 [])) = Array.zeroCreate&amp;lt;int64&amp;gt; (System.Convert.ToInt32(250L))
let (var_1: (string [])) = Array.zeroCreate&amp;lt;string&amp;gt; (System.Convert.ToInt32(250L))
let (var_2: (float32 [])) = Array.zeroCreate&amp;lt;float32&amp;gt; (System.Convert.ToInt32(250L))
var_0.[int32 69L] &amp;lt;- 1L
var_1.[int32 69L] &amp;lt;- &quot;asd&quot;
var_2.[int32 69L] &amp;lt;- 3.300000f
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is the &lt;code&gt;aot&lt;/code&gt; form for good measure.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl tns = 
    tensor_create {layout=.aot; typ=int64,string,float32; dim=10,5,5}
    |&amp;gt; tensor_apply 2
    |&amp;gt; tensor_apply 3
    |&amp;gt; tensor_apply 4

tensor_set tns (1,&quot;asd&quot;,3.3f32)
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: string
    val mem_2: float32
    new(arg_mem_0, arg_mem_1, arg_mem_2) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1; mem_2 = arg_mem_2}
    end
let (var_0: (Tuple0 [])) = Array.zeroCreate&amp;lt;Tuple0&amp;gt; (System.Convert.ToInt32(250L))
var_0.[int32 69L] &amp;lt;- Tuple0(1L, &quot;asd&quot;, 3.300000f)
&lt;/code&gt;
&lt;/pre&gt;
&lt;h6&gt;The Tensor Facade&lt;/h6&gt;
&lt;p&gt;Now that is is possible to create, apply, index and set the tensors the thing that remains is to make it applicable directly. To that, what is needed is to make a facade. The only thing of note in the following is that on standard application the facade rewraps itself. The rest should be straightforward.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl rec tensor_facade tns = function
    | .get -&amp;gt; tensor_get tns
    | .set v -&amp;gt; tensor_set tns v
    | .(_) &amp;amp; x -&amp;gt; tns x
    | x -&amp;gt; tensor_apply x tns |&amp;gt; tensor_facade

inl tensor_create = tensor_create &amp;gt;&amp;gt; tensor_facade

inl tns = tensor_create {layout=.aot; typ=int64,string,float32; dim=10,5,5} 
tns 2 3 4 .set (1,&quot;asd&quot;,3.3f32)
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Tuple0 =
    struct
    val mem_0: int64
    val mem_1: string
    val mem_2: float32
    new(arg_mem_0, arg_mem_1, arg_mem_2) = {mem_0 = arg_mem_0; mem_1 = arg_mem_1; mem_2 = arg_mem_2}
    end
let (var_0: (Tuple0 [])) = Array.zeroCreate&amp;lt;Tuple0&amp;gt; (System.Convert.ToInt32(250L))
var_0.[int32 69L] &amp;lt;- Tuple0(1L, &quot;asd&quot;, 3.300000f)
&lt;/code&gt;
&lt;/pre&gt;
&lt;h4&gt;Closing Comments&lt;/h4&gt;
&lt;p&gt;Views on tensors are similar to apply and just push the offsets without reducing the dimensionality of the tensor. It won't be covered in the tutorial and the interested should just look into the standard library implementation of them for specific details.&lt;/p&gt;
&lt;p&gt;There is an interesting programming lesson that the author (re)experienced while making the tensor tutorial. Near the beginning there was a comment that the tensor design for the tutorial is more uniform than the one in the standard library. It is actually even more than that. The way it has been designed here is in fact how the author remembered the tensor. He knew it was not like this in the standard library, but when he looked he was actually surprised at how it was made indicating that he in fact forgot about it.&lt;/p&gt;
&lt;p&gt;It is not the first time it happened that a piece poorly fit into memory to him and won't be the last. Memory mismatches are a sure sign that a particular piece of software should be redesigned. Having been left alone for a while, all but the most salient features of a program faded from memory indicating that in fact the rest are worthless and should be removed. Once he is done with the documentation that will surely be done.&lt;/p&gt;
&lt;p&gt;With this all that has needed to be said in order to understand tensors has been said, but a digression needs to be made to highlight just how great they really are.&lt;/p&gt;
&lt;p&gt;The tensor tutorial should have been rather clear and straightforward and unless something has badly gone wrong, those reading this chapter should have a clear picture of their essence.&lt;/p&gt;
&lt;p&gt;The number of languages in which a tensor can be implemented in such a manner can at the time of writing, 2 days to 2018, be literally counted on one finger. Being able to implement tensors like this is what essentially convinced the author that now that he has Spiral, it might be a good idea challenge the other deep learning frameworks for supremacy even though they have big corporate sponsorship.&lt;/p&gt;
&lt;p&gt;There is no need to consider how tensors are made in a language made for numeric computation like Julia. Take PyTorch for instance, and go to the &lt;a href=&quot;http://pytorch.org/blog/&quot; rel=&quot;nofollow&quot;&gt;tour of its internals&lt;/a&gt;. It is essentially a tour of poor programming practices: using C macros for everything including making tensors type generic, the absolutely fearsome &lt;code&gt;static PyTypeObject py_FloatType&lt;/code&gt; which is badly in need of modules or at least SML records, the friction between different components that just jumps out.&lt;/p&gt;
&lt;p&gt;From what has been heard of PyTorch in action by the author, there has been nothing but praise.&lt;/p&gt;
&lt;p&gt;Nevertheless, if the best tools for the job in 2018 for making tensors generic are the 1972 C macros then probably something went wrong somewhere and not just with PyTorch specifically.&lt;/p&gt;
&lt;p&gt;Whether it be composability or performance or lack of safety, those kinds of problems exist due to the weak type systems. But it is not like type systems have to be just about solving constraints, nor do they have to be segregated from the rest of the compiler passes. Lisps had the great idea of integrating parsing with the rest of compilation passes. There is nothing preventing similar to be done with a type system.&lt;/p&gt;
&lt;p&gt;Once that fusion is done, a piece of the power that is released can be seen in this chapter - a properly done tensor type.&lt;/p&gt;
&lt;h3&gt;6: The Cuda Backend&lt;/h3&gt;
&lt;p&gt;This section covers how the Cuda backend works in its entirety and is not mandatory for understanding how to program in Spiral. It can be skipped over. It does not cover GPU programming, but instead goes into some depth on what is going on under the hood when Spiral does GPU compilation.&lt;/p&gt;
&lt;h4&gt;Intro&lt;/h4&gt;
&lt;p&gt;After a successful compilation of a Spiral program, at the top of the file there is a &lt;code&gt;let cuda_kernel = ...&lt;/code&gt; statement. When no GPU code is run, that results in an empty string, but otherwise there will be code there.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let kernel1 =
    &quot;kernel1&quot;,[cuda_modules],&quot;Does the map kernel work?&quot;,
    &quot;&quot;&quot;
/// Initializes all the Cuda parts
inb s = CudaModules (1024*1024) // The allocator takes 1Mb of memory from the heap.

/// Creates a host tensor with the given generator function.
inl h = HostTensor.init 32 (inl x -&amp;gt; x + 1) 
/// Loads the tensor on the GPU based on the host tensor
inl a1 = s.CudaTensor.from_host_tensor h
/// Makes a tensor of the same type and dimensions as `a1` and zeroes it.
inl o1 = s.CudaTensor.zero_like a1
/// Calls the map operation. `a1` is the input and `o1` is the output.
s.CudaKernel.map' (inl a _ -&amp;gt; a * 2) a1 o1

/// Transfers the tensor back to host.
inl a2 = s.CudaTensor.to_host_tensor o1
/// Zips the two tensors and prints them out.
HostTensor.zip (h,a2) |&amp;gt; HostTensor.show |&amp;gt; Console.writeline
    &quot;&quot;&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;let cuda_kernels = &quot;&quot;&quot;
#include &quot;cub/cub.cuh&quot;

extern &quot;C&quot; {
    __global__ void method_23(long long int * var_0, long long int * var_1);
    __device__ char method_24(long long int * var_0);
    
    __global__ void method_23(long long int * var_0, long long int * var_1) {
        long long int var_2 = threadIdx.x;
        long long int var_3 = blockIdx.x;
        long long int var_4 = (128 * var_3);
        long long int var_5 = (var_2 + var_4);
        long long int var_6[1];
        var_6[0] = var_5;
        while (method_24(var_6)) {
            long long int var_8 = var_6[0];
            char var_9 = (var_8 &amp;gt;= 0);
            char var_11;
            if (var_9) {
                var_11 = (var_8 &amp;lt; 32);
            } else {
                var_11 = 0;
            }
            char var_12 = (var_11 == 0);
            if (var_12) {
                // &quot;Argument out of bounds.&quot;
            } else {
            }
            char var_14;
            if (var_9) {
                var_14 = (var_8 &amp;lt; 32);
            } else {
                var_14 = 0;
            }
            char var_15 = (var_14 == 0);
            if (var_15) {
                // &quot;Argument out of bounds.&quot;
            } else {
            }
            long long int var_16 = var_0[var_8];
            long long int var_17 = var_1[var_8];
            long long int var_18 = (var_16 * 2); // The actual work is done on this line.
            var_1[var_8] = var_18;
            long long int var_19 = (var_8 + 128);
            var_6[0] = var_19;
        }
        long long int var_20 = var_6[0];
    }
    __device__ char method_24(long long int * var_0) {
        long long int var_1 = var_0[0];
        return (var_1 &amp;lt; 32);
    }
}
&quot;&quot;&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;What is shown above is a map kernel that multiplies all the elements of a tensor by 2. In the kernel there are a bunch of inactive range checks that can be turned on with a compiler switch.&lt;/p&gt;
&lt;p&gt;Here is the output of the above program at runtime.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;[|[1, 2]; [2, 4]; [3, 6]; [4, 8]; [5, 10]; [6, 12]; [7, 14]; [8, 16]; [9, 18]; [10, 20]; [11, 22]; [12, 24]; [13, 26]; [14, 28]; [15, 30]; [16, 32]; [17, 34]; [18, 36]; [19, 38]; [20, 40]; [21, 42]; [22, 44]; [23, 46]; [24, 48]; [25, 50]; [26, 52]; [27, 54]; [28, 56]; [29, 58]; [30, 60]; [31, 62]; [32, 64]|]
&lt;/code&gt;
&lt;/pre&gt;
&lt;h4&gt;How Cuda Kernels Are Compiled&lt;/h4&gt;
&lt;p&gt;At runtime, the program takes everything in the &lt;code&gt;cuda_kernels&lt;/code&gt; variable and writes them to disk into the &lt;code&gt;cuda_kernels.cu&lt;/code&gt; file which is located in the same place as the executable. It also creates a little batch script called &lt;code&gt;nvcc_router.bat&lt;/code&gt;. Here are its contents. The paths are those provided by the &lt;code&gt;cfg&lt;/code&gt; argument to the Spiral compiler directly.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;SETLOCAL
CALL &quot;C:/Program Files (x86)/Microsoft Visual Studio/2017/Community\VC/Auxiliary/Build/vcvarsall.bat&quot; x64 -vcvars_ver=14.11
SET PATH=%PATH%;&quot;C:/Program Files (x86)/Microsoft Visual Studio/2017/Community\VC/Tools/MSVC/14.11.25503/bin/Hostx64/x64&quot;
&quot;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\bin/nvcc.exe&quot; -gencode=arch=compute_52,code=\&quot;sm_52,compute_52\&quot; --use-local-env --cl-version 2017 -I&quot;C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0\include&quot; -I&quot;C:/cub-1.7.4&quot; -I&quot;C:/Program Files (x86)/Microsoft Visual Studio/2017/Community\VC/Tools/MSVC/14.11.25503/include&quot; --keep-dir &quot;C:\Users\Marko\Source\Repos\The Spiral Language\Temporary\bin\Release&quot; -maxrregcount=0  --machine 64 -ptx -cudart static  -o &quot;C:\Users\Marko\Source\Repos\The Spiral Language\Temporary\bin\Release\cuda_kernels.ptx&quot; &quot;C:\Users\Marko\Source\Repos\The Spiral Language\Temporary\bin\Release\cuda_kernels.cu&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;What this does is do some setting up and then calls NVCC (The Nvidia Cuda compiler) from the command line.&lt;/p&gt;
&lt;p&gt;It compiles the &lt;code&gt;cuda_kernels.cu&lt;/code&gt; into &lt;code&gt;cuda_kernels.ptx&lt;/code&gt; which is the LLVM IR assembly Cuda natively uses. It is not the actual assembly for the GPU, but an intermediate representation that various Cuda API functions use. Here is the output of the map kernel example just for show.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-22781540
// Cuda compilation tools, release 9.0, V9.0.176
// Based on LLVM 3.4svn
//

.version 6.0
.target sm_52
.address_size 64

        // .globl       method_23
.global .align 1 .b8 _ZN69_INTERNAL_47_tmpxft_000171e4_00000000_7_cuda_kernels_cpp1_ii_b5c879af6thrust6system6detail10sequential3seqE[1];

.visible .entry method_23(
        .param .u64 method_23_param_0,
        .param .u64 method_23_param_1
)
{
        .reg .pred      %p&amp;lt;3&amp;gt;;
        .reg .b32       %r&amp;lt;3&amp;gt;;
        .reg .b64       %rd&amp;lt;25&amp;gt;;


        ld.param.u64    %rd10, [method_23_param_0];
        ld.param.u64    %rd11, [method_23_param_1];
        mov.u32         %r1, %tid.x;
        cvt.u64.u32     %rd12, %r1;
        mov.u32         %r2, %ctaid.x;
        mul.wide.u32    %rd13, %r2, 128;
        add.s64         %rd24, %rd13, %rd12;
        setp.gt.s64     %p1, %rd24, 31;
        @%p1 bra        BB0_3;

        cvta.to.global.u64      %rd14, %rd11;
        cvta.to.global.u64      %rd15, %rd10;
        add.s64         %rd18, %rd13, %rd12;
        shl.b64         %rd19, %rd18, 3;
        add.s64         %rd23, %rd15, %rd19;
        add.s64         %rd22, %rd14, %rd19;

BB0_2:
        ld.global.u64   %rd20, [%rd23];
        shl.b64         %rd21, %rd20, 1;
        st.global.u64   [%rd22], %rd21;
        add.s64         %rd23, %rd23, 1024;
        add.s64         %rd22, %rd22, 1024;
        add.s64         %rd24, %rd24, 128;
        setp.lt.s64     %p2, %rd24, 32;
        @%p2 bra        BB0_2;

BB0_3:
        ret;
}

        // .globl       _ZN3cub11EmptyKernelIvEEvv
.visible .entry _ZN3cub11EmptyKernelIvEEvv(

)
{



        ret;
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As can be seen, the useless range checks got eliminated. And the multiply by two is converted into a left shift.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;     ld.global.u64   %rd20, [%rd23];
        shl.b64         %rd21, %rd20, 1;
        st.global.u64   [%rd22], %rd21;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is the meat of the loop where it loads from global memory, multiplies by 2 and then stores after that. The rest is setting up the kernel and implementing the loop.&lt;/p&gt;
&lt;p&gt;What can be done with the &lt;code&gt;.ptx&lt;/code&gt; file is load the kernels inside them using the &lt;a href=&quot;http://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__MODULE.html#group__CUDA__MODULE&quot; rel=&quot;nofollow&quot;&gt;Cuda API functions&lt;/a&gt; accessed through the &lt;a href=&quot;https://github.com/kunzmi/managedCuda&quot;&gt;ManagedCuda wrapper library&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Tour Of The Standard Library Cuda Module&lt;/h4&gt;
&lt;p&gt;The &lt;code&gt;Cuda&lt;/code&gt; module is where the context is initialized and where the &lt;code&gt;run&lt;/code&gt; function that actually launches the kernels resides. The kernel compilation to &lt;code&gt;.ptx&lt;/code&gt; happens at runtime. It will be shown here in its entirety, but there is no need to think too deeply about what is going on. It is garden variety plumbing that can be summed in a sentence or two for each part.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let cuda =
    (
    &quot;Cuda&quot;,[loops;console;array;host_tensor;extern_;object],&quot;The Cuda module.&quot;,
    &quot;&quot;&quot;
inl ret -&amp;gt; 
    open Extern
    open Console

    inl cuda_kernels = FS.Constant.cuda_kernels string

    inl cuda_constant a t = !MacroCuda(t,[text: a])

    inl cuda_constant_int constant () = cuda_constant constant int64

    inl __blockDimX = cuda_constant_int &quot;blockDim.x&quot;
    inl __blockDimY = cuda_constant_int &quot;blockDim.y&quot;
    inl __blockDimZ = cuda_constant_int &quot;blockDim.z&quot;
    inl __gridDimX = cuda_constant_int &quot;gridDim.x&quot;
    inl __gridDimY = cuda_constant_int &quot;gridDim.y&quot;
    inl __gridDimZ = cuda_constant_int &quot;gridDim.z&quot;

    inl cuda_toolkit_path = @PathCuda
    inl visual_studio_path = @PathVS2017
    inl cub_path = @PathCub

    inl env_type = fs [text: &quot;System.Environment&quot;]
    inl context_type = fs [text: &quot;ManagedCuda.CudaContext&quot;]
    use context = FS.Constructor context_type false
    FS.Method context .Synchronize() ()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The only piece of functionality that happens at runtime are the last two lines where the Cuda context is created and then synchronized. The rest are merely definitions set up for later.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl compile_kernel_using_nvcc_bat_router (kernels_dir: string) =
        inl path_type = fs [text: &quot;System.IO.Path&quot;]
        inl combine x = FS.StaticMethod path_type .Combine x string
    
        inl file_type = fs [text: &quot;System.IO.File&quot;]
        inl stream_type = fs [text: &quot;System.IO.Stream&quot;]
        inl streamwriter_type = fs [text: &quot;System.IO.StreamWriter&quot;]
        inl process_start_info_type = fs [text: &quot;System.Diagnostics.ProcessStartInfo&quot;]
    
        inl nvcc_router_path = combine (kernels_dir,&quot;nvcc_router.bat&quot;)
        inl procStartInfo = FS.Constructor process_start_info_type ()
        FS.Method procStartInfo .set_RedirectStandardOutput true ()
        FS.Method procStartInfo .set_RedirectStandardError true ()
        FS.Method procStartInfo .set_UseShellExecute false ()
        FS.Method procStartInfo .set_FileName nvcc_router_path ()

        inl process_type = fs [text: &quot;System.Diagnostics.Process&quot;]
        use process = FS.Constructor process_type ()
        FS.Method process .set_StartInfo procStartInfo ()
        inl print_to_standard_output = 
            closure_of (inl args -&amp;gt; FS.Method args .get_Data() string |&amp;gt; writeline) 
                (fs [text: &quot;System.Diagnostics.DataReceivedEventArgs&quot;] =&amp;gt; ())

        FS.Method process .&quot;OutputDataReceived.Add&quot; print_to_standard_output ()
        FS.Method process .&quot;ErrorDataReceived.Add&quot; print_to_standard_output ()

        inl concat = string_concat &quot;&quot;
        inl (+) a b = concat (a, b)

        /// Puts quotes around the string.
        inl quote x = (&quot;\&quot;&quot;,x,&quot;\&quot;&quot;)
        inl vc_vars_args = &quot; x64 -vcvars_ver=14.11&quot;
        inl quoted_vs_path_to_vcvars = combine(visual_studio_path, &quot;VC/Auxiliary/Build/vcvarsall.bat&quot;) |&amp;gt; quote
        inl quoted_vs_path_to_cl = combine(visual_studio_path, &quot;VC/Tools/MSVC/14.11.25503/bin/Hostx64/x64&quot;) |&amp;gt; quote
        inl quoted_cuda_toolkit_path_to_include = combine(cuda_toolkit_path,&quot;include&quot;) |&amp;gt; quote
        inl quoted_vc_path_to_include = combine(visual_studio_path, &quot;VC/Tools/MSVC/14.11.25503/include&quot;) |&amp;gt; quote
        inl quoted_nvcc_path = combine(cuda_toolkit_path,@&quot;bin/nvcc.exe&quot;) |&amp;gt; quote
        inl quoted_cub_path_to_include = cub_path |&amp;gt; quote
        inl quoted_kernels_dir = kernels_dir |&amp;gt; quote
        inl target_path = combine(kernels_dir,&quot;cuda_kernels.ptx&quot;)
        inl quoted_target_path = target_path |&amp;gt; quote
        inl input_path = combine(kernels_dir,&quot;cuda_kernels.cu&quot;)
        inl quoted_input_path = input_path |&amp;gt; quote

        if FS.StaticMethod file_type .Exists input_path bool then FS.StaticMethod file_type .Delete input_path ()
        FS.StaticMethod file_type .WriteAllText(input_path,cuda_kernels) ()
   
        inl _ = 
            if FS.StaticMethod file_type .Exists nvcc_router_path bool then FS.StaticMethod file_type .Delete nvcc_router_path ()
            inl filestream_type = fs [text: &quot;System.IO.FileStream&quot;]

            use nvcc_router_file = FS.StaticMethod file_type .OpenWrite(nvcc_router_path) filestream_type
            use nvcc_router_stream = FS.Constructor streamwriter_type nvcc_router_file

            inl write_to_batch = concat &amp;gt;&amp;gt; inl x -&amp;gt; FS.Method nvcc_router_stream .WriteLine x ()

            &quot;SETLOCAL&quot; |&amp;gt; write_to_batch
            (&quot;CALL &quot;, quoted_vs_path_to_vcvars, vc_vars_args) |&amp;gt; write_to_batch
            (&quot;SET PATH=%PATH%;&quot;, quoted_vs_path_to_cl) |&amp;gt; write_to_batch
            (
            quoted_nvcc_path, &quot; -gencode=arch=compute_52,code=\\\&quot;sm_52,compute_52\\\&quot; --use-local-env --cl-version 2017&quot;,
            &quot; -I&quot;, quoted_cuda_toolkit_path_to_include,
            &quot; -I&quot;, quoted_cub_path_to_include,
            &quot; -I&quot;, quoted_vc_path_to_include,
            &quot; --keep-dir &quot;,quoted_kernels_dir,
            &quot; -maxrregcount=0  --machine 64 -ptx -cudart static  -o &quot;,quoted_target_path,&quot; &quot;,quoted_input_path
            ) |&amp;gt; write_to_batch

        inl stopwatch_type = fs [text: &quot;System.Diagnostics.Stopwatch&quot;]
        inl timer = FS.StaticMethod stopwatch_type .StartNew () stopwatch_type
        if FS.Method process .Start() bool = false then failwith () &quot;NVCC failed to run.&quot;
        FS.Method process .BeginOutputReadLine() ()
        FS.Method process .BeginErrorReadLine() ()
        FS.Method process .WaitForExit() ()

        inl exit_code = FS.Method process .get_ExitCode() int32
        assert (exit_code = 0i32) (&quot;NVCC failed compilation.&quot;, exit_code)
    
        inl elapsed = FS.Method timer .get_Elapsed() (fs [text: &quot;System.TimeSpan&quot;])
        !MacroFs((),[text: &quot;printfn \&quot;The time it took to compile the Cuda kernels is: %A\&quot; &quot;; arg: elapsed])

        FS.Method context .LoadModulePTX target_path (fs [text: &quot;ManagedCuda.BasicTypes.CUmodule&quot;])

    inl current_directory = FS.StaticMethod env_type .get_CurrentDirectory() string
    inl modules = compile_kernel_using_nvcc_bat_router current_directory
    writeline (string_concat &quot;&quot; (&quot;Compiled the kernels into the following directory: &quot;, current_directory))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is the part the actually compiles the modules. It creates that batch script and sets up the &lt;code&gt;Process&lt;/code&gt; object which it then uses to launch the script from the command line. The above code fragment is messy due to heavy use of macros, but it is straightforward.&lt;/p&gt;
&lt;p&gt;After the above is executed successfully, a &lt;code&gt;CUmodule&lt;/code&gt; object is bound to the &lt;code&gt;modules&lt;/code&gt; variable which holds all the kernels in binary format. This is used in the &lt;code&gt;run&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl dim3 = function
        | {x y z} as m -&amp;gt; m
        | x,y,z -&amp;gt; {x=x: int64; y=y: int64; z=z: int64}
        | x,y -&amp;gt; {x=x: int64; y=y: int64; z=1}
        | x -&amp;gt; {x=x: int64; y=1; z=1}

    inl SizeT_type = fs [text: &quot;ManagedCuda.BasicTypes.SizeT&quot;]
    inl CUdeviceptr_type = fs [text: &quot;ManagedCuda.BasicTypes.CUdeviceptr&quot;]
    inl SizeT = FS.Constructor SizeT_type
    inl CUdeviceptr = FS.Constructor CUdeviceptr_type &amp;lt;&amp;lt; SizeT

    met run s {blockDim gridDim kernel} =
        inl blockDim = dim3 blockDim
        inl gridDim = dim3 gridDim
        inl to_obj_ar args =
            inl ty = fs [text: &quot;System.Object&quot;] |&amp;gt; array
            !MacroFs(ty,[fs_array_args: args; text: &quot;: &quot;; type: ty])

        inl kernel =
            inl map_to_op_if_not_static {x y z} (x', y', z') = 
                inl f x x' = if lit_is x then const x else x' 
                f x x', f y y', f z z'
            inl x,y,z = map_to_op_if_not_static blockDim (__blockDimX,__blockDimY,__blockDimZ)
            inl x',y',z' = map_to_op_if_not_static gridDim (__gridDimX,__gridDimY,__gridDimZ)
            inl _ -&amp;gt; // This convoluted way of swaping non-literals for ops is so they do not get called outside of the kernel.
                inl blockDim = {x=x(); y=y(); z=z()}
                inl gridDim = {x=x'(); y=y'(); z=z'()}
                kernel blockDim gridDim

        inl join_point_entry_cuda x = !JoinPointEntryCuda(x())
        inl method_name, args = join_point_entry_cuda kernel
        
        inl dim3 {x y z} = Tuple.map (to uint32) (x,y,z) |&amp;gt; FS.Constructor (fs [text: &quot;ManagedCuda.VectorTypes.dim3&quot;])
    
        inl kernel_type = fs [text: &quot;ManagedCuda.CudaKernel&quot;]
        inl cuda_kernel = FS.Constructor kernel_type (method_name,modules,s.data.context)
        FS.Method cuda_kernel .set_GridDimensions(dim3 gridDim) ()
        FS.Method cuda_kernel .set_BlockDimensions(dim3 blockDim) ()

        match s.data.stream with
        | () -&amp;gt; FS.Method cuda_kernel .Run(to_obj_ar args) float32
        | stream -&amp;gt; FS.Method cuda_kernel .RunAsync(stream.extract,to_obj_ar args) ()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Before moving forward, here is how the above function is used in practice.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let tutorial1 =
    &quot;tutorial1&quot;,[cuda_modules],&quot;The placeholder for the tutorial examples&quot;,
    &quot;&quot;&quot;
inb s = CudaModules (1024*1024) 

inl fact to = Loops.for {from=2; to state=dyn 1; body=inl {state i} -&amp;gt; state * i}

s.run {
    blockDim=1
    gridDim=1
    kernel=inl blockDim gridDim -&amp;gt; 
        fact 3 |&amp;gt; ignore
    }
    &quot;&quot;&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This would result in the factorial function as recursive GPU code.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let cuda_kernels = &quot;&quot;&quot;
#include &quot;cub/cub.cuh&quot;

extern &quot;C&quot; {
    __global__ void method_10();
    __device__ long long int method_11(long long int var_0, long long int var_1);
    
    __global__ void method_10() {
        long long int var_0 = 1;
        long long int var_1 = 2;
        long long int var_2 = method_11(var_0, var_1);
    }
    __device__ long long int method_11(long long int var_0, long long int var_1) {
        char var_2 = (var_1 &amp;lt;= 3);
        if (var_2) {
            long long int var_3 = (var_0 * var_1);
            long long int var_4 = (var_1 + 1);
            return method_11(var_3, var_4);
        } else {
            return var_0;
        }
    }
}
&quot;&quot;&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In the standard library the following form would be used instead.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    kernel=cuda
        fact 3 |&amp;gt; ignore
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;cuda&lt;/code&gt; is just shorthand for &lt;code&gt;inl blockDim gridDim -&amp;gt;&lt;/code&gt; built into the parser.&lt;/p&gt;
&lt;p&gt;There are points of interest that need to be explained for &lt;code&gt;run&lt;/code&gt; to be fully understood.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;            inl kernel =
                inl map_to_op_if_not_static {x y z} (x', y', z') = 
                    inl f x x' = if lit_is x then const x else x' 
                    f x x', f y y', f z z'
                inl x,y,z = map_to_op_if_not_static blockDim (__blockDimX,__blockDimY,__blockDimZ)
                inl x',y',z' = map_to_op_if_not_static gridDim (__gridDimX,__gridDimY,__gridDimZ)
                inl _ -&amp;gt; // This convoluted way of swaping non-literals for ops is so they do not get called outside of the kernel.
                    inl blockDim = {x=x(); y=y(); z=z()}
                    inl gridDim = {x=x'(); y=y'(); z=z'()}
                    kernel blockDim gridDim
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;What the above does is make sure that if the block and grid dimension sizes are known at compile time, that they are also passed into the kernel as literals. Otherwise they are used as intrinsics directly.&lt;/p&gt;
&lt;p&gt;In Cuda C code when &lt;code&gt;blockDim.x&lt;/code&gt; is used directly, that is not a compile time constant, but a runtime variable. Spiral goes to an extra length in order to preserve information and intrinsics for substitutes literals whenever possible. The above code implements that.&lt;/p&gt;
&lt;p&gt;This is actually necessary for interop with the Cuda Unbound library which takes in block and grid dimensions as template parameters and requires them to be literals.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;            inl join_point_entry_cuda x = !JoinPointEntryCuda(x())
            inl method_name, args = join_point_entry_cuda kernel
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The part directly after that is where the kernel gets executed. Cuda has a join point special to it and that is invoked using &lt;code&gt;!JoinPointEntryCuda(x())&lt;/code&gt;. If this was a standard join point then that would be enough to call the function, but in Cuda's case things are a bit more complicated. Because all the calls have to go through the &lt;code&gt;ManagedCuda&lt;/code&gt; library and then through the Cuda API, it would be very difficult to bake that call into the language directly.&lt;/p&gt;
&lt;p&gt;Instead what the Cuda join point does is compile the function and returns the method name and the free variables passed into the join point. The method name is used to extract the kernel and the free variables are passed as arguments to the &lt;code&gt;Run&lt;/code&gt; and &lt;code&gt;RunAsync&lt;/code&gt; method.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;            inl dim3 {x y z} = Tuple.map (to uint32) (x,y,z) |&amp;gt; FS.Constructor (fs [text: &quot;ManagedCuda.VectorTypes.dim3&quot;])
    
            inl kernel_type = fs [text: &quot;ManagedCuda.CudaKernel&quot;]
            inl cuda_kernel = FS.Constructor kernel_type (method_name,modules,context)
            FS.Method cuda_kernel .set_GridDimensions(dim3 gridDim) ()
            FS.Method cuda_kernel .set_BlockDimensions(dim3 blockDim) ()

            match stream with
            | () -&amp;gt; FS.Method cuda_kernel .Run(to_obj_ar args) float32
            | stream -&amp;gt; FS.Method cuda_kernel .RunAsync(stream.extract,to_obj_ar args) ()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is where the kernel is actually loaded.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl cuda_kernel = FS.Constructor kernel_type (method_name,modules,context)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;More specifically, on this line. It loads the kernel of &lt;code&gt;method_name&lt;/code&gt; and uses the &lt;code&gt;modules&lt;/code&gt; objects that holds all the compiled kernels from the previous step and the Cuda &lt;code&gt;context&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;            match stream with
            | () -&amp;gt; FS.Method cuda_kernel .Run(to_obj_ar args) float32
            | stream -&amp;gt; FS.Method cuda_kernel .RunAsync(stream.extract,to_obj_ar args) ()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is where the kernel is invoked synchronously or asynchronously depending on whether a stream was passed in.&lt;/p&gt;
&lt;p&gt;This covers everything needed to know in order to understand the Cuda backend. This is the Cuda backend in nearly its entirety.&lt;/p&gt;
&lt;h4&gt;Why Spiral Was Created&lt;/h4&gt;
&lt;p&gt;The above is simple, but unless the language supports it then it is impossible to build it as a part of a library. Being able to do the above is one of the major motivators for the creation of Spiral. It simply saves such an enormous amount of work.&lt;/p&gt;
&lt;h5&gt;How It Used To Be Done&lt;/h5&gt;
&lt;p&gt;The Spiral language originates from ML library of the same name done by author during the 2016 period.&lt;/p&gt;
&lt;p&gt;Here is how the map kernel used to look there.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// o &amp;lt;- f(x)
type DeviceUnaryTransformModule(op: string, unique_name : string) = 
    let kernel_name = &quot;Map1Kernel&quot;+unique_name
    let kernel_code = 
        [|&quot;
        //Kernel code:
        extern \&quot;C\&quot; {
            typedef float floatType;
            __device__ inline floatType op(floatType x)
            {
                return &quot;;op;&quot;
            }
        
            // Device code
            __global__ void &quot;;kernel_name;&quot;(const floatType* A, floatType* O, const int N)
            {
                int i = blockDim.x * blockIdx.x + threadIdx.x;
                const int stride = blockDim.x * gridDim.x;
                while (i &amp;lt; N)
                {
                    O[i] = op(A[i]);
                    i += stride;
                }
            }
        }

        &quot; |] |&amp;gt; String.concat &quot;&quot;

    member val Kernel = load_kernel_nvrtc kernel_code kernel_name
    member inline t.A
            (str: CudaStream,
                (ext_x: ^a -&amp;gt; CUdeviceptr, x: ^a),
                (ext_o: ^a -&amp;gt; CUdeviceptr, o: ^a)) =
        GuardSizes2(x,o)
        let n = Size x
        map_launcher(str, t.Kernel, n, [|ext_x x; ext_o o; n|])
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It was used in two places.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let squareModule = lazy new DeviceUnaryTransformModule(&quot;x*x;&quot;,&quot;Square&quot;)
let logModule = lazy new DeviceUnaryTransformModule(&quot;logf(x);&quot;,&quot;Log&quot;)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Text macros are used as operations because there is not much choice apart from that for getting some generic kernel functionality. It was not much generic functionality though. Suppose I want to pass in two arguments into the map kernel instead of one.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// o &amp;lt;- f(x,y)
type DeviceBinaryTransformModule(op: string, unique_name) = 
    let kernel_name = &quot;Map2Kernel&quot; + unique_name
    let kernel_code = 
        [|&quot;
        //Kernel code:
        extern \&quot;C\&quot; {
            typedef float floatType;
            __device__ inline floatType op(floatType x, floatType y)
            {
                return &quot;;op;&quot;
            }
        
            // Device code
            __global__ void &quot;;kernel_name;&quot;(const floatType* A, const floatType* B, floatType* O, const int N)
            {
                int i = blockDim.x * blockIdx.x + threadIdx.x;
                const int stride = blockDim.x * gridDim.x;
                while (i &amp;lt; N)
                {
                    O[i] = op(A[i],B[i]);
                    i += stride;
                }
            }
        }

        &quot; |] |&amp;gt; String.concat &quot;&quot;
    
    member val Kernel = load_kernel_nvrtc kernel_code kernel_name
    member inline t.A
            (str: CudaStream,
                (ext_x: ^a -&amp;gt; CUdeviceptr, x: ^a),
                (ext_y: ^a -&amp;gt; CUdeviceptr, y: ^a),
                (ext_o: ^a -&amp;gt; CUdeviceptr, o: ^a)) =
        GuardSizes3(x,y,o)
        let n = Size x
        map_launcher(str, t.Kernel, n, [|ext_x x; ext_y y; ext_o o; n|])
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This one was used only for pointwise multiplication.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let hadamaradMultiplicationModule = lazy new DeviceBinaryTransformModule(&quot;x*y;&quot;, &quot;HadMult&quot;)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;There were many more of such kernels and after 6 variation on the basic map, the author lost drive to more because once he had those he could implement the rest in terms of composition.&lt;/p&gt;
&lt;p&gt;As an example of what that means, here is how binary cross entropy was implemented in the previous library.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let inline cross_entropy_cost (target: ^a) (activations: ^a) = context {
    let lt = target
    let! ll = log_ activations
    let! rt = scalar_matrix_add 1.0f -1.0f target
    let! rl = scalar_matrix_add 1.0f -1.0f activations &amp;gt;&amp;gt;= log_
    return! linear_layer_hadmult [|lt, ll; rt, rl|] 
            &amp;gt;&amp;gt;= sum
            &amp;gt;&amp;gt;= scale (-1.0f / float32 (cols target))
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The author could not write something like...&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl cross_entropy = error {
        fwd = inl x, y -&amp;gt; -(y * log x + (one - y) * log (one - x))
        bck = 
            inl (x, y) _ -&amp;gt; (x - y) / (x * (one - x))
            ,inl (x, y) _ -&amp;gt; log (one - x) - log x
        }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;...but he did have individual operations for log and scalar matrix addition and Hadamarad multiplication so he could piece the required operation together.&lt;/p&gt;
&lt;p&gt;The difference between doing it directly and indirectly is the 6 intermediate allocations that the old library required to perform the same thing. Since GPUs are memory bound, that would make for a vast difference in performance.&lt;/p&gt;
&lt;p&gt;In the old library there simply was not a middle ground between writing all the kernels by hand and having to compose very small pieces in an inefficient manner. But the issues with the old arrangement did not stop there.&lt;/p&gt;
&lt;p&gt;Even if one is resolved to do the kernels by hand, some operations when done in composite require tracking a very large number of variables. Imagine the horror of having to deal with well over a dozen &lt;code&gt;float *&lt;/code&gt; variables differentiated only by their name inside a single kernel and having no boundary or type checks to speak off. The author has had issues with swapping variables around by accident when there are just two of them of the same type in the same function.&lt;/p&gt;
&lt;p&gt;Mentally tracking over a dozen pointers to a tensor, their offsets and sizes with only their names to differentiate them would be simply impossible. And the author quickly realized that he could not take responsibility for such code in old library.&lt;/p&gt;
&lt;p&gt;Machine learning code is the worst in terms of debugging difficulty. Back when he first started, the author did appreciate dimensionality checking and had errors with some matrices being incorrectly transposed. There was one particular example where he hit 96% on Mnist despite the network propagating gradients in the wrong places.&lt;/p&gt;
&lt;p&gt;It very possible for mistakes to go unnoticed because only half the network gets updated or updated incorrectly, but the network still appears to work fine.&lt;/p&gt;
&lt;p&gt;Hence more than anywhere else, being able to reason about all aspects of code is of vital importance in a machine learning context. In fact, it is absolutely important everywhere.&lt;/p&gt;
&lt;p&gt;There is also one other point worth noting. All the operation in the old library have the &lt;code&gt;lazy&lt;/code&gt; prefixed behind them. The reason for that is that they are compiled individually and the author found that NVRTC required around 0.5s to compile a single operation. With 20&amp;gt; operations in the library that made for some massive compile times if all of them are compiled every time. NVCC is not much better. It requires 0.7s to compile an empty file and about 2.1s for a 3k file, so compiling all the kernels in single batch is important for speed's sake.&lt;/p&gt;
&lt;p&gt;This is also something that would be impossible to do without the support of a language. The assembling of information is in fact the dictionary definition for the world 'compilation'.&lt;/p&gt;
&lt;h3&gt;7: Object Orientation&lt;/h3&gt;
&lt;p&gt;Spiral is a functional language at its core, but it is good at doing OO despite not having specific features for it built in like other statically typed languages. Since OO is needed for passing around contexts in various key places, this chapter will be an overview of how it is done.&lt;/p&gt;
&lt;h4&gt;Motivation&lt;/h4&gt;
&lt;p&gt;A significant amount of complexity in the Spiral's ML library comes from needing to manage Cuda memory directly. For a that a design pattern is needed to deal with it in the absence of garbage collection. Meaning all the data needs to be packed into some kind of object and passed around.&lt;/p&gt;
&lt;p&gt;In functional languages reader, state and writer monads are commonly used for this sort of dependency injection. An argument could be made that the reader pattern is a variant on the OO pattern except with the &lt;code&gt;self&lt;/code&gt; argument on the opposite end.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl f self a b = ... // OO pattern
inl f a b self = ... // reader pattern
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;For some parts of the library it is much simpler to pass context in the first place and not bother with composition. The OO pattern shines there. Monads have the disadvantage of requiring all the code to be a part of the same monadic workflow. Some languages like Haskell go through great lengths to make that ergonomic, but Spiral is not such a language and it would be preferable to avoid using monads unless they are needed.&lt;/p&gt;
&lt;p&gt;It is easy to switch between the above patterns anyway.&lt;/p&gt;
&lt;p&gt;One other advantage is that OO breaks the usual top down ordering of functions.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl a x = ...
inl b x = ...
inl c x = ...
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In the above fragment, &lt;code&gt;a&lt;/code&gt; cannot refer to &lt;code&gt;b&lt;/code&gt; or &lt;code&gt;c&lt;/code&gt; as they come after it, and &lt;code&gt;b&lt;/code&gt; cannot refer to &lt;code&gt;c&lt;/code&gt;, but it is possible to get around that using OO in Spiral which can be useful.&lt;/p&gt;
&lt;p&gt;The OO pattern can also be used for easy immutable updates. This is not something OO is known for.&lt;/p&gt;
&lt;h4&gt;The Object&lt;/h4&gt;
&lt;pre&gt;
&lt;code&gt;/// Converts the argument (usually a module) to the object form.
inl obj s x = s x s
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;obj&lt;/code&gt; is defined in &lt;code&gt;Core&lt;/code&gt;. In &lt;code&gt;s x s&lt;/code&gt; the &lt;code&gt;s x&lt;/code&gt; part selects the method from &lt;code&gt;s&lt;/code&gt; and then &lt;code&gt;s&lt;/code&gt; is passed into that method.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let object =
    (
    &quot;Object&quot;,[],&quot;The Object module.&quot;,
    &quot;&quot;&quot;
{
data' = {}
data = inl {data'} -&amp;gt; data'
data_add = inl s v -&amp;gt; {s with data'=module_foldl (inl name s v -&amp;gt; module_add name v s) (indiv self) v |&amp;gt; heap} |&amp;gt; obj
member_add = inl s -&amp;gt; module_foldl (inl name s v -&amp;gt; module_add name (inl s -&amp;gt; v (obj s)) s) s &amp;gt;&amp;gt; obj
module_add = inl s name v -&amp;gt; module_add name (inl s name -&amp;gt; v name (obj s)) s |&amp;gt; obj
unwrap = id
} 
|&amp;gt; obj
    &quot;&quot;&quot;) |&amp;gt; module_
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The base Object in Spiral is implemented like this. It continually wraps its members with the &lt;code&gt;obj&lt;/code&gt; function in order to emulate OO access in standard statically typed OO languages. Maybe at some point remove functions will be added.&lt;/p&gt;
&lt;p&gt;The main thing to keep in mind when considering &lt;code&gt;data_add&lt;/code&gt;, &lt;code&gt;member_add&lt;/code&gt;, &lt;code&gt;module_add&lt;/code&gt; is that the first argument to them is not an object, but an unwrapped module.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;module_add name (inl s -&amp;gt; v (obj s)) s
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Hence when the module &lt;code&gt;s&lt;/code&gt; is being passed into the member, it is always wrapped into an object. What would happen if the fragment was written like this...&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;module_add name v s
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;...is that then the unwrapped &lt;code&gt;s&lt;/code&gt; would get passed into the member. The reason why &lt;code&gt;data_add&lt;/code&gt; does not wrap &lt;code&gt;v&lt;/code&gt; inside the module is to allow data fields to be accessed directly.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;data_add&lt;/code&gt;, &lt;code&gt;member_add&lt;/code&gt;, &lt;code&gt;module_add&lt;/code&gt; always return an object so their return is piped to &lt;code&gt;obj&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;data' = {}
data = inl {data'} -&amp;gt; data'
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The actual data is stored in the &lt;code&gt;data'&lt;/code&gt; field rather than &lt;code&gt;data&lt;/code&gt; because trying to access an object will pass itself to its argument as the first move and result in a type error. Hence the only way to access the data directly in that case would be to call &lt;code&gt;unwrap&lt;/code&gt; first or do it like it has been done and make a member that reroutes to it.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;data_add&lt;/code&gt; in particular also makes sure that the &lt;code&gt;data'&lt;/code&gt; is stored on the heap at all times rather than passed around as individual arguments through join points. It makes the generated code look decently nicer, speeds up compilation and improves runtime performance.&lt;/p&gt;
&lt;p&gt;Here is an example of it in use:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    Object
        .member_add {run}
        .data_add {context stream=()}
    |&amp;gt; ret
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;module_add&lt;/code&gt; can be used similarly.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;s.module_add .CudaTensor methods
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Members can be accessed directly like...&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;s.allocate
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Modules on the other hand require two type literals as arguments.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;s.RegionMem.allocate
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Data member are prefixed with &lt;code&gt;data&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;s.data.context
&lt;/code&gt;
&lt;/pre&gt;
&lt;h3&gt;8: GPU Programming Basics&lt;/h3&gt;
&lt;p&gt;GPU programming is well worth learning for those interested in drawing out as most performance as is possible from the machine, and lessons from it transfer over into CPU programming. At some point Spiral will drop Cuda and switch to supporting neural computing architectures, and when that happens the lesson learning in the GPU arena can be expected to transfer. The reason for that will be because those architectures will be similar to GPUs except with a lot more local memory.&lt;/p&gt;
&lt;p&gt;Knowing how to program effectively will never go out of date.&lt;/p&gt;
&lt;p&gt;This chapter will cover a few select kernels from the &lt;code&gt;CudaKernel&lt;/code&gt; module found in the &lt;code&gt;Learning&lt;/code&gt; project. There are more than will be covered in this chapter and more yet will be made in the future, but these can be counted on to illustrate how Spiral does Cuda programming.&lt;/p&gt;
&lt;h4&gt;map&lt;/h4&gt;
&lt;pre&gt;
&lt;code&gt;met map' w f in out = 
    inl in, out = zip in, zip out
    inl to_dev_tensor = w.CudaTensor.to_dev_tensor
    assert (in.dim = out.dim) &quot;The input and output dimensions must be equal.&quot;
    inl in = flatten in |&amp;gt; to_dev_tensor
    inl out = flatten out |&amp;gt; to_dev_tensor
    inl in_a :: () = in.dim
    
    inl blockDim = 128
    inl gridDim = min 64 (divup (s in_a) blockDim)

    w.run {
        blockDim gridDim
        kernel = cuda // Lexical scoping rocks.
            grid_for {blockDim gridDim} .x in_a {body=inl {i} -&amp;gt;
                inl out = out i
                inl in = in i
                out .set (f in.get out.get)
                }
        }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It actually more of a pain to set up the arguments before passing them into the kernel than writing the kernel itself. In fact, the actual kernel could be shortened to one line without much trouble.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;cuda grid_for {blockDim gridDim} .x in_a {body=inl {i} -&amp;gt; out i .set (f (in i .get) (out i .get))}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Lexical scoping just buys so much in this example that it is amazing. It scoops up both of the input and output tensors and allows me to pass in &lt;code&gt;in_a&lt;/code&gt; as the loop dimension in one move. Considering how wide range of a functionality Spiral's tensors have compared to arrays that makes writing kernels significantly easier.&lt;/p&gt;
&lt;p&gt;The above example is simple as the tensors are &lt;code&gt;flatten&lt;/code&gt;ed to 1d before being shipped off into the kernel, but other kernels will make use of the tensor's full functionality.&lt;/p&gt;
&lt;p&gt;Most of the functionality in the above fragment should be obvious from reading it, but &lt;code&gt;to_dev_tensor&lt;/code&gt; and &lt;code&gt;divup&lt;/code&gt; require an explanation.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl divup a b = (a-1)/b+1 // Integer division with rounding up. (a+b-1)/b is another variant on this.
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;So &lt;code&gt;divup 9 5 = 2&lt;/code&gt; and &lt;code&gt;divup 10 5 = 2&lt;/code&gt;, but &lt;code&gt;divup 11 5 = 3&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;to_dev_tensor&lt;/code&gt; on the other hand does some transformation on the tensor under the hood. &lt;code&gt;CudaTensor&lt;/code&gt;s have a reference to a pointer, but the device tensors need to be raw pointers. So &lt;code&gt;to_dev_tensor&lt;/code&gt; strips the reference in order to allow the tensor to cross the language boundary. In addition to that, it also adds the offset to the pointer and replaces it with 0.&lt;/p&gt;
&lt;p&gt;In .NET land it is not possible to move pointers around - array pointers must always point at the beginning of it, but in C they can point to anywhere. By adding the offset to the pointer and replacing it with 0, that allows the offset to be passed through the join point as a literal and not manifest as an argument.&lt;/p&gt;
&lt;p&gt;In Spiral, some types which are not blittable like strings and chars can be passed onto the Cuda side at compile time as literals. This hold true for all types fully known at compile time.&lt;/p&gt;
&lt;p&gt;Apart from that the context is &lt;code&gt;w&lt;/code&gt; in the &lt;code&gt;CudaKernel&lt;/code&gt; module because &lt;code&gt;s&lt;/code&gt; is taken up by the span &lt;code&gt;{near_to from} -&amp;gt; near_to - from&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;The following example was already shown in the Cuda backend chapter, but here it is as a review.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let kernel1 =
    &quot;kernel1&quot;,[cuda_modules],&quot;Does the map kernel work?&quot;,
    &quot;&quot;&quot;
/// Initializes all the Cuda parts
inb s = CudaModules (1024*1024) // The allocator takes 1Mb of memory from the heap.

/// Creates a host tensor with the given generator function.
inl h = HostTensor.init 32 (inl x -&amp;gt; x + 1) 
/// Loads the tensor on the GPU based on the host tensor
inl a1 = s.CudaTensor.from_host_tensor h
/// Makes a tensor of the same type and dimensions as `a1` and zeroes it.
inl o1 = s.CudaTensor.zero_like a1
/// Calls the map operation. `a1` is the input and `o1` is the output.
s.CudaKernel.map' (inl a _ -&amp;gt; a * 2) a1 o1

/// Transfers the tensor back to host.
inl a2 = s.CudaTensor.to_host_tensor o1
/// Zips the two tensors and prints them out.
HostTensor.zip (h,a2) |&amp;gt; HostTensor.show |&amp;gt; Console.writeline
    &quot;&quot;&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The kind of code this fragment produces was described in the backend chapter, so it won't be covered here.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;map'&lt;/code&gt; as shown here has the notable issue of needing to have its output feed to it. It is possible to abstract that away.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl map w f in =
    indiv join
        inl in = zip in
        inl out = w.CudaTensor.create {dim=in.dim; elem_type=type f in.elem_type}
        map' w (inl in _ -&amp;gt; f in) in out
        stack out
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;All the functions apart from &lt;code&gt;map_redo_map&lt;/code&gt; in the &lt;code&gt;CudaKernel&lt;/code&gt; module have a variant that automatically allocates the output provided to them.&lt;/p&gt;
&lt;p&gt;The reason why map is written like the above instead of like...&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl map w f in =
    inl in = zip in
    inl out = w.CudaTensor.create {dim=in.dim; elem_type=type f in.elem_type}
    map' w (inl in _ -&amp;gt; f in) in out
    out
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;...is because it was found out that the variant that uses join points and layout types has significantly better compile times. This will be covered in more detail later.&lt;/p&gt;
&lt;p&gt;What &lt;code&gt;map&lt;/code&gt; does is allocate the output tensor based on the input dimension and the inferred output type and then passes that to &lt;code&gt;map'&lt;/code&gt;. It also wraps around the input function so it throws away the output argument given to it.&lt;/p&gt;
&lt;p&gt;All auxiliary kernel variants do those and only those things.&lt;/p&gt;
&lt;h4&gt;map_redo_map&lt;/h4&gt;
&lt;p&gt;Zips, flattens the tensor to 1d, maps it, reduces it and then maps the output scalar tensor. This particular kernel is used for cost functions in the ML library.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Zips, flattens the tensor to 1d, maps, reduces it and maps it.
/// Map is optional. Allocates a temporary tensor for the intermediary results.
inl map_redo_map w {d with redo neutral_elem} in =
    indiv join
        inl to_dev_tensor = w.CudaTensor.to_dev_tensor
   
        inl run {map_out map_in blockDim gridDim} (!to_dev_tensor in) =
            inl in_a :: () = in.dim
            inl out = w.CudaTensor.create {elem_type=type map_in in.elem_type |&amp;gt; map_out; dim=gridDim}
            inl out' = to_dev_tensor out

            w.run {
                blockDim gridDim
                kernel = cuda 
                    inl x = 
                        grid_for {blockDim gridDim} .x in_a {state=dyn neutral_elem; body=inl {state i} -&amp;gt; redo state (map_in (in i .get)) }
                        |&amp;gt; cub_block_reduce {blockDim redo} |&amp;gt; map_out
                    if threadIdx.x = 0 then out' blockIdx.x .set x
                }

            out

        inl in = zip in |&amp;gt; flatten
        inl map_in = match d with {map_in} -&amp;gt; map_in | _ -&amp;gt; id
        inl map_out = match d with {map_out} -&amp;gt; map_out | _ -&amp;gt; id

        inl in_a :: () = in.dim
        inl span = s in_a
        inl blockDim = lit_min span 1024
        inl gridDim = 1 //lit_min 64 (divup span blockDim)

        inl r = 
            if gridDim = 1 then
                run {map_out map_in blockDim gridDim} in
            else
                run {map_out=id; map_in blockDim gridDim} in
                |&amp;gt; run {map_out map_in=id; blockDim=gridDim; gridDim=1}
        r 0 |&amp;gt; stack
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The actual Cuda part of the kernel is this...&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl x = 
    grid_for {blockDim gridDim} .x in_a {state=dyn neutral_elem; body=inl {state i} -&amp;gt; redo state (map_in (in i .get)) }
    |&amp;gt; cub_block_reduce {blockDim redo} |&amp;gt; map_out
if threadIdx.x = 0 then out' blockIdx.x .set x
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The kernel can be described in 3 steps:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;The blocks iterate over the global memory and perform the maps and reductions as they go along. This means that if there is one block of 1024 iterating over a 1024*128 array, it will perform 128 reductions before moving to the next step.&lt;/li&gt;
&lt;li&gt;A block reduction. Whenever possible, Spiral offloads work to the Cuda Unbound library as writing Cuda kernels can be tricky.&lt;/li&gt;
&lt;li&gt;When that is done, only the first thread has the result of the block reduce. It writes that result to global memory after mapping it.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;If the &lt;code&gt;gridDim&lt;/code&gt; is 1, then the actually kernel needs to be run only once, but otherwise it must be done twice in order to perform the inter block reduction.&lt;/p&gt;
&lt;h5&gt;flatten&lt;/h5&gt;
&lt;pre&gt;
&lt;code&gt;        inl in = zip in |&amp;gt; flatten
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Like in the map, the input is zipped and then flattened. Since flatten was made after the Tensor chapter was made, it will be covered here. Here it is in full from the &lt;code&gt;HostTensor&lt;/code&gt; module.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Flattens the tensor to a single dimension.
inl flatten tns =
    match tns.dim with
    | () -&amp;gt; tns
    | !(Tuple.map span) dim -&amp;gt;
        tns .set_dim (product dim)
            .update_body (inl {d with size} -&amp;gt;
                Tuple.zip (dim,size)
                |&amp;gt; Tuple.reducel (inl d,s d',s' -&amp;gt;
                    assert (s = d' * s') &quot;The tensor must be contiguous in order to be flattened.&quot;
                    d*s, s'
                    )
                |&amp;gt; inl _,s -&amp;gt; {d with size=s :: ()}
                )
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The dimension of the output tensor is set to the product of its spans, but in order for a tensor to be capable of being flattened it must be contiguous.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                |&amp;gt; Tuple.reducel (inl d,s d',s' -&amp;gt;
                    assert (s = d' * s') &quot;The tensor must be contiguous in order to be flattened.&quot;
                    d*s, s'
                    )
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In order for that to hold true the size of the outer dimension must equal the span of the inner dimension times its size.&lt;/p&gt;
&lt;p&gt;What that means is this - suppose there is a 2d tensor of dimensions (10,10) and size (10,1). That tensor could then be flattened to a 1d tensor of dimension 100 and size 1. But suppose a view of the tensor was taken along the inner dimension.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;tns.view_span (inl a,b -&amp;gt; a, 5)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Now the resulting tensor would have dimensions of (10,5) and size of (10,1).&lt;/p&gt;
&lt;p&gt;Since 10 &amp;lt;&amp;gt; 5 * 1 that means that the tensor is not contiguous. This also means that tensors that have been rotated cannot be flattened. Viewing the outermost dimension would be fine though.&lt;/p&gt;
&lt;p&gt;This covers flattening.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        inl map_in = match d with {map_in} -&amp;gt; map_in | _ -&amp;gt; id
        inl map_out = match d with {map_out} -&amp;gt; map_out | _ -&amp;gt; id
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Since &lt;code&gt;map_in&lt;/code&gt; and &lt;code&gt;map_out&lt;/code&gt; are optional, they are replaced with the identity function if they are missing inside the kernel.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        inl in_a :: () = in.dim
        inl span = s in_a
        inl blockDim = lit_min span 1024
        inl gridDim = 1 //lit_min 64 (divup span blockDim)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here the &lt;code&gt;gridDim&lt;/code&gt; is just set to 1 because the author was lazy and decided to leave the job of fiddling with the launch parameters for later.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// The template for lit_min and lit_max.
inl lit_comp op a b =
    if lit_is a &amp;amp;&amp;amp; lit_is b then op a b
    elif lit_is a then a
    elif lit_is b then b
    else error_type &quot;a or b needs to be a literal&quot;

/// Returns the compile time expressible maximum of the two expressions.
inl lit_max = lit_comp max
/// Returns the compile time expressible minimum of the two expressions.
inl lit_min = lit_comp min
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;lit_min&lt;/code&gt; is defined in &lt;code&gt;Core&lt;/code&gt; like this. A standard min function would return the minimum of the two arguments. &lt;code&gt;lit_min&lt;/code&gt; on the other hand does that only between two literals. If one of the arguments is not a literal, then it just returns the literal. If both are not literals then that is a type error.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        inl r = 
            if gridDim = 1 then
                run {map_out map_in blockDim gridDim} in
            else
                run {map_out=id; map_in blockDim gridDim} in
                |&amp;gt; run {map_out map_in=id; blockDim=gridDim; gridDim=1}
        r 0 |&amp;gt; stack
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is where the kernel is launched either once or twice depending on how many reductions are needed and then returned.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        inl run {map_out map_in blockDim gridDim} (!to_dev_tensor in) =
            inl in_a :: () = in.dim
            inl out = w.CudaTensor.create {elem_type=type map_in in.elem_type |&amp;gt; map_out; dim=gridDim}
            inl out' = to_dev_tensor out
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here the &lt;code&gt;run&lt;/code&gt; infers the type and creates the output before passing it into the kernel.&lt;/p&gt;
&lt;p&gt;As can be seen the scaffolding for the actual kernels has a lot of details and needs to be covered a few times.&lt;/p&gt;
&lt;h5&gt;Cuda Loops&lt;/h5&gt;
&lt;pre&gt;
&lt;code&gt;let kernel2 =
    &quot;kernel2&quot;,[cuda_modules],&quot;Does the map_redo_map kernel work?&quot;,
    &quot;&quot;&quot;
inb s = CudaModules (1024*1024)

inl h = HostTensor.init 1024 ((+) 1)
inl a1 = s.CudaTensor.from_host_tensor h

s.CudaKernel.map_redo_map {neutral_elem=0; redo=(+)} a1
|&amp;gt; s.CudaTensor.print // 524800
    &quot;&quot;&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above sums all the numbers between [1..1024].&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;#include &quot;cub/cub.cuh&quot;

extern &quot;C&quot; {
    __global__ void method_21(long long int * var_0, long long int * var_1);
    __device__ char method_22(long long int * var_0, long long int * var_1);
    
    __global__ void method_21(long long int * var_0, long long int * var_1) {
        long long int var_2 = threadIdx.x;
        long long int var_3 = blockIdx.x;
        long long int var_4 = (1024 * var_3);
        long long int var_5 = (var_2 + var_4);
        long long int var_6 = 0;
        long long int var_7[1];
        long long int var_8[1];
        var_7[0] = var_5;
        var_8[0] = var_6;
        while (method_22(var_7, var_8)) {
            long long int var_10 = var_7[0];
            long long int var_11 = var_8[0];
            char var_12 = (var_10 &amp;gt;= 0);
            char var_14;
            if (var_12) {
                var_14 = (var_10 &amp;lt; 1024);
            } else {
                var_14 = 0;
            }
            char var_15 = (var_14 == 0);
            if (var_15) {
                // &quot;Argument out of bounds.&quot;
            } else {
            }
            long long int var_16 = var_0[var_10];
            long long int var_17 = (var_11 + var_16);
            long long int var_18 = (var_10 + 1024);
            var_7[0] = var_18;
            var_8[0] = var_17;
        }
        long long int var_19 = var_7[0];
        long long int var_20 = var_8[0];
        long long int var_21 = cub::BlockReduce&amp;lt;long long int,1024,cub::BLOCK_REDUCE_WARP_REDUCTIONS,1,1&amp;gt;().Sum(var_20);
        long long int var_22 = threadIdx.x;
        char var_23 = (var_22 == 0);
        if (var_23) {
            long long int var_24 = blockIdx.x;
            char var_25 = (var_24 &amp;gt;= 0);
            char var_27;
            if (var_25) {
                var_27 = (var_24 &amp;lt; 1);
            } else {
                var_27 = 0;
            }
            char var_28 = (var_27 == 0);
            if (var_28) {
                // &quot;Argument out of bounds.&quot;
            } else {
            }
            var_1[var_24] = var_21;
        } else {
        }
    }
    __device__ char method_22(long long int * var_0, long long int * var_1) {
        long long int var_2 = var_0[0];
        long long int var_3 = var_1[0];
        return (var_2 &amp;lt; 1024);
    }
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The produced kernel is of course a decent bit longer than 3 lines. The resulting output in whole is 745 LOC, so it can be pasted here due to its size. As this is a good opportunity to do so, Cuda loops will be covered here.&lt;/p&gt;
&lt;p&gt;Originally these kernels used the tail recursive loops covered in an earlier chapter. It seems like a good idea at the time since the Cuda compiler appeared to be able to perform the tail recursive optimization. Nevertheless, as the author has suspected might happen since literally nobody but him would ever try compiling Cuda loops in such a manner, when tail recursive loops are combined with tuple returns, NVCC will outright produce incorrect code that will lead to local write errors. Based on the reply to the bug report he sent, the NVCC does not support tail recursion optimization and presumably the bug won't be fixed.&lt;/p&gt;
&lt;p&gt;Hence just on the Cuda side, the language supports standard imperative loops, more specifically the while loop.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl whilecd {cond state body} =
    inl r = HostTensor.create {
        array_create=array_create_cuda_local 
        elem_type=state 
        dim=()
        }
    r .set state
    /// Note: While must have a join point around it.
    !While((join cond r.get), (r.set &amp;lt;| body r.get))
    r .get
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As can be seen, Spiral's tensors can be used with Cuda local arrays without any changes even in tuple of array form.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl r = HostTensor.create {
        array_create=array_create_cuda_local 
        elem_type=state 
        dim=()
        }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above fragment corresponds to this one in the code...&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        long long int var_7[1];
        long long int var_8[1];
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Going forward.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;r .set state
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Is...&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        var_7[0] = var_5;
        var_8[0] = var_6;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The...&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;!While((join cond r.get), (r.set &amp;lt;| body r.get))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;...is everything in the while loop. Note that since C requires the conditional a single expression the easiest way to achieve that is to stick it in a join point. Compiler tends to split expressions into statements during the code generation phase which would spill out of the conditional unless it was lifted into a function.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;r .get
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Corresponds to after the while loop.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        long long int var_19 = var_7[0];
        long long int var_20 = var_8[0];
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Of the two variables in the tensor, one is a loop counter and the other is the actual state.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl forcd {d with from body} =
    inl finally =
        match d with
        | {finally} -&amp;gt; finally
        | _ -&amp;gt; id

    inl check =
        match d with
        | {near_to} from -&amp;gt; from &amp;lt; near_to 
        | {to} from -&amp;gt; from &amp;lt;= to
        | {down_to} from -&amp;gt; from &amp;gt;= down_to
        | {near_down_to} from -&amp;gt; from &amp;gt; near_down_to
        | _ -&amp;gt; error_type &quot;Only one of `to`,`near_to`,`down_to`,`near_down_to` needs be present.&quot;

    inl by =
        match d with
        | {by} -&amp;gt; by
        | {to | near_to} -&amp;gt; 1
        | {down_to | near_down_to} -&amp;gt; -1

    inl to =
        match d with
        | {(to ^ near_to ^ down_to ^ near_down_to)=to} -&amp;gt; to
        | _ -&amp;gt; error_type &quot;Only one of `to`,`near_to`,`down_to`,`near_down_to` is allowed.&quot;

    inl state = 
        match d with
        | {state} -&amp;gt; state
        | _ -&amp;gt; ()

    inl state = {from state}
    whilecd {
        state
        cond = inl {from state} -&amp;gt; check from
        body = inl {from state} -&amp;gt; {state=body {state i=from}; from=from+by}
        } .state
    |&amp;gt; finally
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Most of this should be familiar from the loops chapter. Making a for loop from a while loop should not be much of a challenge by this point. It is a fairly straightforward extension of the concept.&lt;/p&gt;
&lt;p&gt;From the for loop comes the Cuda specialized &lt;code&gt;grid_for&lt;/code&gt; loop which makes it straightforward to iterate over a tensor in a block-strided fashion.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl grid_for_template {iteration_mode} {blockDim gridDim} axis dim =
    inl from = threadIdx axis + blockDim axis * blockIdx axis - dim.from
    inl by = gridDim axis * blockDim axis
    inl near_to = dim.near_to

    match iteration_mode with
    | .items_per_thread {d with body} -&amp;gt;
        inl span = s dim
        inl items_per_thread = divup span by
        forcd {d with from=0;near_to=items_per_thread; body=inl {state i=item} -&amp;gt;
            inl i = from + by * item
            inl num_valid = span - by * item
            if i &amp;lt; near_to then body {span num_valid item state i} else state
            }
    | .std d -&amp;gt; forcd {d with from by near_to}

inl grid_for_items = grid_for_template {iteration_mode=.items_per_thread}
inl grid_for = grid_for_template {iteration_mode=.std}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;axis&lt;/code&gt; argument can only be &lt;code&gt;.x&lt;/code&gt;, &lt;code&gt;.y&lt;/code&gt; or &lt;code&gt;.z&lt;/code&gt;. In Cuda examples online, they often compulsively write out...&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;int stride = gridDim.x * blockDim.x;
for {int i = threadIdx.x + blockDim.x * blockIdx.x; i &amp;lt; N; i += stride} {...}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Or something to that effect. &lt;code&gt;grid_for&lt;/code&gt; is inteded to abstract that away so as to make the code more concise and reduce the chance of error. &lt;a href=&quot;https://en.wikipedia.org/wiki/Structured_programming&quot; rel=&quot;nofollow&quot;&gt;Structured programming&lt;/a&gt; started the trend of using loops and if statements, but Cuda itself firmly remains stuck in the late 50s in terms of what it offers inside the kernel. &lt;code&gt;grid_for&lt;/code&gt; is a slight, but a notable improvement over using the raw for loop.&lt;/p&gt;
&lt;p&gt;The standard &lt;code&gt;grid_for&lt;/code&gt; that has been shown functions like a normal loop, but the &lt;code&gt;grid_for_items&lt;/code&gt; has a special purpose to it.&lt;/p&gt;
&lt;p&gt;To understand why it is needed here is a quiz. Where are these variables allocated?&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;int a;
int b[2];
int c[var_11];
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are held in registers. This is despite &lt;code&gt;b&lt;/code&gt; being an array. On the other hand since &lt;code&gt;c&lt;/code&gt;'s size is not statically known, it is allocated in global memory or most likely the cache.&lt;/p&gt;
&lt;p&gt;Whether &lt;code&gt;b&lt;/code&gt; is allocated in registers or not also depends on how it is indexed.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;b[0]; // safe
b[1]; // safe
b[var_12]; // better allocate it in global memory after all
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;So the Cuda array's type depends on how it is used and the compiler will not give any indication of what is going on under the hood the programmer.&lt;/p&gt;
&lt;p&gt;Furthermore, one other important thing to keep in mind is that C compilers, and especially Cuda compilers are specialized for optimizing loops. If the loop boundaries are statically known, which they usually are if one is programming in Spiral, the Cuda compiler will strongly attempt to unroll it.&lt;/p&gt;
&lt;p&gt;If it is inspected with &lt;code&gt;print_static&lt;/code&gt; then &lt;code&gt;item&lt;/code&gt; will show up as a runtime variable, but Cuda will know the difference after unrolling and will be able to determine at compile time what the indices into the array are.&lt;/p&gt;
&lt;p&gt;This is important because variables held in registers can be accessed orders of magnitude more quickly than in main memory, and this particular trick is used in reduce + broadcast kernels which are needed to implement softmax error and layer normalization. It is also used in the sampler.&lt;/p&gt;
&lt;h4&gt;d2_replicate_map&lt;/h4&gt;
&lt;p&gt;This function replicates the first 1d tensor so it matches the 2d second. It is used for adding biases to the result of a matrix multiply.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Replicates the 1d `in` and maps it along the outer dimension as determined by `in'`.
met d2_replicate_map' w f in in' out =
    inl to_dev_tensor = w.CudaTensor.to_dev_tensor
    
    inl in, in', out = zip in, zip in', zip out
    inl dim_in :: () = in.dim
    inl dim_in'_a, dim_in'_b = in'.dim

    assert (dim_in = dim_in'_b) &quot;Input's dimension must equal the second input's inner dimension.&quot;
    assert (in'.dim = out.dim) &quot;Second input must have the same dimension as the output.&quot;

    inl blockDimX = min warp_size (s dim_in)
    inl blockDimY = min 32 (s dim_in'_a)
    inl gridDim = min 64 (divup (s dim_in) blockDimX)

    inl in = to_dev_tensor in
    inl in' = to_dev_tensor in'
    inl out = to_dev_tensor out

    w.run {
        gridDim
        blockDim=blockDimX,blockDimY
        kernel = cuda 
            inl grid_for = grid_for {gridDim blockDim}
            grid_for .x dim_in'_b {body=inl {i} -&amp;gt;
                inl in = in i .get
                inl in' j = in' j i
                inl out j = out j i
                grid_for .y dim_in'_a {body=inl {i} -&amp;gt;
                    inl in', out = in' i, out i
                    out.set (f in in'.get out.get)
                    }
                }
        }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The first half of the kernel is the standard fare - zips, assertions for dimension sizes, calls into &lt;code&gt;to_dev_tensor&lt;/code&gt;, and determination of the block and grid dimensions. The actual kernel part is a bit more interesting than last time.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;            inl grid_for = grid_for {gridDim blockDim}
            grid_for .x dim_in'_b {body=inl {i} -&amp;gt;
                inl in = in i .get
                inl in' j = in' j i
                inl out j = out j i
                grid_for .y dim_in'_a {body=inl {i} -&amp;gt;
                    inl in', out = in' i, out i
                    out.set (f in in'.get out.get)
                    }
                }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;On the very first line there is an example of partial application on the loop function. &lt;code&gt;grid_for&lt;/code&gt; is applied with the module holding the block and grid dimensions. Then it is used twice. First it used to iterate over the innermost dimension along the &lt;code&gt;x&lt;/code&gt; axis.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                inl in = in i .get
                inl in' j = in' j i
                inl out j = out j i
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Inside the loop, the 1d tensor is indexed into and bound to &lt;code&gt;in&lt;/code&gt;. But &lt;code&gt;in'&lt;/code&gt; and &lt;code&gt;out&lt;/code&gt; are wrapped around instead which has the effect of implicitly rotating them. It is the same as saying: &quot;apply the innermost dimension with &lt;code&gt;i&lt;/code&gt; later.&quot;&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                grid_for .y dim_in'_a {body=inl {i} -&amp;gt;
                    inl in', out = in' i, out i
                    out.set (f in in'.get out.get)
                    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;And that is what happens in the inner loop. The same &lt;code&gt;in&lt;/code&gt; gets passed into &lt;code&gt;f&lt;/code&gt; &lt;code&gt;dim_in'_a&lt;/code&gt; number of times which has the effect of replicating it. Of course, often in Cuda programming it is not desirable to just replicate a tensor along some dimension as that would be wasteful. Instead the true benefit of having flexible kernel function is that they allow fusion which minimizes the number of intermediaries. As Cuda kernels are memory bound, that has an extreme positive effect on performance.&lt;/p&gt;
&lt;p&gt;Compilers themselves are utterly incapable of performing such optimizations on their own, so the responsibility for it falls on the user.&lt;/p&gt;
&lt;p&gt;That having said, the above kernel is a somewhat trivial example of this as only replicate and a map are fused, but it is possible to go significantly further.&lt;/p&gt;
&lt;p&gt;Here is the auxiliary for it.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl d2_replicate_map w f in in' =
    indiv join 
        inl in = zip in
        inl in' =
            match in' with
            | by : int64 -&amp;gt; 
                inl dim_in :: () = in.dim
                HostTensor.create {elem_type=(); dim=by,dim_in}
            | in' -&amp;gt; zip in'
        inl out = w.CudaTensor.create {elem_type=type f in.elem_type in'.elem_type; dim=in'.dim}
        d2_replicate_map' w (inl a b _ -&amp;gt; f a b) in in' out
        stack out
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;in'&lt;/code&gt; can also be a scalar value to indicate the size of the outermost dimension. Otherwise it is standard fare, it infers the type of the output, allocates it and passes it to the kernel, and strips the output from the mapping function.&lt;/p&gt;
&lt;h5&gt;Example&lt;/h5&gt;
&lt;pre&gt;
&lt;code&gt;let kernel3 =
    &quot;kernel3&quot;,[cuda_modules],&quot;Does the d2_replicate_map kernel work?&quot;,
    &quot;&quot;&quot;
inb s = CudaModules (1024*1024)

inl inner_size = 8
inl outer_size = 8

inl h = HostTensor.init inner_size (const 123)
inl h' = HostTensor.init (outer_size,inner_size) (inl a b -&amp;gt; a,b)
inl a1 = s.CudaTensor.from_host_tensor h
inl a2 = s.CudaTensor.from_host_tensor h'
inl o1 = s.CudaKernel.d2_replicate_map (inl a b -&amp;gt; a, b) a1 a2
Tuple.iter s.CudaTensor.print (a1,a2,o1)
    &quot;&quot;&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;[|123; 123; 123; 123; 123; 123; 123; 123|]

[|
    [|[0, 0]; [0, 1]; [0, 2]; [0, 3]; [0, 4]; [0, 5]; [0, 6]; [0, 7]|]
    [|[1, 0]; [1, 1]; [1, 2]; [1, 3]; [1, 4]; [1, 5]; [1, 6]; [1, 7]|]
    [|[2, 0]; [2, 1]; [2, 2]; [2, 3]; [2, 4]; [2, 5]; [2, 6]; [2, 7]|]
    [|[3, 0]; [3, 1]; [3, 2]; [3, 3]; [3, 4]; [3, 5]; [3, 6]; [3, 7]|]
    [|[4, 0]; [4, 1]; [4, 2]; [4, 3]; [4, 4]; [4, 5]; [4, 6]; [4, 7]|]
    [|[5, 0]; [5, 1]; [5, 2]; [5, 3]; [5, 4]; [5, 5]; [5, 6]; [5, 7]|]
    [|[6, 0]; [6, 1]; [6, 2]; [6, 3]; [6, 4]; [6, 5]; [6, 6]; [6, 7]|]
    [|[7, 0]; [7, 1]; [7, 2]; [7, 3]; [7, 4]; [7, 5]; [7, 6]; [7, 7]|]
|]

[|
    [|[123, [0, 0]]; [123, [0, 1]]; [123, [0, 2]]; [123, [0, 3]]; [123, [0, 4]]; [123, [0, 5]]; [123, [0, 6]]; [123, [0, 7]]|]
    [|[123, [1, 0]]; [123, [1, 1]]; [123, [1, 2]]; [123, [1, 3]]; [123, [1, 4]]; [123, [1, 5]]; [123, [1, 6]]; [123, [1, 7]]|]
    [|[123, [2, 0]]; [123, [2, 1]]; [123, [2, 2]]; [123, [2, 3]]; [123, [2, 4]]; [123, [2, 5]]; [123, [2, 6]]; [123, [2, 7]]|]
    [|[123, [3, 0]]; [123, [3, 1]]; [123, [3, 2]]; [123, [3, 3]]; [123, [3, 4]]; [123, [3, 5]]; [123, [3, 6]]; [123, [3, 7]]|]
    [|[123, [4, 0]]; [123, [4, 1]]; [123, [4, 2]]; [123, [4, 3]]; [123, [4, 4]]; [123, [4, 5]]; [123, [4, 6]]; [123, [4, 7]]|]
    [|[123, [5, 0]]; [123, [5, 1]]; [123, [5, 2]]; [123, [5, 3]]; [123, [5, 4]]; [123, [5, 5]]; [123, [5, 6]]; [123, [5, 7]]|]
    [|[123, [6, 0]]; [123, [6, 1]]; [123, [6, 2]]; [123, [6, 3]]; [123, [6, 4]]; [123, [6, 5]]; [123, [6, 6]]; [123, [6, 7]]|]
    [|[123, [7, 0]]; [123, [7, 1]]; [123, [7, 2]]; [123, [7, 3]]; [123, [7, 4]]; [123, [7, 5]]; [123, [7, 6]]; [123, [7, 7]]|]
|]
&lt;/code&gt;
&lt;/pre&gt;
&lt;h4&gt;mapi_d2_redo_map&lt;/h4&gt;
&lt;p&gt;The inverse of the &lt;code&gt;d2_replicate&lt;/code&gt; kernel. It reduces the outermost dimension of &lt;code&gt;in&lt;/code&gt; - in other words it turns a tensor of dimensions &lt;code&gt;(a,b)&lt;/code&gt; into &lt;code&gt;(b)&lt;/code&gt;. It is equivalent to a transpose and then a reduce over the innermost dimension. An alternative to using this kernel would be to blast the target tensor with atomics in a regular map. That would work for some cases like in backwards step of &lt;code&gt;add_bias&lt;/code&gt; at the cost of making training non-deterministic.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/// Maps the two inputs and then reduces the first's outer dimension.
met mapi_d2_redo_map' w {d with redo neutral_elem} in in' out =
    inl to_dev_tensor = w.CudaTensor.to_dev_tensor
    
    inl in, in', out = zip in, zip in', zip out
    inl dim_in_a, dim_in_b = in.dim
    inl dim_in' :: () = in'.dim

    assert (dim_in' = dim_in_b) &quot;Input's inner dimension must equal the output's dimension.&quot;
    assert (in'.dim = out.dim) &quot;Input and output's dimensions must be equal.&quot;

    inl blockDimX = lit_min warp_size (s dim_in')
    inl blockDimY = lit_min 32 (s dim_in_a)
    inl gridDim = min 64 (divup (s dim_in') blockDimX)

    inl in = to_dev_tensor in
    inl in' = to_dev_tensor in'
    inl out = to_dev_tensor out
    inl map_out = match d with {map_out} -&amp;gt; map_out | _ a _ _ -&amp;gt; a

    w.run {
        gridDim
        blockDim=blockDimX,blockDimY
        kernel = cuda 
            inl grid_for = grid_for {blockDim gridDim}
            grid_for .x dim_in_b {body=inl {i} -&amp;gt;
                inl in j = in j i
                inl in' = in' i .get
                inl out = out i
                inl finally result = out.set (map_out result out.get)

                inl state = 
                    grid_for .y dim_in_a {state=dyn neutral_elem; body=inl {state i=j} -&amp;gt; 
                        inl in = in j
                        match d with
                        | {map_in} -&amp;gt; redo state (map_in in.get in') 
                        | {mapi_in} -&amp;gt; redo state (mapi_in i j in.get in') 
                        | _ -&amp;gt; redo state in.get
                        }
                        
                if blockDim.y &amp;gt; 1 then
                    inl near_to = blockDim.y
                    inl ar = 
                        HostTensor.create {
                            array_create=array_create_cuda_shared
                            elem_type=state
                            dim={from=1; near_to}, blockDim.x
                            }
                        |&amp;gt; inl ar i -&amp;gt; ar i threadIdx.x

                    whilecd {
                        state={near_to state}
                        cond=inl {near_to} -&amp;gt; near_to &amp;gt;= 2
                        body=inl {near_to state} -&amp;gt;
                            inl by = near_to/2 // It might be worth trying `max 1 (near_to/3)`
                            if threadIdx.y &amp;lt; near_to &amp;amp;&amp;amp; threadIdx.y &amp;gt;= by then ar threadIdx.y .set state
                            syncthreads()

                            {
                            near_to=by 
                            state=
                                if threadIdx.y &amp;lt; by then
                                    forcd {from=threadIdx.y+by; by near_to state 
                                        body=inl {state i} -&amp;gt; redo state (ar i .get)
                                        }
                                else
                                    state
                            }
                        }
                    |&amp;gt; inl {state} -&amp;gt; if threadIdx.y = 0 then finally state
                else
                    finally state
            }
        } |&amp;gt; ignore
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Because the Cub does not have a transposed reduction, the author had to implement it on his own.&lt;/p&gt;
&lt;p&gt;In order to understand what the kernel is doing, first it is required to visualize it. Imagine a 96x32 tensor and one block of size 32x32.&lt;/p&gt;
&lt;p&gt;What that block does is covers [0,31]x[0,31] and loads from main memory, does the reduction in place with the neutral element, slides to [32,63]x[0,31] and loads from main memory, does the reduction in place with the previous state, slides to [64,95]x[0,31] and loads from main memory, and does the final inplace reduction.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                inl state = 
                    grid_for .y dim_in_a {state=dyn neutral_elem; body=inl {state i=j} -&amp;gt; 
                        inl in = in j
                        match d with
                        | {map_in} -&amp;gt; redo state (map_in in.get in') 
                        | {mapi_in} -&amp;gt; redo state (mapi_in i j in.get in') 
                        | _ -&amp;gt; redo state in.get
                        }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;What was described above corresponds to this piece of code.&lt;/p&gt;
&lt;p&gt;There are now 32x32 threads in a block, each having their own &lt;code&gt;state&lt;/code&gt; which after blockwise reduction will be reduced to 1x32 and then stored into the output tensor.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                if blockDim.y &amp;gt; 1 then
                    ...
                else
                    finally state
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Obviously if the number of outermost threads is 1 then the tensor is already reduced and can be outputted.&lt;/p&gt;
&lt;p&gt;Otherwise a block wide reduction is performed.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                    inl near_to = blockDim.y
                    inl ar = 
                        HostTensor.create {
                            array_create=array_create_cuda_shared
                            elem_type=state
                            dim={from=1; near_to}, blockDim.x
                            }
                        |&amp;gt; inl ar i -&amp;gt; ar i threadIdx.x
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It starts by defining a shared memory tensor. Immediately after creating it wrapped with a function that applies it with &lt;code&gt;threadIdx.x&lt;/code&gt; along the innermost dimension. This simple technique of functional programming is greatly useful in facilitating reasoning.&lt;/p&gt;
&lt;p&gt;Whereas previously it was needed to reason about a 2d tensor, it now becomes effectively a 1d tensor and that other dimension can be assumed taken care of and left out of mind.&lt;/p&gt;
&lt;p&gt;Describing how to reduce a 1d tensor is not too complicated. Imagine the goal is to reduce a 1d tensor of size 8. At creation the tensor is uninitialized, so it will be described as thus.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;01234567
________
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;To reduce it in shared memory, the right half of the threads would need to write their states into it.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;01234567
____aaaa
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Then a block synchronization is done and the left half of the threads (&lt;code&gt;0123&lt;/code&gt;) reads from the tensor. They then performs the reduction with the &lt;code&gt;state&lt;/code&gt; in their registers.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;0123
____
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;No need to synchronize here. &lt;code&gt;23&lt;/code&gt; store their states into shared memory.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;0123
__bb
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Then they synchronize. After that &lt;code&gt;01&lt;/code&gt; read from &lt;code&gt;23&lt;/code&gt; and perform the reduction with the state currently in their memory.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;01
__
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The process is then repeated again. &lt;code&gt;1&lt;/code&gt; stores...&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;01
_c
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;...a block synchronization happens, and then &lt;code&gt;0&lt;/code&gt; reads it and does the final reduction.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                    whilecd {
                        state={near_to state}
                        cond=inl {near_to} -&amp;gt; near_to &amp;gt;= 2
                        body=inl {near_to state} -&amp;gt;
                            inl by = near_to/2 // It might be worth trying `max 1 (near_to/3)`
                            if threadIdx.y &amp;lt; near_to &amp;amp;&amp;amp; threadIdx.y &amp;gt;= by then ar threadIdx.y .set state
                            syncthreads()

                            {
                            near_to=by 
                            state=
                                if threadIdx.y &amp;lt; by then
                                    forcd {from=threadIdx.y+by; by near_to state 
                                        body=inl {state i} -&amp;gt; redo state (ar i .get)
                                        }
                                else
                                    state
                            }
                        }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;What was described above is what essentially happens in the code above. The action only happens along the &lt;code&gt;y&lt;/code&gt; axis which makes reasoning easy. Tensor as an abstraction and stateful loops provide a definite benefit in terms reasoning. If the above was Cuda C code, then the programmer would be forced to reason about offsets, indices, mutation and various other things.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;|&amp;gt; inl {state} -&amp;gt; if threadIdx.y = 0 then finally state
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;After the reduction is done, the &lt;code&gt;state&lt;/code&gt; which is the final result is shipped of to be outputted to &lt;code&gt;finally&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl finally result = out.set (map_out result out.get)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It was bound to the environment around 40 lines ago.&lt;/p&gt;
&lt;p&gt;Before the explanation is done there also one subtle point that needs to be made about shared memory array sizing.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;HostTensor.create {
    array_create=array_create_cuda_shared
    elem_type=state
    dim={from=1; near_to}, blockDim.x
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Based on the example shown...&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;01
_c
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;0 never gets used so it should be clear why it was chosen that the outer dimension should start from 1. It is a nice way of preserving a pinch of memory.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;01234567
_cbbaaaa
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;But on closer examination the actual access pattern is like this.&lt;/p&gt;
&lt;p&gt;Would it not be possible to reuse more memory so as to require only 4 slots instead of 7?&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;01234567
____aaaa
____bb
____c
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Something like this? It would be possible, but it would increase the overall latency of the scheme because another call to synchronize would be needed between operations. Cuda makes very little guarantees about order of execution, so without an extra synchronize at the end what might happen is the &lt;code&gt;bb&lt;/code&gt;s might be written before &lt;code&gt;aaaa&lt;/code&gt;s finish reading.&lt;/p&gt;
&lt;p&gt;It is easier to just use an extra tad of memory to not have to deal with data races.&lt;/p&gt;
&lt;h5&gt;Example&lt;/h5&gt;
&lt;pre&gt;
&lt;code&gt;let kernel4 =
    &quot;kernel4&quot;,[cuda_modules],&quot;Does the mapi_d2_redo_map' kernel work?&quot;,
    &quot;&quot;&quot;
inb s = CudaModules (1024*1024)

inl inner_size = 10
inl outer_size = 4

inl h = HostTensor.init (outer_size,inner_size) (inl _ x -&amp;gt; x)
inl h' = HostTensor.init inner_size id
inl a1 = s.CudaTensor.from_host_tensor h
inl a2 = s.CudaTensor.from_host_tensor h'
inl o = 
    s.CudaKernel.mapi_d2_redo_map {
        map_in=(+)
        neutral_elem=0; redo=(+)
        } a1 a2
Tuple.iter s.CudaTensor.print (a1,a2,o)
    &quot;&quot;&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;[|
    [|0; 1; 2; 3; 4; 5; 6; 7; 8; 9|]
    [|0; 1; 2; 3; 4; 5; 6; 7; 8; 9|]
    [|0; 1; 2; 3; 4; 5; 6; 7; 8; 9|]
    [|0; 1; 2; 3; 4; 5; 6; 7; 8; 9|]
|]

[|0; 1; 2; 3; 4; 5; 6; 7; 8; 9|]

[|0; 8; 16; 24; 32; 40; 48; 56; 64; 72|]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It should be noted that the reduction kernel is also capable of replicating one of its arguments during the first map phase.&lt;/p&gt;
&lt;h4&gt;mapi_d1_seq_broadcast&lt;/h4&gt;
&lt;p&gt;This kernel does repeat reduction over items in registers along the inner dimension. It seems like a beast at first, but is actually simpler than the previous one since there is no need to understand how reduction works.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;// Repeatedly reduces along the inner dimension and then maps the result of that reductions over the input in the previous step.
met mapi_d1_seq_broadcast' w {d with seq} in out = 
    inl to_dev_tensor = w.CudaTensor.to_dev_tensor
    
    inl in, out = zip in, zip out
    inl dim_in_a, dim_in_b = in.dim
    assert (in.dim = out.dim) &quot;The input and the output dimensions need to be equal&quot;

    inl num_valid = s dim_in_b
    inl items_per_thread, blockDim =
        assert (lit_is num_valid) &quot;The inner dimension of the input to this kernel must be known at compile time.&quot;
        if num_valid &amp;lt;= 1024 then 1, num_valid
        else divup num_valid 256, 256
    inl gridDimY = min 64 (s dim_in_a)

    inl in = to_dev_tensor in
    inl out = to_dev_tensor out
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This kernel will be unique so far in that it actually takes advantage of Spiral's significant ability to do local memory allocation. As the elements of the inner dimension must be known at compile time the kernel with raise a type error if that dimension is not known at compile time.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl items_per_thread, blockDim =
        assert (lit_is num_valid) &quot;The inner dimension of the input to this kernel must be known at compile time.&quot;
        if num_valid &amp;lt;= 1024 then 1, num_valid
        else divup num_valid 256, 256
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As usual, the number 256 is picked off the cuff and probably different problems will have different ideal best values, but for ML purposes this setting should suffice for the time being.&lt;/p&gt;
&lt;p&gt;The actual kernel will be explained piece by piece as it is too much to take in at once.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    w.run {
        blockDim
        gridDim=1,gridDimY
        kernel = cuda 
            inl dims = {blockDim gridDim}
            grid_for dims .y dim_in_a {body=inl {i=j} -&amp;gt;
                inl in, out = in j, out j
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Since kernel is in essence d1 reduction it begins by iterating over the outer dimension.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                /// Creates the tensor of items.
                inl create_items elem_type = HostTensor.create {
                    array_create = array_create_cuda_local
                    layout=.aot
                    elem_type
                    dim=items_per_thread
                    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It is immediately followed by this function declaration. Notice that it is done in array of tuples format which is not the default for Spiral's tensors. The reason for that is that the Cub functions which accept arrays cannot accept Spiral's tensors in their default form.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl inner_loop = grid_for_items dims .x dim_in_b
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Just like &lt;code&gt;create_items&lt;/code&gt; is often used, so is iterating over the inner dimension so the &lt;code&gt;grid_for_items&lt;/code&gt; is partially applied for that purpose.&lt;/p&gt;
&lt;p&gt;The way this kernel is used is that it does the initial &lt;code&gt;map_in&lt;/code&gt; and then iterates over a tuple of &lt;code&gt;{map_in redo map_out}&lt;/code&gt; executing each in turn.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                inl items = 
                    inl map = 
                        match d with
                        | {map_in} i -&amp;gt; map_in (in i .get)
                        | {mapi_in} i -&amp;gt; mapi_in j i (in i .get)
                        | _ i -&amp;gt; in i .get
                    inl items = create_items (type map (dyn 0))
                    inner_loop {body=inl {item i} -&amp;gt; items item .set (map i)}
                    items
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Having to match on whether the mapping function is &lt;code&gt;map_in&lt;/code&gt; or &lt;code&gt;mapi_in&lt;/code&gt; or not there at all adds a lot of noise for the reader.&lt;/p&gt;
&lt;p&gt;But what it does is create a tensor in register memory, iterates over the input here and sets the tensor to the result of mapping the input over the function. It is a map operation just as the name implies.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;items&lt;/code&gt; at the end is the tensor that has gone through the first phase the kernel.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                inl rec seq_loop items (d :: d') =
                    match d with
                    | {map_in} -&amp;gt; 
                        inl items' = create_items (type map_in items.elem_type)
                        inner_loop {body=inl {item} -&amp;gt; items item .get |&amp;gt; map_in |&amp;gt; items' item .set}
                        items'.bodies.ar
                    | {mapi_in} -&amp;gt;
                        inl items' = create_items (type mapi_in j i items.elem_type)
                        inner_loop {body=inl {item i} -&amp;gt; items item .get |&amp;gt; mapi_in j i |&amp;gt; items' item .set}
                        items'.bodies.ar
                    | _ -&amp;gt; items.bodies.ar
                    |&amp;gt; inl x -&amp;gt; 
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is the function that iterates through the sequence. First, if the &lt;code&gt;map_in&lt;/code&gt; or &lt;code&gt;mapi_in&lt;/code&gt; are present they get are applied to every element in the items tensor.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                        inl block_reduce redo = 
                            inl d = {blockDim redo}
                            if num_valid % blockDim.x = 0 then cub_block_reduce d
                            else cub_block_reduce {d with num_valid} 
                        match d with
                        | {redo} -&amp;gt; block_reduce redo x |&amp;gt; broadcast_zero
                        | {redo'} -&amp;gt; block_reduce redo' x
                    |&amp;gt; inl x -&amp;gt;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Then comes reduction. If the number of items does not exactly align with the block size, then a guarded reduction method is selected to make sure it does not read past the boundary. Also after the reduction is done there are two choices:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;&lt;code&gt;redo&lt;/code&gt; does the reduction and the broadcasts the element to every thread in the block using shared memory.&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code&gt;inl broadcast_zero x =
    inl ar = array_create_cuda_shared x 1
    if threadIdx.x = 0 then ar 0 &amp;lt;- x
    syncthreads()
    ar 0
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The first thread which holds the reduced elements first writes to a shared memory location and then all the elements read from it after synchronizing.&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;&lt;code&gt;redo'&lt;/code&gt; functions much like the above, but without the broadcast.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;After that comes the final step. There are also two branching paths at this point.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                        match d' with
                        | () -&amp;gt; 
                            inner_loop {body=inl {item i} -&amp;gt;
                                inl out = out i
                                match d with
                                | {map_out} -&amp;gt; map_out (items item .get) x (out .get)
                                | {mapi_out} -&amp;gt; mapi_out j i (items item .get) x (out .get)
                                |&amp;gt; out .set
                                }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;If the current element in the sequence is last, then the result of applying &lt;code&gt;map_out&lt;/code&gt; or &lt;code&gt;mapi_out&lt;/code&gt; to each element of &lt;code&gt;items&lt;/code&gt; is feed directly into the output tensor.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                        | _ -&amp;gt;
                            match d with
                            | {map_out} -&amp;gt; 
                                inl items' = create_items type map_out items.elem_type x
                                inner_loop {body=inl {item} -&amp;gt; map_out (items item .get) x |&amp;gt; items' item .set}
                                seq_loop items' d'
                            | {mapi_out} -&amp;gt; 
                                inl items' = create_items type mapi_out (dyn 0) (dyn 0) items.elem_type x
                                inner_loop {body=inl {i item} -&amp;gt; mapi_out j i (items item .get) x |&amp;gt; items' item .set}
                                seq_loop items' d'
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Otherwise, it needs to create a new intermediate tensor and then sets the result of applying &lt;code&gt;map_out&lt;/code&gt; or &lt;code&gt;mapi_out&lt;/code&gt; to each element of the &lt;code&gt;items&lt;/code&gt; tensor to it. Then it passes that as input to the next operation in the sequence.&lt;/p&gt;
&lt;h5&gt;The Softmax Activation&lt;/h5&gt;
&lt;p&gt;The softmax activation will be covered here and the more complex layer normalization activation will be covered in the following chapter.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl softmax x = exp x / replicate (sum (exp x))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above is softmax in pseudo-code. &lt;code&gt;x&lt;/code&gt; is a vector type.&lt;/p&gt;
&lt;p&gt;Alternatively, it would be better to start with this...&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl softmax x = 
    inl z = exp x
    z / replicate (sum z)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;sum&lt;/code&gt; sums up all the values of a vector into a scalar, and replicate unfolds the scalar into a vector each holding the same values.&lt;/p&gt;
&lt;p&gt;Furthermore, for numerical stability the following trick is often employed in softmax.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl softmax x = 
    inl max_x = replicate (max x)
    inl z = exp (x - max_x)
    z / replicate (sum z)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This has no effect on the way gradients are propagated during the backwards step. Derivative of &lt;code&gt;x - max_x&lt;/code&gt; with respect to &lt;code&gt;x&lt;/code&gt; is just 1. &lt;code&gt;max_x&lt;/code&gt; is considered to be a constant. Shifting the input to softmax by a constant does not affect the result during stable regimes, but if the inputs are unstable then they will trend towards 0 rather that towards infinity.&lt;/p&gt;
&lt;p&gt;Going forward, it is important to keep in mind that &lt;code&gt;max&lt;/code&gt; and &lt;code&gt;sum&lt;/code&gt; are reduce operations, and &lt;code&gt;exp&lt;/code&gt; is a map operation. The kernel described in this chapter makes it possible to succinctly implement both softmax's forward and backward phases.&lt;/p&gt;
&lt;p&gt;Here is the forward phase.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl z =
    s.CudaKernel.mapi_d1_seq_broadcast {
        seq = 
            {
            redo=max // max x
            map_out=inl x replicate_max_x -&amp;gt; exp (x - replicate_max_x)
            }
            ,
            {
            redo=(+) // sum z
            map_out=inl z replicate_sum_z -&amp;gt; z / replicate_sum_z
            }
        } (primal x)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The second argument in &lt;code&gt;map_out&lt;/code&gt; is implicitly replicated. That is the softmax forward.&lt;/p&gt;
&lt;p&gt;Assuming the output is present the backward step can be done in one step.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl softmax x = exp x / replicate (sum (exp x))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;For ease of rewriting the sum will be factored out...&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl softmax x = 
    inl s = replicate (sum (exp x))
    exp x / s
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;There are &lt;a href=&quot;https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/&quot; rel=&quot;nofollow&quot;&gt;tutorials&lt;/a&gt; that show how to take the derivative analytically, but for show it will be done here by emulating how automatic differentiation would take the trace through the above. The analysis will have the benefit of not needing to branch on the value of the index used.&lt;/p&gt;
&lt;p&gt;Here are a few simple rules.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;o = a / b
da += do / b
db += -do * da / (b * b)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is the AD rule for propagating adjoints (gradients) through division.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;o = replicate x
dx += sum do
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is the rule for propagating gradients through &lt;code&gt;replicate&lt;/code&gt;. It is a sumation.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;o = sum x
dx += replicate do
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The rule for propagating gradients through summation is the opposite of the previous one.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;o = exp x
dx += do * exp x
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;With this last rule, we are all set to trace the derivative of the softmax with respect to its input.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl v = exp x
inl s = sum v
inl r = replicate s
inl z = v / r
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is just the softmax split into individual pieces similarly to how the compiler would do let insertion.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl z = v / r
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Starting with this, the gradients are propagated through &lt;code&gt;v&lt;/code&gt; and &lt;code&gt;r&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;// inl z = v / r
dv += dz / r
dr += -dz * v / (r * r)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;AD is sensitive to the ordering of operations. The backwards pass needs to be done exactly in the opposite way of forward pass. This is why a tape is frequently used for it.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;//inl r = replicate s
ds += sum dr
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;...&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;//inl s = sum v
dv += replicate ds
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;...&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;// inl v = exp x
dx += dv * v
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here are all the steps on the same page.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;dv += dz / r
dr += -dz * v / (r * r)
ds += sum dr
dv += replicate ds
dx += dv * v
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;A naive AD implementation would do the above in 5 different steps. The goal of making Spiral was to allow fusion of the above steps into a single one. To start things off &lt;code&gt;dr&lt;/code&gt; will be folded into &lt;code&gt;ds&lt;/code&gt; which will then be folded into &lt;code&gt;dv&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;dv += dz / r
dv += replicate (sum (-dz * v / (r * r)))
dx += dv * v
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Both of &lt;code&gt;dv&lt;/code&gt;s can now be folded into &lt;code&gt;dx&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;dx += (dz / r + replicate (sum (-dz * v / (r * r)))) * v
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Since &lt;code&gt;z = v / r&lt;/code&gt; the above can be simplified further.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;dx += (dz / r + replicate (sum (-dz * z / r))) * v
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;A valid operation here would be &lt;code&gt;sum (a * replicate b) = sum a / b&lt;/code&gt;. &lt;code&gt;r&lt;/code&gt; is &lt;code&gt;replicate s&lt;/code&gt;. It is possible to take it out a level.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;dx += (dz / r + replicate (sum (-dz * z) / s)) * v
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;A valid operation here would be &lt;code&gt;replicate (a * b) = replicate a * replicate b&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;dx += (dz / r + replicate (sum (-dz * z)) / replicate s) * v
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;r = replicate s&lt;/code&gt; as per original definition.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;dx += (dz / r + replicate (sum (-dz * z)) / r) * v
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above can be simplified using the distributive law.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;dx += (dz + replicate (sum (-dz * z))) * v / r
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Since &lt;code&gt;z = v / r&lt;/code&gt; the above can be simplified further.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;dx += (dz + replicate (sum (-dz * z))) * z
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Since the &lt;code&gt;-&lt;/code&gt; is a constant it is possible to factor it out.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;dx += (dz - replicate (sum (dz * z))) * z
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In this simplified form, all the steps can easily be implemented in a single kernel. &lt;code&gt;er&lt;/code&gt; is &lt;code&gt;replicate (sum (dz * z))&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;s.CudaKernel.mapi_d1_seq_broadcast' {
    seq = 
        {
        map_in=inl z,dz -&amp;gt; z*dz
        redo=(+)
        map_out=inl (z,dz) er dx -&amp;gt; dx + (dz - er) * z
        }
    } (primal z, adjoint z) (adjoint x)
&lt;/code&gt;
&lt;/pre&gt;
&lt;h3&gt;9: Deep Learning Basics&lt;/h3&gt;
&lt;p&gt;At the time of writing this it is almost April 2018.&lt;/p&gt;
&lt;p&gt;Currently, the two dominant deep learning frameworks are without exception Tensorflow by Google and PyTorch by Facebook. Both are written in C++ and expose a Python front end from which the users access it. It is by no means unusual that big corporate sponsored frameworks would be widely used, but it is interesting to look at the deep learning ecosystems in other languages, or specifically the lack of such ecosystems. What is precisely interesting is that they are non-existent and most of the attempts at making frameworks have petered out.&lt;/p&gt;
&lt;p&gt;In a broader sense, what ML capabilities non-Python ecosystems have tend to be on the shallower end of learning which does not need the extreme GPU capabilities Spiral has.&lt;/p&gt;
&lt;p&gt;Deep learning has been popular for half a decade at least now, and had it been possible to make a great DL library in say F# (.NET), Scala (JVM), Haskell or Racket, then one can be sure that this would already have happened. The reason for that is that languages without first class staging have very deep flaws that preclude them from having effective Cuda backends and GPU abstractions. There is very little languages constructed in the classical manner whether they be static or dynamic offer in the making of a deep learning library.&lt;/p&gt;
&lt;p&gt;Statically typed languages have type systems that are simply too weak to be useful and dynamically typed languages are simply too inefficient. And Lisp macros are not a substitute for first class staged functions and join points.&lt;/p&gt;
&lt;p&gt;Hence it is easy to predict that great libraries will not get made in any of the aforementioned languages nor in any of the mainstream languages that aren't C++. The effort is simply too great and too strenuous in them. At best, .NET for example might get bindings for Tensorflow or CNTK if it has not already.&lt;/p&gt;
&lt;p&gt;And though Tensorflow, CNTK and PyTorch are written in C++ that is hardly a beaming recommendation for the language. C++ is widely known enough that its quality as a language can stand for itself.&lt;/p&gt;
&lt;p&gt;Making a machine learning library is hard enough to crush most langauges, but Spiral can make it a lot easier and the hows of that is what will be covered in this chapter. That having said, this is a language tutorial and not a deep learning tutorial so what won't be covered is to how to actual use deep learning to do interesting things. Instead it will be about library construction.&lt;/p&gt;
&lt;h4&gt;Primitives&lt;/h4&gt;
&lt;p&gt;At the time of writing the Spiral &lt;code&gt;Learning&lt;/code&gt; library is quite small, but it has some advanced capabilities so this is the ideal point to introduce it before it gets any bigger. The one limitation it has on it is that it is restricted to first order &lt;a href=&quot;https://en.wikipedia.org/wiki/Automatic_differentiation&quot; rel=&quot;nofollow&quot;&gt;automatic differentiation&lt;/a&gt;. It would be possible to build a library that supports higher order AD in Spiral, but decision to support higher order AD is not a trivial one to make.&lt;/p&gt;
&lt;p&gt;It would add very non-negligible maintenance burden on it, and it possibly make it difficult to provide effective optimization. Higher order AD does not have a particularly good track record on practical use in deep learning anyway.&lt;/p&gt;
&lt;p&gt;For those not familiar with it, higher order AD would essentially allow adaptive learning methods and possibly metalearning methods like MAML, but while first order AD takes only a single pass per datapoint, higher order AD would require &lt;code&gt;n&lt;/code&gt; passes per datapoint where the &lt;code&gt;n&lt;/code&gt; is the number of parameters in the network. And approximated higher order methods require the whole dataset to be operated at once and do not allow minibatching.&lt;/p&gt;
&lt;p&gt;Spiral's ML library is intended to be of great practical use. It needs to be simple to extend and understand. It needs to be both highly flexible and performant.&lt;/p&gt;
&lt;p&gt;So the only dual number it uses internally is of the primal and adjoint variety.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl float -&amp;gt;
    // #Primitives
    inl zero = to float 0
    inl one = to float 1
    inl two = to float 2
    inl infinity =
        match float with
        | _: float32 -&amp;gt; infinityf32
        | _: float64 -&amp;gt; infinityf64

    inl primal = function {primal} | primal -&amp;gt; primal
    inl adjoint = function {adjoint} -&amp;gt; adjoint | _ -&amp;gt; ()

    inl primals = Struct.map primal
    inl adjoints = Struct.map adjoint

    inl on_non_nil B ret =
        match B with
        | () -&amp;gt; ()
        | B -&amp;gt; ret B

    inl dr s primal = {primal adjoint=s.CudaTensor.zero_like primal; block=()}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The library takes the type of the floating point number it is operating on as its first argument. This is generally used to convert literals to &lt;code&gt;float32&lt;/code&gt; values at compile time and such since Spiral does not allow implicit conversions. &lt;code&gt;zero&lt;/code&gt;, &lt;code&gt;one&lt;/code&gt;, &lt;code&gt;two&lt;/code&gt; and &lt;code&gt;infinity&lt;/code&gt; are just some constants set up for convenience.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Struct.map&lt;/code&gt; is just the &lt;code&gt;toa_map&lt;/code&gt; from the tensor chapter. Since functions that iterate over arbitrary types are so often used in Spiral, they have their own &lt;code&gt;Struct&lt;/code&gt; module now.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;dr&lt;/code&gt; takes in context and the primal and creates a dual with the adjoint set to zero.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;on_non_nil&lt;/code&gt; is just a utility function.&lt;/p&gt;
&lt;h5&gt;map&lt;/h5&gt;
&lt;p&gt;Map is useful for activation functions and pointwise operations like addition and Hadamarad multiplication.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl choose_adjoints in bck =
        Struct.choose2 (function
            | {primal adjoint} bck -&amp;gt; {adjoint bck block=()}
            | _ _ -&amp;gt; ()) in bck
        |&amp;gt; inl x -&amp;gt; Struct.map (inl x -&amp;gt; x.adjoint) x, Struct.map (inl x -&amp;gt; x.bck) x
            
    inl map {fwd bck} in s =
        inl primal = primals in
        inl out = s.CudaKernel.map fwd primal |&amp;gt; dr s

        inl adjoint, bck = choose_adjoints in bck
        out, inl _ -&amp;gt; join
            inl bck (in, out) = Struct.map2 (inl bck -&amp;gt; bck (in, out)) bck
            s.CudaKernel.map' bck (primal, {out without block}) adjoint
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In order to be flexible, while &lt;code&gt;fwd&lt;/code&gt; is always a single function, &lt;code&gt;bck&lt;/code&gt; is intended to be the same as the number of inputs. The reason for that is that not all variables will be dual numbers and hence the gradients won't be propagated to them. Example of such variables would the inputs and the labels for the cost function.&lt;/p&gt;
&lt;p&gt;Here is an example of how the &lt;code&gt;map&lt;/code&gt; primitive can be used to implement activations.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl activation d = map {d with bck = Struct.map (inl bck (in, out) adjoint -&amp;gt; adjoint + out.adjoint * (self in out.primal)) self}

    inl sigmoid_fwd x = one / (one + exp -x)
    inl sigmoid_bck out = out * (one - out)

    inl sigmoid = activation {
        fwd = sigmoid_fwd
        bck = inl _ -&amp;gt; sigmoid_bck
        }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;activation&lt;/code&gt; is there to make sure that in the backward step the adjoint is added to and that the adjoint of &lt;code&gt;out&lt;/code&gt; multiplies whatever flows out from the function.&lt;/p&gt;
&lt;p&gt;The above could also be implemented like.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl sigmoid = map {
    fwd = inl x -&amp;gt; one / (one + exp -x)
    bck = inl (in,out) adjoint -&amp;gt; adjoint + out.adjoint * (out.primal * (one - out.primal))
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Though pretty much always, it is instead preferable to abstract what is possible in order to compress the code size, increase readability and minimize the chance of error. In this case this piece of advice only refers to the &lt;code&gt;activation&lt;/code&gt; function. &lt;code&gt;sigmoid_fwd&lt;/code&gt; and &lt;code&gt;sigmoid_bck&lt;/code&gt; have been factored out due to being used in multiple places.&lt;/p&gt;
&lt;p&gt;The following piece deserves a mention as it is the way AD is done.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        out, inl _ -&amp;gt; 
            inl bck (in, out) = Struct.map2 (inl bck -&amp;gt; bck (in, out)) bck
            s.CudaKernel.map' bck (primal, {out without block}) adjoint
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;All the primitive functions return the output and the function to run in order to trigger the backward step. At the time the output is made, the adjoint of it is always zero, but after the forward pass is done the adjoint at the top of tape is set to 1 and propagated downwards and eventually through that output.&lt;/p&gt;
&lt;p&gt;As a short tutorial, remember the various rules used to take the derivatives of softmax. Here are the rules for division once again. It is the same for the matrix and the scalar case if one assumes the operators are overloaded.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;c = a / b
da += dc / b
db += -dc * a / (b * b)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is how this could be implemented in code in Spiral.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl div a b =
    inl primal = primal a * primal b
    inl adjoint = ref zero
    {primal adjoint block=()}, inl _ -&amp;gt;
        adjoint a := adjoint a + adjoint / primal b
        adjoint b := adjoint b - adjoint * primal a / (primal b * primal b)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above assumes that &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are scalars rather than tensors. Regardless, the structure is quite similar to the &lt;code&gt;map&lt;/code&gt; which is intended to be a generic interface for such functions.&lt;/p&gt;
&lt;p&gt;Being able to support aggressive inlining, Spiral is well suited for making AD libraries. That having said, this particular aspect of it - the ability to always inline the backward step is not particularly important for deep learning. Deep learning deals with large batched operations - a single matrix multiply is already on the factor of thousands scalar operations and what would take extreme optimization heroics by the compiler is generally put in by hand instead for such batched operations.&lt;/p&gt;
&lt;p&gt;What the &lt;code&gt;map&lt;/code&gt; really offers is easy fusion of map operations.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl hadmult = activation {
        fwd = inl a,b -&amp;gt; a*b
        bck = (inl (_,x) _ -&amp;gt; x), (inl (x,_) _ -&amp;gt; x)
        }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is the standard Hadamarad multiplication. Should an operation like &lt;code&gt;a*b + c*d&lt;/code&gt; be needed then it would be trivial to make a fresh activation.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl hadmult2 = activation {
        fwd = inl a,b,c,d -&amp;gt; a*b + c*d
        bck = 
            inl (_,b,_,_) _ -&amp;gt; b
            ,inl (a,_,_,_) _ -&amp;gt; a
            ,inl (_,_,_,d) _ -&amp;gt; d
            ,inl (_,_,c,_) _ -&amp;gt; c
        }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Admittedly, this example a bit unergonomic, but it is a vast improvement over having to write a separate function in Cuda for this sort of thing. It would not be difficult to make a more generic Hadamarad multiplication that looks something like this.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl hadmult' = activation' {
        fwd = Tuple.map (inl (a,b) -&amp;gt; a*b) &amp;gt;&amp;gt; Tuple.foldl (+) zero
        bck = inl x _ -&amp;gt; Tuple.map (inl (a,b) -&amp;gt; (b,a)) x
        }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The exercise for this will be left to the reader.&lt;/p&gt;
&lt;p&gt;Being able to do such generic operations so directly is wonderful. In one sweep it is possible to cover an infinite space of possible map operations. And somewhat paradoxically, it is easier to prove generic code correct than it is specific instances of it. With this capability there is no need to worry whether one of the numerous float pointers are passed in the right order, dumb copy paste errors or array boundary violations which Cuda will not check for.&lt;/p&gt;
&lt;p&gt;Spiral's ML library is intended to cover the middle ground between what mainstream frameworks offer and what naive AD implementations offer.&lt;/p&gt;
&lt;h5&gt;map_redo_map&lt;/h5&gt;
&lt;p&gt;This one is used for the cost functions.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    /// Does not return a `dr` unlike the rest. This is an optimization in order to avoid having to call too many useless kernels that 
    /// just to set the adjoint to 1. The current library is intended for a narrow purpose.
    inl map_redo_map {fwd bck} in s =
        inl primal = primals in
        inl out = s.CudaKernel.map_redo_map fwd primal

        inl adjoint, bck = choose_adjoints in bck
        out, inl _ -&amp;gt; join
            inl out = s.CudaTensor.to_dev_tensor out
            inl bck in = Struct.map2 (inl bck -&amp;gt; bck (in, out.get)) bck
            s.CudaKernel.map' bck primal adjoint
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;If the previous section was understood, then this one should not give any trouble. Here are the examples of two cost functions done using it and the generic template for them.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl error {fwd bck} label input s = 
        inl batch_size = primal input .span_outer |&amp;gt; to float
        inl div_by_minibatch_size x = x / batch_size
        map_redo_map {
            fwd = {
                map_in = fwd
                redo = (+)
                neutral_elem = zero
                map_out = div_by_minibatch_size
                }
            /// The adjoint in error is always assumed to be one.
            bck = Struct.map (inl bck (in, out) adjoint -&amp;gt; adjoint + div_by_minibatch_size (bck in out)) bck
            } (input, label) s

    inl square = error {
        fwd = inl (x,y) -&amp;gt; (y - x) * (y - x)
        bck = 
            inl (x,y) _ -&amp;gt; two * (x - y)
            ,inl (x,y) _ -&amp;gt; -(two * (x - y))
        }

    inl cross_entropy = error {
        fwd = inl x, y -&amp;gt; -(y * log x + (one - y) * log (one - x))
        bck = 
            inl (x, y) _ -&amp;gt; (x - y) / (x * (one - x))
            ,inl (x, y) _ -&amp;gt; log (one - x) - log x
        }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;One design decision that needs to be highlighted is that the adjoint for the cost function is not seeded with one. Instead it is implicitly assumed to be that in the backward step. This is so recurrent nets do not have to make numerous redundant calls just to set the cost to one. One thing the &lt;code&gt;error&lt;/code&gt; does as a service is automatically divide by the batch size.&lt;/p&gt;
&lt;p&gt;Apart from the two above, the library also has the softmax cross entropy which has a more elaborate implementation due to how its backward step works so it won't be shown here. As a matter of fast, that particular implementation is not fully fused yet and that bit of work will be left for the future.&lt;/p&gt;
&lt;p&gt;There is one more primitive to be covered before the tutorial can proceed to the next step.&lt;/p&gt;
&lt;h5&gt;d2_replicate_map&lt;/h5&gt;
&lt;pre&gt;
&lt;code&gt;    inl d2_replicate_map {fwd bck={bck_in bck_in'}} in in' s =
        inl primal, adjoint = primals in, adjoints in
        inl primal', adjoint' = primals in', adjoints in'
        inl out = s.CudaKernel.d2_replicate_map fwd primal primal' |&amp;gt; dr s
        out, inl _ -&amp;gt; join
            inl out = {out without block}
            s.CudaKernel.mapi_d2_redo_map' bck_in (primal', out) primal adjoint
            s.CudaKernel.d2_replicate_map' bck_in' primal (primal', out) adjoint'
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is is used for adding bias terms. It can also be used to implement multiplicative integration in a very easy manner.&lt;/p&gt;
&lt;p&gt;In order to actually be suitable for implementing activations of that sort, a more focused helper method is derived from it.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl d2_replicate_activation {fwd bck_in bck_in'} in =
        inl neutral_elem = Struct.map (const zero) in
        inl bck = {
            bck_in={
                map_in=inl (in', out) in -&amp;gt; Struct.map ((*) out.adjoint) (bck_in in in' out.primal)
                neutral_elem redo=Struct.map2 (+)
                map_out=Struct.map2 (+)
                }
            bck_in'=inl in (in', out) -&amp;gt; Struct.map2 (inl x adjoint -&amp;gt; adjoint + out.adjoint*x) (bck_in' in in' out.primal)
            }
        d2_replicate_map { fwd bck } in
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This one does the important part of multiplying the backwards returns by the output adjoint and adds to the input adjoint. It is similar to the standard &lt;code&gt;activation&lt;/code&gt; shown in the previous section.&lt;/p&gt;
&lt;p&gt;An example of it in use will be shown in the multiplicative integration section.&lt;/p&gt;
&lt;h4&gt;Optimizers&lt;/h4&gt;
&lt;p&gt;Only these two are here for the time being.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl sgd learning_rate s {primal adjoint} = 
        s.CudaKernel.map' (inl _ P, A -&amp;gt; P - learning_rate * A, zero) primal.empty (primal, adjoint)

    inl clipped_sgd max learning_rate s {primal adjoint} = 
        s.CudaKernel.map' (inl _ P, A -&amp;gt; 
            inl A = if A &amp;lt; -max then -max elif A &amp;gt; max then max else A
            P - learning_rate * A, zero
            ) primal.empty (primal, adjoint)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It is possible to do a frightening amount of stuff for deep learning with just a simple map. It covers all of the first order optimizers for one. At any rate, the implementation of optimizers is trivialized by the generic map and extending the library with Adam and RMSProp or whatever else would not be a problem.&lt;/p&gt;
&lt;p&gt;One thing that the library does not do, but should is to fuse all the map calls into a single one during the optimization phase. That would not be hard in Spiral and will be subject of future work.&lt;/p&gt;
&lt;h4&gt;Operations&lt;/h4&gt;
&lt;pre&gt;
&lt;code&gt;    inl apply_bck bck bck' _ = bck'(); bck()

    inl (&amp;gt;&amp;gt;=) a b s =
        inl a,a_bck = a s
        inl b,b_bck = b a s
        b, apply_bck a_bck b_bck

    inl succ x _ = x, const ()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;These are used to combine AD operations in a monadic manner. &lt;code&gt;&amp;gt;&amp;gt;=&lt;/code&gt; is just a specific instance of a writer monad here.&lt;/p&gt;
&lt;p&gt;In essence, it relives the burden of having to deal with backwards step functions explicitly in a pure functional fashion. Remember that &lt;code&gt;inm a = b&lt;/code&gt; is just syntax sugar for &lt;code&gt;b &amp;gt;&amp;gt;= inl a -&amp;gt; ...&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                    inm f = matmultb ((i,input.f),(h,state.f)) bias.f &amp;gt;&amp;gt;= sigmoid
                    inm i' = matmultb ((i,input.i),(h,state.i)) bias.i &amp;gt;&amp;gt;= sigmoid
                    inm o = matmultb ((i,input.o),(h,state.o)) bias.o &amp;gt;&amp;gt;= sigmoid
                    inm c' = matmultb ((i,input.c),(h,state.c)) bias.c &amp;gt;&amp;gt;= tanh
                    inm c =
                        inm x1 = hadmult (f,c)
                        inm x2 = hadmult (i',c')
                        add (x1, x2)
                    inm h' = tanh c
                    inm h = hadmult (o, h')
                    succ (h, (h, c))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is how the LSTM is implemented for example. Monadic computation saves a decent bit of boilerplate here. That having said, this is an example of a heavily unoptimized implementation of a LSTM. An optimized example would have only one or two steps rather than...21.&lt;/p&gt;
&lt;h4&gt;Layers&lt;/h4&gt;
&lt;h5&gt;Multiplicative Integration RNN&lt;/h5&gt;
&lt;p&gt;Finally in this section it becomes possible to give an example of the &lt;code&gt;d2_replicate&lt;/code&gt; activation. This RNN variant is from the paper &lt;a href=&quot;https://arxiv.org/abs/1606.06630&quot; rel=&quot;nofollow&quot;&gt;On Multiplicative Integration with Recurrent Neural Networks&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In 2016 the author was impressed by the novel notion of replacing addition with Hadamarad multiplication and decided there that this is something he wanted to use since getting a good improvement for such a simple change seemed like a worthwhile tradeoff. It was not until just recently however that he managed to implement it properly. It was a long and arduous journey to get it into this form.&lt;/p&gt;
&lt;p&gt;A short summary - the standard activation for a vanilla RNN is &lt;code&gt;f(Wx + Uh + b)&lt;/code&gt;. Multiplicative integration replaces that &lt;code&gt;+&lt;/code&gt; with &lt;code&gt;f(Wx .* Uh + b)&lt;/code&gt;. It is possible to generalize that further by adding a bunch of bias terms so it becomes &lt;code&gt;f((Wx + b1) .* (Uh + b2) + b)&lt;/code&gt;. This simplifies to &lt;code&gt;f(b1 .* Wx .* Uh + b2 .* Uh + b3 .* Wx + b4)&lt;/code&gt;. The sheer amount of terms to be added and multiplied makes it challenging to write a custom Cuda kernel for. The backward step would double the number of variables to 12 as well due needing to pass adjoints. Not to mention, the biases need to be replicated as well.&lt;/p&gt;
&lt;p&gt;The sheer effort that would be needed to implement what would otherwise be a trivial use of &lt;code&gt;map&lt;/code&gt; is one of the main catalysts for the author abandoning the old F# library and creating Spiral.&lt;/p&gt;
&lt;p&gt;So here it is presented, the multiplicative integration RNN in Spiral.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    /// The multiplicative integration RNN.
    inl mi size sublayer = 
        recurrent 
            {
            size sublayer
            weights = inl s -&amp;gt;
                open Initializer
                {
                input = tanh (sublayer.size, size) s |&amp;gt; dr s
                state = tanh (size, size) s |&amp;gt; dr s
                b1 = bias s size one
                b2 = bias s size (to float 0.5)
                b3 = bias s size (to float 0.5)
                b4 = bias0 s size
                } |&amp;gt; heap

            apply = inl {b1 b2 b3 b4 input state} s i -&amp;gt;
                match s with
                | () -&amp;gt;
                    inm i = matmult (i, input)
                    d2_replicate_activation {
                        fwd=inl (b3,b4) i -&amp;gt; b3*i + b4 |&amp;gt; tanh_fwd
                        bck_in=inl (b3,b4) i out -&amp;gt; (i, one) |&amp;gt; Tuple.map ((*) (tanh_bck out))
                        bck_in'=inl (b3,b4) i out -&amp;gt; b3 * tanh_bck out
                        } (b3,b4) i
                | _ -&amp;gt;
                    inm i = matmult (i, input)
                    inm s = matmult (s, state)
                    d2_replicate_activation {
                        fwd=inl (b1,b2,b3,b4) (i,s) -&amp;gt; b1*i*s + b2*s + b3*i + b4 |&amp;gt; tanh_fwd
                        bck_in=inl (b1,b2,b3,b4) (i,s) out -&amp;gt; (i*s, s, i, one) |&amp;gt; Tuple.map ((*) (tanh_bck out))
                        bck_in'=inl (b1,b2,b3,b4) (i,s) out -&amp;gt; (b1*s+b3, b1*i+b2) |&amp;gt; Tuple.map ((*) (tanh_bck out))
                        } (b1,b2,b3,b4) (i,s)
                &amp;gt;&amp;gt;= inl x -&amp;gt; succ (x,x)
            }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;tanh&lt;/code&gt; activation is fused into the linear part. This did in fact make the network run a decent bit faster. It takes 4.6s per epoch on the &lt;code&gt;mini_shakespeare.txt&lt;/code&gt; dataset. In contrast the unoptimized LSTM takes over 18s. It also trains a decent bit better than it, in terms of training error.&lt;/p&gt;
&lt;p&gt;It is quite nice, when presented in this form. The matrix multiplications are still unoptimized in the ML library, so this is probably not yet its full performance.&lt;/p&gt;
&lt;h5&gt;Map + Layer Norm + Relu&lt;/h5&gt;
&lt;p&gt;Before this is presented, layer norm will be shown on its own. This particular implementation is without the affine factor and acts like an activation instead. It is from the &lt;a href=&quot;https://arxiv.org/abs/1611.04520&quot; rel=&quot;nofollow&quot;&gt;Normalizing the Normalizers&lt;/a&gt; paper. It works great with Relus even in RNNs, and not at all with sigmoid activations.&lt;/p&gt;
&lt;p&gt;Layer norm (pseudo-code):&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl o x -&amp;gt; // o is some constant
    inl v = replicate (mean x)
    v / sqrt (o*o + replicate (mean (v*v)))
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;    inl layer_norm =
        inl fwd o i s =
            inl n = (primal i).dim |&amp;gt; snd |&amp;gt; HostTensor.span |&amp;gt; to float
            s.CudaKernel.mapi_d1_seq_broadcast {
                seq = 
                    {
                    redo=(+)
                    map_out=inl i sum -&amp;gt; i - sum / n
                    }
                    ,
                    {
                    map_in=inl v -&amp;gt; v*v
                    redo=(+)
                    map_out=inl v vv -&amp;gt; v / sqrt (o*o + vv / n)
                    }
                } (primal i)

        inl bck o r i s =
            inl n = (primal i).dim |&amp;gt; snd |&amp;gt; HostTensor.span |&amp;gt; to float
            s.CudaKernel.mapi_d1_seq_broadcast' {
                seq = 
                    {
                    map_in=inl dr,i -&amp;gt; i
                    redo=(+)
                    map_out=inl dr,i sum -&amp;gt; dr, i - sum / n
                    }
                    ,
                    {
                    map_in=inl dr,v -&amp;gt; v*v
                    redo=(+)
                    map_out=inl dr,v vv -&amp;gt; dr,v,sqrt (o*o + vv / n)
                    }
                    ,
                    {
                    map_in=inl dr,v,norm -&amp;gt; -dr * v / (norm * norm)
                    redo=(+)
                    map_out=inl dr,v,norm bot -&amp;gt; 
                        inl top = dr / norm
                        inl bot = (bot * v) / (norm * n)
                        top + bot
                    }
                    ,
                    {
                    redo=(+)
                    map_out=inl dv dv_sum adjoint -&amp;gt; adjoint + dv - dv_sum / n
                    }
                } (adjoint r, primal i) (adjoint i)

        inl o i s -&amp;gt;
            inl r = fwd o i s |&amp;gt; dr s
            r, inl _ -&amp;gt; bck o r i s
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;mapi_d1_seq_broadcast&lt;/code&gt; is finally used to its full effect here. The backwards steps won't be elaborated like in the softmax section. As can be seen from the above on the backwards pass the LN is actually recalculated from the input. This is good as the cost of recalculation is pretty much nothing compared to having to do extra reads and writes from intermediaries.&lt;/p&gt;
&lt;p&gt;Immediately after getting the above to work, the author fused the &lt;code&gt;layer_norm&lt;/code&gt; with the &lt;code&gt;relu&lt;/code&gt; and found that it gave a decent speedup in the RNN. The Relu activation is really trivial to add so that specific variant of LN won't be pasted here to prevent it from bloating the size of the tutorial any further.&lt;/p&gt;
&lt;p&gt;But after that step he fused it with multiplicative integration. That came out as something really great. At this moment in time there isn't a single thing in the library that more exemplifies Spiral's approach than it. It also demonstrates the real benefit of &lt;code&gt;mapi&lt;/code&gt; over the standard &lt;code&gt;map&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The function is relatively big, so it will be done in chunks.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    /// Map + layer normalization + relu
    inl map_ln_relu {fwd bck_in bck_in'} =
        inl n bias i = 
            inl a,b = Struct.map (inl o -&amp;gt; {o without block}) i |&amp;gt; HostTensor.zip |&amp;gt; inl x -&amp;gt; x.dim
            Struct.iter (inl {primal adjoint} -&amp;gt;
                inl f x = 
                    inl b' :: () = x.dim
                    assert (b' = b) &quot;The bias has to have a dimension equal to the input&quot;
                f primal; f adjoint
                ) bias
            HostTensor.span b |&amp;gt; to float
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;n&lt;/code&gt; returns the span of the innermost dimension. Despite that it seems more complicated, but all it is doing there are boundary checks.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        inl fwd' o b i w =
            inl n = n b i
            inl b = Struct.map (inl {primal} -&amp;gt; w.CudaTensor.to_dev_tensor primal) b
            w.CudaKernel.mapi_d1_seq_broadcast {
                mapi_in=inl j i' i -&amp;gt; Struct.map (inl x -&amp;gt; x i' .get) b |&amp;gt; inl b -&amp;gt; fwd b i
                seq = 
                    {
                    redo=(+)
                    map_out=inl i sum -&amp;gt; i - sum / n
                    }
                    ,
                    {
                    map_in=inl v -&amp;gt; v*v
                    redo=(+)
                    map_out=inl v vv -&amp;gt; v / sqrt (o*o + vv / n) |&amp;gt; relu_fwd
                    }
                } (Struct.map primal i)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The brilliant part of this is how instead of creating a separate &lt;code&gt;seq_broadcast&lt;/code&gt; in order to replicate the biases, they are instead captured in lexical scope and passed to &lt;code&gt;fwd&lt;/code&gt;. What the above does is maps the input, and then passes it to layer norm and then relu.&lt;/p&gt;
&lt;p&gt;The backward step is more involved, but fairly similar to the standard layer norm.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        inl bck' o r b i w =
            inl n = n b i
            inl b_primals = Struct.map (inl {primal} -&amp;gt; w.CudaTensor.to_dev_tensor primal) b
            inl b_adjoints = Struct.map (inl {adjoint} -&amp;gt; w.CudaTensor.to_dev_tensor adjoint) b
            w.CudaKernel.mapi_d1_seq_broadcast' {
                mapi_in=inl j i' (dr,i) -&amp;gt; 
                    inl b_primals = Struct.map (inl x -&amp;gt; x i' .get) b_primals
                    stack {b_primals i}, dr, fwd b_primals i
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;mapi_in&lt;/code&gt; allows a great deal of extensibility in the kernel. It allows the kernel to index into tensor out of its scope. Of course, what really happens is that they get dragged through the join point and indexed into there, but this is a good way of describing it in actual code.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                seq = 
                    {
                    map_in=inl bis,dr,i -&amp;gt; i
                    redo=(+)
                    map_out=inl bis,dr,i sum -&amp;gt; bis, dr, i - sum / n
                    }
                    ,
                    {
                    map_in=inl bis,dr,v -&amp;gt; v*v
                    redo=(+)
                    map_out=inl bis,dr,v vv -&amp;gt; bis,(if v &amp;gt; zero then dr else zero),v,sqrt (o*o + vv / n)
                    }
                    ,
                    {
                    map_in=inl bis,dr,v,norm -&amp;gt; -dr * v / (norm * norm)
                    redo=(+)
                    map_out=inl bis,dr,v,norm bot -&amp;gt; 
                        inl top = dr / norm
                        inl bot = (bot * v) / (norm * n)
                        bis,top + bot
                    }
                    ,
                    {
                    map_in=snd
                    redo=(+)
                    mapi_out=inl _ i' {b_primals i},dv dv_sum is_adjoints -&amp;gt; 
                        inl dx = dv - dv_sum / n

                        bck_in b_primals i
                        |&amp;gt; Struct.map ((*) dx)
                        // Note: The atomics make training non-deterministic.
                        |&amp;gt; Struct.iter2 (inl a -&amp;gt; atomic_add (a i')) b_adjoints

                        bck_in' b_primals i
                        |&amp;gt; Struct.map ((*) dx)
                        |&amp;gt; Struct.map2 (+) is_adjoints
                    }
                } (adjoint r, Struct.map primal i) (Struct.map adjoint i)

        inl o b i s -&amp;gt;
            inl r = fwd' o b i s |&amp;gt; dr s
            r, inl _ -&amp;gt; bck' o r b i s
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Propagating the adjoints into the biases is achieved through atomics. This is actual the only flaw in the whole kernel. Whereas before the training would be fully deterministic, this causes wide swings from run to run. The author saw the MIRNN range from 160 to 180 on the first epoch. With the biases frozen it gets 163 every time which makes it look like a joke, but the author intends to continue believing in their theoretical advantages.&lt;/p&gt;
&lt;p&gt;What the above function allows is using LN+Relu with any kind of map operation whether it be MI or something else, in either feedforward or recurrent networks. As a result the layer with the fully fused activation looks identical to the standard one.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl miln o size sublayer = 
        recurrent 
            {
            size sublayer
            weights = inl s -&amp;gt;
                open Initializer
                {
                input = relu (sublayer.size, size) s |&amp;gt; dr s
                state = relu (size, size) s |&amp;gt; dr s
                b1 = bias s size one
                b2 = bias s size (to float 0.5)
                b3 = bias s size (to float 0.5)
                b4 = bias0 s size
                } |&amp;gt; heap

            apply = inl {b1 b2 b3 b4 input state} s i -&amp;gt;
                match s with
                | () -&amp;gt;
                    inm i = matmult (i, input)
                    map_ln_relu {
                        fwd=inl (b3,b4) i -&amp;gt; b3*i + b4 
                        bck_in=inl (b3,b4) i -&amp;gt; (i, one) 
                        bck_in'=inl (b3,b4) i -&amp;gt; b3 
                        } o (b3,b4) i
                | _ -&amp;gt;
                    inm i = matmult (i, input)
                    inm s = matmult (s, state)
                    map_ln_relu {
                        fwd=inl (b1,b2,b3,b4) (i,s) -&amp;gt; b1*i*s + b2*s + b3*i + b4
                        bck_in=inl (b1,b2,b3,b4) (i,s) -&amp;gt; (i*s, s, i, one) 
                        bck_in'=inl (b1,b2,b3,b4) (i,s) -&amp;gt; (b1*s+b3, b1*i+b2)
                        } o (b1,b2,b3,b4) (i,s)
                &amp;gt;&amp;gt;= inl x -&amp;gt; succ (x,x)
            }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Since it takes only one instead of two backward steps, it takes 4.3s per epoch versus 4.6s for the standard &lt;code&gt;mi&lt;/code&gt; RNN without layer norm. All those complicated steps shown above take basically nothing - the real overhead is in memory movement to and from global memory.&lt;/p&gt;
&lt;p&gt;Implementing layer norm was one of the other great catalysts that drove the author to create Spiral. After 1.5 years of work, it is possible to present this piece - the fused map + layer norm + relu activation. It cannot be found anywhere else.&lt;/p&gt;
&lt;p&gt;With the first part of the grand quest complete, the Spiral ML library finally exceeds the old one in scope.&lt;/p&gt;
&lt;h4&gt;Layer Combinators&lt;/h4&gt;
&lt;p&gt;The primary purpose of layers is to make it easy to initialize the weights and the biases. The secondary purpose is to allow their parallel evaluation.&lt;/p&gt;
&lt;p&gt;Basic layer structure is this:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl gid _ = .(to string !GID())
    inl layer d = {d with gid=gid(); block=()}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Every layer has a &lt;code&gt;layer_type&lt;/code&gt; field and a &lt;code&gt;gid&lt;/code&gt; field. The &lt;code&gt;gid&lt;/code&gt; field holds a unique integer literal field which the evaluator uses to distinguish between the layers and cache their results so parts of the graph aren't executed more than once.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl input name size = layer {
        layer_type = .input
        name
        size
        }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The input layer is rather simple. It has a &lt;code&gt;name&lt;/code&gt; and a &lt;code&gt;size&lt;/code&gt; field.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl stateful layer_type {weights apply size sublayer} = 
        layer {
            layer_type
            size
            sublayer
            weights
            apply
            }

    inl feedforward = stateful .feedforward
    inl recurrent = stateful .recurrent
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Feedforward and recurrent layers aren't much different from each other. Only their &lt;code&gt;layer_type&lt;/code&gt; field differs. There are other layer types as well apart from the three shown above.&lt;/p&gt;
&lt;p&gt;Layer combinators are used to actually run and initialize the layers. And those combinators are derived from the &lt;code&gt;layer_map&lt;/code&gt; and &lt;code&gt;layer_map_fold&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl layer_map f network =
        inl rec layer_map r = function
            | {x with layer_type gid} -&amp;gt;
                match r with
                | {$gid=x} -&amp;gt; x, r
                | _ -&amp;gt;
                    inl sublayer, r =
                        match x with
                        | {sublayer} -&amp;gt; layer_map r sublayer
                        | _ -&amp;gt; (), r
                    inl x = f {x with sublayer}
                    x, {r with $gid=x}
            | x :: x' -&amp;gt; 
                inl x, r = layer_map r x
                inl x', r = layer_map r x'
                x :: x', r
            | () -&amp;gt; (), r
            | {} as x -&amp;gt;
                module_foldl (inl k (m,r) x -&amp;gt;
                    inl x,r = layer_map r x
                    module_add k x m, r
                    ) ({},r) x
            | x -&amp;gt; error_type (&quot;Expected a layer. Got&quot;, x)
        layer_map {} network |&amp;gt; fst
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;$&lt;/code&gt; is the new injection pattern. &lt;code&gt;$&lt;/code&gt; allows both matching and construction.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl x = .a
{$x=5} // {a=5}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;layer_map&lt;/code&gt; does bottom up mapping while caching the results. It is different than a standard map in that is allows the layers to access the previous result through the &lt;code&gt;sublayer&lt;/code&gt; field. &lt;code&gt;r&lt;/code&gt; is the module it uses to cache internal state.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;layer_map_fold&lt;/code&gt; is similar except it also allows passing of state. This is useful for running recurrent networks.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl init s network = 
        layer_map (function
            | {x with weights} -&amp;gt; {x with weights = const (weights s)}
            | x -&amp;gt; x
            ) network
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is how &lt;code&gt;init&lt;/code&gt; is made. If the layer has a &lt;code&gt;weights&lt;/code&gt; field it is passed in the context &lt;code&gt;s&lt;/code&gt; and that returns the network weights after the function has been evaluated. Though rather than just setting weights to that, it wraps it in a function.&lt;/p&gt;
&lt;p&gt;This has the property that if the layer is reinitialized later, those parts that have already been initialized are not affected. The reason why this is done is that the net might have a body, but also branches that might need to be initialized separately.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl init_parallel s network = 
        layer_map (function
            | {stream} | {layer_type=.input | .parallel} as x -&amp;gt; x
            | {x with weights} -&amp;gt; {x with weights = const (weights s); stream=s.RegionStream.allocate.data.stream}
            | x -&amp;gt; {x with stream=s.RegionStream.allocate.data.stream}
            ) network
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;init_parallel&lt;/code&gt; extends on the standard &lt;code&gt;init&lt;/code&gt; by also allocating a Cuda stream and storing it into the layer which can be used to execute layers in parallel.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl run x d s =
        layer_map_fold (inl {x with layer_type gid} d -&amp;gt;
            match layer_type with
            | .input -&amp;gt; d.input x.name, d
            | .stateless -&amp;gt;
                inl value, bck = indiv join
                    inl a, b = x.apply x.sublayer s
                    stack (a, term_cast b ())
                value, {d with bck = apply_bck self bck}
            | .non_differentiable -&amp;gt;
                inl value = indiv join x.apply x.sublayer s |&amp;gt; stack
                value, d
            | .parallel -&amp;gt; x.sublayer, d
            | .feedforward -&amp;gt;
                inl value, bck = indiv join
                    inl a, b = x.apply (x.weights()) x.sublayer s
                    stack (a, term_cast b ())
                value, {d with bck = apply_bck self bck}
            | .recurrent -&amp;gt;
                inl state = match d.state with {$gid=state} -&amp;gt; state | _ -&amp;gt; ()
                inl (value, state), bck = indiv join
                    inl a, b = x.apply (x.weights()) state x.sublayer s
                    stack (a, term_cast b ())
                value, {d with bck = apply_bck self bck; state = {self with $gid=state}}
            ) x d
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;For every layer, this function applies it and takes care to store the backward steps. It also term casts them so the compiler does not diverge for recurrent nets. Note that the results in the bottom layer are stored in the &lt;code&gt;sublayer&lt;/code&gt; field as noted previously. The state on the other hand is passed through &lt;code&gt;d&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The way to imagine the graph being executed is to think of values flowing upwards from &lt;code&gt;input&lt;/code&gt; layers through the &lt;code&gt;sublayer&lt;/code&gt; nodes all the way up to the single node at the top. Inputs are passed vie &lt;code&gt;d&lt;/code&gt; to the input fields and are accessed by name.&lt;/p&gt;
&lt;h5&gt;A Note On Compilation Times&lt;/h5&gt;
&lt;pre&gt;
&lt;code&gt;There is some funny stuff with `indiv join` and `stack` going on. Throughout the source code of the library there will be such patterns used throughout. They have only a single purpose - to optimize compile times.

When the author first made the LSTM work he received a shock in that it would take 4s to compile and adding another layer would add 2s to compilation. Considering that the nets he intends to train will be deep, the realization just how slow Spiral is made him lose his nerve. He first idea was to maybe rewrite the language in OCaml. He already considered it once before to do it Racket, but that fell through.

The second idea to spread join points and layout types around more readily. As it turns out that turned out to be greatly effective at crushing the compile times to around 1s and a negligible increase for each layer added. What was surprising to him not just how effective join points were at reducing compilation times, but that some of the functions which had no effect at runtime (like the kernel auxiliaries) and only did assertions and shape checking had a large impact on compile times as well.

The author thinks it might be possible to optimize compile times further, but he is at a loss as to how to do it past this point. He would welcome reviews from others on this part.

Some of the things he tried like flattening the AST that he thought would have benefit made absolutely no difference to performance. Things like optimizing the way `Op`s are represented would severely undermine the ergonomics of the compiler. Parser could be sped up significantly, but it is not a overhead. Actually it is, but it is not the one that is bothersome.

The partial evaluator is an enigma - reason tells that it is already fast for the kinds of work it is doing based on a rough comparison with other compilers. It is probably even faster than most other functional languages simply due to how simple it is. On the other hand, if the kinds of things it is doing can't be made any faster that bodes ill for not just Spiral, but for the future of programming languages.

To put it like this - if Spiral was 10x slower than it was now, then it would be much less useful. If it was 100x slower then there would be no point in using it. This value estimate goes into the other direction as well. Spiral would be a lot more valuable if it were 10x faster. Performance simply matters everywhere and always - compilers are not the exception.

There might be some novel kinds of abstraction in the future that will be out of reach because the compilers are too sluggish to deal with them in practice.

The author is pessimistic on the future of software development because of this. There simply does not seem to be any replacement for the immutable data structures Spiral uses internally. They make it flexible and powerful, and they do not make it slow, but they place a ceiling on its performance.

It is just as well that he is interested in machine learning. Compared to classical programming, optimization methods have almost limitless potential to improve from here.
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;    /// The wavefront iteration optimization.
    /// Requires the non-input layers to have preallocated streams.
    inl run_parallel x d s =
        layer_map_fold (inl {x with layer_type gid} d -&amp;gt;
            match layer_type with
            | .input -&amp;gt; {value=d.input x.name; stream=s.data.stream; block=()}, d
            | .parallel -&amp;gt; x.sublayer, d
            | _ -&amp;gt;
                inl stream = x.stream
                inl s = s.data_add {stream}
                inl values = Struct.map (inl {value} -&amp;gt; value) x.sublayer
                inl streams = 
                    Struct.choose (function
                        | {stream=x} -&amp;gt; stream.wait_on x; x
                        | _ -&amp;gt; ()) x.sublayer

                inl wait_bck b =
                    inl b _ =
                        b ()
                        Struct.iter (inl x -&amp;gt; x.wait_on stream) streams
                    term_cast b ()

                match layer_type with
                | .stateless -&amp;gt;
                    inl value, bck = indiv join
                        inl a, b = x.apply values s
                        stack (a, wait_bck b)
                    {value stream block=()}, {d with bck = apply_bck self bck}
                | .non_differentiable -&amp;gt;
                    inl value = indiv join x.apply values s |&amp;gt; stack
                    {value stream block=()}, d
                | .feedforward -&amp;gt;
                    inl value, bck = indiv join
                        inl a, b = x.apply (x.weights()) values s
                        stack (a, wait_bck b)
                    {value stream block=()}, {d with bck = apply_bck self bck}
                | .recurrent -&amp;gt;
                    inl state = match d.state with {$gid=state} -&amp;gt; state | _ -&amp;gt; ()
                    inl (value, state), bck = indiv join
                        inl a, b = x.apply (x.weights()) state values s
                        stack (a, wait_bck b)
                    {value stream block=()}, {d with bck = apply_bck self bck; state = {self with $gid=state}}
                ) x d
        |&amp;gt; inl x, d -&amp;gt; Struct.map (inl {value} -&amp;gt; value) x, d
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The wavefront iteration is from this &lt;a href=&quot;https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5/&quot; rel=&quot;nofollow&quot;&gt;NVidia blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;run_parallel&lt;/code&gt; is similar to &lt;code&gt;run&lt;/code&gt;, but takes advantage of streams to perform the wavefront iteration optimization. The benefit of it is nowhere near as described in blog post - it is more like 20% in practice, but it is a good 20%. Unless there is a reason not to, &lt;code&gt;run_parallel&lt;/code&gt; should be used instead of &lt;code&gt;run&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The author also tried to stream gemms as per that post, but got absolutely horrible results. For one, it is quite more difficult to use streams inside the layer instead of between them like in the above function.&lt;/p&gt;
&lt;p&gt;The first attempt to do is was to allocate them dynamically in order to avoid passing them around, but as it turns out, streams are insanely expensive to allocate so that plan fell through. The second plan was to preallocate and them pass them in, but that would just be too nasty - even if it worked it could potentially lead to data races for some kinds of operations so it was scrapped. It would have made the code too convoluted to bear with.&lt;/p&gt;
&lt;p&gt;The problem with the Cublas gemm is that they are simply too inflexible, not that they need to be streamed. Resolving that will be future work.&lt;/p&gt;
&lt;p&gt;There was an episode a month or so back when the author really wanted GC for Cuda memory and seriously considered rewriting Spiral in something else. Eventually, after trying out the above optimizations and thought about Cuda streams in more depth, he realize the problem was not just Cuda memory. Even if were possible to GC it somehow, GC cleanup and allocation might cause inadverted memory sharing in the presence of streams.&lt;/p&gt;
&lt;p&gt;Streams are probably the most poorly designed of all the Cuda features; not only do they block potential GC, they are so hard to reason about that they are barely usable. Preallocating them and using them for the wavefront iteration like this is probably their one blessed use. In all the other cases, actually fusing the kernels would be a far better choice.&lt;/p&gt;
&lt;p&gt;Within the confines of the ML library there is no problem in using them for the wavefront iteration, but if memory sections are shared with something else, there might be ways of causing memory corruption. In general, that should not be a worry though.&lt;/p&gt;
&lt;h4&gt;Loops&lt;/h4&gt;
&lt;p&gt;The loops in this context refers to the functions that run the net once. Here they are for feedforward nets.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        inl train {d with network} =
            inl rec loop c cost' = 
                function
                | .unwrap -&amp;gt; cost' / to float64 c
                | input s {on_fail on_succ} -&amp;gt;
                    inl cost, {bck} = run network {input state = {}; bck=const ()} s
                    inl cost' = cost' + to float64 (s.CudaTensor.get cost)
                    inl state = loop (c+1) cost'
                    if nan_is cost' then on_fail state
                    else
                        match d with
                        | {optimizer} -&amp;gt;
                            bck()
                            optimize network optimizer s
                        | _ -&amp;gt; ()
                        on_succ state
            loop (dyn 0) (dyn 0.0)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The loop has to do a bunch of things while it is running like abort early on nans in the cost, keep track of costs, call the backwards step and the optimizer. This is something that takes care of that.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        inl test {d with network} =
            inl rec loop c cost' accuracy' accuracy_max' = 
                function
                | .unwrap -&amp;gt; cost' / to float64 c, accuracy', accuracy_max'
                | input s {on_fail on_succ} -&amp;gt;
                    inl (cost, {value max}), {bck} = run network {input state = {}; bck=const ()} s
                    inl cost' = cost' + to float64 (s.CudaTensor.get cost)
                    inl accuracy' = accuracy' + s.CudaTensor.get value
                    inl accuracy_max' = accuracy_max' + max
                    inl state = loop (c+1) cost' accuracy' accuracy_max'
                    if nan_is cost' then on_fail state
                    else
                        match d with
                        | {optimizer} -&amp;gt;
                            bck()
                            optimize network optimizer s
                        | _ -&amp;gt; ()
                        on_succ state
            loop (dyn 0) (dyn 0.0) (dyn 0) (dyn 0)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This one takes care of accuracy and the max accuracy.&lt;/p&gt;
&lt;p&gt;Both of &lt;code&gt;train&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; hold their state internally and are unwrapped at the end of the epoch. &lt;code&gt;train&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; are passed in as body for the &lt;code&gt;for&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl outer data =
        Struct.foldl (inl s x -&amp;gt;
            match s with
            | () -&amp;gt; fst x.dim
            | s -&amp;gt; assert (s = fst x.dim) &quot;The data tensors need to have the same outer dimension.&quot;; s
            ) () data

    inl for {data body} s =
        inl {from near_to} = outer data
           
        Loops.for' {
            from near_to
            state=body
            body=inl {next state i} -&amp;gt;
                inl data = Struct.map (inl x -&amp;gt; x i) data
                s.refresh
                inl s = s.RegionMem.create
                state data s {
                    on_fail=inl state -&amp;gt;
                        s.RegionMem.clear
                        state.unwrap
                    on_succ=inl state -&amp;gt;
                        s.RegionMem.clear
                        next state
                    }
            finally=inl state -&amp;gt; state.unwrap
            }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Until now, no mention whatsoever has been made about how the library manages memory, so this is a good time to do it.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl s = s.RegionMem.create
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The context has &lt;code&gt;RegionMem&lt;/code&gt; and &lt;code&gt;RegionStream&lt;/code&gt;. When &lt;code&gt;.create&lt;/code&gt; is passed into them a new region is created. That does not actually allocate any memory or streams, but instead allocates a new region. Under the hood, the region is a resizable array that holds all the references allocated through it.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;s.RegionMem.clear
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;When this is called, all the references in that region have their count decremented by 1 and if that count is 0, they are set to null. For feedforward nets, that means the allocation always get cleared since their references are never shared with other regions.&lt;/p&gt;
&lt;p&gt;That does not actually dispose of the memory however.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;s.refresh
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It is this thing that does so. It filters out all the references set to null and rebuilds the free cells. It can be thought of as the equivalent of doing a GC run. This system is nowhere as good as actual GC in terms user ease or flexibility, but it should suffice for neural network needs. It can be improved to support things like very short term allocation without having to clean up everything, but that will be future work.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                state data s {
                    on_fail=inl state -&amp;gt;
                        s.RegionMem.clear
                        state.unwrap
                    on_succ=inl state -&amp;gt;
                        s.RegionMem.clear
                        next state
                    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Note that &lt;code&gt;state&lt;/code&gt; here is either &lt;code&gt;train&lt;/code&gt; or &lt;code&gt;test&lt;/code&gt;. The &lt;code&gt;for&lt;/code&gt; loop shown is a bit unusual in that the mapping function for it is also the state. The reason for that is because it is the easiest way of initializing the state. Rather being split into two, everything &lt;code&gt;train&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; need are right there where they should be.&lt;/p&gt;
&lt;p&gt;Doing it like that is a straightforward OO pattern in Spiral, but would be untypable in other static functional languages.&lt;/p&gt;
&lt;p&gt;The recurrent networks have their own &lt;code&gt;train&lt;/code&gt; and will have &lt;code&gt;test&lt;/code&gt;, but they are beyond the scope of this tutorial. They are more complicated, but have exactly the same purpose. Both feedforward and recurrent nets have their own dedicated gradient checking function which are useful for testing additions to the library.&lt;/p&gt;
&lt;p&gt;This covers all the important parts of the library.&lt;/p&gt;
&lt;p&gt;Fundamentally, ML libraries are not something to be worked on forever which is the unfortunate situation author found himself in. Much like parser combinators resolve the need for external parser generators, so can a ML library be fashioned in a similar vein. The greatest feature Spiral offers to the user is that it easily makes it possible to craft pieces like &lt;code&gt;map_ln_relu&lt;/code&gt;. It is in turn made out of flexible pieces like &lt;code&gt;seq_broadcast&lt;/code&gt;. They free the user of the drudgery of low level C programming and allow more a direct expression of desire. They also enable a vast amount of code reuse. Furthermore, Spiral's approach to programming allows the library an absolute level of integration with the language. This will be greatly useful when the time comes to make use of it as a module in a reinforcement learning context.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;map_ln_relu&lt;/code&gt; is just the tip of the iceberg and this can be considered the first unfinished release of the library. But compared to all the work that was needed to make both the language and get the library to this point, the rest should be much easier. The great up front expense has been paid in full.&lt;/p&gt;
&lt;h4&gt;Example - Feedforward Net On Mnist&lt;/h4&gt;
&lt;p&gt;The examples are just to give a feel for how the code looks like in practice.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let learning9 =
    &quot;learning9&quot;,[cuda_modules;learning;mnist],&quot;Does the full training work with Mnist?&quot;,
    &quot;&quot;&quot;
inb s = CudaModules (1024*1024*1024)

inl float = float32
open Learning float

inl minibatch_size = 128
inl { test_images test_labels train_images train_labels} =
    inl mnist_path = @&quot;C:\ML Datasets\Mnist&quot;
    Mnist.load_mnist_tensors mnist_path
    |&amp;gt; s.CudaTensor.from_host_tensors
    |&amp;gt; module_map (inl _ x -&amp;gt; x.round_split' minibatch_size)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is a handy loader just for Mnist. Tensors have the &lt;code&gt;round_split&lt;/code&gt; and &lt;code&gt;round_split'&lt;/code&gt; methods in them that split the outermost dimension to &lt;code&gt;(minibatch_size,rest)&lt;/code&gt; and &lt;code&gt;(rest,minibatch_size)&lt;/code&gt;. So the shape of the tensors after the above would be &lt;code&gt;(rest,128,784)&lt;/code&gt; after the above operation with the &lt;code&gt;rest&lt;/code&gt; being statically known.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl input_size = 784
inl hidden_size = 10

inl network = 
    open Feedforward.Layer

    inl label = input .label hidden_size
    inl network =
        input .input input_size 
        |&amp;gt; ln 0.0f32 256
        |&amp;gt; linear hidden_size 
        |&amp;gt; init s
    inl train = error Error.softmax_cross_entropy label network
    inl test = parallel (train, accuracy label network)
    {train test}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The layers can be easily piped using the &lt;code&gt;|&amp;gt;&lt;/code&gt; operator. There is no danger of failing to connect layers like this and no need to write unit tests for this purpose like some other libraries require the user to do.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Loops.for' {from=0; near_to=10;body=inl {next} -&amp;gt; 
    open Feedforward.Pass
    open Body

    inl cost =
        for {
            data={input=train_images; label=train_labels}
            body=train {
                network=network.train
                optimizer=Optimizer.sgd 0.3f32
                }
            } s

    string_format &quot;Training: {0}&quot; cost |&amp;gt; Console.writeline

    if nan_is cost then
        Console.writeline &quot;Training diverged. Aborting...&quot;
    else
        inl cost, ac, max_ac =
            for {
                data={input=test_images; label=test_labels}
                body=test { network=network.test }
                } s 

        string_format &quot;Testing: {0}({1}/{2})&quot; (cost, ac, max_ac) |&amp;gt; Console.writeline
        next ()
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Prepare the dataset, define the network and then run it. This is the part that does the last one. The really salient parts are those where it calls the &lt;code&gt;Learning&lt;/code&gt; &lt;code&gt;for&lt;/code&gt; function. The rest is just standard IO and checking for Nans.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        for {
            data={input=train_images; label=train_labels}
            body=train {
                network=network.train
                optimizer=Optimizer.sgd 0.3f32
                }
            } s
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is succinct enough that it actually does not need any further comment. One thing that would be good to mention is that since Spiral loves statically known dimensions, the &lt;code&gt;split_round&lt;/code&gt; functions tend to throw away the excess in order to keep the tensor regular. So the test set rather than having 10k examples will have 9984 examples.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Training: 0.309259804475129
Testing: 0.157295000082694(9493/9984)
Training: 0.114737567477899
Testing: 0.111421216703139(9622/9984)
Training: 0.0818591211150345
Testing: 0.0956207461153658(9671/9984)
Training: 0.0638285737313553
Testing: 0.0879654984933157(9693/9984)
Training: 0.0516892505001723
Testing: 0.0810565133280575(9715/9984)
Training: 0.042687166776731
Testing: 0.0776575743668498(9724/9984)
Training: 0.0355717467236667
Testing: 0.075778437509703(9734/9984)
Training: 0.0297816385700105
Testing: 0.0741259951931007(9747/9984)
Training: 0.025186768730975
Testing: 0.0730099095739066(9750/9984)
Training: 0.0212900042135873
Testing: 0.0725844820121076(9749/9984)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Layer norm + relu works rather well as an activation given how they hit 95% in the first epoch. This is almost on par with batch norm. The Mnist test is useful for ensuring that new additions to the library are implemented correctly.&lt;/p&gt;
&lt;h4&gt;Example - Recurrent Net on tiny_shakespeare&lt;/h4&gt;
&lt;p&gt;Prepare the dataset.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let learning11 =
    &quot;learning11&quot;,[cuda_modules;timer;learning],&quot;Does the full training + sampling work with the char-RNN?&quot;,
    &quot;&quot;&quot;
inb s = CudaModules (1024*1024*1024)

inl float = float32
open Learning float

inl size = {
    seq = 1115394
    minibatch = 64
    step = 64
    hot = 128
    }

// I got this dataset from Karpathy.
inl path = @&quot;C:\ML Datasets\TinyShakespeare\tiny_shakespeare.txt&quot;
inl data = 
    macro.fs (array char) [text: &quot;System.IO.File.ReadAllText&quot;; args: path; text: &quot;.ToCharArray()&quot;]
    |&amp;gt; Array.map (inl x -&amp;gt; 
        inl x = to int64 x
        assert (x &amp;lt; size.hot) &quot;The inputs need to be in the [0,127] range.&quot;
        to uint8 x
        )
    |&amp;gt; HostTensor.array_as_tensor
    |&amp;gt; HostTensor.assert_size size.seq
    |&amp;gt; s.CudaTensor.from_host_tensor
    |&amp;gt; inl data -&amp;gt; data.round_split size.minibatch

inl minibatch,seq = data.dim

inl input =
    inl data = s.CudaTensor.to_dev_tensor data 
    s.CudaKernel
        .init {dim=seq,minibatch} (inl seq minibatch -&amp;gt;
            data minibatch seq .get
            )
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This last part where the init is called transposes the dataset to the sequence of minibatches from a minibatch of sequences.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl label = input.view_span (const {from=1}) .round_split' size.step
inl input = input.view_span (inl x :: _ -&amp;gt; x-1) .round_split' size.step
inl data = {input label}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The labels are just the inputs shifted one to the left. Then they are split so that the final dimensions of the tensor are &lt;code&gt;(rest,64,64)&lt;/code&gt; where the middle 64 is the number of steps and the innermost 64 is the minibatch size.&lt;/p&gt;
&lt;p&gt;Define the network.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl network = 
    open Recurrent.Layer
    
    inl label = input .label 1 |&amp;gt; encode.one_hot size.hot
    inl input = input .input 1 |&amp;gt; encode.one_hot size.hot

    inl body =
        input
        |&amp;gt; miln 0.05f32 128
        |&amp;gt; Feedforward.Layer.linear size.hot
        |&amp;gt; init_parallel s

    inl train = 
        error Error.softmax_cross_entropy label body
        |&amp;gt; init_parallel s
    
    {train body}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;encode.one_hot&lt;/code&gt; is a layer that does encoding into the one-hot vector.&lt;/p&gt;
&lt;p&gt;Then here is the part that runs the network. It is similar to the feedforward case.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inb _ = Timer.timeit &quot;whole loop&quot;
Loops.for' {from=0; near_to=5; body=inl {next i} -&amp;gt; 
    open Recurrent.Pass
    open Body

    inl cost = 
        Timer.timeit (string_format &quot;iteration {0}&quot; i)
        &amp;lt;| inl _ -&amp;gt;
            for {
                data
                body=train {
                    network=network.train
                    optimizer=Optimizer.sgd 0.01f32
                    }
                } s

    Console.writeline &quot;----&quot;

    sample 0.6f32 2048 network.body '\n' s

    string_format &quot;Training: {0}&quot; cost |&amp;gt; Console.writeline

    if nan_is cost then
        Console.writeline &quot;Training diverged. Aborting...&quot;
    else
        next ()
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The key differences are - there is timing code, there is the &lt;code&gt;sample 0.6f32 2048 network.body '\n' s&lt;/code&gt; function in the middle and there is no test pass.&lt;/p&gt;
&lt;p&gt;Here is a sample run.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Starting timing for: whole loop
Starting timing for: iteration 0
The time was 00:00:04.6501455 for: iteration 0
----
Sample:
wing somest gooteded, and, singe,
The to betith in sumese the pearated.
Whis in sallers,�D you and(ed ansting the if the thou hinser, storsuld,
And of in coment not in whatss,
Ind thin bevers,ing in drain,
The trust,
And the sake thisher, and the that the deake wast,
And all of marst the thatpen and at cond waiching to thessirserent,�:
Sill I here.

Thand the be thered lows,
I and you cand I and tringher,
Bre this the me that
Thenens
Anded I thingent then, sursted the peave were the whingerew the mane
The be, coeld me the wave, the cour:
I the thend bete the deact, sird you make to the at wither.

Thery oreded mors beno, ripe comering thise dore siandsure,
As::
What now the misest wearse
Thoughty,
Thouping the the ceelles ourte.
The farthersest now
And at is wither, and that hath I with in you diantaces, the wast
And all in,
And the and ast yourd the the ceeptingers
I his of the siand thend that to way, bytherest theard the not to the is thet the wime, themell the thisser:
That thees,
And the wantersed be,
Ind asenoon,Theres,
Hereest to hard wers of in the of with yishinging this the mastereds,
And cakere thes in brony and of forderselt:
Then to gout, store torate the that?

MELIR:
The will of fand the is's porto the thet save wed nothtont of thenciint, ind,
To me,
And is with in and thenesand and himing wish not strogh
And wither,
And to dearons and m| mair beast a the whinges and the whath, as tomes, of my then the the that wis;
Bing, I good, make the stoll ot I me siet stome there sught.
Thenetront,
And the to wime betore an thou lodser:
And she for the this in your to meang't, sien and hourituteround.

LARA:
I werainert
ThertedE
I laveres?
To lading inest the thend
Yould, will wionds
And the hearf in goed ther,
But our of wach, the wagk
'ot pith and not to dearse the bute.

SIINO:
And his this fore thisest note,
Hear'd ather's and mask?

KIIIIO:
Thend and the wout.y this, fave and lathers, buster.

SIO:
And pare, Hinged in the couthar yourstast the and it lingels, lard whe sith histound on ond thourd in
-----
Training: 170.018245865317
Starting timing for: iteration 1
The time was 00:00:04.6143145 for: iteration 1
----
Sample:
Now therenowerad's forthy all the distarion theeld and and and the blainise is the trues
Sire, on to be the onound our lord and betion singed lord.

PUCES:
Thenet is lord of the courteday,
The and halood a mone of the coundey countien words thou with is thou all toome sonerites ourself your with the and to there and tongues enday, of not came the of apoing to fallow and that how's it that to muding bove to beting the look of singanes
Well to beastien madys, his as to to the devend and thenese lird is bother of refore, of our to apour.
That bunder,
To fars to their the parioule.

OUCELIO:
Ore with it distlation of with so, and earded to him a worce whing the gooke to sourt that to the sontle upon the biend that be and it that with tayer hport and and to that of the to then the foold porn the po that is wither the with thenenoloue one to sonery come are parding the know lead in land you dirst the rad the tood comes have yet hare and the sire and stard to seariens and that revererow(
And of then I mare
in then the to somered his are the prother&amp;lt;ing thou with thy lonted, and bothing thou may horeferst
I come to the to and that am their will then deariour and bray be and betrows thou toole afere to that and and turation you way, and the stole will sindares0er:
I saye, and his beanied thou asear,
And are of too hishers
My the beis to hinging the wirl wasting the dayes, then the known the sanded, and to in not ture the wortle thou to the word.

under forterald thee this thought as pake dainice lot hast to theight hather,
Belood is ase the mahe tonest to
So thou hathous tain to there astand them of is the downatterse: moy bearsice and thou sould to thes, to are thou and not to sunatard dearce?

POUDARO:
And took, to come beating the your a may they with the eagen,
To to my that in fore to thereford,
And mare the west in to thisker. I upon the power this downiald be songert as as to th|inus to piet more nother theee and and to to to thou thou and therede then of asse,A:
To to drood,
Nobe, becain I told the will thou how
-----
Training: 130.864040150362
Starting timing for: iteration 2
The time was 00:00:04.5930354 for: iteration 2
----
Sample:
ridown of, and seeping the with of this is loved that bold that padyed and your the bedang kauch of bess thought to thou live,
The king, thou hast, thou sire of the till this thoughtders the everied,
For to shall my to to him the constance of toot and that therefore and sonronion
Thou gince,
Mard, and bard,
And dastertart the surseding the sware.

PETEN:
Your love, the sentle,
And but all the can to feres say is conferty this to but with that again, fallon that stome arm not I are the sealt forting the fored,
The tonely, them thou sine now that to the sinter, m|inger down the surtion less.

First toous are to mare Hartery forst, that soorerage, bard letten,
Have of in are with of the provedious the lord:
I other in the romaster him, art of will thy soke sound.

POUMENTIO:
Dose:
And thou will wister all seed fare
And souls to to dost that toome all
sleast us this the lick, and to doingsy head!

ANTIO:
Comence are that as the make told bear,
What hand the deal your losk to my lord!

PETCANDNO:
Ward, of the might and fortly manrrngness,
That hawe of butise Buty are mone my loves;
Here, make the vant;
And hpard, the will the good, fortows and the with to the word and not hearing of the to fartor.

ALIUS:
My that I landly comenion would what every not to lord;
These with though his ever will with father, and but with all stiding, Edward,
And down: father, with now the encition:
Sack met your hand of have is then in the thought your hearts,
As my of mander stearself oftle manion the king they well.

KING RUCHARD III:
Weres the ling my fands you on the great and hpards the sile, in their comest the say as as a ment,
The looks of soncease sir thou beangled botter'd and then throst the stant forkented to the will of hath hpard that is beit though a seem the stand the sakes of the day hearted,
Would with and the with this the west be to the doughtere you gime hath the contence.

KING IO:
Herecome,
The fard the cance ases to the word's wears,
Hend with then the erterver:
Fore fallon and the peanion'sted.

GORINGLAN:
Some th
-----
Training: 120.591899198644
Starting timing for: iteration 3
The time was 00:00:04.6190544 for: iteration 3
----
Sample:
rest worther that lork
To the seaven the pard that branch and where bray and his with the word mamallue that of the tower fast in the forsed but of is like his to the bust m|ine thy forther a thou stard thou are come to soes and but in that to with thou coot man spord and never of the world ressibing be father for the fall would were to the some stain, truch sorford,
To be so this to the wards this say the to buny of be thy sir, strut thou falled to and to be the king.

ARCISTEN:
Sir, whose fair, his day, are my last in this this but to to thou lord.
owe the it.
Mare's pardous in the confort of your have with thou to be by the backers, that thou is this litter to the can poor your fail toome heart:
And the stant should to the king of his dread of thy of the of the caunted say stay the rawell, and take thou how it that shall thou strenhon our king
Whow a sing the to me sold abeary ware, cale to my pown.

DERENTER:
Where should of the counter be toot, the hand, be prain so entrant thou carn, it the might with the purilit the will to the sucting mistruck at is to the seaver mone too lood not diest the ere his warther of the well my lord, this counst of a son, beden the fourtion of her to adiend, thoo both of his be proms the with at before of the fail the kingl| the surse thereing be chook of this of dowers seaster is sporisher returm hus a to blousing a that to denself,H:
How is but the shall m|ot come the horst both as of the strank we wife ble?
And the it resign: and thy she do think not sirtus as the world hd holl me sower the gate a farle the surve say that drantly show, and thousence
Romeos.

PETROLIO:
Thy may not some is that with tpost it that well the complain the rest mine forturess in to the to the should not mucked hpard unterding thy love is this strants of so confort did make of my lord hand the lord, lost of the shall to say the man a my lord:
And you to the coursely man all thou gran murse to the way to the king a brotce.

GLOUCESTER:
My fall you go, I say good that for and a sind to of man if yo
-----
Training: 114.656255665947
Starting timing for: iteration 4
The time was 00:00:04.6133752 for: iteration 4
----
Sample:
the king
And rothy of thou lord, my trient our hath young rest lpettard tporcious are and most a was sould hlast tpose more his tords:
The darther are to thou farsing in
Sir, their command a manis me npard tpough for.

QUEEN ELIZALUSNE:
You son,
And thou vard is fires thou worst tporst that the man well in that at hland master's son, with in his man this live.

POLARUS:
But life, word!

MENENIUS:
What where or your bear me tpought for man are th|in tpose of word and npard0ent, the deping the strunch and of happy the long widims
There of your hast of did say, and but but is consignds husbard own beseefords of seeding, thou sound of that ment,
And thou fortion thou man the courte, good of this with that lands to him, and are bring then Seevous not son; and one of the doth the stand it hand days
What my mardiomer:
And my the down that did birdured were best soining do with of therefore that kind which more thou what with you all for to than my one of minder('s sirst and with him mone to drand.
�eloss better I purstian;
And name.
To make are unto thy hand that shall be so.

Thisineding thou day dood of what to him such mand his time; us bispited hlard and say is the blatten are in holy sings.

HENRY CANIE:
That with thou wpand with mistrous that words missenself!

CORINA:
Goother fdow you are thou have be come to to the wprown had tpought is meation, my life, monaster the call the ronds so such it, and surved the mast that fake that would thou re.

KING RICHARD III:
Bath the parton but rather constains, my exery as be in must,
For of ma,
this shall sir, be the promes us would and as this is to comes
And m|orant tooblain of know m|halst whose of must hd parton it it is to have lady, and how so king send thou are sear to this reason; a word to comes with and tpough, I diveress
And the sirrle sir;
Why, like the sont.

Second to tpatter, our farding is they but grace the part as some of cannot say, the falt our thou not thou lord.

ROMEO:
And to took in seaven of friar it thou propost stands thou am for from the last in
-----
Training: 110.771023666157
The time was 00:00:25.4366961 for: whole loop
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The net does not take long to start spouting Shakespearean sounding gibberish. Better nets and bigger datasets would improve performance no doubt. But this run is just intended to be equivalent of Mnist for recurrent nets. It would be possible to shave 0.3s from the runtime of each epoch by preprocessing the dataset more, but this way is the most convenient for when sampling is done.&lt;/p&gt;
&lt;p&gt;Based on looking at the above, it is clear even a tiny recurrent net with only 128 hidden units and a single layer is capable of learning a great deal of the structure of the dataset which is what the test intended to demonstrate.&lt;/p&gt;
&lt;p&gt;The author is definitely curious how a net with multiple layers and dilated connections would learn. That will definitely be subject of future work.&lt;/p&gt;
&lt;h4&gt;Future Work&lt;/h4&gt;
&lt;p&gt;Some of the loose ends currently in the library that need to be worked on when the author gets back to it.&lt;/p&gt;
&lt;p&gt;Getting to this point is a great relief to the author because the problem is no longer making the language which could take over a year, nor the huge arduous task of catching up and exceeding the old library. Rather, the problem is that the he has nothing to use it on. So the next task on his agenda will be to work on games for the agents. And what sort of better games than the ones where dumb people are willing to wager both money and status against potentially superhuman opponents.&lt;/p&gt;
&lt;p&gt;The future sure looks bright. The plan is simple; while much can't be expected with a simple one layer recurrent net like the one demonstrated on the char-RNN, it is a starting point and the continual process of constantly improving on that should yield bounty. It certainly beats the reality of pounding the pavement for the last 2.5 years.&lt;/p&gt;
&lt;p&gt;The following items are roughly in the order of priority. They describe the potential main avenues of improvement once the time is ripe.&lt;/p&gt;
&lt;h5&gt;Generic Matrix Multiplication&lt;/h5&gt;
&lt;p&gt;Usually the good parts of the library would want to be highlighted, but the quality of this function is poor. Nonetheless matrix multiplication is absolutely indispensable so it needs to be covered.&lt;/p&gt;
&lt;p&gt;Before it is shown, here is a short primer on how AD is done through matrix multiplication. &lt;code&gt;.T&lt;/code&gt; represents matrix transpose.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;C = A * B
dA += C * B.T
dB += A.T * C
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is the function that implements that. It also adds the bias term.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    inl matmultb l bias s = 
        inl l =
            match l with
            | () -&amp;gt; error_type &quot;The first argument must not be empty.&quot;
            | (_,_) :: _ -&amp;gt; l
            | _ :: _ -&amp;gt; l :: ()
        inl C = 
            Tuple.foldl (inl C (A,B) -&amp;gt;
                match C with
                | () -&amp;gt; s.CudaBlas.gemm .nT .nT one (primal A) (primal B) |&amp;gt; dr s
                | C -&amp;gt; s.CudaBlas.gemm' .nT .nT one (primal A) (primal B) one (primal C); C
                ) () l
        match bias with
        | () -&amp;gt; ()
        | _ -&amp;gt; s.CudaKernel.d2_replicate_map' (inl a b _ -&amp;gt; a+b) (primal bias) (primal C) (primal C)
        C, inl _ -&amp;gt; join
            inl C' = adjoint C
            inl l =
                Tuple.iter (inl A, B -&amp;gt; 
                    on_non_nil (adjoint A) (inl A -&amp;gt; s.CudaBlas.gemm' .nT .T one C' (primal B) one A)
                    on_non_nil (adjoint B) (inl B -&amp;gt; s.CudaBlas.gemm' .T .nT one (primal A) C' one B)
                    ) l
            match bias with
            | () -&amp;gt; ()
            | _ -&amp;gt; on_non_nil (adjoint bias) (inl bias -&amp;gt; s.CudaKernel.mapi_d2_redo_map' {map_in=const;neutral_elem=zero;redo=(+);map_out=(+)} C' bias.empty bias)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As a rough example of how it would be used in pseudo-code:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;// Equivalent of C = x*W + h*U + b
inl C = matmultb ((x,W),(h,U)) b // for vanilla recurrent networks
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;// Equivalent of C = x*W + b
inl C = matmultb (x,W) b // for feedforward networks
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;So the function does have a bit flexibility in that it can accept an arbitrary number of terms to multiply and it does optimize memory usage a little. A naive AD implementation would create intermediaries for every multiplication, but that is all that it offers. In terms of functionality offered, this function is identical to the one found in the old ML library written in F#.&lt;/p&gt;
&lt;p&gt;Here is the first pain point:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                    on_non_nil (adjoint A) (inl A -&amp;gt; s.CudaBlas.gemm' .nT .T one C' (primal B) one A)
                    on_non_nil (adjoint B) (inl B -&amp;gt; s.CudaBlas.gemm' .T .nT one (primal A) C' one B)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Two gemm calls in the backward steps. This is a problem because it does not at all take advantage that &lt;code&gt;C&lt;/code&gt; is shared. Had the two calls been fused, that could have been taken advantage of for a significant boost in performance.&lt;/p&gt;
&lt;p&gt;Pain point two is this:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;            Tuple.foldl (inl C (A,B) -&amp;gt;
                match C with
                | () -&amp;gt; s.CudaBlas.gemm .nT .nT one (primal A) (primal B) |&amp;gt; dr s
                | C -&amp;gt; s.CudaBlas.gemm' .nT .nT one (primal A) (primal B) one (primal C); C
                ) () l
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is not a problem when there is only a single pair of matrices to multiply, but in recurrent nets there tends to be two pairs. This function is wasteful because it would not take advantage of any potential sharing in the inputs, but it also would need to write multiple times to the output.&lt;/p&gt;
&lt;p&gt;There is a missed fusion opportunity here. This one is actually much more severe than it appears at first glance.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        match bias with
        | () -&amp;gt; ()
        | _ -&amp;gt; s.CudaKernel.d2_replicate_map' (inl a b _ -&amp;gt; a+b) (primal bias) (primal C) (primal C)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Adding bias during the linear part seems sensible at first glance, but the author has started to reconsider this design choice as a throwback to the old ML library where he never even considered fusing the bias and the activation. It depends on the complexity of the activation though, but since matrix multiply is a n^3 operation a strong case can be made that biases should not be dealt with in the linear part.&lt;/p&gt;
&lt;p&gt;There is one more point to be made - the age of shallow nets is absolutely over. The ideal network is the one that connects to many previous layers in order to allow easy gradient propagation. That means many convolutional or matrix operations done in parallel that sum up at the end.&lt;/p&gt;
&lt;p&gt;Vertically speaking that would be the &lt;a href=&quot;https://arxiv.org/abs/1608.06993&quot; rel=&quot;nofollow&quot;&gt;DenseNet&lt;/a&gt; architecture.&lt;/p&gt;
&lt;p&gt;In the paper they represent the connectivity pattern by concatenating all the past inputs.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;C = W * concat (a,b,c...)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is actually a common optimization in &lt;a href=&quot;https://devblogs.nvidia.com/optimizing-recurrent-neural-networks-cudnn-5/&quot; rel=&quot;nofollow&quot;&gt;LSTMs for example&lt;/a&gt;, to turn multiple smaller matrix multiplies into a single big one. It is possible to make an alternative representation of the above operation.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;C = sum(Wa*a,Wb*b,Wc*c...)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In other words, the operation can be done by using a large number of small matrix multiplications (or convolution operations.) And such a sufficiently fast and flexible kernel can be written in Spiral.&lt;/p&gt;
&lt;p&gt;There are also indications that this would be incredibly important in (quasi)recurrent networks.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://openreview.net/forum?id=B1DmUzWAW&quot; rel=&quot;nofollow&quot;&gt;SNAIL&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1803.01271&quot; rel=&quot;nofollow&quot;&gt;temporal convolution&lt;/a&gt; architectures have especially good training properties due to ability to easily propagate gradients. Those general architectures would also benefit greatly for having flexible and fast matrix multiply and convolutional kernels that are not offered by the Cuda libraries.&lt;/p&gt;
&lt;p&gt;This makes the case for why ML libraries should offer their own matrix multiply primitive - based on the understanding and the knowledge accumulated during the past few years, it seems increasingly credible that the ideal nets are those that do a large number of small operations with numerous residual connections that stretch deep both into time and space.&lt;/p&gt;
&lt;p&gt;As the author does not need such networks quite yet, he is content to leave that work for the future.&lt;/p&gt;
&lt;h5&gt;Fuse The Weight Updates&lt;/h5&gt;
&lt;p&gt;The way it is currently structured, the weights are updated one by one. If there are 100 weights, then the map kernel must be called 100 times, once per weight.&lt;/p&gt;
&lt;p&gt;It should be possible to fuse all those calls into a single kernel.&lt;/p&gt;
&lt;p&gt;It would improve performance and be a cool thing to do that the other libraries can't touch.&lt;/p&gt;
&lt;h5&gt;Metalearning Via Optimization&lt;/h5&gt;
&lt;p&gt;While MAML is beyond the scope of the &lt;code&gt;Learning&lt;/code&gt; library, the recent &lt;a href=&quot;https://blog.openai.com/reptile/&quot; rel=&quot;nofollow&quot;&gt;Reptile&lt;/a&gt; might be worth looking into. At the moment there are model based approaches to metalearning which use RNNs, and then there are optimization based approaches like Reptile. Right now the author does not understand how those two would mix together and would be interested in finding out.&lt;/p&gt;
&lt;p&gt;Hopefully, in the following months papers will come out that try to bridge that gap. The results will be interesting either way.&lt;/p&gt;
&lt;h5&gt;Dilated RNNs&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.02224&quot; rel=&quot;nofollow&quot;&gt;Dilated Recurrent Neural Networks&lt;/a&gt; is one architecture that needs implementation. Very recently, like in the past two years DenseNets and temporal convolutions have emerged as an effective way to train feedforward nets and quasi-recurrent nets. Dilated connections are their recurrent counterpart.&lt;/p&gt;
&lt;p&gt;This might be rude to say about it, but LSTMs need ditching. While the internal state mechanism it uses provides a crucial insight in how to stabilize the training RNNs for long sequences, it still does mean that gradient descent can do the right thing for deep sequences.&lt;/p&gt;
&lt;p&gt;LSTM trains better than a vanilla RNN. It has more feature than it. Its defining equation is definitely bigger. But from a different perspective, LSTM does not improve the generality of a standard RNN, but rather it restricts it. Batch norm, layer norm, activation functions, residual connections...all the significant improvements over the base network are in essence constraints.&lt;/p&gt;
&lt;p&gt;Dilated connections by restricting the amount of work the net can do increase its power and shorten the gradient path. They should be the starting point for deep recurrent networks.&lt;/p&gt;
&lt;p&gt;Dilated connections will require the generic gemm and redoing the loop bodies in the library.&lt;/p&gt;
&lt;h5&gt;Dense Connections&lt;/h5&gt;
&lt;p&gt;Originally introduced in the &lt;a href=&quot;https://arxiv.org/abs/1608.06993&quot; rel=&quot;nofollow&quot;&gt;Densely Connected Convolutional Networks&lt;/a&gt;, the dense connections are a general method of significantly easing the training the networks in the vertical direction. They can be applied to both feedforward and recurrent networks and the author is interested in their application towards the later.&lt;/p&gt;
&lt;p&gt;Given how much improvement they demonstrate for deep networks, the author's view is that they are integral component of all modern networks and they should be a permanent fixture in any kind of net.&lt;/p&gt;
&lt;p&gt;Dense connections will require the generic gemm and redoing the loop bodies in the library.&lt;/p&gt;
&lt;h5&gt;Deep Layer Compilation Optimizations&lt;/h5&gt;
&lt;p&gt;In particular, deep layer of over 10 will require some modifications to how kernels are compiled. Instead of passing in arguments directly at some point it will be necessary to pass them in array in order to preserve the user's sanity. That will require implementing asynchronous memory copies from host to device among others. Without this, kernels will need to be specialized for every single argument change. For nets with 50 layers, it would not be good if there were 50 different variations of gemm.&lt;/p&gt;
&lt;p&gt;The priority of this item depends on the kinds of networks being compiled. Right now it is completely unnecessary, but eventually it will be indispensable.&lt;/p&gt;
&lt;h5&gt;Generic Convolutions&lt;/h5&gt;
&lt;p&gt;This one should be done at some point as well for the sake of completeness, though the CuDNN one will suffice in the meantime. Probably there will be new kinds of operations in the future. There is no way that matrix multiply and convolutions are the only two possible choices. Hence mastering convolutions should be on the roadmap at some point. Doing that would also have the benefit of completely breaking the dependency on the CuDNN library.&lt;/p&gt;
&lt;p&gt;Divisive normalization should also be implemented at some point and that can be expressed in terms of the generic convolution operator.&lt;/p&gt;
&lt;h5&gt;1D Block Scan and Reduce&lt;/h5&gt;
&lt;p&gt;The author already did the transposed versions of them, but in order to take advantage of Spiral's default toa representation, it might be good to break the dependency on Cub for the 1d case. C++ is really limited in various ways, and much like the majority of statically typed languages falls flat when it comes to defining new datatypes. It is not a high priority item and more like a homework assignment suitable for newcomers to the language should they feel like it.&lt;/p&gt;
&lt;h5&gt;Softmax Cost&lt;/h5&gt;
&lt;p&gt;It would be better to recalculate the input much like in layer norm. Right now it is implemented inefficiently. The author is learning the language he created as he goes along much like anyone else would. He did not think through the current implementation properly.&lt;/p&gt;
&lt;h5&gt;Checkpointing&lt;/h5&gt;
&lt;p&gt;The ability to save arbitrary types to disk and load them will get added to the language and the standard library. Checkpointing for the ML library will succinctly get built on top of that. This is something that will be necessary when training times get longer.&lt;/p&gt;
&lt;h5&gt;Improve The Allocator&lt;/h5&gt;
&lt;p&gt;Allocator should be more capable of handling short term allocations. This can be done by taking the idea from the previous stack based allocator and incorporating them into the current one.&lt;/p&gt;
&lt;h5&gt;Plastic Neurons&lt;/h5&gt;
&lt;p&gt;The results in the &lt;a href=&quot;https://arxiv.org/abs/1804.02464&quot; rel=&quot;nofollow&quot;&gt;Differentiable Plasticity&lt;/a&gt; are amazing and have completely convinced the author that these kinds of neurons are an essential architectural feature for what I want to do. Adding them to the library would be trivial with a generic gemm kernel, which just puts more weight on implementing it.&lt;/p&gt;
&lt;p&gt;The author expected there would be novel architectural discoveries necessitating the generic gemm beyond dense connections, but he was surprised how quickly it happened.&lt;/p&gt;
&lt;h2&gt;User Guide: The Spiral Power&lt;/h2&gt;
&lt;p&gt;While the tutorial was meant to be a light introduction to the language, the user guide is intended to be an in depth guide not just into the language, but into the workings of the compiler. It is not intended to be exhaustive, but to cover the core language features and their implementation for the sake of making the user more confident in traversing the Spiral's source should that be needed.&lt;/p&gt;
&lt;p&gt;As it currently stands, the compiler in its entirety is at about 4.1k LOC. And while the parser and the codegens have their charm points, the part of interest which is the partial evaluator for Spiral is only 1.6k LOC. Out of that 1.6k probably around 1000 lines are vital to its functioning, meaning as the language continues to grow and more operatives get added, the essence of it should remain intact.&lt;/p&gt;
&lt;p&gt;It took the author 14 months to get the language up to this point, but with it as a reference it would be closer to 1 month long student project in terms of difficulty if a sensible tool like F# is used instead of C++.&lt;/p&gt;
&lt;p&gt;The first 9 chapters document the language's core. The rest can be read in any order.&lt;/p&gt;
&lt;h3&gt;1: Data Structures, Abstraction and Destructuring&lt;/h3&gt;
&lt;pre&gt;
&lt;code&gt;and TypedExpr =
    // Data structures
    | TyT of Ty
    | TyV of TyTag // int * Ty
    | TyBox of TypedExpr * Ty
    | TyList of TypedExpr list
    | TyMap of EnvTerm * MapType
    | TyLit of Value

    // Operations
    | TyLet of TyTag * TypedExpr * TypedExpr * Ty * Trace
    | TyState of TypedExpr * TypedExpr * Ty * Trace
    | TyOp of Op * TypedExpr list * Ty
    | TyJoinPoint of JoinPointKey * JoinPointType * Arguments * Ty
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Nearly everything that can be done is Spiral is by manipulating the above 6 data structures. Chop off the first 3 and Spiral would be a pure dynamic functional language. &lt;code&gt;TyBox&lt;/code&gt; represents a staged union type. &lt;code&gt;TyMap&lt;/code&gt; is reused for both functions and modules. Modules are in essence functions without a body, they can be thought of as first class environments. In &lt;code&gt;TyMap&lt;/code&gt; &lt;code&gt;EnvTerm&lt;/code&gt; is almost the immutable map and can be thought of as that for all intents and purposes for now.&lt;/p&gt;
&lt;p&gt;The dogma of information flows in Spiral is to let as much of it through forward. Inlineables always preserve all information and unlike most other languages, Spiral allows the exact structures, that is &lt;code&gt;TypedExpr&lt;/code&gt;s themselves, to be propagated forward through specialization boundaries. In Spiral those are join points, but in most other languages those are standard functions.&lt;/p&gt;
&lt;p&gt;In order to for there to be a separation between compile time and runtime, abstraction is necessary.&lt;/p&gt;
&lt;p&gt;Abstraction is the process of turning a non-variable (&lt;code&gt;TyBox&lt;/code&gt;,&lt;code&gt;TyList&lt;/code&gt;,&lt;code&gt;TyMap&lt;/code&gt;,&lt;code&gt;TyLit&lt;/code&gt;) into an abstract variable. There are 3 ways of doing that: join point returns, and dynamic if branch and dynamic case returns.&lt;/p&gt;
&lt;p&gt;When that is done, apart from what is preserved in the common subexpression dictionary, all information about the structure apart from its type is thrown away and it is replaced with a &lt;code&gt;TyV&lt;/code&gt;. &lt;code&gt;TyV&lt;/code&gt; is just a tag and a type representing a variable.&lt;/p&gt;
&lt;p&gt;Here are a few examples just to make sure the lesson sticks. It is rough typing the following out by hand. Keeping track of information is why compilers exist. The more a language allows the user to reason locally, the better it is at doing its job.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;TyLit (TyInt64 1) -&amp;gt; TyV(100, PrimT Int64T)
TyBox (TyT(LitT (LitString &quot;None&quot;)),UnionT {LitT (LitString &quot;None&quot;); ListT [LitT (LitString Some); PrimT Int64T]}) -&amp;gt; TyV(101,UnionT {LitT (LitString &quot;None&quot;); ListT [LitT (LitString Some); PrimT Int64T]})
TyList [TyV(2,PrimT Float32T); TyLit (LitString &quot;Hello&quot;); TyV(3,PrimT CharT)] -&amp;gt; TyV(102,ListT [PrimT Float32T; PrimT StringT; PrimT CharT])
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Destructuring is what usually comes after abstraction as it is used so often in the language. It is not the opposite of abstraction - it can't be since abstraction is the process throwing away information in a principled manner. Once that information is lost it can't be recovered.&lt;/p&gt;
&lt;p&gt;Destructuring is always done at bindings and at list and map creation and before join points entries, so fully abstracted variables will never appear in the environment when the printed via &lt;code&gt;print_static&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here is how the process roughly works.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;TyV(100, PrimT Int64T) -&amp;gt; TyV(100, PrimT Int64T) // no change
TyV(101,UnionT {LitT (LitString &quot;None&quot;); ListT [LitT (LitString Some); PrimT Int64T]}) -&amp;gt; TyV(101,UnionT {LitT (LitString &quot;None&quot;); ListT [LitT (LitString Some); PrimT Int64T]}) // no change
TyV(102,ListT [PrimT Float32T; PrimT StringT; PrimT CharT]) -&amp;gt; TyList [TyV(103,PrimT Float32T); TyV(104,PrimT StringT); TyV(105,PrimT CharT)]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Spiral is crazy about turning variables inside out whenever it can. This has the effect of flattening them when they are passed through join points and makes it easy to support having partially static maps and lists in the language. Note that this does not change their type in any way. In the above examples the left and the right sides have completely equal types.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;destructure&lt;/code&gt; is probably the single most important function in the language as without it everything else would be impossible. It is the first hurdle to overcome when making a language with first class staging.&lt;/p&gt;
&lt;p&gt;Note the the way F# for example or other functional languages do destructuring is not the same. There is a notable difference between the philosophy of Spiral and the rest of the pack. Spiral follows the dogma of &lt;code&gt;inline first and optimize later&lt;/code&gt; while generally high level languages inherit the Lisp philosophy of &lt;code&gt;heap allocate first and optimize later&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;That means in F# for example pattern matching is necessary to destructure a tuple since otherwise they would be packed. This is done at runtime. Here is a look into how F# does it based on its AST. The following examples uses its &lt;a href=&quot;https://docs.microsoft.com/en-us/dotnet/fsharp/language-reference/code-quotations&quot; rel=&quot;nofollow&quot;&gt;code quotation&lt;/a&gt; feature.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let x = 1,2,3
let t =
    &amp;lt;@
    let a,b,c = x
    ()
    @&amp;gt;
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;val x : int * int * int = (1, 2, 3)
val t : Quotations.Expr&amp;lt;unit&amp;gt; =
  Let (c, TupleGet (PropertyGet (None, x, []), 2),
     Let (b, TupleGet (PropertyGet (None, x, []), 1),
          Let (a, TupleGet (PropertyGet (None, x, []), 0), Value (&amp;lt;null&amp;gt;))))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Most compilers worth their salt would optimize this away, but Spiral's optimizations are actually a part of its semantics thereby making them guarantees. Replacing optimizations with guarantees and making it a part of the type system is important for getting predictable and predictably good performance.&lt;/p&gt;
&lt;p&gt;In Spiral something like a simple binding would mean something completely different.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl a,b,c = x
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;There is no way that in the above example that &lt;code&gt;x&lt;/code&gt; would not have already been destructured, hence what that tuple pattern does is tries to unbox a union type, but otherwise does absolutely nothing at runtime. It adds a few names to the environment and that is it. &lt;code&gt;destructure&lt;/code&gt; would be called and then it would do nothing once it saw &lt;code&gt;TyList&lt;/code&gt; as input.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl a,b,c = join 1,2,3
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Now if it was something like the above then it would be doing destructuring. It would go roughly &lt;code&gt;1,2,3 -&amp;gt; var [int64; int64; int64]&lt;/code&gt; in the join point and then &lt;code&gt;var [int64; int64; int64] -&amp;gt; [var int64; var int64; var int64]&lt;/code&gt; on destructuring.&lt;/p&gt;
&lt;p&gt;Keeping the environments fully destructured at all times is important for than just efficiency at runtime, making specialization coherent, supporting partially static structures and making other operations easier - it also solves one of the great language interop problems. &quot;How are arguments to be passed through join points to other languages?&quot;&lt;/p&gt;
&lt;p&gt;It is impossible to pass tuples of primitives to the Cuda side in their default form. The reason for that is that C might decide to layout the struct differently than the .NET JIT does and there is no way to access the layout information directly.&lt;/p&gt;
&lt;p&gt;By flattening structures out of the way completely at runtime, interop becomes a much easier task. That is one of the main motivations for supporting tuple of array tensors as the default as well.&lt;/p&gt;
&lt;p&gt;The time it took for the author to write the &lt;code&gt;destructure&lt;/code&gt; function can be measured in many months.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let rec destructure (d: LangEnv) (r: TypedExpr): TypedExpr = 
    let inline destructure r = destructure d r

    let inline cse_eval on_succ on_fail r = 
        match Map.tryFind r !d.cse_env with
        | Some x -&amp;gt; on_succ x
        | None -&amp;gt; on_fail r
    let inline cse_recurse r = cse_eval destructure id r

    let inline let_insert_cse r = 
        cse_eval 
            cse_recurse
            (fun r -&amp;gt;
                let x = make_tyv_and_push_typed_expr d r
                cse_add d r x
                x)
            r

    let index_tuple_args r tuple_types = 
        let unpack (s,i as state) typ = 
            if is_unit typ then tyt typ :: s, i
            else (destructure &amp;lt;| TyOp(ListIndex,[r;TyLit &amp;lt;| LitInt64 (int64 i)],typ)) :: s, i + 1
        List.fold unpack ([],0) tuple_types
        |&amp;gt; fst |&amp;gt; List.rev

    let env_unseal r x =
        let unseal (m,i as state) (k: string) typ = 
            if is_unit typ then Map.add k (tyt typ) m, i
            else
                let r = TyOp(MapGetField,[r; tyv(i,typ)], typ) |&amp;gt; destructure 
                Map.add k r m, i + 1
        Map.fold unseal (Map.empty,0) x |&amp;gt; fst

    let inline destructure_var r map_vvt map_funt =
        match get_type r with
        | ListT tuple_types -&amp;gt; tyvv(map_vvt tuple_types)
        | MapT (env,t) -&amp;gt; tymap(map_funt env, t)
        | _ -&amp;gt; cse_recurse r
    
    match r with
    | TyMap _ | TyList _ | TyLit _ -&amp;gt; r
    | TyBox _ -&amp;gt; cse_recurse r
    | TyT _ -&amp;gt; destructure_var r (List.map (tyt &amp;gt;&amp;gt; destructure)) (Map.map (fun _ -&amp;gt; (tyt &amp;gt;&amp;gt; destructure)) &amp;gt;&amp;gt; Env)
    | TyV _ -&amp;gt; destructure_var r (index_tuple_args r) (env_unseal r &amp;gt;&amp;gt; Env)
    | TyOp _ -&amp;gt; let_insert_cse r
    | TyJoinPoint _ | TyLet _ | TyState _ -&amp;gt; on_type_er (trace d) &quot;Compiler error: This should never appear in destructure. It should go directly into d.seq.&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;What was covered so far is essentially the &lt;code&gt;TyV _&lt;/code&gt; case. &lt;code&gt;TyT _&lt;/code&gt; case works very similarly to that except there is no need to make any work for the code generator in the later stage. Understanding the specifics of the function is not necessary at this point. What is desired is to show that there is a direct mapping between what was talked about and code.&lt;/p&gt;
&lt;p&gt;If one can understand the above function and how join points work, that would be enough to understand 90% of Spiral so it a goal to strive for.&lt;/p&gt;
&lt;p&gt;Based on what was discussed up to now, the first time reader of the user guide's model should roughly be as if the above function was written like this. The following is &lt;code&gt;destructure&lt;/code&gt; without common subexpression elimination.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let rec destructure (d: LangEnv) (r: TypedExpr): TypedExpr = 
    let inline destructure r = destructure d r

    let index_tuple_args r tuple_types = 
        let unpack (s,i as state) typ = 
            if is_unit typ then tyt typ :: s, i
            else (destructure &amp;lt;| TyOp(ListIndex,[r;TyLit &amp;lt;| LitInt64 (int64 i)],typ)) :: s, i + 1
        List.fold unpack ([],0) tuple_types
        |&amp;gt; fst |&amp;gt; List.rev

    let env_unseal r x =
        let unseal (m,i as state) (k: string) typ = 
            if is_unit typ then Map.add k (tyt typ) m, i
            else
                let r = TyOp(MapGetField,[r; tyv(i,typ)], typ) |&amp;gt; destructure 
                Map.add k r m, i + 1
        Map.fold unseal (Map.empty,0) x |&amp;gt; fst

    let inline destructure_var r map_vvt map_funt =
        match get_type r with
        | ListT tuple_types -&amp;gt; tyvv(map_vvt tuple_types)
        | MapT (env,t) -&amp;gt; tymap(map_funt env, t)
        | _ -&amp;gt; cse_recurse r
    
    match r with
    | TyMap _ | TyList _ | TyLit _ -&amp;gt; r
    | TyBox _ -&amp;gt; make_tyv_and_push_typed_expr d r
    | TyT _ -&amp;gt; destructure_var r (List.map (tyt &amp;gt;&amp;gt; destructure)) (Map.map (fun _ -&amp;gt; (tyt &amp;gt;&amp;gt; destructure)) &amp;gt;&amp;gt; Env)
    | TyV _ -&amp;gt; destructure_var r (index_tuple_args r) (env_unseal r &amp;gt;&amp;gt; Env)
    | TyOp _ -&amp;gt; make_tyv_and_push_typed_expr d r
    | TyJoinPoint _ | TyLet _ | TyState _ -&amp;gt; on_type_er (trace d) &quot;Compiler error: This should never appear in destructure. It should go directly into d.seq.&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;destructure_var&lt;/code&gt; can be completely thought of as a recursive map.&lt;/p&gt;
&lt;p&gt;Suppose something like &lt;code&gt;var [int64; int64]&lt;/code&gt; bound to &lt;code&gt;var_12&lt;/code&gt; is destructured.&lt;/p&gt;
&lt;p&gt;Inside &lt;code&gt;destructure&lt;/code&gt; the type of the variable is iterated over.&lt;/p&gt;
&lt;p&gt;The first element is converted to &lt;code&gt;list_index var_12 0&lt;/code&gt;. Then a recursive call is made on that. What happens is that then the &lt;code&gt;TyOp _&lt;/code&gt; case gets hit. &lt;code&gt;make_tyv_and_push_typed_expr&lt;/code&gt; then converts that into a &lt;code&gt;TyV&lt;/code&gt; which then gets returned.&lt;/p&gt;
&lt;p&gt;The second element is converted to &lt;code&gt;list_index var_12 1&lt;/code&gt;. Then a recursive call is made on that. What happens is that then the &lt;code&gt;TyOp _&lt;/code&gt; case gets hit. &lt;code&gt;make_tyv_and_push_typed_expr&lt;/code&gt; then converts that into a &lt;code&gt;TyV&lt;/code&gt; which then gets returned.&lt;/p&gt;
&lt;p&gt;The reason why this is done recursively is because lists might have sublists and such. Without that only a single level would get destructured which would be poor.&lt;/p&gt;
&lt;p&gt;The reason why first converting to a &lt;code&gt;TyOp(ListIdex,[var_12,0])&lt;/code&gt; and such is needed is for the code generation pass. As can be inferred from that, it stands to reason that &lt;code&gt;make_tyv_and_push_typed_expr&lt;/code&gt; is doing something to pass that information along. What is that function is doing is commonly known as let insertion.&lt;/p&gt;
&lt;h3&gt;2: Let Insertion and Common Subexpression Elimination&lt;/h3&gt;
&lt;p&gt;When the code is generated, it tends to be a series of &lt;code&gt;let&lt;/code&gt; statements punctuated by an expression.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let var_0 = ...
let var_1 = ...
let var_2 = ...
...
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Based on that information it be guessed that during evaluation a list of let statements is maintained in the environment that gets added every time &lt;code&gt;make_tyv_and_push_typed_expr&lt;/code&gt; gets called. That is in fact exactly what happens, but the way the function is implemented might seem convoluted at first.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let inline make_tyv_and_push_typed_expr_template (even_if_unit: bool) (d: LangEnv) (ty_exp: TypedExpr): TypedExpr =
    let ty = get_type ty_exp
    if is_unit ty then
        if even_if_unit then
            let seq = !d.seq
            let trace = trace d
            d.seq := fun rest -&amp;gt; TyState(ty_exp,rest,get_type rest,trace) |&amp;gt; seq
        tyt ty
    else
        let v = make_tyv_ty d ty
        let seq = !d.seq
        let trace = trace d
        d.seq := fun rest -&amp;gt; TyLet(v,ty_exp,rest,get_type rest,trace) |&amp;gt; seq
        tyv v
    
let make_tyv_and_push_typed_expr_even_if_unit d ty_exp = make_tyv_and_push_typed_expr_template true d ty_exp
let make_tyv_and_push_typed_expr d ty_exp = make_tyv_and_push_typed_expr_template false d ty_exp
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;There are actually two variants of the function packed into a single one based on whether the type of the expression is a unit. Using some partial evaluation by hand, it should be possible to clarify the function by splitting them into two.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let make_tyv_and_push_typed_expr_even_if_unit (d: LangEnv) (ty_exp: TypedExpr): TypedExpr = //make_tyv_and_push_typed_expr_template true d ty_exp
    let ty = get_type ty_exp
    let seq = !d.seq
    let trace = trace d
    
    if is_unit ty then
        d.seq := fun rest -&amp;gt; TyState(ty_exp,rest,get_type rest,trace) |&amp;gt; seq
        tyt ty
    else
        let v = make_tyv_ty d ty
        d.seq := fun rest -&amp;gt; TyLet(v,ty_exp,rest,get_type rest,trace) |&amp;gt; seq
        tyv v

let make_tyv_and_push_typed_expr (d: LangEnv) (ty_exp: TypedExpr): TypedExpr = //make_tyv_and_push_typed_expr_template false d ty_exp
    let ty = get_type ty_exp
    if is_unit ty then
        tyt ty
    else
        let v = make_tyv_ty d ty
        let seq = !d.seq
        let trace = trace d
        d.seq := fun rest -&amp;gt; TyLet(v,ty_exp,rest,get_type rest,trace) |&amp;gt; seq
        tyv v
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;With this rewrite hopefully the distinction between the two functions should be clearer.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;d.seq&lt;/code&gt; is in fact a list, but it is implemented as a closure of type &lt;code&gt;(TypedExpr -&amp;gt; TypedExpr) ref&lt;/code&gt;. To explain the reason for that, a look at the full &lt;code&gt;TypedExpr&lt;/code&gt; type is needed once again.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;and TypedExpr =
    // Data structures
    | TyT of Ty
    | TyV of TyTag
    | TyBox of TypedExpr * Ty
    | TyList of TypedExpr list
    | TyMap of EnvTerm * MapType
    | TyLit of Value

    // Operations
    | TyLet of TyTag * TypedExpr * TypedExpr * Ty * Trace
    | TyState of TypedExpr * TypedExpr * Ty * Trace
    | TyOp of Op * TypedExpr list * Ty
    | TyJoinPoint of JoinPointKey * JoinPointType * Arguments * Ty
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The way it is implemented like that is because of &lt;code&gt;TyLet&lt;/code&gt;. Using a list directly would be annoying because &lt;code&gt;TyLet&lt;/code&gt; is a list itself and it would be troublesome to add to end if the arraignment was different.&lt;/p&gt;
&lt;p&gt;So the solution is to just do some lambda magic to do that instead and emulate an immutable stack that way in a statically safe manner. The same goes for &lt;code&gt;TyState&lt;/code&gt; which is like &lt;code&gt;TyLet&lt;/code&gt; except for side effecting operations and not new bindings.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;make_tyv_and_push_typed_expr&lt;/code&gt; and &lt;code&gt;make_tyv_and_push_typed_expr_even_if_unit&lt;/code&gt; are used for non side effecting and side effecting operations respectively. Destructuring that was shown in the previous chapter is an example of the former, for example it should not happen that runtime unit types such as type level literals should ever be instantiated. As those get printed as &quot;&quot; during code generation trying to do so would give an error when the F# or Cuda compilers get the code.&lt;/p&gt;
&lt;p&gt;As a part of general mechanism of supporting the first class types in Spiral, in both let insertion and destructuring any operations that would produce a unit type gets converted to a unwrapped (naked) type. Side effecting or potentially side effecting operations such as join points therefore must call &lt;code&gt;make_tyv_and_push_typed_expr_even_if_unit&lt;/code&gt; before &lt;code&gt;destructure&lt;/code&gt; gets to it otherwise it will cause errors. For example if &lt;code&gt;make_tyv_and_push_typed_expr_even_if_unit&lt;/code&gt; was not called inside join points then those with unit type as return would not get printed at all.&lt;/p&gt;
&lt;p&gt;Even worse, repeating join points in local scope would get rewritten to the same variable. This would not be a problem for pure functions, but for side effecting ones it would lead to erroneous results.&lt;/p&gt;
&lt;p&gt;The way (CSE) common subexpression elimination works is simple in Spiral.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl q = x + 3
...
inl w = x + 3
...
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;All CSE happens in &lt;code&gt;destructure&lt;/code&gt;. What happens is that the call to &lt;code&gt;destructure&lt;/code&gt; gets &lt;code&gt;TyOp(Add,[TyV(5,PrimT Int64T); TyLit (LitInt64 3L)])&lt;/code&gt; or similar as the argument. It hits the &lt;code&gt;TyOp&lt;/code&gt; branch and then performs a check against the CSE dictionary with is a standard map of type &lt;code&gt;Map&amp;lt;TypedExpr,TypedExpr&amp;gt; ref&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The first time it happens in &lt;code&gt;inl q = x + 3&lt;/code&gt;, the key is not present in the dictionary so what it does is calls &lt;code&gt;make_tyv_and_push_typed_expr&lt;/code&gt; with the argument. It gets back something like &lt;code&gt;TyV(9,PrimT Int64T)&lt;/code&gt;. That gets bound to the key and added to the dictionary like &lt;code&gt;Map.add (TyOp(Add,[TyV(5,PrimT Int64T); TyLit (LitInt64 3L)])) (TyV(9,PrimT Int64T))&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The actual code is a tad more complicated.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let cse_add' d r x = let e = !d.cse_env in if r &amp;lt;&amp;gt; x then Map.add r x e else e
let cse_add d r x = d.cse_env := cse_add' d r x
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;There is a check to make sure that the variable being added to the map is not itself because that could lead to divergence, but otherwise the intent should be straightforward.&lt;/p&gt;
&lt;p&gt;Once the new key and its value in the map, the program proceeds and &lt;code&gt;inl w = x + 3&lt;/code&gt; is hit eventually. As the arguments flows through &lt;code&gt;destructure&lt;/code&gt; the check for &lt;code&gt;TyOp(Add,[TyV(5,PrimT Int64T); TyLit (LitInt64 3L)])&lt;/code&gt; and this time the key is present dictionary. Since it is present, instead calling &lt;code&gt;make_tyv_and_push_typed_expr&lt;/code&gt; to perform let insertion, the expression just gets rewritten.&lt;/p&gt;
&lt;p&gt;Note that this happens recursively. Here is the full &lt;code&gt;destructure&lt;/code&gt; once again.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let rec destructure (d: LangEnv) (r: TypedExpr): TypedExpr = 
    let inline destructure r = destructure d r

    let inline cse_eval on_succ on_fail r = 
        match Map.tryFind r !d.cse_env with
        | Some x -&amp;gt; on_succ x
        | None -&amp;gt; on_fail r
    let inline cse_recurse r = cse_eval destructure id r

    let inline let_insert_cse r = 
        cse_eval 
            cse_recurse
            (fun r -&amp;gt;
                let x = make_tyv_and_push_typed_expr d r |&amp;gt; destructure
                cse_add d r x
                x)
            r

    let index_tuple_args r tuple_types = 
        let unpack (s,i as state) typ = 
            if is_unit typ then tyt typ :: s, i
            else (destructure &amp;lt;| TyOp(ListIndex,[r;TyLit &amp;lt;| LitInt64 (int64 i)],typ)) :: s, i + 1
        List.fold unpack ([],0) tuple_types
        |&amp;gt; fst |&amp;gt; List.rev

    let env_unseal r x =
        let unseal (m,i as state) (k: string) typ = 
            if is_unit typ then Map.add k (tyt typ) m, i
            else
                let r = TyOp(MapGetField,[r; tyv(i,typ)], typ) |&amp;gt; destructure 
                Map.add k r m, i + 1
        Map.fold unseal (Map.empty,0) x |&amp;gt; fst

    let inline destructure_var r map_vvt map_funt =
        match get_type r with
        | ListT tuple_types -&amp;gt; tyvv(map_vvt tuple_types)
        | MapT (env,t) -&amp;gt; tymap(map_funt env, t)
        | _ -&amp;gt; cse_recurse r
    
    match r with
    | TyMap _ | TyList _ | TyLit _ -&amp;gt; r
    | TyBox _ -&amp;gt; cse_recurse r
    | TyT _ -&amp;gt; destructure_var r (List.map (tyt &amp;gt;&amp;gt; destructure)) (Map.map (fun _ -&amp;gt; (tyt &amp;gt;&amp;gt; destructure)) &amp;gt;&amp;gt; Env)
    | TyV _ -&amp;gt; destructure_var r (index_tuple_args r) (env_unseal r &amp;gt;&amp;gt; Env)
    | TyOp _ -&amp;gt; let_insert_cse r
    | TyJoinPoint _ | TyLet _ | TyState _ -&amp;gt; on_type_er (trace d) &quot;Compiler error: This should never appear in destructure. It should go directly into d.seq.&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;And here it is pared down so that the CSE using parts are highlighted.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let rec destructure (d: LangEnv) (r: TypedExpr): TypedExpr = 
    let inline destructure r = destructure d r

    let inline cse_eval on_succ on_fail r = 
        match Map.tryFind r !d.cse_env with
        | Some x -&amp;gt; on_succ x
        | None -&amp;gt; on_fail r
    let inline cse_recurse r = 
        cse_eval 
            destructure // on_succ
            id // on_fail
            r

    let inline let_insert_cse r = 
        cse_eval 
            cse_recurse // on_succ
            (fun r -&amp;gt; // on_fail
                let x = make_tyv_and_push_typed_expr d r |&amp;gt; destructure
                cse_add d r x
                x)
            r
    
    match r with
    | TyMap _ | TyList _ | TyLit _ -&amp;gt; r
    | TyBox _ -&amp;gt; cse_recurse r
    | TyOp _ -&amp;gt; let_insert_cse r
    | TyJoinPoint _ | TyLet _ | TyState _ -&amp;gt; on_type_er (trace d) &quot;Compiler error: This should never appear in destructure. It should go directly into d.seq.&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It is possible to simplify it even further.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let rec destructure (d: LangEnv) (r: TypedExpr): TypedExpr = 
    let inline destructure r = destructure d r
    let cse_recurse r = 
        match Map.tryFind r !d.cse_env with
        | Some x -&amp;gt; destructure x
        | None -&amp;gt; r

    let let_insert_cse r = 
        match Map.tryFind r !d.cse_env with
        | Some x -&amp;gt; cse_recurse x
        | None -&amp;gt; 
            let x = make_tyv_and_push_typed_expr d r |&amp;gt; destructure
            cse_add d r x
            x
    
    match r with
    | TyMap _ | TyList _ | TyLit _ -&amp;gt; r
    | TyBox _ -&amp;gt; cse_recurse r
    | TyOp _ -&amp;gt; let_insert_cse r
    | TyJoinPoint _ | TyLet _ | TyState _ -&amp;gt; on_type_er (trace d) &quot;Compiler error: This should never appear in destructure. It should go directly into d.seq.&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This should hopefully be clear. &lt;code&gt;cse_recurse&lt;/code&gt; does not call itself recursively. It does call &lt;code&gt;destructure&lt;/code&gt; instead, but for all intents and purposes the function can thought of calling itself recursively.&lt;/p&gt;
&lt;p&gt;Suppose that the dictionary is like the following (in pseudo-code):&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;TyOp(Add,[TyV(5,PrimT Int64T); TyLit (LitInt64 3L)]) =&amp;gt; TyV(9,PrimT Int64T)
TyV(9,PrimT Int64T) =&amp;gt; TyV(12,PrimT Int64T)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;So &lt;code&gt;x+3&lt;/code&gt; will first get rewritten to &lt;code&gt;TyV(9,PrimT Int64T)&lt;/code&gt;. Then &lt;code&gt;destructure&lt;/code&gt; will be called again.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;| TyV _ -&amp;gt; destructure_var r (index_tuple_args r) (env_unseal r &amp;gt;&amp;gt; Env)&lt;/code&gt; case will get hit. After that happens it will attempt to pattern match on this.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let inline destructure_var r map_vvt map_funt =
    match get_type r with
    | ListT tuple_types -&amp;gt; tyvv(map_vvt tuple_types)
    | MapT (env,t) -&amp;gt; tymap(map_funt env, t)
    | _ -&amp;gt; cse_recurse r
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It is not &lt;code&gt;ListT&lt;/code&gt; nor &lt;code&gt;MapT&lt;/code&gt; so &lt;code&gt;cse_recurse&lt;/code&gt; in the last branch gets called. Then &lt;code&gt;TyV(9,PrimT Int64T)&lt;/code&gt; would get rewritten to &lt;code&gt;TyV(12,PrimT Int64T)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;let_insert_cse&lt;/code&gt; is just the same as &lt;code&gt;cse_recurse&lt;/code&gt; apart from the extra step of doing let insertion.&lt;/p&gt;
&lt;p&gt;With this the let insertion and destructuring have been covered in full. The functions related to them serve as focal points of the entire language and with the understanding of them the paths to understanding of everything else become unblocked. In programming in general, programs tend to be easier to write than to read.&lt;/p&gt;
&lt;p&gt;Those two functions on the other hand are definitely on the exact opposite of the spectrum as they literally took months and months of refinement to make.&lt;/p&gt;
&lt;p&gt;CSE rewriting has some additional uses which will be covered in the following chapter.&lt;/p&gt;
&lt;h3&gt;3: The If Statement&lt;/h3&gt;
&lt;p&gt;Spiral originally started out with the dynamic &lt;code&gt;if&lt;/code&gt; as is found in most statically typed languages, but as an experiment the author decided to make the static the default one. After two month of use, he noted that not even once had he used the other version and when he ran into a bug with the dynamic &lt;code&gt;if&lt;/code&gt; that cemented its removal for good from the language.&lt;/p&gt;
&lt;p&gt;Here is the static &lt;code&gt;if&lt;/code&gt; in full.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let if_static (d: LangEnv) (cond: Expr) (tr: Expr) (fl: Expr): TypedExpr =
    match tev d cond with
    | TyLit (LitBool true) -&amp;gt; tev d tr
    | TyLit (LitBool false) -&amp;gt; tev d fl
    | TyType (PrimT BoolT) as cond -&amp;gt;
        let b x = cse_add' d cond (TyLit &amp;lt;| LitBool x)
        let tr = tev_assume (b true) d tr
        let fl = tev_assume (b false) d fl
        let type_tr, type_fl = get_type tr, get_type fl
        if type_tr = type_fl then
            match tr, fl with
            | TyLit (LitBool true), TyLit (LitBool false) -&amp;gt; cond
            | _ when tr = fl -&amp;gt; tr
            | _ -&amp;gt; TyOp(IfStatic,[cond;tr;fl],type_tr) |&amp;gt; make_tyv_and_push_typed_expr_even_if_unit d
        else on_type_er (trace d) &amp;lt;| sprintf &quot;Types in branches of If do not match.\nGot: %s and %s&quot; (show_ty type_tr) (show_ty type_fl)
    | TyType cond -&amp;gt; on_type_er (trace d) &amp;lt;| sprintf &quot;Expected a bool in conditional.\nGot: %s&quot; (show_ty cond)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The first 2 cases should be straightforward.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let if_static (d: LangEnv) (cond: Expr) (tr: Expr) (fl: Expr): TypedExpr =
    match tev d cond with
    | TyLit (LitBool true) -&amp;gt; tev d tr
    | TyLit (LitBool false) -&amp;gt; tev d fl
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;At first the conditional is evaluated and if it turns out to be a literal only one of the branches is evaluated.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    | TyType (PrimT BoolT) as cond -&amp;gt;
        let b x = cse_add' d cond (TyLit &amp;lt;| LitBool x)
        let tr = tev_assume (b true) d tr
        let fl = tev_assume (b false) d fl
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here the two branches are evaluated with an little extra twist. Suppose an example like the following.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;if x &amp;gt; 0 then
    if x &amp;gt; 0 then
        123
    else
        &quot;qwe&quot;
else
    456
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In the outer if the conditional &lt;code&gt;x &amp;gt; 0&lt;/code&gt; will evaluate to an abstract variable and in &lt;code&gt;if_static&lt;/code&gt; the third branch will get hit.&lt;/p&gt;
&lt;p&gt;Then once the conditional of the inner if is evaluated, what it will find in the CSE dictionary is that the &lt;code&gt;x &amp;gt; 0&lt;/code&gt; can be rewritten to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        let b x = cse_add' d cond (TyLit &amp;lt;| LitBool x)
        let tr = tev_assume (b true) d tr
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above two lines are responsible for this.&lt;/p&gt;
&lt;p&gt;Here is how &lt;code&gt;tev_assume&lt;/code&gt; is implemented.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;// #Type directed partial evaluation
let rec expr_peval (d: LangEnv) (expr: Expr): TypedExpr =
    let inline tev d expr = expr_peval d expr
    let inline apply_seq d x = !d.seq x
    let inline tev_seq d expr = let d = {d with seq=ref id; cse_env=ref !d.cse_env} in tev d expr |&amp;gt; apply_seq d
    let inline tev_assume cse_env d expr = let d = {d with seq=ref id; cse_env=ref cse_env} in tev d expr |&amp;gt; apply_seq d
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;tev&lt;/code&gt; itself just calls the main evaluation functions. The &lt;code&gt;expr_eval&lt;/code&gt; function is one of the core passes of Spiral as that is where all the partial evaluation happens. The rest are parsing and code generation.&lt;/p&gt;
&lt;p&gt;As &lt;code&gt;tev_seq&lt;/code&gt; and &lt;code&gt;tev_assume&lt;/code&gt; are very similar, it would be easier to start with an explanation of &lt;code&gt;tev_seq&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In Spiral there are constructs such as if statements, case expressions and join points which require their own scope.&lt;/p&gt;
&lt;p&gt;On entry therefore, the let statements that are being carried in the environment need to be cleared instead of being carried into the branch. &lt;code&gt;seq=ref id&lt;/code&gt; is what represents this. If it was a standard list, it would just be set to &lt;code&gt;[]&lt;/code&gt;, but here it is set to the identity function.&lt;/p&gt;
&lt;p&gt;Then there is the CSE dictionary that also needs to be handled. Since Spiral is lexically scoped it is fine if the expressions inside the local scope get rewritten to expressions that came before it, but what should not happen after the scope is exited for expressions to get rewritten with the ones that are now out of scope.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;if c then 
    inl q = x + 2
    ...
else
    ...

inl e = x + 2 // Do not want this to be rewritten to `q` inside the if statement.
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;If &lt;code&gt;cse_env&lt;/code&gt; was a standard .NET mutable dictionary it would need to be copied at this point so that invariant is ensured, but since it is an F# immutable map, what should just be done is replacing the reference to it.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;tev_assume&lt;/code&gt; has similar intent to &lt;code&gt;tev_seq&lt;/code&gt; except the map for &lt;code&gt;cse_env&lt;/code&gt; is taken as an argument; it is a more generic version of &lt;code&gt;tev_seq&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;With this it has been established that the two functions just evaluate an expression in its own scope.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;tev d expr |&amp;gt; apply_seq d&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;After that evaluation is done, &lt;code&gt;apply_seq&lt;/code&gt; is called. &lt;code&gt;tev&lt;/code&gt; by itself only returns the last expression in the scope. After the call to &lt;code&gt;tev&lt;/code&gt; the rest of the expressions can be found in &lt;code&gt;d.seq&lt;/code&gt; where they have been let inserted.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let inline tev_seq (d: LangEnv) (expr: TypedExpr): TypedExpr = let d = {d with seq=ref id; cse_env=ref !d.cse_env} in tev d expr |&amp;gt; apply_seq d
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Appending the last expression to it is quite easy. The last expression just need to be applied to &lt;code&gt;d.seq&lt;/code&gt; and the result will be all the statements in scope neatly arranged. If the &lt;code&gt;d.seq&lt;/code&gt; is empty, then it would be the identity function and so applying an expression to it would return the same thing; there wouldn't be any empty statements. This is why &lt;code&gt;(TypedExpr -&amp;gt; TypedExpr) ref&lt;/code&gt; representation is so convenient.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;| TyType (PrimT BoolT) as cond -&amp;gt;
    let b x = cse_add' d cond (TyLit &amp;lt;| LitBool x)
    let tr = tev_assume (b true) d tr
    let fl = tev_assume (b false) d fl
    let type_tr, type_fl = get_type tr, get_type fl
    if type_tr = type_fl then
        match tr, fl with
        | TyLit (LitBool true), TyLit (LitBool false) -&amp;gt; cond
        | _ when tr = fl -&amp;gt; tr
        | _ -&amp;gt; TyOp(IfStatic,[cond;tr;fl],type_tr) |&amp;gt; make_tyv_and_push_typed_expr_even_if_unit d
    else on_type_er (trace d) &amp;lt;| sprintf &quot;Types in branches of If do not match.\nGot: %s and %s&quot; (show_ty type_tr) (show_ty type_fl)
| TyType cond -&amp;gt; on_type_er (trace d) &amp;lt;| sprintf &quot;Expected a bool in conditional.\nGot: %s&quot; (show_ty cond)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Hopefully what the first 4 lines are doing now should now be clear.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let type_tr, type_fl = get_type tr, get_type fl
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;get_type&lt;/code&gt; is important so it will get its own treatment later, but its essence is in entirety described by its type which is &lt;code&gt;TypedExpr -&amp;gt; Ty&lt;/code&gt;. That is, it gets the type of the expression.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;if type_tr = type_fl then
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Since the conditional is dynamic, the branches need to be compared for equality. Then a tad optimization is done after the check.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;match tr, fl with
| TyLit (LitBool true), TyLit (LitBool false) -&amp;gt; cond
| _ when tr = fl -&amp;gt; tr
| _ -&amp;gt; TyOp(IfStatic,[cond;tr;fl],type_tr) |&amp;gt; make_tyv_and_push_typed_expr_even_if_unit d
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The first case does trigger sometimes such as in structural equality for example. The second case just checks if both branches are equal. In that case obviously the if statement is unnecessary.&lt;/p&gt;
&lt;p&gt;In the third branch is where the if statement gets let inserted. Note how &lt;code&gt;make_tyv_and_push_typed_expr_even_if_unit&lt;/code&gt; needs to be called here. If that did not happen and the return was let to be grabbed by &lt;code&gt;destructure&lt;/code&gt; it would not get printed if it had unit return and would be added to the CSE dictionary there.&lt;/p&gt;
&lt;p&gt;That is not the desired behavior.&lt;/p&gt;
&lt;p&gt;It is the desired behavior when destructuring unit types from a tuple, or with pure built-in arithmetic expressions, but not with if statements.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;on_type_er (trace d) &amp;lt;| sprintf &quot;Types in branches of If do not match.\nGot: %s and %s&quot; (show_ty type_tr) (show_ty type_fl)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;If the types of the branches do not match the above will raise a type error. Here is how it is implemented&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let on_type_er trace message = TypeError(trace,message) |&amp;gt; raise
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The exception just takes the trace and the messages and then gets raised.&lt;/p&gt;
&lt;h4&gt;Raising Type Errors&lt;/h4&gt;
&lt;p&gt;It is possible to raise a type error directly in Spiral.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;if c then
    ...
else
    error_type &quot;Some error.&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;If the conditional &lt;code&gt;c&lt;/code&gt; is dynamic then the type error will always get triggered so this would not be very useful, but plenty of times functions of such form will get used with static conditionals. Plenty of times has &lt;code&gt;assert&lt;/code&gt; been used in the tutorials.&lt;/p&gt;
&lt;p&gt;Here is how the &lt;code&gt;assert&lt;/code&gt; is implemented in the core library. There is a slightly more sophisticated version in &lt;code&gt;Extern&lt;/code&gt; calls &lt;code&gt;show&lt;/code&gt; as well.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl assert c msg = 
    inl raise = 
        if lit_is c then error_type
        else failwith unit
    
    if c = false then raise msg
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is how it is implemented in the partial evaluator.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;| ErrorType,[a] -&amp;gt; tev d a |&amp;gt; fun a -&amp;gt; on_type_er (trace d) &amp;lt;| sprintf &quot;%s&quot; (show_typedexpr a)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;So it is a simple &lt;code&gt;tev&lt;/code&gt; and then the &lt;code&gt;on_type_er&lt;/code&gt; gets called on the result of that. It is not particularly sophisticated, but then it does not have to be.&lt;/p&gt;
&lt;h3&gt;4: Boxing of Union Types&lt;/h3&gt;
&lt;p&gt;Some languages can afford to be minimalist like Lisp, but Spiral is not that sort of language. It depends on a large number of first class features working seamlessly together. Without union types and term casting it would be harder to ensure convergence and the language would be poorer as a result. The lack of such features in a language with first class staging like Spiral would immediately become obvious and the need for them would arise on its own.&lt;/p&gt;
&lt;p&gt;The way boxing works is simple. If the argument being boxed is a subset of the type, then it gets wrapped in a &lt;code&gt;TyBox&lt;/code&gt; otherwise it is a type error.&lt;/p&gt;
&lt;p&gt;Any complexity in the following function is just from needing to handle the various edge cases.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let type_box (d: LangEnv) (typec: Expr) (args: Expr): TypedExpr =
    let typec &amp;amp; TyType ty, args = tev2 d typec args
    let substitute_ty = function
        | TyBox(x,_) -&amp;gt; tybox(x,ty)
        | x -&amp;gt; tybox(x,ty)

    let (|TyRecUnion|_|) = function
        | UnionT ty' -&amp;gt; Some ty'
        | RecT key -&amp;gt; Some (set_field (rect_unbox d key))
        | _ -&amp;gt; None
    
    match ty, get_type args with
    | x, r when x = r -&amp;gt; args
    | TyRecUnion ty', UnionT ty_args when Set.isSubset ty_args ty' -&amp;gt;
        let lam = inl' [&quot;typec&quot;; &quot;args&quot;] (op(Case,[v &quot;args&quot;; type_box (v &quot;typec&quot;) (v &quot;args&quot;)])) |&amp;gt; inner_compile
        apply d (apply d lam typec) args
    | TyRecUnion ty', x when Set.contains x ty' -&amp;gt; substitute_ty args
    | _ -&amp;gt; on_type_er (trace d) &amp;lt;| sprintf &quot;Type constructor application failed. %s does not intersect %s.&quot; (show_ty ty) (get_type args |&amp;gt; show_ty)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;To start things off, here is how &lt;code&gt;tev2&lt;/code&gt; is implemented.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let inline tev2 d a b = tev d a, tev d b
let inline tev3 d a b c = tev d a, tev d b, tev d c
let inline tev4 d a b c d' = tev d a, tev d b, tev d c, tev d d'
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Not a big deal.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let typec &amp;amp; TyType ty, args = tev2 d typec args
let substitute_ty = function
    | TyBox(x,_) -&amp;gt; tybox(x,ty)
    | x -&amp;gt; tybox(x,ty)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;substitute_ty&lt;/code&gt; ty is fairly straightforward. The reason why it exists is because the argument being boxed might already by a &lt;code&gt;TyBox&lt;/code&gt; in which case the box is just replaced.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let (|TyRecUnion|_|) = function
    | UnionT ty' -&amp;gt; Some ty'
    | RecT key -&amp;gt; Some (set_field (rect_unbox d key))
    | _ -&amp;gt; None
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Since the type being boxed to must be a union type, the above active pattern is what is responsible for that. If the type being boxed is a straightforward union type, then it just returns it. But if the type being boxed is a recursive type then it is unrolled a single level and treated like a union type.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;match ty, get_type args with
| x, r when x = r -&amp;gt; args
| TyRecUnion ty', UnionT ty_args when Set.isSubset ty_args ty' -&amp;gt;
    let lam = inl' [&quot;typec&quot;; &quot;args&quot;] (op(Case,[v &quot;args&quot;; type_box (v &quot;typec&quot;) (v &quot;args&quot;)])) |&amp;gt; inner_compile
    apply d (apply d lam typec) args
| TyRecUnion ty', x when Set.contains x ty' -&amp;gt; substitute_ty args
| _ -&amp;gt; on_type_er (trace d) &amp;lt;| sprintf &quot;Type constructor application failed. %s does not intersect %s.&quot; (show_ty ty) (get_type args |&amp;gt; show_ty)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The first case is straightforward. It is true that types can only be boxed into union types, but there is no need to throw an error on &lt;code&gt;box int64 1&lt;/code&gt; for example. In that case returning the original should be fine.&lt;/p&gt;
&lt;p&gt;The second case is a bit less straightforward and deals with the case when a union type is being boxed with a larger union type as the target. Note the call to &lt;code&gt;Set.isSubset&lt;/code&gt;. The description for it says &lt;code&gt;Evaluated to 'true' if all the elements of the first set are in the second.&lt;/code&gt; What will happen in this case is that all the elements will get unboxed and then boxed into the new type.&lt;/p&gt;
&lt;p&gt;The third case is straightforward and just does the substitution.&lt;/p&gt;
&lt;p&gt;The fourth case is straightforward and just raises a type error.&lt;/p&gt;
&lt;h3&gt;5: Unboxing of Union Types&lt;/h3&gt;
&lt;p&gt;Originally CSE was added to Spiral in order to propagate unboxing information. In the following function that handles the entirety of unboxing, it will be shown how that works.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        let case_ d v case =
            let inline assume d v x branch = tev_assume (cse_add' d v x) d branch
            match tev d v with
            | a &amp;amp; TyBox(b,_) -&amp;gt; 
                cse_add d a b
                let r = tev d case
                cse_remove d a
                r
            | (TyV(_, t &amp;amp; (UnionT _ | RecT _)) | TyT(t &amp;amp; (UnionT _ | RecT _))) as v -&amp;gt;
                let make_up_vars_for_ty (l: Ty list): TypedExpr list = List.map (make_up_vars_for_ty d) l
                let map_to_cases (l: TypedExpr list): (TypedExpr * TypedExpr) list = List.map (fun x -&amp;gt; x, assume d v x case) l
                            
                match case_type d t |&amp;gt; make_up_vars_for_ty |&amp;gt; map_to_cases with
                | (_, TyType p) :: cases as cases' -&amp;gt; 
                    if List.forall (fun (_, TyType x) -&amp;gt; x = p) cases then 
                        TyOp(Case,v :: List.collect (fun (a,b) -&amp;gt; [a;b]) cases', p) 
                        |&amp;gt; make_tyv_and_push_typed_expr_even_if_unit d
                    else 
                        let l = List.map (snd &amp;gt;&amp;gt; get_type) cases'
                        on_type_er (trace d) &amp;lt;| sprintf &quot;All the cases in pattern matching clause with dynamic data must have the same type.\nGot: %s&quot; (listt l |&amp;gt; show_ty)
                | _ -&amp;gt; failwith &quot;There should always be at least one clause here.&quot;
            | _ -&amp;gt; tev d case
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Near the very top there is &lt;code&gt;tev_assume&lt;/code&gt; that was also used in &lt;code&gt;if_static&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        match tev d v with
        | a &amp;amp; TyBox(b,_) -&amp;gt; 
            d.cse_env := cse_add' d a b
            let r = tev d case
            d.cse_env := cse_remove' d a
            r
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The first case is straightforward. If the type is a &lt;code&gt;TyBox _&lt;/code&gt;, that is a staged boxed type instead of being fully boxed then the box is thrown away. &lt;code&gt;cse_env&lt;/code&gt; is used to rewrite the variable &lt;code&gt;a&lt;/code&gt; to what is inside it &lt;code&gt;b&lt;/code&gt; as it goes to evaluate case. When if finishes evaluation the rewrite is removed and the result is returned.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        let rec case_type d args_ty =
            let union_case = function
                | UnionT l -&amp;gt; Set.toList l
                | _ -&amp;gt; [args_ty]
            match args_ty with
            | RecT key -&amp;gt; union_case (rect_unbox d key)
            | x -&amp;gt; union_case x
...
            | (TyV(_, t &amp;amp; (UnionT _ | RecT _)) | TyT(t &amp;amp; (UnionT _ | RecT _))) as v -&amp;gt;
                let make_up_vars_for_ty (l: Ty list): TypedExpr list = List.map (make_up_vars_for_ty d) l
                let map_to_cases (l: TypedExpr list): (TypedExpr * TypedExpr) list = List.map (fun x -&amp;gt; x, assume d v x case) l
                            
                match case_type d t |&amp;gt; make_up_vars_for_ty |&amp;gt; map_to_cases with
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The 4 lines here make the meat of the function. First the boxed type is taken apart. If it is a union type then its member are returned in a list. If it is a recursive type then it is unrolled a level into a union type and then its members are returned into a list.&lt;/p&gt;
&lt;p&gt;Here is an example in pseudo-code.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;union {.Some, int64 | .None} -&amp;gt; [.Some, int64; .None]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Since raw types can't be in the case expression what is needed is to assign them variables. &lt;code&gt;make_up_vars_for_ty&lt;/code&gt; is what does this.&lt;/p&gt;
&lt;p&gt;Here is an example in pseudo-code.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;.Some, int64 -&amp;gt; .Some, var int64
.None -&amp;gt; .None
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Then those two members are mapped to their case bodies in &lt;code&gt;map_to_cases&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This part might be a bit confusing. There aren't any case bodies strictly speaking; there is only one &lt;code&gt;case&lt;/code&gt;. But that one &lt;code&gt;case&lt;/code&gt; is in fact enough to cover all the pattern matching needs due to the magic of intensional polymorphism and first class staging. It will be shown later how the pattern compiler works and hopefully it will be clear why this works after that. As a short summary, the patterns are all CPS'd and the static ifs suffice to cover the entirety of the pattern matching needs.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;                | (_, TyType p) :: cases as cases' -&amp;gt; 
                    if List.forall (fun (_, TyType x) -&amp;gt; x = p) cases then 
                        TyOp(Case,v :: List.collect (fun (a,b) -&amp;gt; [a;b]) cases', p) 
                        |&amp;gt; make_tyv_and_push_typed_expr_even_if_unit d
                    else 
                        let l = List.map (snd &amp;gt;&amp;gt; get_type) cases'
                        on_type_er (trace d) &amp;lt;| sprintf &quot;All the cases in pattern matching clause with dynamic data must have the same type.\nGot: %s&quot; (listt l |&amp;gt; show_ty)
                | _ -&amp;gt; failwith &quot;There should always be at least one clause here.&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;By this point, all the important work has been done and the rest is error checking and let insertion for the code generator in the next phase of compilation.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;            | _ -&amp;gt; tev d case
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;If the argument is not a caseable (union or recursive) type then the unboxing is skipped. The partial evaluation just proceeds as normal.&lt;/p&gt;
&lt;p&gt;This is the entirety of unboxing.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let inline assume d v x branch = tev_assume (cse_add' d v x) d branch
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As a recap, consider how in the tutorial chapters it was warned against using join points in tandem with partial active patterns and union types. By now it should be clear why this is so. When a join point is entered, the rewriting information in &lt;code&gt;cse_env&lt;/code&gt; is thrown away. Since &lt;code&gt;cse_env&lt;/code&gt; is used to propagate unboxing information as shown in the above function, that is where the difficulty arises.&lt;/p&gt;
&lt;p&gt;As it stands, unboxing in Spiral is fairly primitive. ML compilers go a lot further in optimizing pattern matching expressions than Spiral and in its current form, Spiral is not really suitable for writing runtime interpreters as its unboxing facilities are not developed enough.&lt;/p&gt;
&lt;p&gt;For example, it has structural equality, but...&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl ty = .A \/ .B \/ .C \/ .E
inl a = box ty .A |&amp;gt; dyn
inl b = box ty .B |&amp;gt; dyn

a = b
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;type Union0 =
    | Union0Case0
    | Union0Case1
    | Union0Case2
    | Union0Case3
let rec method_0((var_0: Union0), (var_1: Union0)): bool =
    match var_0 with
    | Union0Case0 -&amp;gt;
        match var_1 with
        | Union0Case0 -&amp;gt;
            true
        | Union0Case1 -&amp;gt;
            false
        | Union0Case2 -&amp;gt;
            false
        | Union0Case3 -&amp;gt;
            false
    | Union0Case1 -&amp;gt;
        match var_1 with
        | Union0Case0 -&amp;gt;
            false
        | Union0Case1 -&amp;gt;
            true
        | Union0Case2 -&amp;gt;
            false
        | Union0Case3 -&amp;gt;
            false
    | Union0Case2 -&amp;gt;
        match var_1 with
        | Union0Case0 -&amp;gt;
            false
        | Union0Case1 -&amp;gt;
            false
        | Union0Case2 -&amp;gt;
            true
        | Union0Case3 -&amp;gt;
            false
    | Union0Case3 -&amp;gt;
        match var_1 with
        | Union0Case0 -&amp;gt;
            false
        | Union0Case1 -&amp;gt;
            false
        | Union0Case2 -&amp;gt;
            false
        | Union0Case3 -&amp;gt;
            true
let (var_0: Union0) = Union0Case0
let (var_1: Union0) = Union0Case1
method_0((var_0: Union0), (var_1: Union0))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is n^2 compilation for structural equality for fully boxed types. It could be dealt with by introducing a different kind of case, but it is not a priority by any means at the moment.&lt;/p&gt;
&lt;p&gt;For &lt;code&gt;Result&lt;/code&gt; and &lt;code&gt;Option&lt;/code&gt; types, what Spiral has currently has is sufficient. The rest can be done by propagating types at compile time.&lt;/p&gt;
&lt;h3&gt;6: Join Points&lt;/h3&gt;
&lt;p&gt;Along with &lt;code&gt;inl&lt;/code&gt;ineables, join points are Spiral's most iconic feature and what make it the crystallization of the staged functional programming style.&lt;/p&gt;
&lt;p&gt;Currently there are 4 kinds of join points in total and this section will cover the vanilla one. The rest are quite similar to it though. The join point for term casting cannot be placed directly, instead &lt;code&gt;term_cast&lt;/code&gt; does it.&lt;/p&gt;
&lt;p&gt;Join points have a long history in Spiral. When work first started on Spiral the language originally had only inlineables, but some way of handling recursion had been necessary. So then the second thing that was added were &lt;code&gt;met&lt;/code&gt;hods. They weren't join points at first, but a different kind of function that memoizes its result using a global dictionary.&lt;/p&gt;
&lt;p&gt;There wasn't a single great point where methods suddenly evolved into join points. It took the author a considerable amount of refactoring work in order to separate the concept of functions and where they join at the conceptual level. He had never seen such a feature in any other language up to that point so it was quite hard for him to internalize them and how they fit into the language.&lt;/p&gt;
&lt;p&gt;Regardless, join points are such an integral part of the whole staged functional programming experiences that language with staging, but no join points can be considered as simply toys and disregarded based on that fact.&lt;/p&gt;
&lt;p&gt;There are 2 aspects of join points that need to be kept in mind in order to understand them.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Unused variable elimination.&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code&gt;inl a _ = ...
inl b _ = ...a... // `a` should be in `b`'s lexical environment when the function is called
inl c _ = ...a...b... // `a` and `b` should be in b's lexical environment when the function is called
...
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;inl a _ = ...
inl b _ = ... // empty env
inl c _ = ... // empty env
...
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Spiral always does unused variable filtering at function boundaries and that is necessary to guarantee predicable convergence for join points.&lt;/p&gt;
&lt;p&gt;If the environment always had everything it passed inside the join point, then those sucked in unused arguments would get specialized to as well which would not be good. This kind of optimization would not be necessary in dynamic languages, but is necessary in Spiral.&lt;/p&gt;
&lt;p&gt;Strictly speaking, Spiral does not do immediate filtration. It tends to keep a list of unused variables and delay the filtration until the last possible moment, but the semantics of immediate and lazy unused argument filtration are intended to be the same. It is just intended to be a performance optimization.&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;Renaming.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;If the filtered arguments into join points are something like &lt;code&gt;{a=var_3; b=var_6; c=var_10}&lt;/code&gt; they should be renamed to &lt;code&gt;{a=var_0; b=var_1; c=var_2}&lt;/code&gt; before the expression can be memoized. This is simple enough to do and took the author around 2.5 month in order to understand properly and that is despite thinking about it the whole time.&lt;/p&gt;
&lt;p&gt;In practice, Spiral's renamer also collects the join point arguments along the way so the traversal is not repeated. It also collects the renamed arguments as well.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    let rec renamer_apply_env' (r: EnvRenamer) (C x) = Map.map (fun k -&amp;gt; renamer_apply_typedexpr' r) x
    and renamer_apply_typedexpr' ({memo=memo; renamer=renamer; ref_call_args=call_args; ref_method_pars=method_pars} as r) e =
        let inline f e = renamer_apply_typedexpr' r e
        let inline rename (n,t as k) =
            match renamer.TryGetValue n with
            | true, v -&amp;gt; v,t
            | false, _ -&amp;gt;
                let n' = renamer.Count
                renamer.Add(n,n')
                call_args := k :: !call_args 
                let k' = n', t
                method_pars := k' :: !method_pars
                k'

        match memo.TryGetValue e with
        | true, e -&amp;gt; e
        | false, _ -&amp;gt;
            match e with
            | TyT _ -&amp;gt; e
            | TyBox (n,t) -&amp;gt; tybox(f n,t)
            | TyList l -&amp;gt; tyvv(List.map f l)
            | TyMap(l,t) -&amp;gt; tymap(renamer_apply_env' r l |&amp;gt; consify_env_term, t)
            | TyV (n,t as k) -&amp;gt;
                let n', _ as k' = rename k
                if n' = n then e else tyv k'
            | TyLit _ -&amp;gt; e
            | TyJoinPoint _ | TyOp _ | TyState _ | TyLet _ -&amp;gt; failwith &quot;Only data structures in the env can be renamed.&quot;
            |&amp;gt; fun x -&amp;gt; memo.[e] &amp;lt;- x; x
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above is in fact a map over &lt;code&gt;TypedExpr&lt;/code&gt; that just does memoization using the &lt;code&gt;memo&lt;/code&gt;. Memoization inside the renamer is an important optimization when doing renaming since the standard and core library function will repeat themselves significantly in nested functions.&lt;/p&gt;
&lt;p&gt;There isn't too much need to dwell on the above function. It is merely big, but not particularly complicated and does what it is name says.&lt;/p&gt;
&lt;p&gt;With that out of the way, here is the actual join point.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let join_point_method (d: LangEnv) (expr: Expr): TypedExpr = 
    let {call_args=call_arguments; method_pars=method_parameters; renamer'=renamer}, renamed_env = renamer_apply_env d.env
    let length = renamer.Count
    let join_point_key: Node&amp;lt;Expr * EnvTerm&amp;gt; = nodify_memo_key (expr, renamed_env)
    
    let ret_ty = 
        let d = {d with env=renamed_env; ltag=ref length}
        let join_point_dict: Dictionary&amp;lt;_,_&amp;gt; = join_point_dict_method
        match join_point_dict.TryGetValue join_point_key with
        | false, _ -&amp;gt;
            join_point_dict.[join_point_key] &amp;lt;- JoinPointInEvaluation ()
            let typed_expr = tev_method d expr
            join_point_dict.[join_point_key] &amp;lt;- JoinPointDone (method_parameters, typed_expr)
            typed_expr
        | true, JoinPointInEvaluation _ -&amp;gt; 
            tev_rec d expr
        | true, JoinPointDone (used_vars, typed_expr) -&amp;gt; 
            typed_expr
        |&amp;gt; get_type

    ty_join_point join_point_key JoinPointMethod call_arguments ret_ty 
    |&amp;gt; make_tyv_and_push_typed_expr_even_if_unit d
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;let join_point_key: Node&amp;lt;Expr * EnvTerm&amp;gt; = nodify_memo_key (expr, renamed_env)&lt;/code&gt; line is a performance optimization so dictionary keys can be compared quickly. The actual key is &lt;code&gt;Expr * EnvTerm&lt;/code&gt;, but a wrapper is put around it that ensures fast comparison.&lt;/p&gt;
&lt;p&gt;Here is the &lt;code&gt;Node&lt;/code&gt; type.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;type Node&amp;lt;'a&amp;gt;(expr:'a, symbol:int) = 
    member x.Expression = expr
    member x.Symbol = symbol
    override x.ToString() = sprintf &quot;&amp;lt;tag %i&amp;gt;&quot; symbol
    override x.GetHashCode() = symbol
    override x.Equals(y) = 
        match y with 
        | :? Node&amp;lt;'a&amp;gt; as y -&amp;gt; symbol = y.Symbol
        | _ -&amp;gt; failwith &quot;Invalid equality for Node.&quot;

    interface IComparable with
        member x.CompareTo(y) = 
            match y with
            | :? Node&amp;lt;'a&amp;gt; as y -&amp;gt; compare symbol y.Symbol
            | _ -&amp;gt; failwith &quot;Invalid comparison for Node.&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;What &lt;code&gt;Node&lt;/code&gt; does is allows two arbitrary expressions to be assigned an unique &lt;code&gt;symbol&lt;/code&gt; and be compared using that.&lt;/p&gt;
&lt;p&gt;After renaming and making the &lt;code&gt;join_point_key&lt;/code&gt;, the body of the join point is what comes.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let ret_ty = 
    let d = {d with env=renamed_env; ltag=ref length}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Now the environment is assigned the renamed environment and the tag reference which is incremented every time a new variable is made is set to one past the highest argument in scope.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let join_point_dict: Dictionary&amp;lt;_,_&amp;gt; = join_point_dict_method
match join_point_dict.TryGetValue join_point_key with
| false, _ -&amp;gt;
    join_point_dict.[join_point_key] &amp;lt;- JoinPointInEvaluation ()
    let typed_expr = tev_method d expr
    join_point_dict.[join_point_key] &amp;lt;- JoinPointDone (method_parameters, typed_expr)
    typed_expr
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is the heart of the join point. In the first case which is hit when the join point is entered the first time, it sets the dictionary to indicate that the join point is being evaluated in case it is ever called recursively.&lt;/p&gt;
&lt;p&gt;Then it calls &lt;code&gt;tev_method&lt;/code&gt;. That particular method is similar to &lt;code&gt;tev_seq&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let inline tev_seq d expr = let d = {d with seq=ref id; cse_env=ref !d.cse_env} in tev d expr |&amp;gt; apply_seq d
let inline tev_assume cse_env d expr = let d = {d with seq=ref id; cse_env=ref cse_env} in tev d expr |&amp;gt; apply_seq d
let inline tev_method d expr = let d = {d with seq=ref id; cse_env=ref Map.empty} in tev d expr |&amp;gt; apply_seq d
let inline tev_rec d expr = let d = {d with seq=ref id; cse_env=ref Map.empty; rbeh=AnnotationReturn} in tev d expr |&amp;gt; apply_seq d
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As can be seen when entering a new scope, all 4 functions set the &lt;code&gt;seq&lt;/code&gt; to identity. &lt;code&gt;tev_seq&lt;/code&gt; and &lt;code&gt;tev_assume&lt;/code&gt; when entering a new scope make a copy of the &lt;code&gt;cse_env&lt;/code&gt;. &lt;code&gt;tev_method&lt;/code&gt; and &lt;code&gt;tev_rec&lt;/code&gt; which are join point functions outright set it to empty.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;tev_method&lt;/code&gt; finishes running and returns the body of the join point, the join point is set to finished and the &lt;code&gt;typed_expr&lt;/code&gt; is returned from the branch.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;| true, JoinPointInEvaluation _ -&amp;gt; 
    tev_rec d expr
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This particular case can only ever happen during recursion. If it hits it means the function has been called recursively. In order for it to not diverge it needs to get the return type.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;tev_rec&lt;/code&gt; is exactly like &lt;code&gt;tev_method&lt;/code&gt; except for setting the return behavior to &lt;code&gt;rbeh=AnnotationReturn&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There is only a single place where &lt;code&gt;rbeh&lt;/code&gt; ever comes into play in the language. In the type annotation function.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let type_annot d a b =
    match d.rbeh with
    | AnnotationReturn -&amp;gt; tev_annot {d with rbeh=AnnotationDive} b
    | AnnotationDive -&amp;gt;
        let a, b = tev d a, tev_annot d b
        let ta, tb = get_type a, get_type b
        if ta = tb then a else on_type_er (trace d) &amp;lt;| sprintf &quot;Type annotation mismatch.\n%s &amp;lt;&amp;gt; %s&quot; (show_ty ta) (show_ty tb)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;When &lt;code&gt;rbeh&lt;/code&gt; is set to annotation return then it won't evaluate the body, but will just evaluate the right side of the annotation instead.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let inline tev_seq d expr = let d = {d with seq=ref id; cse_env=ref !d.cse_env} in tev d expr |&amp;gt; apply_seq d
let inline tev_annot d expr = let d = {d with seq=ref id; cse_env=ref !d.cse_env} in tev d expr
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;tev_annot&lt;/code&gt; is identical to &lt;code&gt;tev_seq&lt;/code&gt; except it throws away the intermediate statements and only returns the final expression.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        | true, JoinPointDone (used_vars, typed_expr) -&amp;gt; 
            typed_expr
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The last case does no evaluation and just returns the body.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        |&amp;gt; get_type

    ty_join_point join_point_key JoinPointMethod call_arguments ret_ty 
    |&amp;gt; make_tyv_and_push_typed_expr_even_if_unit d
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is the final stretch. The returned body gets converted into the return type &lt;code&gt;ret_ty&lt;/code&gt; and then a join point is made and let inserted.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let ty_join_point key jp_type args ret_type = TyJoinPoint(key,jp_type,args,ret_type)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It might be more readable to make the join point directly, but a lot of the functions in the Spiral compiler that essentially act as the identity might have been been doing memoization in the past, so the author prefers to leave them alone in case inspiration hits him in order to make redesign easier. At any rate, the final part of the join point is extremely straightforward.&lt;/p&gt;
&lt;p&gt;Before this chapter is done, a note is needed regarding the join point dictionaries.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;// #Main
let spiral_peval (settings: CompilerSettings) (Module(N(module_name,_,_,_)) as module_main) = 
    let join_point_dict_method = d0()
    let join_point_dict_closure = d0()
    let join_point_dict_type = d0()
    let join_point_dict_cuda = d0()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;They are initialized at the very start of compilation and hash via structural equality. Because passing state would be annoying otherwise and since they are only ever added to, they are present during the entire compilation. Parsing and prepass does not touch them, but the code generator reads from them.&lt;/p&gt;
&lt;p&gt;Apart from the types and the core library, the entire compiler is a part of a single 3200 line function. The author didn't like stepping into that at first, but once he did so he found that the arrangement works surprisingly well and has had no trouble with it since then.&lt;/p&gt;
&lt;h3&gt;7: The Prepass and Unused Variable Filtering&lt;/h3&gt;
&lt;p&gt;The prepass works before partial evaluation, but after parsing. Its goal is to convert &lt;code&gt;Function&lt;/code&gt;s to &lt;code&gt;FunctionFilt&lt;/code&gt;s which have the set of used variables in their bodies. It also calls &lt;code&gt;pattern_compile&lt;/code&gt; in order to compile the &lt;code&gt;Pattern&lt;/code&gt;s into &lt;code&gt;Expr&lt;/code&gt;s.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;and Expr = 
    | V of Node&amp;lt;string&amp;gt;
    | Lit of Node&amp;lt;Value&amp;gt;
    | Pattern of Node&amp;lt;Pattern&amp;gt;
    | Function of Node&amp;lt;FunctionCore&amp;gt;
    | FunctionFilt of Node&amp;lt;Set&amp;lt;string&amp;gt; * Node&amp;lt;FunctionCore&amp;gt;&amp;gt;
    | VV of Node&amp;lt;Expr list&amp;gt;
    | Op of Node&amp;lt;Op * Expr list&amp;gt;
    | ExprPos of Pos&amp;lt;Expr&amp;gt;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The way it works is rather simple. Prepass is a rightwards fold + map over &lt;code&gt;Expr&lt;/code&gt;. It also does memoization.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    and expr_prepass e =
        e |&amp;gt; memoize prepass_dict (fun e -&amp;gt;
            let inline f e = expr_prepass e
            match e with
            | V (N n) -&amp;gt; Set.singleton n, e
            | Op(N(op',l)) -&amp;gt;
                let l,l' = List.map f l |&amp;gt; List.unzip
                Set.unionMany l, op(op',l')
            | VV (N l) -&amp;gt; 
                let l,l' = List.map f l |&amp;gt; List.unzip
                Set.unionMany l, vv l'
            | FunctionFilt(N (vars,N(name,body))) -&amp;gt;
                Set.remove name vars, e
            | Function(N(name,body)) -&amp;gt;
                let vars,body = f body
                Set.remove name vars, func_filt(vars,nodify_func(name,body))
            | Lit _ -&amp;gt; Set.empty, e
            | Pattern pat -&amp;gt; pattern_compile pat
            | ExprPos p -&amp;gt; 
                let vars, body = f p.Expression
                vars, expr_pos p.Pos body
            )
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In a nutshell, if it find a &lt;code&gt;V _&lt;/code&gt; then it adds it to the set. If it finds a function it removes variable with the name of its binding from the set. It does it top to bottom so the thing works.&lt;/p&gt;
&lt;p&gt;With this in mind, now it can be revealed what &lt;code&gt;EnvTerm&lt;/code&gt; is.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;and EnvTerm = 
| EnvConsed of ConsedNode&amp;lt;Map&amp;lt;string, TypedExpr&amp;gt;&amp;gt;
| Env of Map&amp;lt;string, TypedExpr&amp;gt;
| EnvUnfiltered of Map&amp;lt;string, TypedExpr&amp;gt; * Set&amp;lt;string&amp;gt;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In the previous versions of Spiral, the unused arguments were filtered every time &lt;code&gt;FunctionFilt&lt;/code&gt; was reached, but the author decided he did not want the entire environment to be iterated over every time so he decided to add some laziness to the mix.&lt;/p&gt;
&lt;p&gt;Every time &lt;code&gt;EnvTerm&lt;/code&gt; is accessed it is either converted to &lt;code&gt;Env&lt;/code&gt; if it is unfiltered or a check is made first against the used variable set. &lt;code&gt;EnvConsed&lt;/code&gt; is similar to &lt;code&gt;Env&lt;/code&gt;. The environment is &lt;a href=&quot;https://en.wikipedia.org/wiki/Hash_consing&quot; rel=&quot;nofollow&quot;&gt;hash consed&lt;/a&gt; in the renamer as a performance optimization. To see how that works check out &lt;code&gt;SpiralHashConsing.fs&lt;/code&gt;. For the understanding of language semantics understanding hash consing is not particularly important. It might be worth reevaluating whether hash consing is worth having since the evaluator has went through several iteration since that was useful. It might turn out that reference equality in the renamer is all that is needed.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let c = function
| Env env -&amp;gt; env
| EnvUnfiltered (env,used_vars) -&amp;gt; Map.filter (fun k _ -&amp;gt; used_vars.Contains k) env
| EnvConsed env -&amp;gt; env.node

let (|C|) x = c x
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Whenever &lt;code&gt;C env&lt;/code&gt; is seen as a part of the pattern, this is what happens. If it is unfiltered, then it gets filtered otherwise it is just extracted.&lt;/p&gt;
&lt;p&gt;This might seem complicated, but the type system takes care that no mistakes are made when dealing with environments. It was really trivial to move from immediate to lazy filtering.&lt;/p&gt;
&lt;h3&gt;8: Pattern Compilation&lt;/h3&gt;
&lt;pre&gt;
&lt;code&gt;let pattern_dict = d0()
let rec pattern_compile (pat: Node&amp;lt;_&amp;gt;) = 
    pat |&amp;gt; memoize pattern_dict (fun pat -&amp;gt;
        let node = pat.Symbol
        let pat = pat.Expression

        let new_pat_var =
            let mutable i = 0
            let get_pattern_tag () = 
                let x = i
                i &amp;lt;- i + 1
                x
            fun () -&amp;gt; sprintf &quot; pat_var_%i_%i&quot; node (get_pattern_tag())

        let rec pattern_compile (arg: Expr) (pat: Pattern) (on_succ: Expr) (on_fail: Expr): Expr =
            let inline cp arg pat on_succ on_fail = pattern_compile arg pat on_succ on_fail

            let pat_tuple_helper l =
                List.foldBack (fun pat (c,s,on_succ) -&amp;gt; 
                    let arg = new_pat_var()
                    c + 1, arg :: s,cp (v arg) pat on_succ on_fail) l (0,[],on_succ)
        
            match pat with
            | PatClauses l -&amp;gt; List.foldBack (fun (pat, exp) on_fail -&amp;gt; cp arg pat exp on_fail) l on_fail
            | PatE -&amp;gt; on_succ
            | PatVar x -&amp;gt; l x arg on_succ
            | PatTypeEq (exp,typ) -&amp;gt;
                let on_succ = cp arg exp on_succ on_fail
                if_static (eq_type arg typ) on_succ on_fail
                |&amp;gt; case arg
            | PatTuple l -&amp;gt; 
                let count, args, on_succ = pat_tuple_helper l
                list_taken_cps count arg on_fail (inl' args on_succ) |&amp;gt; case arg
            | PatCons l -&amp;gt; 
                let count, args, on_succ = pat_tuple_helper l
                list_taken_tail_cps count arg on_fail (inl' args on_succ) |&amp;gt; case arg
            | PatActive (a,b) -&amp;gt;
                let pat_var = new_pat_var()
                l pat_var (ap (v a) arg) (cp (v pat_var) b on_succ on_fail)
            | PatPartActive (a,pat) -&amp;gt; 
                let pat_var = new_pat_var()
                let on_succ = inl pat_var (cp (v pat_var) pat on_succ on_fail)
                let on_fail = inl &quot;&quot; on_fail
                ap' (v a) [arg; on_fail; on_succ]
            | PatOr l -&amp;gt; List.foldBack (fun pat on_fail -&amp;gt; cp arg pat on_succ on_fail) l on_fail
            | PatAnd l -&amp;gt; List.foldBack (fun pat on_succ -&amp;gt; cp arg pat on_succ on_fail) l on_succ
            | PatNot p -&amp;gt; cp arg p on_fail on_succ // switches the on_fail and on_succ arguments
            | PatXor l -&amp;gt;
                let state_var = new_pat_var()
                let state_var' = v state_var
                let bool x = lit &amp;lt;| LitBool x
                let rec just_one = function
                    | x :: xs -&amp;gt; 
                        let xs = just_one xs
                        inl state_var 
                            (cp arg x 
                                (if_static state_var' on_fail (ap xs (bool true))) // true case
                                (ap xs state_var')) // false case
                    | [] -&amp;gt; inl state_var (if_static state_var' on_succ on_fail)
                ap (just_one l) (bool false)
            | PatLit x -&amp;gt; 
                let x = lit x
                let on_succ = if_static (eq arg x) on_succ on_fail
                if_static (eq_type arg x) on_succ on_fail |&amp;gt; case arg
            | PatTypeLit x -&amp;gt; 
                if_static (eq_type arg (type_lit_lift x)) on_succ on_fail 
                |&amp;gt; case arg
            | PatTypeLitBind x -&amp;gt; 
                if_static (type_lit_is arg) (l x (type_lit_cast arg) on_succ) on_fail 
                |&amp;gt; case arg
            | PatWhen (p, e) -&amp;gt; cp arg p (if_static e on_succ on_fail) on_fail
            | PatModuleIs p -&amp;gt; module_is_cps arg on_fail (cp arg p on_succ on_fail) |&amp;gt; case arg
            | PatModuleMember name -&amp;gt; module_member_cps arg name on_fail (inl name on_succ) |&amp;gt; case arg
            | PatModuleRebind(name,b) -&amp;gt; 
                let arg' = new_pat_var()    
                module_member_cps arg name on_fail (inl arg' (cp (v arg') b on_succ on_fail)) 
                |&amp;gt; case arg
            | PatPos p -&amp;gt; expr_pos p.Pos (cp arg p.Expression on_succ on_fail)
            | PatTypeTermFunction(a,b) -&amp;gt; 
                let va, vb = new_pat_var(), new_pat_var()
                term_fun_dom_range_cps arg on_fail 
                &amp;lt;| inl' [va; vb] (cp (v va) a (cp (v vb) b on_succ on_fail) on_fail)
                
        let main_arg = new_pat_var()
        let arg = v main_arg
                
        let pattern_compile_def_on_succ = op(ErrorPatClause,[])
        let pattern_compile_def_on_fail = op(ErrorPatMiss,[arg])
        inl main_arg (pattern_compile arg pat pattern_compile_def_on_succ pattern_compile_def_on_fail) |&amp;gt; expr_prepass
        )
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The pattern compilation function is a rather large function so it is worth going on it in turn. Its purpose is to turn a &lt;code&gt;Pattern&lt;/code&gt; into an &lt;code&gt;Expr&lt;/code&gt;ession for easier compilation down the road.&lt;/p&gt;
&lt;p&gt;As a brief history of its development, this way of compiling or rather dealing with patterns came only after 4.5 months into Spiral's development and it represents one of the big breakthroughs that gives Spiral the expressiveness that it has now.&lt;/p&gt;
&lt;p&gt;Before that Spiral had pattern matching and had so from the start, but instead of compiling &lt;code&gt;Pattern&lt;/code&gt;s to &lt;code&gt;Expr&lt;/code&gt;s what it did was try to evaluate them directly in the evaluator. That made dealing with them really, really difficult. Essentially, having another interpreted language directly in the evaluator made the patterns very non-composable and isolated from the rest of the language and during those times as a result Spiral could only deal with simple patterns that could be dealt completely at compile time. Without compiling them it would have been very difficult to implement partially static patterns such as for union types.&lt;/p&gt;
&lt;p&gt;Also not doing compilation would necessitate that the prepass also take care of propagating variables through the &lt;code&gt;Pattern&lt;/code&gt; which made it much more complicated.&lt;/p&gt;
&lt;p&gt;Dealing with control flow in &lt;code&gt;Pattern&lt;/code&gt;s is especially difficult when they are interpreted.&lt;/p&gt;
&lt;p&gt;What &lt;code&gt;pattern_compile&lt;/code&gt; does is not just convert them to &lt;code&gt;Expr&lt;/code&gt;, but while it is doing that it also CPS's them.&lt;/p&gt;
&lt;p&gt;It is worth going through the function top to bottom.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let pattern_dict = d0()
let rec pattern_compile (pat: Node&amp;lt;_&amp;gt;) = 
    pat |&amp;gt; memoize pattern_dict (fun pat -&amp;gt;
        let node = pat.Symbol
        let pat = pat.Expression

        let new_pat_var =
            let mutable i = 0
            let get_pattern_tag () = 
                let x = i
                i &amp;lt;- i + 1
                x
            fun () -&amp;gt; sprintf &quot; pat_var_%i_%i&quot; node (get_pattern_tag())
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is its entry. In order to take a precaution that a pattern is only compiled once, it is memoized. It slips the authors mind whether this is actually necessary, but here it is.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;new_pat_var&lt;/code&gt; is just a tagger function that returns fresh names.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        let rec pattern_compile (arg: Expr) (pat: Pattern) (on_succ: Expr) (on_fail: Expr): Expr =
            let inline cp arg pat on_succ on_fail = pattern_compile arg pat on_succ on_fail
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;If there is a programing problem, the continuation passing style probably has a solution for it. Note that using the &lt;code&gt;on_fail on_succ&lt;/code&gt; order is preferable to &lt;code&gt;on_succ on_fail&lt;/code&gt;, but the author got careless this time. This would be fixable, but the type system will not be as helpful as usual because both are of type &lt;code&gt;Expr&lt;/code&gt; so refactoring is dangerous here. Modules were added in Spiral in order to make deciding function argument order easier and take care of situations like this. The modules are also there for extensibility. In F# since the type system is always on hand it is better to rely on it instead, though the author does hope that Spiral will inspire F# to improve its record syntax.&lt;/p&gt;
&lt;p&gt;Instead of diving directly into the function, it would be worth to start near the beginning just before it is called.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;            let main_arg = new_pat_var()
            let arg = v main_arg
                    
            let pattern_compile_def_on_succ = op(ErrorPatClause,[])
            let pattern_compile_def_on_fail = op(ErrorPatMiss,[arg])
            inl main_arg (pattern_compile arg pat pattern_compile_def_on_succ pattern_compile_def_on_fail) |&amp;gt; expr_prepass
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Always, the pattern is wrapped in an inlineable, and its main argument is passed into the function. By default, the on_succ and on_fail are errors which will trigger if the evaluator ever reaches them.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PatClauses&lt;/code&gt; is always the first to trigger and is never found inside another pattern.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;| PatClauses of (Pattern * Expr) list
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;            match pat with
            | PatClauses l -&amp;gt; List.foldBack (fun (pat, exp) on_fail -&amp;gt; cp arg pat exp on_fail) l on_fail
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Compilation of the pattern is a rightwards fold, meaning it proceeds from the end to the beginning.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PatClauses&lt;/code&gt; could be simplified by turning it into a combination of the &lt;code&gt;PatAnd&lt;/code&gt; and the &lt;code&gt;PatOr&lt;/code&gt; patterns. The way patterns work is as follows - if the pattern is successful, in the later stage the evaluator will call &lt;code&gt;on_succ&lt;/code&gt;. Hence why &lt;code&gt;exp&lt;/code&gt; replaces the default &lt;code&gt;on_succ&lt;/code&gt; in &lt;code&gt;PatClauses&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;On the other hand if the evaluator fail to find a match, it will call &lt;code&gt;on_fail&lt;/code&gt;. It can be visualized like so:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;pat1 pat2 pat3 -&amp;gt; ...
pat4 -&amp;gt; ...
pat5 -&amp;gt; ...
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Evaluator always starts at &lt;code&gt;pat1&lt;/code&gt; and then moves to &lt;code&gt;pat2&lt;/code&gt; if is successful. If it is a failure it goes to &lt;code&gt;pat4&lt;/code&gt; and works from there.&lt;/p&gt;
&lt;p&gt;That the &lt;code&gt;pattern_compile&lt;/code&gt; does is through the magic of CPS give the evaluator a choice of simply having the continuations there for it to call. It compiles the patterns into such a form that the desired behavior happens.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;| PatE -&amp;gt; on_succ
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Suppose a pattern like &lt;code&gt;inl _ -&amp;gt; body&lt;/code&gt; directly in the language. There is only one clause and only &lt;code&gt;PatE&lt;/code&gt; here.&lt;/p&gt;
&lt;p&gt;So based on what is known it could be expected that it would get compiled as something like &lt;code&gt;inl main_arg -&amp;gt; on_succ&lt;/code&gt;. &lt;code&gt;on_succ&lt;/code&gt; is just the body here and so it works.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;| PatVar x -&amp;gt; l x arg on_succ
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;PatVar&lt;/code&gt; is similar, but it it the arg must be bound to &lt;code&gt;x&lt;/code&gt; first.&lt;/p&gt;
&lt;p&gt;So &lt;code&gt;inl x -&amp;gt; body&lt;/code&gt; would be compiled to something like (in pseudo-code):&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl pat_var_0_0 -&amp;gt; 
    inl x = pat_var_0_0
    body
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is how &lt;code&gt;l&lt;/code&gt;et is implemented in the language.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let l v b e = ap (inl v e) b
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Let statements can easily be desugared into function abstraction and then application. In HM style languages there are some advantages to having separate let statements for the sake of generalization, but not in Spiral so it choses the easiest route. It made debugging a pain since it turns the program inside out and hence unprintable in raw form, but that does not matter anymore.&lt;/p&gt;
&lt;h4&gt;Type Equality Pattern&lt;/h4&gt;
&lt;pre&gt;
&lt;code&gt;            | PatTypeEq (exp,typ) -&amp;gt;
                let on_succ = cp arg exp on_succ on_fail
                if_static (eq_type arg typ) on_succ on_fail
                |&amp;gt; case arg
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Type equality is the &lt;code&gt;:&lt;/code&gt; operator when on the pattern side. How this works could be written directly in the language itself as the following.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;function
| _: int64 -&amp;gt; body1
| _: float32 -&amp;gt; body2
| _: string -&amp;gt; body3
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;inl pat_var_1_0 -&amp;gt;
    if eq_type pat_var_1_0 int64 then body1
    elif eq_type pat_var_1_0 float32 then body2
    elif eq_type pat_var_1_0 string then body3
    else on_fail // pattern miss
&lt;/code&gt;
&lt;/pre&gt;
&lt;h4&gt;Tuples&lt;/h4&gt;
&lt;pre&gt;
&lt;code&gt;let list_taken_cps count arg on_fail on_succ = op(ListTakeNCPS,[lit (LitInt32 count);arg;on_fail;on_succ])
let list_taken_tail_cps count arg on_fail on_succ = op(ListTakeNTailCPS,[lit (LitInt32 (count-1));arg;on_fail;on_succ])
...
            let pat_tuple_helper l =
                List.foldBack (fun pat (c,s,on_succ) -&amp;gt; 
                    let arg = new_pat_var()
                    c + 1, arg :: s,cp (v arg) pat on_succ on_fail) l (0,[],on_succ)
...
            | PatTuple l -&amp;gt; 
                let count, args, on_succ = pat_tuple_helper l
                list_taken_cps count arg on_fail (inl' args on_succ) |&amp;gt; case arg
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;pat_tuple_helper&lt;/code&gt; takes the count of the pattern, makes up new pattern variables for each of the pattern arguments and also recursively maps the inner pattern.&lt;/p&gt;
&lt;p&gt;Here is an example of how this would work. The &lt;code&gt;|&amp;gt; case&lt;/code&gt; will be pretended not to exist for the sake of brevity.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl a,b,c -&amp;gt; body
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;inl pat_var_2_0 -&amp;gt; // pat_var_2_0 is main_arg
    !ListTakeNCPS(
        3i32
        ,pat_var_2_0 
        ,inl pat_var_2_1 pat_var_2_2 pat_var_2_3 -&amp;gt; 
            inl a = pat_var_2_1
            inl b = pat_var_2_2
            inl c = pat_var_2_3
            body
        ,on_fail // pattern miss
        )
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As you can see in the above, the pattern compiler is a bit stupid and is doing redundant bindings. This has no runtime impact as &lt;code&gt;inl&lt;/code&gt;ineables get inlined period, but does cause slowdown at compile time.&lt;/p&gt;
&lt;p&gt;Here an example with multiple patterns.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;function
| a,b,c -&amp;gt; body1
| q,w -&amp;gt; body2
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;inl pat_var_3_0 -&amp;gt; // pat_var_3_0 is main_arg
    !ListTakeNCPS(
        3i32
        ,pat_var_3_0
        ,inl pat_var_3_1 pat_var_3_2 pat_var_3_3 -&amp;gt; 
            inl a = pat_var_3_1
            inl b = pat_var_3_2
            inl c = pat_var_3_3
            body1
        ,!ListTakeNCPS(
            2i32
            ,pat_var_3_0
            ,inl pat_var_3_4 pat_var_3_5 -&amp;gt; 
                inl q = pat_var_3_4
                inl w = pat_var_3_5
                body2
            ,on_fail // pattern_miss
            )
        )
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is how &lt;code&gt;ListTakeN&lt;/code&gt; operation is implemented in the evaluator.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let inline list_taken_template op_name loop d a arg on_fail on_succ = 
    match tev d a with
    | TyLitIndex c -&amp;gt; 
        match tev d arg with
        | TyList args -&amp;gt; loop [] (c,args)
        | _ -&amp;gt; tev d on_fail
    | x -&amp;gt; on_type_er (trace d) &quot;Expected an int literal as the first input to %s.\nGot: %s&quot; op_name (show_typedexpr x)

let list_taken d a arg on_fail on_succ =
    let rec loop args = function
        | 0,[] -&amp;gt; List.foldBack (fun arg on_succ -&amp;gt; apply d on_succ arg) args (tev d on_succ)
        | _,[] | 0, _ -&amp;gt; tev d on_fail
        | c,x :: x' -&amp;gt; loop (x :: args) (c-1,x')
    list_taken_template &quot;ListTakeN&quot; loop d a arg on_fail on_succ
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Figuring this out will take some specialization by hand.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let list_taken (d: LangEnv) (a: Expr) (arg: Expr) (on_fail: Expr) (on_succ: Expr) = 
    match tev d a with
    | TyLitIndex c -&amp;gt; 
        match tev d arg with
        | TyList args -&amp;gt; 
            let rec loop args = function
                | 0,[] -&amp;gt; List.foldBack (fun arg on_succ -&amp;gt; apply d on_succ arg) args (tev d on_succ)
                | _,[] | 0, _ -&amp;gt; tev d on_fail
                | c,x :: x' -&amp;gt; loop (x :: args) (c-1,x')
            loop [] (c,args)
        | _ -&amp;gt; tev d on_fail
    | x -&amp;gt; on_type_er (trace d) &quot;Expected an int literal as the first input to ListTakeN.\nGot: %s&quot; (show_typedexpr x)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The recursive loop is a bit convoluted so it is worth going through step by step. The loop can be split into two phases:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Making sure that the tuple has exactly the specified number of arguments.&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code&gt;                | _,[] | 0, _ -&amp;gt; tev d on_fail
                | c,x :: x' -&amp;gt; loop (x :: args) (c-1,x')
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;That is ensured by this particular segment.&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;Assuming the number arguments in the tuple is correct, then they are all applied to the &lt;code&gt;on_succ&lt;/code&gt; in correct order.&lt;/li&gt;
&lt;/ol&gt;&lt;pre&gt;
&lt;code&gt;                | 0,[] -&amp;gt; List.foldBack (fun arg on_succ -&amp;gt; apply d on_succ arg) args (tev d on_succ)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Why does the loop reverse the argument list before applying it? Absolutely no reason.&lt;/p&gt;
&lt;p&gt;If this were the tutorials, the author would quietly fix this, but since the user guide is meant for power users, it would be worth showing how the compiler can be improved in action. Here is the ammended function.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let list_taken (d: LangEnv) (a: Expr) (arg: Expr) (on_fail: Expr) (on_succ: Expr) = 
    match tev d a with
    | TyLitIndex c -&amp;gt; 
        match tev d arg with
        | TyList args -&amp;gt; 
            let rec loop = function
                | 0,[] -&amp;gt; List.fold (fun on_succ arg -&amp;gt; apply d on_succ arg) (tev d on_succ) args
                | _,[] | 0, _ -&amp;gt; tev d on_fail
                | c,_ :: x' -&amp;gt; loop (c-1,x')
            loop (c,args)
        | _ -&amp;gt; tev d on_fail
    | x -&amp;gt; on_type_er (trace d) &quot;Expected an int literal as the first input to ListTakeN.\nGot: %s&quot; (show_typedexpr x)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This actually sped up the compiler by a few % since the tuple pattern is so frequently used.&lt;/p&gt;
&lt;p&gt;Moving on to the cons pattern.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;            | PatCons l -&amp;gt; 
                let count, args, on_succ = pat_tuple_helper l
                list_taken_tail_cps count arg on_fail (inl' args on_succ) |&amp;gt; case arg
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It is quite similar to the standard tuple pattern.&lt;/p&gt;
&lt;p&gt;So is how it is implemented in the evaluator.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let list_taken_tail d a arg on_fail on_succ = 
    match tev d a with
    | TyLitIndex c -&amp;gt; 
        match tev d arg with
        | TyList args -&amp;gt; 
            let rec loop args = function
                | 0,x' -&amp;gt; List.foldBack (fun arg on_succ -&amp;gt; apply d on_succ arg) (tyvv x' :: args) (tev d on_succ)
                | _,[] -&amp;gt; tev d on_fail
                | c,x :: x' -&amp;gt; loop (x :: args) (c-1,x')
            loop [] (c,args)
        | _ -&amp;gt; tev d on_fail
    | x -&amp;gt; on_type_er (trace d) &quot;Expected an int literal as the first input to ListTakeNTail.\nGot: %s&quot; (show_typedexpr x)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Note that in the cons pattern now collecting the leftover arguments has a definite purpose.&lt;/p&gt;
&lt;h4&gt;Active Patterns&lt;/h4&gt;
&lt;pre&gt;
&lt;code&gt;            | PatActive (a,b) -&amp;gt;
                let pat_var = new_pat_var()
                l pat_var (ap (v a) arg) (cp (v pat_var) b on_succ on_fail)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is an example how this works.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl !dyn x -&amp;gt; body
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;inl pat_var_4_0 -&amp;gt;
    inl pat_var_1 = dyn pat_var_4_0
    inl x = pat_var_1
    body
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is the partial active pattern.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;            | PatPartActive (a,pat) -&amp;gt; 
                let pat_var = new_pat_var()
                let on_succ = inl pat_var (cp (v pat_var) pat on_succ on_fail)
                let on_fail = inl &quot;&quot; on_fail
                ap' (v a) [arg; on_fail; on_succ]
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is an example of its compilation in action.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl f arg on_fail on_succ = on_succ arg
inl @f x -&amp;gt; body
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;inl pat_var_5_0 -&amp;gt;
    inl on_succ pat_var_5_1 = 
        inl x = pat_var_5_1
        body
    inl on_fail _ = on_fail // pattern miss
    f pat_var_5_0 on_fail on_succ
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Spiral's ability to propagate information deeply makes it easy to compile active patterns to CPS'd control flow.&lt;/p&gt;
&lt;h4&gt;Boolean Patterns&lt;/h4&gt;
&lt;pre&gt;
&lt;code&gt;            | PatOr l -&amp;gt; List.foldBack (fun pat on_fail -&amp;gt; cp arg pat on_succ on_fail) l on_fail
            | PatAnd l -&amp;gt; List.foldBack (fun pat on_succ -&amp;gt; cp arg pat on_succ on_fail) l on_succ
            | PatNot p -&amp;gt; cp arg p on_fail on_succ // switches the on_fail and on_succ arguments
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;PatOr&lt;/code&gt; links the patterns through the &lt;code&gt;on_fail&lt;/code&gt;. &lt;code&gt;PatAnd&lt;/code&gt; links the arguments through the &lt;code&gt;on_succ&lt;/code&gt;. &lt;code&gt;PatNot&lt;/code&gt; just switches the arguments.&lt;/p&gt;
&lt;p&gt;The examples won't be provided for the above 3, the users can rest assured that they are doing their job.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;            | PatXor l -&amp;gt;
                let state_var = new_pat_var()
                let state_var' = v state_var
                let bool x = lit &amp;lt;| LitBool x
                let rec just_one = function
                    | x :: xs -&amp;gt; 
                        let xs = just_one xs
                        inl state_var 
                            (cp arg x 
                                (if_static state_var' on_fail (ap xs (bool true))) // true case
                                (ap xs state_var')) // false case
                    | [] -&amp;gt; inl state_var (if_static state_var' on_succ on_fail)
                ap (just_one l) (bool false)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;PatXor&lt;/code&gt; is definitely the most complex of all the patterns. Currently it can only be placed inside the module pattern. It is also misleadingly named as unlike the boolean pattern, it cannot be used to flip back and forth.&lt;/p&gt;
&lt;p&gt;For &lt;code&gt;pat0 ^ pat1 ... ^ patn&lt;/code&gt; it ensures that only one of the patterns triggers. And is false when there are no patterns.&lt;/p&gt;
&lt;p&gt;It is a bit annoying to sketch out what it does hand by hand, so as an alternative consider this example written in F#.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let rec just_one state = function
    | true :: xs -&amp;gt; if state then false else just_one true xs
    | false :: xs -&amp;gt; just_one state xs
    | [] -&amp;gt; state

just_one false [] // false
just_one false [true;false] // true
just_one false [false;false] // false
just_one false [false;true;false] // true
just_one false [false;false;true] // true
just_one false [true;false;true] // false
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;What the &lt;code&gt;PatXor&lt;/code&gt; does is translate the above into execution flow for the evaluator. It is just one example of how compilation makes composing languages much easier than interpretation. This is also an argument in favor of powerful type systems such as Spiral's because they make composition much easier. After a certain point this translates into a permanent advantage beyond what even dynamic languages offer due to the ability to propagate information through language boundaries.&lt;/p&gt;
&lt;h4&gt;Literal Patterns&lt;/h4&gt;
&lt;pre&gt;
&lt;code&gt;            | PatLit x -&amp;gt; 
                let x = lit x
                let on_succ = if_static (eq arg x) on_succ on_fail
                if_static (eq_type arg x) on_succ on_fail |&amp;gt; case arg
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;At some point in the future these might get their own dedicated CPS operations just like tuples, but it is interesting to study them the way they are right now. If the later this changes in the source there is no need to be surprised. The meaning of the pattern won't change.&lt;/p&gt;
&lt;p&gt;What is happening above is that first a type equality check is made (otherwise the equality could throw a type error) and then the equality is tested for. Here is how that would compile.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;function
| 1 -&amp;gt; body1
| 2 -&amp;gt; body2
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;inl pat_var_6_0 -&amp;gt;
    if eq_type pat_var_6_0 1 then
        if pat_var_6_0 = 1 then body1
        else 
            if eq_type pat_var_6_0 2 then
                if pat_var_6_0 = 2 then body2
                else on_fail // pattern miss
            else on_fail // pattern miss

    elif eq_type pat_var_6_0 2 then
        if pat_var_6_0 = 2 then body2
        else on_fail // pattern miss
    else on_fail // pattern miss
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above demonstrates why would it be good to replace the operation with a single CPS'd operation.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;            | PatTypeLit x -&amp;gt; 
                if_static (eq_type arg (type_lit_lift x)) on_succ on_fail 
                |&amp;gt; case arg
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Since type literals are types they are compared using &lt;code&gt;eq_type&lt;/code&gt; rather than &lt;code&gt;=&lt;/code&gt;. Trying to do so with &lt;code&gt;=&lt;/code&gt; would cause a type error.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;            | PatTypeLitBind x -&amp;gt; 
                if_static (type_lit_is arg) (l x (type_lit_cast arg) on_succ) on_fail 
                |&amp;gt; case arg
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The language has specialized operations in order to cast type literals to the term level like it is possible with functions. The full list of &lt;code&gt;Op&lt;/code&gt;eratives will be in the reference.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let type_lit_cast x = (TypeLitCast,[x]) |&amp;gt; op
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is how &lt;code&gt;type_lit_cast&lt;/code&gt; is implemented. The following is how it is done in the evaluator.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let type_lit_cast d a =
    match tev d a with
    | TyT (LitT x) -&amp;gt; TyLit x
    | _ -&amp;gt; on_type_er (trace d) &quot;Expected a literal in type literal cast.&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Type literals are always expected to be naked types.&lt;/p&gt;
&lt;h4&gt;Module Patterns&lt;/h4&gt;
&lt;pre&gt;
&lt;code&gt;            | PatModuleIs p -&amp;gt; module_is_cps arg on_fail (cp arg p on_succ on_fail) |&amp;gt; case arg
            | PatModuleMember name -&amp;gt; module_member_cps arg name on_fail (inl name on_succ) |&amp;gt; case arg
            | PatModuleRebind(name,b) -&amp;gt; 
                let arg' = new_pat_var()    
                module_member_cps arg name on_fail (inl arg' (cp (v arg') b on_succ on_fail)) 
                |&amp;gt; case arg
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The patterns remaining are nearly over. Explicit examples won't be provided for the above as it should be straightforward by now. As a note, &lt;code&gt;PatModuleIs&lt;/code&gt; is only created when the module pattern is empty.&lt;/p&gt;
&lt;p&gt;Here is how &lt;code&gt;module_is_cps&lt;/code&gt; is implemented in the evaluator.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let module_is_cps d a on_fail on_succ =
    match tev d a with
    | M(_,_,MapTypeModule) -&amp;gt; tev d on_succ
    | _ -&amp;gt; tev d on_fail
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here is the other one.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let module_member_cps d a b on_fail on_succ =
    match tev2 d a b with
    | M(layout,env_term,MapTypeModule) &amp;amp; recf, b -&amp;gt; 
        match b with
        | TyLit (LitString n) -&amp;gt; 
            let inline unpack k = v_find env_term n (fun () -&amp;gt; tev d on_fail) k
            match layout with
            | None -&amp;gt; unpack (apply d (tev d on_succ))
            | _ -&amp;gt; unpack (apply d (tev d on_succ) &amp;lt;&amp;lt; layout_boxed_unseal d recf)
        | x -&amp;gt; on_type_er (trace d) &amp;lt;| sprintf &quot;Expecting a string as the second argument to ModuleMemberCPS.\nGot: %s&quot; (show_typedexpr x)
    | x,_ -&amp;gt; tev d on_fail
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Whatever complexity is here, it is because the module pattern has to also work on layout types. Layout types need to be explicitly unsealed instead of reached into directly.&lt;/p&gt;
&lt;p&gt;Here is how &lt;code&gt;v_find&lt;/code&gt; is implemented.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let inline v_find env x on_fail on_succ = 
    let run env = 
        match Map.tryFind x env with
        | Some v -&amp;gt; on_succ v
        | None -&amp;gt; on_fail()
    match env with
    | Env env -&amp;gt; run env
    | EnvConsed env -&amp;gt; run env.node
    | EnvUnfiltered (env, used_vars) -&amp;gt; if used_vars.Contains x then run env else on_fail()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It roughly as one would expect. It takes an &lt;code&gt;env&lt;/code&gt; and two continuations and runs them depending on whether it finds the member in the dictionary.&lt;/p&gt;
&lt;p&gt;With that in mind, here is &lt;code&gt;unpack&lt;/code&gt; again with the type of &lt;code&gt;k&lt;/code&gt; added.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let inline unpack (k: TypedExpr -&amp;gt; TypedExpr) = v_find env_term n (fun () -&amp;gt; tev d on_fail) k
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;k&lt;/code&gt; is merely the &lt;code&gt;on_succ&lt;/code&gt; argument.&lt;/p&gt;
&lt;p&gt;If the code is unclear then what the suggested course of action is to try inlining the code by hand and seeing if starts making sense.&lt;/p&gt;
&lt;h4&gt;When Pattern&lt;/h4&gt;
&lt;pre&gt;
&lt;code&gt;            | PatWhen (p, e) -&amp;gt; cp arg p (if_static e on_succ on_fail) on_fail
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This one is straightforward. The author had more difficulty parsing it than implementing it. What it does is first tries the left pattern side and makes the &lt;code&gt;on_succ&lt;/code&gt; argument the when check.&lt;/p&gt;
&lt;h4&gt;Term Function Type Pattern&lt;/h4&gt;
&lt;pre&gt;
&lt;code&gt;            | PatTypeTermFunction(a,b) -&amp;gt; 
                let va, vb = new_pat_var(), new_pat_var()
                term_fun_dom_range_cps arg on_fail 
                &amp;lt;| inl' [va; vb] (cp (v va) a (cp (v vb) b on_succ on_fail) on_fail)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This the &lt;code&gt;=&amp;gt;&lt;/code&gt; pattern that was covered during the implementation of &lt;code&gt;closure_of&lt;/code&gt;. Here is how it is implemented in the evaluator.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;        let term_fun_dom_range_cps d x on_fail on_succ =
            match tev d x with
            | TyType(TermFunctionT (a,b)) -&amp;gt; 
                let on_succ = tev d on_succ    
                apply d (apply d on_succ (tyt a)) (tyt b)
            | x -&amp;gt; tev d on_fail
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It a pretty straightforward mapping. It extracts the types and applies them to the &lt;code&gt;on_succ&lt;/code&gt; continuation otherwise it calls &lt;code&gt;on_fail&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;The Pos Pattern&lt;/h4&gt;
&lt;pre&gt;
&lt;code&gt;| PatPos p -&amp;gt; expr_pos p.Pos (cp arg p.Expression on_succ on_fail)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This one is not placed by the user but by the parser. This pattern is converted into the &lt;code&gt;ExprPos&lt;/code&gt; node.&lt;/p&gt;
&lt;p&gt;Here is how it looks like in the evaluator.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let inline add_trace (d: LangEnv) x = {d with trace = x :: d.trace}
...
| ExprPos p -&amp;gt; tev (add_trace d p.Pos) p.Expression
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The trace is of course what the user gets when he makes a type error.&lt;/p&gt;
&lt;h4&gt;A Note On Case&lt;/h4&gt;
&lt;p&gt;For the purpose of simplification whenever &lt;code&gt;|&amp;gt; case arg&lt;/code&gt; has appeared, it has been ignored.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let case arg case = (Case,[arg;case]) |&amp;gt; op
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;What it does has been covered in the &lt;code&gt;Unboxing of Union Types&lt;/code&gt; chapter of the user guide.&lt;/p&gt;
&lt;h3&gt;9: Layout Types&lt;/h3&gt;
&lt;p&gt;Layout types came relatively late in Spiral's development, about six months in. The reason why that is remarkable is because the author thought for a long time on how to do them and eventually concluded that they were impossible. The reason for that is because having them would mean having to essentially capture chunks of scope and how could something like that possibly be done?&lt;/p&gt;
&lt;p&gt;Quite simply.&lt;/p&gt;
&lt;p&gt;First, it is time to take the lid off the list of all the &lt;code&gt;Ty&lt;/code&gt;pes in Spiral. All of these have been demonstrated so far.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;and Ty =
    | PrimT of PrimitiveType
    | ListT of Ty list
    | LitT of Value
    | MapT of EnvTy * MapType // function or module
    | LayoutT of LayoutType * EnvTerm * MapType
    | TermFunctionT of Ty * Ty
    | UnionT of Set&amp;lt;Ty&amp;gt;
    | RecT of JoinPointKey
    | ArrayT of ArrayType * Ty
    | DotNetTypeT of TypedExpr // macro
    | CudaTypeT of TypedExpr // macro
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Note the similarities between &lt;code&gt;MapT&lt;/code&gt; and &lt;code&gt;LayoutT&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The type of &lt;code&gt;EnvTy&lt;/code&gt; is &lt;code&gt;Map&amp;lt;string,Ty&amp;gt;&lt;/code&gt;. While &lt;code&gt;EnvTerm&lt;/code&gt; can be thought of as &lt;code&gt;Map&amp;lt;string,TypedExpr&amp;gt;&lt;/code&gt;. &lt;code&gt;LayoutType&lt;/code&gt; is rather simple.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;type LayoutType =
    | LayoutStack
    | LayoutPackedStack
    | LayoutHeap
    | LayoutHeapMutable
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;It is just some extra data denoting what kind of type it is. Functionally though, &lt;code&gt;MapT&lt;/code&gt; and &lt;code&gt;LayoutT&lt;/code&gt; can be thought of as duals of each other. &lt;code&gt;LayoutT&lt;/code&gt; is just a map with a bit more information propagated in it.&lt;/p&gt;
&lt;p&gt;Here is how the types are created.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let layout_to_none' d = function
    | TyMap _ as a -&amp;gt; a
    | TyType(LayoutT(_,env,t)) as a -&amp;gt; tymap(layout_env_term_unseal d a env,t)
    | x -&amp;gt; on_type_er (trace d) &amp;lt;| sprintf &quot;Cannot turn the argument into a non-layout type. Got: %s&quot; (show_typedexpr x)
let layout_to_none d a = layout_to_none' d (tev d a)

let rec layoutify (layout: LayoutType) (d: LangEnv) = function
    | TyMap(C env,t) -&amp;gt;
        let {renamer'=r}, env' = renamer_apply_envc env
        if r.Count = 0 then LayoutT(layout,env',t) |&amp;gt; tyt
        else TyOp(layout_to_op layout,[a],LayoutT(layout,env',t)) |&amp;gt; destructure d
    | TyType(LayoutT(layout',_,_)) as a -&amp;gt;
        if layout &amp;lt;&amp;gt; layout' then layout_to_none' d a |&amp;gt; layoutify layout d else a
    | x -&amp;gt; on_type_er (trace d) &amp;lt;| sprintf &quot;Cannot turn the argument into a layout type. Got: %s&quot; (show_typedexpr x)
let layout_to layout d a = layoutify layout d (tev d a)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The above could be summarized as - run the renamer and then plug the resulting environment into the &lt;code&gt;LayoutT&lt;/code&gt;. This is exactly what the join points do for their keys.&lt;/p&gt;
&lt;p&gt;If one can understand renaming then one can understand layout types.&lt;/p&gt;
&lt;p&gt;The only remaining thing is to show how the can be unsealed.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let rec layout_boxed_unseal d recf x =
    let inline f x = layout_boxed_unseal d recf x
    match x with
    | TyV _ as v -&amp;gt; TyOp(MapGetField,[recf;v],get_type v) |&amp;gt; destructure d
    | TyList l -&amp;gt; tyvv (List.map f l)
    | TyBox(a,b) -&amp;gt; tybox (f a, b)
    | TyMap(env, b) -&amp;gt; tymap (layout_env_term_unseal d recf env, b)
    | x -&amp;gt; x
        
and layout_env_term_unseal d recf (C env) = Map.map (fun _ -&amp;gt; layout_boxed_unseal d recf) env |&amp;gt; Env
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;recf&lt;/code&gt; is in the above two functions is just the variable being unsealed.&lt;/p&gt;
&lt;p&gt;Unsealing layout types is a standard map over &lt;code&gt;TypedExpr&lt;/code&gt;. Note that if the unsealing is done twice on the same variable in local scope, CSE will prevent duplicate work from being done.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;TyOp(MapGetField,[recf;v],get_type v) |&amp;gt; destructure d
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is due to &lt;code&gt;destructure&lt;/code&gt;. It will send the &lt;code&gt;TyOp&lt;/code&gt;s to the CSE dictionary and will remember if they tried to be unsealed again.&lt;/p&gt;
&lt;p&gt;This is what is needed to understand layout types. With this all the features that are core to the full Spiral experience have been covered in depth.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;layout_boxed_unseal&lt;/code&gt; and &lt;code&gt;layout_env_term_unseal&lt;/code&gt; are used in various places throughout the evaluator when opening the layout types is needed. This won't be covered in the user guide.&lt;/p&gt;
&lt;p&gt;If you've managed to get this far, by now you are reasonably familiar with not just the language, but with its internals as well.&lt;/p&gt;
&lt;p&gt;Past this if you would like to know more, looking into the source is a reasonable option. Spiral is not a huge overbearing monolith of a language with hundreds of thousands of lines of code. The evaluator itself is at the time of writing 1.6k lines long. It is not commented, but a significant amount of effort has gone into refactoring it and you can be sure that if there is a feature in the language, you can find it in just one place in the evaluator.&lt;/p&gt;
&lt;p&gt;Well, apart from the &lt;code&gt;-&lt;/code&gt; and &lt;code&gt;.&lt;/code&gt; operators in the parsing stage. The author hates those things.&lt;/p&gt;
&lt;p&gt;If that is not enough, just ask him.&lt;/p&gt;
&lt;h3&gt;10: Macros&lt;/h3&gt;
&lt;p&gt;In Spiral macros can be treated as types and used to instantiate types which makes them unique in a statically typed language. They are definitely not needed to understand Spiral, but they should be in the user guide nonetheless.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let inline codegen_macro' show_typedexpr codegen print_type x = 
    let strb = StringBuilder()
    let inline append (x: string) = strb.Append x |&amp;gt; ignore
    let (|LS|) = function
            | TyLit (LitString x) | TypeString x -&amp;gt; x
            | _ -&amp;gt; failwithf &quot;Iter's first three arguments must be strings.&quot;
    let er x = failwithf &quot;Unknown argument in macro. Got: %s&quot; (show_typedexpr x)
    let rec f = function
        | TyList [TypeString &quot;text&quot;; LS x] -&amp;gt; append x
        | TyList [TypeString &quot;arg&quot;; x] -&amp;gt; append (codegen x)
        | TyList [TypeString &quot;args&quot;; TyTuple l] -&amp;gt; append &quot;(&quot;; List.map codegen l |&amp;gt; String.concat &quot;, &quot; |&amp;gt; append; append &quot;)&quot;
        | TyList [TypeString &quot;fs_array_args&quot;; TyTuple l] -&amp;gt; append &quot;[|&quot;; List.map codegen l |&amp;gt; String.concat &quot;; &quot; |&amp;gt; append; append &quot;|]&quot;
        | TyList [TypeString &quot;type&quot;; TyType x] -&amp;gt; append (print_type x)
        | TyList [TypeString &quot;types&quot;; TyTuple l] -&amp;gt; append &quot;&amp;lt;&quot;; List.map (get_type &amp;gt;&amp;gt; print_type) l |&amp;gt; String.concat &quot;, &quot; |&amp;gt; append; append &quot;&amp;gt;&quot; 
        | TyList [TypeString &quot;iter&quot;; TyList [LS begin_;LS sep;LS end_;ops]] -&amp;gt;
                append begin_
                match ops with
                | TyList (x :: xs) -&amp;gt; f x; List.iter (fun x -&amp;gt; append sep; f x) xs
                | TyList [] -&amp;gt; ()
                | x -&amp;gt; er x
                append end_
        | x -&amp;gt; er x
    match x with
    | TyList x -&amp;gt; List.iter f x
    | x -&amp;gt; er x
    strb.ToString()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This function definitely looks like it came from the code generation phase and that would be correct. &lt;code&gt;show_typedexpr&lt;/code&gt; is just there for printing errors. &lt;code&gt;codegen&lt;/code&gt; for printing &lt;code&gt;TypedExpr&lt;/code&gt;s and &lt;code&gt;print_type&lt;/code&gt; for printing &lt;code&gt;Ty&lt;/code&gt;pes are provided by the evaluator and the code generators separately and that allows macros to be printed in one way inside errors and in another way in code while retaining the same structure underneath.&lt;/p&gt;
&lt;p&gt;Macros are an ad-hoc feature there just for the sake of language interoperability and while needed, they should not be considered core to the language nor should be used for abstraction. They should be redone into something more principled.&lt;/p&gt;
&lt;p&gt;As it is their functionality just arose from needs of various sorts.&lt;/p&gt;
&lt;p&gt;List of operations:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;&lt;code&gt;text&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Takes in a literal or a type string literal and prints it.&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;&lt;code&gt;arg&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Takes in an argument and prints it.&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;&lt;code&gt;args&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Takes in an argument or a tuple of arguments and prints them between parentheses and separated by commas.&lt;/p&gt;
&lt;ol start=&quot;3&quot;&gt;&lt;li&gt;&lt;code&gt;fs_array_args&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Takes in an argument or a tuple of arguments and prints them between &lt;code&gt;[|&lt;/code&gt; and &lt;code&gt;|]&lt;/code&gt; and separated by semicolons.&lt;/p&gt;
&lt;ol start=&quot;4&quot;&gt;&lt;li&gt;&lt;code&gt;type&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Takes in an argument and prints its type.&lt;/p&gt;
&lt;ol start=&quot;5&quot;&gt;&lt;li&gt;&lt;code&gt;types&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Takes in an argument or a tuple of arguments and prints their type them between &lt;code&gt;&amp;lt;&lt;/code&gt; and &lt;code&gt;&amp;gt;&lt;/code&gt; and separated by commas.&lt;/p&gt;
&lt;ol start=&quot;6&quot;&gt;&lt;li&gt;&lt;code&gt;iter&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;It takes in the opener, separator and closer as strings or type level strings and a tuple of macro operations and executes them while printing the separator in-between them and the opener and the closer at the beginning and the end respectively.&lt;/p&gt;
&lt;h4&gt;Parser Macros&lt;/h4&gt;
&lt;p&gt;There are only 3 of them.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;let case_parser_macro expr = 
    inbuilt_op_core '@' &amp;gt;&amp;gt;= fun a -&amp;gt;
        match a with
        | &quot;PathCuda&quot; -&amp;gt; settings.path_cuda90 |&amp;gt; LitString |&amp;gt; lit |&amp;gt; preturn
        | &quot;PathCub&quot; -&amp;gt; settings.path_cub |&amp;gt; LitString |&amp;gt; lit |&amp;gt; preturn
        | &quot;PathVS2017&quot; -&amp;gt; settings.path_vs2017 |&amp;gt; LitString |&amp;gt; lit |&amp;gt; preturn
        | a -&amp;gt; failFatally &amp;lt;| sprintf &quot;%s is not a valid parser macro.&quot; a
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;They are invoked like &lt;code&gt;inl path_cuda = @PathCuda&lt;/code&gt; and so on. They are just there as a safeguard in case join point dictionaries ever get floated up a level. If that happens changing the compiler setting might give incorrect results if the paths were inserted during the partial evaluation stage since join points would not recognize the changes.&lt;/p&gt;
&lt;h3&gt;11: Operatives and the Core Library&lt;/h3&gt;
&lt;p&gt;Invoking operatives is the main way the user communicates with the partial evaluator. In Spiral most operations are virtualized and compile to calls to core library functions.&lt;/p&gt;
&lt;p&gt;Here is how it can be done.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl (+) a b = !Add(a,b)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;Add&lt;/code&gt; is an &lt;code&gt;Op&lt;/code&gt;erative and the &lt;code&gt;!&lt;/code&gt; is the operative invocation unary operator. At the time of writing there are 93 &lt;code&gt;Op&lt;/code&gt;eratives in total. A full list of them can be found with the rest of the types in &lt;code&gt;SpiralTypes.fs&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The core library can be found in &lt;code&gt;SpiralCoreLib.fs&lt;/code&gt;. It is opened automatically every time the program is compiled.&lt;/p&gt;
&lt;p&gt;Here is a copy of it for easy perusal.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;module Spiral.CoreLib

let module_ = Types.module_
let core =
    (
    &quot;Core&quot;,[],&quot;The Core module.&quot;,
    &quot;&quot;&quot;
/// Lifts a literal to the type level.
inl type_lit_lift x = !TypeLitCreate(x)

/// Raises a type error.
inl error_type x = !ErrorType(x)
/// Prints an expression at compile time.
inl print_static x = !PrintStatic(x)
/// Pushes the expression to runtime.
inl dyn x = !Dynamize(x)
/// Creates a term function with the given two types.
inl (=&amp;gt;) a b = !TermFunctionTypeCreate(a,b)
/// Splits the union or recursive type into a tuple.
inl split x = !TypeSplit(x)
/// Boxes a type.
inl box a b = !TypeBox(a,b)
/// Converts module or a function to a stack layout type.
inl stack x = !LayoutToStack(x)
/// Converts module or a function to a packed stack layout type.
inl packed_stack x = !LayoutToPackedStack(x)
/// Converts module or a function to a heap layout type.
inl heap x = !LayoutToHeap(x)
/// Converts module or a function to a mutable heap layout type.
inl heapm x = !LayoutToHeapMutable(x)
/// Converts a layout type to a module or a function.
inl indiv x = !LayoutToNone(x)

/// The type of a boolean.
inl bool = type true 

/// The type of a int64.
inl int64 = type 0i64 
/// The type of a int32.
inl int32 = type 0i32 
/// The type of a int16.
inl int16 = type 0i16 
/// The type of a int8.
inl int8 = type 0i8 

/// The type of a uint64.
inl uint64 = type 0u64 
/// The type of a uint32.
inl uint32 = type 0u32 
/// The type of a uint16.
inl uint16 = type 0u16 
/// The type of a uint8.
inl uint8 = type 0u8

/// The type of a float64.
inl float64 = type 0f64
/// The type of a float32.
inl float32 = type 0f32

/// The type of a string.
inl string = type &quot;&quot;
/// The type of a char.
inl char = type ' '
/// The type of a empty tuple.
inl unit = type ()

/// Casts a type literal to the term level.
inl type_lit_cast x = !TypeLitCast(x)
/// Returns whether the expression is a type literal as a bool.
inl type_lit_is x = !TypeLitIs(x)
/// Cast a function to the term level.
inl term_cast to from = !TermCast(to,from)
/// Does unchecked conversion for primitives.
inl unsafe_convert to from = !UnsafeConvert(to,from) 
/// Unary negation.
inl negate x = !Neg(x)
/// Evaluates an expression and throws away the result.
inl ignore x = ()
/// Returns an expression after evaluating it.
inl id x = x
/// Throws away the second argument and returns the first.
inl const x _ = x
/// Creates a reference.
inl ref x = !ReferenceCreate(x)

/// Creates an array with the given type and the size.
inl array_create typ size = !ArrayCreate(size,typ)
/// Returns the length of an array. Not applicable to Cuda arrays.
inl array_length ar = !ArrayLength(ar)
/// Partial active pattern. In `on_succ` it also passes the type of the array as a type string.
inl array_is x on_fail on_succ = !ArrayIs(x,on_fail,on_succ)
/// Type of an array with the given type.
inl array t = type (array_create t 1)

/// Binary addition.
inl (+) a b = !Add(a,b)
/// Binary subtraction.
inl (-) a b = !Sub(a,b)
/// Binary multiplication.
inl (*) a b = !Mult(a,b)
/// Binary division.
inl (/) a b = !Div(a,b)
/// Binary modulo.
inl (%) a b = !Mod(a,b)

/// Applies the first argument to the second.
inl (|&amp;gt;) a b = b a
/// Applies the second argument to the first.
inl (&amp;lt;|) a b = a b
/// Applies the third argument to the first and then the result of that to the second.
inl (&amp;gt;&amp;gt;) a b x = b (a x)
/// Applies the third argument to the second and then the result of that to the first.
inl (&amp;lt;&amp;lt;) a b x = a (b x)

/// Binary less-than-or-equals.
inl (&amp;lt;=) a b = !LTE(a,b)
/// Binary less-than.
inl (&amp;lt;) a b = !LT(a,b)
/// Binary equals.
inl (=) a b = !EQ(a,b)
/// Binary unequals.
inl (&amp;lt;&amp;gt;) a b = !NEQ(a,b)
/// Binary greater-than.
inl (&amp;gt;) a b = !GT(a,b)
/// Binary greater-than-or-equals.
inl (&amp;gt;=) a b = !GTE(a,b)

/// Bitwise and.
inl (&amp;amp;&amp;amp;&amp;amp;) a b = !BitwiseAnd(a,b)
/// Bitwise or.
inl (|||) a b = !BitwiseOr(a,b)
/// Bitwise xor.
inl (^^^) a b = !BitwiseXor(a,b)

/// Tuple cons.
inl (::) a b = !ListCons(a,b)
/// Shift left.
inl (&amp;lt;&amp;lt;&amp;lt;) a b = !ShiftLeft(a,b)
/// Shift right.
inl (&amp;gt;&amp;gt;&amp;gt;) a b = !ShiftRight(a,b)

/// Gets the first elements of a tuple.
inl fst x :: _ = x
/// Gets the second element of a tuple.
inl snd _ :: x :: _ = x

/// Unary negation.
inl not x = x = false
/// Returns the length of a string.
inl string_length x = !StringLength(x)
/// The .NET String.Format function.
/// https://msdn.microsoft.com/en-us/library/system.string.format(v=vs.110).aspx
/// When its arguments are literals, it evaluates at compile time.
inl string_format a b = !StringFormat(a,b)
/// The F# String.concat function
/// https://msdn.microsoft.com/en-us/visualfsharpdocs/conceptual/string.concat-function-%5Bfsharp%5D
/// When its arguments are literals, it evaluates at compile time.
inl string_concat a b = !StringConcat(a,b)
/// Returns boolean whether the expression is a literal.
inl lit_is x = !LitIs(x)
/// Returns boolean whether the expression is a box (but not an union type.)
inl box_is x = !BoxIs(x)
/// Returns boolean whether the expression is a union or a recursive type (excluding boxes.)
inl caseable_is x = !CaseableIs(x)
/// Returns boolean whether the expression is a union or a recursive type.
inl caseable_box_is x = !CaseableBoxIs(x)
/// Raises an exception at runtime.
inl failwith typ msg = !FailWith(typ,msg)
/// Asserts an expression. If the conditional is a literal it raises a type error instead.
inl assert c msg = 
    inl raise = 
        if lit_is c then error_type
        else failwith unit
    
    if c = false then raise msg
/// Returns the maximum of the two expressions.
inl max a b = if a &amp;gt; b then a else b
/// Returns the minimum of the two expressions.
inl min a b = if a &amp;gt; b then b else a
/// Returns boolean whether the two expressions are equal in their types.
inl eq_type a b = !EqType(a,b)
/// Returns the values of a module in a tuple.
inl module_values x = !ModuleValues(x)
/// Maps over a module.
/// (string type_lit -&amp;gt; a -&amp;gt; b) -&amp;gt; a module -&amp;gt; b module
inl module_map f a = !ModuleMap(f,a)
/// Filters a module at compile time.
/// (string type_lit -&amp;gt; a -&amp;gt; bool) -&amp;gt; a module -&amp;gt; a module
inl module_filter f a = !ModuleFilter(f,a)
/// Folds over a module left to right.
/// (string type_lit -&amp;gt; state -&amp;gt; a -&amp;gt; state) -&amp;gt; state -&amp;gt; a module -&amp;gt; state
inl module_foldl f s a = !ModuleFoldL(f,s,a)
/// Folds over a module right to left.
/// (string type_lit -&amp;gt; a -&amp;gt; state -&amp;gt; state) -&amp;gt; a module -&amp;gt; state -&amp;gt; state
inl module_foldr f a s = !ModuleFoldR(f,s,a)
/// Returns boolean whether the module has a member.
/// a module -&amp;gt; string type_lit -&amp;gt; bool
inl module_has_member m x = !ModuleHasMember(m,x)
/// Unsafe upcast. Unlike the F# compiler, Spiral won't check its correctness.
inl (:&amp;gt;) a b = !UnsafeUpcastTo(b,a)
/// Unsafe downcast. Unlike the F# compiler, Spiral won't check its correctness.
inl (:?&amp;gt;) a b = !UnsafeDowncastTo(b,a)

/// Structural polymorphic equality for every type in the language (apart from functions.)
inl (=) a b =
    inl prim_eq = (=)
    inl rec (=) a b =
        inl body = function
            | .(a), .(b) -&amp;gt; a = b
            | a :: as', b :: bs -&amp;gt; a = b &amp;amp;&amp;amp; as' = bs
            | {} &amp;amp; a, {} &amp;amp; b -&amp;gt; module_values a = module_values b
            | (), () -&amp;gt; true
                        | a, () -&amp;gt; false // Just in case `b` has not been `Case`d up to this point. This can happen for `(int64, int64) \/ int64` kind of types.
            | a, b when eq_type a b -&amp;gt; prim_eq a b // This repeat eq_type check is because unboxed union types might lead to variables of different types to be compared.
            | _ -&amp;gt; false
        if caseable_is a &amp;amp;&amp;amp; caseable_is b then join (body (a, b) : bool)
        else body (a, b)
    if eq_type a b then a = b
    else error_type (&quot;Trying to compare variables of two different types. Got:&quot;,a,b)

/// Returns the size a type.
/// type -&amp;gt; int64
inl sizeof x = !SizeOf(x)

/// Creates a .NET type from a macro.
inl fs x = !DotNetTypeCreate(x)
/// Creates a Cuda type from a macro.
inl cd x = !CudaTypeCreate(x)

/// Natural Logarithm.
inl log x = !Log(x)
/// Exponent.
inl exp x = !Exp(x)
/// Hyperbolic tangent. 
inl tanh x = !Tanh(x)

/// Macros.
inl macro = {
    /// F# macro.
    fs = inl typ expr -&amp;gt; !MacroFs(typ,expr)
    /// Cuda macro.
    cd = inl typ expr -&amp;gt; !MacroCuda(typ,expr)
    }

{type_lit_lift error_type print_static dyn (=&amp;gt;) cd fs log exp tanh array_create array_length array_is array
 split box stack packed_stack heap heapm indiv bool int64 int32 int16 int8 uint64 uint32 uint16 uint8 float64 float32
 string char unit type_lit_cast type_lit_is term_cast unsafe_convert negate ignore id const ref (+) (-) (*) (/) (%)
 (|&amp;gt;) (&amp;lt;|) (&amp;gt;&amp;gt;) (&amp;lt;&amp;lt;) (&amp;lt;=) (&amp;lt;) (=) (&amp;lt;&amp;gt;) (&amp;gt;) (&amp;gt;=) (&amp;amp;&amp;amp;&amp;amp;) (|||) (^^^) (::) (&amp;lt;&amp;lt;&amp;lt;) (&amp;gt;&amp;gt;&amp;gt;) fst snd not macro
 string_length lit_is box_is failwith assert max min eq_type module_values caseable_is caseable_box_is (:&amp;gt;)
 (:?&amp;gt;) (=) module_map module_filter module_foldl module_foldr module_has_member sizeof string_format string_concat} |&amp;gt; stack
    &quot;&quot;&quot;) |&amp;gt; module_
&lt;/code&gt;
&lt;/pre&gt;
&lt;h4&gt;Virtualization&lt;/h4&gt;
&lt;p&gt;Most operations in Spiral are virtualized, meaning they can be overridden.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl f m =
    open m
    (1 + 2) * (3 + 4)

f {}
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;21L
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;If &lt;code&gt;+&lt;/code&gt; and &lt;code&gt;*&lt;/code&gt; overriden, the meaning of the expression can change completely.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl f m =
    open m
    (1 + 2) * (3 + 4)

f {
    (+) = inl a b -&amp;gt; string_format &quot;({0} + {1})&quot; (a,b)
    (*) = inl a b -&amp;gt; string_format &quot;({0} * {1})&quot; (a,b)
    }
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;&quot;((1 + 2) * (3 + 4))&quot;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Currently &lt;code&gt;if&lt;/code&gt;,&lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;, &lt;code&gt;||&lt;/code&gt; and &lt;code&gt;.&lt;/code&gt; are keywords, but there is no reason why they couldn't be virtualized.&lt;/p&gt;
&lt;p&gt;The keyword &lt;code&gt;use&lt;/code&gt; can be overridden by putting parentheses around it. This is how &lt;code&gt;use&lt;/code&gt; is implemented in &lt;code&gt;Extern&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;inl (use) a b =
    inl r = b a
    FS.Method a .Dispose() unit
    r
&lt;/code&gt;
&lt;/pre&gt;
&lt;h3&gt;Known Bugs&lt;/h3&gt;
&lt;ol&gt;&lt;li&gt;
&lt;p&gt;Passing &lt;code&gt;nan&lt;/code&gt;s through join points causes the compiler to diverge. Since join points use structural equality for memoization and &lt;code&gt;nan = nan&lt;/code&gt; is always &lt;code&gt;false&lt;/code&gt;. That means that they cannot be compared and will always result in a new entry to their dictionary at join point boundaries. Note to numerical standards designers - don't do this again! Comparing &lt;code&gt;nan&lt;/code&gt;s should be invalid, but returning &lt;code&gt;false&lt;/code&gt; is not the answer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;.NET&lt;/code&gt; will happily marshal non &lt;a href=&quot;https://docs.microsoft.com/en-us/dotnet/framework/interop/blittable-and-non-blittable-types&quot; rel=&quot;nofollow&quot;&gt;&lt;code&gt;blittable&lt;/code&gt;&lt;/a&gt; types past language boundaries corrupting memory and writing past the ends of the arrays. Spiral will give a type error at join points, but it is still possible to corrupt memory using transfer functions which aren't doing proper checking.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;/article&gt;</description>
<pubDate>Thu, 12 Jul 2018 22:27:23 +0000</pubDate>
<dc:creator>Nelkins</dc:creator>
<og:image>https://avatars1.githubusercontent.com/u/6266635?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>mrakgr/The-Spiral-Language</og:title>
<og:url>https://github.com/mrakgr/The-Spiral-Language</og:url>
<og:description>The-Spiral-Language - Functional language with intensional polymorphism and first-class staging.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/mrakgr/The-Spiral-Language</dc:identifier>
</item>
<item>
<title>Postmortem for Malicious Packages Published on July 12th, 2018</title>
<link>https://eslint.org/blog/2018/07/postmortem-for-malicious-package-publishes</link>
<guid isPermaLink="true" >https://eslint.org/blog/2018/07/postmortem-for-malicious-package-publishes</guid>
<description>&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;/&gt;&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;/&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot;/&gt;&lt;meta name=&quot;keywords&quot; content=&quot;JavaScript, Linter, Linting, Pluggable, Configurable, Code Quality&quot;/&gt;&lt;meta name=&quot;description&quot; content=&quot;A pluggable and configurable linter tool for identifying and reporting on patterns in JavaScript. Maintain your code quality with ease.&quot;/&gt;&lt;meta name=&quot;theme-color&quot; content=&quot;#463fd4&quot;/&gt;&lt;meta property=&quot;og:locale&quot; content=&quot;en_US&quot;/&gt;&lt;meta property=&quot;og:site_name&quot; content=&quot;ESLint - Pluggable JavaScript linter&quot;/&gt;&lt;meta property=&quot;og:title&quot; content=&quot;Postmortem for Malicious Packages Published on July 12th, 2018&quot;/&gt;&lt;meta property=&quot;og:url&quot; content=&quot;https://eslint.org/blog/2018/07/postmortem-for-malicious-package-publishes&quot;/&gt;&lt;meta property=&quot;og:image&quot; content=&quot;https://eslint.org/img/favicon.512x512.png&quot;/&gt;&lt;meta name=&quot;twitter:site&quot; content=&quot;@geteslint&quot;/&gt;&lt;meta name=&quot;twitter:title&quot; content=&quot;Postmortem for Malicious Packages Published on July 12th, 2018&quot;/&gt;&lt;meta name=&quot;twitter:description&quot; content=&quot;A pluggable and configurable linter tool for identifying and reporting on patterns in JavaScript. Maintain your code quality with ease.&quot;/&gt;&lt;meta name=&quot;twitter:url&quot; content=&quot;https://eslint.org/blog/2018/07/postmortem-for-malicious-package-publishes&quot;/&gt;&lt;meta name=&quot;twitter:card&quot; content=&quot;summary&quot;/&gt;&lt;meta name=&quot;twitter:image&quot; content=&quot;https://eslint.org/img/favicon.512x512.png&quot;/&gt;&lt;title&gt;Postmortem for Malicious Packages Published on July 12th, 2018 - ESLint - Pluggable JavaScript linter&lt;/title&gt;&lt;link rel=&quot;preconnect&quot; href=&quot;https://www.google-analytics.com&quot;/&gt;&lt;link href=&quot;https://eslint.org/blog/2018/07/postmortem-for-malicious-package-publishes&quot; rel=&quot;canonical&quot;/&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;https://eslint.org/styles/main.css&quot; type=&quot;text/css&quot;/&gt;&lt;link rel=&quot;manifest&quot; href=&quot;https://eslint.org/manifest.json&quot;/&gt;&lt;link rel=&quot;icon&quot; href=&quot;https://eslint.org/img/favicon.512x512.png&quot;/&gt;&lt;link rel=&quot;alternate&quot; type=&quot;application/rss+xml&quot; title=&quot;ESLint - Pluggable JavaScript linter&quot; href=&quot;https://eslint.org/feed.xml&quot;/&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css&quot; type=&quot;text/css&quot;/&gt;&lt;link href=&quot;https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600&quot; rel=&quot;stylesheet&quot; type=&quot;text/css&quot;/&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;36.594488188976&quot;&gt;

&lt;header class=&quot;navbar navbar-default navbar-demo navbar-fixed-top eslint-nav&quot; id=&quot;top&quot; role=&quot;banner&quot; readability=&quot;0.6036036036036&quot;&gt;&lt;div class=&quot;container&quot; readability=&quot;4.2252252252252&quot;&gt;&lt;a href=&quot;https://eslint.org/&quot; class=&quot;navbar-brand&quot;&gt;&lt;img alt=&quot;ESLint&quot; src=&quot;https://eslint.org/img/logo.svg&quot; itemprop=&quot;image&quot;/&gt;ESLint&lt;/a&gt;
&lt;p&gt;&lt;button class=&quot;navbar-toggle collapsed&quot; type=&quot;button&quot; data-toggle=&quot;collapse&quot; data-target=&quot;#eslint-navbar&quot; aria-controls=&quot;eslint-navbar&quot; aria-expanded=&quot;false&quot;&gt;&lt;span class=&quot;sr-only&quot;&gt;Toggle navigation&lt;/span&gt;&lt;/button&gt; &lt;label for=&quot;eslint-toggle-search&quot; class=&quot;navbar-toggle eslint-toggle-search-open&quot;&gt;&lt;span class=&quot;sr-only&quot;&gt;Open search&lt;/span&gt; &lt;/label&gt;&lt;/p&gt;
&lt;nav id=&quot;eslint-navbar&quot; class=&quot;collapse navbar-collapse eslint-navbar&quot;&gt;&lt;label for=&quot;eslint-toggle-search&quot; class=&quot;navbar-toggle eslint-toggle-search-open&quot;&gt;&lt;span class=&quot;sr-only&quot;&gt;Open search&lt;/span&gt;&lt;/label&gt;

&lt;/nav&gt;&lt;/div&gt;
&lt;/header&gt;&lt;article class=&quot;container&quot; readability=&quot;30.530303030303&quot;&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;On July 12th, 2018, an attacker compromised the npm account of an ESLint maintainer and published malicious versions of the &lt;a href=&quot;https://github.com/eslint/eslint-scope&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;eslint-scope&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://github.com/eslint/eslint/tree/9aaf195ca691d307e8896096cefffe975218c701/packages/eslint-config-eslint&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;eslint-config-eslint&lt;/code&gt;&lt;/a&gt; packages to the npm registry. On installation, the malicious packages downloaded and executed code from &lt;code class=&quot;highlighter-rouge&quot;&gt;pastebin.com&lt;/code&gt; which sent the contents of the user’s &lt;code class=&quot;highlighter-rouge&quot;&gt;.npmrc&lt;/code&gt; file to the attacker. An &lt;code class=&quot;highlighter-rouge&quot;&gt;.npmrc&lt;/code&gt; file typically contains access tokens for publishing to npm.&lt;/p&gt;
&lt;p&gt;The malicious package versions are &lt;code class=&quot;highlighter-rouge&quot;&gt;eslint-scope@3.7.2&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;eslint-config-eslint@5.0.2&lt;/code&gt;, both of which have been unpublished from npm. The &lt;code class=&quot;highlighter-rouge&quot;&gt;pastebin.com&lt;/code&gt; paste linked in these packages has also been taken down.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://status.npmjs.org/incidents/dn7c1fgrr7ng&quot;&gt;npm has revoked&lt;/a&gt; all access tokens issued before 2018-07-12 12:30 UTC. As a result, all access tokens compromised by this attack should no longer be usable.&lt;/p&gt;
&lt;p&gt;The maintainer whose account was compromised had reused their npm password on several other sites and did not have two-factor authentication enabled on their npm account.&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;We, the ESLint team, are sorry for allowing this to happen. We hope that other package maintainers can learn from our mistakes and improve the security of the whole npm ecosystem.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;affected-packages&quot;&gt;Affected Packages&lt;/h2&gt;
&lt;p&gt;If you run your own npm registry, you should unpublish the malicious versions of each package. They have already been unpublished from the npmjs.com registry.&lt;/p&gt;
&lt;h2 id=&quot;attack-method&quot;&gt;Attack Method&lt;/h2&gt;
&lt;p&gt;Further details on the attack can be found &lt;a href=&quot;https://gist.github.com/hzoo/51cb84afdc50b14bffa6c6dc49826b3e&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;recommendations&quot;&gt;Recommendations&lt;/h2&gt;
&lt;p&gt;With the hindsight of this incident, we have a few recommendations for npm package maintainers and users in the future:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Package maintainers and users should avoid reusing the same password across multiple different sites. A password manager like &lt;a href=&quot;https://1password.com/&quot;&gt;1Password&lt;/a&gt; or &lt;a href=&quot;https://www.lastpass.com/&quot;&gt;LastPass&lt;/a&gt; can help with this.&lt;/li&gt;
&lt;li&gt;Package maintainers should &lt;a href=&quot;https://www.npmjs.com/settings/~/tfa&quot;&gt;enable npm two-factor authentication&lt;/a&gt;. npm has a guide &lt;a href=&quot;https://docs.npmjs.com/getting-started/using-two-factor-authentication&quot;&gt;here&lt;/a&gt;.
&lt;ul&gt;&lt;li&gt;If you use Lerna, you can follow this &lt;a href=&quot;https://github.com/lerna/lerna/issues/1091&quot;&gt;issue&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Package maintainers should audit and limit the number of people who have access to publish on npm.&lt;/li&gt;
&lt;li&gt;Package maintainers should be careful with using any services that auto-merge dependency upgrades.&lt;/li&gt;
&lt;li&gt;Application developers should use a lockfile (&lt;code class=&quot;highlighter-rouge&quot;&gt;package-lock.json&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;yarn.lock&lt;/code&gt;) to prevent the auto-install of new packages.&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;timeline&quot;&gt;Timeline&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Before the incident&lt;/strong&gt;: The attacker presumably found the maintainer’s reused email and password in a third-party breach and used them to log in to the maintainer’s npm account.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Early morning July 12th, 2018&lt;/strong&gt;: The attacker generated an authentication token in the maintainer’s npm account.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2018-07-12 9:49 UTC&lt;/strong&gt;: The attacker used the generated authentication token to publish &lt;code class=&quot;highlighter-rouge&quot;&gt;eslint-config-eslint@5.0.2&lt;/code&gt;, which contained a malicious &lt;code class=&quot;highlighter-rouge&quot;&gt;postinstall&lt;/code&gt; script that attempts to exfiltrate the local machine’s &lt;code class=&quot;highlighter-rouge&quot;&gt;.npmrc&lt;/code&gt; authentication token.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2018-07-12 10:25 UTC&lt;/strong&gt;: The attacker unpublished &lt;code class=&quot;highlighter-rouge&quot;&gt;eslint-config-eslint@5.0.2&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2018-07-12 10:40 UTC&lt;/strong&gt;: The attacker published &lt;code class=&quot;highlighter-rouge&quot;&gt;eslint-scope@3.7.2&lt;/code&gt;, which contained the same malicious &lt;code class=&quot;highlighter-rouge&quot;&gt;postinstall&lt;/code&gt; script.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2018-07-12 11:17 UTC&lt;/strong&gt;: A user posted &lt;a href=&quot;https://github.com/eslint/eslint-scope/issues/39&quot;&gt;eslint/eslint-scope#39&lt;/a&gt;, notifying the ESLint team of the issue.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2018-07-12 12:27 UTC&lt;/strong&gt;: The pastebin.com link containing malicious code was taken down.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2018-07-12 12:37 UTC&lt;/strong&gt;: The npm team unpublished &lt;code class=&quot;highlighter-rouge&quot;&gt;eslint-scope@3.7.2&lt;/code&gt; after being contacted by an ESLint maintainer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2018-07-12 17:41 UTC&lt;/strong&gt;: The ESLint team published &lt;code class=&quot;highlighter-rouge&quot;&gt;eslint-scope@3.7.3&lt;/code&gt; with the code from &lt;code class=&quot;highlighter-rouge&quot;&gt;eslint-scope@3.7.1&lt;/code&gt; so that caches could pick up the new version.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2018-07-12 18:42 UTC&lt;/strong&gt;: npm revoked all access tokens generated before 2018-07-12 12:30 UTC.&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;
&lt;/article&gt;&lt;hr/&gt;
&lt;/body&gt;</description>
<pubDate>Thu, 12 Jul 2018 21:06:42 +0000</pubDate>
<dc:creator>ingve</dc:creator>
<og:title>Postmortem for Malicious Packages Published on July 12th, 2018</og:title>
<og:url>https://eslint.org/blog/2018/07/postmortem-for-malicious-package-publishes</og:url>
<og:image>https://eslint.org/img/favicon.512x512.png</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://eslint.org/blog/2018/07/postmortem-for-malicious-package-publishes</dc:identifier>
</item>
<item>
<title>Web Architecture 101</title>
<link>https://engineering.videoblocks.com/web-architecture-101-a3224e126947/?ref=abhimanyu</link>
<guid isPermaLink="true" >https://engineering.videoblocks.com/web-architecture-101-a3224e126947/?ref=abhimanyu</guid>
<description>&lt;h2 name=&quot;9767&quot; id=&quot;9767&quot; class=&quot;graf graf--h4 graf-after--h3 graf--subtitle&quot;&gt;The basic architecture concepts I wish I knew when I was getting started as a web developer&lt;/h2&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*K6M-x-6e39jMq_c-2xqZIQ.png&quot; data-width=&quot;665&quot; data-height=&quot;421&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*K6M-x-6e39jMq_c-2xqZIQ.png&quot;/&gt;&lt;/div&gt;
Modern web application architecture overview
&lt;p name=&quot;ca90&quot; id=&quot;ca90&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The above diagram is a fairly good representation of our architecture at Storyblocks. If you’re not an experienced web developer, you’ll likely find it complicated. The walk through below should make it more approachable before we dive into the details of each component.&lt;/p&gt;
&lt;blockquote name=&quot;df77&quot; id=&quot;df77&quot; class=&quot;graf graf--blockquote graf-after--p&quot; readability=&quot;9.6984924623116&quot;&gt;
&lt;p&gt;A user searches on Google for “Strong Beautiful Fog And Sunbeams In The Forest”. The &lt;a href=&quot;https://www.graphicstock.com/stock-image/strong-beautiful-fog-and-sunbeams-in-the-forest-246703&quot; data-href=&quot;https://www.graphicstock.com/stock-image/strong-beautiful-fog-and-sunbeams-in-the-forest-246703&quot; class=&quot;markup--anchor markup--blockquote-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;first result&lt;/a&gt; happens to be from Storyblocks, our leading stock photo and vectors site. The user clicks the result which redirects their browser to the image details page. Underneath the hood the user’s browser sends a request to a DNS server to lookup how to contact Storyblocks, and then sends the request.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;ac55&quot; id=&quot;ac55&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;12&quot;&gt;
&lt;p&gt;The request hits our load balancer, which randomly chooses one of the 10 or so web servers we have running the site at the time to process the request. The web server looks up some information about the image from our caching service and fetches the remaining data about it from the database. We notice that the color profile for the image has not been computed yet, so we send a “color profile” job to our job queue, which our job servers will process asynchronously, updating the database appropriately with the results.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;219c&quot; id=&quot;219c&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;11&quot;&gt;
&lt;p&gt;Next, we attempt to find similar photos by sending a request to our full text search service using the title of the photo as input. The user happens to be a logged into Storyblocks as a member so we look up his account information from our account service. Finally, we fire off a page view event to our data firehose to be recorded on our cloud storage system and eventually loaded into our data warehouse, which analysts use to help answer questions about the business.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;25e6&quot; id=&quot;25e6&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;12&quot;&gt;
&lt;p&gt;The server now renders the view as HTML and sends it back to the user’s browser, passing first through the load balancer. The page contains Javascript and CSS assets that we load into our cloud storage system, which is connected to our CDN, so the user’s browser contacts the CDN to retrieve the content. Lastly, the browser visibly renders the page for the user to see.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;ffc5&quot; id=&quot;ffc5&quot; class=&quot;graf graf--p graf-after--blockquote&quot;&gt;Next I’ll walk you through each component, providing a “101” introduction to each that should give you a good mental model for thinking through web architecture going forward. I’ll follow up with another series of articles providing specific implementation recommendations based on what I’ve learned in my time at Storyblocks.&lt;/p&gt;
&lt;h3 name=&quot;9d0a&quot; id=&quot;9d0a&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;1. DNS&lt;/h3&gt;
&lt;p name=&quot;f960&quot; id=&quot;f960&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;DNS stands for “Domain Name Server” and it’s a backbone technology that makes the world wide web possible. At the most basic level DNS provides a key/value lookup from a domain name (e.g., google.com) to an IP address (e.g., 85.129.83.120), which is required in order for your computer to route a request to the appropriate server. Analogizing to phone numbers, the difference between a domain name and IP address is the difference between “call John Doe” and “call 201-867–5309.” Just like you needed a phone book to look up John’s number in the old days, you need DNS to look up the IP address for a domain. So you can think of DNS as the phone book for the internet.&lt;/p&gt;
&lt;p name=&quot;6fe5&quot; id=&quot;6fe5&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;There’s a lot more detail we could go into here but we’ll skip over it because it’s not critical for our 101-level intro.&lt;/p&gt;
&lt;h3 name=&quot;cccb&quot; id=&quot;cccb&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;2. Load Balancer&lt;/h3&gt;
&lt;p name=&quot;88bb&quot; id=&quot;88bb&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Before diving into details on load balancing, we need to take a step back to discuss horizontal vs. vertical application scaling. What are they and what’s the difference? Very simply put in &lt;a href=&quot;https://stackoverflow.com/questions/11707879/difference-between-scaling-horizontally-and-vertically-for-databases&quot; data-href=&quot;https://stackoverflow.com/questions/11707879/difference-between-scaling-horizontally-and-vertically-for-databases&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;this StackOverflow post&lt;/a&gt;, horizontal scaling means that you scale by adding more machines into your pool of resources whereas “vertical” scaling means that you scale by adding more power (e.g., CPU, RAM) to an existing machine.&lt;/p&gt;
&lt;p name=&quot;9f4c&quot; id=&quot;9f4c&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In web development, you (almost) always want to scale horizontally because, to keep it simple, stuff breaks. Servers crash randomly. Networks degrade. Entire data centers occasionally go offline. Having more than one server allows you to plan for outages so that your application continues running. In other words, your app is “fault tolerant.” Secondly, horizontal scaling allows you to minimally couple different parts of your application backend (web server, database, service X, etc.) by having each of them run on different servers. Lastly, you may reach a scale where it’s not possible to vertically scale any more. There is no computer in the world big enough to do all your app’s computations. Think Google’s search platform as a quintessential example though this applies to companies at much smaller scales. Storyblocks, for example, runs 150 to 400 AWS EC2 instances at any given point in time. It would be challenging to provide that entire compute power via vertical scaling.&lt;/p&gt;
&lt;p name=&quot;930a&quot; id=&quot;930a&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Ok, back to load balancers. They’re the magic sauce that makes scaling horizontally possible. They route incoming requests to one of many application servers that are typically clones / mirror images of each other and send the response from the app server back to the client. Any one of them should process the request the same way so it’s just a matter of distributing the requests across the set of servers so none of them are overloaded.&lt;/p&gt;
&lt;p name=&quot;bf2b&quot; id=&quot;bf2b&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;That’s it. Conceptually load balancers are fairly straight forward. Under the hood there are certainly complications but no need to dive in for our 101 version.&lt;/p&gt;
&lt;h3 name=&quot;8cdc&quot; id=&quot;8cdc&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;3. Web Application Servers&lt;/h3&gt;
&lt;p name=&quot;8b38&quot; id=&quot;8b38&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;At a high level web application servers are relatively simple to describe. They execute the core business logic that handles a user’s request and sends back HTML to the user’s browser. To do their job, they typically communicate with a variety of backend infrastructure such as databases, caching layers, job queues, search services, other microservices, data/logging queues, and more. As mentioned above, you typically have at least two and often times many more, plugged into a load balancer in order to process user requests.&lt;/p&gt;
&lt;p name=&quot;8c8c&quot; id=&quot;8c8c&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;You should know that app server implementations require choosing a specific language (Node.js, Ruby, PHP, Scala, Java, C# .NET, etc.) and a web MVC framework for that language (Express for Node.js, Ruby on Rails, Play for Scala, Laravel for PHP, etc.). However, diving into the details of these languages and frameworks is beyond the scope of this article.&lt;/p&gt;
&lt;h3 name=&quot;b1f1&quot; id=&quot;b1f1&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;4. Database Servers&lt;/h3&gt;
&lt;p name=&quot;51eb&quot; id=&quot;51eb&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Every modern web application leverages one or more databases to store information. Databases provide ways of defining your data structures, inserting new data, finding existing data, updating or deleting existing data, performing computations across the data, and more. In most cases the web app servers talk directly to one, as will the job servers. Additionally, each backend service may have it’s own database that’s isolated from the rest of the application.&lt;/p&gt;
&lt;p name=&quot;e7dc&quot; id=&quot;e7dc&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;While I’m avoiding a deep dive on particular technologies for each architecture component, I’d be doing you a disservice not to mention the next level of detail for databases: SQL and NoSQL.&lt;/p&gt;
&lt;p name=&quot;1be9&quot; id=&quot;1be9&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;SQL stands for “Structured Query Language” and was invented in the 1970s to provide a standard way of querying relational data sets that was accessible to a wide audience. SQL databases store data in tables that are linked together via common IDs, typically integers. Let’s walk through a simple example of storing historical address information for users. You might have two tables, users and user_addresses, linked together by the user’s id. See the image below for a simplistic version. The tables are linked because the user_id column in user_addresses is a “foreign key” to the id column in the users table.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*Ln39QPggpJVMAScUBsrcCQ.png&quot; data-width=&quot;451&quot; data-height=&quot;171&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*Ln39QPggpJVMAScUBsrcCQ.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;6d1e&quot; id=&quot;6d1e&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;If you don’t know much about SQL, I highly recommend walking through a tutorial like you can find on Khan Academy &lt;a href=&quot;https://www.khanacademy.org/computing/computer-programming/sql&quot; data-href=&quot;https://www.khanacademy.org/computing/computer-programming/sql&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;. It’s ubiquitous in web development so you’ll at least want to know the basics in order to properly architect an application.&lt;/p&gt;
&lt;p name=&quot;ea83&quot; id=&quot;ea83&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;NoSQL, which stands for “Non-SQL”, is a newer set of database technologies that has emerged to handle the massive amounts of data that can be produced by large scale web applications (most variants of SQL don’t scale horizontally very well and can only scale vertically to a certain point). If you don’t know anything about NoSQL, I recommend starting with some high level introductions like these:&lt;/p&gt;
&lt;p name=&quot;e54c&quot; id=&quot;e54c&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;I would also keep in mind that, by and large, &lt;a href=&quot;https://blog.timescale.com/why-sql-beating-nosql-what-this-means-for-future-of-data-time-series-database-348b777b847a&quot; data-href=&quot;https://blog.timescale.com/why-sql-beating-nosql-what-this-means-for-future-of-data-time-series-database-348b777b847a&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;the industry is aligning on SQL as an interface even for NoSQL databases&lt;/a&gt; so you really should learn SQL if you don’t know it. There’s almost no way to avoid it these days.&lt;/p&gt;
&lt;h3 name=&quot;d933&quot; id=&quot;d933&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;5. Caching Service&lt;/h3&gt;
&lt;p name=&quot;9a2d&quot; id=&quot;9a2d&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;A caching service provides a simple key/value data store that makes it possible to save and lookup information in close to O(1) time. Applications typically leverage caching services to save the results of expensive computations so that it’s possible to retrieve the results from the cache instead of recomputing them the next time they’re needed. An application might cache results from a database query, calls to external services, HTML for a given URL, and many more. Here are some examples from real world applications:&lt;/p&gt;
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;82ec&quot; id=&quot;82ec&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;Google caches search results for common search queries like “dog” or “Taylor Swift” rather than re-computing them each time&lt;/li&gt;
&lt;li name=&quot;14d1&quot; id=&quot;14d1&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Facebook caches much of the data you see when you log in, such as post data, friends, etc. Read a detailed article on Facebook’s caching tech &lt;a href=&quot;https://medium.com/@shagun/scaling-memcache-at-facebook-1ba77d71c082&quot; data-href=&quot;https://medium.com/@shagun/scaling-memcache-at-facebook-1ba77d71c082&quot; class=&quot;markup--anchor markup--li-anchor&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li name=&quot;1ba4&quot; id=&quot;1ba4&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Storyblocks caches the HTML output from server-side React rendering, search results, typeahead results, and more.&lt;/li&gt;
&lt;/ul&gt;&lt;p name=&quot;eb61&quot; id=&quot;eb61&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;The two most widespread caching server technologies are Redis and Memcache. I’ll go into more detail here in another post.&lt;/p&gt;
&lt;h3 name=&quot;c0b3&quot; id=&quot;c0b3&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;6. Job Queue &amp;amp; Servers&lt;/h3&gt;
&lt;p name=&quot;73ca&quot; id=&quot;73ca&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Most web applications need to do some work asynchronously behind the scenes that’s not directly associated with responding to a user’s request. For instance, Google needs to crawl and index the entire internet in order to return search results. It does not do this every time you search. Instead, it crawls the web asynchronously, updating the search indexes along the way.&lt;/p&gt;
&lt;p name=&quot;f4f0&quot; id=&quot;f4f0&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;While there are different architectures that enable asynchronous work to be done, the most ubiquitous is what I’ll call the “job queue” architecture. It consists of two components: a queue of “jobs” that need to be run and one or more job servers (often called “workers”) that run the jobs in the queue.&lt;/p&gt;
&lt;p name=&quot;d0c2&quot; id=&quot;d0c2&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Job queues store a list of jobs that need to be run asynchronously. The simplest are first-in-first-out (FIFO) queues though most applications end up needing some sort of priority queuing system. Whenever the app needs a job to be run, either on some sort of regular schedule or as determined by user actions, it simply adds the appropriate job to the queue.&lt;/p&gt;
&lt;p name=&quot;fc7f&quot; id=&quot;fc7f&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Storyblocks, for instance, leverages a job queue to power a lot of the behind-the-scenes work required to support our marketplaces. We run jobs to encode videos and photos, process CSVs for metadata tagging, aggregate user statistics, send password reset emails, and more. We started with a simple FIFO queue though we upgraded to a priority queue to ensure that time-sensitive operations like sending password reset emails were completed ASAP.&lt;/p&gt;
&lt;p name=&quot;6ae4&quot; id=&quot;6ae4&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Job servers process jobs. They poll the job queue to determine if there’s work to do and if there is, they pop a job off the queue and execute it. The underlying languages and frameworks choices are as numerous as for web servers so I won’t dive into detail in this article.&lt;/p&gt;
&lt;h3 name=&quot;a34f&quot; id=&quot;a34f&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;7. Full-text Search Service&lt;/h3&gt;
&lt;p name=&quot;593e&quot; id=&quot;593e&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Many if not most web apps support some sort of search feature where a user provides a text input (often called a “query”) and the app returns the most “relevant” results. The technology powering this functionality is typically referred to as “&lt;a href=&quot;https://en.wikipedia.org/wiki/Full-text_search&quot; data-href=&quot;https://en.wikipedia.org/wiki/Full-text_search&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;full-text search&lt;/a&gt;”, which leverages an &lt;a href=&quot;https://en.wikipedia.org/wiki/Inverted_index&quot; data-href=&quot;https://en.wikipedia.org/wiki/Inverted_index&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;inverted index&lt;/a&gt; to quickly look up documents that contain the query keywords.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*gun_BpdDH9KrNna1NnaocA.png&quot; data-width=&quot;561&quot; data-height=&quot;189&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*gun_BpdDH9KrNna1NnaocA.png&quot;/&gt;&lt;/div&gt;
Example showing how three document titles are converted into an inverted index to facilitate fast lookup from a specific keyword to the documents with that keyword in the title. Note, common words such as “in”, “the”, “with”, etc. (called stop words), are typically not included in an inverted index.
&lt;p name=&quot;17b8&quot; id=&quot;17b8&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;While it’s possible to do full-text search directly from some databases (e.g., &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/fulltext-search.html&quot; data-href=&quot;https://dev.mysql.com/doc/refman/5.7/en/fulltext-search.html&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;MySQL supports full-text search&lt;/a&gt;), it’s typical to run a separate “search service” that computes and stores the inverted index and provides a query interface. The most popular full-text search platform today is &lt;a href=&quot;https://www.elastic.co/products/elasticsearch&quot; data-href=&quot;https://www.elastic.co/products/elasticsearch&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Elasticsearch&lt;/a&gt; though there are other options such as &lt;a href=&quot;http://sphinxsearch.com/&quot; data-href=&quot;http://sphinxsearch.com/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Sphinx&lt;/a&gt; or &lt;a href=&quot;http://lucene.apache.org/solr/features.html&quot; data-href=&quot;http://lucene.apache.org/solr/features.html&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Apache Solr&lt;/a&gt;.&lt;/p&gt;
&lt;h3 name=&quot;32fd&quot; id=&quot;32fd&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;8. Services&lt;/h3&gt;
&lt;p name=&quot;90f1&quot; id=&quot;90f1&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Once an app reaches a certain scale, there will likely be certain “services” that are carved out to run as separate applications. They’re not exposed to the external world but the app and other services interact with them. Storyblocks, for example, has several operational and planned services:&lt;/p&gt;
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;c8f9&quot; id=&quot;c8f9&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Account service&lt;/strong&gt; stores user data across all our sites, which allows us to easily offer cross-sell opportunities and create a more unified user experience&lt;/li&gt;
&lt;li name=&quot;4282&quot; id=&quot;4282&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Content service&lt;/strong&gt; stores metadata for all of our video, audio, and image content. It also provides interfaces for downloading the content and viewing download history.&lt;/li&gt;
&lt;li name=&quot;5144&quot; id=&quot;5144&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Payment service&lt;/strong&gt; provides an interface for billing customer credit cards.&lt;/li&gt;
&lt;li name=&quot;7f9f&quot; id=&quot;7f9f&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;HTML → PDF service&lt;/strong&gt; provides a simple interface that accepts HTML and returns a corresponding PDF document.&lt;/li&gt;
&lt;/ul&gt;&lt;h3 name=&quot;e18c&quot; id=&quot;e18c&quot; class=&quot;graf graf--h3 graf-after--li&quot;&gt;9. Data&lt;/h3&gt;
&lt;p name=&quot;4d99&quot; id=&quot;4d99&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Today, companies live and die based on how well they harness data. Almost every app these days, once it reaches a certain scale, leverages a data pipeline to ensure that data can be collected, stored, and analyzed. A typical pipeline has three main stages:&lt;/p&gt;
&lt;ol class=&quot;postList&quot;&gt;&lt;li name=&quot;d07b&quot; id=&quot;d07b&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;The app sends data, typically events about user interactions, to the data “firehose” which provides a streaming interface to ingest and process the data. Often times the raw data is transformed or augmented and passed to another firehose. AWS Kinesis and Kafka are the two most common technologies for this purpose.&lt;/li&gt;
&lt;li name=&quot;4bc3&quot; id=&quot;4bc3&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;The raw data as well as the final transformed/augmented data are saved to cloud storage. AWS Kinesis provides a setting called “firehose” that makes saving the raw data to it’s cloud storage (S3) extremely easy to configure.&lt;/li&gt;
&lt;li name=&quot;f705&quot; id=&quot;f705&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;The transformed/augmented data is often loaded into a data warehouse for analysis. We use AWS Redshift, as does a large and growing portion of the startup world, though larger companies will often use Oracle or other proprietary warehouse technologies. If the data sets are large enough, a Hadoop-like NoSQL MapReduce technology may be required for analysis.&lt;/li&gt;
&lt;/ol&gt;&lt;p name=&quot;f1e9&quot; id=&quot;f1e9&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;Another step that’s not pictured in the architecture diagram: loading data from the app and services’ operational databases into the data warehouse. For example at Storyblocks we load our VideoBlocks, AudioBlocks, Storyblocks, account service, and contributor portal databases into Redshift every night. This provides our analysts a holistic dataset by co-locating the core business data alongside our user interaction event data.&lt;/p&gt;
&lt;h3 name=&quot;9811&quot; id=&quot;9811&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;10. Cloud storage&lt;/h3&gt;
&lt;p name=&quot;0346&quot; id=&quot;0346&quot; class=&quot;graf graf--p graf--startsWithDoubleQuote graf-after--h3&quot;&gt;“Cloud storage is a simple and scalable way to store, access, and share data over the Internet” &lt;a href=&quot;https://aws.amazon.com/what-is-cloud-storage/&quot; data-href=&quot;https://aws.amazon.com/what-is-cloud-storage/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;according to AWS&lt;/a&gt;. You can use it to store and access more or less anything you’d store on a local file system with the benefits of being able to interact with it via a RESTful API over HTTP. Amazon’s S3 offering is by far the most popular cloud storage available today and the one we rely on extensively here at Storyblocks to store our video, photo, and audio assets, our CSS and Javascript, our user event data and much more.&lt;/p&gt;
&lt;h3 name=&quot;311c&quot; id=&quot;311c&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;11. CDN&lt;/h3&gt;
&lt;p name=&quot;7402&quot; id=&quot;7402&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;CDN stands for “Content Delivery Network” and the technology provides a way of serving assets such as static HTML, CSS, Javascript, and images over the web much faster than serving them from a single origin server. It works by distributing the content across many “edge” servers around the world so that users end up downloading assets from the “edge” servers instead of the origin server. For instance in the image below, a user in Spain requests a web page from a site with origin servers in NYC, but the static assets for the page are loaded from a CDN “edge” server in England, preventing many slow cross-Atlantic HTTP requests.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*ZkC_5865Hx-Cgph3iPJghw.png&quot; data-width=&quot;918&quot; data-height=&quot;411&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*ZkC_5865Hx-Cgph3iPJghw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*ZkC_5865Hx-Cgph3iPJghw.png&quot;/&gt;&lt;/div&gt;
&lt;a href=&quot;https://www.creative-artworks.eu/why-use-a-content-delivery-network-cdn/&quot; data-href=&quot;https://www.creative-artworks.eu/why-use-a-content-delivery-network-cdn/&quot; class=&quot;markup--anchor markup--figure-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Source&lt;/a&gt;
&lt;p name=&quot;402e&quot; id=&quot;402e&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;&lt;a href=&quot;https://www.creative-artworks.eu/why-use-a-content-delivery-network-cdn/&quot; data-href=&quot;https://www.creative-artworks.eu/why-use-a-content-delivery-network-cdn/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Check out this article&lt;/a&gt; for a more thorough introduction. In general a web app should always use a CDN to serve CSS, Javascript, images, videos and any other assets. Some apps might also be able to leverage a CDN to serve static HTML pages.&lt;/p&gt;
&lt;h3 name=&quot;0532&quot; id=&quot;0532&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Parting thoughts&lt;/h3&gt;
&lt;p name=&quot;1777&quot; id=&quot;1777&quot; class=&quot;graf graf--p graf-after--h3 graf--trailing&quot;&gt;And that’s a wrap on Web Architecture 101. I hope you found this useful. I’ll hopefully post a series of 201 articles that provide deep dives into some of these components over the course of the next year or two.&lt;/p&gt;
</description>
<pubDate>Thu, 12 Jul 2018 18:11:46 +0000</pubDate>
<dc:creator>aksxna</dc:creator>
<og:title>Web Architecture 101 – VideoBlocks Product &amp; Engineering</og:title>
<og:url>https://engineering.videoblocks.com/web-architecture-101-a3224e126947/</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*K6M-x-6e39jMq_c-2xqZIQ.png</og:image>
<og:description>The basic architecture concepts I wish I knew when I was getting started as a web developer</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://engineering.videoblocks.com/web-architecture-101-a3224e126947/?ref=abhimanyu&amp;gi=975caf43ba81</dc:identifier>
</item>
<item>
<title>The IceCube Neutrino Detector at the South Pole Hits Paydirt</title>
<link>https://spectrum.ieee.org/tech-talk/aerospace/astrophysics/the-icecube-neutrino-detector-at-the-south-pole-hits-paydirt</link>
<guid isPermaLink="true" >https://spectrum.ieee.org/tech-talk/aerospace/astrophysics/the-icecube-neutrino-detector-at-the-south-pole-hits-paydirt</guid>
<description>&lt;img src=&quot;https://spectrum.ieee.org/image/MzA4OTIzOQ.jpeg&quot; alt=&quot;Illustration of neutrinos beneath the IceCube Neutrino Observatory&quot;/&gt; Photo-illustration: IceCube Collaboration/NSF
&lt;section id=&quot;side-module&quot;&gt;&lt;div class=&quot;medium-top-ad&quot;&gt;
&lt;div class=&quot;ad-module&quot;&gt;
&lt;p&gt;Advertisement&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;recommended-side&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;Editor’s Picks&lt;/p&gt;
&lt;div id=&quot;article-rec&quot; readability=&quot;38&quot;&gt;
&lt;div class=&quot;image-column&quot;&gt;&lt;img src=&quot;https://spectrum.ieee.org/image/MjY2NTY3Ng&quot; alt=&quot;null&quot;/&gt;&lt;/div&gt;
&lt;p&gt;
&lt;h4&gt;Gigantic Antarctic Instrument, IceCube, Finds Mysterious Cosmic Neutrinos&lt;/h4&gt;
&lt;/p&gt;

&lt;div class=&quot;image-column&quot;&gt;&lt;img src=&quot;https://spectrum.ieee.org/image/Mjc5NjgyOA&quot; alt=&quot;Particle detectors are lowered into a 2.5-kilometer-deep hole drilled into the Antarctic ice near South Pole Station.&quot;/&gt;&lt;/div&gt;
&lt;p&gt;
&lt;h4&gt;IceCube: The Polar Particle Hunter&lt;/h4&gt;
&lt;/p&gt;

&lt;div class=&quot;image-column&quot;&gt;&lt;img src=&quot;https://spectrum.ieee.org/image/Mjk2NjgyNQ&quot; alt=&quot;An illustration shows a collision of two neutron stars, as glowing bright pink balls against a black surface.&quot;/&gt;&lt;/div&gt;
&lt;p&gt;
&lt;h4&gt;LIGO Detects Collision of Neutron Stars&lt;/h4&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/section&gt;&lt;p&gt;After 3.9 billion years of hurtling unhindered through the vast reaches of the universe, a ghostly neutrino particle died on 22 September 2017. It was annihilated when it collided with an atom in the frozen darkness two kilometers beneath the surface of the south polar ice cap.&lt;/p&gt;&lt;p&gt;But this subatomic particle’s death did not go unmarked. It was announced &lt;a href=&quot;http://science.sciencemag.org/content/early/2018/07/11/science.aat2890&quot;&gt;today in &lt;em&gt;Science&lt;/em&gt;&lt;/a&gt; that its moment of passing—labelled neutrino event 170922A—triggered a world-wide cascade of astronomical observations using a raft of varied technologies. And these led to the first ever identification of the birthplace of a neutrino from outside our galaxy: in this case, the unimaginably violent cosmic forge of a &lt;a href=&quot;https://kipac.stanford.edu/research/topics/blazars-and-active-galactic-nuclei&quot;&gt;blazar&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Blazar&quot;&gt;Blazars&lt;/a&gt; are incredibly bright natural sources of radio waves. They form when some of the swirling material falling into a supermassive black hole is converted into a hot radiating soup of elementary particles and then gets blasted back out into space in the form of twin jets moving at close to the speed of light. Tracing the 170922A neutrino back to a blazar known as TXS 0506+056, located billions of light years away in the Orion constellation, required the rapid coordinated response of a network of observatories around the world and in orbit above it.&lt;/p&gt;
&lt;p&gt;The initial observation that kicked this so-called multimessenger observation campaign off was made by the &lt;a href=&quot;https://icecube.wisc.edu/&quot;&gt;IceCube detector&lt;/a&gt; at the South Pole. IceCube was created by using pressurized hot water to melt 86 shafts into the polar ice over a square kilometer. Before the shafts refroze, cables strung with 60 digital optical modules apiece were lowered down so that the modules sit evenly spaced every 17 meters between 1450 and 2450 meters deep. The result is a detector that encompasses a cubic kilometer of solid ice. The optical modules are sealed into basketball-sized spheres of borosilicate glass to withstand the crushing pressure, and are designed to spot the signature flashes of light that occur when a neutrino smashes into an atom in the clear ice.&lt;/p&gt;
&lt;img alt=&quot;Illustration explaining the IceCube detector&quot; src=&quot;https://spectrum.ieee.org/image/MzA4OTI2MA.jpeg&quot;/&gt; Illustration: Emily Cooper
&lt;p&gt;When a neutrino collides with any atom, sometimes a muon is produced—a particle that’s essentially a heavier version of an electron. When this happens in ice, the muon travels faster than the local speed of light. (Nothing can travel faster than light in a vacuum, but in ice the speed of light is about 24 percent slower, so a fast particle can outpace it.) When something goes faster than local light speed, a shockwave of photons forms, much like the way that a sonic boom is produced when a plane breaks through the sound barrier. This shockwave creates an eerie blue light known as &lt;a href=&quot;http://www.radioactivity.eu.com/site/pages/Cherenkov_Effect.htm&quot;&gt;Cherenkov radiation&lt;/a&gt;, and measuring the direction and intensity of the light reveals how much energy the original neutrino possessed, and where in the sky it came from.&lt;/p&gt;
&lt;p&gt;Neutrino collisions are exceedingly rare—trillions of neutrinos from the sun stream through your body every second without so much as wobbling an electron—so IceCube has to be big for it to have a statistical chance of catching a collision before the researchers die of old age (IceCube is even bigger than it seems: as long as the resulting muon passes through the detector array, it can spot neutrino collisions that occur in the surrounding ice cap up to about 10 kilometers away). And IceCube has to be deep beneath the surface because only there is the pressure intense enough—700 times normal atmospheric pressure—to squeeze all the air bubbles out of the ice. The bubbles have to be eliminated because they would otherwise scatter the Cherenkov radiation that the detectors are looking for. For a comprehensive description of IceCube’s design, right down to the FPGAs used in the optical modules, you can read &lt;a href=&quot;https://spectrum.ieee.org/aerospace/astrophysics/icecube-the-polar-particle-hunter&quot;&gt;the comprehensive account that team member Spencer Klein wrote&lt;/a&gt; for &lt;em&gt;IEEE Spectrum&lt;/em&gt; in 2011. That article is still &lt;em&gt;au courant&lt;/em&gt;: since it was published “there haven’t been any major changes,” says Klein, “The optical modules are buried under a mile and a half of ice, so there’s no way to really access the hardware. There have been some very minor firmware updates.”&lt;/p&gt;
&lt;p&gt;The biggest change since, says Klein, has been in the creation of an automated alert system. This system broadcast an alert to astronomers working around the world just 43 seconds after the 170922A event occurred. IceCube detects muon shockwaves all the time, but these are generally due to low-energy muons that are produced in the Earth’s atmosphere by cosmic rays. These background muons have a different shockwave signature than those produced by high-energy extragalactic neutrinos, but filtering them out automatically requires a detailed model of the optical properties of the specific ice sheet in which IceCube is embedded.&lt;/p&gt;
&lt;p&gt;In particular, the ice is slightly contaminated by dust. The dust comes from two sources, accumulated over tens of thousands of years as the sheet slowly formed from surface snowfalls. One is “volcanic eruptions which produce very, very thin, but relatively dense, layers” that run throughout the ice, says Klein. The other source is regular atmospheric dust which isn’t as dense, but occurs throughout the ice: “If there’s dust in the atmosphere, some of that dust will get dragged down with the snow. That does change with time somewhat, so we have a picture of the dust content in Antarctica over the last 70,000 years.”&lt;/p&gt;
&lt;p&gt;With experience gained from several years of operation, the IceCube team has considerably improved their understanding of their patch of ice and how the detector behaves in response. Consequently, they spun up the automated system in April 2016.  “The detectors and the computer systems at the south pole look for interesting events and can automatically send out an alert when it sees that something interesting has happened. It takes a fair amount of confidence to get to that point,” says Klein. “It used to be that [a candidate event for an alert] would go to a human being, who would look at it and then send out the alert. That takes time. This alert went out in under a minute.”&lt;/p&gt;
&lt;p&gt;The 170922A event alert, with its estimated coordinates of the neutrino’s origin in the sky, went out to astronomers running instruments which can detect gamma rays, such as those onboard the orbiting &lt;a href=&quot;https://swift.gsfc.nasa.gov/&quot;&gt;Swift observatory&lt;/a&gt;. Swift quickly spotted that the 170922A event coordinates matched with those of known blazar TXS 0506+056, and that the blazar was flaring in brightness. “Thanks to the automated trigger … Swift was observing within four hours of the neutrino detection,” said Jamie Kennea, science operation team lead for Swift in a press release.&lt;/p&gt;
&lt;p&gt;As the next 14 days rolled on, more and more instruments were brought to bear on TXS 0506+056, allowing it to be monitored across a range of wavelengths from radio, through optical, all the way to x-ray. The 170922A event coincided with a period of heightened activity of TXS 0506+056, and researchers have concluded that it’s 99.7 percent likely that the detected neutrino originated in the flaring blazar. “The fact that we could tie gamma rays and neutrinos together tells us very exciting things about the particle jet,” said &lt;span&gt;Regina Caputo, analysis coordinator for the satellite-based &lt;a href=&quot;http://www-glast.stanford.edu/&quot;&gt;Fermi-LAT&lt;/a&gt; gamma ray telescope, at an NSF press conference today.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With the evidence pointing to &lt;span&gt;TXS 0506+056&lt;/span&gt; in hand, the IceCube researchers also checked through their complete records and found that there were between 8 and 18 previous neutrino events that hadn’t met the threshold required for sending an alert, but which likely were also produced by neutrinos streaming from the blazar.&lt;/p&gt;
&lt;p&gt;“It’s a pretty amazing finding,” says Klein. “but more data is needed. The multimessenger campaign was based on one neutrino. It’s great to know about one [extragalactic source] but we have a ways to go before we have a systemic understanding.” The IceCube team hopes to get more data by building a next-generation array by spreading out a similar number of digital optical modules over a larger area, and adding seven more closely-spaced strings to the original IceCube array. The seven strings in particular “would allow us to do a much better job of understanding the optical properties of the ice,” says Klein, which would let them pinpoint the sources of neutrinos with even greater accuracy.&lt;/p&gt;
</description>
<pubDate>Thu, 12 Jul 2018 17:55:45 +0000</pubDate>
<dc:creator>amynordrum</dc:creator>
<og:title>The IceCube Neutrino Detector at the South Pole Hits Paydirt</og:title>
<og:url>https://spectrum.ieee.org/tech-talk/aerospace/astrophysics/the-icecube-neutrino-detector-at-the-south-pole-hits-paydirt</og:url>
<og:description>A single subatomic collision has opened a new door in astronomy</og:description>
<og:image>https://spectrum.ieee.org/image/MzA4OTI3OQ.jpeg</og:image>
<og:type>blog-tech-talk</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://spectrum.ieee.org/tech-talk/aerospace/astrophysics/the-icecube-neutrino-detector-at-the-south-pole-hits-paydirt</dc:identifier>
</item>
<item>
<title>Seedbank – Collection of Interactive Machine Learning Examples</title>
<link>http://tools.google.com/seedbank/</link>
<guid isPermaLink="true" >http://tools.google.com/seedbank/</guid>
<description>[unable to retrieve full-text content]&lt;p&gt;Article URL: &lt;a href=&quot;http://tools.google.com/seedbank/&quot;&gt;http://tools.google.com/seedbank/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=17516709&quot;&gt;https://news.ycombinator.com/item?id=17516709&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Points: 235&lt;/p&gt;&lt;p&gt;# Comments: 14&lt;/p&gt;</description>
<pubDate>Thu, 12 Jul 2018 17:20:25 +0000</pubDate>
<dc:creator>gunzor</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>http://tools.google.com/seedbank/</dc:identifier>
</item>
<item>
<title>Why Kubernetes Is the New Application Server</title>
<link>https://developers.redhat.com/blog/2018/06/28/why-kubernetes-is-the-new-application-server/</link>
<guid isPermaLink="true" >https://developers.redhat.com/blog/2018/06/28/why-kubernetes-is-the-new-application-server/</guid>
<description>&lt;div class=&quot;entry-meta&quot;&gt;
&lt;div class=&quot;row entry-meta-row&quot;&gt;
&lt;div class=&quot; columns medium-16 small-24 author-post-info&quot;&gt;
&lt;div class=&quot;posted-on&quot;&gt;
&lt;div class=&quot;avatar-outer&quot;&gt;&lt;img class=&quot;avatar&quot; src=&quot;https://secure.gravatar.com/avatar/087b923821c70dc1f8965b036d6aa6e2?s=30&amp;amp;d=mm&amp;amp;r=g&quot; alt=&quot;rafabene&quot;/&gt;&lt;/div&gt;
By &lt;a title=&quot;Rafael Benevides&quot; href=&quot;https://developers.redhat.com/blog/author/rafabene/&quot;&gt;Rafael Benevides&lt;/a&gt; &lt;time class=&quot;entry-date published&quot; datetime=&quot;2018-06-28T07:00:34+00:00&quot;&gt;June 28, 2018&lt;/time&gt;&lt;time class=&quot;updated&quot; datetime=&quot;2018-06-28T14:28:55+00:00&quot;&gt;June 28, 2018&lt;/time&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;columns medium-8 small-24 ratings-column&quot;&gt;
&lt;div class=&quot;post-ratings-container&quot;&gt;
&lt;div id=&quot;post-ratings-495187&quot; class=&quot;post-ratings&quot; data-nonce=&quot;66b95084fa&quot;&gt;&lt;span class=&quot;vote-box&quot;&gt;&lt;img id=&quot;rating_495187_1&quot; src=&quot;https://developers.redhat.com/blog/wp-content/plugins/wp-postratings/images/thumbs/rating_1_half.gif&quot; alt=&quot;-1&quot; title=&quot;-1&quot; onmouseover=&quot;current_rating(495187, 1, '-1');&quot; onmouseout=&quot;ratings_off(0.6, 1, 0);&quot; onclick=&quot;rate_post();&quot; onkeypress=&quot;rate_post();&quot;/&gt;&lt;img id=&quot;rating_495187_2&quot; src=&quot;https://developers.redhat.com/blog/wp-content/plugins/wp-postratings/images/thumbs/rating_2_off.gif&quot; alt=&quot;+1&quot; title=&quot;+1&quot; onmouseover=&quot;current_rating(495187, 2, '+1');&quot; onmouseout=&quot;ratings_off(0.6, 1, 0);&quot; onclick=&quot;rate_post();&quot; onkeypress=&quot;rate_post();&quot;/&gt;&lt;/span&gt;  &lt;strong&gt;+10&lt;/strong&gt; rating, &lt;strong&gt;16&lt;/strong&gt; votes&lt;/div&gt;
&lt;div id=&quot;post-ratings-495187-loading&quot; class=&quot;post-ratings-loading&quot;&gt;&lt;img src=&quot;https://developers.redhat.com/blog/wp-content/plugins/wp-postratings/images/loading.gif&quot; width=&quot;16&quot; height=&quot;16&quot; class=&quot;post-ratings-image&quot;/&gt;Loading...&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;img width=&quot;640&quot; height=&quot;491&quot; src=&quot;https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-05-18-21.20.31-1024x786.png&quot; class=&quot;single-post-featured-img wp-post-image&quot; alt=&quot;&quot; srcset=&quot;https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-05-18-21.20.31-1024x786.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-05-18-21.20.31-300x230.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-05-18-21.20.31-768x589.png 768w&quot; sizes=&quot;(max-width: 640px) 100vw, 640px&quot;/&gt;&lt;p&gt;Have you ever wondered why you are deploying your multi-platform applications using containers? Is it just a matter of “following the hype”? In this article, I’m going to ask some provocative questions to make my case for &lt;em&gt;Why Kubernetes is the new application server&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;You might have noticed that the majority of languages are interpreted and use “runtimes” to execute your source code. In theory, most Node.js, Python, and Ruby code can be easily moved from one platform (Windows, Mac, Linux) to another platform. Java applications go even further by having the compiled Java class turned into a bytecode, capable of running anywhere that has a JVM (Java Virtual Machine).&lt;/p&gt;
&lt;p&gt;The Java ecosystem provides a standard format to distribute all Java classes that are part of the same application. You can package these classes as a JAR (Java Archive), WAR (Web Archive), and EAR (Enterprise Archive) that contains the front end, back end, and libraries embedded. So I ask you: Why do you use containers to distribute your Java application? Isn’t it already supposed to be easily portable between environments?&lt;/p&gt;

&lt;p&gt;Answering this question from a developer perspective isn’t always obvious. But think for a moment about your development environment and some possible issues caused by the difference between it and the production environment:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Do you use Mac, Windows, or Linux? Have you ever faced an issue related to &lt;code&gt;\&lt;/code&gt; versus &lt;code&gt;/&lt;/code&gt; as the file path separator?&lt;/li&gt;
&lt;li&gt;What version of JDK do you use? Do you use Java 10 in development, but production uses JRE 8? Have you faced any bugs introduced by  JVM differences?&lt;/li&gt;
&lt;li&gt;What version of the application server do you use? Is the production environment using the same configuration, security patches, and library versions?&lt;/li&gt;
&lt;li&gt;During production deployment, have you encountered a JDBC driver issue that you didn’t face in your development environment due to different versions of the driver or database server?&lt;/li&gt;
&lt;li&gt;Have you ever asked the application server admin to create a datasource or a JMS queue and it had a typo?&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;All the issues above are caused by factors external to your application, and one of the greatest things about containers is that you can deploy everything (for example, a Linux distribution, the JVM, the application server, libraries, configurations and, finally, your application) inside a pre-built container. Plus, executing a single container that has everything built in is incredibly easier than moving your code to a production environment and trying to resolve the differences when it doesn’t work. Since it’s easy to execute, it is also easy to scale the same container image to multiple replicas.&lt;/p&gt;
&lt;h2&gt;Empowering Your Application&lt;/h2&gt;
&lt;p&gt;Before containers became very popular, several &lt;a href=&quot;https://en.wikipedia.org/wiki/Non-functional_requirement#Examples&quot;&gt;NFR (non-functional requirements)&lt;/a&gt; such as security, isolation, fault tolerance, configuration management, and others were provided by application servers. As an analogy, the application servers were planned to be to applications what CD (Compact Disc) players are to CDs.&lt;/p&gt;
&lt;p&gt;As a developer, you would be responsible to follow a predefined standard and distribute the application in a specific format, while on the other hand the application server would “execute” your application and give additional capabilities that could vary from different “brands.”  Note: In the Java world, the standard for enterprise capabilities provided by an application server has recently moved under the Eclipse foundation. The work on Eclipse Enterprise for Java (&lt;a href=&quot;https://projects.eclipse.org/projects/ee4j&quot;&gt;EE4J&lt;/a&gt;), has resulted in &lt;a href=&quot;https://jakarta.ee/&quot;&gt;Jakarta EE&lt;/a&gt;.  (For more info, read the article &lt;a href=&quot;https://developers.redhat.com/blog/2018/04/24/jakarta-ee-is-officially-out/&quot;&gt;&lt;em&gt;Jakarta EE is officially out&lt;/em&gt;&lt;/a&gt; or watch the &lt;a href=&quot;https://developers.redhat.com/videos/youtube/f2EwhTUmeOI/&quot;&gt;DevNation video: &lt;em&gt;Jakarta EE: The future of Java EE&lt;/em&gt;&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Following the same CD player analogy, with the ascension of containers, the &lt;a href=&quot;https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction/&quot;&gt;&lt;em&gt;container image&lt;/em&gt;&lt;/a&gt; has become the new CD format. In fact, a container image is nothing more than a format for distributing your containers. (If you need to get a better handle on what container images are and how they are distributed see &lt;em&gt;&lt;a href=&quot;https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction/&quot;&gt;A Practical Introduction to Container Terminology&lt;/a&gt;&lt;/em&gt;.)&lt;/p&gt;
&lt;p&gt;The real benefits of containers happen when you need to add enterprise capabilities to your application. And the best way to provide these capabilities to a containerized application is by using Kubernetes as a platform for them. Additionally, the Kubernetes platform provides a great foundation for other projects such as &lt;a href=&quot;https://www.openshift.com/&quot;&gt;Red Hat OpenShift&lt;/a&gt;, &lt;a href=&quot;https://istio.io/&quot;&gt;Istio&lt;/a&gt;, and &lt;a href=&quot;https://openwhisk.apache.org/&quot;&gt;Apache OpenWhisk&lt;/a&gt; to build on and make it easier to build and deploy robust production quality applications.&lt;/p&gt;
&lt;p&gt;Let’s explore nine of these capabilities:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-05-18-21.20.31.png&quot;&gt;&lt;img src=&quot;https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-05-18-21.20.31.png&quot; alt=&quot;&quot; width=&quot;1634&quot; height=&quot;1254&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;1 – Service Discovery&lt;/h3&gt;
&lt;p&gt;Service discovery is the process of figuring out how to connect to a service.  To get many of the benefits of containers and cloud-native applications, you need to remove configuration from your container images so you can use the same container image in all environments. Externalized configuration from applications is one of the key principles of the &lt;a href=&quot;https://developers.redhat.com/blog/2017/06/22/12-factors-to-cloud-success/&quot;&gt;12-factor application&lt;/a&gt;. Service discovery is one of the ways to get configuration information from the runtime environment instead of it being hardcoded in the application. Kubernetes provides &lt;a href=&quot;https://kubernetes.io/docs/concepts/services-networking/service/#discovering-services&quot;&gt;service discovery&lt;/a&gt; out of the box. Kubernetes also provides &lt;a href=&quot;https://kubernetes-v1-4.github.io/docs/user-guide/configmap/&quot;&gt;ConfigMaps&lt;/a&gt; and &lt;a href=&quot;https://kubernetes.io/docs/concepts/configuration/secret/&quot;&gt;Secrets&lt;/a&gt; for removing configuration from your application containers.  Secrets solve some of the challenges that arise when you need to store the credentials for connecting to a service like a database in your runtime environment.&lt;/p&gt;
&lt;p&gt;With Kubernetes, there’s no need to use an external server or framework.  While you can manage the environment settings for each runtime environment through Kubernetes YAML files, Red Hat OpenShift provides a GUI and CLI that can make it easier for DevOps teams to manage.&lt;/p&gt;
&lt;h3&gt;2 – Basic Invocation&lt;/h3&gt;
&lt;p&gt;Applications running inside containers can be accessed through &lt;a href=&quot;https://kubernetes.io/docs/concepts/services-networking/ingress/&quot;&gt;Ingress&lt;/a&gt; access— in other words, routes from the outside world to the service you are exposing. OpenShift provides &lt;a href=&quot;https://docs.openshift.com/container-platform/3.9/architecture/networking/routes.html#overview&quot;&gt;route objects&lt;/a&gt; using HAProxy, which has several capabilities and load-balancing strategies.  You can use the routing capabilities to do rolling deployments. This can be the basis of some very sophisticated CI/CD strategies. See “6 – Build and Deployment Pipelines” below.&lt;/p&gt;
&lt;p&gt;What if you need to run a one-time job, such as a batch process, or simply leverage the cluster to compute a result (such as computing the digits of Pi)? Kubernetes provides &lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/&quot;&gt;job objects&lt;/a&gt; for this use case. There is also a &lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/&quot;&gt;cron job&lt;/a&gt; that manages time-based jobs.&lt;/p&gt;
&lt;h3&gt;3 – Elasticity&lt;/h3&gt;
&lt;p&gt;Elasticity is solved in Kubernetes by using &lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/&quot;&gt;ReplicaSets&lt;/a&gt; (which used to be called Replication Controllers). Just like most configurations for Kubernetes, a ReplicaSet is a way to reconcile a desired state: you tell Kubernetes what state the system should be in and Kubernetes figures out how to make it so. A ReplicaSet controls the number of replicas or exact copies of the app that should be running at any time.&lt;/p&gt;
&lt;p&gt;But what happens when you build a service that is even more popular than you planned for and you run out of compute? You can use the Kubernetes &lt;a href=&quot;https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#what-is-the-horizontal-pod-autoscaler&quot;&gt;Horizontal Pod Autoscaler&lt;/a&gt;, which scales the number of pods based on observed CPU utilization (or, with &lt;a href=&quot;https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/custom-metrics-api.md&quot;&gt;custom metrics&lt;/a&gt; support, on some other application-provided metrics).&lt;/p&gt;
&lt;h3&gt;4 – Logging&lt;/h3&gt;
&lt;p&gt;Since your Kubernetes cluster can and will run several replicas of your containerized application, it’s important that you aggregate these logs so they can be viewed in one place. Also, in order to utilize benefits like autoscaling (and other cloud-native capabilities), your containers need to be immutable. So you need to store your logs outside of your container so they will be persistent across runs. OpenShift allows you to deploy the EFK stack to aggregate logs from hosts and applications, whether they come from multiple containers or even from deleted pods.&lt;/p&gt;
&lt;p&gt;The EFK stack is composed of:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://www.elastic.co/products/elasticsearch&quot;&gt;Elasticsearch&lt;/a&gt; (ES), an object store where all logs are stored&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.fluentd.org/architecture&quot;&gt;Fluentd&lt;/a&gt;, which gathers logs from nodes and feeds them to Elasticsearch&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/kibana/current/introduction.html&quot;&gt;Kibana&lt;/a&gt;, a web UI for Elasticsearch&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt;5 – Monitoring&lt;/h3&gt;
&lt;p&gt;Although logging and monitoring seem to solve the same problem, they are different from each other. Monitoring is observation, checking, often alerting, as well as recording. Logging is recording only.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://prometheus.io/&quot;&gt;Prometheus&lt;/a&gt; is an open-source monitoring system that includes time series database. It can be used for storing and querying metrics, alerting, and using visualizations to gain insights into your systems. Prometheus is perhaps the most popular choice for monitoring Kubernetes clusters. On the &lt;a href=&quot;https://developers.redhat.com/blog/&quot;&gt;Red Hat Developers blog&lt;/a&gt;, there are several articles covering monitoring using &lt;a href=&quot;https://developers.redhat.com/blog/tag/prometheus/&quot;&gt;Prometheus&lt;/a&gt;. You can also find Prometheus articles on the &lt;a href=&quot;https://blog.openshift.com/tag/prometheus/&quot;&gt;OpenShift blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can also see Prometheus in action together with Istio at &lt;a href=&quot;https://learn.openshift.com/servicemesh/3-monitoring-tracing&quot;&gt;https://learn.openshift.com/servicemesh/3-monitoring-tracing&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;6 – Build and Deployment Pipelines&lt;/h3&gt;
&lt;p&gt;CI/CD (Continuous Integration/Continuous Delivery) pipelines are not a strict “must have” requirement for your applications. However, CI/CD are often cited as pillars of successful software development and &lt;a href=&quot;https://devops.com/optimizing-effective-cicd-pipeline/&quot;&gt;DevOps&lt;/a&gt; practices.  No software should be deployed into production without a CI/CD pipeline. The book &lt;a href=&quot;https://www.amazon.com/dp/0321601912?tag=contindelive-20&quot;&gt;&lt;em&gt;Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation&lt;/em&gt;&lt;/a&gt;, by Jez Humble and David Farley, says this about CD: “Continuous Delivery is the ability to get changes of all types—including new features, configuration changes, bug fixes and experiments—into production, or into the hands of users, safely and quickly in a sustainable way.”&lt;/p&gt;
&lt;p&gt;OpenShift provides CI/CD pipelines out of the box as a “&lt;a href=&quot;https://docs.openshift.com/container-platform/3.7/dev_guide/builds/build_strategies.html#pipeline-strategy-options&quot;&gt;build strategy&lt;/a&gt;.” Check out &lt;a href=&quot;https://www.youtube.com/watch?v=N8R3-eNVoEc&quot;&gt;this video&lt;/a&gt; that I recorded two years ago, which has an example of a Jenkins CI/CD pipeline that deploys a new microservice.&lt;/p&gt;
&lt;h3&gt;7 – Resilience&lt;/h3&gt;
&lt;p&gt;While Kubernetes provides resilience options for the &lt;a href=&quot;https://docs.openshift.com/container-platform/3.9/admin_guide/high_availability.html&quot;&gt;cluster itself&lt;/a&gt;, it can also help the application be resilient by providing &lt;a href=&quot;https://kubernetes.io/docs/concepts/storage/persistent-volumes/&quot;&gt;PersistentVolumes&lt;/a&gt; that support replicated volumes. Kubernetes’ &lt;a href=&quot;https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/&quot;&gt;ReplicationControllers&lt;/a&gt;/deployments ensure that the specified numbers of pod replicas are consistently deployed across the cluster, which automatically handles any possible &lt;a href=&quot;https://kubernetes.io/docs/concepts/architecture/nodes/#what-is-a-node&quot;&gt;node&lt;/a&gt; failure.&lt;/p&gt;
&lt;p&gt;Together with resilience, fault tolerance serves as an effective means to address users’ reliability and availability concerns. Fault tolerance can also be provided to an application that is running on Kubernetes through &lt;a href=&quot;https://istio.io/&quot;&gt;Istio&lt;/a&gt; by its retries rules, circuit breaker, and pool ejection. Do you want to see it for yourself? Try the Istio Circuit Breaker tutorial at &lt;a href=&quot;https://learn.openshift.com/servicemesh/7-circuit-breaker&quot;&gt;https://learn.openshift.com/servicemesh/7-circuit-breaker&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;8 – Authentication&lt;/h3&gt;
&lt;p&gt;Authentication in Kubernetes can also be provided by Istio through its &lt;a href=&quot;https://istio.io/docs/concepts/security/mutual-tls.html&quot;&gt;mutual TLS authentication&lt;/a&gt;, which aims to enhance the security of microservices and their communication without requiring service code changes. It is responsible for:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Providing each service with a strong identity that represents its role to enable interoperability across clusters and clouds&lt;/li&gt;
&lt;li&gt;Securing service-to-service communication and end user-to-service communication&lt;/li&gt;
&lt;li&gt;Providing a key management system to automate key and certificate generation, distribution, rotation, and revocation&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Additionally, it is worth mentioning that you can also run &lt;a href=&quot;https://www.keycloak.org/&quot;&gt;Keycloak&lt;/a&gt; inside a Kubernetes/OpenShift cluster to provide both authentication and authorization. Keycloak is the upstream product for Red Hat Single Sign-on. For more information, read &lt;a href=&quot;https://developers.redhat.com/blog/tag/keycloak/&quot;&gt;Single-Sign On Made Easy with Keycloak&lt;/a&gt;. If you are using Spring Boot, watch the DevNation video: &lt;a href=&quot;https://developers.redhat.com/videos/youtube/Bdg_DjuoX0A/&quot;&gt;Secure Spring Boot Microservices with Keycloak&lt;/a&gt; or &lt;a href=&quot;https://developers.redhat.com/blog/tag/keycloak/&quot;&gt;read the blog article&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;9 – Tracing&lt;/h3&gt;
&lt;p&gt;Istio-enabled applications can be configured to collect trace spans using &lt;a href=&quot;https://zipkin.io/&quot;&gt;Zipkin&lt;/a&gt; or &lt;a href=&quot;https://www.jaegertracing.io/docs/&quot;&gt;Jaeger&lt;/a&gt;. Regardless of what language, framework, or platform you use to build your application, Istio can enable distributed tracing. Check it out at &lt;a href=&quot;https://learn.openshift.com/servicemesh/3-monitoring-tracing&quot;&gt;https://learn.openshift.com/servicemesh/3-monitoring-tracing&lt;/a&gt;.  See also &lt;a href=&quot;https://developers.redhat.com/blog/2018/05/08/getting-started-with-istio-and-jaeger-on-your-laptop/&quot;&gt;Getting Started with Istio and Jaeger on your laptop&lt;/a&gt; and the recent DevNation video: &lt;a href=&quot;https://developers.redhat.com/blog/2018/06/20/next-devnation-live-advanced-microservices-tracing-with-jaeger-june-21st-12pm-edt/&quot;&gt;Advanced microservices tracing with Jaeger&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Are Application Servers Dead?&lt;/h2&gt;
&lt;p&gt;Going through these capabilities, you can realize how Kubernetes + OpenShift + Istio can really empower your application and provide features that used to be the responsibility of an application server or a software framework such as &lt;a href=&quot;https://netflix.github.io/&quot;&gt;Netflix OSS&lt;/a&gt;. Does that mean application servers are dead?&lt;/p&gt;
&lt;p&gt;In this new containerized world, application servers are mutating into becoming more like frameworks. It’s natural that the evolution of software development caused the evolution of application servers. A great example of this evolution is the &lt;a href=&quot;http://microprofile.io/&quot;&gt;Eclipse MicroProfile&lt;/a&gt; specification having &lt;a href=&quot;http://wildfly-swarm.io&quot;&gt;WildFly Swarm&lt;/a&gt; as the application server, which provides to the developer features such as fault tolerance, configuration, tracing, REST (client and server), and so on. However, WildFly Swarm and the MicroProfile specification are designed to be very lightweight. WildFly Swarm doesn’t have the vast array of components required by a full Java enterprise application server. Instead, it focuses on microservices and having just enough of the application server to build and run your application as a simple executable .jar file.  You can read more about &lt;a href=&quot;https://developers.redhat.com/blog/tag/microprofile/&quot;&gt;MicroProfile&lt;/a&gt; on this blog.&lt;/p&gt;
&lt;p&gt;Furthermore, Java applications can have features such as the Servlet engine, a datasource pool, dependency injection, transactions, messaging, and so forth. Of course, frameworks can provide these features, but an application server must also have everything you need to build, run, deploy, and manage enterprise applications in any environment, regardless of whether they are inside containers. In fact, application servers can be executed anywhere, for instance, on bare metal, on virtualization platforms such as &lt;a href=&quot;https://www.redhat.com/en/technologies/virtualization/enterprise-virtualization&quot;&gt;Red Hat Virtualization&lt;/a&gt;, on private cloud environments such as &lt;a href=&quot;https://www.openstack.org/&quot;&gt;Red Hat OpenStack Platform&lt;/a&gt;, and also on public cloud environments such as &lt;a href=&quot;https://azure.microsoft.com/en-us/&quot;&gt;Microsoft Azure&lt;/a&gt; or &lt;a href=&quot;https://aws.amazon.com/&quot;&gt;Amazon Web Services&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A good application server ensures consistency between the APIs that are provided and their implementations. Developers can be sure that deploying their business logic, which requires certain capabilities, will work because the application server developers (and the defined standards) have ensured that these components work together and have evolved together. Furthermore, a good application server is also responsible for maximizing throughput and scalability, because it will handle all the requests from the users; having reduced latency and improved load times, because it will help your application’s &lt;a href=&quot;https://12factor.net/disposability&quot;&gt;disposability&lt;/a&gt;; be lightweight with a small footprint that minimizes hardware resources and costs; and finally, be secure enough to avoid any security breach. For Java developers, Red Hat provides &lt;a href=&quot;https://www.redhat.com/en/technologies/jboss-middleware/application-platform&quot;&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt;, which fulfills all the requirements of a modern, modular application server.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Container images have become the standard packaging format to distribute cloud-native applications. While containers “per se” don’t provide real business advantages to applications, Kubernetes and its related projects, such as OpenShift and Istio, provide the non-functional requirements that used to be part of an application server.&lt;/p&gt;
&lt;p&gt;Most of these non-functional requirements that developers used to get from an application server or from a library such as &lt;a href=&quot;https://netflix.github.io/&quot;&gt;Netflix OSS&lt;/a&gt; were bound to a specific language, for example, Java. On the other hand, when developers choose to meet these requirements using Kubernetes + OpenShift + Istio, they are not attached to any specific language, which can encourage the use of the best technology/language for each use case.&lt;/p&gt;
&lt;p&gt;Finally, application servers still have their place in software development. However, they are mutating into becoming more like language-specific frameworks that are a great shortcut when developing applications, since they contain lots of already written and tested functionality.&lt;/p&gt;
&lt;p&gt;One of the best things about moving to containers, Kubernetes, and microservices is that you don’t have to choose a single application server, framework, architectural style or even language for your application. You can easily deploy a container with JBoss EAP running your existing Java EE application, alongside other containers that have new microservices using Wildfly Swarm, or Eclipse Vert.x for reactive programming. These containers can all be managed through Kubernetes. To see this concept in action, take a look at &lt;a href=&quot;https://developers.redhat.com/products/rhoar/overview/&quot;&gt;Red Hat OpenShift Application Runtimes&lt;/a&gt;. Use the &lt;a href=&quot;https://developers.redhat.com/launch/&quot;&gt;Launch service&lt;/a&gt; to build and deploy a sample app online using WildFly Swarm, Vert.x, Spring Boot, or Node.js. Select the Externalized Configuration mission to learn how to use Kubernetes ConfigMaps. This will get you started on your path to cloud-native applications.&lt;/p&gt;
&lt;p&gt;You can say that &lt;a href=&quot;https://www.linkedin.com/pulse/openshift-new-enterprise-linux-daniel-riek/&quot;&gt;Kubernetes/OpenShift is the new Linux&lt;/a&gt; or even that “Kubernetes is the new application server.” But the fact is that an application server/runtime + OpenShift/Kubernetes + Istio has become the “de facto” cloud-native application platform!&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;If you haven’t been to the Red Hat Developer site lately, you should check out the pages on:&lt;/p&gt;

&lt;div class=&quot;author&quot;&gt;
&lt;p&gt;&lt;img class=&quot;author-photo alignleft&quot; title=&quot;Rafael&quot; src=&quot;http://rafabene.com/images/rafaelbenevides.jpg&quot; alt=&quot;Rafael Benevides&quot; width=&quot;119&quot; height=&quot;119&quot;/&gt;&lt;/p&gt;
&lt;h3&gt;About the author:&lt;/h3&gt;
&lt;p&gt;Rafael Benevides is Director of Developer Experience at &lt;a href=&quot;http://www.redhat.com&quot;&gt;Red Hat&lt;/a&gt;. With many years of experience in several fields of the IT industry, he helps developers and companies all over the world to be more effective in software development. Rafael considers himself a problem solver who has a big love for sharing. He is a member of Apache DeltaSpike PMC—a Duke’s Choice Award winner project—and a speaker in conferences such as JavaOne, Devoxx, TDC, DevNexus, and many others.| &lt;a href=&quot;https://www.linkedin.com/in/rafaelbenevides&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;LinkedIn&lt;/a&gt; | &lt;a href=&quot;http://rafabene.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;rafabene.com&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;



&lt;div class=&quot;addtoany_share_save_container addtoany_content addtoany_content_bottom&quot;&gt;
&lt;div class=&quot;a2a_kit a2a_kit_size_16 addtoany_list&quot; data-a2a-url=&quot;https://developers.redhat.com/blog/2018/06/28/why-kubernetes-is-the-new-application-server/&quot; data-a2a-title=&quot;Why Kubernetes is The New Application Server&quot;&gt;&lt;a class=&quot;a2a_dd addtoany_share_save addtoany_share&quot; href=&quot;https://www.addtoany.com/share&quot;&gt;&lt;img src=&quot;https://static.addtoany.com/buttons/favicon.png&quot; alt=&quot;Share&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Thu, 12 Jul 2018 17:20:13 +0000</pubDate>
<dc:creator>rterzi</dc:creator>
<og:type>article</og:type>
<og:title>Why Kubernetes is The New Application Server - RHD Blog</og:title>
<og:description>Kubernetes and related technologies, such as Red Hat OpenShift and Istio, provide the non-functional requirements that used to be part of an application server and the additional capabilities described in this article. Does that mean application servers are dead?</og:description>
<og:url>https://developers.redhat.com/blog/2018/06/28/why-kubernetes-is-the-new-application-server/</og:url>
<og:image>https://developers.redhat.com/blog/wp-content/uploads/2018/05/Screenshot-2018-05-18-21.20.31.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://developers.redhat.com/blog/2018/06/28/why-kubernetes-is-the-new-application-server/</dc:identifier>
</item>
</channel>
</rss>