<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>A Guide to Natural Language Processing (NLP)</title>
<link>https://tomassetti.me/guide-natural-language-processing/</link>
<guid isPermaLink="true" >https://tomassetti.me/guide-natural-language-processing/</guid>
<description>&lt;h2&gt;What can you use Natural Language Processing for?&lt;/h2&gt;
&lt;p&gt;Natural Language Processing (NLP) comprises a set of techniques that can be used to achieve many different objectives. Take a look at the following table to figure out which technique can solve your particular problem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We are going to talk about parsing in the general sense of analyzing a document and extracting its meaning.&lt;/strong&gt; So, we are going to talk about actual parsing of natural languages, but we will spend most of the time on other techniques. When it comes to understanding programming languages parsing is the way to go, however you can pick specific alternatives for natural languages. In other words, we are mostly going to talk about what you would use instead of parsing, to accomplish your goals.&lt;/p&gt;
&lt;p&gt;For instance, if you wanted to find all for statements a programming language file, you would parse it and then count the number of for. Instead, you are probably going to use something like stemming to find all mentions of cats in a natural language document.&lt;/p&gt;
&lt;p&gt;This is necessary because &lt;a href=&quot;https://en.wikipedia.org/wiki/Formal_language&quot;&gt;the theory&lt;/a&gt; behind the parsing of natural languages might be the same one that is behind the parsing of programming languages, however the practice is very dissimilar. In fact, you are not going to build a parser for a natural language. That is unless you work in artificial intelligence or as researcher. You are even rarely going to use one. Rather you are going to find an algorithm that work a simplified model of the document that can only solve your specific problem.&lt;/p&gt;
&lt;p&gt;In short, you are going to find tricks to avoid to actually having to parse a natural language. That is why this area of computer science is usually called &lt;strong&gt;natural language processing&lt;/strong&gt; rather than natural language parsing.&lt;/p&gt;
&lt;div class=&quot;ck_form_container ck_slide_up&quot;&gt;

&lt;div class=&quot;ck_form ck_vertical_subscription_form&quot;&gt;
&lt;div class=&quot;ck_form_content&quot;&gt;
&lt;h3 class=&quot;ck_form_title&quot;&gt;The Guide to Natural Language Processing&lt;/h3&gt;
&lt;div class=&quot;ck_description&quot;&gt;&lt;span class=&quot;ck_image&quot;&gt;&lt;img alt=&quot;Guide_to_natural_language_processing&quot; src=&quot;https://i1.wp.com/s3.amazonaws.com/convertkit/subscription_forms/images/005/108/263/standard/Guide_to_Natural_Language_Processing.png?w=1500&amp;amp;ssl=1&quot; data-recalc-dims=&quot;1&quot;/&gt;&lt;/span&gt;
&lt;p&gt;Get the Guide to NLP delivered to your email and read it when you want on the device you want&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;h2&gt;Algorithms That Require Data&lt;/h2&gt;
&lt;p&gt;We are going to see specific solutions to each problem. Mind you that these specific solutions can be quite complex themselves. The more advanced they are, the less they rely on simple algorithms. Usually they need a vast database of data about the language. A logical consequence of this is that it is rarely easy to adopt a tool for one language to be used for another one. Or rather, the tool might work with few adaptations, but to build the database would require a lot of investment. So, for example, you would probably find a ready to use tool to create a summary of an English text, but maybe not one for an Italian one.&lt;/p&gt;
&lt;p&gt;For this reason, in this article we concentrate mostly on English language tools. Although we mention if these tools work for other languages. You do not need to know the theoretical differences between languages, such as the number of genders or cases they have. However, you should be aware that the more different a language is from English, the harder would be to apply these techniques or tools to it.&lt;/p&gt;
&lt;p&gt;For example, you should not expect to find tools that can work with Chinese (or rather the &lt;em&gt;Chinese writing system&lt;/em&gt;). It is not necessarily that these languages are harder to understand programmatically, but there might be less research on them or the methods might be completely different from the ones adopted for English.&lt;/p&gt;
&lt;h2&gt;The Structure of This Guide&lt;/h2&gt;
&lt;p&gt;This article is organized according to the tasks we want to accomplish. Which means that the tools and explanation are grouped according to the task they are used for. For instance, there is a section about measuring the properties of a text, such as its difficulty. They are also generally in ascending order of difficulty: it is easier to classify words than entire documents. We start with simple information retrieval techniques and we end in the proper field of natural language processing.&lt;/p&gt;
&lt;p&gt;We think it is the most useful way to provide the information you need: you need to do X, we directly show the methods and tools you can use.&lt;/p&gt;
&lt;h3&gt;Table Of Contents&lt;/h3&gt;
&lt;p&gt;The following table of contents shows the whole content of this guide.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;https://tomassetti.me/guide-natural-language-processing/#classWords&quot;&gt;Classifying Words&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tomassetti.me/guide-natural-language-processing/#classDocs&quot;&gt;Classifying Documents&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tomassetti.me/guide-natural-language-processing/#understandingDocs&quot;&gt;Understanding Documents&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tomassetti.me/guide-natural-language-processing/#summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;h2 id=&quot;classWords&quot;&gt;Classifying Words&lt;/h2&gt;
&lt;p&gt;With the expression classifying words, we intend to include techniques and libraries that group words together.&lt;/p&gt;
&lt;h3 id=&quot;grouping&quot;&gt;Grouping Similar Words&lt;/h3&gt;
&lt;p&gt;We are going to talk about two methods that can group together similar words, for the purpose of information retrieval. Basically, these are methods used to find the documents, with the words we care about, from a pool of all documents. That is useful because if a user search for documents containing the word &lt;code&gt;friend&lt;/code&gt; he is probably equally interested in documents containing &lt;code&gt;friends&lt;/code&gt; and possibly &lt;code&gt;friended&lt;/code&gt; and &lt;code&gt;friendship&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So, to be clear, in this section we are not going to talk about methods to group semantically connected words, such as identifying all pets or all English towns.&lt;/p&gt;
&lt;p&gt;The two methods are stemming and division of words into group of characters. The algorithms for the first ones are language dependent, while the ones for the second ones are not. We are going to examine each of them in separate paragraphs.&lt;/p&gt;
&lt;h4 id=&quot;stemming&quot;&gt;Stemming&lt;/h4&gt;
&lt;p&gt;Stemming is the process of finding the stem, or root, of a word. In this context, the stem is not necessarily the morphological root according to linguists. So, it is not the form of a word that you would find, say, in a vocabulary. For example, an algorithm may produce the stem &lt;code&gt;consol&lt;/code&gt; for the word &lt;code&gt;consoling&lt;/code&gt;. While in a vocabulary as a root you would find &lt;code&gt;console&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A typical application of stemming is grouping together all instances of words with the same stem for usage in a search library. So, if a user search for documents containing &lt;code&gt;friend&lt;/code&gt; he can also find ones with &lt;code&gt;friends&lt;/code&gt; or &lt;code&gt;friended&lt;/code&gt;.&lt;/p&gt;
&lt;h5 id=&quot;porter&quot;&gt;Porter Stemming Algorithm&lt;/h5&gt;
&lt;p&gt;Let’s talk about an algorithm that remove suffixes to find the stem: the effective and widely used Porter Stemming Algorithm. The algorithm was originally created by Martin Porter for English. There are also Porter based/inspired algorithms for other languages: such as French or Russian. You can find all of them at the website of &lt;a href=&quot;http://snowballstem.org/algorithms/&quot;&gt;Snowball&lt;/a&gt;. Snowball is a simple language to describe stemming algorithms, but the algorithms are also described in plain English.&lt;/p&gt;
&lt;p&gt;A complete description of the algorithm is beyond the scope of this guide. However, its foundation is easy to grasp. Fundamentally the algorithm divides a word in regions and then replace or remove certain suffixes, if they are completely contained in said region. So, for example, the Porter2 (i.e., the updated version) algorithm, state that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;R&lt;/em&gt;1 is the region after the first non-vowel following a vowel, or the end of the word if there is no such non-vowel&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And then that you should replace &lt;em&gt;-tional&lt;/em&gt; with &lt;em&gt;-tion&lt;/em&gt;, if it is found inside R1.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;the word &lt;code&gt;confrontational&lt;/code&gt; has as R1 region &lt;code&gt;-frontational&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-tional&lt;/code&gt; is completely contained in its R1&lt;/li&gt;
&lt;li&gt;so &lt;code&gt;confrontational&lt;/code&gt; becomes &lt;code&gt;confrontation&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The Porter Stemmer is purely algorithmic, it does not rely on an external database or computed rules (i.e., rules created according to a training database). This is a great advantage, because it makes it predictable and easy to implement. The disadvantage is that it cannot handle exceptional cases and known mistakes cannot be easily solved. For example, the algorithm creates the same stem for &lt;code&gt;university&lt;/code&gt; and &lt;code&gt;universal&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A Porter stemmer is not perfect, but it is simple, effective and easy to implement. For a language like English, a stemmer can be realized by any competent developer. So, there are many out there for all notable programming languages and we are not going to list them here.&lt;/p&gt;
&lt;h5&gt;Typical Issues with Other Languages&lt;/h5&gt;
&lt;p&gt;Most languages that are somewhat close to English, like German or even Romance languages, are generally easy to stem. Actually, the creation of the algorithm itself is complex and requires a great knowledge of the language. However, once somebody has done the hard work of creating an algorithm, implementing one is easy.&lt;/p&gt;
&lt;p&gt;In stemming there are many problems with two kinds of languages you will usually encounter. The first kind is &lt;a href=&quot;https://en.wikipedia.org/wiki/Agglutinative_language&quot;&gt;agglutinative languages&lt;/a&gt;. Setting aside the linguistic meaning of the expression, the issue is that agglutinative languages pile up prefixes and suffixes on the root of a word.&lt;/p&gt;
&lt;p&gt;In particular Turkish is problematic because is both an agglutinative language and a concatenative one. Which mean basically that in Turkish a word can represent a whole English sentence. This makes hard to develop a stemming algorithm for Turkish, but it also makes it less useful. That is because if you stem a Turkish word you might end up with one stem for each sentence, so you lose a lot of information.&lt;/p&gt;
&lt;p&gt;The second kind of issue is related to language with no clearly defined words. Chinese is the prime example as a language that has no alphabet, but only symbols that represent concepts. So, stemming has no meaning for Chinese. Even determining the boundaries of concepts is hard. The problem of dividing a text in its component words is called word segmentation. With English you can find the boundaries of words just by looking at whitespace or punctuation. There are no such things in Chinese.&lt;/p&gt;
&lt;h4 id=&quot;dividing&quot;&gt;Splitting Words&lt;/h4&gt;
&lt;p&gt;An alternative method to group together similar words relies on splitting them. The foundation of this method is taking apart words into sequence of characters. These characters are called &lt;em&gt;k-grams&lt;/em&gt;, but they are also known as &lt;em&gt;n-grams characters&lt;/em&gt; (n-grams might also indicate groups of words). The sequence of characters is built in a sliding manner, advancing by one character at each step, starting and ending with a special symbol that indicates the boundaries of the word. For example, the 3-grams for happy are:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;$ha&lt;/li&gt;
&lt;li&gt;hap&lt;/li&gt;
&lt;li&gt;app&lt;/li&gt;
&lt;li&gt;ppy&lt;/li&gt;
&lt;li&gt;py$&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;With the symbol &lt;code&gt;$&lt;/code&gt; used to indicate the beginning and the end of the word.&lt;/p&gt;
&lt;p&gt;The exact method used for search is beyond the scope of this article. In general terms: you apply the same process to the search term(s) and then compare the occurrences of the k-grams of the input with the one of the words in the documents. Usually you apply a statistical coefficient, like the &lt;a href=&quot;https://en.wikipedia.org/wiki/Jaccard_index&quot;&gt;Jaccard coefficient&lt;/a&gt;, to determine how much similar the words have to be to be grouped together (i.e., how many grams have to have in common). For example, with an high coefficient you might group together &lt;code&gt;cat&lt;/code&gt; and &lt;code&gt;cats&lt;/code&gt; or divide &lt;code&gt;cat&lt;/code&gt; and &lt;code&gt;catty&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It is important to note a couple of things: the order of the k-grams and spelling mistakes. The order of the k-grams does not matter, in theory you could have completely different words that happens to have the same k-grams. In practice, this does not happen. This method is imprecise, which means that it can also protect from spelling mistakes of the user. For example, even if the user input &lt;code&gt;locamotive&lt;/code&gt; instead of &lt;code&gt;locomotive&lt;/code&gt;, it will probably still show the correct results. That is because 7 of 10 3-grams matches; exact matches would rank higher, but the words locamotive does not exist and so it probably has no matches.&lt;/p&gt;
&lt;h5&gt;Limits and Effectiveness&lt;/h5&gt;
&lt;p&gt;The great advantage of this technique is that it is not just purely algorithmic and very simple, but it works with all languages. You do not need to build k-grams for English differently from the ones for French. You just take apart the words in the same way. Although it is important to note that the effectiveness is in the details: you have to pick the right number of &lt;em&gt;k&lt;/em&gt; to have the best results.&lt;/p&gt;
&lt;p&gt;The ideal number depends on the average length of the word in the language: it should be lower or equal than that. Different languages might have different values, but in general you can get away with 4 or 5. You will not have the absolute best results with only one choice, but it will work.&lt;/p&gt;
&lt;p&gt;The disadvantage is that it looks extremely stupid, let’s face it: it so simple that it should not work. But it actually does, &lt;a href=&quot;https://staff.fnwi.uva.nl/m.derijke/wp-content/papercite-data/pdf/hollink-monolingual-2004.pdf&quot;&gt;it works well if not better than stemming (PDF)&lt;/a&gt;. It is shamelessly effective, and it has many other uses. We are going to see one right now.&lt;/p&gt;
&lt;h5&gt;Generating Names&lt;/h5&gt;
&lt;p&gt;The general case of generating fake words that looks like real words is hard and of limited use. You could create phrases for a fake language, but that is pretty much it. However, it is possible to programmatically create realistic fake names for use in games or for any world building need.&lt;/p&gt;
&lt;p&gt;There are several feasible methods. The easiest works roughly like that:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;create a database of names of the same kind you want to generate (e.g., Roman names, Space Lizards Names, etc.)&lt;/li&gt;
&lt;li&gt;divide the input names in k-grams (e.g., 3-grams of Mark -&amp;gt; $ma – mar – ark – rk$)&lt;/li&gt;
&lt;li&gt;associate a probability to the k-grams: the more frequently they appear in the original database, the higher the chance they appear in the generated name)&lt;/li&gt;
&lt;li&gt;generate the new names&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;There are many variants, for instance you could combine a different number of k-grams for special purposes (e.g., all names start with a 2-gram, but end in a 4-gram).&lt;/p&gt;
&lt;p&gt;You could also improve the soundness of the generated names, simply by looking at the probabilities of the sequences appearing in a certain order. For example, if you randomly start with &lt;code&gt;ar&lt;/code&gt; the following syllable might be more likely &lt;code&gt;th&lt;/code&gt; than &lt;code&gt;or&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This method is not perfect, but it generally works good enough. You can see a few simple examples in &lt;a href=&quot;https://github.com/ftomassetti/langgen&quot;&gt;langgen&lt;/a&gt; or &lt;a href=&quot;https://github.com/Valkryst/VNameGenerator&quot;&gt;VNameGenerator&lt;/a&gt;, which shows variations of said method and a few others.&lt;/p&gt;
&lt;h2 id=&quot;classDocs&quot;&gt;Classifying Documents&lt;/h2&gt;
&lt;p&gt;In this section we include techniques and libraries that measure and identifies documents. For example, they can detect the language in which a document is written or measure how difficult it is to read it.&lt;/p&gt;
&lt;h3 id=&quot;measuring&quot;&gt;Text Metrics&lt;/h3&gt;
&lt;p&gt;There are two popular metrics of a text that can be easily implemented: reading time and difficulty of the text. These measurements are useful to inform the reader or to help the writer checking that the document respects certain criteria, such as being accessible to a young audience (i.e., low difficulty).&lt;/p&gt;
&lt;h4 id=&quot;readingTime&quot;&gt;Reading Time&lt;/h4&gt;
&lt;p&gt;The simplest way of measuring the reading time of a text is to calculate the words in the document, and then divide them by a predefined &lt;em&gt;words per minute&lt;/em&gt; (wpm) number. The words per minute figure represent the words read by an average reader in a minute. So, if a text has 1000 words and the &lt;em&gt;wpm&lt;/em&gt; is set to 200, the text would take 5 minutes to read.&lt;/p&gt;
&lt;p&gt;That is easy to understand and easy to implement. The catch is that you have to pick the correct wpm rate and that the rate varies according to each language. For example, English readers might read 230 wpm, but French readers might instead read 200 wpm. This is related to the length of the words and the natural flow of the language (i.e., a language could be more concise than another, for instance it might frequently omit subjects).&lt;/p&gt;
&lt;p&gt;The first issue is easily solved: for English most estimates put the correct wpm between 200 and 230. However, there is still the problem of dealing with different languages. This requires to have the correct data for each language and to be able to understand the language in which a text is written.&lt;/p&gt;
&lt;p&gt;To mitigate both problems you might opt to use &lt;a href=&quot;http://iovs.arvojournals.org/article.aspx?articleid=2166061&quot;&gt;a measurement of characters count&lt;/a&gt; in order to estimate the reading time. Basically, you remove the punctuation and spaces, then count the characters and divide the sum by 900-1000.&lt;/p&gt;
&lt;p&gt;On linguistic grounds the measure makes less sense, since people do not read single characters. However, the difference between languages are less evident by counting characters. For example, an &lt;a href=&quot;https://en.wikipedia.org/wiki/Agglutinative_language&quot;&gt;agglutinative language&lt;/a&gt; might have very long words, and thus fewer of them. So it ends up with a similar number of characters to a fusional language like English.&lt;/p&gt;
&lt;p&gt;This works better because the differences in speed of reading characters in each language is smaller as a percentage of the total speed. Imagine for example that the typical reader of English can read 200 wpm and 950 cpm, while the typical reader of French can read 250 wpm and 1000 cpm. The absolute difference is the same, but it is less relevant for reading characters. Of course, this is still less than ideal, but it is a simple solution.&lt;/p&gt;
&lt;p&gt;Neither of the measure consider the difficulty of the text. That texts that are difficult to read take more time to read, even with the same number of words or characters.&lt;/p&gt;
&lt;h4 id=&quot;readability&quot;&gt;Calculating the Readability of a Text&lt;/h4&gt;
&lt;p&gt;Usually the calculation of the readability of a text is linked to grades of education (i.e., years of schooling). So, an easy text might be one that can be read by 4th graders, while a harder one might need a 10th grade education. That is both a byproduct of the fact that the algorithms were created for educational purposes and because education is a useful anchor for ranking difficulty. Saying that a text is difficult in absolute terms is somewhat meaningless, saying that it is difficult for 7th-graders makes more sense.&lt;/p&gt;
&lt;p&gt;There are several formulas, but they are generally all based on the number of words and sentences in addition to either syllables or the number of difficult words. Let’s see two of the most famous: Flesch-Kincaid and Dale–Chall.&lt;/p&gt;
&lt;p&gt;None of these formulas is perfect, but both have been scientifically tested. The only caveat is that they should be used only for checking and not as a guideline. They work if you write normally. If you try to edit a text to lower the score, the results might be incorrect. For example, &lt;em&gt;if you use short words just to make a text seem easy, it looks bad&lt;/em&gt;.&lt;/p&gt;
&lt;h5&gt;Flesch-Kincaid Readability Formula&lt;/h5&gt;
&lt;p&gt;There are two variants of this formula: &lt;em&gt;Flesch reading ease&lt;/em&gt; and &lt;em&gt;Flesch–Kincaid grade level&lt;/em&gt;. They are equivalent, but one output a score (the higher it is, the easier is the text) and the other a corresponding US grade level. We are going to show the first one.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-full wp-image-3786&quot; src=&quot;https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/flesch.png?resize=800%2C60&amp;amp;ssl=1&quot; alt=&quot;Flesch Reading Ease&quot; srcset=&quot;https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/flesch.png?w=800&amp;amp;ssl=1 800w, https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/flesch.png?resize=300%2C23&amp;amp;ssl=1 300w, https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/flesch.png?resize=768%2C58&amp;amp;ssl=1 768w, https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/flesch.png?resize=705%2C53&amp;amp;ssl=1 705w, https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/flesch.png?resize=450%2C34&amp;amp;ssl=1 450w&quot; sizes=&quot;(max-width: 800px) 100vw, 800px&quot; data-recalc-dims=&quot;1&quot;/&gt;The readability is generally between 100 to 20. A result of 100 indicates a document that can be easily understood by a 11-years old student, while a result of 30 or less indicates a document suited for university graduates. You can find a more complete explanation and ranking table in &lt;a href=&quot;http://www.mang.canterbury.ac.nz/writing_guide/writing/flesch.shtml&quot;&gt;How to Write Plain English&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The different parameters can be obtained easily. Only calculating the total number of syllables requires a significant amount of work. The good news is that it is actually doable and there is a reliable algorithm for it. The bad news is that the author of TeX (Frank Liang) &lt;a href=&quot;https://www.tug.org/docs/liang/&quot;&gt;wrote his PhD thesis about his hyphenation algorithm&lt;/a&gt;. You can find an implementation and an accessible explanation of the algorithm in &lt;a href=&quot;https://github.com/mnater/Hyphenopoly&quot;&gt;Hyphenopoly.js&lt;/a&gt;. The two problems are equivalent, since you can only divide a word in two parts between two syllables.&lt;/p&gt;
&lt;p&gt;An alternative is to use a hack: instead of calculating syllables, count the vowels. This hack has been reporting to work for English, but it is not applicable to other languages. Although if you use it you lose the scientific validity of the formula and you just get a somewhat accurate number.&lt;/p&gt;
&lt;p&gt;The general structure of the formula has been applied to other languages (e.g., &lt;a href=&quot;https://it.wikipedia.org/wiki/Roberto_Vacca#Indice_di_leggibilit.C3.A0_Flesch-Vacca&quot;&gt;Flesch-Vacca for Italian&lt;/a&gt;), but each language have different coefficients.&lt;/p&gt;
&lt;h5&gt;Dale–Chall Readability Formula&lt;/h5&gt;
&lt;p&gt;This formula relies also on the number of words and sentences, but instead of syllables it uses the number of difficult words present in the text.&lt;/p&gt;
&lt;p&gt;A difficult word is defined as one that do not belong to a list of 3000 simple words, that 80% of fourth graders understand.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;size-full wp-image-3785 aligncenter&quot; src=&quot;https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/dale-chall.png?resize=800%2C60&amp;amp;ssl=1&quot; alt=&quot;Dale-Chall Readability Formula&quot; srcset=&quot;https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/dale-chall.png?w=800&amp;amp;ssl=1 800w, https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/dale-chall.png?resize=300%2C23&amp;amp;ssl=1 300w, https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/dale-chall.png?resize=768%2C58&amp;amp;ssl=1 768w, https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/dale-chall.png?resize=705%2C53&amp;amp;ssl=1 705w, https://i0.wp.com/tomassetti.me/wp-content/uploads/2017/11/dale-chall.png?resize=450%2C34&amp;amp;ssl=1 450w&quot; sizes=&quot;(max-width: 800px) 100vw, 800px&quot; data-recalc-dims=&quot;1&quot;/&gt;Thus, the formula is easy to use and calculate. The only inconvenience is that you have to maintain a database of these 3000 words. We are not aware of the formula having been adapted to languages other than English.&lt;/p&gt;
&lt;p&gt;The formula generally output a score between 4 and 10. Less than 5 indicates a text suitable for 4th graders, a result of 9 or more indicates a text for college students. You can find a complete table of results at &lt;a href=&quot;http://www.readabilityformulas.com/new-dale-chall-readability-formula.php&quot;&gt;The New Dale-Chall Readability Formula&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It is natural to think that you could modify the formula, to calculate the difficulty in understanding a specialized text. That is to say you could define difficult words as words belonging to technical terminology. For example, you could calculate how difficult would be to understand a text for an average person, according to how much computer jargon it contains. Words like &lt;em&gt;parser&lt;/em&gt; or &lt;em&gt;compiler&lt;/em&gt; could be difficult, while &lt;em&gt;computer&lt;/em&gt; or &lt;em&gt;mouse&lt;/em&gt; could be considered easy. This might work: however you would have to calculate the correct coefficients yourself.&lt;/p&gt;
&lt;h3&gt;Identifying a Language&lt;/h3&gt;
&lt;p&gt;When you need to work with a natural language document the need to identifying a language comes up often. For starters, almost all the algorithms we have seen works only on a specific language and not all of them. Even overcoming this problem it is useful to be able to understand in which language is written a certain document. For instance, so that you can show or hide a document to certain users based on the language it understand. Imagine that a user search for documents containing the word &lt;code&gt;computer&lt;/code&gt;: if you simply return all documents that contain that word you will get results even in languages other than English. That is because the word has been adopted by other languages (e.g., Italian).&lt;/p&gt;
&lt;p&gt;Reliable language identification can be achieved with statistical methods. We are going to talk about two methods: a vocabulary based one and one based on frequencies of groups of letters.&lt;/p&gt;
&lt;p&gt;In theory you could also hack together ad-hoc methods based on language clues, such as the one listed in &lt;a href=&quot;https://en.wikipedia.org/wiki/Wikipedia:Language_recognition_chart&quot;&gt;Wikipedia:Language recognition chart&lt;/a&gt;. And then come up with a ranking method to order the probability of each language. The advantage of this approach is that you would not need any database. However, it has not been tested scientifically.&lt;/p&gt;
&lt;p&gt;For example, two features of Italian are: most words end with a vowel and the word &lt;code&gt;è&lt;/code&gt; (i.e., is) is quite common. So you could check for the presence and frequencies of these features in a certain document and use them to calculate the probability that said document were in Italian (e.g., at least 70% of the words ends in vowel -&amp;gt; +50% chance that the document is in Italian; the word &lt;code&gt;è&lt;/code&gt; is present in at least 10% phrases -&amp;gt; +50% chance that the document is in Italian).&lt;/p&gt;
&lt;h5&gt;Words in a Vocabulary&lt;/h5&gt;
&lt;p&gt;A basic method consists in comparing words in the text with the ones included in a vocabulary. First, you get a vocabulary of each language you care about. Then, you compare the text with the words in each vocabulary and count the number of occurrences. The vocabulary which includes the highest number of words denotes the language of the text.&lt;/p&gt;
&lt;p&gt;For example, the following phrase: &lt;code&gt;the cat was in the boudoir&lt;/code&gt;, would be identified as English because there are 5 English words (&lt;code&gt;the&lt;/code&gt;, &lt;code&gt;cat&lt;/code&gt;, &lt;code&gt;was&lt;/code&gt;, &lt;code&gt;in&lt;/code&gt;, &lt;code&gt;the&lt;/code&gt;) and 1 French word (&lt;code&gt;boudoir&lt;/code&gt;). In case you are wondering, a boudoir is essentially a female version of a man cave, but for sophisticated people.&lt;/p&gt;
&lt;p&gt;Despite its simplicity this method works well for large documents, but not so much for Twitter-like texts: texts that are short and frequently contains errors. The author of the &lt;a href=&quot;https://github.com/peterc/whatlanguage&quot;&gt;Whatlanguage&lt;/a&gt; library, which implements this method, suggest a minimum of 10 words for reliable identification.&lt;/p&gt;
&lt;h5&gt;Frequencies of Groups of Letters&lt;/h5&gt;
&lt;p&gt;The most currently used technique relies on building language models of the frequencies of groups of letters. We are again talking about k-grams, that have to be analyzed for each language. We are going to describe the procedure outlined by the original paper &lt;a href=&quot;http://odur.let.rug.nl/~vannoord/TextCat/textcat.pdf&quot;&gt;N-Gram-Based Text Categorization (PDF)&lt;/a&gt;, although an implementation might adopt a slight variation of it. Two well-known libraries implementing this method are: &lt;a href=&quot;https://github.com/wikimedia/wikimedia-textcat&quot;&gt;the PHP TextCat library by Wikimedia&lt;/a&gt; and &lt;a href=&quot;https://github.com/ivanakcheurov/ntextcat&quot;&gt;NTextCat&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First we are going to build a model for a language and then we are going to use it for scoring a document.&lt;/p&gt;
&lt;h6&gt;Building a Language Model&lt;/h6&gt;
&lt;p&gt;To create such language models, you need to have a large set of documents for each language. You divide the words in these documents to find the most used k-grams. The paper suggests calculating the number of k-grams for k from 1 to 5.&lt;/p&gt;
&lt;p&gt;Once you have the frequencies of k-grams, you order them in descending order to build a language model. For instance, a language model for English might have &lt;code&gt;th&lt;/code&gt; in first place, &lt;code&gt;ing&lt;/code&gt; in second and so on. In this final stage the actual number of occurrences it is not relevant, only their final ranking in frequency. That is to say the fact that &lt;code&gt;th&lt;/code&gt; might appear 2648 or 5895 times depends only on the size of your set of documents and it is irrelevant for the success of the method. What it is relevant is just the relative frequency in your set of documents and thus the respective ranking.&lt;/p&gt;
&lt;p&gt;Once you have build this language model you can use it to identify the language in which a document is written. You just have to apply the same procedure to build a document model of the text that you are trying to identify. So, you end up with a ranking of the frequencies of k-gram in the document.&lt;/p&gt;
&lt;p&gt;Finally, you calculate the differences between the rankings in the document and the ones for each language model you have. You could calculate this distance metric with many statistical formulas, but the paper uses a simple out-of-place method.&lt;/p&gt;
&lt;h6&gt;Scoring Languages&lt;/h6&gt;
&lt;p&gt;The method consists of comparing the position in each language model. For each position, you add a number equivalent to the differences of ranks between each language model and the document model. For example, if the language model for English put &lt;code&gt;th&lt;/code&gt; in first place, but the document model for the document that we are classifying put it in 6th place you add 5 to the English score. At the end of the process, the language with the lowest score should be the language of the text.&lt;/p&gt;
&lt;p&gt;Obviously, you do not compare all positions of k-grams up to the last one, because the lower you go the more the position becomes somewhat arbitrary. It will depend on the particular set of documents you will have chosen to build a language model. The paper uses a limit of 300 positions, but it also says that it depends on the short length of the documents the scientists have chosen. So, you would have to find the best limit for yourself.&lt;/p&gt;
&lt;p&gt;There is also the chance of wrongly classifying a text. This is more probable if a text is written in a language for which there is no model. In that case the algorithm might wrongly classify the text as one belonging to a language close to the real one. That could happen because the method find a language that has a good enough score for the document. The real language would have scored better , but it is not available, so the good enough score wins.&lt;/p&gt;
&lt;p&gt;For instance, if you have a model for Italian, but do not have one for Spanish, your software might classify a Spanish text as an Italian one because Italian is similar enough to Spanish and the closest you have to it.&lt;/p&gt;
&lt;p&gt;This problem can be mitigated by using a proper threshold. That is to say a match is found only if the input has a low score (compared to its length) for a language. Of course, now you have the problem of finding such proper threshold.&lt;/p&gt;
&lt;h6&gt;Limitations&lt;/h6&gt;
&lt;p&gt;This method works better with short texts, but it is not perfect. The suggested limit is based on the implementation and the quality of text and model. For example, NTextCat recommends a limit of 5 words. Generally speaking this method would not work reliably for Twitter messages or similar short and frequently incorrect texts. However, that would be true even for a human expert.&lt;/p&gt;
&lt;h2 id=&quot;understandingDocs&quot;&gt;Understanding Documents&lt;/h2&gt;
&lt;p&gt;This section contains more advanced libraries to understand documents. We use the term somewhat loosely: we talk about how a computer can extract or manage the content of a document beyond simple manipulation of words and characters.&lt;/p&gt;
&lt;p&gt;We are going to see how you can:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;generate summary of a document (i.e., an algorithmic answer to the question what is this article about?)&lt;/li&gt;
&lt;li&gt;sentiment analysis (i.e., does this document contain a positive or negative opinion?)&lt;/li&gt;
&lt;li&gt;parsing a document written in a natural language&lt;/li&gt;
&lt;li&gt;translate a document in another language&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;For the methods listed in the previous sections you could build a library yourself with a reasonable effort. From now on, it will get harder. That is because they might require vast amount of annotated data (e.g., a vocabulary having each word with the corresponding part of speech) or rely on complex machine learning methods. So, we will mostly suggest using libraries.&lt;/p&gt;
&lt;p&gt;This is an area with many open problems and active research, so you could find most libraries in Python, a language adopted by the research community. Though you could find the occasional research-ready library in another language.&lt;/p&gt;
&lt;p&gt;A final introductory note is that statistics and machine learning are the current kings of natural language processing. So there is probably somebody trying to use &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt; to accomplish each of these tasks (e.g., &lt;a href=&quot;https://github.com/hengluchang/deep-news-summarization&quot;&gt;deep news summarization&lt;/a&gt;). You might try that too, if you take in account a considerable amount of time for research.&lt;/p&gt;
&lt;h3 id=&quot;summaries&quot;&gt;Generation of Summaries&lt;/h3&gt;
&lt;p&gt;The creation of a summary, or a headline, to correctly represent the meaning of a document it is achievable with several methods. Some of them rely on information retrieval techniques, while others are more advanced. The theory is also divided in two strategies: extracting sentences or parts thereof from the original text, generating abstract summaries.&lt;/p&gt;
&lt;p&gt;The second strategy it is still an open area of research, so we will concentrate on the first one.&lt;/p&gt;
&lt;h4 id=&quot;sumBasic&quot;&gt;SumBasic&lt;/h4&gt;
&lt;p&gt;SumBasic is a method that relies on the probability of individual words being present in a sentence to determine the most representative sentence:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;First, you have to account the number of times a word appears in the whole document. With that you calculate the probability of each word appearing in the document. I.e., if the word appears 5 times and the document has 525 words its probability is 5/525.&lt;/li&gt;
&lt;li&gt;You calculate a weight for each sentence that is the average of the probabilities of all the words in the sentence. I.e., if a sentence contains three words with probability 3/525, 5/525 and 10/525, the weight would be 6/525.&lt;/li&gt;
&lt;li&gt;Finally you score the sentences by multiplying the highest probability word of each sentence with its weight. I.e., a sentence with a weight of 0.1 and whose best word had the probability of 0.5 would score 0.1 * 0. 5 = 0.05, while another one with weight 0.2 and a word with probability 0.4 would score 0.2 * 0.4 = 0.08.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Having found the best sentence, you recalculate the probabilities for each word in the chosen sentence. You recalculate the probabilities as if the chosen sentence was removed from the document. The idea is that the included sentence already contains a part of the whole meaning of the document. So that part become less important and this helps avoiding excessive repetition. You repeat the process until you reach the needed summary length.&lt;/p&gt;
&lt;p&gt;This technique is quite simple. It does not require to have a database of documents to build a general probability of a word appearing in any document. You just need to calculate the probabilities in each input document. However, for this to work you have to exclude what are called &lt;em&gt;stopwords&lt;/em&gt;. These are common words present in most documents, such as &lt;code&gt;the&lt;/code&gt; or &lt;code&gt;is&lt;/code&gt;. Otherwise you might include meaningless sentences that include lots of them. You could also include stemming to improve the results.&lt;/p&gt;
&lt;p&gt;It was first described in &lt;a href=&quot;http://www.cs.middlebury.edu/~mpettit/tr-2005-101.pdf&quot;&gt;The Impact of Frequency on Summarization (PDF)&lt;/a&gt;; there is an implementation available as &lt;a href=&quot;https://github.com/EthanMacdonald/SumBasic&quot;&gt;a Python library&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The approach based on frequencies is an old and popular one, because it is generally effective and simple to implement. SumBasic is good enough that is frequently used as a baseline in the literature. However, there are even simpler methods. For example, &lt;a href=&quot;https://github.com/neopunisher/Open-Text-Summarizer&quot;&gt;Open Text Summarizer&lt;/a&gt; is a 2003 library that uses an even simpler approach. Basically you count the frequency of each word, then you exclude the common English words (e.g., &lt;code&gt;the&lt;/code&gt;, &lt;code&gt;is&lt;/code&gt;) and finally you calculate the score of a sentence according to the frequencies of the word it contains.&lt;/p&gt;
&lt;h4 id=&quot;graphMethods&quot;&gt;Graph-based Methods: TextRank&lt;/h4&gt;
&lt;p&gt;There are more complex methods of calculating the relevance of the individual sentences. A couple of them take inspiration from PageRank: they are called LexRank and TextRank. They both rely on the relationship between different sentences to obtain a more sophisticate measurement of the importance of sentences, but they differ in the way they calculate similarity of sentences.&lt;/p&gt;
&lt;p&gt;PageRank measures the importance of a document according to the importance of other documents that links to it. The importance of each document, and thus each link, is computed recursively until a balance is reached.&lt;/p&gt;
&lt;p&gt;TextRank works on the same principle: the relationship between elements can be used to understand the importance of each individual element. TextRank actually uses a more complex formula than the original PageRank algorithm, because a link can be only present or not, while textual connections might be partially present. For instance, you might calculate that two sentences containing different words with the same stem (e.g., &lt;code&gt;cat&lt;/code&gt; and &lt;code&gt;cats&lt;/code&gt; both have &lt;code&gt;cat&lt;/code&gt; as their stem) are only partially related.&lt;/p&gt;
&lt;p&gt;The original paper describes a generic approach, rather than a specific method. In fact, it also describes two applications: keyword extraction and summarization. The key differences are:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;the units you choose as a foundation of the relationship&lt;/li&gt;
&lt;li&gt;the way you calculate the connection and its strength&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;For instance, you might choose as units n-grams of words or whole phrases. N-grams of words are sequences of n words, computed the same way you do k-gram for characters. So, for the phrase &lt;code&gt;dogs are better than cats&lt;/code&gt;, there are these 3-grams:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;dogs are better&lt;/li&gt;
&lt;li&gt;are better than&lt;/li&gt;
&lt;li&gt;better than cats&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Phrases might create weighted links according to how similar they are. Or they might simply create links according to the position they are (i.e., a phrase might link to the previous and following one). The method works the same.&lt;/p&gt;
&lt;h5&gt;TextRank for Sentence Extraction&lt;/h5&gt;
&lt;p&gt;TextRank for extracting phrases uses as a unit whole sentences, and as a similarity measure the number of words in common between them. So, if two phrases contain the words &lt;code&gt;tornado&lt;/code&gt;, &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;center&lt;/code&gt; they are more similar than if they contain only two common words. The similarity is normalized based on the length of the phrases. To avoid the issue of having longer phrases having higher similarity than shorter ones.&lt;/p&gt;
&lt;p&gt;The words used for the similarity measure could be stemmed. Stopwords are usually excluded by the calculation. A further improvement could be to also exclude verbs, although that might be complicated if you do not already have a way to identify the parts of speech.&lt;/p&gt;
&lt;p&gt;LexRank differs mainly because as a similarity measure it uses a standard TF-IDF (Term Frequency – Inverse Document Frequency). Basically with TF-IDF the value of individual words is first weighted according to how frequently they appear in all documents and in each specific document. For example, if you are summarizing articles for a car magazine, there will be a lot of occurrences of the word &lt;code&gt;car&lt;/code&gt; in every document. So, the word &lt;code&gt;car&lt;/code&gt; would be of little relevance for each document. However, the word &lt;code&gt;explosion&lt;/code&gt; would appear in few documents (hopefully), so it will matter more in each document it appears.&lt;/p&gt;
&lt;p&gt;The paper &lt;a href=&quot;http://web.eecs.umich.edu/%7Emihalcea/papers/mihalcea.emnlp04.pdf&quot;&gt;TextRank: Bringing Order into Texts (PDF)&lt;/a&gt; describe the approach. &lt;a href=&quot;https://github.com/jjangsangy/ExplainToMe&quot;&gt;ExplainToMe&lt;/a&gt; contains a Python implementation of TextRank.&lt;/p&gt;
&lt;h4 id=&quot;lsa&quot;&gt;Latent Semantic Analysis&lt;/h4&gt;
&lt;p&gt;The methods we have seen so far have a weakness: they do not take into account semantics. This weakness is evident when you consider that there are words that have similar meanings (i.e., synonyms) and that most words can have different meaning depending on the context (i.e., polysemy).  &lt;em&gt;Latent Semantic Analysis&lt;/em&gt; attempt to overcome these issues.&lt;/p&gt;
&lt;p&gt;The expression &lt;em&gt;Latent Semantic Analysis&lt;/em&gt; describes a technique more than a specific method. A technique that could be useful whenever you need to represent the meaning of words. It can be used for summarization, but also for search purposes, to find words like the query of the user. For instance, if the user search for &lt;code&gt;happiness&lt;/code&gt; a search library using LSA could also return results for &lt;code&gt;joy&lt;/code&gt;.&lt;/p&gt;
&lt;h5&gt;A Simple Description&lt;/h5&gt;
&lt;p&gt;The specific mathematical formulas are a bit complex and involve matrices and operations on them. However, the founding idea is quite simple: words with similar meaning will appear in similar parts of a text. So you start with a normal TF-IDF matrix. Such matrix contains nothing else than the frequencies of individual words, both inside a specific document and in all the documents evaluated.&lt;/p&gt;
&lt;p&gt;The problem is that we want to find a relation between words that do not necessarily appear together. For example, imagine that different documents contain phrases containing the words &lt;code&gt;joy&lt;/code&gt; and &lt;code&gt;happiness&lt;/code&gt; together other words &lt;code&gt;cookie&lt;/code&gt; or &lt;code&gt;chocolate&lt;/code&gt;. The words do not appear in the same sentence, but they appear in the same document. One document contains a certain number of such phrases: &lt;code&gt;a dog create happiness&lt;/code&gt; and &lt;code&gt;dogs bring joy to children&lt;/code&gt;. For this document, LSA should be able to find a connection between &lt;code&gt;joy&lt;/code&gt; and &lt;code&gt;happiness&lt;/code&gt; through their mutual connection with &lt;code&gt;dog&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The connection is build based on the frequency the words appear together or with related words in the whole set of documents. This allows to build connection even in a sentence or document where they do not appear together. So if &lt;code&gt;joy&lt;/code&gt; and &lt;code&gt;happiness&lt;/code&gt; appears frequently with &lt;code&gt;dog&lt;/code&gt;, LSA would associate the specific document with the words (&lt;code&gt;joy&lt;/code&gt;, &lt;code&gt;happiness&lt;/code&gt;) and &lt;code&gt;dog&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Basically, this technique will transform the original matrix from one linking each term with its frequency, into one with a (weighted) combination of terms linked to each document.&lt;/p&gt;
&lt;p&gt;The issue is that there are a lot of words, and combinations thereof, so you need to make a lot of calculation and simplifications. And that is where the complex math is needed.&lt;/p&gt;
&lt;p&gt;Once you have this matrix, the world is your oyster. That is to say you could use this measurement of meaning in any number of ways. For instance, you could find the most relevant phrase and then find the phrases with are most close to it, using a graph-based method.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.researchgate.net/publication/226424326_Text_Summarization_and_Singular_Value_Decomposition&quot;&gt;Text Summarization and Singular Value Decomposition&lt;/a&gt; describes one way to find the best sentences. The Python library &lt;a href=&quot;https://github.com/miso-belica/sumy&quot;&gt;sumy&lt;/a&gt; offers an implementation.&lt;/p&gt;
&lt;h4 id=&quot;otherMethods&quot;&gt;Other Methods and Libraries&lt;/h4&gt;
&lt;p&gt;The creation of summaries is a fertile area of research with many valid methods already devised. In fact, much more than the ones we have described here. They vary for the approaches and the objective they are designed for. For example, some are created specifically to provide an answer to a question of the user, others to summarize multiple documents, etc.&lt;/p&gt;
&lt;p&gt;You can read a brief taxonomy of other methods in &lt;a href=&quot;http://textmining.zcu.cz/publications/Z08.pdf&quot;&gt;Automatic Text Summarization (PDF)&lt;/a&gt;. The Python library &lt;a href=&quot;https://github.com/miso-belica/sumy&quot;&gt;sumy&lt;/a&gt;, that we have already mentioned, implements several methods, though not necessarily the ones mentioned in the paper.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://sourceforge.net/projects/classifier4j/&quot;&gt;Classifier4J&lt;/a&gt; (Java), &lt;a href=&quot;https://sourceforge.net/projects/nclassifier/&quot;&gt;NClassifier&lt;/a&gt; (C#) and &lt;a href=&quot;https://github.com/thavelick/summarize/&quot;&gt;Summarize&lt;/a&gt; (Python) implements a Bayes classifier in an algorithm described as such:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In order to summarize a document this algorithm first determines the frequencies of the words in the document. It then splits the document into a series of sentences. Then it creates a summary by including the first sentence that includes each of the most frequent words. Finally summary’s sentences are reordered to reflect that of those in the original document.&lt;/p&gt;
&lt;p&gt;– &lt;a href=&quot;https://github.com/thavelick/summarize/blob/master/summarize.py&quot;&gt;Summarize.py&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;These projects that implements a Bayes classifier are all dead, but they are useful to understand how the method could be implemented.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/DataTeaser/textteaser&quot;&gt;DataTeaser&lt;/a&gt; and &lt;a href=&quot;https://github.com/xiaoxu193/PyTeaser&quot;&gt;PyTeaser&lt;/a&gt; (both in Python, but originally DataTeaser was in Scala) use a custom approach that combines several simple measurements to create a summary of an article.&lt;/p&gt;
&lt;h4 id=&quot;otherApps&quot;&gt;Other Uses&lt;/h4&gt;
&lt;p&gt;You can apply the same techniques employed to create summaries to different tasks. That is particularly true for the more advanced and semantic based ones. Notice that the creation of only one summary for many documents is also a different task. That is because you have to take in account different documents lengths and avoid the repetitions, among other things.&lt;/p&gt;
&lt;p&gt;A natural application is the identification of similar documents. If you can devise a method to identify the most meaningful sentences of one document, you can also compare the meaning of two documents.&lt;/p&gt;
&lt;p&gt;Another objective with common techniques is information retrieval. In short, if a user search for one word, say &lt;code&gt;car&lt;/code&gt;, you could use some of these techniques to find documents containing also &lt;code&gt;automobile&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, there is &lt;em&gt;topic modelling&lt;/em&gt;, which consists in finding the topics of a collection of documents. In simple terms, it means grouping together words with similar themes. It uses more complex statistical methods that the one used for the creation of summaries. The current state of art is based upon a method called Latent Dirichlet allocation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://radimrehurek.com/gensim/index.html&quot;&gt;Gensim&lt;/a&gt; is a very popular and production-ready library, that have many such applications. Naturally is written in Python.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://mallet.cs.umass.edu/&quot;&gt;Mallet&lt;/a&gt; is a Java library mainly designed for topic modelling.&lt;/p&gt;
&lt;h3 id=&quot;parsing&quot;&gt;Parsing Documents&lt;/h3&gt;
&lt;p&gt;Most computer languages are easy to parse. This is not true for natural languages. There are approaches that give good results, but ultimately this is still an open area of research. Fundamentally the issue is that the parsing a sentence (i.e., analyzing its syntax) and its meaning are interconnected in a natural language. A subject or a verb, a noun or an adverb are all words and most words that can be subject can also be object.&lt;/p&gt;
&lt;p&gt;In practical terms, this means that there are no ready to use libraries that are good for every use you can think of. We present some libraries that can be used for restricted tasks, such as recognizing parts of speech, that can also be of use for improving other methods, like the ones for creation of summaries.&lt;/p&gt;
&lt;p&gt;There is also the frustrating fact is that a lot of software is made by academic researchers. Which means that it could be easily abandoned for another approach or lacking documentation. You cannot really use a work-in-progress, badly maintained software for anything productive. Especially if you care about a language other than English, you might find yourself seeing a good working demo, which was written ten years ago, by somebody with no contact information and without any open source code available.&lt;/p&gt;
&lt;h4 id=&quot;needData&quot;&gt;You Need Data&lt;/h4&gt;
&lt;p&gt;To achieve any kind of result with parsing, or generally extracting information from a natural language document, you need a lot of data to train the algorithms. This group of data is called a &lt;em&gt;corpus&lt;/em&gt;. For use in a system that uses statistical or machine learning techniques, you might just need a lot of real world data possibly divided in the proper groups (e.g., Wikipedia articles divided by category).&lt;/p&gt;
&lt;p&gt;However, if you are using a smart system, you might need this corpus of data to be manually constructed or annotated (e.g., the word &lt;code&gt;dog&lt;/code&gt; is a noun that has these X possible meanings). A smart system is one that tries to imitate human understanding, or at a least that uses a process that can be followed by humans. For instance, a parser that relies on a grammar which uses rules such as &lt;code&gt;Phrase → Subject Verb&lt;/code&gt; (read: a phrase is made of a subject and a verb), but also defines several classes of verbs that humans would not normally use (e.g., verbs related to motion).&lt;/p&gt;
&lt;p&gt;In these cases, the corpus often uses a custom format and is built for specific needs. For example, this &lt;a href=&quot;http://www.cs.utexas.edu/users/ml/geo.html&quot;&gt;system that can answer geographical questions about United States&lt;/a&gt; uses information stored in a Prolog format. The natural consequence is that even what is generally available information, such as dictionary data, can be incompatible between different programs.&lt;/p&gt;
&lt;p&gt;On the other hand, there are also good databases that are so valuable that many programs are built around them. &lt;a href=&quot;https://en.wikipedia.org/wiki/WordNet&quot;&gt;WordNet&lt;/a&gt; is an example of such database. It is a lexical database that links groups of words with similar meaning (i.e., synonyms) with their associated definition. It works thus as both a dictionary and a thesaurus. The original version is for English, but it has inspired similar databases for other languages.&lt;/p&gt;
&lt;h4 id=&quot;things&quot;&gt;What You Can Do&lt;/h4&gt;
&lt;p&gt;We have presented some of the practical challenges to build your own library to understand text. And we have not even mentioned all the issues related to ambiguity of human languages. So differently from what we did for past sections we are just going to explain what you can do. We are not going to explain the algorithms used to realized them, both because there is no space and also without the necessary data they would be worthless. Instead in the next paragraph we are just going to introduce the most used libraries that you can use to achieve what you need.&lt;/p&gt;
&lt;h5&gt;Named-entity Recognition&lt;/h5&gt;
&lt;p&gt;Named-entity recognition basically means finding the entities mentioned in the document. For example, in the phrase &lt;code&gt;John Smith is going to Italy&lt;/code&gt;, it should identify &lt;code&gt;John Smith&lt;/code&gt; and &lt;code&gt;Italy&lt;/code&gt; as entities. It should also be able to correctly keep track of them in different documents.&lt;/p&gt;
&lt;h5&gt;Sentiment Analysis&lt;/h5&gt;
&lt;p&gt;Sentiment analysis classifies the sentiment represented by a phrase. In the most basic terms, it means understanding if a phrase indicates a positive or negative statement. A naive Bayes classifier can suffice for this level of understanding. It works in a similar way a spam filter works: it divides the messages into two categories (i.e., spam and non-spam) relying on the probability of each word being present in any of the two categories.&lt;/p&gt;
&lt;p&gt;An alternative is to manually associate an emotional ranking to a word. For example, a value between -10/-5 and 0 for &lt;code&gt;catastrophic&lt;/code&gt; and one between 0 and 5/10 for &lt;code&gt;nice&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;If you need a subtler evaluation you need to resort to machine learning techniques.&lt;/p&gt;
&lt;h5&gt;Parts of Speech Tagging&lt;/h5&gt;
&lt;p&gt;Parts of Speech Tagging (usually abbreviated as POS-tagging) indicates the identification and labelling of the different parts of speech (e.g., what is a noun, verb, adjective, etc.). While is an integral part of parsing, it can also be used to simplify other tasks. For instance, it can be used in the creation of summaries to simplify the sentences chosen for the summary (e.g., removing subordinates’ clauses).&lt;/p&gt;
&lt;h5&gt;Lemmatizer&lt;/h5&gt;
&lt;p&gt;A lemmatizer return the lemma for a given word and a part of speech tag. Basically, it gives the corresponding dictionary form of a word. In some ways it can be considered an advanced form of a stemmer. It can also be used for similar purposes, namely it can ensure that all different forms of a word are correctly linked to the same concepts.&lt;/p&gt;
&lt;p&gt;For instance, it can transform all instances of &lt;code&gt;cats&lt;/code&gt; in &lt;code&gt;cat&lt;/code&gt;, for search purposes. However, it can also distinguish between the cases of &lt;code&gt;run&lt;/code&gt; as in the verb &lt;code&gt;to run&lt;/code&gt; and &lt;code&gt;run&lt;/code&gt; as in the noun synonym of a &lt;code&gt;jog&lt;/code&gt;.&lt;/p&gt;
&lt;h5&gt;Chunking&lt;/h5&gt;
&lt;p&gt;Parts of speech tagging can be considered equivalent to lexing in natural languages. Chunking, also known as shallow parsing, is a step above parts of speech tagging, but one below the final parsing. It connects parts of speech in higher units of meaning, for example complements. Imagine the phrase &lt;code&gt;John always wins our matches of Russian roulette&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;a POS-tagger identifies that Russian is an adjective and roulette a noun&lt;/li&gt;
&lt;li&gt;a chunker groups together &lt;code&gt;(of) Russian roulette&lt;/code&gt; as a complement or two related parts of speech&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The chunker might work to produce units that are going to be used by a parser. It can also work independently, for example to help in named-entity recognition.&lt;/p&gt;
&lt;h5&gt;Parsing&lt;/h5&gt;
&lt;p&gt;The end result is the same as for computer languages: &lt;a href=&quot;https://tomassetti.me/guide-parsing-algorithms-terminology/#parsingTree&quot;&gt;a parse tree&lt;/a&gt;. Though the process is quite different, and it might start with a probabilistic grammar or even with no grammar at all. It also usually continues with a lot of probabilities and statistical methods.&lt;/p&gt;
&lt;p&gt;The following is a parse tree created by the Stanford Parser (we are going to see it later) for the phrase &lt;code&gt;My dog likes hunting cats and people&lt;/code&gt;. Groups of letters such as NP indicates parts of speech or complements.&lt;/p&gt;
&lt;div id=&quot;crayon-5a0c62d356fb3427025699&quot; class=&quot;crayon-syntax crayon-theme-son-of-obsidian crayon-font-monaco crayon-os-pc print-yes notranslate&quot; data-settings=&quot; minimize scroll-mouseover wrap&quot;&gt;
&lt;div class=&quot;crayon-plain-wrap&quot;&gt;
&lt;textarea class=&quot;crayon-plain print-no&quot; data-settings=&quot;dblclick&quot; readonly=&quot;readonly&quot;&gt;
(ROOT
  (S
    (NP (PRP$ My) (NN dog))
    (VP (VBZ likes)
      (NP (NN hunting) (NNS cats)
        (CC and)
        (NNS people)))))
&lt;/textarea&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-main&quot;&gt;
&lt;table class=&quot;crayon-table&quot;&gt;&lt;tr class=&quot;crayon-row&quot;&gt;&lt;td class=&quot;crayon-nums&quot; data-settings=&quot;show&quot;&gt;

&lt;/td&gt;
&lt;td class=&quot;crayon-code&quot;&gt;
&lt;div class=&quot;crayon-pre&quot;&gt;


&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5a0c62d356fb3427025699-3&quot;&gt;    (NP (PRP$ My) (NN dog))&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5a0c62d356fb3427025699-4&quot;&gt;    (VP (VBZ likes)&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5a0c62d356fb3427025699-5&quot;&gt;      (NP (NN hunting) (NNS cats)&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5a0c62d356fb3427025699-6&quot;&gt;        (CC and)&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5a0c62d356fb3427025699-7&quot;&gt;        (NNS people)))))&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h5&gt;Translation&lt;/h5&gt;
&lt;p&gt;The current best methods for automatic machine translation rely on machine learning. The good news is that this means you just need a great number of documents in the languages you care about, without any annotation. Typical sources of such texts are Wikipedia and the official documentation of the European Union (which requires documents to be translated in all the official languages of the Union).&lt;/p&gt;
&lt;p&gt;As anybody that have tried Google Translate or Bing Translator can attest, the results are generally good enough for understanding, but still often a bit off. They cannot substitute a human translator.&lt;/p&gt;
&lt;h4 id=&quot;libraries&quot;&gt;The Best Libraries Available&lt;/h4&gt;
&lt;p&gt;The following libraries can be used for multiple purposes, so we are going to divide this section by the title of the libraries. Most of them are in Python or Java.&lt;/p&gt;
&lt;h5&gt;Apache OpenNLP&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;The &lt;a href=&quot;http://opennlp.apache.org/&quot;&gt;Apache OpenNLP&lt;/a&gt; library is a machine learning based toolkit for the processing of natural language text. It supports the most common NLP tasks, such as tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, and coreference resolution. These tasks are usually required to build more advanced text processing services. OpenNLP also included maximum entropy and perceptron based machine learning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Apache OpenNLP is a Java library with an &lt;a href=&quot;http://opennlp.apache.org/docs/1.8.1/manual/opennlp.html&quot;&gt;excellent documentation&lt;/a&gt; that can fulfill most of the tasks we have just discussed, except for sentiment analysis and translation. The developers provide &lt;a href=&quot;http://opennlp.sourceforge.net/models-1.5/&quot;&gt;language models for a few languages in addition to English&lt;/a&gt;, the most notable are German, Spanish and Portuguese.&lt;/p&gt;
&lt;h5&gt;The Classical Language Toolkit&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;The &lt;a href=&quot;http://cltk.org/&quot;&gt;Classical Language Toolkit (CLTK)&lt;/a&gt; offers natural language processing (NLP) support for the languages of Ancient, Classical, and Medieval Eurasia. Greek and Latin functionality are currently most complete.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As the name implies the major feature of the Classical Language Toolkit is the support for classical (ancient) languages, such as Greek and Latin. It has basic NLP tools, such as a lemmatizer, but also indispensable tools to work with ancient languages, such as transliteration support, and peculiar things like &lt;a href=&quot;http://docs.cltk.org/en/latest/latin.html#clausulae-analysis&quot;&gt;Clausulae Analysis&lt;/a&gt;. It has a good documentation and it is your only choice for ancient languages.&lt;/p&gt;
&lt;h5&gt;FreeLing&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://nlp.lsi.upc.edu/freeling/&quot;&gt;FreeLing&lt;/a&gt; is a C++ library providing language analysis functionalities (morphological analysis, named entity detection, PoS-tagging, parsing, Word Sense Disambiguation, Semantic Role Labelling, etc.) for a variety of languages (English, Spanish, Portuguese, Italian, French, German, Russian, Catalan, Galician, Croatian, Slovene, among others).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is a library with a good documentation and even a &lt;a href=&quot;http://nlp.lsi.upc.edu/freeling/demo/demo.php&quot;&gt;demo&lt;/a&gt;. It supports many languages usually excluded by other tools, but it is released the Affero GPL which is probably the least user-friendly license ever conceived.&lt;/p&gt;
&lt;h5&gt;Moses&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;http://www.statmt.org/moses/index.php?n=Main.HomePage&quot;&gt;Moses&lt;/a&gt; is a statistical machine translation system that allows you to automatically train translation models for any language pair. All you need is a collection of translated texts (parallel corpus). Once you have a trained model, an efficient search algorithm quickly finds the highest probability translation among the exponential number of choices.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The only thing to add is that the system is written in C++ and there is ample documentation.&lt;/p&gt;
&lt;h5&gt;NLTK&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Natural Language Toolkit (NLTK) is probably the most known NLP library for Python. The library can accomplish many tasks in different ways (i.e., using different algorithms). It even has a good documentation (if you include the freely available book).&lt;/p&gt;
&lt;p&gt;Simply put: it is the standard library for NLP research. Though one issue that some people have is exactly that: it is designed for research and educational purposes. If there are ten ways to do something NLTK would allow you to choose among them all. The intended user is a person with a deep understanding of NLP&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://textblob.readthedocs.io/en/dev/index.html&quot;&gt;TextBlob&lt;/a&gt; is a library that builds upon NLTK (and Pattern) to simplify processing of textual data. The library also provides translation, but it does not implement it directly: it is simply an interface for Google Translate.&lt;/p&gt;
&lt;h5&gt;Pattern&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/clips/pattern&quot;&gt;Pattern&lt;/a&gt; is the most peculiar software in our collection because it is a collection of Python libraries for web mining. It has support for data mining from services such as Google and Twitter (i.e., it provide functions to directly search from Google/Twitter) , an HTML parser and many other things. Among these things there is natural language processing for English and a few other languages, including German, Spanish, French and Italian.&lt;/p&gt;
&lt;p&gt;Though English support is more advanced than the rest.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The pattern.en module contains a fast part-of-speech tagger for English (identifies nouns, adjectives, verbs, etc. in a sentence), sentiment analysis, tools for English verb conjugation and noun singularization &amp;amp; pluralization, and a WordNet interface.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The rest of the libraries can only support POS-tagging.&lt;/p&gt;
&lt;h5&gt;Polyglot&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;http://polyglot-nlp.com/&quot;&gt;Polyglot&lt;/a&gt; is a set of NLP libraries for many natural languages in Python. It looks great, although it has only little documentation.&lt;/p&gt;
&lt;p&gt;It supports fewer languages for the more advanced tasks, such as POS tagging (16) or named entity recognition (40). However, for sentiment analysis and language identification can work with more than a hundred of them.&lt;/p&gt;
&lt;h5&gt;Sentiment and Sentiment&lt;/h5&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/thisandagain/sentiment&quot;&gt;Sentiment&lt;/a&gt; is JavaScript (Node.js) library for sentiment analysis. The library relies on AFINN (a collection of English words with an associated emotional value) and a similar database for Emoji. These database associate to each word/Emoji a positive or negative value, to indicate a positive or negative sentiment. For example, the word &lt;code&gt;joy&lt;/code&gt; has a score of &lt;code&gt;3&lt;/code&gt;, while &lt;code&gt;sad&lt;/code&gt; has &lt;code&gt;-2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The code for the library itself is quite trivial, but it works, and it is easy to use.&lt;/p&gt;
&lt;div id=&quot;crayon-5a0c62d356fc5662774858&quot; class=&quot;crayon-syntax crayon-theme-son-of-obsidian crayon-font-monaco crayon-os-pc print-yes notranslate&quot; data-settings=&quot; minimize scroll-mouseover wrap&quot;&gt;
&lt;div class=&quot;crayon-plain-wrap&quot;&gt;
&lt;textarea class=&quot;crayon-plain print-no&quot; data-settings=&quot;dblclick&quot; readonly=&quot;readonly&quot;&gt;
var sentiment = require('sentiment');

var r1 = sentiment('Cats are stupid.');
console.dir(r1);        // Score: -2, Comparative: -0.666

var r2 = sentiment('Cats are totally amazing!');
console.dir(r2);        // Score: 4, Comparative: 1
&lt;/textarea&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-main&quot;&gt;
&lt;table class=&quot;crayon-table&quot;&gt;&lt;tr class=&quot;crayon-row&quot;&gt;&lt;td class=&quot;crayon-nums&quot; data-settings=&quot;show&quot;&gt;

&lt;/td&gt;
&lt;td class=&quot;crayon-code&quot;&gt;
&lt;div class=&quot;crayon-pre&quot;&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5a0c62d356fc5662774858-1&quot;&gt;&lt;span class=&quot;crayon-t&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;crayon-v&quot;&gt;sentiment&lt;/span&gt; &lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;crayon-e&quot;&gt;require&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;'sentiment'&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;;&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5a0c62d356fc5662774858-3&quot;&gt;&lt;span class=&quot;crayon-t&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;crayon-v&quot;&gt;r1&lt;/span&gt; &lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;crayon-e&quot;&gt;sentiment&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;'Cats are stupid.'&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5a0c62d356fc5662774858-4&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;dir&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;r1&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;// Score: -2, Comparative: -0.666&lt;/span&gt;&lt;/div&gt;

&lt;div class=&quot;crayon-line crayon-striped-line&quot; id=&quot;crayon-5a0c62d356fc5662774858-6&quot;&gt;&lt;span class=&quot;crayon-t&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;crayon-v&quot;&gt;r2&lt;/span&gt; &lt;span class=&quot;crayon-o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;crayon-e&quot;&gt;sentiment&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-s&quot;&gt;'Cats are totally amazing!'&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;crayon-line&quot; id=&quot;crayon-5a0c62d356fc5662774858-7&quot;&gt;&lt;span class=&quot;crayon-v&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;crayon-e&quot;&gt;dir&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;crayon-v&quot;&gt;r2&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;crayon-sy&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;crayon-h&quot;&gt;        &lt;/span&gt;&lt;span class=&quot;crayon-c&quot;&gt;// Score: 4, Comparative: 1&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Sentiment analysis using machine learning techniques.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That is the extent of the documentation for the Python library &lt;a href=&quot;https://github.com/vivekn/sentiment&quot;&gt;sentiment&lt;/a&gt;. Although there is also a &lt;a href=&quot;https://arxiv.org/abs/1305.6143&quot;&gt;paper&lt;/a&gt; and a &lt;a href=&quot;http://sentiment.vivekn.com/&quot;&gt;demo&lt;/a&gt;. The paper mentions that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We have explored different methods of improving the accuracy of a Naive Bayes classifier for sentiment analysis. We observed that a combination of methods like negation handling, word n-grams and feature selection by mutual information results in a significant improvement in accuracy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Which means that it can be a good starting point to understand how to build your own sentiment analysis library.&lt;/p&gt;
&lt;h5&gt;SpaCy&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;Industrial-Strength Natural Language Processing in Python&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The library &lt;a href=&quot;https://spacy.io/&quot;&gt;spaCy&lt;/a&gt; claims to be a much more efficient, ready for the real world and easy to use library than NLTK. In practical terms it has two advantages over NLTK:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;better performance&lt;/li&gt;
&lt;li&gt;it does not give you the chance of choosing among the many algorithms the one you think is best, instead it chooses the best one for each task. While less choices might seem bad, it can actually be a good thing. That is if you have no idea what the algorithms do and you have to learn them before making a decision.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;In practical terms it is a library that supports most of the basic tasks we mentioned (i.e., things like named entity recognition and POS-tagging, but not translation or parsing) with a great code-first documentation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/chartbeat-labs/textacy&quot;&gt;Textacy&lt;/a&gt; is a library built on top of spaCY for higher-level NLP tasks. Basically, it simplifies some things including features for cleaning data or managing it better.&lt;/p&gt;
&lt;h5&gt;The Stanford Natural Language Processing Group Software&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;The Stanford NLP Group makes some of our Natural Language Processing software available to everyone! We provide statistical NLP, deep learning NLP, and rule-based NLP tools for major computational linguistics problems, which can be incorporated into applications with human language technology needs. These packages are widely used in industry, academia, and government.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The Stanford NLP group creates and support many great tools that cover all the purposes we have just mentioned. The only thing missing is sentiment analysis. The most notable software are &lt;a href=&quot;https://stanfordnlp.github.io/CoreNLP/&quot;&gt;CoreNLP&lt;/a&gt; and &lt;a href=&quot;https://nlp.stanford.edu/software/lex-parser.html&quot;&gt;Parser&lt;/a&gt;. The parser can be seen in action in a &lt;a href=&quot;http://nlp.stanford.edu:8080/parser/index.jsp&quot;&gt;web demo&lt;/a&gt;. CoreNLP is a combination of several tools, including the parser.&lt;/p&gt;
&lt;p&gt;The tools are all in Java. The parser supports a few languages: English, Chinese, Arabic, Spanish, etc. The only downside is that the tools are licensed under the GPL. Commercial licensing is available for proprietary software.&lt;/p&gt;
&lt;h5&gt;Excluded Software&lt;/h5&gt;
&lt;p&gt;We think that the libraries we choose are the best ones for parsing, or processing, natural languages. However we excluded some other interesting software, which are usually mentioned, like &lt;a href=&quot;https://github.com/CogComp/cogcomp-nlp&quot;&gt;CogCompNLP&lt;/a&gt; or &lt;a href=&quot;https://gate.ac.uk/&quot;&gt;GATE&lt;/a&gt; for several reasons:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;there might have little to no documentation&lt;/li&gt;
&lt;li&gt;it might have a purely educational or any non-standard license&lt;/li&gt;
&lt;li&gt;it might not be designed for developers, but for end-users&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;In this article we have seen many ways to deal with a document in a natural language to get the information you need from it. Most of them tried to find smart ways to bypass the complex task of parsing natural language. Despite being hard to parse natural languages it is still possible to do so, if you use the libraries available.&lt;/p&gt;
&lt;p&gt;Essentially, when dealing with natural languages hacking a solution is the suggested way of doing things, since nobody can figure out how to do it properly.&lt;/p&gt;
&lt;p&gt;Where it was possible we explained the algorithms that you can use. For the most advanced tasks this would have been impractical, so we just pointed at ready-to-use libraries. In any case, if you think we missed something, be it a subject or an important library, please contact us.&lt;/p&gt;


&lt;div class=&quot;ck_form_container ck_inline&quot; data-ck-version=&quot;6&quot;&gt;
&lt;div class=&quot;ck_form ck_vertical_subscription_form&quot;&gt;
&lt;div class=&quot;ck_form_content&quot;&gt;
&lt;h3 class=&quot;ck_form_title&quot;&gt;Parsing: Tools and Libraries&lt;/h3&gt;
&lt;div class=&quot;ck_description&quot;&gt;&lt;span class=&quot;ck_image&quot;&gt;&lt;img alt=&quot;Parsing_-_tools_and_libraries_-_cover&quot; src=&quot;https://i0.wp.com/s3.amazonaws.com/convertkit/subscription_forms/images/005/053/473/standard/Parsing_-_Tools_and_Libraries_-_Cover.png?w=1500&amp;amp;ssl=1&quot; data-recalc-dims=&quot;1&quot;/&gt;&lt;/span&gt;
&lt;p&gt;Receive the guide to your inbox to read it on all your devices when you have time. Learn about parsing in Java, Python, C#, and JavaScript&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;span id=&quot;tve_leads_end_content&quot;/&gt;

</description>
<pubDate>Wed, 15 Nov 2017 10:12:41 +0000</pubDate>
<dc:creator>ftomassetti</dc:creator>
<og:type>article</og:type>
<og:title>A Guide to Natural Language Processing - Federico Tomassetti - Software Architect</og:title>
<og:description>Natural Language Processing (NLP) offers amazing possibilities to elaborate text and extract information from it. This guide is a complete overview of NLP.</og:description>
<og:url>https://tomassetti.me/guide-natural-language-processing/</og:url>
<og:image>https://tomassetti.me/wp-content/uploads/2017/10/Parsing-Natural-Languages.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://tomassetti.me/guide-natural-language-processing/</dc:identifier>
</item>
<item>
<title>Hotbed of Misinformation</title>
<link>https://www.tesla.com/blog/hotbed-misinformation</link>
<guid isPermaLink="true" >https://www.tesla.com/blog/hotbed-misinformation</guid>
<description>&lt;p&gt;Tesla is absolutely against any form of discrimination, harassment, or unfair treatment of any kind. When we hear complaints, we take them very seriously, investigate thoroughly and, if proven to be true, take immediate action.&lt;/p&gt;
&lt;p&gt;Everyone at Tesla, without exception, is required to go through an anti-discrimination course. Our human resources team also conducts regular in-person spot training sessions when an allegation or complaint has been made, even if the evidence is not conclusive enough to warrant disciplinary action. We have also created a dedicated team focused exclusively on investigating workplace concerns, recommending corrective actions and assisting managers with implementing those actions.&lt;/p&gt;
&lt;p&gt;Regarding yesterday’s lawsuit, several months ago we had already investigated disappointing behavior involving a group of individuals who worked on or near Marcus Vaughn’s team. At the time, our investigation identified a number of conflicting accusations and counter-accusations between several African-American and Hispanic individuals, alleging use of racial language, including the &quot;n-word&quot; and &quot;w-word,&quot; towards each other and a threat of violence. After a thorough investigation, immediate action was taken, which included terminating the employment of three of the individuals.&lt;/p&gt;
&lt;p&gt;We believe this was the fair and just response to the facts that we learned. There will be further action as necessary, including parting ways with anyone whose behavior prevents Tesla from being a great place to work and making sure we do everything possible to stop bad behavior from happening in the first place. Our company has more than 33,000 employees, with over 10,000 in the Fremont factory alone, so it is not humanly possible to stop all bad conduct, but we will do our best to make it is as close to zero as possible.&lt;/p&gt;
&lt;p margin-top:=&quot;&quot;&gt;There are a number of other false statements in the class action lawsuit alleging a so-called “hotbed of discrimination”:&lt;/p&gt;
&lt;p&gt;- There is only one actual plaintiff (Marcus Vaughn), not 100. The reference to 100 is a complete fabrication with no basis in fact at all.&lt;/p&gt;&lt;p&gt;- The plaintiff was employed by a temp agency, not by Tesla as claimed in the lawsuit.&lt;/p&gt;&lt;p&gt;- Marcus was not fired, he was on a six month temp contract that simply ended as contracted.&lt;/p&gt;&lt;p&gt;- His email to Elon was about his commute and Tesla’s shuttles, which was addressed as he requested. There was no mention of racial discrimination whatsoever.&lt;/p&gt;&lt;p&gt;- The trial lawyer who filed this lawsuit has a long track record of extorting money for meritless claims and using the threat of media attacks and expensive trial costs to get companies to settle. At Tesla, we would rather pay ten times the settlement demand in legal fees and fight to the ends of the Earth than give in to extortion and allow this abuse of the legal system.&lt;/p&gt;&lt;p&gt;- We would also like to clear up the description of Elon’s prior email to employees. It is dedicated to ensuring that Tesla employees always try to do the right thing, that being a jerk is not allowed, that everyone should be contributing to an atmosphere where people look forward to coming to work in the morning and that no one should feel excluded, uncomfortable, or unfairly treated. As one of many points in that email, Elon also explained that if someone makes an offensive or hurtful statement on a single occasion, but subsequently offers a sincere apology, then we believe that apology should be accepted. The counterpoint would be that a single careless comment should ruin a person’s life and career, even if they truly regret their action and do their best to make amends. That would be a cold world with no forgiveness and no heart.&lt;/p&gt;
&lt;p&gt;Elon’s full email is below:&lt;br/&gt;&lt;img src=&quot;https://www.tesla.com/sites/default/files/images/blogs/doing-the-right-thing.jpg&quot;/&gt;&lt;/p&gt;
</description>
<pubDate>Wed, 15 Nov 2017 05:42:23 +0000</pubDate>
<dc:creator>runesoerensen</dc:creator>
<og:title>Hotbed of Misinformation</og:title>
<og:description>Tesla is absolutely against any form of discrimination, harassment, or unfair treatment of any kind. When we hear complaints, we take them very seriously, investigate thoroughly and, if proven to be true, take immediate action.</og:description>
<og:image>https://www.tesla.com/sites/default/files/blog_images/tesla_announcement_social.jpg</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.tesla.com/blog/hotbed-misinformation</dc:identifier>
</item>
<item>
<title>Fearless Concurrency in Firefox Quantum</title>
<link>https://blog.rust-lang.org/2017/11/14/Fearless-Concurrency-In-Firefox-Quantum.html</link>
<guid isPermaLink="true" >https://blog.rust-lang.org/2017/11/14/Fearless-Concurrency-In-Firefox-Quantum.html</guid>
<description>&lt;p&gt;These days, Rust is used for &lt;a href=&quot;https://www.rust-lang.org/friends.html&quot;&gt;all kinds of things&lt;/a&gt;. But its founding application was &lt;a href=&quot;https://servo.org/&quot;&gt;Servo&lt;/a&gt;, an experimental browser engine.&lt;/p&gt;&lt;p&gt;Now, after years of effort, a major part of Servo is shipping in production: Mozilla is releasing &lt;a href=&quot;https://www.mozilla.org/en-US/firefox/quantum/&quot;&gt;Firefox Quantum&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Rust code &lt;a href=&quot;https://hacks.mozilla.org/2016/07/shipping-rust-in-firefox/&quot;&gt;began shipping in Firefox&lt;/a&gt; last year, starting with relatively small pilot projects like an MP4 metadata parser to replace some uses of libstagefright. These components performed well and caused effectively no crashes, but browser development had yet to see large benefits from the full power Rust could offer. This changes today.&lt;/p&gt;

&lt;p&gt;Firefox Quantum includes Stylo, a pure-Rust CSS engine that makes full use of Rust’s “&lt;a href=&quot;http://blog.rust-lang.org/2015/04/10/Fearless-Concurrency.html&quot;&gt;Fearless Concurrency&lt;/a&gt;” to speed up page styling. It’s the first major component of Servo to be integrated with Firefox, and is a major milestone for Servo, Firefox, and Rust. It replaces approximately 160,000 lines of C++ with 85,000 lines of Rust.&lt;/p&gt;
&lt;p&gt;When a browser is loading a web page, it looks at the CSS and parses the rules. It then determines which rules apply to which elements and their precedence, and “cascades” these down the DOM tree, computing the final style for each element. Styling is a top-down process: you need to know the style of a parent to calculate the styles of its children, but the styles of its children can be calculated independently thereafter.&lt;/p&gt;
&lt;p&gt;This top-down structure is ripe for parallelism; however, since styling is a complex process, it’s hard to get right. Mozilla made two previous attempts to parallelize its style system in C++, and both of them failed. But Rust’s fearless concurrency has made parallelism practical! We use &lt;a href=&quot;https://crates.io/crates/rayon&quot;&gt;rayon&lt;/a&gt; —one of the hundreds of &lt;a href=&quot;http://crates.io/&quot;&gt;crates&lt;/a&gt; Servo uses from Rust’s ecosystem — to drive a work-stealing cascade algorithm. You can read more about that in &lt;a href=&quot;https://hacks.mozilla.org/2017/08/inside-a-super-fast-css-engine-quantum-css-aka-stylo/&quot;&gt;Lin Clark’s post&lt;/a&gt;. Parallelism leads to a lot of performance improvements, including a 30% page load speedup for Amazon’s homepage.&lt;/p&gt;

&lt;p&gt;An example of Rust preventing thread safety bugs is how style information is shared in Stylo. Computed styles are grouped into “style structs” of related properties, e.g. there’s one for all the font properties, one for all the background properties, and so on. Now, most of these are shared; for example, the font of a child element is usually the same as its parent, and often sibling elements share styles even if they don’t have the same style as the parent. Stylo uses Rust’s atomically reference counted &lt;a href=&quot;https://doc.rust-lang.org/std/sync/struct.Arc.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Arc&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/a&gt; to share style structs between elements. &lt;code class=&quot;highlighter-rouge&quot;&gt;Arc&amp;lt;T&amp;gt;&lt;/code&gt; makes its contents immutable, so it’s thread safe — you can’t accidentally modify a style struct when there’s a chance it is being used by other elements.&lt;/p&gt;
&lt;p&gt;We supplement this immutable access with &lt;code class=&quot;highlighter-rouge&quot;&gt;Arc::make_mut()&lt;/code&gt;; for example, &lt;a href=&quot;https://github.com/servo/servo/blob/657b2339a1e68f3a9c4525f35db023d3f149ffac/components/style/values/computed/font.rs#L182&quot;&gt;this line&lt;/a&gt; calls &lt;code class=&quot;highlighter-rouge&quot;&gt;.mutate_font()&lt;/code&gt; (a thin wrapper around &lt;code class=&quot;highlighter-rouge&quot;&gt;Arc::make_mut()&lt;/code&gt; for the font style struct) to set the font size. If the given element is the only element that has a reference to this specific font struct, it will just mutate it in place. But if it is not, &lt;code class=&quot;highlighter-rouge&quot;&gt;make_mut()&lt;/code&gt; will copy the entire style struct into a new, unique reference, which will then be mutated in place and eventually stored on the element.&lt;/p&gt;
&lt;div class=&quot;language-rust highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;.builder&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.mutate_font&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;.set_font_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;computed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;On the other hand, Rust guarantees that it is impossible to mutate the style of the &lt;em&gt;parent&lt;/em&gt; element, because it is &lt;a href=&quot;https://github.com/servo/servo/blob/657b2339a1e68f3a9c4525f35db023d3f149ffac/components/style/properties/properties.mako.rs#L2623-L2627&quot;&gt;kept behind an immutable reference&lt;/a&gt;. Rayon’s scoped threading functionality makes sure that there is no way for that struct to even obtain/store a mutable reference if it wanted to. The parent style is something which one thread was allowed to write to to create (when the parent element was being processed), after which everyone is only allowed to read from it. You’ll notice that the reference is a zero-overhead “borrowed pointer”, &lt;em&gt;not&lt;/em&gt; a reference counted pointer, because Rust and Rayon let you share data across threads without needing reference counting when they can guarantee that the data will be alive at least as long as the thread.&lt;/p&gt;
&lt;p&gt;Personally, my “aha, I now fully understand the power of Rust” moment was when thread safety issues cropped up on the C++ side. Browsers are complex beings, and despite Stylo being Rust code, it needs to call back into Firefox’s C++ code a lot. Firefox has a single “main thread” per process, and while it does use other threads they are relatively limited in what they do. Stylo, being quite parallel, occasionally calls into C++ code off the main thread. That was usually fine, but would regularly surface thread safety bugs in the C++ code when there was a cache or global mutable state involved, things which basically never were a problem on the Rust side.&lt;/p&gt;
&lt;p&gt;These bugs were not easy to notice, and were often very tricky to debug. And that was with only the &lt;em&gt;occasional&lt;/em&gt; call into C++ code off the main thread; It feels like if we had tried this project in pure C++ we’d be dealing with this far too much to be able to get anything useful done. And indeed, bugs like these have thwarted multiple attempts to parallelize styling in the past, both in Firefox and other browsers.&lt;/p&gt;

&lt;p&gt;Firefox developers had a great time learning and using Rust. People really enjoyed being able to aggressively write code without having to worry about safety, and many mentioned that Rust’s ownership model was close to how they implicitly reason about memory within Firefox’s large C++ codebase. It was refreshing to have fuzzers catch mostly explicit &lt;em&gt;panics&lt;/em&gt; in Rust code, which are much easier to debug and fix than segfaults and other memory safety issues on the C++ side.&lt;/p&gt;
&lt;p&gt;A conversation amongst Firefox developers that stuck with me — one that was included in Josh Matthews’ &lt;a href=&quot;https://www.joshmatthews.net/rbr17&quot;&gt;talk&lt;/a&gt; at Rust Belt Rust — was&lt;/p&gt;
&lt;blockquote readability=&quot;20&quot;&gt;
&lt;p&gt;&amp;lt;heycam&amp;gt; one of the best parts about stylo has been how much easier it has been to implement these style system optimizations that we need, because Rust&lt;/p&gt;
&lt;p&gt;&amp;lt;heycam&amp;gt; can you imagine if we needed to implement this all in C++ in the timeframe we have&lt;/p&gt;
&lt;p&gt;&amp;lt;heycam&amp;gt; yeah srsly&lt;/p&gt;
&lt;p&gt;&amp;lt;bholley&amp;gt; heycam: it’s so rare that we get fuzz bugs in rust code&lt;/p&gt;
&lt;p&gt;&amp;lt;bholley&amp;gt; heycam: considering all the complex stuff we’re doing&lt;/p&gt;
&lt;p&gt;*heycam remembers getting a bunch of fuzzer bugs from all kinds of style system stuff in gecko&lt;/p&gt;
&lt;p&gt;&amp;lt;bholley&amp;gt; heycam: think about how much time we could save if each one of those annoying compiler errors today was swapped for a fuzz bug tomorrow :-)&lt;/p&gt;
&lt;p&gt;&amp;lt;heycam&amp;gt; heh&lt;/p&gt;
&lt;p&gt;&amp;lt;njn&amp;gt; you guys sound like an ad for Rust&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Overall, Firefox Quantum benefits significantly from Stylo, and thus from Rust. Not only does it speed up page load, but it also speeds up interaction times since styling information can be recalculated much faster, making the entire experience smoother.&lt;/p&gt;
&lt;p&gt;But Stylo is only the beginning. There are two major Rust integrations getting close to the end of the pipeline. One is integrating &lt;a href=&quot;https://github.com/servo/webrender/&quot;&gt;Webrender&lt;/a&gt; into Firefox; Webrender &lt;a href=&quot;https://hacks.mozilla.org/2017/10/the-whole-web-at-maximum-fps-how-webrender-gets-rid-of-jank/&quot;&gt;heavily uses the GPU to speed up rendering&lt;/a&gt;. Another is &lt;a href=&quot;https://github.com/pcwalton/pathfinder&quot;&gt;Pathfinder&lt;/a&gt;, a project that offloads font rendering to the GPU. And beyond those, there remains Servo’s parallel layout and DOM work, which are continuing to grow and improve. Firefox has a very bright future ahead.&lt;/p&gt;
&lt;p&gt;As a Rust team member, I’m really happy to see Rust being successfully used in production to such great effect! As a Servo and Stylo developer, I’m grateful to the tools Rust gave us to be able to pull this off, and I’m happy to see a large component of Servo finally make its way to users!&lt;/p&gt;
&lt;p&gt;Experience the benefits of Rust yourself — try out &lt;a href=&quot;https://www.mozilla.org/en-US/firefox/quantum/&quot;&gt;Firefox Quantum&lt;/a&gt;!&lt;/p&gt;
</description>
<pubDate>Wed, 15 Nov 2017 03:27:20 +0000</pubDate>
<dc:creator>ahomescu1</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.rust-lang.org/2017/11/14/Fearless-Concurrency-In-Firefox-Quantum.html</dc:identifier>
</item>
<item>
<title>Fructan, not gluten, induces symptoms in patients with gluten sensitivity</title>
<link>http://www.gastrojournal.org/article/S0016-5085(17)36302-3/pdf</link>
<guid isPermaLink="true" >http://www.gastrojournal.org/article/S0016-5085(17)36302-3/pdf</guid>
<description>[unable to retrieve full-text content]&lt;p&gt;Article URL: &lt;a href=&quot;http://www.gastrojournal.org/article/S0016-5085(17)36302-3/pdf&quot;&gt;http://www.gastrojournal.org/article/S0016-5085(17)36302-3/pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=15699841&quot;&gt;https://news.ycombinator.com/item?id=15699841&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Points: 213&lt;/p&gt;&lt;p&gt;# Comments: 107&lt;/p&gt;</description>
<pubDate>Tue, 14 Nov 2017 22:33:51 +0000</pubDate>
<dc:creator>kmundnic</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.rust-lang.org/2017/11/14/Fearless-Concurrency-In-Firefox-Quantum.html</dc:identifier>
</item>
<item>
<title>“It is never a compiler error”</title>
<link>https://blog.plover.com/2017/11/12/</link>
<guid isPermaLink="true" >https://blog.plover.com/2017/11/12/</guid>
<description>&lt;p&gt;&lt;a class=&quot;storytitle&quot; name=&quot;compiler-error&quot; href=&quot;https://blog.plover.com/prog/compiler-error.html&quot; id=&quot;compiler-error&quot;&gt;No, it is not a compiler error. It is never a compiler error.&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;When I used to hang out in the &lt;code&gt;comp.lang.c&lt;/code&gt; Usenet group, back when there &lt;em&gt;was&lt;/em&gt; a &lt;code&gt;comp.lang.c&lt;/code&gt; Usenet group, people would show up fairly often with some program they had written that didn't work, and ask if their compiler had a bug. The compiler did not have a bug. The compiler never had a bug. The bug was always in the programmer's code and usually in their understanding of the language.&lt;/p&gt;
&lt;p&gt;When I worked at the University of Pennsylvania, a grad student posted to one of the internal bulletin boards looking for help with a program that didn't work. Another graduate student, a super-annoying know-it-all, said confidently that it was certainly a compiler bug. It was not a compiler bug. It was caused by a misunderstanding of the way arguments to unprototyped functions were automatically promoted.&lt;/p&gt;
&lt;p&gt;This is actually a subtle point, obscure and easily misunderstood. Most examples I have seen of people blaming the compiler are much sillier. I used to be on the mailing list for discussing the development of Perl 5, and people would show up from time to time to ask if Perl's &lt;code&gt;if&lt;/code&gt; statement was broken. This is a little mind-boggling, that someone could think this. Perl was first released in 1987. (How time flies!) The &lt;code&gt;if&lt;/code&gt; statement is not exactly an obscure or little-used feature. If there had been a bug in &lt;code&gt;if&lt;/code&gt; it would have been discovered and fixed by 1988. Again, the bug was always in the programmer's code and usually in their understanding of the language.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://groups.google.com/d/msg/comp.lang.perl.misc/l95-_OoO8ck/zG3ob9-b5wsJ&quot;&gt;Here's something I wrote in October 2000&lt;/a&gt;, which I think makes the case very clearly, this time concerning a claimed bug in the &lt;code&gt;stat()&lt;/code&gt; function, another feature that first appeared in Perl 1.000:&lt;/p&gt;
&lt;blockquote readability=&quot;21&quot;&gt;
&lt;p&gt;On the one hand, there's a chance that the compiler has a broken &lt;code&gt;stat&lt;/code&gt; and is subtracting 6 or something. Maybe that sounds likely to you but it sounds really weird to me. I cannot imagine how such a thing could possibly occur. Why 6? It all seems very unlikely.&lt;/p&gt;
&lt;p&gt;Well, in the absence of an alternative hypothesis, we have to take what we can get. But in this case, there is an alternative hypothesis! The alternative hypothesis is that [this person's] program has a bug.&lt;/p&gt;
&lt;p&gt;Now, which seems more likely to you?&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Weird, inexplicable compiler bug that nobody has ever seen before&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;or&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Programmer fucked up&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Hmmm. Let me think.&lt;/p&gt;
&lt;p&gt;I'll take Door #2, Monty.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Presumably I had to learn this myself at some point. A programmer can waste a lot of time looking for the bug in the compiler instead of looking for the bug in their program. I have a file of (obnoxious) Good Advice for Programmers that I wrote about twenty years ago, and one of these items is:&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;Looking for a compiler bug is the strategy of LAST resort. LAST resort.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Anyway, I will get to the point. As I mentioned a few months ago, &lt;a href=&quot;https://blog.plover.com/math/24-puzzle-3.html#phoneapp&quot;&gt;I built a simple phone app&lt;/a&gt; that Toph and I can use to find solutions to “twenty-four puzzles”. In these puzzles, you are given four single-digit numbers and you have to combine them arithmetically to total 24. Pennsylvania license plates have four digits, so as we drive around we play the game with the license plate numbers we see. Sometimes we can't solve a puzzle, and then we wonder: is it because there is no solution, or because we just couldn't find one? Then we ask the phone app.&lt;/p&gt;
&lt;p&gt;The other day we saw the puzzle «5 4 5 1», which is very easy, but I asked the phone app, to find out if there were any other solutions that we missed. And it announced `No solutions.” Which is wrong. So my program had a bug, as my programs often do.&lt;/p&gt;
&lt;p&gt;The app has a pre-populated dictionary containing all possible solutions to all the puzzles that have solutions, which I generated ahead of time and embedded into the app. My first guess was that bug had been in the process that generated this dictionary, and that it had somehow missed the solutions of «5 4 5 1». These would be indexed under the key &lt;code&gt;1455&lt;/code&gt;, which is the same puzzle, because each list of solutions is associated with the four input numbers in ascending order. Happily I still had the original file containing the dictionary data, but when I looked in it under &lt;code&gt;1455&lt;/code&gt; I saw exactly the two solutions that I expected to see.&lt;/p&gt;
&lt;p&gt;So then I looked into the app itself to see where the bug was. Code Studio's underlying language is Javascript, and Code Studio has a nice debugger. I ran the app under the debugger, and stopped in the relevant code, which was:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    var x = [getNumber(&quot;a&quot;), getNumber(&quot;b&quot;), getNumber(&quot;c&quot;), getNumber(&quot;d&quot;)].sort().join(&quot;&quot;);
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This constructs a hash key (&lt;code&gt;x&lt;/code&gt;) that is used to index into the canned dictionary of solutions. The &lt;code&gt;getNumber()&lt;/code&gt; calls were retrieving the four numbers from the app's menus, and I verified that the four numbers were «5 4 5 1» as they ought to be. But what I saw next astounded me: &lt;code&gt;x&lt;/code&gt; was not being set to &lt;code&gt;1455&lt;/code&gt; as it should have been. It was set to &lt;code&gt;4155&lt;/code&gt;, which was not in the dictionary. And it was set to &lt;code&gt;4155&lt;/code&gt; because&lt;/p&gt;
&lt;p&gt;the built-in &lt;code&gt;sort()&lt;/code&gt; function&lt;/p&gt;
&lt;p&gt;was sorting the numbers&lt;/p&gt;
&lt;p&gt;into&lt;/p&gt;
&lt;p&gt;the&lt;/p&gt;
&lt;p&gt;wrong&lt;/p&gt;
&lt;p&gt;order.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://pic.blog.plover.com/prog/compiler-error/WTF.jpg&quot; /&gt;&lt;/p&gt;
&lt;p&gt;For a while I could not believe my eyes. But after another fifteen or thirty minutes of tinkering, I sent off a bug report… no, I did not. I still didn't believe it. I asked the front-end programmers at my company what my mistake had been. Nobody had any suggestions.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Then&lt;/em&gt; I sent off a bug report that began:&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;I think that Array.prototype.sort() returned a wrongly-sorted result when passed a list of four numbers. This seems impossible, but …&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I was about 70% expecting to get a reply back explaining what I had misunderstood about the behavior of Javascript's &lt;code&gt;sort()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But to my astonishment, the reply came back only an hour later:&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;Wow! You're absolutely right. We'll investigate this right away.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In case you're curious, the bug was as follows: The &lt;code&gt;sort()&lt;/code&gt; function was using a bubble sort. (This is of course a bad choice, and I think the maintainers plan to replace it.) The bubble sort makes several passes through the input, swapping items that are out of order. It keeps a count of the number of swaps in each pass, and if the number of swaps is zero, the array is already ordered and the sort can stop early and skip the remaining passes. The test for this was:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    if (changes &amp;lt;= 1) break;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;but it should have been:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;    if (changes == 0) break;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Ouch.&lt;/p&gt;
&lt;p&gt;The Code Studio folks handled this very creditably, and did indeed fix it the same day. (&lt;a href=&quot;https://support.code.org/hc/en-us/requests/115548&quot;&gt;The support system ticket is available for your perusal&lt;/a&gt;, as is &lt;a href=&quot;https://github.com/code-dot-org/JS-Interpreter/pull/23&quot;&gt;the Github pull request with the fix&lt;/a&gt;, in case you are interested.)&lt;/p&gt;
&lt;p&gt;I still can't quite believe it. I feel as though I have accidentally spotted the Loch Ness Monster, or Bigfoot, or something like that, a strange and legendary monster that until now I thought most likely didn't exist.&lt;/p&gt;
&lt;p&gt;A bug in the &lt;code&gt;sort()&lt;/code&gt; function. O day and night, but this is wondrous strange!&lt;/p&gt;
&lt;p&gt;[ Addendum 20171113: Thanks to &lt;a href=&quot;https://www.reddit.com/user/spotter&quot;&gt;Reddit user spotter&lt;/a&gt; for pointing me to a related 2008 blog post of Jeff Atwood's, &lt;a href=&quot;https://blog.codinghorror.com/the-first-rule-of-programming-its-always-your-fault/&quot;&gt;“The First Rule of Programming: It's Always Your Fault”&lt;/a&gt;. ]&lt;/p&gt;
&lt;p&gt;[ Addendum 20171113: Yes, yes, I know &lt;code&gt;sort()&lt;/code&gt; is in the library, not in the compiler. I am using “compiler error” as a &lt;a href=&quot;https://en.wikipedia.org/wiki/Synecdoche&quot;&gt;synecdoche&lt;/a&gt; for “system software error”. ]&lt;/p&gt;
&lt;p align=&quot;right&quot;&gt;&lt;em&gt;[&lt;a href=&quot;https://blog.plover.com/prog&quot;&gt;Other articles in category /prog&lt;/a&gt;] &lt;a href=&quot;https://blog.plover.com/prog/compiler-error.html&quot;&gt;permanent link&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;br /&gt;</description>
<pubDate>Tue, 14 Nov 2017 22:10:51 +0000</pubDate>
<dc:creator>zeveb</dc:creator>
<og:title>The Universe of Discourse : No, it is not a compiler error. It is never a compiler error.</og:title>
<og:type>article</og:type>
<og:image>https://pic.blog.plover.com/prog/compiler-error/WTF.jpg</og:image>
<og:url>https://blog.plover.com/aliens/dd/p20.html</og:url>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.plover.com/2017/11/12/</dc:identifier>
</item>
<item>
<title>The best laptop ever made</title>
<link>https://marco.org/2017/11/14/best-laptop-ever</link>
<guid isPermaLink="true" >https://marco.org/2017/11/14/best-laptop-ever</guid>
<description>&lt;header readability=&quot;0.59340659340659&quot;&gt;
&lt;p&gt;&lt;time datetime=&quot;2017-11-14T16:38:50-05:00&quot; pubdate=&quot;pubdate&quot;&gt;November 14, 2017&lt;/time&gt;&lt;a class=&quot;permalink&quot; title=&quot;Permalink&quot; href=&quot;https://marco.org/2017/11/14/best-laptop-ever&quot;&gt;&lt;span class=&quot;noprint&quot;&gt;∞&lt;/span&gt;&lt;span class=&quot;printonly&quot;&gt;https://marco.org/2017/11/14/best-laptop-ever&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/header&gt;&lt;p&gt;Apple has made many great laptops, but the 15-inch Retina MacBook Pro (2012–2015) is the epitome of usefulness, elegance, practicality, and power for an overall package that still hasn’t been (and may never be) surpassed.&lt;/p&gt;&lt;p&gt;Introduced in 2012, less than a year after Steve Jobs died, I see it as the peak of Jobs’ vision for the Mac.&lt;/p&gt;
&lt;p&gt;It was the debut of high-DPI Macs, starting down the long road (which we still haven’t finished) to an all-Retina lineup. And with all-SSD storage, quad-core i7 processors, and a healthy amount of RAM all standard, &lt;em&gt;every&lt;/em&gt; configuration was fast, capable, and pleasant to use.&lt;/p&gt;
&lt;p&gt;At its introduction, it was criticized only for ditching the optical drive and Ethernet port, but these were defensible, well-timed removals: neither could’ve even come close to physically fitting in the new design, very few MacBook Pro users were still using either on a regular basis, and almost none of us needed to buy external optical drives or Ethernet adapters to fit the new laptop into our lives. In exchange for those removals, we got substantial reductions in thickness and weight, and a huge new battery.&lt;/p&gt;
&lt;p&gt;There were no other downsides. Everything else about this machine was an upgrade: thinner, lighter, faster, better battery life, quieter fans, better speakers, better microphones, a second Thunderbolt port, and a convenient new HDMI port.&lt;/p&gt;

&lt;p&gt;The MagSafe 2 power adapter breaks away safely if it’s tripped over, and the LED on the connector quickly, clearly, and silently indicates whether it’s charging and when the battery is fully charged.&lt;/p&gt;
&lt;p&gt;The pair of Thunderbolt (later Thunderbolt 2) ports gave us high-end, high-speed connectivity when we needed it, and the pair of standard USB 3 ports — one on each side — let us connect or charge our world of standard USB devices.&lt;/p&gt;
&lt;p&gt;The headphone jack was thoughtfully located on the left side, because nearly all headphones run their cables down from the left earcup. (External-mouse users also appreciate this frequently-used cable not intruding in their right-side mousing area.)&lt;/p&gt;

&lt;p&gt;The keyboard was completely unremarkable, in the best possible way. The crowd-pleasing design was neither fanatically loved nor widely despised. It quietly and reliably did its job, as all great tools should, and nobody ever really had to think about it.&lt;/p&gt;
&lt;p&gt;The trackpad struck a great balance between size and usability. It provided ample room for multitouch gestures, but without being too large or close to the keyboard, so people’s fingers wouldn’t inadvertently brush against it while typing.&lt;/p&gt;
&lt;p&gt;Not every owner needed the SD-card slot or HDMI port, but both were provided for times when we might. This greatly increased the versatility and convenience of this MacBook Pro, as many pro customers use A/V gear that records to SD cards or occasionally need to plug into a TV or projector. The SD-card slot could also serve as inexpensive storage expansion.&lt;/p&gt;

&lt;p&gt;The power adapter’s built-in cable management keeps bags tidy. And if you need a longer cable, the extension comes in the box at no additional charge.&lt;/p&gt;
&lt;p&gt;Versatile USB-A ports allow travelers to standardize on just &lt;em&gt;one&lt;/em&gt; type of charging cable that can charge their iPhones and iPads from the laptop itself, multi-port wall or car chargers, portable batteries, airplanes, many outlets, and nearly all other chargers likely to be found in the world around them.&lt;/p&gt;
&lt;p&gt;The 2015 revision brought the modern Force Touch trackpad and used the space savings to increase the battery to 99.5 Wh, &lt;em&gt;just&lt;/em&gt; under the 100 Wh carry-on limit for most commercial airlines. When paired with the integrated-only GPU base configuration, this offered an unparalleled option for great battery life without giving up the large Retina screen.&lt;/p&gt;

&lt;p&gt;And I like the backlit Apple logo on the lid. Maybe I’m old-fashioned, or maybe I just miss Steve, but it — along with the MagSafe LED and the startup chime — reminds me of a time when Mac designs celebrated personality, humanity, and whimsy.&lt;/p&gt;
&lt;p&gt;*    *    *&lt;/p&gt;
&lt;p&gt;I recently returned to the 2015 15-inch MacBook Pro after a year away.&lt;/p&gt;
&lt;p&gt;Apple still sells this model, brand new, just limited to the integrated-only GPU option (which I prefer as a non-gamer for its battery, heat, and longevity advantages), but I got mine lightly used for over $1000 less.&lt;/p&gt;
&lt;p&gt;I thought it would feel like a downgrade, or like going back in time. I feared that it would feel thick, heavy, and cumbersome. I expected it to just look impossibly &lt;em&gt;old&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;It didn’t.&lt;/p&gt;
&lt;p&gt;It feels as delightful as when I first got one in 2012. It’s fast, capable, and reliable. It gracefully does what I need it to do. It’s &lt;em&gt;barely&lt;/em&gt; heavier or thicker, and I got to remove so many accessories from my travel bag that I think I’m actually coming out ahead.&lt;/p&gt;
&lt;p&gt;It feels like a professional tool, made by people who love and need computers, at the top of their game.&lt;/p&gt;
&lt;p&gt;It’s designed &lt;em&gt;for&lt;/em&gt; us, rather than asking us to adapt ourselves to it.&lt;/p&gt;
&lt;p&gt;It helps us perform our work, rather than adding to our workload.&lt;/p&gt;
&lt;p&gt;This is the peak. This is the best laptop that has ever existed.&lt;/p&gt;
&lt;p&gt;I hope it’s not the best laptop that &lt;em&gt;will&lt;/em&gt; ever exist.&lt;/p&gt;
</description>
<pubDate>Tue, 14 Nov 2017 21:39:53 +0000</pubDate>
<dc:creator>hodgesmr</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://marco.org/2017/11/14/best-laptop-ever</dc:identifier>
</item>
<item>
<title>Developer preview of TensorFlow Lite</title>
<link>https://developers.googleblog.com/2017/11/announcing-tensorflow-lite.html</link>
<guid isPermaLink="true" >https://developers.googleblog.com/2017/11/announcing-tensorflow-lite.html</guid>
<description></description>
<pubDate>Tue, 14 Nov 2017 21:02:50 +0000</pubDate>
<dc:creator>runesoerensen</dc:creator>
<og:type>article</og:type>
<og:title> Announcing TensorFlow Lite</og:title>
<og:image>http://4.bp.blogspot.com/-fWEPzhrhdqk/WKbYSrN1_8I/AAAAAAAATXE/m2-ZgsirkMYKc_YhcehEb0iUdyGtLs8KACK4B/s1600/lockup_google_developers_horizontal_knockout_wht.png</og:image>
<og:url>https://developers.googleblog.com/2017/11/announcing-tensorflow-lite.html</og:url>
<og:description>News and insights on Google platforms, tools, and events.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://developers.googleblog.com/2017/11/announcing-tensorflow-lite.html</dc:identifier>
</item>
<item>
<title>Augmented reality with Python and OpenCV</title>
<link>https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/</link>
<guid isPermaLink="true" >https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/</guid>
<description>&lt;p class=&quot;m_-6224572379386709677p3&quot;&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;You may (or may not) have heard of or seen the augmented reality Invizimals video game or the &lt;a href=&quot;http://www.nytimes.com/2009/03/09/technology/09topps.html?mcubz=1&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Topps 3D&lt;/a&gt; baseball cards. The main idea is to render in the screen of a tablet, PC or smartphone a 3D model of a specific figure on top of a card according to the position and orientation of the card.&lt;span class=&quot;m_-6224572379386709677Apple-converted-space&quot;&gt; &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div data-shortcode=&quot;caption&quot; id=&quot;attachment_801&quot; class=&quot;wp-caption alignnone&quot;&gt;&lt;img data-attachment-id=&quot;801&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/img_1139/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/img_1139.png?w=625&quot; data-orig-size=&quot;1136,640&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;IMG_1139&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/img_1139.png?w=625?w=300&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/img_1139.png?w=625?w=625&quot; class=&quot;alignnone size-full wp-image-801&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/img_1139.png?w=625&quot; alt=&quot;IMG_1139&quot; srcset=&quot;https://bitesofcode.files.wordpress.com/2017/09/img_1139.png?w=625 625w, https://bitesofcode.files.wordpress.com/2017/09/img_1139.png?w=150 150w, https://bitesofcode.files.wordpress.com/2017/09/img_1139.png?w=300 300w, https://bitesofcode.files.wordpress.com/2017/09/img_1139.png?w=768 768w, https://bitesofcode.files.wordpress.com/2017/09/img_1139.png?w=1024 1024w, https://bitesofcode.files.wordpress.com/2017/09/img_1139.png 1136w&quot; sizes=&quot;(max-width: 625px) 100vw, 625px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;Figure 1: Invizimal augmented reality cards. Source: &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p class=&quot;m_-6224572379386709677p3&quot;&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;Well, this past semester I took a course in Computer Vision where we studied some aspects of projective geometry and thought it would be an entertaining project to develop my own implementation of a card based augmented reality application. I warn you that we will need a bit of algebra to make it work but I’ll try to keep it as light as possible. To make the most out of it you should be comfortable working with different coordinate systems and transformation matrices.&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;m_-6224572379386709677p3&quot;&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;&lt;strong&gt;&amp;lt;disclaimer&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;m_-6224572379386709677p3&quot;&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;First, this post does not pretend to be a tutorial, a comprehensive guide or an explanation of the Computer Vision techniques involved and I will just mention the essentials required to follow the post. However, I encourage you to dig deeper in the concepts that will appear along the way.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Secondly, do not expect some professional looking results. I did this just for fun and there are plenty of decisions I made that could have been done better. The main idea is to develop a proof of concept application.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;/disclaimer&amp;gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;m_-6224572379386709677p3&quot;&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;With that said, here it goes my take on it.&lt;/span&gt;&lt;/p&gt;
&lt;h2 class=&quot;m_-6224572379386709677p5&quot;&gt;&lt;span&gt;&lt;span class=&quot;m_-6224572379386709677s4&quot;&gt;Where do we start?&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;p class=&quot;m_-6224572379386709677p3&quot;&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;Looking at the project as a whole may make it seem more difficult than it really is. Luckily for us, we will be able to divide it into smaller parts that, when combined one on top of another, will allow us to have our augmented reality application working. The question now is, which are these smaller chunks that we need?&lt;span class=&quot;m_-6224572379386709677Apple-converted-space&quot;&gt; &lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;m_-6224572379386709677p3&quot;&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;Let’s take a closer look into what we want to achieve. As stated before, we want to project in a screen a 3D model of a figure whose position and orientation matches the position and orientation of some predefined flat surface. Furthermore, we want to do it in real time, so that if the surface changes its position or orientation the projected model does so accordingly.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;m_-6224572379386709677p3&quot;&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;To achieve this we first have to be able to identify the flat surface of reference in an image or video frame. Once identified, we can easily determine the transformation from the reference surface image (2D) to the target image (2D). This transformation is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Homography_(computer_vision)&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;homography&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;. However, if what we want is to project a 3D model placed on top of the reference surface to the target image we need to extend the previous transformation to handle cases were the height of the point to project in the reference surface coordinate system is different than zero. This can be achieved with a bit of algebra.&lt;/span&gt; &lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;Finally, we should apply this transformation to our 3D model and draw it on the screen. Bearing the previous points in mind our project can be divided into:&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;m_-6224572379386709677p3&quot;&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;&lt;span class=&quot;m_-6224572379386709677Apple-converted-space&quot;&gt;          &lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;m_-6224572379386709677s5&quot;&gt;1.&lt;span class=&quot;m_-6224572379386709677Apple-converted-space&quot;&gt;  &lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;Recognize the reference flat surface.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;m_-6224572379386709677p3&quot;&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;&lt;span class=&quot;m_-6224572379386709677Apple-converted-space&quot;&gt;         &lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;m_-6224572379386709677s5&quot;&gt;2.&lt;span class=&quot;m_-6224572379386709677Apple-converted-space&quot;&gt; &lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;Estimate the homography.&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;m_-6224572379386709677p3&quot;&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;&lt;span class=&quot;m_-6224572379386709677Apple-converted-space&quot;&gt;          &lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;m_-6224572379386709677s5&quot;&gt;3.&lt;span class=&quot;m_-6224572379386709677Apple-converted-space&quot;&gt;  &lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;Derive from the homography the transformation from the reference surface coordinate system to the target image coordinate system.&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;m_-6224572379386709677p3&quot;&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;&lt;span class=&quot;m_-6224572379386709677Apple-converted-space&quot;&gt;          &lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;m_-6224572379386709677s5&quot;&gt;4.&lt;span class=&quot;m_-6224572379386709677Apple-converted-space&quot;&gt;  &lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;Project our 3D model in the image (pixel space) and draw it.&lt;/span&gt;&lt;/p&gt;
&lt;div data-shortcode=&quot;caption&quot; id=&quot;attachment_1500&quot; class=&quot;wp-caption aligncenter&quot;&gt;&lt;img data-attachment-id=&quot;1500&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/ar-page-11/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/ar-page-112.png?w=625&quot; data-orig-size=&quot;1490,1717&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;AR – Page 1(1)&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/ar-page-112.png?w=625?w=260&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/ar-page-112.png?w=625?w=625&quot; class=&quot; size-full wp-image-1500 aligncenter&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/ar-page-112.png?w=625&quot; alt=&quot;AR - Page 1(1)&quot; srcset=&quot;https://bitesofcode.files.wordpress.com/2017/09/ar-page-112.png?w=625 625w, https://bitesofcode.files.wordpress.com/2017/09/ar-page-112.png?w=1250 1250w, https://bitesofcode.files.wordpress.com/2017/09/ar-page-112.png?w=130 130w, https://bitesofcode.files.wordpress.com/2017/09/ar-page-112.png?w=260 260w, https://bitesofcode.files.wordpress.com/2017/09/ar-page-112.png?w=768 768w, https://bitesofcode.files.wordpress.com/2017/09/ar-page-112.png?w=889 889w&quot; sizes=&quot;(max-width: 625px) 100vw, 625px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 2: Overview of the whole process that brings to life our augmented reality application.&lt;/p&gt;
&lt;/div&gt;
&lt;p class=&quot;m_-6224572379386709677p3&quot;&gt;&lt;span class=&quot;m_-6224572379386709677s2&quot;&gt;The main tools we will use are Python and OpenCV because they are both open source, easy to set up and use and it is fast to build prototypes with them. For the needed algebra bit I will be using numpy.&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h2 class=&quot;m_-6224572379386709677p5&quot;&gt;&lt;span&gt;&lt;span class=&quot;m_-6224572379386709677s4&quot;&gt;Recognizing the target surface&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;From the many possible techniques that exist to perform object recognition I decided to tackle the problem with a feature based recognition method. This kind of methods, without going into much detail, consist in three main steps: feature detection or extraction, feature description and feature matching.&lt;/p&gt;
&lt;h3&gt;&lt;span&gt;Feature extraction&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Roughly speaking, this step consists in first looking in both the reference and target images for features that stand out and, in some way, describe part the object to be recognized. This features can be later used to find the reference object in the target image. We will assume we have found the object when a certain number of positive feature matches are found between the target and reference images. For this to work it is important to have a reference image where the only thing seen is the object (or surface, in this case) to be found.  We don’t want to detect features that are not part of the surface. And, although we will deal with this later, we will use the dimensions of the reference image when estimating the pose of the surface in a scene.&lt;/p&gt;
&lt;p&gt;For a region or point of an image to be labeled as feature it should fulfill two important properties: first of all, it should present some uniqueness at least locally. Good examples of this could be corners or edges. Secondly, since we don’t know beforehand which will be, for example, the orientation, scale or brightness conditions of this same object in the image where we want to recognize it a feature should, ideally, be invariant to transformations; i.e, invariant against scale, rotation or brightness changes. As a rule of thumb, the more invariant the better.&lt;/p&gt;
&lt;div data-shortcode=&quot;caption&quot; id=&quot;attachment_989&quot; class=&quot;wp-caption aligncenter&quot;&gt;&lt;img data-attachment-id=&quot;989&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/blank-diagram-page-1/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/blank-diagram-page-1.png?w=625&quot; data-orig-size=&quot;3029,1305&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;Blank Diagram – Page 1&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/blank-diagram-page-1.png?w=625?w=300&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/blank-diagram-page-1.png?w=625?w=625&quot; class=&quot;size-full wp-image-989 aligncenter&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/blank-diagram-page-1.png?w=625&quot; alt=&quot;Blank Diagram - Page 1&quot; srcset=&quot;https://bitesofcode.files.wordpress.com/2017/09/blank-diagram-page-1.png?w=625 625w, https://bitesofcode.files.wordpress.com/2017/09/blank-diagram-page-1.png?w=1250 1250w, https://bitesofcode.files.wordpress.com/2017/09/blank-diagram-page-1.png?w=150 150w, https://bitesofcode.files.wordpress.com/2017/09/blank-diagram-page-1.png?w=300 300w, https://bitesofcode.files.wordpress.com/2017/09/blank-diagram-page-1.png?w=768 768w, https://bitesofcode.files.wordpress.com/2017/09/blank-diagram-page-1.png?w=1024 1024w&quot; sizes=&quot;(max-width: 625px) 100vw, 625px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 3: On the left, features extracted from the model of the surface I will be using. On the right, features extracted from a sample scene. Note how corners have been detected as interest points in the rightmost image.&lt;/p&gt;
&lt;/div&gt;
&lt;h3&gt;&lt;span&gt;Feature description&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Once features have been found we should find a suitable representation of the information they provide. This will allow us to look for them in other images and also to obtain a measure of how similar two detected features are when being compared. This is were descriptors roll in.  A descriptor provides a representation of the information given by a feature and its surroundings. Once the descriptors have been computed the object to be recognized can then be abstracted to a &lt;a href=&quot;https://en.wikipedia.org/wiki/Feature_vector&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;feature vector&lt;/a&gt;,  which is a vector that contains the descriptors of the keypoints found in the image with the reference object.&lt;/p&gt;
&lt;p&gt;This is for sure a nice idea, but how can it actually be done? There are many algorithms that extract image features and compute its descriptors and, since I won’t go into much more detail (a whole post could be devoted only to this) if you are interested in knowing more take a look at &lt;a href=&quot;http://aishack.in/tutorials/sift-scale-invariant-feature-transform-introduction/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SIFT&lt;/a&gt;, &lt;a href=&quot;http://www.vision.ee.ethz.ch/~surf/eccv06.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SURF&lt;/a&gt;, or &lt;a href=&quot;http://aishack.in/tutorials/harris-corner-detector/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Harris&lt;/a&gt;. The one we will be using was developed at the OpenCV Lab and it is called &lt;a href=&quot;http://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_orb/py_orb.html&quot;&gt;ORB&lt;/a&gt; (Oriented FAST and Rotated BRIEF). The shape and values of the descriptor depend on the algorithm used and, in our case,  the descriptors obtained will be binary strings.&lt;/p&gt;
&lt;p&gt;With OpenCV, extracting features and its descriptors via the ORB detector is as easy as:&lt;/p&gt;

&lt;div&gt;
&lt;pre&gt;
img &lt;span&gt;=&lt;/span&gt; cv2&lt;span&gt;.&lt;/span&gt;imread(&lt;span&gt;'scene.jpg'&lt;/span&gt;,&lt;span&gt;0&lt;/span&gt;)

&lt;span&gt;# Initiate ORB detector&lt;/span&gt;
orb &lt;span&gt;=&lt;/span&gt; cv2&lt;span&gt;.&lt;/span&gt;ORB_create()

&lt;span&gt;# find the keypoints with ORB&lt;/span&gt;
kp &lt;span&gt;=&lt;/span&gt; orb&lt;span&gt;.&lt;/span&gt;detect(img, &lt;span&gt;None&lt;/span&gt;)

&lt;span&gt;# compute the descriptors with ORB&lt;/span&gt;
kp, des &lt;span&gt;=&lt;/span&gt; orb&lt;span&gt;.&lt;/span&gt;compute(img, kp)

&lt;span&gt;# draw only keypoints location,not size and orientation&lt;/span&gt;
img2 &lt;span&gt;=&lt;/span&gt; cv2&lt;span&gt;.&lt;/span&gt;drawKeypoints(img, kp, img, color&lt;span&gt;=&lt;/span&gt;(&lt;span&gt;0&lt;/span&gt;,&lt;span&gt;255&lt;/span&gt;,&lt;span&gt;0&lt;/span&gt;), flags&lt;span&gt;=&lt;/span&gt;&lt;span&gt;0&lt;/span&gt;)
cv2&lt;span&gt;.&lt;/span&gt;imshow(&lt;span&gt;'keypoints'&lt;/span&gt;,img2)
cv2&lt;span&gt;.&lt;/span&gt;waitKey(&lt;span&gt;0&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;&lt;span&gt;Feature matching&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Once we have found the features of both the object and the scene were the object is to be found and computed its descriptors it is time to look for matches between them. The simplest way of doing this is to take the descriptor of each feature in the first set, compute the distance to all the descriptors in the second set and return the closest one as the best match (I should state here that it is important to choose a way of measuring distances suitable with the descriptors being used. Since our descriptors will be binary strings we will use &lt;a href=&quot;https://en.wikipedia.org/wiki/Hamming_distance&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hamming distance&lt;/a&gt;). This is a brute force approach, and more sophisticated methods exist.&lt;/p&gt;
&lt;p&gt;For example, and this is what we will be also using, we could check that the match found as explained before is also the best match when computing matches the other way around, from features in the second set to features in the first set. This means that both features match each other. Once the matching has finished in both directions we will take as valid matches only the ones that fulfilled the previous condition. Figure 4 presents the best 15 matches found using this method.&lt;/p&gt;
&lt;p&gt;Another option to reduce the number of false positives would be to check if the distance to the second to best match is below a certain threshold.  If it is, then the match is considered valid.&lt;/p&gt;
&lt;div data-shortcode=&quot;caption&quot; id=&quot;attachment_1097&quot; class=&quot;wp-caption alignnone&quot;&gt;&lt;img data-attachment-id=&quot;1097&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/matches_2/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/matches_2.png?w=625&quot; data-orig-size=&quot;970,416&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;matches_2&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/matches_2.png?w=625?w=300&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/matches_2.png?w=625?w=625&quot; class=&quot;alignnone size-full wp-image-1097&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/matches_2.png?w=625&quot; alt=&quot;matches_2&quot; srcset=&quot;https://bitesofcode.files.wordpress.com/2017/09/matches_2.png?w=625 625w, https://bitesofcode.files.wordpress.com/2017/09/matches_2.png?w=150 150w, https://bitesofcode.files.wordpress.com/2017/09/matches_2.png?w=300 300w, https://bitesofcode.files.wordpress.com/2017/09/matches_2.png?w=768 768w, https://bitesofcode.files.wordpress.com/2017/09/matches_2.png 970w&quot; sizes=&quot;(max-width: 625px) 100vw, 625px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 4: Closest 15 brute force matches found between the reference surface and the scene&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Finally, after matches have been found, we should define some criteria to decide if the object has been found or not. For this I defined a threshold on the minimum number of matches that should be found. If the number of matches is above the threshold, then we assume the object has been found. Otherwise we consider that there isn’t enough evidence to say that the recognition was successful.&lt;/p&gt;
&lt;p&gt;With OpenCV all this recognition process can be done in a few lines of code:&lt;/p&gt;

&lt;div&gt;
&lt;pre&gt;
MIN_MATCHES &lt;span&gt;=&lt;/span&gt; &lt;span&gt;15&lt;/span&gt;
cap &lt;span&gt;=&lt;/span&gt; cv2&lt;span&gt;.&lt;/span&gt;imread(&lt;span&gt;'scene.jpg'&lt;/span&gt;, &lt;span&gt;0&lt;/span&gt;)    
model &lt;span&gt;=&lt;/span&gt; cv2&lt;span&gt;.&lt;/span&gt;imread(&lt;span&gt;'model.jpg'&lt;/span&gt;, &lt;span&gt;0&lt;/span&gt;)
&lt;span&gt;# ORB keypoint detector&lt;/span&gt;
orb &lt;span&gt;=&lt;/span&gt; cv2&lt;span&gt;.&lt;/span&gt;ORB_create()              
&lt;span&gt;# create brute force  matcher object&lt;/span&gt;
bf &lt;span&gt;=&lt;/span&gt; cv2&lt;span&gt;.&lt;/span&gt;BFMatcher(cv2&lt;span&gt;.&lt;/span&gt;NORM_HAMMING, crossCheck&lt;span&gt;=&lt;/span&gt;&lt;span&gt;True&lt;/span&gt;)  
&lt;span&gt;# Compute model keypoints and its descriptors&lt;/span&gt;
kp_model, des_model &lt;span&gt;=&lt;/span&gt; orb&lt;span&gt;.&lt;/span&gt;detectAndCompute(model, &lt;span&gt;None&lt;/span&gt;)  
&lt;span&gt;# Compute scene keypoints and its descriptors&lt;/span&gt;
kp_frame, des_frame &lt;span&gt;=&lt;/span&gt; orb&lt;span&gt;.&lt;/span&gt;detectAndCompute(cap, &lt;span&gt;None&lt;/span&gt;)
&lt;span&gt;# Match frame descriptors with model descriptors&lt;/span&gt;
matches &lt;span&gt;=&lt;/span&gt; bf&lt;span&gt;.&lt;/span&gt;match(des_model, des_frame)
&lt;span&gt;# Sort them in the order of their distance&lt;/span&gt;
matches &lt;span&gt;=&lt;/span&gt; &lt;span&gt;sorted&lt;/span&gt;(matches, key&lt;span&gt;=&lt;/span&gt;&lt;span&gt;lambda&lt;/span&gt; x: x&lt;span&gt;.&lt;/span&gt;distance)

&lt;span&gt;if&lt;/span&gt; &lt;span&gt;len&lt;/span&gt;(matches) &lt;span&gt;&amp;gt;&lt;/span&gt; MIN_MATCHES:
    &lt;span&gt;# draw first 15 matches.&lt;/span&gt;
    cap &lt;span&gt;=&lt;/span&gt; cv2&lt;span&gt;.&lt;/span&gt;drawMatches(model, kp_model, cap, kp_frame,
                          matches[:MIN_MATCHES], &lt;span&gt;0&lt;/span&gt;, flags&lt;span&gt;=&lt;/span&gt;&lt;span&gt;2&lt;/span&gt;)
    &lt;span&gt;# show result&lt;/span&gt;
    cv2&lt;span&gt;.&lt;/span&gt;imshow(&lt;span&gt;'frame'&lt;/span&gt;, cap)
    cv2&lt;span&gt;.&lt;/span&gt;waitKey(&lt;span&gt;0&lt;/span&gt;)
&lt;span&gt;else&lt;/span&gt;:
    &lt;span&gt;print&lt;/span&gt; &lt;span&gt;&quot;Not enough matches have been found - &lt;/span&gt;&lt;span&gt;%d&lt;/span&gt;&lt;span&gt;/&lt;/span&gt;&lt;span&gt;%d&lt;/span&gt;&lt;span&gt;&quot;&lt;/span&gt; &lt;span&gt;%&lt;/span&gt; (&lt;span&gt;len&lt;/span&gt;(matches),
                                                          MIN_MATCHES)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;On a final note and before stepping into the next step of the process I must point out that, since we want a real time application, it would have been better to implement a tracking technique and not just plain recognition. This is due to the fact that object recognition will be performed in each frame independently without taking into account previous frames that could add valuable information about the location of the reference object. Another thing to take into account is that, the easier to found the reference surface the more robust detection will be. In this particular sense, the reference surface I’m using might not be the best option, but it helps to understand the process.&lt;/p&gt;
&lt;h2&gt;&lt;span&gt;Homography estimation&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Once we have identified the reference surface in the current frame and have a set of valid matches we can proceed to estimate the homography between both images. As explained before, we want to find the transformation that maps points from the surface plane to the image plane (see Figure 5). This transformation will have to be updated each new frame we process.&lt;/p&gt;
&lt;div data-shortcode=&quot;caption&quot; id=&quot;attachment_1317&quot; class=&quot;wp-caption aligncenter&quot;&gt;&lt;img data-attachment-id=&quot;1317&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/homography/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/homography.png?w=503&amp;amp;h=197&quot; data-orig-size=&quot;529,207&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;homography&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/homography.png?w=503&amp;amp;h=197?w=300&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/homography.png?w=503&amp;amp;h=197?w=529&quot; class=&quot; wp-image-1317 aligncenter&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/homography.png?w=503&amp;amp;h=197&quot; alt=&quot;homography&quot; width=&quot;503&quot; height=&quot;197&quot; srcset=&quot;https://bitesofcode.files.wordpress.com/2017/09/homography.png?w=503&amp;amp;h=197 503w, https://bitesofcode.files.wordpress.com/2017/09/homography.png?w=150&amp;amp;h=59 150w, https://bitesofcode.files.wordpress.com/2017/09/homography.png?w=300&amp;amp;h=117 300w, https://bitesofcode.files.wordpress.com/2017/09/homography.png 529w&quot; sizes=&quot;(max-width: 503px) 100vw, 503px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 5: Homography between a plane and an image. Source: &lt;a href=&quot;http://www.iri.upc.edu/people/fmoreno/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;F. Moreno.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;How can we find such a transformation? Since we have already found a set of matches between both images we can certainly find directly by any of the existing methods (I advance we will be using RANSAC) an &lt;a href=&quot;https://en.wikipedia.org/wiki/Transformation_matrix&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;homogeneous transformation&lt;/a&gt; that performs the mapping, but let’s get some insight into what we are doing here (see Figure 6). You can skip the following part (and continue reading after Figure 10) if desired, since I will only explain the reasoning behind the transformation we are going to estimate.&lt;/p&gt;
&lt;p&gt;What we have is an object (a plane in this case) with known coordinates in the, let’s say, World coordinate system and we take a picture of it with a camera located at a certain position and orientation with respect to the World coordinate system. We will assume the camera works following the &lt;a href=&quot;https://en.wikipedia.org/wiki/Pinhole_camera_model&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pinhole model&lt;/a&gt;, which roughly means that the rays passing through a 3D point &lt;strong&gt;p&lt;/strong&gt; and the corresponding 2D point &lt;strong&gt;u&lt;/strong&gt; intersect at &lt;strong&gt;c&lt;/strong&gt;, the camera center. A good resource if you are interested in knowing more about the pinhole model can be found &lt;a href=&quot;http://alumni.media.mit.edu/~maov/classes/comp_photo_vision08f/lect/09_image_formation.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div data-shortcode=&quot;caption&quot; id=&quot;attachment_1346&quot; class=&quot;wp-caption aligncenter&quot;&gt;&lt;img data-attachment-id=&quot;1346&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/selection_009/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_009.png?w=540&amp;amp;h=232&quot; data-orig-size=&quot;765,329&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;Selection_009&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_009.png?w=540&amp;amp;h=232?w=300&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_009.png?w=540&amp;amp;h=232?w=625&quot; class=&quot; wp-image-1346 aligncenter&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_009.png?w=540&amp;amp;h=232&quot; alt=&quot;Selection_009&quot; width=&quot;540&quot; height=&quot;232&quot; srcset=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_009.png?w=540&amp;amp;h=232 540w, https://bitesofcode.files.wordpress.com/2017/09/selection_009.png?w=150&amp;amp;h=65 150w, https://bitesofcode.files.wordpress.com/2017/09/selection_009.png?w=300&amp;amp;h=129 300w, https://bitesofcode.files.wordpress.com/2017/09/selection_009.png 765w&quot; sizes=&quot;(max-width: 540px) 100vw, 540px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 6: Image formation assuming a camera pinhole model.  Source: F. Moreno.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Although not entirely true, the pinhole model assumption eases our calculations and works well enough for our purposes. The &lt;strong&gt;u, v&lt;/strong&gt; coordinates (coordinates in the image plane) of a point &lt;strong&gt;p&lt;/strong&gt; expressed in the Camera coordinate system if we assume a pinhole camera can be computed as (the derivation of the equation is left as an exercise to the reader):&lt;/p&gt;
&lt;div data-shortcode=&quot;caption&quot; id=&quot;attachment_1393&quot; class=&quot;wp-caption aligncenter&quot;&gt;&lt;img data-attachment-id=&quot;1393&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/selection_011/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_011.png?w=526&amp;amp;h=106&quot; data-orig-size=&quot;814,164&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;Selection_011&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_011.png?w=526&amp;amp;h=106?w=300&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_011.png?w=526&amp;amp;h=106?w=625&quot; class=&quot; wp-image-1393 aligncenter&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_011.png?w=526&amp;amp;h=106&quot; alt=&quot;Selection_011&quot; width=&quot;526&quot; height=&quot;106&quot; srcset=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_011.png?w=526&amp;amp;h=106 526w, https://bitesofcode.files.wordpress.com/2017/09/selection_011.png?w=150&amp;amp;h=30 150w, https://bitesofcode.files.wordpress.com/2017/09/selection_011.png?w=300&amp;amp;h=60 300w, https://bitesofcode.files.wordpress.com/2017/09/selection_011.png?w=768&amp;amp;h=155 768w, https://bitesofcode.files.wordpress.com/2017/09/selection_011.png 814w&quot; sizes=&quot;(max-width: 526px) 100vw, 526px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 7: Image formation assuming a pinhole camera model. Source: F. Moreno.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Where the focal length is the distance from the pinhole to the image plane, the projection of the optical center is the position of the optical center in the image plane and &lt;strong&gt;k&lt;/strong&gt; is a scaling factor. The previous equation then tells us how the image is formed. However, as stated before, we know the coordinates of the point &lt;strong&gt;p&lt;/strong&gt; in the World coordinate system and not in the Camera coordinate system, so we have to add another transformation that maps points from the World coordinate system to the Camera coordinate system. The transformation that tells us the coordinates in the image plane of a point &lt;strong&gt;p&lt;/strong&gt; in the World coordinate system is then:&lt;/p&gt;
&lt;div data-shortcode=&quot;caption&quot; id=&quot;attachment_1421&quot; class=&quot;wp-caption aligncenter&quot;&gt;&lt;img data-attachment-id=&quot;1421&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/selection_012/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_012.png?w=458&amp;amp;h=252&quot; data-orig-size=&quot;771,424&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;Selection_012&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_012.png?w=458&amp;amp;h=252?w=300&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_012.png?w=458&amp;amp;h=252?w=625&quot; class=&quot; wp-image-1421 aligncenter&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_012.png?w=458&amp;amp;h=252&quot; alt=&quot;Selection_012&quot; width=&quot;458&quot; height=&quot;252&quot; srcset=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_012.png?w=458&amp;amp;h=252 458w, https://bitesofcode.files.wordpress.com/2017/09/selection_012.png?w=150&amp;amp;h=82 150w, https://bitesofcode.files.wordpress.com/2017/09/selection_012.png?w=300&amp;amp;h=165 300w, https://bitesofcode.files.wordpress.com/2017/09/selection_012.png?w=768&amp;amp;h=422 768w, https://bitesofcode.files.wordpress.com/2017/09/selection_012.png 771w&quot; sizes=&quot;(max-width: 458px) 100vw, 458px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 8: Computation of the projection matrix. Source: F. Moreno.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Luckily for us, since the points in the reference surface plane do always have its &lt;strong&gt;z&lt;/strong&gt; coordinate equal to 0 (see Figure 5) we can simplify the transformation that we found above. It can be easily seen that the product of the &lt;strong&gt;z&lt;/strong&gt; coordinate and the third column of the projection matrix will always be 0 so we can drop this column and the &lt;strong&gt;z&lt;/strong&gt; coordinate from the previous equation. By renaming the calibration matrix as &lt;strong&gt;A&lt;/strong&gt; and taking into account that the external calibration matrix is an homogeneous transformation:&lt;/p&gt;
&lt;div data-shortcode=&quot;caption&quot; id=&quot;attachment_1320&quot; class=&quot;wp-caption aligncenter&quot;&gt;&lt;img data-attachment-id=&quot;1320&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/selection_003/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_003.png?w=483&amp;amp;h=203&quot; data-orig-size=&quot;595,250&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;Selection_003&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_003.png?w=483&amp;amp;h=203?w=300&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_003.png?w=483&amp;amp;h=203?w=595&quot; class=&quot; wp-image-1320 aligncenter&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_003.png?w=483&amp;amp;h=203&quot; alt=&quot;Selection_003&quot; width=&quot;483&quot; height=&quot;203&quot; srcset=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_003.png?w=483&amp;amp;h=203 483w, https://bitesofcode.files.wordpress.com/2017/09/selection_003.png?w=150&amp;amp;h=63 150w, https://bitesofcode.files.wordpress.com/2017/09/selection_003.png?w=300&amp;amp;h=126 300w, https://bitesofcode.files.wordpress.com/2017/09/selection_003.png 595w&quot; sizes=&quot;(max-width: 483px) 100vw, 483px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 9: Simplification of the projection matrix. Source: F. Moreno.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From Figure 9 we can conclude that the homography between the reference surface and the image plane, which is the matrix we will estimate from the previous matches we found is:&lt;/p&gt;
&lt;div data-shortcode=&quot;caption&quot; id=&quot;attachment_1318&quot; class=&quot;wp-caption aligncenter&quot;&gt;&lt;img data-attachment-id=&quot;1318&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/selection_001/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_001.png?w=314&amp;amp;h=117&quot; data-orig-size=&quot;323,120&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;Selection_001&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_001.png?w=314&amp;amp;h=117?w=300&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_001.png?w=314&amp;amp;h=117?w=323&quot; class=&quot; wp-image-1318 aligncenter&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_001.png?w=314&amp;amp;h=117&quot; alt=&quot;Selection_001&quot; width=&quot;314&quot; height=&quot;117&quot; srcset=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_001.png?w=314&amp;amp;h=117 314w, https://bitesofcode.files.wordpress.com/2017/09/selection_001.png?w=150&amp;amp;h=56 150w, https://bitesofcode.files.wordpress.com/2017/09/selection_001.png?w=300&amp;amp;h=111 300w, https://bitesofcode.files.wordpress.com/2017/09/selection_001.png 323w&quot; sizes=&quot;(max-width: 314px) 100vw, 314px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 10: Homography between the reference surface plane and the target image plane. Source: F. Moreno.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;There are several methods that allow us to estimate the values of the homography matrix, and you maight be familiar with some of them. The one we will be using is &lt;strong&gt;RAN&lt;/strong&gt;dom &lt;strong&gt;SA&lt;/strong&gt;mple &lt;strong&gt;C&lt;/strong&gt;onsensus (&lt;a href=&quot;https://en.wikipedia.org/wiki/Random_sample_consensus&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;RANSAC&lt;/a&gt;).  RANSAC is an iterative algorithm used for model fitting in the presence of a large number of outliers, and Figure 12 ilustrates the main outline of the process. Since we cannot guarantee that all the matches we have found are actually valid matches we have to consider that there might be some false matches (which will be our outliers) and, hence, we have to use an estimation method that is robust against outliers. Figure 11 illustrates the problems we could have when estimating the homography if we considered that there were no outliers.&lt;/p&gt;
&lt;div data-shortcode=&quot;caption&quot; id=&quot;attachment_1501&quot; class=&quot;wp-caption aligncenter&quot;&gt;&lt;img data-attachment-id=&quot;1501&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/selection_013/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_013.png?w=625&quot; data-orig-size=&quot;854,512&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;Selection_013&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_013.png?w=625?w=300&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_013.png?w=625?w=625&quot; class=&quot; size-full wp-image-1501 aligncenter&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_013.png?w=625&quot; alt=&quot;Selection_013&quot; srcset=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_013.png?w=625 625w, https://bitesofcode.files.wordpress.com/2017/09/selection_013.png?w=150 150w, https://bitesofcode.files.wordpress.com/2017/09/selection_013.png?w=300 300w, https://bitesofcode.files.wordpress.com/2017/09/selection_013.png?w=768 768w, https://bitesofcode.files.wordpress.com/2017/09/selection_013.png 854w&quot; sizes=&quot;(max-width: 625px) 100vw, 625px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 11: Homography estimation in the presence of outliers. Source: F. Moreno.&lt;/p&gt;
&lt;/div&gt;
&lt;div data-shortcode=&quot;caption&quot; id=&quot;attachment_1502&quot; class=&quot;wp-caption aligncenter&quot;&gt;&lt;img data-attachment-id=&quot;1502&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/selection_014/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_014.png?w=471&amp;amp;h=165&quot; data-orig-size=&quot;797,279&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;Selection_014&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_014.png?w=471&amp;amp;h=165?w=300&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_014.png?w=471&amp;amp;h=165?w=625&quot; class=&quot; wp-image-1502 aligncenter&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_014.png?w=471&amp;amp;h=165&quot; alt=&quot;Selection_014&quot; width=&quot;471&quot; height=&quot;165&quot; srcset=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_014.png?w=471&amp;amp;h=165 471w, https://bitesofcode.files.wordpress.com/2017/09/selection_014.png?w=150&amp;amp;h=53 150w, https://bitesofcode.files.wordpress.com/2017/09/selection_014.png?w=300&amp;amp;h=105 300w, https://bitesofcode.files.wordpress.com/2017/09/selection_014.png?w=768&amp;amp;h=269 768w, https://bitesofcode.files.wordpress.com/2017/09/selection_014.png 797w&quot; sizes=&quot;(max-width: 471px) 100vw, 471px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 12: RANSAC algorithm outline. Source: F. Moreno.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As a demonstration of how RANSAC works and to make things clearer, assume we had the following set of points for which we wanted to fit a line using RANSAC:&lt;/p&gt;
&lt;div data-shortcode=&quot;caption&quot; id=&quot;attachment_1522&quot; class=&quot;wp-caption aligncenter&quot;&gt;&lt;img data-attachment-id=&quot;1522&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/selection_017/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_017.png?w=301&amp;amp;h=196&quot; data-orig-size=&quot;568,370&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;Selection_017&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_017.png?w=301&amp;amp;h=196?w=300&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_017.png?w=301&amp;amp;h=196?w=568&quot; class=&quot; wp-image-1522 aligncenter&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_017.png?w=301&amp;amp;h=196&quot; alt=&quot;Selection_017&quot; width=&quot;301&quot; height=&quot;196&quot; srcset=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_017.png?w=301&amp;amp;h=196 301w, https://bitesofcode.files.wordpress.com/2017/09/selection_017.png?w=150&amp;amp;h=98 150w, https://bitesofcode.files.wordpress.com/2017/09/selection_017.png 568w&quot; sizes=&quot;(max-width: 301px) 100vw, 301px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 13: Initial set of points. Source: F. Moreno&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From the general outline presented in Figure 12 we can derive the specific process to fit a line using RANSAC (Figure 14).&lt;/p&gt;
&lt;div data-shortcode=&quot;caption&quot; id=&quot;attachment_1503&quot; class=&quot;wp-caption alignnone&quot;&gt;&lt;img data-attachment-id=&quot;1503&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/selection_015/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_015.png?w=625&quot; data-orig-size=&quot;836,182&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;Selection_015&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_015.png?w=625?w=300&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_015.png?w=625?w=625&quot; class=&quot;alignnone size-full wp-image-1503&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_015.png?w=625&quot; alt=&quot;Selection_015&quot; srcset=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_015.png?w=625 625w, https://bitesofcode.files.wordpress.com/2017/09/selection_015.png?w=150 150w, https://bitesofcode.files.wordpress.com/2017/09/selection_015.png?w=300 300w, https://bitesofcode.files.wordpress.com/2017/09/selection_015.png?w=768 768w, https://bitesofcode.files.wordpress.com/2017/09/selection_015.png 836w&quot; sizes=&quot;(max-width: 625px) 100vw, 625px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 14: RANSAC algorithm to fit a line to a set of points. Source: F. Moreno.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A possible outcome of running the algorithm presented above can be seen in Figure 15. Note that the first 3 steps of the algorithm are only shown for the first iteration (indicated by the bottom right number), and from that on only the scoring step is shown.&lt;/p&gt;
&lt;div data-shortcode=&quot;caption&quot; id=&quot;attachment_1555&quot; class=&quot;wp-caption aligncenter&quot;&gt;&lt;img data-attachment-id=&quot;1555&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/ransac/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/ransac1.png?w=625&quot; data-orig-size=&quot;1627,1414&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;ransac&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/ransac1.png?w=625?w=300&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/ransac1.png?w=625?w=625&quot; class=&quot; size-full wp-image-1555 aligncenter&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/ransac1.png?w=625&quot; alt=&quot;ransac.png&quot; srcset=&quot;https://bitesofcode.files.wordpress.com/2017/09/ransac1.png?w=625 625w, https://bitesofcode.files.wordpress.com/2017/09/ransac1.png?w=1250 1250w, https://bitesofcode.files.wordpress.com/2017/09/ransac1.png?w=150 150w, https://bitesofcode.files.wordpress.com/2017/09/ransac1.png?w=300 300w, https://bitesofcode.files.wordpress.com/2017/09/ransac1.png?w=768 768w, https://bitesofcode.files.wordpress.com/2017/09/ransac1.png?w=1024 1024w&quot; sizes=&quot;(max-width: 625px) 100vw, 625px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 15: Using RANSAC to fit a line to a set of points. Source: F. Moreno.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now back to our use case, homography estimation. For homography estimation the algorithm is presented in Figure 16. Since it is mainly math, I won’t go into details on why 4 matches are needed or on how to estimate &lt;strong&gt;H&lt;/strong&gt;. However, if you want to know why and how it’s done, &lt;a href=&quot;http://www.uio.no/studier/emner/matnat/its/UNIK4690/v16/forelesninger/lecture_4_3-estimating-homographies-from-feature-correspondences.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;this&lt;/a&gt; is a good explanation of it.&lt;/p&gt;
&lt;div data-shortcode=&quot;caption&quot; id=&quot;attachment_1504&quot; class=&quot;wp-caption alignnone&quot;&gt;&lt;img data-attachment-id=&quot;1504&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/selection_016/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_016.png?w=625&quot; data-orig-size=&quot;765,179&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;Selection_016&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_016.png?w=625?w=300&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_016.png?w=625?w=625&quot; class=&quot;alignnone size-full wp-image-1504&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_016.png?w=625&quot; alt=&quot;Selection_016&quot; srcset=&quot;https://bitesofcode.files.wordpress.com/2017/09/selection_016.png?w=625 625w, https://bitesofcode.files.wordpress.com/2017/09/selection_016.png?w=150 150w, https://bitesofcode.files.wordpress.com/2017/09/selection_016.png?w=300 300w, https://bitesofcode.files.wordpress.com/2017/09/selection_016.png 765w&quot; sizes=&quot;(max-width: 625px) 100vw, 625px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 16: RANSAC for homography estimation. Source: F. Moreno.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Before seeing how OpenCV can handle this for us we should  discuss one final aspect of the algorithm, which is what does it mean that a match is consistent with &lt;strong&gt;H&lt;/strong&gt;. What this mainly means is that if after estimating an homography we project into the target image the matches that were not used to estimate it then the projected points from the reference surface should be close to its matches in the target image. How close they should be to be considered consistent is up to you.&lt;/p&gt;
&lt;p&gt;I know it has been tough to reach this point, but thankfully there is a reward. In OpenCV estimating the homography with RANSAC is as easy as:&lt;/p&gt;

&lt;div&gt;
&lt;pre&gt;
&lt;span&gt;# assuming matches stores the matches found and &lt;/span&gt;
&lt;span&gt;# returned by bf.match(des_model, des_frame)&lt;/span&gt;
&lt;span&gt;# differenciate between source points and destination points&lt;/span&gt;
src_pts &lt;span&gt;=&lt;/span&gt; np&lt;span&gt;.&lt;/span&gt;float32([kp_model[m&lt;span&gt;.&lt;/span&gt;queryIdx]&lt;span&gt;.&lt;/span&gt;pt &lt;span&gt;for&lt;/span&gt; m &lt;span&gt;in&lt;/span&gt; matches])&lt;span&gt;.&lt;/span&gt;reshape(&lt;span&gt;-&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;, &lt;span&gt;1&lt;/span&gt;, &lt;span&gt;2&lt;/span&gt;)
dst_pts &lt;span&gt;=&lt;/span&gt; np&lt;span&gt;.&lt;/span&gt;float32([kp_frame[m&lt;span&gt;.&lt;/span&gt;trainIdx]&lt;span&gt;.&lt;/span&gt;pt &lt;span&gt;for&lt;/span&gt; m &lt;span&gt;in&lt;/span&gt; matches])&lt;span&gt;.&lt;/span&gt;reshape(&lt;span&gt;-&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;, &lt;span&gt;1&lt;/span&gt;, &lt;span&gt;2&lt;/span&gt;)
&lt;span&gt;# compute Homography&lt;/span&gt;
M, mask &lt;span&gt;=&lt;/span&gt; cv2&lt;span&gt;.&lt;/span&gt;findHomography(src_pts, dst_pts, cv2&lt;span&gt;.&lt;/span&gt;RANSAC, &lt;span&gt;5.0&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Where 5.0 is the threshold distance to determine if a match is consistent with the estimated homography. If after estimating the homography we project the four corners of the reference surface on the target image and connect them with a line we should expect the resulting lines to enclose the reference surface in the target image. We can do this by:&lt;/p&gt;
&lt;div&gt;
&lt;pre&gt;
&lt;span&gt;# Draw a rectangle that marks the found model in the frame&lt;/span&gt;
h, w &lt;span&gt;=&lt;/span&gt; model&lt;span&gt;.&lt;/span&gt;shape
pts &lt;span&gt;=&lt;/span&gt; np&lt;span&gt;.&lt;/span&gt;float32([[&lt;span&gt;0&lt;/span&gt;, &lt;span&gt;0&lt;/span&gt;], [&lt;span&gt;0&lt;/span&gt;, h &lt;span&gt;-&lt;/span&gt; &lt;span&gt;1&lt;/span&gt;], [w &lt;span&gt;-&lt;/span&gt; &lt;span&gt;1&lt;/span&gt;, h &lt;span&gt;-&lt;/span&gt; &lt;span&gt;1&lt;/span&gt;], [w &lt;span&gt;-&lt;/span&gt; &lt;span&gt;1&lt;/span&gt;, &lt;span&gt;0&lt;/span&gt;]])&lt;span&gt;.&lt;/span&gt;reshape(&lt;span&gt;-&lt;/span&gt;&lt;span&gt;1&lt;/span&gt;, &lt;span&gt;1&lt;/span&gt;, &lt;span&gt;2&lt;/span&gt;)
&lt;span&gt;# project corners into frame&lt;/span&gt;
dst &lt;span&gt;=&lt;/span&gt; cv2&lt;span&gt;.&lt;/span&gt;perspectiveTransform(pts, M)  
&lt;span&gt;# connect them with lines&lt;/span&gt;
img2 &lt;span&gt;=&lt;/span&gt; cv2&lt;span&gt;.&lt;/span&gt;polylines(img_rgb, [np&lt;span&gt;.&lt;/span&gt;int32(dst)], &lt;span&gt;True&lt;/span&gt;, &lt;span&gt;255&lt;/span&gt;, &lt;span&gt;3&lt;/span&gt;, cv2&lt;span&gt;.&lt;/span&gt;LINE_AA) 
cv2&lt;span&gt;.&lt;/span&gt;imshow(&lt;span&gt;'frame'&lt;/span&gt;, cap)
cv2&lt;span&gt;.&lt;/span&gt;waitKey(&lt;span&gt;0&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;which results in:&lt;/p&gt;
&lt;div data-shortcode=&quot;caption&quot; id=&quot;attachment_1632&quot; class=&quot;wp-caption aligncenter&quot;&gt;&lt;img data-attachment-id=&quot;1632&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/homography-2/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/homography1.png?w=625&quot; data-orig-size=&quot;600,338&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;homography&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/homography1.png?w=625?w=300&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/homography1.png?w=625?w=600&quot; class=&quot; size-full wp-image-1632 aligncenter&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/homography1.png?w=625&quot; alt=&quot;homography&quot; srcset=&quot;https://bitesofcode.files.wordpress.com/2017/09/homography1.png 600w, https://bitesofcode.files.wordpress.com/2017/09/homography1.png?w=150 150w, https://bitesofcode.files.wordpress.com/2017/09/homography1.png?w=300 300w&quot; sizes=&quot;(max-width: 600px) 100vw, 600px&quot;/&gt;&lt;p class=&quot;wp-caption-text&quot;&gt;Figure 17: Projected corners of the reference surface with the estimated homography.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;I think this is enough for today. On the next post we will see how to extend the homography we already estimated to project not only points in the reference surface plane but any 3D point from the reference surface coordinate system to the target image. We will then use this method to compute in real time, for each video frame, the specific projection matrix and then project in a video stream a 3D model of our choice from an .obj file. What you can expect at the end of the next post is something similar to what you can see in the gif below:&lt;/p&gt;
&lt;p&gt;&lt;img data-attachment-id=&quot;1657&quot; data-permalink=&quot;https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/1zd8b9/&quot; data-orig-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/1zd8b9.gif?w=494&amp;amp;h=277&quot; data-orig-size=&quot;360,202&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;1zd8b9&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/1zd8b9.gif?w=494&amp;amp;h=277?w=300&quot; data-large-file=&quot;https://bitesofcode.files.wordpress.com/2017/09/1zd8b9.gif?w=494&amp;amp;h=277?w=360&quot; class=&quot; wp-image-1657 aligncenter&quot; src=&quot;https://bitesofcode.files.wordpress.com/2017/09/1zd8b9.gif?w=494&amp;amp;h=277&quot; alt=&quot;1zd8b9&quot; width=&quot;494&quot; height=&quot;277&quot;/&gt;&lt;/p&gt;
&lt;p&gt;As always, I will upload the whole code of the project as well as some 3D models to GitHub for you to test when publishing part 2.&lt;/p&gt;
&lt;div class=&quot;wpcnt&quot;&gt;
&lt;div class=&quot;wpa wpmrec&quot;&gt;&lt;span class=&quot;wpa-about&quot;&gt;Advertisements&lt;/span&gt;



&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;jp-post-flair&quot; class=&quot;sharedaddy sd-like-enabled sd-sharing-enabled&quot;&gt;
&lt;div class=&quot;sharedaddy sd-sharing-enabled&quot;&gt;
&lt;div class=&quot;robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing&quot;&gt;
&lt;h3 class=&quot;sd-title&quot;&gt;Comparte esto:&lt;/h3&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded&quot; id=&quot;like-post-wrapper-121554408-798-5a0c63ffa1ef8&quot; data-src=&quot;//widgets.wp.com/likes/#blog_id=121554408&amp;amp;post_id=798&amp;amp;origin=bitesofcode.wordpress.com&amp;amp;obj_id=121554408-798-5a0c63ffa1ef8&quot; data-name=&quot;like-post-frame-121554408-798-5a0c63ffa1ef8&quot;&gt;
&lt;h3 class=&quot;sd-title&quot;&gt;Like this:&lt;/h3&gt;
&lt;div class=&quot;likes-widget-placeholder post-likes-widget-placeholder&quot;&gt;&lt;span class=&quot;button&quot;&gt;&lt;span&gt;Like&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;loading&quot;&gt;Loading...&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Tue, 14 Nov 2017 18:08:45 +0000</pubDate>
<dc:creator>galloafro</dc:creator>
<og:type>article</og:type>
<og:title>Augmented reality with Python and OpenCV (part 1)</og:title>
<og:url>https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/</og:url>
<og:description>You may (or may not) have heard of or seen the augmented reality Invizimals video game or the Topps 3D baseball cards. The main idea is to render in the screen of a tablet, PC or smartphone a 3D mo…</og:description>
<og:image>https://bitesofcode.files.wordpress.com/2017/09/1zd8b9.gif</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://bitesofcode.wordpress.com/2017/09/12/augmented-reality-with-python-and-opencv-part-1/</dc:identifier>
</item>
<item>
<title>California Could Legalize Magic Mushrooms in 2018</title>
<link>https://hightimes.com/news/california-could-legalize-magic-mushrooms-in-2018/</link>
<guid isPermaLink="true" >https://hightimes.com/news/california-could-legalize-magic-mushrooms-in-2018/</guid>
<description>&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;It is distinctly possible that California voters could be the first in the nation to decide whether psychedelic mushrooms should be made legal in a manner similar to alcohol and marijuana.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;Earlier last week, paperwork was filed with the state attorney general’s office asking to put the question of legal psilocybin (otherwise known as magic mushrooms) on the ballot in the 2018 election. The proposal, which was submitted by veteran cannabis advocate Kevin Saunders, would legalize the hallucinogen for adults 21 and over all across the Golden State.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;“This initiative exempts adults, 21 and over, from criminal penalties and decriminalizes adult use of psilocybin,” &lt;a href=&quot;https://oag.ca.gov/system/files/initiatives/pdfs/17-0024%20%28Legalize%20Psilocybin%29_0.pdf&quot;&gt;the initiative reads&lt;/a&gt;. The would-be law modification also “exempts adults, 21 and over, from California health and safety codes which otherwise prohibit possession, sale, transport and cultivation of psilocybin.”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;Saunders, who is also running a campaign for mayor in the town of Marina, which is located about 100 miles south of San Francisco, &lt;a href=&quot;http://www.sacbee.com/news/politics-government/capitol-alert/article169861452.html&quot;&gt;told&lt;/a&gt; the &lt;em&gt;Sacramento Bee&lt;/em&gt; that people should no longer be subjected to criminal records for a drug that is gaining ground in the mainstream.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;“What I want to do is take the shackles off. I want to have an adult conversation,” he &lt;a href=&quot;http://www.sacbee.com/news/politics-government/capitol-alert/article169861452.html&quot;&gt;told&lt;/a&gt; the news source. “Not only are the soccer moms high now, but some of them are taking mushrooms.”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;To the outside world, Saunders’ mission to legitimize a mind-altering substance that the U.S. government believes is one of the most dangerous drugs in the world might sound ambitiously insane, especially considering that the nation is now just starting to warm up to the concept of legal weed.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;But in actuality, Saunders is onto something.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;Not only have researchers proven that psilocybin is safer than other drugs, like alcohol, there are a couple of clinical trials taking place right now that could eventually lead to the substance winning approval from the U.S. Food and Drug Administration—an achievement that not even the cannabis plant has yet managed to secure.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;There is also some evidence that psilocybin is effective in the treatment of severe anxiety and addiction. German researchers recently concluded that, “these mushroom drugs may soon also be in use as pharmaceuticals that treat the existential anxiety of advanced-stage cancer patients, depression, and nicotine addiction,” &lt;a href=&quot;https://www.sciencedaily.com/releases/2017/08/170825103955.htm&quot;&gt;reported&lt;/a&gt; &lt;em&gt;Science Daily&lt;/em&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;Strangely, however, the issue of whether to eliminate the criminal penalties associated with the possession of magic mushrooms has not attracted the immediate support of popular drug reform groups.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;Tamar Todd, senior legal affairs director for the Drug Policy Alliance (DPA), recently &lt;a href=&quot;http://www.laweekly.com/news/measure-would-put-magic-mushroom-legalization-on-the-california-ballot-8591172&quot;&gt;told&lt;/a&gt; &lt;em&gt;L.A. Weekly&lt;/em&gt; that while the DPA “agrees that no one should be arrested or incarcerated simply because they possess or use psilocybin or other drugs… there are many factors to consider when deciding whether to run or support a ballot measure in California.”&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;The organization said it does not yet have an opinion on California’s psilocybin initiative.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;For now, the DPA says it is focused only on “the safe and just rollout of marijuana regulation and our work to reduce the number of people incarcerated for nonviolent drug offenses or deported for entering drug treatment post-arrest, and to reduce the number of people who die of drug overdose in California,” according to a statement provided by Todd.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;Nevertheless, there is some momentum building for legal psilocybin.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;A recent YouGov &lt;a href=&quot;https://today.yougov.com/news/2017/06/23/americans-ready-embrace-psychedelic-therapy/?belboon=031b3908984b04d39400589a,4711850,subid=38395X1559799Xcaf33fa74b8ccc14ef93c54d5f5ed7e7&amp;amp;pdl.rlid=203577&quot;&gt;survey&lt;/a&gt; found 63 percent of the population is in favor of the drug being used for psychedelic therapies.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;But the fight to legalize magic mushrooms in California is still an uphill battle. Organizers with the campaign would need to collect 356,880 valid signatures in order to be allowed on the 2018 ballot.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;As we have seen with marijuana legalization campaigns across the United States, the process of qualifying ballot measures is an expensive and time-consuming endeavor. So, the likelihood of the initiative going the distance without the help of influential organizations, like the DPA, or funding in the millions of dollars, is not good.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;But then again, this was once the case for marijuana.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;Now, over half the nation has a law on the books allowing the possession, cultivation and sale of the herb for medical and recreational purposes.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;Although Saunder’s shrooms initiative may be ahead of its time, we have no doubt the issue will eventually receive the appropriate consideration.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;whatsapp-but1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;&lt;em&gt;You can keep up with all of HIGH TIMES’ news &lt;/em&gt;&lt;a href=&quot;http://www.hightimes.com/news/&quot;&gt;&lt;span class=&quot;s2&quot;&gt;&lt;em&gt;right here&lt;/em&gt;&lt;/span&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Tue, 14 Nov 2017 17:59:19 +0000</pubDate>
<dc:creator>whatyoucantsay</dc:creator>
<og:type>article</og:type>
<og:image>https://hightimes.com/wp-content/uploads/2017/08/Magic-Mushrooms-Shrooms-Psilocybin.jpg</og:image>
<og:url>https://hightimes.com/news/california-could-legalize-magic-mushrooms-in-2018/</og:url>
<og:title>California Could Legalize Magic Mushrooms in 2018</og:title>
<og:description>It is distinctly possible that California voters could be the first in the nation to decide whether psychedelic mushrooms should be made legal in a manner similar to alcohol and marijuana. Earlier last week, paperwork was filed with the state attorney general’s office asking to put the question of legal psilocybin (otherwise known as magic […]</og:description>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://hightimes.com/news/california-could-legalize-magic-mushrooms-in-2018/</dc:identifier>
</item>
<item>
<title>How Discord Resizes 150M Images Every Day with Go and C++</title>
<link>https://blog.discordapp.com/how-discord-resizes-150-million-images-every-day-with-go-and-c-c9e98731c65d</link>
<guid isPermaLink="true" >https://blog.discordapp.com/how-discord-resizes-150-million-images-every-day-with-go-and-c-c9e98731c65d</guid>
<description>&lt;div class=&quot;section-inner sectionLayout--fullWidth&quot;&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*WNy93pNIoIiBsfViOZnK0Q.png&quot; data-width=&quot;2000&quot; data-height=&quot;800&quot; data-is-featured=&quot;true&quot; src=&quot;https://cdn-images-1.medium.com/max/2000/1*WNy93pNIoIiBsfViOZnK0Q.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;section-inner sectionLayout--insetColumn&quot; readability=&quot;26.78865248227&quot;&gt;
&lt;p name=&quot;67d8&quot; id=&quot;67d8&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Despite being a voice and text chat app, Discord sees over a hundred million images passing through its tubes every day. While we wish it was as simple as sending them out across the tubes to your friends, delivering these images creates some pretty large technical problems. Linking directly to images would leak users’ IP addresses to image hosts, and large images use up lots of bandwidth. To circumvent these problems, Discord needs an intermediary service to fetch images on behalf of users and then resize them to reduce bandwidth usage.&lt;/p&gt;
&lt;h3 name=&quot;9b6f&quot; id=&quot;9b6f&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--h3-strong&quot;&gt;Enter the Image Proxy&lt;/strong&gt;&lt;/h3&gt;
&lt;p name=&quot;7426&quot; id=&quot;7426&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;To handle this job, we created a Python service creatively named Image Proxy. It fetched images from remote URLs and then used the &lt;a href=&quot;https://github.com/uploadcare/pillow-simd&quot; data-href=&quot;https://github.com/uploadcare/pillow-simd&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;pillow-simd&lt;/a&gt; package to do the heavy lifting of image resizing. Pillow-simd is wonderfully fast and uses &lt;a href=&quot;https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions&quot; data-href=&quot;https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;x86 SSE instructions&lt;/a&gt; to accelerate resizing where it can. The Image Proxy would receive a HTTP request containing a URL to fetch, resize, and finally respond with the image.&lt;/p&gt;
&lt;p name=&quot;b79b&quot; id=&quot;b79b&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;On top of this, we setup a caching layer that would try to keep resized images around in memory and respond directly from cache when it could. A &lt;a href=&quot;http://www.haproxy.org/&quot; data-href=&quot;http://www.haproxy.org/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;HAProxy&lt;/a&gt; layer routes requests based on a URL hash to the &lt;a href=&quot;https://www.nginx.com/&quot; data-href=&quot;https://www.nginx.com/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Nginx&lt;/a&gt; caching layer. The cache performs &lt;a href=&quot;http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache_lock&quot; data-href=&quot;http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache_lock&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;request coalescing&lt;/a&gt; in order to minimize the number of resize transformations required. This combination of cache and proxy was enough to scale our image proxy up well into millions of users.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;section-inner sectionLayout--outsetColumn&quot;&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*pV0ZUbW1dURx-_YOWu1mzQ.png&quot; data-width=&quot;2224&quot; data-height=&quot;1668&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*pV0ZUbW1dURx-_YOWu1mzQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/2000/1*pV0ZUbW1dURx-_YOWu1mzQ.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;section-inner sectionLayout--insetColumn&quot; readability=&quot;50.120842572062&quot;&gt;
&lt;p name=&quot;7d01&quot; id=&quot;7d01&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Still, as Discord grew, the Image Proxy started to show signs of strain. The biggest problem was the Image Proxy did not have an even workload distribution, which hampered its throughput. Image proxying requests saw a wide variance of response times, with some taking multiple seconds to complete. We likely could have addressed this behavior in Image Proxy, but we had been experimenting with using more Go, and it seemed like a good place to try Go out.&lt;/p&gt;
&lt;h3 name=&quot;1019&quot; id=&quot;1019&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--h3-strong&quot;&gt;And Then Came Media Proxy&lt;/strong&gt;&lt;/h3&gt;
&lt;p name=&quot;9b4d&quot; id=&quot;9b4d&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;It’s not an &lt;a href=&quot;https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/&quot; data-href=&quot;https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;easy decision&lt;/a&gt; to rewrite a service that’s already working. Thankfully, Image Proxy was relatively simple, and comparing results between it and the replacement service would be straightforward. In addition to serving requests faster, the new service would also get new features, including the ability to get the first frame from .mp4 and .webm videos — hence, Media Proxy.&lt;/p&gt;
&lt;p name=&quot;8312&quot; id=&quot;8312&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;We began by &lt;a href=&quot;https://gist.github.com/b1naryth1ef/7a7c06d210ee4a56ed253c89925ed978&quot; data-href=&quot;https://gist.github.com/b1naryth1ef/7a7c06d210ee4a56ed253c89925ed978&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;benchmarking existing image resizing packages for Go&lt;/a&gt; and quickly discovered something disheartening. While Go is generally a faster language than Python, none of the resizing packages we could find could beat Pillow-simd consistently in performance. Most of the work done by the Image Proxy was in the image transcoding and resizing, so this would be a significant bottleneck in Media Proxy if it were slower. Go might be a little faster at handling HTTP, but if it can’t resize images quickly, the extra speedup would be lost to the extra time spent resizing.&lt;/p&gt;
&lt;p name=&quot;94b5&quot; id=&quot;94b5&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;We decided to &lt;a href=&quot;http://wiki.c2.com/?NotInventedHere&quot; data-href=&quot;http://wiki.c2.com/?NotInventedHere&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;double down&lt;/a&gt; and put together our own image resizing package for Go. We had seen some promise when we benchmarked one Go package that wrapped &lt;a href=&quot;https://opencv.org/&quot; data-href=&quot;https://opencv.org/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;OpenCV&lt;/a&gt;, but that package didn’t support all of the features we wanted. We created our own Go image resizer, named &lt;a href=&quot;https://github.com/discordapp/lilliput&quot; data-href=&quot;https://github.com/discordapp/lilliput&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Lilliput&lt;/a&gt;, which has its own &lt;a href=&quot;https://blog.golang.org/c-go-cgo&quot; data-href=&quot;https://blog.golang.org/c-go-cgo&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Cgo&lt;/a&gt; wrapper on top of OpenCV. Lilliput has been made with careful consideration toward not creating garbage in Go. Lilliput’s OpenCV wrapper does almost everything we want, though we still had to &lt;a href=&quot;https://github.com/opencv/opencv/pull/8511&quot; data-href=&quot;https://github.com/opencv/opencv/pull/8511&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;fork OpenCV&lt;/a&gt; slightly before we were happy with it. In particular, we wanted to be able to inspect image headers before deciding whether to start decompressing them, since this would allow us to immediately refuse to resize any that are too large.&lt;/p&gt;
&lt;p name=&quot;bb3c&quot; id=&quot;bb3c&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Lilliput uses the existing and mature C libraries for image compression and decompression (e.g. libjpeg-turbo for JPEG, libpng for PNG) and OpenCV’s fast vectorized resizing code. We added &lt;a href=&quot;https://github.com/valyala/fasthttp&quot; data-href=&quot;https://github.com/valyala/fasthttp&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;fasthttp&lt;/a&gt; to handle our concurrent HTTP client and server requirements. Finally with this combination we had a service which steadily beat Image Proxy in synthetic benchmarks. When comparing lilliput to pillow-simd, we found that lilliput &lt;a href=&quot;https://gist.github.com/brian-armstrong-discord/1eb10046edb91167b7187513cc306d65&quot; data-href=&quot;https://gist.github.com/brian-armstrong-discord/1eb10046edb91167b7187513cc306d65&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;performed as well as or better than&lt;/a&gt; pillow-simd in the use cases we care about.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;section-inner sectionLayout--outsetColumn&quot;&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*_8v1kGdIhkluS8Ube8QMbQ.png&quot; data-width=&quot;2224&quot; data-height=&quot;1668&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*_8v1kGdIhkluS8Ube8QMbQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/2000/1*_8v1kGdIhkluS8Ube8QMbQ.png&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;section-inner sectionLayout--insetColumn&quot; readability=&quot;105.3033919598&quot;&gt;
&lt;p name=&quot;bd8c&quot; id=&quot;bd8c&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The early code was not without issues. Initially, Media Proxy would leak 16 bytes on each request. This is a small enough loss that it takes quite a while to manifest, especially when testing at a small scale. Compounding the issue, Media Proxy keeps large static pixel buffers around for resizing purposes. It uses two of these buffers per CPU, so on a 32-core host its initial memory usage is several GB. It would take hours for Media Proxy to be restarted due to exhausting all of the system memory during testing. This was a long enough duration that it was hard to tell if we actually had a memory leak or if runtime usage was just putting us over the limit.&lt;/p&gt;
&lt;p name=&quot;e09e&quot; id=&quot;e09e&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Eventually, we concluded that there must indeed be some kind of memory leak. We weren’t sure whether the leak was present in Go or C++, and reviewing the code failed to turn up the source of the leak. Fortunately, Xcode ships with an outstanding memory profiler — the &lt;a href=&quot;https://developer.apple.com/library/content/documentation/DeveloperTools/Conceptual/InstrumentsUserGuide/FindingLeakedMemory.html&quot; data-href=&quot;https://developer.apple.com/library/content/documentation/DeveloperTools/Conceptual/InstrumentsUserGuide/FindingLeakedMemory.html&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Leaks tool in Instruments&lt;/a&gt;. This tool revealed the size of the leak and approximately where it was occurring. This was enough of a hint that further review allowed us to identify and fix the leak.&lt;/p&gt;
&lt;p name=&quot;d708&quot; id=&quot;d708&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;We encountered another showstopper bug in Media Proxy. Sometimes it would respond with strangely corrupted images where half of the image would be correct and the other half would appear “glitched”. We initially suspected that we might have been decoding partially retrieved images or somehow calling OpenCV incorrectly. This bug occurred infrequently and was hard to diagnose.&lt;/p&gt;
&lt;p name=&quot;ed2f&quot; id=&quot;ed2f&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In order to deal with this, we developed a high throughput request simulator that provided image URLs that linked to an HTTP server within the simulator, so that the simulator would act both as requesting client and hosting server. The simulator randomly delayed its responses in order to provoke this strange image corrupting behavior from Media Proxy. With a reliable reproduction, we were able to isolate components in Media Proxy until we discovered a race condition on the output buffer that contained the resized image. We had been writing one image to this buffer and then writing another to the same buffer before the first had finished being put back on the network. The glitched images we had seen were actually two JPEGs written on top of one another.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;0*6QtSaZicUVGE_vPL.&quot; data-width=&quot;400&quot; data-height=&quot;300&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/0*6QtSaZicUVGE_vPL.&quot;/&gt;&lt;/div&gt;
An actual glitched JPEG rendered by Media Proxy
&lt;p name=&quot;cac2&quot; id=&quot;cac2&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Another way to discover bugs in a complex system is fuzzing, which is a technique that generates random inputs and sends them into a system. This can cause the system to exhibit strange behavior or crash, and since our system needs to be resilient against all inputs, we decided to utilize this important technique while testing. &lt;a href=&quot;http://lcamtuf.coredump.cx/afl/&quot; data-href=&quot;http://lcamtuf.coredump.cx/afl/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;AFL&lt;/a&gt; is an &lt;a href=&quot;http://lcamtuf.coredump.cx/afl/#bugs&quot; data-href=&quot;http://lcamtuf.coredump.cx/afl/#bugs&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;exceptionally good fuzzer&lt;/a&gt;, so we picked it and ran it against Lilliput, which revealed several crashes due to uninitialized variables.&lt;/p&gt;
&lt;p name=&quot;f45a&quot; id=&quot;f45a&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;After fixing the above bugs, we were confident enough to ship Media Proxy to production and were happy to find that our work had paid off. Media Proxy needed &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;60% fewer server instances&lt;/strong&gt; to handle as many requests as Image Proxy while completing requests with much less variance in latency. Profiling shows that more than 90% of CPU time in this new service is spent performing image decompression, resizing, and compression. These libraries are already highly optimized, suggesting further gains would not be easily achievable. Additionally, the service creates almost no garbage at runtime.&lt;/p&gt;
&lt;p name=&quot;3af8&quot; id=&quot;3af8&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Today, Media Proxy operates with a median per-image resize of 25ms and a median total response latency of 85ms. It resizes more than 150 million images every day. Media Proxy runs on an autoscaled GCE group of &lt;a href=&quot;https://cloud.google.com/compute/docs/machine-types&quot; data-href=&quot;https://cloud.google.com/compute/docs/machine-types&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;n1-standard-16 host type&lt;/a&gt;, peaking at 12 instances on a typical day.&lt;/p&gt;
&lt;h3 name=&quot;5818&quot; id=&quot;5818&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--h3-strong&quot;&gt;Putting the Media in Media Proxy&lt;/strong&gt;&lt;/h3&gt;
&lt;p name=&quot;ef6c&quot; id=&quot;ef6c&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;After we had static images working, we wanted to support animated GIF resizing as well, which OpenCV would not handle for us. We decided to add another Cgo wrapper on top of &lt;a href=&quot;http://giflib.sourceforge.net/gif_lib.html&quot; data-href=&quot;http://giflib.sourceforge.net/gif_lib.html&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;giflib&lt;/a&gt; to Lilliput so that it could resize full GIFs, as well as output the first frame as PNG.&lt;/p&gt;
&lt;p name=&quot;0a15&quot; id=&quot;0a15&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Resizing GIFs turned out to be somewhat challenging as the &lt;a href=&quot;https://www.w3.org/Graphics/GIF/spec-gif89a.txt&quot; data-href=&quot;https://www.w3.org/Graphics/GIF/spec-gif89a.txt&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;GIF standard&lt;/a&gt; specifies per-frame palettes of 256 colors, but the resizer operates in &lt;a href=&quot;https://en.wikipedia.org/wiki/RGB_color_model&quot; data-href=&quot;https://en.wikipedia.org/wiki/RGB_color_model&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;RGB space&lt;/a&gt;. We decided to preserve each frame’s palette rather than attempting to recompute new palettes. In order to convert RGB back into palette indices, we gave Lilliput a simple lookup table that crushes some of the RGB bits and uses the result as a key into a palette index table. This performs well and preserves the original colors, though it does mean that Lilliput can only create a GIF from a source GIF.&lt;/p&gt;
&lt;p name=&quot;394f&quot; id=&quot;394f&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;We also &lt;a href=&quot;https://sourceforge.net/p/giflib/patches/28/&quot; data-href=&quot;https://sourceforge.net/p/giflib/patches/28/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;patched giflib&lt;/a&gt; so that it would be easier to decode just a single frame at a time. This allows us to decode one frame, resize it, and then encode and compress it before moving on to the next, reducing the memory footprint of the GIF resizer. This does add some complexity to Lilliput as it must preserve some GIF state from frame to frame, but having more predictable memory usage in Media Proxy seems like a clear advantage.&lt;/p&gt;
&lt;p name=&quot;3f1f&quot; id=&quot;3f1f&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Lilliput’s giflib wrapper fixed a number of issues we had previously seen in Image Proxy’s GIF resizing as giflib gave us full control of the image resizing process. A significant number of our &lt;a href=&quot;https://discordapp.com/nitro&quot; data-href=&quot;https://discordapp.com/nitro&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Nitro users&lt;/a&gt; had uploaded animated GIF avatars which would have glitches or transparency errors when resized by the Image Proxy but which worked perfectly through Media Proxy. In general, we found that image resizers had problems with some aspects of the GIF format and produced visual glitches for frames with transparency or partial frames. Creating our own wrapper allows us to address these issues as we encounter them.&lt;/p&gt;
&lt;p name=&quot;c55e&quot; id=&quot;c55e&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Finally, we gave Lilliput a Cgo wrapper on &lt;a href=&quot;https://ffmpeg.org/libavcodec.html&quot; data-href=&quot;https://ffmpeg.org/libavcodec.html&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;libavcodec&lt;/a&gt; so that it could freeze the first frame from MP4 and WEBM videos. This functionality will allow Media Proxy to give previews of user posted videos so that users can decide from the preview whether they want to play the video. Freezing the first frame of videos was one of the remaining blockers for us to add an in-client video player for videos in message attachments and links.&lt;/p&gt;
&lt;h3 name=&quot;866e&quot; id=&quot;866e&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--h3-strong&quot;&gt;More Open Source&lt;/strong&gt;&lt;/h3&gt;
&lt;p name=&quot;d04a&quot; id=&quot;d04a&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Now that we’re satisfied with Media Proxy, we’re releasing &lt;a href=&quot;https://github.com/discordapp/lilliput&quot; data-href=&quot;https://github.com/discordapp/lilliput&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Lilliput&lt;/a&gt; under MIT license. We hope that this package will be useful for anybody who needs a performant image resizing service, and that this post will help others build new Go packages.&lt;/p&gt;
&lt;p name=&quot;8dba&quot; id=&quot;8dba&quot; class=&quot;graf graf--p graf-after--p graf--trailing&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;We are hiring, so&lt;/em&gt; &lt;a href=&quot;https://discordapp.com/jobs&quot; data-href=&quot;https://discordapp.com/jobs&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;come join us&lt;/em&gt;&lt;/a&gt; &lt;em class=&quot;markup--em markup--p-em&quot;&gt;if this type of stuff tickles your fancy.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
<pubDate>Tue, 14 Nov 2017 17:03:41 +0000</pubDate>
<dc:creator>b1naryth1ef</dc:creator>
<og:title>How Discord Resizes 150 Million Images Every Day with Go and C++</og:title>
<og:url>https://blog.discordapp.com/how-discord-resizes-150-million-images-every-day-with-go-and-c-c9e98731c65d</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*WNy93pNIoIiBsfViOZnK0Q.png</og:image>
<og:description>Despite being a voice and text chat app, Discord sees over a hundred million images passing through its tubes every day. While we wish it…</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.discordapp.com/how-discord-resizes-150-million-images-every-day-with-go-and-c-c9e98731c65d?gi=cfa86f3f9ab1</dc:identifier>
</item>
</channel>
</rss>