<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Stanford CS007: Personal Finance For Engineers</title>
<link>https://cs007.blog/</link>
<guid isPermaLink="true" >https://cs007.blog/</guid>
<description>&lt;h2&gt;Session 1: Introduction&lt;/h2&gt;
&lt;p&gt;Blog Post: &lt;a href=&quot;https://adamnash.blog/2017/09/27/stanford-cs-007-personal-finance-for-engineers-kickoff/&quot;&gt;Kickoff&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This seminar focuses on an introduction to personal finance and the purpose of this course.&lt;/p&gt;

&lt;h2&gt;Session 2: Predictably Irrational&lt;/h2&gt;
&lt;p&gt;This seminar focuses on behavioral finance.&lt;/p&gt;

&lt;h2&gt;Session 3: Getting Paid&lt;/h2&gt;
&lt;p&gt;This seminar focuses on compensation &amp;amp; job offers.&lt;/p&gt;

&lt;h2&gt;Session 4: Spend Less Than You Make&lt;/h2&gt;
&lt;p&gt;This seminar focuses on income, expenses, saving &amp;amp; budgeting.&lt;/p&gt;

&lt;h2&gt;Session 5: Know Your Worth&lt;/h2&gt;
&lt;p&gt;This seminar focuses on liquidity, emergency funds, assets &amp;amp; liabilities, and net worth.&lt;/p&gt;

&lt;h2&gt;Session 6: All About Debt&lt;/h2&gt;
&lt;p&gt;This seminar focuses on compounding, debt, credit scores, amortization &amp;amp; payoff strategies.&lt;/p&gt;

&lt;h2&gt;Session 7:  Good Investing is Boring&lt;/h2&gt;
&lt;p&gt;This seminar focuses on compounding, types of investment, diversification, how to invest, and the four keys to good investing (keep saving, low fees, stay diversified, minimize taxes).&lt;/p&gt;

&lt;h2&gt;Session 8: Financial Planning &amp;amp; Goals&lt;/h2&gt;
&lt;p&gt;This seminar focuses on putting together all of the components covered in previous weeks to understand financial planning, financial goals, complexities when dealing with couples &amp;amp; life insurance.&lt;/p&gt;

&lt;h2&gt;Session 9: Real Estate&lt;/h2&gt;
&lt;p&gt;There will be no class the week of November 21st. The next seminar will be on Tuesday, November 28th at 4:30pm in Building 200, Room 034.&lt;/p&gt;
&lt;div class=&quot;wpcnt&quot;&gt;
&lt;div class=&quot;wpa wpmrec&quot;&gt;&lt;span class=&quot;wpa-about&quot;&gt;Advertisements&lt;/span&gt;



&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Mon, 27 Nov 2017 12:06:49 +0000</pubDate>
<dc:creator>destraynor</dc:creator>
<og:type>website</og:type>
<og:title>CS 007: Personal Finance for Engineers</og:title>
<og:description>Stanford University 2017-8</og:description>
<og:url>https://cs007.blog/</og:url>
<og:image>https://s0.wp.com/i/blank.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://cs007.blog/</dc:identifier>
</item>
<item>
<title>Time Inc. Sells Itself to Meredith Corp, owner of Family Circle and AllRecipes</title>
<link>https://www.nytimes.com/2017/11/26/business/dealbook/time-inc-meredith-corporation-koch-brothers.html</link>
<guid isPermaLink="true" >https://www.nytimes.com/2017/11/26/business/dealbook/time-inc-meredith-corporation-koch-brothers.html</guid>
<description>&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;298&quot; data-total-count=&quot;2063&quot; id=&quot;story-continues-3&quot;&gt;Meredith, based in Des Moines, is a Midwestern publisher through and through. Its founder, Edwin Thomas Meredith, entered the media business in 1902 with a magazine called Successful Farming. He soon began the still-thriving Better Homes and Gardens, which has a circulation of more than 7 million.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;343&quot; data-total-count=&quot;2406&quot;&gt;Its popular magazines have long focused on families and women, taking aim more at Middle America. It has eschewed an expensive headquarters in Manhattan and maintained a diversified portfolio — the company also owns local television stations — that has allowed Meredith to better weather the economic storm that has faced print publishers.&lt;/p&gt;
&lt;span class=&quot;visually-hidden&quot;&gt;Photo&lt;/span&gt;
&lt;div class=&quot;image&quot;&gt;&lt;img src=&quot;https://static01.nyt.com/images/2017/11/27/business/27time2/27time2-master675.jpg&quot; alt=&quot;&quot; class=&quot;media-viewer-candidate&quot; data-mediaviewer-src=&quot;https://static01.nyt.com/images/2017/11/27/business/27time2/27time2-superJumbo.jpg&quot; data-mediaviewer-caption=&quot;The billionaire brothers David H. Koch, left, and Charles G. Koch have backed Meredith&amp;#x2019;s bid.&quot; data-mediaviewer-credit=&quot;Paul Vernon/Associated Press, left, and Bo Rader, via The Wichita Eagle, via Associated Press&quot; itemprop=&quot;url&quot; itemid=&quot;https://static01.nyt.com/images/2017/11/27/business/27time2/27time2-master675.jpg&quot;/&gt;&lt;/div&gt;
&lt;span class=&quot;caption-text&quot;&gt;The billionaire brothers David H. Koch, left, and Charles G. Koch have backed Meredith’s bid.&lt;/span&gt; &lt;span class=&quot;credit&quot; itemprop=&quot;copyrightHolder&quot;&gt;&lt;span class=&quot;visually-hidden&quot;&gt;Credit&lt;/span&gt; Paul Vernon/Associated Press, left, and Bo Rader, via The Wichita Eagle, via Associated Press&lt;/span&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;309&quot; data-total-count=&quot;2715&quot;&gt;But as Meredith has stood relatively strong, Time Inc. has stumbled. The company failed to keep pace as the industrywide transformation from print to digital rendered old methods of magazine-making obsolete and publishing companies crumbled under the pressure of declines in print advertising and circulation.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;124&quot; data-total-count=&quot;2839&quot;&gt;For Meredith, a hardy company with a loyal print readership, the acquisition of Time Inc. represents a long-elusive victory.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;191&quot; data-total-count=&quot;3030&quot;&gt;“This is a transformative transaction for Meredith Corporation,” Tom Harty, Meredith’s president and chief operating officer, said in the company’s statement announcing the agreement.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;443&quot; data-total-count=&quot;3473&quot;&gt;Charles Koch, the chief executive of Koch Industries, and David Koch have long sought to shape political discourse through their support of nonprofit organizations, universities and think tanks. But in its announcement of the deal, Meredith said that the private equity fund, Koch Equity Development, would not have a seat on Meredith’s board of directors and would “have no influence on Meredith’s editorial or managerial operations.”&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;320&quot; data-total-count=&quot;3793&quot;&gt;Steve Lombardo, a spokesman for Koch Industries, also said that the Kochs had no plans to take an active role in the expanded company. “This is a passive financial investment made through our equity development arm,” Mr. Lombardo said. The company’s role in the transaction, he said, was similar to that of a bank.&lt;/p&gt;
&lt;div id=&quot;story-ad-2&quot; class=&quot;story-ad ad ad-placeholder nocontent robots-nocontent&quot;&gt;

&lt;a class=&quot;visually-hidden skip-to-text-link&quot; href=&quot;https://www.nytimes.com/2017/11/26/business/dealbook/time-inc-meredith-corporation-koch-brothers.html#story-continues-4&quot;&gt;Continue reading the main story&lt;/a&gt;&lt;/div&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;80&quot; data-total-count=&quot;3873&quot; id=&quot;story-continues-4&quot;&gt;Mr. Lombardo said the company is constantly evaluating investment opportunities.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;127&quot; data-total-count=&quot;4000&quot;&gt;“We’re looking at deals across all sectors, all industries,” he said. “This just happened to be one that made sense.”&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;415&quot; data-total-count=&quot;4415&quot;&gt;A deal between Meredith and Time Inc. fell apart in 2013 after Meredith reportedly said that it did not want to acquire some of Time Inc.’s best-known titles, including Time, Fortune and Sports Illustrated. Meredith also expressed interest in buying Time Inc. earlier this year before it walked away — in part because it could not secure sufficient financing. The Kochs helped the company overcome that problem.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;399&quot; data-total-count=&quot;4814&quot;&gt;Adding Time Inc.’s portfolio will give Meredith even more national scale, which will help it continue to appeal to advertisers on both the print and digital sides. But the company will also have to adjust to printing weekly titles, which it currently does not do. Meredith said it expected its deal for Time Inc. would result in $400 million to $500 million in cost savings in its first two years.&lt;br/&gt;&lt;/p&gt;
&lt;span class=&quot;visually-hidden&quot;&gt;Photo&lt;/span&gt;
&lt;div class=&quot;image&quot;&gt;&lt;img src=&quot;https://static01.nyt.com/images/2017/11/27/business/27time1/27time1-master675.jpg&quot; alt=&quot;&quot; class=&quot;media-viewer-candidate&quot; data-mediaviewer-src=&quot;https://static01.nyt.com/images/2017/11/27/business/27time1/27time1-superJumbo.jpg&quot; data-mediaviewer-caption=&quot;It was not clear how much influence, if any, the Koch brothers would wield over Time Inc., should the deal be completed.&quot; data-mediaviewer-credit=&quot;Richard Drew/Associated Press&quot; itemprop=&quot;url&quot; itemid=&quot;https://static01.nyt.com/images/2017/11/27/business/27time1/27time1-master675.jpg&quot;/&gt;&lt;/div&gt;
&lt;span class=&quot;caption-text&quot;&gt;It was not clear how much influence, if any, the Koch brothers would wield over Time Inc., should the deal be completed.&lt;/span&gt; &lt;span class=&quot;credit&quot; itemprop=&quot;copyrightHolder&quot;&gt;&lt;span class=&quot;visually-hidden&quot;&gt;Credit&lt;/span&gt; Richard Drew/Associated Press&lt;/span&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;232&quot; data-total-count=&quot;5046&quot;&gt;In a note to its staff members on Sunday night, Rich Battista, the chief executive of Time Inc., said he believed in “our strategic transformation plan and in our ability to write the next great chapter of this storied company.”&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;298&quot; data-total-count=&quot;5344&quot;&gt;“That said, as a publicly traded company, and one operating in such a dynamic industry as media, we know circumstances can change quickly,” he said. Meredith, Mr. Battista added, “presented us with an opportunity to combine companies to create even greater scale and financial flexibility.”&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;202&quot; data-total-count=&quot;5546&quot;&gt;Under the terms of the deal, Meredith will pay $18.50 a share for Time Inc. The boards of both companies finalized the deal on Sunday evening. The deal is expected to close in the first quarter of 2018.&lt;/p&gt;


&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;219&quot; data-total-count=&quot;5765&quot;&gt;The investment from the Kochs, Meredith said, “underscores a strong belief in Meredith’s strength as a business operator, its strategies and its ability to unlock significant value from the Time Inc. acquisition.”&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;519&quot; data-total-count=&quot;6284&quot;&gt;Some Koch allies have suggested that the brothers would view their investment purely as a moneymaking opportunity. But others familiar with the Kochs’ thinking speculated that they could nonetheless use the media properties — which reach millions of online and print readers — to promote their brand of conservatism. The investment would also give the Kochs a way to combine the arsenal of voter information held by a data analytics company controlled by their network, i360, with the publishers’ consumer data.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;331&quot; data-total-count=&quot;6615&quot;&gt;After Time Warner, the home of HBO and Warner Bros., spun off Time Inc. in 2014, the publisher was left to fend for itself in a world increasingly turning its back on print media. Bedeviled by relentless cost cuts and executive turnover, the company has struggled to articulate a business strategy less focused on the printed page.&lt;/p&gt;
&lt;div id=&quot;story-ad-3&quot; class=&quot;story-ad ad ad-placeholder nocontent robots-nocontent&quot;&gt;

&lt;a class=&quot;visually-hidden skip-to-text-link&quot; href=&quot;https://www.nytimes.com/2017/11/26/business/dealbook/time-inc-meredith-corporation-koch-brothers.html#story-continues-5&quot;&gt;Continue reading the main story&lt;/a&gt;&lt;/div&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;384&quot; data-total-count=&quot;6999&quot; id=&quot;story-continues-5&quot;&gt;Mr. Battista, who was named Time Inc.’s chief executive last year, and the new chief operating officer, Jen Wong, &lt;a href=&quot;https://www.nytimes.com/2017/03/05/business/media/time-inc-magazine-jen-wong-and-rich-battista.html&quot;&gt;embarked on an aggressive strategy&lt;/a&gt; to increase digital revenue, including enhancing advertising technology capabilities and offering customers paid services, such as insurance for pets and a food and wine club. The company had also earmarked $400 million in cost cuts.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;397&quot; data-total-count=&quot;7396&quot;&gt;Time Inc. executives had been adamant that their stand-alone strategy could position the company for a successful future. But in an industry that increasingly values size and breadth, Time Inc. was staring into ongoing uncertainty. For its most recent quarter, it reported a 9 percent drop in total revenue compared with the same period last year, and a 12 percent decrease in advertising revenue.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;118&quot; data-total-count=&quot;7514&quot;&gt;Mr. Battista is expected to stay on at Time Inc. through the close of the deal, after which he will leave the company.&lt;/p&gt;
&lt;a class=&quot;visually-hidden skip-to-text-link&quot; href=&quot;https://www.nytimes.com/2017/11/26/business/dealbook/time-inc-meredith-corporation-koch-brothers.html#whats-next&quot;&gt;Continue reading the main story&lt;/a&gt;</description>
<pubDate>Mon, 27 Nov 2017 03:11:45 +0000</pubDate>
<dc:creator>mudil</dc:creator>
<og:url>https://www.nytimes.com/2017/11/26/business/dealbook/time-inc-meredith-corporation-koch-brothers.html</og:url>
<og:type>article</og:type>
<og:title>Time Inc. Sells Itself to Meredith Corp., Backed by Koch Brothers</og:title>
<og:description>Meredith Corporation, the publisher of Family Circle and Better Homes and Gardens, clinches a deal in a cash transaction valued at nearly $3 billion.</og:description>
<og:image>https://static01.nyt.com/images/2017/11/27/business/27time-top/27time-top-facebookJumbo.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nytimes.com/2017/11/26/business/dealbook/time-inc-meredith-corporation-koch-brothers.html</dc:identifier>
</item>
<item>
<title>A Year in Computer Vision</title>
<link>http://www.themtank.org/a-year-in-computer-vision</link>
<guid isPermaLink="true" >http://www.themtank.org/a-year-in-computer-vision</guid>
<description>&lt;div readability=&quot;8&quot;&gt;
&lt;p class=&quot;c57&quot;&gt;&lt;span class=&quot;c7 c51&quot;&gt;A Year in Computer Vision:&lt;/span&gt; &lt;span class=&quot;c27 c7 c51&quot;&gt;The M Tank, 2017&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;



&lt;table class=&quot;c58&quot;&gt;&lt;tbody readability=&quot;1&quot;&gt;&lt;tr class=&quot;c48&quot; readability=&quot;2&quot;&gt;&lt;td class=&quot;c55&quot; colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;











&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c5&quot;&gt;Edited for The M Tank by&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c20 c12 c6&quot;&gt;Benjamin F. Duffy&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c5&quot;&gt;&amp;amp;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c20 c12 c6&quot;&gt;Daniel R. Flynn&lt;/span&gt;&lt;/p&gt;









&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c54 c12 c56&quot;&gt;The M Tank&lt;/span&gt;&lt;/p&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p class=&quot;c1 c9&quot;&gt;&lt;span class=&quot;c5&quot;&gt;Also on Medium: &lt;a href=&quot;https://medium.com/@info_84181/a-year-in-computer-vision-part-1-of-4-eaeb040b6f46&quot;&gt;Part 1&lt;/a&gt;, &lt;a href=&quot;https://medium.com/@info_84181/a-year-in-computer-vision-part-2-of-4-893e18e12be0&quot;&gt;Part 2&lt;/a&gt;, &lt;a href=&quot;https://medium.com/@info_84181/a-year-in-computer-vision-part-3-of-4-861216d71607&quot;&gt;Part 3&lt;/a&gt;, &lt;a href=&quot;https://medium.com/@info_84181/a-year-in-computer-vision-part-4-of-4-515c61d41a00&quot;&gt;Part 4&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Computer Vision typically refers to the scientific discipline of giving machines the ability of sight, or perhaps more colourfully, enabling machines to visually analyse their environments and the stimuli within them. This process typically involves the evaluation of an image, images or video.&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;The British Machine Vision Association (BMVA) defines Computer Vision as&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6 c26 c14&quot;&gt;the automatic extraction, analysis and&lt;/span&gt; &lt;span class=&quot;c7 c6 c26 c14 c22&quot;&gt;understanding&lt;/span&gt;&lt;span class=&quot;c7 c6 c26 c14&quot;&gt; of useful information from a single image or a sequence of images.&lt;/span&gt;&lt;span class=&quot;c6 c26 c14&quot;&gt;”&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt1&quot; id=&quot;ftnt_ref1&quot; name=&quot;ftnt_ref1&quot;&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6 c26 c14&quot;&gt;The term&lt;/span&gt; &lt;span class=&quot;c7 c6 c26 c14&quot;&gt;understanding&lt;/span&gt;&lt;span class=&quot;c29 c6 c26 c14&quot;&gt; provides an interesting counterpoint to an otherwise mechanical definition of vision, one which serves to demonstrate both the significance and complexity of the Computer Vision field. True understanding of our environment is not achieved through visual representations alone. Rather, visual cues travel through the optic nerve to the primary visual cortex and are interpreted by the brain, in a highly stylised sense. The interpretations drawn from this sensory information encompass the near-totality of our natural programming and subjective experiences, i.e. how evolution has wired us to survive and what we learn about the world throughout our lives.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6 c26 c14&quot;&gt;In this respect,&lt;/span&gt; &lt;span class=&quot;c7 c6 c26 c14&quot;&gt;vision&lt;/span&gt;&lt;span class=&quot;c6 c26 c14&quot;&gt; only relates to the transmission of images for interpretation; while&lt;/span&gt; &lt;span class=&quot;c7 c6 c26 c14&quot;&gt;computing&lt;/span&gt; &lt;span class=&quot;c6 c26 c14&quot;&gt;said images is more analogous to thought&lt;/span&gt; &lt;span class=&quot;c6 c14&quot;&gt;or cognition&lt;/span&gt;&lt;span class=&quot;c6 c26 c14&quot;&gt;, drawing on a multitude of the brain’s faculties. Hence, many believe that Computer Vision, a true understanding of visual environments and their contexts,&lt;/span&gt; &lt;span class=&quot;c6 c26 c14&quot;&gt;paves the way for future iterations of Strong Artificial Intelligence, due to its cross-domain mastery&lt;/span&gt;&lt;span class=&quot;c29 c6 c26 c14&quot;&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c29 c6 c26 c14&quot;&gt;However, put down the pitchforks as we’re still very much in the embryonic stages of this fascinating field. This piece simply aims to shed some light on 2016’s biggest Computer Vision advancements. And hopefully ground some of these advancements in a healthy mix of expected near-term societal-interactions and, where applicable, tongue-in-cheek prognostications of the end of life as we know it.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6 c26 c14&quot;&gt;While o&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;ur work is always written to be as accessible as possible, sections within this particular piece may be oblique at times due to the subject matter. We do provide rudimentary definitions throughout, however, these only convey a facile understanding of key concepts. In keeping our focus on work produced in 2016, often omissions are made in the interest of brevity.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;One such glaring omission relates to the functionality of Convolutional Neural Networks (hereafter CNNs or ConvNets), which are ubiquitous within the field of Computer Vision. The success of&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; AlexNet&lt;/span&gt; &lt;sup class=&quot;c6 c12&quot; id=&quot;#ftnt_ref2&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt2&quot; id=&quot;ftnt_ref2&quot; name=&quot;ftnt_ref2&quot;&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; in 201&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;2, a CNN architecture which blindsided ImageNet competitors, proved instigator of a de facto revolution within the field, with numerous researchers adopting neural network-based approaches as part of Computer Vision’s new period of ‘normal science’.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt3&quot; id=&quot;ftnt_ref3&quot; name=&quot;ftnt_ref3&quot;&gt;[3]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; &lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;Over four years later and CNN variants still make up the bulk of new neural network architectures for vision tasks, with researchers reconstructing them like legos; a working testament to the power of both open source information and Deep Learning. However, an explanation of CNNs could easily span several postings and is best left to those with a deeper expertise on the subject and an affinity for making the complex understandable.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;For casual readers who wish to gain a quick grounding before proceeding we recommend the first two resources below. For those who wish to go further still, we have ordered the resources below to facilitate that:&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_240e30rf1rxr-0 start&quot;&gt;&lt;li class=&quot;c2 c13 c40&quot;&gt;

&lt;/li&gt;
&lt;li class=&quot;c40 c2 c13&quot;&gt;

&lt;/li&gt;
&lt;li class=&quot;c40 c2 c13&quot;&gt;

&lt;/li&gt;
&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Deep Learning&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; (Goodfellow, Bengio &amp;amp; Courville, 2016) provides detailed explanations of CNN features and functionality in Chapter 9. The textbook has been kindly made available for free in HTML format by the authors.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt7&quot; id=&quot;ftnt_ref7&quot; name=&quot;ftnt_ref7&quot;&gt;[7]&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;For those wishing to understand more about Neural Networks and Deep Learning in general we suggest:&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_kskz43o1igm-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Neural Networks and Deep Learning&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;(Nielsen, 2017) is a free online textbook which provides the reader with a really intuitive understanding of the complexities of Neural Networks and Deep Learning. Even just completing&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;chapter one&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; should greatly illuminate the subject matter of this piece for first-timers.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt8&quot; id=&quot;ftnt_ref8&quot; name=&quot;ftnt_ref8&quot;&gt;[8]&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;As a whole this piece is disjointed and spasmodic, a reflection of the authors’ excitement and the spirit in which it was intended to be utilised,&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;ection by section. Information is partitioned using our own heuristics and judgements, a necessary compromise due to the cross-domain influence of much of the work presented.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;We hope that readers benefit from our aggregation of the information here to further their own knowledge, regardless of previous experience.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;From all our contributors,&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c9&quot;&gt;&lt;span class=&quot;c5&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;http://www.themtank.org/images/Signature-transparent.png&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;





&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;The M Tank&lt;/span&gt;&lt;/p&gt;


&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;The task of classification, when it relates to images,&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;generally&lt;/span&gt;&lt;span class=&quot;c6 c28&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;refers to assigning a label to the whole image, e.g. ‘cat’. Assuming this,&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;Localisation&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;may then refer to finding where the object is in said image,&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;usually&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; denoted by the output of some form of bounding box around the object. Current classification/localisation techniques on&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; ImageNet&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt9&quot; id=&quot;ftnt_ref9&quot; name=&quot;ftnt_ref9&quot;&gt;[9]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; have likely surpassed an ensemble of trained humans.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt10&quot; id=&quot;ftnt_ref10&quot; name=&quot;ftnt_ref10&quot;&gt;[10]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;For this reason, we place greater emphasis on subsequent sections of the blog.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c12 c6 c26 c14&quot;&gt;Figure 1&lt;/span&gt;&lt;span class=&quot;c6 c26 c14 c29&quot;&gt;: Computer Vision Tasks&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 image-margin&quot;&gt;&lt;span&gt;&lt;img class=&quot;border cntr&quot; alt=&quot;&quot; src=&quot;http://www.themtank.org/images/c-image12.jpg&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12 c14 c26&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3 c26 c14&quot;&gt;: Fei-Fei Li, Andrej Karpathy &amp;amp; Justin Johnson (2016) cs231n, Lecture 8 - Slide 8,&lt;/span&gt; &lt;span class=&quot;c7 c3 c26 c14&quot;&gt;Spatial Localization and Detection&lt;/span&gt;&lt;span class=&quot;c3 c26 c14&quot;&gt; (01/02/2016). Available:&lt;/span&gt; &lt;a class=&quot;a-word-wrap&quot; href=&quot;http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf&quot;&gt;http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf&lt;/a&gt; &lt;span class=&quot;c3 c26 c14&quot;&gt; &lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;However, the introduction of larger datasets with an increased number of classes&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt11&quot; id=&quot;ftnt_ref11&quot; name=&quot;ftnt_ref11&quot;&gt;[11]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; will likely provide new metrics for progress&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; in the near future&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;. On that point, François Chollet, the creator of&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;Keras&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;,&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt12&quot; id=&quot;ftnt_ref12&quot; name=&quot;ftnt_ref12&quot;&gt;[12]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; has applied new techniques, including the popular architecture Xception, to an internal google dataset with over 350 million multi-label images containing 17,000 classes.&lt;/span&gt; &lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt13&quot; id=&quot;ftnt_ref13&quot; name=&quot;ftnt_ref13&quot;&gt;[13]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6 c45&quot;&gt;,&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt14&quot; id=&quot;ftnt_ref14&quot; name=&quot;ftnt_ref14&quot;&gt;[14]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c12 c6 c26 c14&quot;&gt;Figure 2&lt;/span&gt;&lt;span class=&quot;c29 c6 c26 c14&quot;&gt;: Classification/Localisation results from ILSVRC (2010-2016)&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 image-margin&quot;&gt;&lt;span&gt;&lt;img class=&quot;border cntr&quot; alt=&quot;&quot; src=&quot;http://www.themtank.org/images/image19.png&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: ImageNet Large Scale Visual Recognition Challenge (ILSVRC). The change in results from 2011-2012 resulting from the AlexNet submission.&lt;/span&gt;&lt;span class=&quot;c3 c28&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;For a review of the challenge requirements relating to Classification and Localization see:&lt;/span&gt; &lt;span class=&quot;c21 c3&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;http://www.image-net.org/challenges/LSVRC/2016/index%23comp&quot;&gt;http://www.image-net.org/challenges/LSVRC/2016/index#comp&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c8 c3&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Jia Deng (2016).&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;ILSVRC2016 object localisation: introduction, results&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Slide 2. Available:&lt;/span&gt; &lt;span class=&quot;c21 c3&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;http://image-net.org/challenges/talks/2016/ILSVRC2016_10_09_clsloc.pdf&quot;&gt;http://image-net.org/challenges/talks/2016/ILSVRC2016_10_09_clsloc.pdf&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c8 c3&quot;&gt; &lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Interesting takeaways from the ImageNet LSVRC (2016):&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; &lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_jklfmxwzkhun-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Scene Classification&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; refers to the task of labelling an image with a certain scene class like ‘greenhouse’, ‘stadium’, ‘cathedral’, etc. ImageNet held a Scene Classification challenge last year with a subset of the Places2&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt15&quot; id=&quot;ftnt_ref15&quot; name=&quot;ftnt_ref15&quot;&gt;[15]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; dataset: 8 million images for training with 365 scene categories.&lt;br /&gt;Hikvision&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt16&quot; id=&quot;ftnt_ref16&quot; name=&quot;ftnt_ref16&quot;&gt;[16]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; won with a 9% top-5 error with an ensemble of deep Inception-style networks, and not-so-deep residuals networks.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_jklfmxwzkhun-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Trimps-Soushen&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; won the ImageNet Classi&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;fication task with 2.99% top-5 classification error and 7.71% localisation error. The team employed an ensemble for classification (averaging the results of Inception, Inception-Resnet, ResNet and Wide Residual Networks models&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt17&quot; id=&quot;ftnt_ref17&quot; name=&quot;ftnt_ref17&quot;&gt;[17]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt;) and Faster R-CNN for localisation based on the labels.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt18&quot; id=&quot;ftnt_ref18&quot; name=&quot;ftnt_ref18&quot;&gt;[18]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; The dataset was distributed across 1000 image classes with 1.2 million images provided as training data. The partitioned test data compiled a further 100 thousand unseen images.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_jklfmxwzkhun-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;ResNeXt&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; by Facebook came a close second in top-5 classification error with 3.03% by using a new architecture that extends th&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;e original ResNet architecture.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt19&quot; id=&quot;ftnt_ref19&quot; name=&quot;ftnt_ref19&quot;&gt;[19]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;As one can imagine the process of&lt;/span&gt; &lt;span class=&quot;c12 c6&quot;&gt;Object Detection&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; does exactly that, detects objects within images.&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;The definition provided for object detection by the ILSVRC 2016&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt20&quot; id=&quot;ftnt_ref20&quot; name=&quot;ftnt_ref20&quot;&gt;[20]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6 c14&quot;&gt; includes outputting bounding boxes and labels for individual objects.&lt;/span&gt;&lt;span class=&quot;c5 c14&quot;&gt; This differs from the classification/localisation task by applying classification and localisation to many objects instead of just a single dominant object.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c20 c12 c6 c14&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt;Figure 3&lt;/span&gt;&lt;span class=&quot;c5 c14&quot;&gt;: Object Detection With Face as the Only Class&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 image-margin&quot;&gt;&lt;span&gt;&lt;img class=&quot;border cntr&quot; alt=&quot;&quot; src=&quot;http://www.themtank.org/images/c-image8-40.jpg&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note:&lt;/span&gt; &lt;span class=&quot;c3&quot;&gt;Picture is an example of face detection, Object Detection of a single class. The authors cite one of the persistent issues in Object Detection to be the detection of small objects. Using small faces as a test class they explore the&lt;/span&gt; &lt;span class=&quot;c8 c3 c14&quot;&gt;role of scale invariance, image resolution, and contextual reasoning.&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;c3 c12 c14&quot;&gt;Source:&lt;/span&gt; &lt;span class=&quot;c3 c14&quot;&gt;Hu and Ramanan (2016, p. 1)&lt;/span&gt;&lt;sup class=&quot;c3 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt21&quot; id=&quot;ftnt_ref21&quot; name=&quot;ftnt_ref21&quot;&gt;[21]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; &lt;/span&gt;&lt;/p&gt;


&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;One of 2016’s major trends in&lt;/span&gt; &lt;span class=&quot;c12 c6&quot;&gt;Object Detection&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; was the shift towards a quicker, more efficient detection system. This was visible in approaches like YOLO, SSD and R-FCN as a move towards sharing computation on a whole image. Hence,&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;differentiating&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; themselves from the costly subnetworks associated with Fast/Faster R-CNN techniques.&lt;/span&gt; &lt;span class=&quot;c5&quot;&gt;This is typically referred to as ‘end-to-end training/learning’ and features throughout this piece.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;The rationale generally is to avoid having separate algorithms focus on their respective subproblems in isolation as this typically increase&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; training time and can lower network accuracy. That being said this end-to-end adaptation of networks typically takes place after initial sub-network solutions and, as such, is a retrospective optimisation.&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;However, Fast/Faster R-CNN techniques remain highly effective and are still used extensively for object detection.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;                           &lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;c18 lst-kix_tdd8q3xf6s8o-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;SSD: Single Shot MultiBox Detector&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt22&quot; id=&quot;ftnt_ref22&quot; name=&quot;ftnt_ref22&quot;&gt;[22]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; utilises a single Neural Network which encapsulates all the necessary computation and eliminates the costly proposal generation of other methods. It achieves “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;75.1% mAP, outperforming a comparable state of the art Faster R-CNN model&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;” (Liu et al. 2016).&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_tdd8q3xf6s8o-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c6&quot;&gt;One of the most impressive systems we saw in 2016 was from the aptly named “&lt;/span&gt;&lt;span class=&quot;c12 c6&quot;&gt;YOLO9000: Better, Faster, Stronger&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt23&quot; id=&quot;ftnt_ref23&quot; name=&quot;ftnt_ref23&quot;&gt;[23]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt;, which introduces the YOLOv2 and YOLO9000 detection systems.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt24&quot; id=&quot;ftnt_ref24&quot; name=&quot;ftnt_ref24&quot;&gt;[24]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; YOLOv2 vastly improves the initial YOLO model from mid-2015,&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt25&quot; id=&quot;ftnt_ref25&quot; name=&quot;ftnt_ref25&quot;&gt;[25]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; and is able to achieve better results at very high FPS&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;(up to 90 FPS on low resolution images using the original GTX Titan X)&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;. In addition to completion speed, the system outperforms Faster RCNN with ResNet and SSD on certain object detection datasets.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;        &lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;YOLO9000 implements a joint training method for detection and classification extending its prediction capabilities beyond the labelled detection data available i.e. it is able to detect objects that it&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;has never seen labelled detection data for&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;. The YOLO9000 model provides real-time object detection across 9000+ categories, closing the dataset size gap between classification and detection. Additional details, pre-trained models and a video showing it in action is available&lt;/span&gt; &lt;span class=&quot;c0&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;http://pjreddie.com/darknet/yolo/&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;.&lt;/span&gt; &lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt26&quot; id=&quot;ftnt_ref26&quot; name=&quot;ftnt_ref26&quot;&gt;[26]&lt;/a&gt;&lt;br /&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;iframe class=&quot;utube-embed utube-center&quot; src=&quot;https://www.youtube.com/embed/VOC3huqHrss&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_qxrwgmaqld8t-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Feature Pyramid Networks for Object Detection&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt27&quot; id=&quot;ftnt_ref27&quot; name=&quot;ftnt_ref27&quot;&gt;[27]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; comes from FAIR&lt;/span&gt; &lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt28&quot; id=&quot;ftnt_ref28&quot; name=&quot;ftnt_ref28&quot;&gt;[28]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; and capitalises on the “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”, meaning that representations remain powerful without compromising speed or memory. Lin et al. (2016) achieve state-of-the-art (hereafter SOTA) single-model results on COCO&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt29&quot; id=&quot;ftnt_ref29&quot; name=&quot;ftnt_ref29&quot;&gt;[29]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt;. Beating the results achieved by winners in 2016 when combined with a basic Faster R-CNN system.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_eonlgzn3npwq-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;R-FCN: Object Detection via Region-based Fully Convolutional Networks&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;:&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt30&quot; id=&quot;ftnt_ref30&quot; name=&quot;ftnt_ref30&quot;&gt;[30]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; This is another method that avoids applying a costly per-region subnetwork hundreds of times over an image by making the region-based detector fully convolutional and sharing computation on the whole image. “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;Our result is achieved at a test-time speed of 170ms per image, 2.5-20x faster than the Faster R-CNN counterpart&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;” (Dai et al., 2016).&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Figure 4&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;: Accuracy tradeoffs in Object Detection&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 image-margin&quot;&gt;&lt;span&gt;&lt;img alt=&quot;&quot; class=&quot;border cntr&quot; src=&quot;http://www.themtank.org/images/image16.png&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Y-axis displays mAP (mean Average Precision) and the X-axis displays meta-architecture variability across each feature extractor (VGG, MobileNet...Inception ResNet V2). Additionally, mAP small, medium and large describe the average precision for small, medium and large objects, respectively. As such accuracy is “&lt;/span&gt;&lt;span class=&quot;c7 c3&quot;&gt;stratified by object size, meta-architecture and feature extractor&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;” and “&lt;/span&gt;&lt;span class=&quot;c7 c3&quot;&gt;image resolution is fixed to 300&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;”. While Faster R-CNN performs comparatively well in the above sample, it is worth noting that the meta-architecture is considerably slower than more recent approaches, such as R-FCN.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Huang et al. (2016, p. 9)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt31&quot; id=&quot;ftnt_ref31&quot; name=&quot;ftnt_ref31&quot;&gt;[31]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Huang et al. (2016)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt32&quot; id=&quot;ftnt_ref32&quot; name=&quot;ftnt_ref32&quot;&gt;[32]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; present a paper which provides an in depth performance&lt;/span&gt;&lt;span class=&quot;c28 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;comparison between R-FCN, SSD and Faster R-CNN. Due to the issues around accurate comparison of Machine Learning (ML) techniques we’d like to point to the merits of producing a standardised approach here. They view these architectures as ‘meta-architectures’ since they can be combined with different kinds of feature extractors such as ResNet or Inception.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;The authors study the trade-off between accuracy and speed by varying meta-architecture, feature extractor and image resolution. The choice of feature extractor for example produces large variations between meta-architectures.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;The trend of making object detection cheap and efficient while still retaining the accuracy required for real-time commercial applications, notably in autonomous driving applications, is also demonstrated by SqueezeDet&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt33&quot; id=&quot;ftnt_ref33&quot; name=&quot;ftnt_ref33&quot;&gt;[33]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; and PVANet&lt;/span&gt; &lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt34&quot; id=&quot;ftnt_ref34&quot; name=&quot;ftnt_ref34&quot;&gt;[34]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; papers. While a Chinese company, DeepGlint, provides a good example of object detection in operation as a CCTV integration, albeit in a vaguely Orwellian manner:&lt;/span&gt; &lt;span class=&quot;c0&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://www.youtube.com/watch?v=xhp47v5OBXQ&quot;&gt;Video&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;.&lt;/span&gt; &lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt35&quot; id=&quot;ftnt_ref35&quot; name=&quot;ftnt_ref35&quot;&gt;[35]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;iframe class=&quot;utube-embed utube-center&quot; src=&quot;https://www.youtube.com/embed/xhp47v5OBXQ&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Results from ILSVRC and COCO Detection Challenge&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;COCO&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt36&quot; id=&quot;ftnt_ref36&quot; name=&quot;ftnt_ref36&quot;&gt;[36]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; (Common Objects in Context) is another popular image dataset. However, it is comparatively sma&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;ller and more curated than alternatives like ImageNet, with a focus on object recognition within the broader context of scene understanding. The organizers host a yearly challenge for Object Detection, segmentation and keypoints. Detection results from both the ILSVRC&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt37&quot; id=&quot;ftnt_ref37&quot; name=&quot;ftnt_ref37&quot;&gt;[37]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; and the COCO&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt38&quot; id=&quot;ftnt_ref38&quot; name=&quot;ftnt_ref38&quot;&gt;[38]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; Detection Challenge are;&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_ihpmhvl4dcp7-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;ImageNet LSVRC Object Detection from Images (DET):&lt;/span&gt; &lt;span class=&quot;c5&quot;&gt;CUImage 66% meanAP. Won 109 out of 200 object categories.&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;ImageNet LSVRC Object Detection from video (VID):&lt;/span&gt; &lt;span class=&quot;c5&quot;&gt;NUIST 80.8% mean AP&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;ImageNet LSVRC Object Detection from video with tracking:&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;CUvideo 55.8% mean AP&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;COCO 2016 Detection Challenge (bounding boxes):&lt;/span&gt; &lt;span class=&quot;c5&quot;&gt;G-RMI (Google) 41.5% AP (4.2% absolute percentage increase from 2015 winner MSRAVC)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;In review of the detection results for 2016, ImageNet stated that the ‘MSRAVC 2015 set a very high bar for performance [introduction of ResNets to competition]. Performance on all classes has improved across entries. Localization improved greatly in both challenges. High relative improvement on small object instances’ (ImageNet, 2016).&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt39&quot; id=&quot;ftnt_ref39&quot; name=&quot;ftnt_ref39&quot;&gt;[39]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Figure 5&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;c12 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;ILSVRC detection results from images (2013-2016)&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2 image-margin&quot;&gt;&lt;span&gt;&lt;img alt=&quot;&quot; class=&quot;border cntr&quot; src=&quot;http://www.themtank.org/images/image17.png&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note&lt;/span&gt;&lt;span class=&quot;c8 c3&quot;&gt;: ILSVRC Object Detection results from images (DET) (2013-2016).&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;c3 c12&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;ImageNet. 2016.&lt;/span&gt;&lt;span class=&quot;c3 c28&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;[Online]&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;Workshop Presentation, Slide 2&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c3&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;http://image-net.org/challenges/talks/2016/ECCV2016_ilsvrc_coco_detection_segmentation.pdf&quot;&gt;http://image-net.org/challenges/talks/2016/ECCV2016_ilsvrc_coco_detection_segmentation.pdf&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt; &lt;/span&gt;&lt;/p&gt;



&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Refers to the process of following a specific object of interest, or multiple objects, in a given scene. It traditionally has applications in video and real-world interactions where observations are made following an initial&lt;/span&gt; &lt;span class=&quot;c12 c6&quot;&gt;object detection&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;; the process is crucial to autonomous driving systems for example.&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_19zzdcpw893p-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Fully-Convolutional Siamese Networks for Object Tracking&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt40&quot; id=&quot;ftnt_ref40&quot; name=&quot;ftnt_ref40&quot;&gt;[40]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; combines a basic tracking algorithm with a Siamese network, trained end-to-end, which achieves SOTA and operates at frame-rates in excess of real-time. This paper attempts to tackle the lack of richness available to tracking models from traditional online learning methods.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_19zzdcpw893p-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Learning to Track at 100 FPS with Deep Regression Networks&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt41&quot; id=&quot;ftnt_ref41&quot; name=&quot;ftnt_ref41&quot;&gt;[41]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; is another paper which attempts to ameliorate the existing issues with online training methods. The authors produce a tracker which leverages a feed-forward network to learn the generic relationships surrounding object motion, appearance and orientation which effectively track novel objects&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;without online training&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;. Provides SOTA on a standard tracking benchmark while also managing “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;to track generic objects at 100 fps&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;” (Held et al., 2016).&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c5&quot;&gt;Video of GOTURN (Generic Object Tracking Using Regression&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Networks) available:&lt;/span&gt; &lt;span class=&quot;c0&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://www.youtube.com/watch?v=kMhwXnLgT_I&quot;&gt;Video&lt;/a&gt;&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt42&quot; id=&quot;ftnt_ref42&quot; name=&quot;ftnt_ref42&quot;&gt;[42]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c28 c6&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;iframe class=&quot;utube-embed utube-center&quot; src=&quot;https://www.youtube.com/embed/kMhwXnLgT_I&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_19zzdcpw893p-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Deep Motion Features for Visual Tracking&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt43&quot; id=&quot;ftnt_ref43&quot; name=&quot;ftnt_ref43&quot;&gt;[43]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;merge hand-crafted features, deep RGB/appearance features (from CNNs), and deep motion features (trained on optical flow images) to achieve SOTA. While deep motion features are commonplace in Action Recognition and Video Classification, the authors claim this is the first time they are used for visual tracking. The paper was also awarded&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;Best Paper&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt; in ICPR 2016, for “Computer Vision and Robot Vision” track.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;This paper presents an investigation of the impact of deep motion features in a tracking-by-detection framework. We further show that hand-crafted, deep RGB, and deep motion features contain complementary information. To the best of our knowledge, we are the first to propose fusing appearance information with deep motion features for visual tracking. Comprehensive experiments clearly suggest that our fusion approach with deep motion features outperforms standard methods relying on appearance information alone.&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;c18 lst-kix_u9pb33er4e37-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Virtual Worlds as Proxy for Multi-Object Tracking Analysis&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt44&quot; id=&quot;ftnt_ref44&quot; name=&quot;ftnt_ref44&quot;&gt;[44]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;approaches the lack of true-to-life variability present in existing video-tracking benchmarks and datasets. The paper proposes a new method for real-world cloning which generates rich, virtual, synthetic, photo-realistic environments from scratch with full-labels that overcome some of the sterility present in existing datasets. The generated images are automatically labelled with accurate ground truth allowing a range of applications aside from object detection&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;/tracking&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;, such as depth and optical flow.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_u9pb33er4e37-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Globally Optimal Object Tracking with Fully Convolutional Networks&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt45&quot; id=&quot;ftnt_ref45&quot; name=&quot;ftnt_ref45&quot;&gt;[45]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; deals with object variance and occlusion, citing these as two of the root limitations within object tracking. &quot;&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;Our proposed method solves the object appearance variation problem with the use of a Fully Convolutional Network and deals with occlusion by Dynamic Programming&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;&quot; (Lee et al., 2016).&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Central to Computer Vision is the process of&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;Segmentation&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;, which divides whole images into pixel groupings which can then be labelled and classified. Moreover,&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;Semantic Segmentation&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;goes further by trying to semantically understand the role of each pixel in the image e.g. is it a cat, car or some other type of class?&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;Instance Segmentation&lt;/span&gt; &lt;span class=&quot;c5&quot;&gt;takes this even further by segmenting different instances of classes e.g. labelling three different dogs with three different colours. It is one of a barrage of Computer Vision applications currently employed in autonomous driving technology suites.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Perhaps some of the best improvements in the area of segmentation come courtesy of FAIR, who continue to build upon their DeepMask work from 2015.&lt;/span&gt; &lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt46&quot; id=&quot;ftnt_ref46&quot; name=&quot;ftnt_ref46&quot;&gt;[46]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; DeepMask generates rough ‘masks’ over objects as an initial form of segmentation. In 2016, Fair introduced SharpMask&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt47&quot; id=&quot;ftnt_ref47&quot; name=&quot;ftnt_ref47&quot;&gt;[47]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; which refines the ‘masks’ provided by DeepMask, correcting the loss of detail and improving semantic segmentation&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; In addition to this, MultiPathNet&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt48&quot; id=&quot;ftnt_ref48&quot; name=&quot;ftnt_ref48&quot;&gt;[48]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; identifies the objects delineated by each mask.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;To capture general object shape, you have to have a high-level understanding of what you are looking at (DeepMask), but to accurately place the boundaries you need to look back at lower-level features all the way down to the pixels (SharpMask).&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; - Piotr Dollar, 2016.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt49&quot; id=&quot;ftnt_ref49&quot; name=&quot;ftnt_ref49&quot;&gt;[49]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; &lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Figure 6&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;: Demonstration of FAIR techniques in action&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c10 image-margin&quot;&gt;&lt;span&gt;&lt;img alt=&quot;&quot; class=&quot;border cntr&quot; src=&quot;http://www.themtank.org/images/c-image11.jpg&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note&lt;/span&gt;&lt;span class=&quot;c8 c3&quot;&gt;: The above pictures demonstrate the segmentation techniques employed by FAIR. These include the application of DeepMask, SharpMask and MultiPathNet techniques which are applied in that order. This process allows accurate segmentation and classification in a variety of scenes.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Dollar (2016).&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt50&quot; id=&quot;ftnt_ref50&quot; name=&quot;ftnt_ref50&quot;&gt;[50]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Video Propagation Networks&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt51&quot; id=&quot;ftnt_ref51&quot; name=&quot;ftnt_ref51&quot;&gt;[51]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; attempt to create a simple model to propagate accurate object masks, assigned at first frame, through the entire video sequence along with some additional information.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;In 2016, researchers worked on&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;finding alternative network configurations to tackle the&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;aforementioned issues of scale and localisation. DeepLab&lt;/span&gt;&lt;sup class=&quot;c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt52&quot; id=&quot;ftnt_ref52&quot; name=&quot;ftnt_ref52&quot;&gt;[52]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; is one such example of this which achieves encouraging results for semantic image segmentation tasks. Khoreva et al. (&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;2016&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt53&quot; id=&quot;ftnt_ref53&quot; name=&quot;ftnt_ref53&quot;&gt;[53]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; build on Deeplab’s earlier work (circa 2015)&lt;/span&gt;&lt;span class=&quot;c28 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;and propose a weakly supervised training method which achieves comparable results to fully supervised networks.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;Computer Vision further refined the network sharing of useful information approach through the use of end-to-end networks, which reduce the computational requirements of multiple omni-directional subtasks for classification. Two key papers using this approach are:&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_3eigggtxldag-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;100 Layers Tiramisu&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt54&quot; id=&quot;ftnt_ref54&quot; name=&quot;ftnt_ref54&quot;&gt;[54]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;is a fully-convolutional DenseNet which&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; connects every layer,&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;to every other layer,&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;in a feed-forward fashion. It also&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;achieves SOTA on multiple benchmark datasets with fewer parameters and training/processing.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;ul class=&quot;c18 lst-kix_nk9139oynrts-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Fully Convolutional Instance-aware Semantic Segmentation&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt55&quot; id=&quot;ftnt_ref55&quot; name=&quot;ftnt_ref55&quot;&gt;[55]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;performs instance mask prediction and classification jointly (two subtasks).&lt;br /&gt;&lt;/span&gt; &lt;span class=&quot;c12 c6&quot;&gt;COCO Segmentation challenge winner&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;MSRA. 37.3% AP.&lt;br /&gt;9.1% absolute jump from MSRAVC in 2015 in COCO challenge.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;While&lt;/span&gt; &lt;span class=&quot;c12 c6&quot;&gt;ENet&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;,&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt56&quot; id=&quot;ftnt_ref56&quot; name=&quot;ftnt_ref56&quot;&gt;[56]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;a DNN architecture for real-time semantic segmentation, is not of this category, it does demonstrate the commercial merits of reducing computation costs and giving greater access to mobile devices.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Our work wishes to relate as much of these advancements back to tangible public applications as possible. With this in mind, the following contains some of the most interesting&lt;/span&gt; &lt;span class=&quot;c12 c6&quot;&gt;healthcare&lt;/span&gt; &lt;span class=&quot;c12 c6&quot;&gt;application&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt; of segmentation in 2016;&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;One of our favourite quasi-medical segmentation applications is&lt;/span&gt; &lt;span class=&quot;c12 c6&quot;&gt;FusionNet&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt63&quot; id=&quot;ftnt_ref63&quot; name=&quot;ftnt_ref63&quot;&gt;[63]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt;- a deep fully residual convolutional neural network for image segmentation in connectomics&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt64&quot; id=&quot;ftnt_ref64&quot; name=&quot;ftnt_ref64&quot;&gt;[64]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; benchmarked against SOTA electron microscopy (EM) segmentation methods.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;iframe class=&quot;utube-embed utube-center&quot; src=&quot;https://www.youtube.com/embed/PNzQ4PNZSzc&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/p&gt;



&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Not all research in Computer Vision serves to extend the pseudo-cognitive abilities of machines, and often the fabled malleability of neural networks, as well as other ML techniques, lend themselves to a variety of other novel applications that spill into the public space. Last year’s advancements in Super-resolution,&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;Style Transfer &amp;amp; Colourisation&lt;/span&gt;&lt;span class=&quot;c28 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;occupied that space for us.&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Super-resolution&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;refers to the process of estimating a high resolution image from a low resolution counterpart, and also the prediction of image features at different magnifications, something which the human brain can do almost effortlessly.&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;Originally super-resolution was performed by simple techniques like bicubic-interpolation and nearest neighbours.&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt; In terms of commercial applications, the desire to overcome low-resolution constraints stemming from source quality and realisation of ‘CSI Miami’ style image enhancement has driven research in the field. Here are some of the year’s advances and their potential impact:&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_xj6qqr6gdbue-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Neural Enhance&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt65&quot; id=&quot;ftnt_ref65&quot; name=&quot;ftnt_ref65&quot;&gt;[65]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;is the brainchild of&lt;/span&gt; &lt;span class=&quot;c5 c14&quot;&gt;Alex J. Champandard and combines approaches from four different research papers to achieve its Super-resolution method.&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt;Real-Time Video Super Resolution&lt;/span&gt; &lt;span class=&quot;c6 c14&quot;&gt;was also attempted in 2016 in two notable instances.&lt;/span&gt;&lt;sup class=&quot;c12 c6 c14&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt66&quot; id=&quot;ftnt_ref66&quot; name=&quot;ftnt_ref66&quot;&gt;[66]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6 c45 c14&quot;&gt;,&lt;/span&gt;&lt;sup class=&quot;c12 c6 c14&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt67&quot; id=&quot;ftnt_ref67&quot; name=&quot;ftnt_ref67&quot;&gt;[67]&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt;RAISR:&lt;/span&gt; &lt;span class=&quot;c6 c14&quot;&gt;Rapid and Accurate Image Super-Resolution&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt68&quot; id=&quot;ftnt_ref68&quot; name=&quot;ftnt_ref68&quot;&gt;[68]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6 c14&quot;&gt; from Google avoids the costly memory and speed requirements of neural network approaches by training filters with low-resolution and high-resolution image pairs. RAISR, as a learning-based framework, is two orders of magnitude faster than competing algorithms and has minimal memory requirements when&lt;/span&gt; &lt;span class=&quot;c6 c14&quot;&gt;compared with neural network-based approaches&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt;. Hence super-resolution is extendable to personal devices. There is a research blog available&lt;/span&gt; &lt;span class=&quot;c0 c14&quot;&gt;&lt;a class=&quot;c4&quot; href=&quot;https://research.googleblog.com/2016/11/enhance-raisr-sharp-images-with-machine.html&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt;.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt69&quot; id=&quot;ftnt_ref69&quot; name=&quot;ftnt_ref69&quot;&gt;[69]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5 c14&quot;&gt; &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt;Figure 7&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt;: Super-resolution SRGAN examp&lt;/span&gt;&lt;span class=&quot;c5 c14&quot;&gt;le&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 image-margin&quot;&gt;&lt;span&gt;&lt;img alt=&quot;&quot; class=&quot;border cntr&quot; src=&quot;http://www.themtank.org/images/c-image10.jpg&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: From left to right: bicubic interpolation (the objective worst performer for focus), Deep residual network optimised for MSE, deep residual generative adversarial network optimized for a loss more sensitive to human perception, original High Resolution (HR) image. Corresponding peak signal to noise ratio (PSNR) and structural similarity (SSIM) are shown in two brackets. [4 x upscaling] The reader may wish to zoom in on the middle two images (SRResNet and SRGAN) to see the difference between image smoothness vs more realistic fine details.&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;c3 c12&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Ledig et al. (2017)&lt;/span&gt;&lt;sup class=&quot;c3&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt70&quot; id=&quot;ftnt_ref70&quot; name=&quot;ftnt_ref70&quot;&gt;[70]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6 c14&quot;&gt;The use of Generative Adversarial Networks (GANs) represent current SOTA for Super-resolution:&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;c18 lst-kix_3a0j39gr05nl-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt;SRGAN&lt;/span&gt;&lt;sup class=&quot;c12 c6 c14&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt71&quot; id=&quot;ftnt_ref71&quot; name=&quot;ftnt_ref71&quot;&gt;[71]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c5 c14&quot;&gt;provides photo-realistic textures from heavily downsampled images on public benchmarks, using a discriminator network trained to differentiate between super-resolved and original photo-realistic images.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6 c14&quot;&gt;Qualitatively SRGAN performs the best, although SRResNet performs best with peak-signal-to-noise-ratio (PSNR) metric but SRGAN gets the finer texture details and achieves the best Mean Opinion Score (MOS).&lt;/span&gt; &lt;span class=&quot;c7 c6 c14&quot;&gt;“To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4× upscaling factors.”&lt;/span&gt;&lt;sup class=&quot;c7 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt72&quot; id=&quot;ftnt_ref72&quot; name=&quot;ftnt_ref72&quot;&gt;[72]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c28 c6 c14&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c5 c14&quot;&gt;All previous approaches fail to recover the finer texture details at large upscaling factors.&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_s5md9ry0mo50-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Amortised MAP Inference for Image Super-resolution&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt73&quot; id=&quot;ftnt_ref73&quot; name=&quot;ftnt_ref73&quot;&gt;[73]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;proposes a method for calculation of Maximum a Posteriori (MAP) inference using a Convolutional Neural Network. However, their research presents three approaches for optimisation, all of which GANs perform markedly better&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;on real image data at present.&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt;Figure 8&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt;: Style Transfer from Nikulin &amp;amp; Novak&lt;/span&gt;&lt;span class=&quot;c5 c14&quot;&gt;le&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c47&quot;&gt; &lt;span class=&quot;image-margin&quot;&gt;&lt;img alt=&quot;&quot; class=&quot;border cntr&quot; src=&quot;http://www.themtank.org/images/c-image20.jpg&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Transferring different styles to a photo of a cat (original top left).&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;c3 c12&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Nikulin &amp;amp; Novak (2016)&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c46&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Undoubtedly,&lt;/span&gt; &lt;span class=&quot;c12 c6&quot;&gt;Style Transfer&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; epitomises a novel use of neural networks that has ebbed into the public domain, specifically through last year’s facebook integrations and companies like&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;Prisma&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt74&quot; id=&quot;ftnt_ref74&quot; name=&quot;ftnt_ref74&quot;&gt;[74]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; and Artomatix&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt75&quot; id=&quot;ftnt_ref75&quot; name=&quot;ftnt_ref75&quot;&gt;[75]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt;. Style transfer is an older technique but converted to a neural networks in 2015 with the publication of&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; Neural Algorithm of Artistic Style.&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt76&quot; id=&quot;ftnt_ref76&quot; name=&quot;ftnt_ref76&quot;&gt;[76]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; Since then, the concept of style transfer was expanded upon by Nikulin and Novak&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt77&quot; id=&quot;ftnt_ref77&quot; name=&quot;ftnt_ref77&quot;&gt;[77]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; and also applied to video,&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt78&quot; id=&quot;ftnt_ref78&quot; name=&quot;ftnt_ref78&quot;&gt;[78]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; as is the common progression within Computer Vision.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c47&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Figure 9&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;: Further examples of Style Transfer&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c46&quot;&gt;&lt;span&gt;&lt;img alt=&quot;&quot; class=&quot;border cntr&quot; src=&quot;http://www.themtank.org/images/c-image18.jpg&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: The top row (left to right) represent the artistic style which is transposed onto the original images which are displayed in the first column (Woman, Golden Gate Bridge and Meadow Environment). Using conditional instance normalisation a single style transfer network can capture 32 style simultaneously, five of which are displayed here. The full suite of images in available in the source paper’s appendix. This work will feature in the International Conference on Learning Representations (ICLR) 2017.            &lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;c3 c12&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Dumoulin et al. (2017, p. 2)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt79&quot; id=&quot;ftnt_ref79&quot; name=&quot;ftnt_ref79&quot;&gt;[79]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c3&quot;&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;c6&quot;&gt;Style transfer as a topic is fairly intuitive once visualised; take an image and imagine it with the stylistic features of a different image. For example, in the style of a famous painting or artist. This year Facebook released Caffe2Go,&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt80&quot; id=&quot;ftnt_ref80&quot; name=&quot;ftnt_ref80&quot;&gt;[80]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; their deep learning system which integrates into mobile devices. Google also released some interesting work which sought to blend multiple styles to generate entirely unique image styles: Research blog&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt81&quot; id=&quot;ftnt_ref81&quot; name=&quot;ftnt_ref81&quot;&gt;[81]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; and full paper.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt82&quot; id=&quot;ftnt_ref82&quot; name=&quot;ftnt_ref82&quot;&gt;[82]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; &lt;/span&gt;&lt;/p&gt;&lt;p&gt;Besides mobile integrations, style transfer has applications in the creation of game assets. Members of our team recently saw a presentation by the Founder and CTO of Artomatix, Eric Risser, who discussed the technique’s novel application for content generation in games (texture mutation, etc.) and, therefore, dramatically minimises the work of a conventional texture artist.&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Colourisation&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;is the process of changing monochrome images to new full-colour versions. Originally this was done manually by people who painstakingly selected colours to represent specific pixels in each image. In 2016, it became possible to automate this process while maintaining the appearance of realism indicative of the human-centric colourisation process. While humans may not accurately represent the true colours of a given scene, their real world knowledge allows the application of colours in a way which is consistent with the image and another person viewing said image.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;The process of colourisation is interesting in that the network assigns the most likely colouring for images based on its understanding of object location, textures and environment, e.g. it learns that skin is pinkish and the sky is blueish.&lt;/p&gt;&lt;p&gt;Three of the most influential works of the year are as follows:&lt;/p&gt;
&lt;ul class=&quot;c18 lst-kix_8j5v1moclfjn-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Zhang et al.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt83&quot; id=&quot;ftnt_ref83&quot; name=&quot;ftnt_ref83&quot;&gt;[83]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; produced a method that was able to successfully fool humans on 32% of their trials. Their methodology is comparable to a “colourisation Turing test.”&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_8j5v1moclfjn-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Larsson et al.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt84&quot; id=&quot;ftnt_ref84&quot; name=&quot;ftnt_ref84&quot;&gt;[84]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; fully automate their image colourisation system using Deep Learning for Histogram estimation.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_8j5v1moclfjn-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Finally, Lizuka, Simo-Serra and Ishikawa&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt85&quot; id=&quot;ftnt_ref85&quot; name=&quot;ftnt_ref85&quot;&gt;[85]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; demonstrate a colourisation model also based upon CNNs. The work outperformed the existing SOTA, we [the team] feel as though this work is qualitatively best also, appearing to be the most realistic. Figure 10 provides comparisons, however the image is taken from Lizuka et al.&lt;br /&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt;Figure 10&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt;: Comparison of Colourisation Research&lt;/span&gt;&lt;span class=&quot;c5 c14&quot;&gt;le&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 image-margin&quot;&gt;&lt;span&gt;&lt;img alt=&quot;&quot; class=&quot;border cntr&quot; src=&quot;http://www.themtank.org/images/c-image7.jpg&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: From top to bottom -  column one contains the original monochrome image input which is subsequently colourised through various techniques. The remaining columns display the results generated by other prominent colourisation research in 2016. When viewed from left to right, these are Larsson et al.&lt;/span&gt; &lt;span class=&quot;c3 c45&quot;&gt;84&lt;/span&gt; &lt;span class=&quot;c3&quot;&gt; 2016 (column two), Zhang et al.&lt;/span&gt; &lt;span class=&quot;c3 c45&quot;&gt;83&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt; 2016 (Column three), and Lizuka, Simo-Serra and Ishikawa.&lt;/span&gt; &lt;span class=&quot;c3 c45&quot;&gt;85&lt;/span&gt;  &lt;span class=&quot;c8 c3&quot;&gt; 2016, also referred to as “ours” by the authors (Column four). The quality difference in colourisation is most evident in row three (from the top) which depicts a group of young boys. We believe Lizuka et al.’s work to be qualitatively superior (Column four).&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Lizuka et al. 2016&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt86&quot; id=&quot;ftnt_ref86&quot; name=&quot;ftnt_ref86&quot;&gt;[86]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;Furthermore, our architecture can process images of any resolution, unlike most existing approaches based on CNN.&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”&lt;/span&gt;&lt;/p&gt;&lt;p&gt;In a test to see how natural their colourisation was, users were given a random image from their models and were asked, &quot;does this image look natural to you?&quot;&lt;/p&gt;&lt;p&gt;Their approach achieved 92.6%, the baseline achieved roughly 70% and the ground truth (the actual colour photos) were considered 97.7% of the time to be natural.&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;The task of action recognition refers to the both the classification of an action within a given video frame, and more recently, algorithms which can predict the likely outcomes of interactions given only a few frames before the action takes place. In this respect we see recent research attempt to imbed context into algorithmic decisions, similar to other areas of Computer Vision. Some key papers in this space are:&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_w7w41dfpoe57-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Long-term Temporal Convolutions for Action Recognition&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt87&quot; id=&quot;ftnt_ref87&quot; name=&quot;ftnt_ref87&quot;&gt;[87]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; leverages the spatio-temporal structure of human actions, i.e. the particular movement and duration, to correctly recognise actions using a CNN variant. To overcome the sub-optimal temporal modelling of longer term actions by CNNs, the authors propose a neural network with long-term temporal convolutions (LTC-CNN) to improve the accuracy of action recognition. Put simply,&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt; the LTCs can look at larger parts of the video to recognise actions. Their approach uses and extends 3D CNNs ‘to enable action representation at a fuller temporal scale’.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;We report state-of-the-art results on two challenging benchmarks for human action recognition UCF101 (92.7%) and HMDB51 (67.2%).&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;”&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_w7w41dfpoe57-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Spatiotemporal Residual Networks for Video Action Recognition&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt88&quot; id=&quot;ftnt_ref88&quot; name=&quot;ftnt_ref88&quot;&gt;[88]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; apply a variation of two stream CNN to the task of action recognition, which combines techniques from both traditional CNN approaches and recently popularised Residual Networks (ResNets). The two stream approach takes its inspiration from a neuroscientific hypothesis on the functioning of the visual cortex, i.e. separate pathways recognise object shape/colour and movement. The authors combine the classification benefits of ResNets by injecting residual connections between the two CNN streams.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;Each stream initially performs video recognition on its own and for final classification, softmax scores are combined by late fusion. To date, this approach is the most effective approach of applying deep learning to action recognition, especially with limited training data. In our work we directly convert image ConvNets into 3D architectures and show greatly improved performance over the two-stream baseline.&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;” - 94% on UCF101 and 70.6% on HMDB51. Feichtenhofer et al. made improvements over traditional improved dense trajectory (iDT) methods and generated better results through use of both techniques.&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_w7w41dfpoe57-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Anticipating Visual Representations from Unlabeled Video&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt89&quot; id=&quot;ftnt_ref89&quot; name=&quot;ftnt_ref89&quot;&gt;[89]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; is an interesting paper, although not strictly action classification. The program&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;predicts the action&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; which is likely to take place given a sequence of video frames up to one second before an action. The approach uses visual representations rather than pixel-by-pixel classification, which means that the program can operate without labeled data, by taking advantage of the feature learning properties of deep neural networks.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt90&quot; id=&quot;ftnt_ref90&quot; name=&quot;ftnt_ref90&quot;&gt;[90]&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;&quot;&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;The key idea behind our approach is that we can train deep networks to predict the visual representation of images in the future. Visual representations are a promising prediction target because they encode images at a higher semantic level than pixels yet are automatic to compute. We then apply recognition algorithms on our predicted representation to anticipate objects and actions&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;&quot;.&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;c18 lst-kix_w7w41dfpoe57-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c6&quot;&gt;The organisers of the&lt;/span&gt; &lt;span class=&quot;c12 c6&quot;&gt;Thumos Action Recognition Challenge&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt91&quot; id=&quot;ftnt_ref91&quot; name=&quot;ftnt_ref91&quot;&gt;[91]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; released a paper describing the general approaches for Action Recognition from the last number of years. The paper also provides a rundown of the Challenges from 2013-2015, future directions for the challenge and ideas on how to give computers a more holistic understanding of video through Action Recognition. We hope that the Thumos Action Recognition Challenge returns in 2017 after its (seemingly) unexpected hiatus.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c6&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;A key goal of Computer Vision is to recover the underlying 3D structure from 2D observations of the world.&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;” - Rezende et al. (2016, p. 1)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt92&quot; id=&quot;ftnt_ref92&quot; name=&quot;ftnt_ref92&quot;&gt;[92]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;In Computer Vision, the classification of scenes, objects and activities, along with the output of bounding boxes and image segmentation is, as we have seen, the focus of much new research. In essence, these approaches apply computation to gain an ‘understanding’ of the 2D space of an image. However, detractors note that a 3D understanding is imperative for systems to successfully&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;interpret, and navigate, the real world.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;For instance, a network may locate a cat in an image, colour all of its pixels and classify it as a cat. But does the network&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;fully&lt;/span&gt;&lt;span class=&quot;c28 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;understand where the cat in the image is, in the context of the cat’s environment?&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;One could argue that the computer learns very little about the 3D world from the above tasks. Contrary to this, humans understand the world in 3D even when examining 2D pictures, i.e. perspective, occlusion, depth, how objects in a scene are related&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;c28 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;etc. Imparting these 3D representations and their associated knowledge to artificial systems represents one of the next great frontiers of Computer Vision. A major reason for thinking this is that&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;, generally;&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt; &lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;the 2D projection of a scene is a complex function of the attributes and positions of the camera, lights and objects that make up the scene. If endowed with 3D understanding, agents can abstract away from this complexity to form stable, disentangled representations, e.g., recognizing that a chair is a chair whether seen from above or from the side, under different lighting conditions, or under partial occlusion.&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt93&quot; id=&quot;ftnt_ref93&quot; name=&quot;ftnt_ref93&quot;&gt;[93]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;However, 3D understanding has traditionally faced several impediments. The first concerns the problem of both ‘&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;self and normal occlusion’&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; along with the numerous 3D shapes which fit a given 2D representation. Understanding problems are further compounded by the inability to map different images of the same structures to&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;the same&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;3D space, and in the handling of the multi-modality of these representations.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt94&quot; id=&quot;ftnt_ref94&quot; name=&quot;ftnt_ref94&quot;&gt;[94]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; Finally, ground-truth 3D datasets were traditionally quite expensive and difficult to obtain which, when coupled with divergent approaches for representing 3D structures, may have led to training limitations&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;We feel that the&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; work being conducted in this space is important to be mindful of. From the embryonic, albeit titillating early theoretical applications for future AGI systems and robotics, to the immersive, captivating applications in augmented, virtual and mixed reality which will affect our societies in the near future. We cautiously predict exponential growth in this area of Computer Vision, as a result of lucrative commercial applications, which means that soon computers may&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;start reasoning about the&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;world&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; rather than just about&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;pixels&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;



&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;This first section is a tad scattered, acting as a catch-all for computation applied to objects represented with 3D data, inference of 3D object shape from 2D images and Pose Estimation; determining the transformation of an object’s 3D pose from 2D images.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt95&quot; id=&quot;ftnt_ref95&quot; name=&quot;ftnt_ref95&quot;&gt;[95]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; The process of reconstruction also creeps in ahead of the following section which deals with it explicitly.&lt;/span&gt;&lt;span class=&quot;c28 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;However, with these points in mind, we present the work which excited our team the most in this general area:&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_cltoahc9dmiq-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt;OctNet: Learning Deep 3D Representations at High Resolutions&lt;/span&gt;&lt;sup class=&quot;c12 c6 c14&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt96&quot; id=&quot;ftnt_ref96&quot; name=&quot;ftnt_ref96&quot;&gt;[96]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6 c14&quot;&gt; continue&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt; the recent development of convolutional networks which operate on 3D data, or Voxels (which are like 3D pixels), using 3D convolutions. OctNet is ‘&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt;a novel 3D representation which makes deep learning with high-resolution inputs tractable&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt;’. The authors test OctNet representations by ‘analyzing the impact of resolution on several 3D tasks including 3D object classification, orientation estimation and point cloud labeling.’ The paper’s central contribution is its exploitation of sparsity&lt;/span&gt; &lt;span class=&quot;c5 c14&quot;&gt;in 3D input data which then enables much more efficient use of memory and computation.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_7rbz49bxi8wy-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;ObjectNet3D: A Large Scale Database for 3D Object Recognition&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt97&quot; id=&quot;ftnt_ref97&quot; name=&quot;ftnt_ref97&quot;&gt;[97]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; - contributes a database for 3D object recognition, presenting 2D images and 3D shapes for 100 object categories. ‘&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;Objects in the images in our database [taken from ImageNet] are aligned with the 3D shapes [taken from the ShapeNet repository], and the alignment provides both accurate 3D pose annotation and the closest 3D shape annotation for each 2D object.&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;’ Baseline experiments are provided on&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;: Region proposal generation, 2D object detection, joint 2D detection and 3D object pose estimation, and image-based 3D shape retrieval.&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt98&quot; id=&quot;ftnt_ref98&quot; name=&quot;ftnt_ref98&quot;&gt;[98]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;- creates a reconstruction of an object ‘in the form of a 3D occupancy grid using single or multiple images of object instance from arbitrary viewpoints.’ Mappings from images of objects to 3D shapes are learned using primarily synthetic data, and the network can train and test without requiring ‘any image annotations or object class labels’. The network comprises a 2D-CNN, a 3D Convolutional LSTM (an architecture newly created for purpose) and a 3D Deconvolutional Neural Network. How these different components interact&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;and are trained together end-to-end&lt;/span&gt;&lt;span class=&quot;c28 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;is a perfect illustration of the layering capable with Neural Networks.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;c10 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;c12 c6&quot;&gt;Figure 11&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;: Example of 3D-R2N2 functionality&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c10 image-margin&quot;&gt;&lt;span&gt;&lt;img alt=&quot;Screenshot 2017-03-07 18.08.04.png&quot; src=&quot;http://www.themtank.org/images/c-image4.jpg&quot; class=&quot;border cntr&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note&lt;/span&gt;&lt;span class=&quot;c8 c3&quot;&gt;: Images taken from Ebay (left) and an overview of the functionality of 3D-R2N2 (right).&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note from source&lt;/span&gt;&lt;span class=&quot;c8 c3&quot;&gt;: Some sample images of the objects we [the authors] wish to reconstruct - notice that views are separated by a large baseline and objects’ appearance shows little texture and/or are non-lambertian. (b) An overview of our proposed 3D-R2N2: The network takes a sequence of images (or just one image) from arbitrary (uncalibrated) viewpoints as input (in this example, 3 views of the armchair) and generates voxelized 3D reconstruction as an output. The reconstruction is incrementally refined as the network sees more views of the object.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Choy et al. (2016, p. 3)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt99&quot; id=&quot;ftnt_ref99&quot; name=&quot;ftnt_ref99&quot;&gt;[99]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c5&quot;&gt;3D-R2N2 generates ‘rendered images and voxelized models’ using ShapeNet models and facilitates 3D object reconstruction where structure from motion (SfM) and simultaneous localisation and mapping (SLAM) approaches typically fail:  &lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;Our extensive experimental analysis shows that our reconstruction framework i) outperforms the state-of-the-art methods for single view reconstruction, and ii) enables the 3D reconstruction of objects in situations when traditional SFM/SLAM methods fail.&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;”&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_7rbz49bxi8wy-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;3D Shape Induction from 2D Views of Multiple Objects&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt100&quot; id=&quot;ftnt_ref100&quot; name=&quot;ftnt_ref100&quot;&gt;[100]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;uses “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;Projective Generative Adversarial Networks&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;” (PrGANs), which train a deep generative model allowing accurate representation of 3D shapes, with the discriminator only being shown 2D images. The projection module captures the 3D representations and converts them to 2D images before passing to the discriminator. Through iterative training cycles the generator improves projections by improving the 3D voxel shapes it generates.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Figure 12&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;: PrGAN architecture segment&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;        &lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span&gt;&lt;img alt=&quot;&quot; class=&quot;border cntr&quot; src=&quot;http://www.themtank.org/images/c-image1.jpg&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note&lt;/span&gt; &lt;span class=&quot;c3 c12&quot;&gt;from source&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: The PrGAN architecture for generating 2D images of shapes. A 3D voxel representation (32&lt;/span&gt;&lt;span class=&quot;c3 c45&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;) and viewpoint are independently generated from the input z (201-d vector). The projection module renders the voxel shape from a given viewpoint (θ, φ) to create an image. The discriminator consists of 2D convolutional and pooling layers and aims to classify if the input image is generated or real.&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;c3 c12&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Gadhelha et al. (2016, p. 3)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt101&quot; id=&quot;ftnt_ref101&quot; name=&quot;ftnt_ref101&quot;&gt;[101]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c5&quot;&gt;In this way the inference ability is learned through an unsupervised environment:&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;The addition of a projection module allows us to infer the underlying 3D shape distribution without using any 3D, viewpoint information, or annotation during the learning phase.&lt;/span&gt; &lt;span class=&quot;c5&quot;&gt;”&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c5&quot;&gt;Additionally, the internal representation of the shapes can be interpolated, meaning discrete commonalities in voxel shapes allow transformations from object to object, e.g. from car to aeroplane.&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_4wkftt7qogfl-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Unsupervised Learning of 3D Structure from Images&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt102&quot; id=&quot;ftnt_ref102&quot; name=&quot;ftnt_ref102&quot;&gt;[102]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; presents a completely unsupervised, generative model which demonstrates ‘the feasibility of learning to infer 3D representations of the world’ for the first time. In a nutshell the DeepMind team present a model which “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;learns strong deep generative models of 3D structures, and recovers these structures from 3D and 2D images via probabilistic inference&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”,&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; meaning that inputs can be both 3D and 2D.&lt;br /&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;DeepMind’s strong generative model runs on both volumetric and mesh-based representations. The use of Mesh-based representations with OpenGL allows more knowledge to be built in, e.g. how light affects the scene and the materials used. “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;Using a 3D mesh-based representation and training with a fully-fledged black-box renderer in the loop enables learning of the interactions between an object’s colours, materials and textures, positions of lights, and of other objects&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;.”&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt103&quot; id=&quot;ftnt_ref103&quot; name=&quot;ftnt_ref103&quot;&gt;[103]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;The models are of high quality, capture uncertainty and are amenable to probabilistic inference, allowing for applications in 3D generation and simulation. The team achieve the first quantitative benchmark for 3D density modelling on 3D MNIST and ShapeNet. This approach demonstrates that models may be trained end-to-end unsupervised on 2D images, requiring no ground-truth 3D labels.&lt;/p&gt;



&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Human Pose Estimation attempts to find the orientation and configuration of human body parts. 2D Human Pose Estimation, or Keypoint Detection, generally re&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;fers to localising body parts of humans e.g finding the 2D location of the knees, eyes, feet, etc.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c28 c6&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;However, 3D Pose Estimation takes this even further by finding the orientation of the body parts in 3D space and then an optional step of shape estimation/modelling can be performed. There has been a tremendous amount of improvement across these sub-domains in the last few years.&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;In terms of competitive evaluation “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;c7 c6 c14&quot;&gt;he COCO 2016 Keypoint Challenge involves simultaneously detecting people and localizing their keypoints&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt;”.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt104&quot; id=&quot;ftnt_ref104&quot; name=&quot;ftnt_ref104&quot;&gt;[104]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6 c14&quot;&gt; The European Convention on Computer Vision (ECCV)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt105&quot; id=&quot;ftnt_ref105&quot; name=&quot;ftnt_ref105&quot;&gt;[105]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6 c14&quot;&gt; provides more extensive literature on these subjects, however we would like to highlight:&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_al4bo8mwtqn4-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt106&quot; id=&quot;ftnt_ref106&quot; name=&quot;ftnt_ref106&quot;&gt;[106]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;This method set SOTA performance on the inaugural MSCOCO 2016 keypoints challenge with 60% average precision (AP) and won the best demo award at ECCV, video:&lt;/span&gt; &lt;span class=&quot;c0&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://www.youtube.com/watch?v=pW6nZXeWlGM&quot;&gt;Video&lt;/a&gt;&lt;/span&gt; &lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt107&quot; id=&quot;ftnt_ref107&quot; name=&quot;ftnt_ref107&quot;&gt;[107]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;iframe class=&quot;utube-embed utube-center&quot; src=&quot;https://www.youtube.com/embed/pW6nZXeWlGM&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_al4bo8mwtqn4-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt108&quot; id=&quot;ftnt_ref108&quot; name=&quot;ftnt_ref108&quot;&gt;[108]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; This method first predicts 2D body joint locations and then uses another model called SMPL to create the 3D body shape mesh, which allows it to understand 3D aspects working from 2D pose estimation. The 3D mesh is capable of capturing both pose and shape, versus previous methods which could only find 2D human pose.&lt;/span&gt;&lt;span class=&quot;c28 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;The authors provide an excellent video analysis of their work here:&lt;/span&gt; &lt;span class=&quot;c0&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://www.youtube.com/watch?v=eUnZ2rjxGaE&quot;&gt;Video&lt;/a&gt;&lt;/span&gt; &lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt109&quot; id=&quot;ftnt_ref109&quot; name=&quot;ftnt_ref109&quot;&gt;[109]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; &lt;br /&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;iframe class=&quot;utube-embed utube-center&quot; src=&quot;https://www.youtube.com/embed/eUnZ2rjxGaE&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt110&quot; id=&quot;ftnt_ref110&quot; name=&quot;ftnt_ref110&quot;&gt;[110]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c28 c6&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;


&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;As mentioned, a previous section presented some examples of reconstruction but with a general focus on objects, specifically their shape and pose.&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;While some of this is technically reconstruction, the field itself comprises many different types of reconstruction, e.g. scene reconstruction, multi-view and single view reconstruction, structure from motion (SfM), SLAM, etc. Furthermore, some reconstruction approaches leverage additional (and multiple) sensors and equipment, such&lt;/span&gt;&lt;span class=&quot;c28 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;as Event or RGB-D cameras, and can often layer multiple techniques to drive progress.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;The result?&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt; Whole scenes can be reconstructed non-rigidly and change spatio-temporally, e.g. a high-fidelity reconstruction of yourself, and your movements, updated in real-time.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;As identified previously, issues persist around the mapping of 2D images to 3D space. The following papers present a plethora of approaches to create high-fidelity, real-time reconstructions:&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;c18 lst-kix_qhe9txsqma08-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Fusion4D: Real-time Performance Capture of Challenging Scenes&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt111&quot; id=&quot;ftnt_ref111&quot; name=&quot;ftnt_ref111&quot;&gt;[111]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;veers towards the domain of Computer Graphics, however the interplay between Computer Vision and Graphics cannot be&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;overstated&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;. The authors’ approach uses&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;RGB-D&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt; and Segmentation as inputs to form a real-time, multi-view reconstruction which is outputted using Voxels.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Figure 13&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;: Fusion4D examples from real-time feed&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 image-margin&quot;&gt;&lt;span&gt;&lt;img alt=&quot;&quot; class=&quot;border cntr&quot; src=&quot;http://www.themtank.org/images/c-image13.jpg&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note from source&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: “&lt;/span&gt;&lt;span class=&quot;c7 c3&quot;&gt;We present a new method for real-time high quality 4D (i.e. spatio-temporally coherent) performance capture, allowing for incremental non-rigid reconstruction from noisy input from multiple RGBD cameras. Our system demonstrates unprecedented reconstructions of challenging non-rigid sequences, at real-time rates, including robust handling of large frame-to-frame motions and topology changes.&lt;/span&gt;&lt;span class=&quot;c8 c3&quot;&gt;”&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Dou et al. (2016, p. 1)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt112&quot; id=&quot;ftnt_ref112&quot; name=&quot;ftnt_ref112&quot;&gt;[112]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c3&quot;&gt;&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span class=&quot;c6&quot;&gt;Fusion4D creates real-time, high fidelity voxel representations which have impressive applications in virtual reality, augmented reality and telepresence. This work from Microsoft will likely revolutionise motion capture, possibly for live sports. An example of the technology in real-time use is available here:&lt;/span&gt; &lt;span class=&quot;c0&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://youtu.be/2dkcJ1YhYw4&quot;&gt;Video&lt;/a&gt;&lt;/span&gt; &lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt113&quot; id=&quot;ftnt_ref113&quot; name=&quot;ftnt_ref113&quot;&gt;[113]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;iframe class=&quot;utube-embed utube-center&quot; src=&quot;https://www.youtube.com/embed/2dkcJ1YhYw4&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;For an astounding example of telepresence/holoportation by Microsoft, see here:&lt;/span&gt; &lt;span class=&quot;c0&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://youtu.be/7d59O6cfaM0&quot;&gt;Video&lt;/a&gt;&lt;/span&gt; &lt;sup class=&quot;c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt114&quot; id=&quot;ftnt_ref114&quot; name=&quot;ftnt_ref114&quot;&gt;[114]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;iframe class=&quot;utube-embed utube-center&quot; src=&quot;https://www.youtube.com/embed/7d59O6cfaM0&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_qhe9txsqma08-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt115&quot; id=&quot;ftnt_ref115&quot; name=&quot;ftnt_ref115&quot;&gt;[115]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; won best paper at the European Convention on Computer Vision (ECCV) in 2016. The authors propose a novel algorithm capable of tracking 6D motion and various reconstructions in real-time using a single Event Camera.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c10 c2&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Figure 14&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;: Examples of the Real-Time 3D Reconstruction&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c10 image-margin&quot;&gt;&lt;span&gt;&lt;img alt=&quot;&quot; class=&quot;border cntr&quot; src=&quot;http://www.themtank.org/images/c-image5.jpg&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note from source&lt;/span&gt;&lt;span class=&quot;c8 c3&quot;&gt;: Demonstrations in various settings of the different aspects of our joint estimation algorithm. (a) visualisation of the input event stream; (b) estimated gradient keyframes; (c) reconstructed intensity keyframes with super resolution and high dynamic range properties; (d) estimated depth maps; (e) semi-dense 3D point clouds.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Source:&lt;/span&gt; &lt;span class=&quot;c3&quot;&gt;Kim et al. (2016, p. 12)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt116&quot; id=&quot;ftnt_ref116&quot; name=&quot;ftnt_ref116&quot;&gt;[116]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;The Event camera is gaining favour with researchers in Computer Vision due to its reduced latency, lower power consumption and higher dynamic range when compared to traditional cameras. Instead of a sequence of frames outputted by a regular camera, the event camera outputs “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;a stream of asynchronous spikes, each with pixel location, sign and precise timing, indicating when individual pixels record a threshold log intensity change.&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt117&quot; id=&quot;ftnt_ref117&quot; name=&quot;ftnt_ref117&quot;&gt;[117]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;For an explanation of event camera functionality, real-time 3D reconstruction and 6-DoF tracking, see the paper’s accompanying video here:&lt;/span&gt; &lt;span class=&quot;c0&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://www.youtube.com/watch?v=yHLyhdMSw7w&quot;&gt;Video&lt;/a&gt;&lt;/span&gt; &lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt118&quot; id=&quot;ftnt_ref118&quot; name=&quot;ftnt_ref118&quot;&gt;[118]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; &lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;iframe class=&quot;utube-embed utube-center&quot; src=&quot;https://www.youtube.com/embed/yHLyhdMSw7w&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c5&quot;&gt;This approach is incredibly impressive when one considers the real-time image rendering and depth estimation involved using a single view-point:&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;We propose a method which can perform real-time 3D reconstruction from a&lt;/span&gt; &lt;span class=&quot;c7 c6 c22&quot;&gt;single hand-held event camera&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt; with no additional sensing, and works in unstructured scenes of which it has no prior knowledge.&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;”&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_jiwkhisoudvb-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt119&quot; id=&quot;ftnt_ref119&quot; name=&quot;ftnt_ref119&quot;&gt;[119]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;proposes an unsupervised method for training a deep CNN for single view depth prediction with results comparable to SOTA using supervised methods. Traditional deep CNN approaches for single view depth prediction require large amounts of manually labelled data, however unsupervised methods again demonstrate their value by removing this necessity. The authors achieve this “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;by training the network in a manner analogous to an autoencoder&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”, using a stereo-rig.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class=&quot;c18 lst-kix_wbj7rym4u0qv-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;IM2CAD&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt120&quot; id=&quot;ftnt_ref120&quot; name=&quot;ftnt_ref120&quot;&gt;[120]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; describes the process of transferring an ‘image to CAD model’, CAD meaning computer-assisted design, which is a prominent method used to create 3D scenes for architectural depictions, engineering, product design and many other fields.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6 c14&quot;&gt;Given a single photo of a room and a large database of furniture CAD models, our goal is to reconstruct a scene that is as similar as possible to the scene depicted in the photograph, and composed of objects drawn from the database.&lt;/span&gt;&lt;span class=&quot;c5 c14&quot;&gt;”&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c5 c14&quot;&gt;The authors present an automatic system which ‘iteratively optimizes object placements and scales’ to best match input from real images. The rendered scenes validate against the original images using metrics trained using deep CNNs.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Figure 15&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;: Example of IM2CAD rendering bedroom scene&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 image-margin&quot;&gt;&lt;span&gt;&lt;img alt=&quot;&quot; class=&quot;border cntr&quot; src=&quot;http://www.themtank.org/images/c-image6.jpg&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note&lt;/span&gt; &lt;span class=&quot;c3&quot;&gt;: Left: input image. Right: Automatically created CAD model from input.&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note from source&lt;/span&gt;&lt;span class=&quot;c8 c3&quot;&gt;: The reconstruction results. In each example the left image is the real input image and the right image is the rendered 3D CAD model produced by IM2CAD.&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;c3 c12&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Izadinia et al. (2016, p. 10)&lt;/span&gt; &lt;sup class=&quot;c3 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt121&quot; id=&quot;ftnt_ref121&quot; name=&quot;ftnt_ref121&quot;&gt;[121]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c8 c3&quot;&gt; &lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt;Why care about IM2CAD?&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;The issue tackled by the authors is one of the first meaningful advancements on the techniques demonstrated by Lawrence Roberts in 1963, which allowed inference of a 3D scene from a photo using a known-object database, albeit in the very simple case of line drawings.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;While Robert’s method was visionary, more than a half century of subsequent research in Computer Vision has still not yet led to practical extensions of his approach that work reliably on realistic images and scenes.&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;”&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;The authors introduce a variant of the problem, aiming to reconstruct a high fidelity scene from a photo using ‘&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;objects taken from a database of 3D object models&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;’ for reconstruction.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;The process behind IM2CAD is quite involved and includes:&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;c18 lst-kix_d2qvshahiupy-0 start&quot;&gt;&lt;li class=&quot;c1 c13 c23&quot;&gt;&lt;span class=&quot;c6&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;Fully Convolutional Network that is trained end-to-end to find Geometric Features for&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;Room Geometry Estimation&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;c1 c13 c23&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Faster R-CNN for&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;Object Detection&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;c1 c13 c23&quot;&gt;&lt;span class=&quot;c6&quot;&gt;After finding the objects within the image,&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;CAD Model Alignment&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt; is completed to find the closest models within the ShapeNet repository for the detected objects. For example, the type of chair, given shape and approximate 3D pose. Each 3D model is rendered to 32 viewpoints which are then compared with the bounding box generated in object detection using&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;deep features&lt;/span&gt; &lt;sup class=&quot;c6 c12&quot;&gt;&lt;a class=&quot;a-word-wrap&quot; href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt122&quot; id=&quot;ftnt_ref122&quot; name=&quot;ftnt_ref122&quot;&gt;[122]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;c1 c13 c23&quot;&gt;&lt;span class=&quot;c6 c22&quot;&gt;Object Placement in the Scene&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;c1 c13 c23&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Finally Scene Optimization further refines the placement of the objects by optimizing the visual similarity between the camera views of the rendered scene and input image.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c20 c12 c6&quot;&gt;Again in this domain, ShapeNet proves invaluable:&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;First, we leverage ShapeNet, which contains millions of 3D models of objects, including thousands of different chairs, tables, and other household items. This dataset is a game changer for 3D scene understanding research, and was key to enabling our work.&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;”&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_wbj7rym4u0qv-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Learning Motion Patterns in Videos&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt123&quot; id=&quot;ftnt_ref123&quot; name=&quot;ftnt_ref123&quot;&gt;[123]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; proposes to solve the issue of determining&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;object motion independent of camera movement&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; using synthetic video sequences to teach the networks. “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;The core of our approach is a fully convolutional network, which is learnt entirely from synthetic video sequences, and their ground-truth optical flow and motion segmentation.&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;The authors test their approach on the new moving object segmentation dataset called DAVIS,&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt124&quot; id=&quot;ftnt_ref124&quot; name=&quot;ftnt_ref124&quot;&gt;[124]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; as well as the Berkeley motion segmentation dataset and achieve SOTA on both.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_wbj7rym4u0qv-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Deep Image Homography Estimation&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt125&quot; id=&quot;ftnt_ref125&quot; name=&quot;ftnt_ref125&quot;&gt;[125]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; comes from the Magic Leap team, a secretive US startup working in Computer Vision and Mixed Reality. The authors reclassify the task of homography estimation as ‘&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;a learning problem&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;’ and present two deep CNNs architectures which form “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;HomographyNet: a regression network which directly estimates the real-valued homography parameters, and a classification network which produces a distribution over quantized homographies.&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;”&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;The term homography comes from projective geometry and refers to a type of transformation that maps one plane to another. ‘&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;Estimating a 2D homography from a pair of images is a fundamental task in computer vision, and an essential part of monocular SLAM systems&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;’.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;The authors also provide a method for producing a “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;seemingly infinite dataset&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”, from existing datasets of real images such as MS-COCO, which offsets some of data requirements of deeper networks. They manage to create&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;a nearly unlimited number of labeled training examples by applying random projective transformations to a large image dataset&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;”.&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;c18 lst-kix_wbj7rym4u0qv-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;gvnn: Neural Network Library for Geometric Computer Vision&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt126&quot; id=&quot;ftnt_ref126&quot; name=&quot;ftnt_ref126&quot;&gt;[126]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; introduces a new neural network library for Torch, a popular computing framework for machine learning. Gvnn aims to ‘bridge the gap between classic geometric computer vision and deep learning’. The gvnn library allows developers to add geometric capabilities to their existing networks and training methods.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;In this work, we build upon the 2D transformation layers originally proposed in the spatial transformer networks and provide various novel extensions that perform geometric transformations which are often used in geometric computer vision.&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;”&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;This opens up applications in learning invariance to 3D geometric transformation for place recognition, end-to-end visual odometry, depth estimation and unsupervised learning through warping with a parametric transformation for image reconstruction error.&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;&quot;&lt;/span&gt;&lt;/p&gt;


&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Throughout this section we cut a swath across the field of 3D understanding, focusing primarily on the areas of&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;ose&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;stimation,&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;econstruction,&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;epth&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;stimation and&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;omography. But there is considerably more superb work which will go unmentioned by us, constrained as we are by volume. And so, we hope to have provided the reader with a valuable starting point, which is to say by no means an absolute.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;A large portion of the highlighted work may be classified under Geometric Vision, which generally deals with measuring real-world quantities like distances, shapes, areas and volumes directly from images. Our heuristic is that recognition-based tasks focus more on higher level semantic information than typically concerns applications in Geometric Vision. However, often we find that much of these different areas of 3D understanding are inextricably linked.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;&lt;br /&gt;One of the largest Geometric problems is that of simultaneous localisation and mapping (SLAM), with researchers considering whether SLAM will be in the next problems tackled by Deep Learning. Skeptics of the so-called ‘universality’ of deep learning, of which there are many, point to the importance and functionality of SLAM as an algorithm:&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;Visual SLAM algorithms are able to simultaneously build 3D maps of the world while tracking the location and orientation of the camera.&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt127&quot; id=&quot;ftnt_ref127&quot; name=&quot;ftnt_ref127&quot;&gt;[127]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; The geometric estimation portion of the SLAM approach is not currently suited to deep learning approaches and end-to-end learning remains unlikely. SLAM represents one of the most important algorithms in robotics and was designed with large input from the Computer Vision field. The technique has found its home in applications like Google Maps, autonomous vehicles, AR devices like Google Tango&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt128&quot; id=&quot;ftnt_ref128&quot; name=&quot;ftnt_ref128&quot;&gt;[128]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; and even the Mars Rover.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;That being said, Tomasz Malisiewicz delivers the anecdotal aggregate opinion of some prominent researchers on the issue, who agree “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;c7 c6&quot;&gt;semantics are necessary to build bigger and better SLAM systems.&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt129&quot; id=&quot;ftnt_ref129&quot; name=&quot;ftnt_ref129&quot;&gt;[129]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; This potentially shows promise for future applications of Deep Learning in the SLAM domain.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;We reached out to Mark Cummins, co-founder of Plink and Pointy, who provided us with his thoughts on the issue. Mark completed his PhD on SLAM techniques:&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6 c27&quot;&gt;The core geometric estimation part of SLAM is pretty well solved by the current approaches, but the high-level semantics and the lower-level system components can all benefit from deep learning. In particular:&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;br /&gt;&lt;ul class=&quot;c18 lst-kix_gi47s7ptc0lo-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c27 c7 c6&quot;&gt;Deep learning can greatly improve the quality of map semantics - i.e. going beyond poses or point clouds to a full understanding of the different kind of objects or regions in the map. This is much more powerful for many applications, and can also help with general robustness (for example through better handling of dynamic objects and environmental changes).&lt;br /&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;br /&gt;&lt;ul class=&quot;c18 lst-kix_xeitgb3brsv-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c27 c7 c6&quot;&gt;At a lower level, many components can likely be improved via deep learning. Obvious candidates are place recognition / loop closure detection / relocalization, better point descriptors for sparse SLAM methods, etc&lt;br /&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c7 c6&quot;&gt;Overall the structure of SLAM solvers probably remains the same, but the components improve. It is possible to imagine doing something radically new with deep learning, like throwing away the geometry entirely and have a more recognition-based navigation system. But for systems where the goal is a precise geometric map, deep learning in SLAM is likely more about improving components than doing something completely new.&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;”&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;In summation, we believe that SLAM is not likely to be completely&lt;/span&gt;&lt;span class=&quot;c28 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;replaced by Deep Learning. However, it is entirely likely that the two approaches may become complements to each other going forward. If you wish to learn more about SLAM, and its current SOTA, we wholeheartedly recommend Tomasz Malisiewicz’s blog for that task:&lt;/span&gt; &lt;span class=&quot;c0&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html&quot;&gt;The Future of Real-Time SLAM and Deep Learning vs SLAM&lt;/a&gt;&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt130&quot; id=&quot;ftnt_ref130&quot; name=&quot;ftnt_ref130&quot;&gt;[130]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;



&lt;span class=&quot;c25&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;c32 c6&quot;&gt;ConvNet architectures have recently found many novel applications outside of Computer Vision, some of which will feature in our forthcoming publications. However, they continue to feature prominently in&lt;/span&gt; &lt;span class=&quot;c32 c6&quot;&gt;Computer Vision, with architectural advancements providing improvements in speed, accuracy and training for many of the aforementioned applications and tasks in this paper.&lt;/span&gt; &lt;span class=&quot;c32&quot;&gt;&lt;/span&gt; &lt;span class=&quot;c5&quot;&gt;For this reason, ConvNet architectures are of fundamental importance to Computer Vision as a whole. The following features some noteworthy ConvNet architectures from 2016, many of which take inspiration from the recent success of ResNets.&lt;/span&gt;
&lt;ul class=&quot;c18 lst-kix_jiwkhisoudvb-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt131&quot; id=&quot;ftnt_ref131&quot; name=&quot;ftnt_ref131&quot;&gt;[131]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; - present Inception v4, a new Inception architecture which builds on the Inception v2 and v3 from the end of 2015.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt132&quot; id=&quot;ftnt_ref132&quot; name=&quot;ftnt_ref132&quot;&gt;[132]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; The paper also provides an analysis of using residual connections for training Inception Networks along with some Residual-Inception hybrid networks.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_nfilvffzvtk4-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Densely Connected Convolutional Networks&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt133&quot; id=&quot;ftnt_ref133&quot; name=&quot;ftnt_ref133&quot;&gt;[133]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; or&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; “&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;DenseNets&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;” take direct inspiration from the identity/skip connections of ResNets. The approach extends this concept to ConvNets by having each layer connect to every other layer in a feed forward fashion, sharing feature maps from previous layers as inputs, thus creating DenseNets.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt134&quot; id=&quot;ftnt_ref134&quot; name=&quot;ftnt_ref134&quot;&gt;[134]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; &lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Figure 16&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;: Example of DenseNet Architecture&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c10 c2 image-margin&quot;&gt; &lt;span&gt;&lt;img alt=&quot;&quot; class=&quot;border cntr&quot; src=&quot;http://www.themtank.org/images/image14.png&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: A 5-layer dense block with a growth rate of&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;k = 4&lt;/span&gt;&lt;span class=&quot;c8 c3&quot;&gt;. Each layer takes all preceding feature-maps as input.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Huang et al. (2016)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt135&quot; id=&quot;ftnt_ref135&quot; name=&quot;ftnt_ref135&quot;&gt;[135]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;The model was evaluated on CIFAR-10, CIFAR-100, SVHN and ImageNet; it achieved SOTA on a number of them. Impressively, DenseNets achieve these results while using less memory and with reduced computational requirements. There are multiple implementations (Keras, Tensorflow, etc)&lt;/span&gt; &lt;span class=&quot;c0&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://github.com/liuzhuang13/DenseNet&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt136&quot; id=&quot;ftnt_ref136&quot; name=&quot;ftnt_ref136&quot;&gt;[136]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_bgc1ik7oba91-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;FractalNet&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c12 c6&quot;&gt;Ultra-Deep Neural Networks without Residuals&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt137&quot; id=&quot;ftnt_ref137&quot; name=&quot;ftnt_ref137&quot;&gt;[137]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt; utilises interacting subpaths of different lengths, without pass-through or residual connections, instead altering internal signals using filter and nonlinearities for transformations.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;FractalNets repeatedly combine several parallel layer sequences with different numbers of convolutional blocks to obtain a large nominal depth, while maintaining many short paths in the network&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt138&quot; id=&quot;ftnt_ref138&quot; name=&quot;ftnt_ref138&quot;&gt;[138]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c5&quot;&gt;The network achieved SOTA performance on CIFAR and ImageNet, while demonstrating some additional properties. For instance, they call into question the role of residuals in the success of extremely deep ConvNets, while also providing insight into the nature of answers attained by various subnetwork depths.&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_nfilvffzvtk4-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Lets keep it simple: using simple architectures to outperform deeper architectures&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt139&quot; id=&quot;ftnt_ref139&quot; name=&quot;ftnt_ref139&quot;&gt;[139]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; focuses on creating a simplified mother architecture. The architecture achieved SOTA results, or parity with existing approaches, on ‘datasets such as CIFAR10/100, MNIST and SVHN with simple or no data-augmentation’. We feel their exact words provide the best description of the motivation here:&lt;br /&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c27 c7 c6&quot;&gt;In this work, we present a very simple fully convolutional network architecture of 13 layers, with minimum reliance on new features which outperforms almost all deeper architectures with 2 to 25 times fewer parameters. Our architecture can be a very good candidate for many scenarios, especially for use in embedded devices.”&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c7 c6&quot;&gt;“It can be furthermore compressed using methods such as DeepCompression and thus its memory consumption can be decreased drastically. We intentionally tried to create a mother architecture with minimum reliance on new features proposed recently, to show the effectiveness of a well-crafted yet simple convolutional architecture which can then later be enhanced with existing or new methods presented in the literature.&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt140&quot; id=&quot;ftnt_ref140&quot; name=&quot;ftnt_ref140&quot;&gt;[140]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;Here are some additional techniques which complement ConvNet Architectures:&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_nfilvffzvtk4-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Swapout: Learning an ensemble of deep architectures&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt141&quot; id=&quot;ftnt_ref141&quot; name=&quot;ftnt_ref141&quot;&gt;[141]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; generalises dropout and stochastic depth methods to prevent co-adaptation of units, both in a specific layer and across network layers. The ensemble training method samples from multiple architectures including “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;dropout, stochastic depth and residual architectures&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;”. Swapout outperforms ResNets of identical network structure on the CIFAR-10 and CIFAR-100 and can be classified as a regularisation technique.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_nfilvffzvtk4-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;SqueezeNet&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt142&quot; id=&quot;ftnt_ref142&quot; name=&quot;ftnt_ref142&quot;&gt;[142]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;posits that smaller DNNs offer various benefits, from less computationally taxing training to easier information transmission to, and operation on, devices with limited storage or processing power. SqueezeNet is a small DNN architecture which achieves ‘AlexNet-level accuracy with significantly reduced parameters and memory requirements using model compression techniques which make it 510x smaller than AlexNet.’&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;A Rectified Linear Unit (ReLU) is traditionally the dominant activation function for all Neural Networks. However, here are some recent alternatives:&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;c18 lst-kix_kg9o9flo8md3-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Concatenated Rectified Linear Units (CRelu)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt143&quot; id=&quot;ftnt_ref143&quot; name=&quot;ftnt_ref143&quot;&gt;[143]&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Exponential Linear Units (ELUs)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt144&quot; id=&quot;ftnt_ref144&quot; name=&quot;ftnt_ref144&quot;&gt;[144]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; from the close of 2015&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Parametric Exponential Linear Unit (PELU)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt145&quot; id=&quot;ftnt_ref145&quot; name=&quot;ftnt_ref145&quot;&gt;[145]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c20 c12 c6&quot;&gt;Moving towards equivariance in ConvNets&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;ConvNets are translation invariant - meaning they can identify the same features in multiple parts of an image. However, the typical CNN isn’t rotation invariant - meaning that if a feature or the whole image is rotated then the network’s performance suffers. Usually ConvNets learn to (sort of) deal with rotation invariance through data augmentation (e.g. purposefully rotating the images by small random amounts during training). This means the network gains slight rotation invariant properties without specifically designing rotation invariance into the network. This means that rotation invariance is fundamentally limited in networks using current techniques. This is an interesting parallel with humans who also typically fare worse at recognising characters upside down, although there is no reason for machines to suffer this limitation.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;The following papers&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;tackle&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c12 c6&quot;&gt;rotation-invariant ConvNets&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;. While each approach has novelties, they all improve rotation invariance through more efficient parameter usage leading to eventual global rotation equivariance:&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;c18 lst-kix_t4jp9ig9481p-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Harmonic CNNs&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt146&quot; id=&quot;ftnt_ref146&quot; name=&quot;ftnt_ref146&quot;&gt;[146]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; replace regular CNN filters with ‘circular harmonics’.&lt;br /&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Group Equivariant Convolutional Networks (G-CNNs)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt147&quot; id=&quot;ftnt_ref147&quot; name=&quot;ftnt_ref147&quot;&gt;[147]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;uses G-Convolutions, which&lt;/span&gt;&lt;span class=&quot;c28 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;are a new type of layer that “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;enjoys a substantially higher degree of weight sharing than regular convolution layers and increases the expressive capacity of the network without increasing the number of parameters.&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;”&lt;br /&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Exploiting Cyclic Symmetry in Convolutional Neural Networks&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt148&quot; id=&quot;ftnt_ref148&quot; name=&quot;ftnt_ref148&quot;&gt;[148]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;presents four operations as layers which augment neural network layers to partially increase rotational equivariance.&lt;br /&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Steerable CNNs&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt149&quot; id=&quot;ftnt_ref149&quot; name=&quot;ftnt_ref149&quot;&gt;[149]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;c12 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;Cohen and Welling build on the work they did with&lt;/span&gt; &lt;span class=&quot;c12 c6&quot;&gt;G-CNNs&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;, demonstrating that “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;steerable architectures”&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;outperform residual and dense networks&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt; on the CIFARs. They also provide a succinct overview of the invariance problem:&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;To improve the statistical efficiency of machine learning methods, many have sought to learn invariant representations. In deep learning, however, intermediate layers should not be fully invariant, because the relative pose of local features must be preserved for further layers. Thus, one is led to the idea of&lt;/span&gt; &lt;span class=&quot;c7 c12 c6&quot;&gt;equivariance&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;: a network is equivariant if the representations it produces transform in a predictable linear manner under transformations of the input. In other words, equivariant networks produce representations that are steerable. Steerability makes it possible to apply filters not just in every position (as in a standard convolution layer), but in every pose, thus allowing for increased parameter sharing.&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;c6 c45&quot;&gt;107&lt;/span&gt;&lt;/p&gt;



&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c20 c12 c25&quot;&gt;Residual Networks&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Figure 17&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;: Test-Error Rates on CIFAR Datasets&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c10 image-margin&quot;&gt;&lt;span&gt;&lt;img alt=&quot;Screenshot 2017-03-07 15.37.19.png&quot; src=&quot;http://www.themtank.org/images/c-image2.jpg&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Yellow highlight indicates that these papers feature within this piece. Pre-resnet refers to &quot;&lt;/span&gt;&lt;span class=&quot;c7 c3&quot;&gt;Identity Mappings in Deep Residual Networks&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;&quot; (see following section). Furthermore, while not included in the table we believe that&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c3 c12&quot;&gt;Learning Identity Mappings with Residual Gates&lt;/span&gt;&lt;span class=&quot;c8 c3&quot;&gt;” produced some of the lowest error rates of 2016 with 3.65% and 18.27% on CIFAR-10 and CIFAR-100, respectively.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Abdi and Nahavandi (2016, p. 6)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt150&quot; id=&quot;ftnt_ref150&quot; name=&quot;ftnt_ref150&quot;&gt;[150]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Residual Networks and their variants became incredibly popular in 2016, following the success of Microsoft’s ResNet,&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt151&quot; id=&quot;ftnt_ref151&quot; name=&quot;ftnt_ref151&quot;&gt;[151]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; with many open source versions and pre-trained models now available. In 2015, ResNet won 1&lt;/span&gt;&lt;span class=&quot;c6 c45&quot;&gt;st&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;place in ImageNet’s Detection, Localisation and Classification tasks as well as in COCO’s Detection and Segmentation challenges.&lt;/span&gt;&lt;span class=&quot;c28 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;Although questions still abound about depth, ResNets tackling of the vanishing gradient problem provided more impetus for the “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;increased depth produces superior abstraction&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;” philosophy which underpins much of Deep Learning at present.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;ResNets are often conceptualised as an ensemble of shallower networks, which somewhat counteract the hierarchical nature of Deep Neural Networks (DNNs) by running shortcut connections parallel to their convolutional layers. These shortcuts or&lt;/span&gt; &lt;span class=&quot;c7 c12 c6&quot;&gt;skip connections&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; mitigate vanishing/exploding gradient problems associated with DNNs, by allowing easier back-propagation of gradients throughout the network layers. For more information there is a Quora thread available&lt;/span&gt; &lt;span class=&quot;c0&quot;&gt;&lt;a class=&quot;c4&quot; href=&quot;https://www.quora.com/What-is-an-intuitive-explanation-of-Deep-Residual-Networks&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt152&quot; id=&quot;ftnt_ref152&quot; name=&quot;ftnt_ref152&quot;&gt;[152]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;


&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Residual Learning&lt;/span&gt;&lt;span class=&quot;c20 c12 c6&quot;&gt;, Theory and Improvements&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_jiwkhisoudvb-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Wide Residual Networks&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt153&quot; id=&quot;ftnt_ref153&quot; name=&quot;ftnt_ref153&quot;&gt;[153]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; is now an extremely common ResNet approach. The authors conduct an experimental study on the architecture of ResNet blocks, and improve residual network performance by increasing the width and reducing the depth of the networks, which mitigates the diminishing feature reuse problem. This approach produces new SOTA on multiple benchmarks including 3.89% and 18.3% on CIFAR-10 and CIFAR-100 respectively. The authors show that a ‘&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;16-layer-deep wide ResNet performs as well or better in accuracy and efficiency than many other ResNets (including 1000 layer networks)’.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_jiwkhisoudvb-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Deep Networks with Stochastic Depth&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt154&quot; id=&quot;ftnt_ref154&quot; name=&quot;ftnt_ref154&quot;&gt;[154]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; essentially applies dropout to whole layers of neurons instead of to bunches of&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;individual&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;neurons. “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function.&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;”&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;Stochastic depth allows quicker training and better accuracy even when training networks greater than 1200 layers.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_jiwkhisoudvb-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Learning Identity Mappings with Residual Gates&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt155&quot; id=&quot;ftnt_ref155&quot; name=&quot;ftnt_ref155&quot;&gt;[155]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; -&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;by using a scalar parameter to control each gate, we provide a way to learn identity mappings by optimizing only one parameter.&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;” The authors use these Gated ResNets to improve the optimisation of deep models, while providing ‘high tolerance to full layer removal’ such that 90% of performance remains following significant removal at random. Using Wide Gated ResNets the model achieves 3.65% and 18.27% error on CIFAR- 10 and CIFAR-100, respectively.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_jiwkhisoudvb-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Residual Networks Behave Like Ensembles of Relatively Shallow Networks&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt156&quot; id=&quot;ftnt_ref156&quot; name=&quot;ftnt_ref156&quot;&gt;[156]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;c5&quot;&gt;ResNets can be viewed as collections of many paths, which don’t strongly depend upon one another and hence reinforce the notion of ensemble behaviour. Furthermore, residual pathways vary in length with the short paths contributing to gradient during training while the deeper paths don’t factor in this stage.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_jiwkhisoudvb-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Identity Mappings in Deep Residual Networks&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt157&quot; id=&quot;ftnt_ref157&quot; name=&quot;ftnt_ref157&quot;&gt;[157]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; comes as an improvement from the original Resnet authors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Identity mappings are shown to allow ‘forward and backward signals to be propagated between any ResNet block when used as the skip connections and after-addition activation’. The approach improves generalisation, training and results “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;”&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_jiwkhisoudvb-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;c12 c6&quot;&gt;ulti-Residual Networks: Improving the Speed and Accuracy of Residual Networks&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt158&quot; id=&quot;ftnt_ref158&quot; name=&quot;ftnt_ref158&quot;&gt;[158]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; again advocates for the ensemble behaviour of ResNets and favours a wider-over-deeper approach to ResNet architecture. “&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;The proposed multi-residual network increases the number of residual functions in the residual blocks.&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;” Improved accuracy produces 3.73% and 19.45% error on CIFAR-10 and CIFAR-100, respectively. The table presented in Fig. 17 was taken from this paper, and more up-to-date versions are available which consider the work produced in 2017 thus far.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Other residual theory and improvements&lt;/span&gt;&lt;span class=&quot;c12 c28 c6&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;Although a relatively recent idea, there is quite a considerable body of work being created around ResNets presently. The following represents some additional theories and improvements which we wished to highlight for interested readers:&lt;/span&gt;&lt;/p&gt;



&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6 c26 c14&quot;&gt;The significance of rich datasets for all facets of machine learning cannot be overstated. Hence, we feel it is prudent to include some of the largest advancements in this domain. To paraphrase Ben Hamner, the&lt;/span&gt; &lt;span class=&quot;c6 c14&quot;&gt;CTO and co-founder of&lt;/span&gt;&lt;span class=&quot;c28 c6 c14&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6 c26 c14&quot;&gt;Kaggle, ‘&lt;/span&gt;&lt;span class=&quot;c7 c6&quot;&gt;a new dataset can make a thousand papers flourish&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;’&lt;/span&gt;&lt;span class=&quot;c6 c26 c14&quot;&gt;,&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt168&quot; id=&quot;ftnt_ref168&quot; name=&quot;ftnt_ref168&quot;&gt;[168]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;that is to say the availability of data can promote new approaches, as well as breath new life into previously ineffectual techniques.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6 c26 c14&quot;&gt;In 2016, traditional datasets such as ImageNet&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt169&quot; id=&quot;ftnt_ref169&quot; name=&quot;ftnt_ref169&quot;&gt;[169]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6 c26 c14&quot;&gt;, Common Objects in Context (COCO)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt170&quot; id=&quot;ftnt_ref170&quot; name=&quot;ftnt_ref170&quot;&gt;[170]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6 c26 c14&quot;&gt;, the CIFARs&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt171&quot; id=&quot;ftnt_ref171&quot; name=&quot;ftnt_ref171&quot;&gt;[171]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6 c26 c14&quot;&gt; and MNIST&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt172&quot; id=&quot;ftnt_ref172&quot; name=&quot;ftnt_ref172&quot;&gt;[172]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6 c26 c14&quot;&gt; were joined by a host of new entries. We also noted the rise of synthetic datasets spurred on by progress in graphics. Synthetic datasets are an interesting work-around of the large data requirements for Artificial Neural Networks (ANNs). In the interest of brevity, we have selected our (subjective) most important&lt;/span&gt; &lt;span class=&quot;c7 c6 c26 c14&quot;&gt;new&lt;/span&gt;&lt;span class=&quot;c29 c6 c26 c14&quot;&gt; datasets for 2016:&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_d8mii7sh8xk5-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt;Places2&lt;/span&gt;&lt;sup class=&quot;c12 c6 c14&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt173&quot; id=&quot;ftnt_ref173&quot; name=&quot;ftnt_ref173&quot;&gt;[173]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6 c14&quot;&gt; is a scene classification dataset, i.e. the task is to label an image with a scene class like ‘Stadium’, ‘Park’, etc. While prediction models and image understanding will undoubtedly be improved by the Places2 dataset, an interesting finding from networks that are trained on this dataset is that in the process of learning to classify scenes, the network learns to detect objects in them&lt;/span&gt; &lt;span class=&quot;c6 c14 c22&quot;&gt;without ever being explicitly taught this&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt;. For example, that bedrooms contain beds and that sinks can be in both kitchens and bathrooms. This means that&lt;/span&gt; &lt;span class=&quot;c6 c14 c22&quot;&gt;the objects themselves&lt;/span&gt;&lt;span class=&quot;c5 c14&quot;&gt; are lower level features in the abstraction hierarchy for the classification of scenes.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt;Figure 18&lt;/span&gt;&lt;span class=&quot;c5 c14&quot;&gt;: Examples from SceneNet RGB-D&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c10 image-margin&quot;&gt;&lt;span&gt;&lt;img alt=&quot;&quot; class=&quot;border cntr&quot; src=&quot;http://www.themtank.org/images/c-image15.jpg&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12 c26 c14&quot;&gt;Note&lt;/span&gt;&lt;span class=&quot;c3 c26 c14&quot;&gt;: Examples taken from SceneNet RGB-D, a dataset with 5M Photorealistic Images of Synthetic Indoor Trajectories with Ground Truth. The photo (a) is rendered through computer graphics with available ground truth for specific tasks from (b) to (e). Creation of synthetic datasets should aid the process of domain adaptation.&lt;/span&gt; &lt;span class=&quot;c8 c3 c14&quot;&gt;Synthetic datasets are somewhat pointless if the knowledge learned from them cannot be applied to the real world. This is where domain adaptation comes in, which refers to this transfer learning process of moving knowledge from one domain to another, e.g. from synthetic to real-world environments. Domain adaptation has recently been improving very rapidly again highlighting the recent efforts in transfer learning. Columns (c) vs (d) show the difference between instance and semantic/class segmentation.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12 c26 c14&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3 c26 c14&quot;&gt;: McCormac et al. (2017)&lt;/span&gt;&lt;sup class=&quot;c3 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt174&quot; id=&quot;ftnt_ref174&quot; name=&quot;ftnt_ref174&quot;&gt;[174]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_d8mii7sh8xk5-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt;SceneNet RGB-D&lt;/span&gt;&lt;sup class=&quot;c12 c6 c14&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt175&quot; id=&quot;ftnt_ref175&quot; name=&quot;ftnt_ref175&quot;&gt;[175]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c5 c14&quot;&gt;- This synthetic dataset expands on the original SceneNet dataset and provides pixel-perfect ground truth for scene understanding problems such as semantic segmentation, instance segmentation, and object detection, and also for geometric computer vision problems such as optical flow, depth estimation, camera pose estimation, and 3D reconstruction. The dataset granularizes the chosen environment by providing pixel-perfect representations.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_d8mii7sh8xk5-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt;CMPlaces&lt;/span&gt;&lt;sup class=&quot;c12 c6 c14&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt176&quot; id=&quot;ftnt_ref176&quot; name=&quot;ftnt_ref176&quot;&gt;[176]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5 c14&quot;&gt; is a cross-modal scene dataset from MIT. The task is to recognize scenes across many different modalities beyond natural images and in the process hopefully transfer that knowledge across modalities too. Some of the modalities are: Real, Clip Art, Sketches, Spatial Text (words written which correspond to spatial locations of objects) and natural language descriptions. The paper also discusses methods for how to deal with this type of problem with cross-modal convolutional neural networks.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt;Figure 19&lt;/span&gt;&lt;span class=&quot;c5 c14&quot;&gt;: CMPlaces cross-modal scene representations&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1 image-margin&quot;&gt;&lt;span&gt;&lt;img alt=&quot;&quot; class=&quot;border cntr&quot; src=&quot;http://www.themtank.org/images/c-image3-40.jpg&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;bullet-point-margin&quot; readability=&quot;13.802259887006&quot;&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12 c26 c14&quot;&gt;Note&lt;/span&gt;&lt;span class=&quot;c29 c3 c26 c14&quot;&gt;: Taken from the CMPlaces paper showing two examples, bedrooms and kindergarten classrooms,   across different modalities. Conventional Neural Network approaches learn representations that don’t transfer well across modalities and this paper attempts to generate a shared representation “agnostic of modality”.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12 c26 c14&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3 c26 c14&quot;&gt;: Aytar et al. (2016)&lt;/span&gt;&lt;sup class=&quot;c3 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt177&quot; id=&quot;ftnt_ref177&quot; name=&quot;ftnt_ref177&quot;&gt;[177]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6 c14&quot;&gt;In CMPlaces we see explicit mention of transfer learning, domain invariant representations, domain adaptation and multi-modal learning, all of which serve to demonstrate further the current undertow of Computer Vision research. The authors focus on trying to find&lt;/span&gt; &lt;span class=&quot;c6 c14&quot;&gt;“&lt;/span&gt;&lt;span class=&quot;c7 c6 c14&quot;&gt;domain/modality-independent representations&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt;”, which could correspond to the higher level abstractions where humans draw their unified representations from. For instance take ‘cat’ across its various modalities, humans see the word ‘cat’ in writing, a picture drawn in a sketchbook, a real world-image or mentioned in speech but we still have the same unified representation abstracted at a higher level above these modalities.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c7 c6 c14&quot;&gt;“Humans are able to leverage knowledge and experiences independently of the modality they perceive it in, and a similar capability in machines would enable several important applications in retrieval and recognition”&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;c29 c28 c6 c14&quot;&gt; &lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_d8mii7sh8xk5-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt;MS-Celeb-1M&lt;/span&gt;&lt;sup class=&quot;c12 c6 c14&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt178&quot; id=&quot;ftnt_ref178&quot; name=&quot;ftnt_ref178&quot;&gt;[178]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6 c14&quot;&gt; contains images of one million celebrities with ten million training images in a training set for Facial Recognition.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_d8mii7sh8xk5-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt;Open Images&lt;/span&gt;&lt;sup class=&quot;c12 c6 c14&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt179&quot; id=&quot;ftnt_ref179&quot; name=&quot;ftnt_ref179&quot;&gt;[179]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6 c14&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt;comes courtesy of Google Inc. and comprises ~9 million URLs to images complete with multiple labels, a vast improvement over typical single label images. Open images spans 6000 categories, a large improvement over the 1000 classes offered previously by ImageNet (with less focus on canines) and should prove indispensable to the Machine Learning community.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_d8mii7sh8xk5-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt;YouTube-8M&lt;/span&gt;&lt;sup class=&quot;c12 c6 c14&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt180&quot; id=&quot;ftnt_ref180&quot; name=&quot;ftnt_ref180&quot;&gt;[180]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c12 c6 c14&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt;also comes courtesy of Google with 8 million video URLs, 500,000 hours of video, 4800 classes, Avg. 1.8 Labels per video. Some examples of the labels are: ‘Arts &amp;amp; Entertainment’, ‘Shopping’ and ‘Pets &amp;amp; Animals’. Video datasets are much more difficult to label and collect hence the massive value this dataset provides.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6 c14&quot;&gt;That being said, a&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt;dvancements in image understanding, such as segmentation, object classification and detection have brought video understanding to the fore of research. However, prior to this dataset release there was a real lack in the variety and scale of real-world video datasets available.&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt; Furthermore, this dataset was just recently updated,&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt181&quot; id=&quot;ftnt_ref181&quot; name=&quot;ftnt_ref181&quot;&gt;[181]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6 c14&quot;&gt; and this year in association with Kaggle, Google&lt;/span&gt; &lt;span class=&quot;c6 c14&quot;&gt;is&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt; organis&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt;ing&lt;/span&gt;&lt;span class=&quot;c6 c14&quot;&gt; a video understanding competition as part of CVPR 2017.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt182&quot; id=&quot;ftnt_ref182&quot; name=&quot;ftnt_ref182&quot;&gt;[182]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6 c14&quot;&gt;General information about YouTube-8M:&lt;/span&gt; &lt;span class=&quot;c0 c14&quot;&gt;&lt;a class=&quot;c4&quot; href=&quot;https://research.google.com/youtube8m/&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt183&quot; id=&quot;ftnt_ref183&quot; name=&quot;ftnt_ref183&quot;&gt;[183]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;


&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c32 c6&quot;&gt;As this piece draws to a close, we lament the limitations under which we had to construct it. Indeed, the field of Computer Vision is too expansive to cover in any real, meaningful depth, and as such many omissions were made. One such omission is, unfortunately, almost everything that&lt;/span&gt; &lt;span class=&quot;c6 c22 c32&quot;&gt;didn’t use Neural Networks&lt;/span&gt;&lt;span class=&quot;c32 c6&quot;&gt;. We know there is great work outside of NNs, and we acknowledge our own biases, but we feel that the impetus lies with these approaches currently, and our subjective selection of material for inclusion was predominantly based on the reception received from the research community at large&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; (and the results speak for themselves)&lt;/span&gt;&lt;span class=&quot;c32 c6&quot;&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c32 c6&quot;&gt;We would also like to&lt;/span&gt; &lt;span class=&quot;c32 c6 c22&quot;&gt;stress&lt;/span&gt;&lt;span class=&quot;c32 c6&quot;&gt; that there are&lt;/span&gt; &lt;span class=&quot;c32 c6 c22&quot;&gt;hundreds of other papers&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt; in the above topics, and this amalgam of topics is not curated as a definitive, but rather hopes to encourage interested parties to read further along the entrances we provide. As such, this final section acts as a catch all for some of the other applications we loved, trends we wished to highlight and justifications we wanted to make to the reader.&lt;/span&gt;&lt;/p&gt;


&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c20 c12 c6&quot;&gt;Applications/use cases&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;c18 lst-kix_1yf6lzm6ivvf-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Applications for the blind from Facebook&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt184&quot; id=&quot;ftnt_ref184&quot; name=&quot;ftnt_ref184&quot;&gt;[184]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; and hardware from Baidu.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt185&quot; id=&quot;ftnt_ref185&quot; name=&quot;ftnt_ref185&quot;&gt;[185]&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_1yf6lzm6ivvf-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Emotion detection combines facial detection and semantic analysis, and is growing rapidly. There are 20+ APIs currently available.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt186&quot; id=&quot;ftnt_ref186&quot; name=&quot;ftnt_ref186&quot;&gt;[186]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Extracting roads from aerial imagery,&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt187&quot; id=&quot;ftnt_ref187&quot; name=&quot;ftnt_ref187&quot;&gt;[187]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; land use classification from aerial maps and population density maps.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt188&quot; id=&quot;ftnt_ref188&quot; name=&quot;ftnt_ref188&quot;&gt;[188]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_1yf6lzm6ivvf-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Amazon Go further raised the profile of Computer Vision by demonstrating a&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;queue-less shopping experience&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;,&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt189&quot; id=&quot;ftnt_ref189&quot; name=&quot;ftnt_ref189&quot;&gt;[189]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; although there remain some functional issues at present.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt190&quot; id=&quot;ftnt_ref190&quot; name=&quot;ftnt_ref190&quot;&gt;[190]&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_1yf6lzm6ivvf-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c6&quot;&gt;There is a&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;huge volume&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;of work being done for Autonomous Vehicles that we largely didn’t touch. However, for those wishing to delve into general market trends, there’s an excellent piece by Moritz Mueller-Freitag of Twenty Billion Neurons about the German auto industry and the impact of autonomous vehicles.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt191&quot; id=&quot;ftnt_ref191&quot; name=&quot;ftnt_ref191&quot;&gt;[191]&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_1yf6lzm6ivvf-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Other interesting areas: Image Retrieval/Search,&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt192&quot; id=&quot;ftnt_ref192&quot; name=&quot;ftnt_ref192&quot;&gt;[192]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; Gesture Recognition, Inpainting and Facial Reconstruction.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_1yf6lzm6ivvf-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c5&quot;&gt;There is considerable work around Digital Imaging and Communications in Medicine (DICOM) and other medical applications, especially related to imaging. For instance, there have been (and still are) numerous Kaggle detection competitions (lung cancer, cervical cancer), some with large monetary incentives, in which algorithms attempt to outperform specialists at the classification/detection tasks in question.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;However, while work continues on improving the error rates of these algorithms their value as a tool for medical practitioners appears increasingly evident. This is&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;particularly striking&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; when we consider the performance improvements in breast cancer detection achieved by&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;combining AI systems&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt193&quot; id=&quot;ftnt_ref193&quot; name=&quot;ftnt_ref193&quot;&gt;[193]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; with&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;medical specialists&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt194&quot; id=&quot;ftnt_ref194&quot; name=&quot;ftnt_ref194&quot;&gt;[194]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; In this instance, robot-human symbiosis produces accuracy far greater than the sum of its parts at 99.5%.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;This is just one example of the torrent of medical applications currently being pursued by the deep learning/machine learning communities. Some cynical members of our team jokingly make light of these attempts as a means to ingratiate society to the idea of AI research as a ubiquitous, benevolent force. But as long as the technology helps the healthcare industry, and it is introduced in a safe and considered manner, we wholeheartedly welcome such advances.&lt;/span&gt;&lt;/p&gt;


&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c20 c12 c6&quot;&gt;Hardware/markets&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;c18 lst-kix_1yf6lzm6ivvf-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Growing markets for Robotic Vision/Machine Vision (separate fields) and potential target markets for IoT. A personal favourite of ours is the use of Deep Learning, a Raspberry Pi and TensorFlow by a farmer’s son to sort cucumbers in Japan based on unique producer heuristics for quality, e.g. shape, size and colour.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt195&quot; id=&quot;ftnt_ref195&quot; name=&quot;ftnt_ref195&quot;&gt;[195]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;This produced massive decreases in human-time spent by his mother sorting cucumbers.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_1yf6lzm6ivvf-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c6&quot;&gt;The trend of shrinking compute requirements and migrating to mobile is evident,  but it’s also complemented by steep hardware acceleration. Soon we’ll see pocket sized CNNs and Vision Processing Units (VPUs) everywhere. For instance, the Movidius Myriad2 is used in Google’s Project Tango and drones.&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt196&quot; id=&quot;ftnt_ref196&quot; name=&quot;ftnt_ref196&quot;&gt;[196]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; &lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;iframe class=&quot;utube-embed utube-center&quot; src=&quot;https://www.youtube.com/embed/hX0UELNRR1I&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/p&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;The Movidius Fathom stick,&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt197&quot; id=&quot;ftnt_ref197&quot; name=&quot;ftnt_ref197&quot;&gt;[197]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt; which also uses the Myriad2’s technology, allows users to add SOTA Computer Vision performance to consumer devices.&lt;/span&gt; &lt;span class=&quot;c5&quot;&gt;The Fathom stick, which has the physical properties of a USB stick, brings the power of a Neural Network to almost any device: Brains on a stick.&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_1yf6lzm6ivvf-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c5&quot;&gt;Sensors and systems that use something other than visible light. Examples include radar, thermographic cameras, hyperspectral imaging, sonar, magnetic resonance imaging, etc.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_1yf6lzm6ivvf-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Reduction in cost of LIDAR, which use light and radar to measure distances, and offer many advantages over normal RGB cameras. There are many LIDAR devices for&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;currently&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;less than&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt; $500.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_1yf6lzm6ivvf-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Hololens and the near-countless other Augmented Reality headsets&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt198&quot; id=&quot;ftnt_ref198&quot; name=&quot;ftnt_ref198&quot;&gt;[198]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; entering the market.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_1yf6lzm6ivvf-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Project Tango by Google&lt;/span&gt;&lt;sup class=&quot;c12 c6&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt199&quot; id=&quot;ftnt_ref199&quot; name=&quot;ftnt_ref199&quot;&gt;[199]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c5&quot;&gt; represents the next big commercialisation of SLAM. Tango is an augmented reality computing platform, comprising both novel software and hardware. Tango allows the detection of mobile device position, relative to the world, without the use of GPS or other external information while simultaneously mapping the area around the device in 3D.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Corporate partners Lenovo brought affordable Tango enabled phones to market in 2016, allowing hundreds of developers to begin creating applications for the platform. Tango employs the following software&lt;/span&gt;&lt;span class=&quot;c28 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;technologies: Motion Tracking, Area Learning, and Depth Perception.&lt;/span&gt;&lt;/p&gt;


&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c20 c12 c6&quot;&gt;Omissions based on forthcoming publications&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;There is also considerable, and increasing overlap between Computer Vision techniques and other domains in Machine Learning and Artificial Intelligence. These other domains and hybrid use cases are the subject of The M Tank’s forthcoming publications and, as with the whole of this piece, we partitioned content based on our own heuristics.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;For instance, we decided to place the two integral Computer Vision tasks, Image Captioning&lt;/span&gt;&lt;span class=&quot;c28 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;and Visual Question Answering, in our forthcoming NLP piece along with Visual Speech Recognition because of the combination of CV and NLP involved. Whereas the application of Generative Models to images we place in our work on Generative Models. Examples included in these future works are:&lt;/span&gt;&lt;/p&gt;

&lt;ul class=&quot;c18 lst-kix_olf35kpnlxqs-0 start&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Lip Reading&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;: In 2016 we saw huge lip reading advancements in programs such as LipNet&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt200&quot; id=&quot;ftnt_ref200&quot; name=&quot;ftnt_ref200&quot;&gt;[200]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt;, which combine Computer Vision and NLP into&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;Visual Speech Recognition&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;c18 lst-kix_1yf6lzm6ivvf-0&quot;&gt;&lt;li class=&quot;c1 c2 c13&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Generative models&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt; applied to images will feature as part of our depiction of the violent* battle between the Autoregressive Models (PixelRNN, PixelCNN, ByteNet, VPN, WaveNet), Generative Adversarial Networks (GANs), Variational Autoencoders and, as you should expect by this stage, all of their variants, combinations and hybrids.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p class=&quot;c1 c2&quot;&gt;&lt;span class=&quot;c3&quot;&gt;*Disclaimer: The team wishes to mention that they&lt;/span&gt; &lt;span class=&quot;c3 c22&quot;&gt;do not condone&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt; Network on Network (NoN) violence in any form and are sympathisers to the movement towards Generative Unadversarial Networks (GUNs).&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt201&quot; id=&quot;ftnt_ref201&quot; name=&quot;ftnt_ref201&quot;&gt;[201]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c8 c3&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;In the final section, we’ll offer some concluding remarks and a recapitulation of some of the trends we identified. We would hope that we were comprehensive enough to show a bird’s-eye view&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; of where the Computer Vision field is loosely situated and where it is headed in the near-term. We also would like to draw particular attention to the fact that our work&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;does not cover&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; January-August 2017. The blistering pace of research output means that much of this work could be outdated already; we encourage readers to go and find out whether it is for themselves.&lt;/span&gt; &lt;span class=&quot;c5&quot;&gt;But this rapid pace of growth also brings with it lucrative opportunities as the Computer Vision hardware and software markets are expected to reach $48.6 Billion by 2022.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c10&quot;&gt;&lt;span class=&quot;c12 c6&quot;&gt;Figure 20&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;: Computer Vision Revenue by Application Market&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt202&quot; id=&quot;ftnt_ref202&quot; name=&quot;ftnt_ref202&quot;&gt;[202]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p class=&quot;c10 image-margin&quot;&gt;&lt;span class=&quot;c6&quot;&gt; &lt;/span&gt;&lt;span&gt;&lt;img alt=&quot;&quot; class=&quot;border cntr&quot; src=&quot;http://www.themtank.org/images/c-image9.jpg&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c3 c12&quot;&gt;Note&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Estimation of Computer Vision revenue by application market spanning the period from 2015-2022. The largest growth is forecasted to come from applications within the automotive, consumer, robotics and machine vision sectors.  &lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;c3 c12&quot;&gt;Source&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;: Tractica (2016)&lt;/span&gt;&lt;sup class=&quot;c6 c12&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt203&quot; id=&quot;ftnt_ref203&quot; name=&quot;ftnt_ref203&quot;&gt;[203]&lt;/a&gt;&lt;/sup&gt;&lt;span class=&quot;c6&quot;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;In conclusion we’d like to highlight some of the trends and recurring themes that cropped up repeatedly throughout our research review process. First and foremost, we’d like to draw attention to the Machine Learning research community’s voracious pursuit of optimisation. This is most notable in the year on year changes in accuracy rates, but especially in the&lt;/span&gt; &lt;span class=&quot;c6 c22&quot;&gt;intra-year changes in accuracy&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt;. We’d like to underscore this point and return to it in a moment.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Error rates are not the only fanatically optimised parameter, with researchers working on improving speed, efficiency and even the algorithm’s ability to generalise to other tasks and problems in completely new ways. We are acutely aware of the research coming to the fore with approaches like one-shot learning, generative modelling, transfer learning and, as of recently, evolutionary learning, and we feel that these research principles are gradually exerting greater influence on the approaches of the best performing work.&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;While this last point is unequivocally meant in commendation for, rather than denigration of, this trend, one can’t help but to cast their mind toward the (very) distant spectre of Artificial General Intelligence, whether merited a thought or not. Far from being alarmist, we just wish to highlight to both experts and laypersons that this concern arises from here, from the startling progress that’s already evident in Computer Vision and other AI subfields. Properly articulated concerns from the public can only come through education about these advancements and their impacts in general. This may then in turn quell the power of media sentiment and misinformation in AI.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;We chose to focus on a one year timeline for two reasons. The first relates to the sheer volume of work being produced.&lt;/span&gt; &lt;span class=&quot;c5&quot;&gt;Even for people who follow the field very closely, it is becoming increasingly difficult to remain abreast of research as the number of publications grow exponentially. The second brings us back to our point on intra-year changes.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;In&lt;/span&gt;&lt;span class=&quot;c6&quot;&gt; taking a single year snapshot of progress, the reader can begin to comprehend the pace of research at present. We see improvement after improvement in such short time spans, but why? Researchers have cultivated a global community where building on previous approaches (architectures, meta-architectures, techniques, ideas,&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;tips,&lt;/span&gt;&lt;span class=&quot;c28 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;wacky hacks, results, etc.), and infrastructures (libraries like&lt;/span&gt; &lt;span class=&quot;c6&quot;&gt;Keras, TensorFlow and PyTorch,&lt;/span&gt; &lt;span class=&quot;c5&quot;&gt;GPUs, etc.), is not only encouraged but also celebrated. A predominantly open source community with few parallels, which is continuously attracting new researchers and having its techniques reappropriated by fields like economics, physics and countless others.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;It’s important to understand for those who have yet to notice, that among the already frantic chorus of divergent voices proclaiming divine insight into the true nature of this technology, there is at least agreement; agreement that this technology will alter the world in new and exciting ways. However, much disagreement still comes over the timeline on which these alterations will unravel.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;Until such a time as we can accurately model the progress of these developments we will continue to provide information to the best of our abilities. With this resource we hoped to cater to the spectrum of AI experience, from researchers playing catch-up to anyone who simply wishes to obtain a grounding in Computer Vision and Artificial Intelligence.&lt;/span&gt;&lt;span class=&quot;c28 c6&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c6&quot;&gt;With this our&lt;/span&gt;&lt;span class=&quot;c5&quot;&gt; project hopes to have added some value to the open source revolution that quietly hums beneath the technology of a lifetime.&lt;/span&gt;&lt;/p&gt;

&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c6&quot;&gt;With thanks,&lt;/span&gt;&lt;/p&gt;



&lt;p class=&quot;c1 c9&quot;&gt;&lt;span class=&quot;c5&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;http://www.themtank.org/images/Signature-transparent.png&quot; title=&quot;&quot; /&gt;&lt;/span&gt;&lt;/p&gt;


&lt;p class=&quot;c1&quot;&gt;&lt;span class=&quot;c5&quot;&gt;The M Tank&lt;/span&gt;&lt;/p&gt;
&lt;hr class=&quot;c43&quot; /&gt;
&lt;br /&gt;&lt;strong&gt;Part One&lt;/strong&gt;


&lt;div readability=&quot;8.7731092436975&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref3&quot; id=&quot;ftnt3&quot; name=&quot;ftnt3&quot;&gt;[3]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Kuhn, T. S. 1962.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;The Structure of Scientific Revolutions&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. 4th ed. United States: The University of Chicago Press.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;


&lt;div readability=&quot;6.5454545454545&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref6&quot; id=&quot;ftnt6&quot; name=&quot;ftnt6&quot;&gt;[6]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Stanford University. 2016.&lt;/span&gt; &lt;span class=&quot;c3 c14&quot;&gt;Convolutional Neural Networks for Visual Recognition.&lt;/span&gt; &lt;span class=&quot;c7 c3 c14&quot;&gt;[Online] CS231n&lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;c7 c3 c45 c14&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt;Available:&lt;/span&gt; &lt;span class=&quot;c21 c3 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;http://cs231n.stanford.edu/&quot;&gt;http://cs231n.stanford.edu/&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt; [Accessed 21/12/2016]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;


&lt;div readability=&quot;7.2277992277992&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref9&quot; id=&quot;ftnt9&quot; name=&quot;ftnt9&quot;&gt;[9]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; ImageNet refers to a popular image dataset for Computer Vision. Each year entrants compete in a series of different tasks called the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Available:&lt;/span&gt; &lt;span class=&quot;c21 c3&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;http://image-net.org/challenges/LSVRC/2016/index&quot;&gt;http://image-net.org/challenges/LSVRC/2016/index&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;6.3488372093023&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref11&quot; id=&quot;ftnt11&quot; name=&quot;ftnt11&quot;&gt;[11]&lt;/a&gt;&lt;span class=&quot;c8 c3 a-word-wrap&quot;&gt; See new datasets later in this piece.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;7.7625&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref13&quot; id=&quot;ftnt13&quot; name=&quot;ftnt13&quot;&gt;[13]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Chollet, F. 2016.&lt;/span&gt; &lt;span class=&quot;c3 c14&quot;&gt;Information-theoretical label embeddings for large-scale image classification.&lt;/span&gt; &lt;span class=&quot;c7 c3 c14&quot;&gt;[Online] arXiv: 1607.05691&lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1607.05691v1&quot;&gt;arXiv:1607.05691v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;7.6438356164384&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref14&quot; id=&quot;ftnt14&quot; name=&quot;ftnt14&quot;&gt;[14]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Chollet, F. 2016. Xception: Deep Learning with Depthwise Separable Convolutions.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online]&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c7 c3&quot;&gt;arXiv:1610.02357&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c14 c24&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1610.02357v2&quot;&gt;arXiv:1610.02357v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;


&lt;div readability=&quot;6.6410256410256&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref17&quot; id=&quot;ftnt17&quot; name=&quot;ftnt17&quot;&gt;[17]&lt;/a&gt;&lt;span class=&quot;c8 c3 a-word-wrap&quot;&gt; See Residual Networks in Part Four of this publication for more details.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;14.07650273224&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref19&quot; id=&quot;ftnt19&quot; name=&quot;ftnt19&quot;&gt;[19]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Xie, S., Girshick, R., Dollar, P., Tu, Z. &amp;amp; He, K. 2016.&lt;/span&gt; &lt;span class=&quot;c3 c14&quot;&gt;Aggregated Residual Transformations for Deep Neural Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3 c14&quot;&gt;[Online]&lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c7 c3 c14&quot;&gt;arXiv: 1611.05431&lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c12 c14 c17&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1611.05431v1&quot;&gt;arXiv:1611.05431v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;


&lt;div readability=&quot;6.5084745762712&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref22&quot; id=&quot;ftnt22&quot; name=&quot;ftnt22&quot;&gt;[22]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;Liu et al. 2016.&lt;/span&gt; &lt;span class=&quot;c3 c14&quot;&gt;SSD: Single Shot MultiBox Detector.&lt;/span&gt; &lt;span class=&quot;c7 c3 c14&quot;&gt;[Online] arXiv: 1512.02325v5&lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1512.02325v5&quot;&gt;arXiv:1512.02325v5&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;10&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref23&quot; id=&quot;ftnt23&quot; name=&quot;ftnt23&quot;&gt;[23]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Redmon, J. Farhadi, A. 2016. YOLO9000: Better, Faster, Stronger.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1612.08242v1&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1612.08242v1&quot;&gt;arXiv:1612.08242v1&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.4042553191489&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref24&quot; id=&quot;ftnt24&quot; name=&quot;ftnt24&quot;&gt;[24]&lt;/a&gt;&lt;span class=&quot;c8 c3 a-word-wrap&quot;&gt; YOLO stands for “You Only Look Once”.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;7.6056338028169&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref25&quot; id=&quot;ftnt25&quot; name=&quot;ftnt25&quot;&gt;[25]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Redmon et al. 2016.&lt;/span&gt; &lt;span class=&quot;c3 c14&quot;&gt;You Only Look Once: Unified, Real-Time Object Detection.&lt;/span&gt; &lt;span class=&quot;c7 c3 c14&quot;&gt;[Online] arXiv: 1506.02640&lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1506.02640v5&quot;&gt;arXiv:1506.02640v5&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;6.6141732283465&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref27&quot; id=&quot;ftnt27&quot; name=&quot;ftnt27&quot;&gt;[27]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Lin et al. 2016.&lt;/span&gt; &lt;span class=&quot;c3 c14&quot;&gt;Feature Pyramid Networks for Object Detection.&lt;/span&gt; &lt;span class=&quot;c7 c3 c14&quot;&gt;[Online] arXiv: 1612.03144&lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1612.03144v1&quot;&gt;arXiv:1612.03144v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.4509803921569&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref28&quot; id=&quot;ftnt28&quot; name=&quot;ftnt28&quot;&gt;[28]&lt;/a&gt;&lt;span class=&quot;c8 c3 a-word-wrap&quot;&gt; Facebook’s Artificial Intelligence Research&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.4615384615385&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref29&quot; id=&quot;ftnt29&quot; name=&quot;ftnt29&quot;&gt;[29]&lt;/a&gt;&lt;span class=&quot;c8 c3 a-word-wrap&quot;&gt; Common Objects in Context (COCO) image dataset&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.8496732026144&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref30&quot; id=&quot;ftnt30&quot; name=&quot;ftnt30&quot;&gt;[30]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Dai et al. 2016.&lt;/span&gt; &lt;span class=&quot;c3 c14&quot;&gt;R-FCN: Object Detection via Region-based Fully Convolutional Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3 c14&quot;&gt;[Online] arXiv: 1605.06409&lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1605.06409v2&quot;&gt;arXiv:1605.06409v2&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.8344370860927&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref31&quot; id=&quot;ftnt31&quot; name=&quot;ftnt31&quot;&gt;[31]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Huang et al. 2016.&lt;/span&gt; &lt;span class=&quot;c3 c14&quot;&gt;Speed/accuracy trade-offs for modern convolutional object detectors.&lt;/span&gt; &lt;span class=&quot;c7 c3 c14&quot;&gt;[Online] arXiv: 1611.10012&lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1611.10012v1&quot;&gt;arXiv:1611.10012v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;9.8365384615385&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref33&quot; id=&quot;ftnt33&quot; name=&quot;ftnt33&quot;&gt;[33]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Wu et al. 2016.&lt;/span&gt; &lt;span class=&quot;c3 c14&quot;&gt;SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving.&lt;/span&gt; &lt;span class=&quot;c7 c3 c14&quot;&gt;[Online] arXiv: 1612.01051&lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1612.01051v2&quot;&gt;arXiv:1612.01051v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.8717948717949&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref34&quot; id=&quot;ftnt34&quot; name=&quot;ftnt34&quot;&gt;[34]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Hong et al. 2016. PVANet: Lightweight Deep Neural Networks for Real-time Object Detection.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1611.08588v2&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1611.08588v2&quot;&gt;arXiv:1611.08588v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;








&lt;div readability=&quot;6.5806451612903&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref43&quot; id=&quot;ftnt43&quot; name=&quot;ftnt43&quot;&gt;[43]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Gladh et al. 2016. Deep Motion Features for Visual Tracking.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1612.06615&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1612.06615v1&quot;&gt;arXiv:1612.06615v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.7692307692308&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref44&quot; id=&quot;ftnt44&quot; name=&quot;ftnt44&quot;&gt;[44]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Gaidon et al. 2016.&lt;/span&gt; &lt;span class=&quot;c3 c14&quot;&gt;Virtual Worlds as Proxy for Multi-Object Tracking Analysis.&lt;/span&gt; &lt;span class=&quot;c7 c3 c14&quot;&gt;[Online] arXiv: 1605.06457&lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1605.06457v1&quot;&gt;arXiv:1605.06457v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.8344370860927&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref45&quot; id=&quot;ftnt45&quot; name=&quot;ftnt45&quot;&gt;[45]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Lee et al. 2016.&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt;Globally Optimal Object Tracking with Fully Convolutional Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3 c14&quot;&gt;[Online] arXiv: 1612.08274&lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1612.08274v1&quot;&gt;arXiv:1612.08274v1&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c3 c14&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;br /&gt;&lt;strong&gt;Part Two&lt;/strong&gt;
&lt;div readability=&quot;7.6056338028169&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref46&quot; id=&quot;ftnt46&quot; name=&quot;ftnt46&quot;&gt;[46]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Pinheiro, Collobert and Dollar. 2015. Learning to Segment Object Candidates.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1506.06204&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1506.06204v2&quot;&gt;arXiv:1506.06204v2&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c8 c3 a-word-wrap&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.5691056910569&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref47&quot; id=&quot;ftnt47&quot; name=&quot;ftnt47&quot;&gt;[47]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Pinheiro et al. 2016. Learning to Refine Object Segments.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1603.08695&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1603.08695v2&quot;&gt;arXiv:1603.08695v2&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c8 c3 a-word-wrap&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;7.4409448818898&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref48&quot; id=&quot;ftnt48&quot; name=&quot;ftnt48&quot;&gt;[48]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Zagoruyko, S. 2016. A MultiPath Network for Object Detection.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1604.02135v2.&lt;/span&gt; &lt;span class=&quot;c3&quot;&gt;Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1604.02135v2&quot;&gt;arXiv:1604.02135v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;


&lt;div readability=&quot;6.4285714285714&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref51&quot; id=&quot;ftnt51&quot; name=&quot;ftnt51&quot;&gt;[51]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Jampani et al. 2016. Video Propagation Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1612.05478&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1612.05478v2&quot;&gt;arXiv:1612.05478v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;9.7589743589744&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref52&quot; id=&quot;ftnt52&quot; name=&quot;ftnt52&quot;&gt;[52]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Chen et al., 2016. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1606.00915&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1606.00915v1&quot;&gt;arXiv:1606.00915v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.8717948717949&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref53&quot; id=&quot;ftnt53&quot; name=&quot;ftnt53&quot;&gt;[53]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Khoreva et al. 2016. Simple Does It: Weakly Supervised Instance and Semantic Segmentation.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1603.07485v2.&lt;/span&gt; &lt;span class=&quot;c3&quot;&gt;Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1603.07485v2&quot;&gt;arXiv:1603.07485v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.9942857142857&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref54&quot; id=&quot;ftnt54&quot; name=&quot;ftnt54&quot;&gt;[54]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Jégou et al. 2016. The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1611.09326v2.&lt;/span&gt; &lt;span class=&quot;c3&quot;&gt;Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1611.09326v2&quot;&gt;arXiv:1611.09326v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.7338129496403&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref55&quot; id=&quot;ftnt55&quot; name=&quot;ftnt55&quot;&gt;[55]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Li et al. 2016. Fully Convolutional Instance-aware Semantic Segmentation.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1611.07709v1&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1611.07709v1&quot;&gt;arXiv:1611.07709v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.920245398773&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref56&quot; id=&quot;ftnt56&quot; name=&quot;ftnt56&quot;&gt;[56]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Paszke et al. 2016. ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1606.02147v1&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4&quot; href=&quot;https://arxiv.org/abs/1606.02147v1&quot;&gt;arXiv:1606.02147v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.8645161290323&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref57&quot; id=&quot;ftnt57&quot; name=&quot;ftnt57&quot;&gt;[57]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Vázquez et al. 2016. A Benchmark for Endoluminal Scene Segmentation of Colonoscopy Images.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1612.00799&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4&quot; href=&quot;https://arxiv.org/abs/1612.00799v1&quot;&gt;arXiv:1612.00799v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.9707602339181&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref58&quot; id=&quot;ftnt58&quot; name=&quot;ftnt58&quot;&gt;[58]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Dolz et al. 2016. 3D fully convolutional networks for subcortical segmentation in MRI: A large-scale study.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1612.03925&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4&quot; href=&quot;https://arxiv.org/abs/1612.03925v1&quot;&gt;arXiv:1612.03925v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;7.0222222222222&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref59&quot; id=&quot;ftnt59&quot; name=&quot;ftnt59&quot;&gt;[59]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Alex et al. 2017. Semi-supervised Learning using Denoising Autoencoders for Brain Lesion Detection and Segmentation.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1611.08664&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4&quot; href=&quot;https://arxiv.org/abs/1611.08664v4&quot;&gt;arXiv:1611.08664v4&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.6666666666667&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref60&quot; id=&quot;ftnt60&quot; name=&quot;ftnt60&quot;&gt;[60]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Mozaffari and Lee. 2016. 3D Ultrasound image segmentation: A Survey.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1611.09811&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4&quot; href=&quot;https://arxiv.org/abs/1611.09811v1&quot;&gt;arXiv:1611.09811v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;8.0294117647059&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref61&quot; id=&quot;ftnt61&quot; name=&quot;ftnt61&quot;&gt;[61]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Dasgupta and Singh. 2016. A Fully Convolutional Neural Network based Structured Prediction Approach Towards the Retinal Vessel Segmentation.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1611.02064&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4&quot; href=&quot;https://arxiv.org/abs/1611.02064v2&quot;&gt;arXiv:1611.02064v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.7777777777778&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref62&quot; id=&quot;ftnt62&quot; name=&quot;ftnt62&quot;&gt;[62]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Yi et al. 2016. 3-D Convolutional Neural Networks for Glioblastoma Segmentation.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1611.04534&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4&quot; href=&quot;https://arxiv.org/abs/1611.04534v1&quot;&gt;arXiv:1611.04534v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;7.0382513661202&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref63&quot; id=&quot;ftnt63&quot; name=&quot;ftnt63&quot;&gt;[63]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Quan et al. 2016. FusionNet: A deep fully residual convolutional neural network for image segmentation in connectomics.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1612.05360&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4&quot; href=&quot;https://arxiv.org/abs/1612.05360v2&quot;&gt;arXiv:1612.05360v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;8.7372262773723&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref64&quot; id=&quot;ftnt64&quot; name=&quot;ftnt64&quot;&gt;[64]&lt;/a&gt;&lt;span class=&quot;c8 c3 a-word-wrap&quot;&gt; Connectomics refers to the mapping of all connections within an organism’s nervous system, i.e. neurons and their connections.  &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;6.9885057471264&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref66&quot; id=&quot;ftnt66&quot; name=&quot;ftnt66&quot;&gt;[66]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Caballero et al. 2016. Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1611.05250&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1611.05250v1&quot;&gt;arXiv:1611.05250v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;7.0687830687831&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref67&quot; id=&quot;ftnt67&quot; name=&quot;ftnt67&quot;&gt;[67]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Shi et al. 2016. Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1609.05158&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1609.05158v2&quot;&gt;arXiv:1609.05158v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.6766917293233&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref68&quot; id=&quot;ftnt68&quot; name=&quot;ftnt68&quot;&gt;[68]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Romano et al. 2016. RAISR: Rapid and Accurate Image Super Resolution.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1606.01299&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1606.01299v3&quot;&gt;arXiv:1606.01299v3&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;


&lt;div readability=&quot;6.952380952381&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref71&quot; id=&quot;ftnt71&quot; name=&quot;ftnt71&quot;&gt;[71]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Ledig et al. 2017. Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1609.04802&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1609.04802v3&quot;&gt;arXiv:1609.04802v3&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;6.7246376811594&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref73&quot; id=&quot;ftnt73&quot; name=&quot;ftnt73&quot;&gt;[73]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Sønderby et al. 2016. Amortised MAP Inference for Image Super-resolution.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1610.04490&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1610.04490v1&quot;&gt;arXiv:1610.04490v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;


&lt;div readability=&quot;6.5333333333333&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref76&quot; id=&quot;ftnt76&quot; name=&quot;ftnt76&quot;&gt;[76]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Gatys et al. 2015. A Neural Algorithm of Artistic Style.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1508.06576&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1508.06576v2&quot;&gt;arXiv:1508.06576v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.7058823529412&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref77&quot; id=&quot;ftnt77&quot; name=&quot;ftnt77&quot;&gt;[77]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Nikulin &amp;amp; Novak. 2016. Exploring the Neural Algorithm of Artistic Style.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1602.07188&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1602.07188v2&quot;&gt;arXiv:1602.07188v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.5333333333333&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref78&quot; id=&quot;ftnt78&quot; name=&quot;ftnt78&quot;&gt;[78]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Ruder et al. 2016. Artistic style transfer for videos.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1604.08610&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1604.08610v2&quot;&gt;arXiv:1604.08610v2&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;



&lt;div readability=&quot;6.6666666666667&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref82&quot; id=&quot;ftnt82&quot; name=&quot;ftnt82&quot;&gt;[82]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Dumoulin et al. 2017. A Learned Representation For Artistic Style.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1610.07629&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1610.07629v5&quot;&gt;arXiv:1610.07629v5&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c8 c3 a-word-wrap&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.4424778761062&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref83&quot; id=&quot;ftnt83&quot; name=&quot;ftnt83&quot;&gt;[83]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Zhang et al. 2016. Colorful Image Colorization.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1603.08511&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4&quot; href=&quot;https://arxiv.org/abs/1603.08511v5&quot;&gt;arXiv:1603.08511v5&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c8 c3 a-word-wrap&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.7153284671533&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref84&quot; id=&quot;ftnt84&quot; name=&quot;ftnt84&quot;&gt;[84]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Larsson et al. 2016. Learning Representations for Automatic Colorization.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1603.06668&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4&quot; href=&quot;https://arxiv.org/abs/1603.06668v2&quot;&gt;arXiv:1603.06668v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;9.7297297297297&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref85&quot; id=&quot;ftnt85&quot; name=&quot;ftnt85&quot;&gt;[85]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Lizuka, Simo-Serra and Ishikawa. 2016. Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] ACM Transaction on Graphics (Proc. of SIGGRAPH), 35(4):110&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c3&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/en/&quot;&gt;http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/en/&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c8 c3 a-word-wrap&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;6.7428571428571&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref87&quot; id=&quot;ftnt87&quot; name=&quot;ftnt87&quot;&gt;[87]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Varol et al. 2016. Long-term Temporal Convolutions for Action Recognition.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1604.04494&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1604.04494v1&quot;&gt;arXiv:1604.04494v1&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.8496732026144&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref88&quot; id=&quot;ftnt88&quot; name=&quot;ftnt88&quot;&gt;[88]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Feichtenhofer et al. 2016. Spatiotemporal Residual Networks for Video Action Recognition.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1611.02155&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1611.02155v1&quot;&gt;arXiv:1611.02155v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.7692307692308&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref89&quot; id=&quot;ftnt89&quot; name=&quot;ftnt89&quot;&gt;[89]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Vondrick et al. 2016. Anticipating Visual Representations from Unlabeled Video.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1504.08023&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1504.08023v2&quot;&gt;arXiv:1504.08023v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;6.8421052631579&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref91&quot; id=&quot;ftnt91&quot; name=&quot;ftnt91&quot;&gt;[91]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Idrees et al. 2016. The THUMOS Challenge on Action Recognition for Videos &quot;in the Wild&quot;.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1604.06182&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1604.06182v1&quot;&gt;arXiv:1604.06182v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;br /&gt;&lt;strong&gt;Part Three&lt;/strong&gt;
&lt;div readability=&quot;6.6962962962963&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref92&quot; id=&quot;ftnt92&quot; name=&quot;ftnt92&quot;&gt;[92]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Rezende et al. 2016. Unsupervised Learning of 3D Structure from Images.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1607.00662&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1607.00662v1&quot;&gt;arXiv:1607.00662v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;


&lt;div readability=&quot;8.6974789915966&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref95&quot; id=&quot;ftnt95&quot; name=&quot;ftnt95&quot;&gt;[95]&lt;/a&gt;&lt;span class=&quot;c8 c3 a-word-wrap&quot;&gt; Pose Estimation can refer to either just an object’s orientation, or both orientation and position in 3D space.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.7945205479452&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref96&quot; id=&quot;ftnt96&quot; name=&quot;ftnt96&quot;&gt;[96]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Riegler et al. 2016. OctNet: Learning Deep 3D Representations at High Resolutions.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1611.05009&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1611.05009v3&quot;&gt;arXiv:1611.05009v3&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;6.9068322981366&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref98&quot; id=&quot;ftnt98&quot; name=&quot;ftnt98&quot;&gt;[98]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Choy et al. 2016. 3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1604.00449&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1604.00449v1&quot;&gt;arXiv:1604.00449v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;6.6857142857143&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref100&quot; id=&quot;ftnt100&quot; name=&quot;ftnt100&quot;&gt;[100]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Gadelha et al. 2016. 3D Shape Induction from 2D Views of Multiple Objects.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1612.058272&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1612.05872v1&quot;&gt;arXiv:1612.05872v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;6.6470588235294&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref102&quot; id=&quot;ftnt102&quot; name=&quot;ftnt102&quot;&gt;[102]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Rezende et al. 2016. Unsupervised Learning of 3D Structure from Images.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1607.00662&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1607.00662v1&quot;&gt;arXiv:1607.00662v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;



&lt;div readability=&quot;6.7651006711409&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref106&quot; id=&quot;ftnt106&quot; name=&quot;ftnt106&quot;&gt;[106]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Cao et al. 2016. Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 161108050&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1611.08050v1&quot;&gt;arXiv:1611.08050v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;6.8848484848485&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref108&quot; id=&quot;ftnt108&quot; name=&quot;ftnt108&quot;&gt;[108]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Bogo et al. 2016. Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1607.08128&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1607.08128v1&quot;&gt;arXiv:1607.08128v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;










&lt;div readability=&quot;6.828025477707&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref119&quot; id=&quot;ftnt119&quot; name=&quot;ftnt119&quot;&gt;[119]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Garg et al. 2016. Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1603.04992&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1603.04992v2&quot;&gt;arXiv:1603.04992v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;


&lt;div readability=&quot;6.125&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref122&quot; id=&quot;ftnt122&quot; name=&quot;ftnt122&quot;&gt;[122]&lt;/a&gt;&lt;span class=&quot;c8 c3 a-word-wrap&quot;&gt; Yet more neural network spillover&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.4918032786885&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref123&quot; id=&quot;ftnt123&quot; name=&quot;ftnt123&quot;&gt;[123]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Tokmakov et al. 2016. Learning Motion Patterns in Videos.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1612.07217&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1612.07217v1&quot;&gt;arXiv:1612.07217v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;6.4406779661017&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref125&quot; id=&quot;ftnt125&quot; name=&quot;ftnt125&quot;&gt;[125]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; DeTone et al. 2016. Deep Image Homography Estimation.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1606.03798&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1606.03798v1&quot;&gt;arXiv:1606.03798v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.7132867132867&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref126&quot; id=&quot;ftnt126&quot; name=&quot;ftnt126&quot;&gt;[126]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Handa et al. 2016. gvnn: Neural Network Library for Geometric Computer Vision.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1607.07405&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1607.07405v3&quot;&gt;arXiv:1607.07405v3&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;




&lt;br /&gt;&lt;strong&gt;Part Four&lt;/strong&gt;
&lt;div readability=&quot;7.7678571428571&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref131&quot; id=&quot;ftnt131&quot; name=&quot;ftnt131&quot;&gt;[131]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Szegedy et al. 2016. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1602.07261&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1602.07261v2&quot;&gt;arXiv:1602.07261v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.7222222222222&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref132&quot; id=&quot;ftnt132&quot; name=&quot;ftnt132&quot;&gt;[132]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Szegedy et al. 2015. Rethinking the Inception Architecture for Computer Vision.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1512.00567&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1512.00567v3&quot;&gt;arXiv:1512.00567v3&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.528&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref133&quot; id=&quot;ftnt133&quot; name=&quot;ftnt133&quot;&gt;[133]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Huang et al. 2016. Densely Connected Convolutional Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1608.06993&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1608.06993v3&quot;&gt;arXiv:1608.06993v3&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;



&lt;div readability=&quot;6.7132867132867&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref137&quot; id=&quot;ftnt137&quot; name=&quot;ftnt137&quot;&gt;[137]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Larsson et al. 2016. FractalNet: Ultra-Deep Neural Networks without Residuals.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1605.07648&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1605.07648v2&quot;&gt;arXiv:1605.07648v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;7.4436090225564&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref138&quot; id=&quot;ftnt138&quot; name=&quot;ftnt138&quot;&gt;[138]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Huang et al. 2016. Densely Connected Convolutional Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1608.06993&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1608.06993v3&quot;&gt;arXiv:1608.06993v3&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;, pg. 1.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.9720670391061&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref139&quot; id=&quot;ftnt139&quot; name=&quot;ftnt139&quot;&gt;[139]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Hossein HasanPour et al. 2016. Lets keep it simple: using simple architectures to outperform deeper architectures.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1608.06037&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1608.06037v3&quot;&gt;arXiv:1608.06037v3&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;6.6470588235294&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref141&quot; id=&quot;ftnt141&quot; name=&quot;ftnt141&quot;&gt;[141]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Singh et al. 2016. Swapout: Learning an ensemble of deep architectures.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1605.06465&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1605.06465v1&quot;&gt;arXiv:1605.06465v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.9112426035503&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref142&quot; id=&quot;ftnt142&quot; name=&quot;ftnt142&quot;&gt;[142]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Iandola et al. 2016. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &amp;lt;0.5MB model size.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1602.07360&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1602.07360v4&quot;&gt;arXiv:1602.07360v4&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.9945355191257&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref143&quot; id=&quot;ftnt143&quot; name=&quot;ftnt143&quot;&gt;[143]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Shang et al. 2016. Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units.&lt;/span&gt;&lt;span class=&quot;c7 c3&quot;&gt; [Online] arXiv: 1603.05201&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1603.05201v2&quot;&gt;arXiv:1603.05201v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.8571428571429&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref144&quot; id=&quot;ftnt144&quot; name=&quot;ftnt144&quot;&gt;[144]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Clevert et al. 2016. Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs).&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1511.07289&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1511.07289v5&quot;&gt;arXiv:1511.07289v5&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.8571428571429&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref145&quot; id=&quot;ftnt145&quot; name=&quot;ftnt145&quot;&gt;[145]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Trottier et al. 2016. Parametric Exponential Linear Unit for Deep Convolutional Neural Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1605.09332&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1605.09332v3&quot;&gt;arXiv:1605.09332v3&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.7567567567568&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref146&quot; id=&quot;ftnt146&quot; name=&quot;ftnt146&quot;&gt;[146]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Worrall et al. 2016. Harmonic Networks: Deep Translation and Rotation Equivariance.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1612.04642&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1612.04642v1&quot;&gt;arXiv:1612.04642v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.5736434108527&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref147&quot; id=&quot;ftnt147&quot; name=&quot;ftnt147&quot;&gt;[147]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Cohen &amp;amp; Welling. 2016. Group Equivariant Convolutional Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1602.07576&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1602.07576v3&quot;&gt;arXiv:1602.07576v3&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.7482993197279&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref148&quot; id=&quot;ftnt148&quot; name=&quot;ftnt148&quot;&gt;[148]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Dieleman et al. 2016. Exploiting Cyclic Symmetry in Convolutional Neural Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1602.02660&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1602.02660v2&quot;&gt;arXiv:1602.02660v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;9.5459770114943&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref150&quot; id=&quot;ftnt150&quot; name=&quot;ftnt150&quot;&gt;[150]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Abdi, M., Nahavandi, S. 2016. Multi-Residual Networks: Improving the Speed and Accuracy of Residual Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1609.05672&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1609.05672v3&quot;&gt;arXiv:1609.05672v3&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.5625&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref151&quot; id=&quot;ftnt151&quot; name=&quot;ftnt151&quot;&gt;[151]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; He et al. 2015. Deep Residual Learning for Image Recognition.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1512.03385&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4&quot; href=&quot;https://arxiv.org/abs/1512.03385v1&quot;&gt;arXiv:1512.03385v1&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c8 c3 a-word-wrap&quot;&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;8.1746031746032&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref153&quot; id=&quot;ftnt153&quot; name=&quot;ftnt153&quot;&gt;[153]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Zagoruyko, S. and Komodakis, N. 2017. Wide Residual Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1605.07146&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1605.07146v3&quot;&gt;arXiv:1605.07146v3&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.4666666666667&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref154&quot; id=&quot;ftnt154&quot; name=&quot;ftnt154&quot;&gt;[154]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Huang et al. 2016. Deep Networks with Stochastic Depth.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1603.09382&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1603.09382v3&quot;&gt;arXiv:1603.09382v3&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.6268656716418&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref155&quot; id=&quot;ftnt155&quot; name=&quot;ftnt155&quot;&gt;[155]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Savarese et al. 2016. Learning Identity Mappings with Residual Gates.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1611.01260&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1611.01260v2&quot;&gt;arXiv:1611.01260v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;7.7751479289941&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref156&quot; id=&quot;ftnt156&quot; name=&quot;ftnt156&quot;&gt;[156]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Veit, Wilber and Belongie. 2016. Residual Networks Behave Like Ensembles of Relatively Shallow Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1605.06431&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1605.06431v2&quot;&gt;arXiv:1605.06431v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.528&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref157&quot; id=&quot;ftnt157&quot; name=&quot;ftnt157&quot;&gt;[157]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; He at al. 2016. Identity Mappings in Deep Residual Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1603.05027&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1603.05027v3&quot;&gt;arXiv:1603.05027v3&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;9.5459770114943&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref158&quot; id=&quot;ftnt158&quot; name=&quot;ftnt158&quot;&gt;[158]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Abdi, M., Nahavandi, S. 2016. Multi-Residual Networks: Improving the Speed and Accuracy of Residual Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1609.05672&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1609.05672v3&quot;&gt;arXiv:1609.05672v3&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.7814569536424&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref159&quot; id=&quot;ftnt159&quot; name=&quot;ftnt159&quot;&gt;[159]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Greff et al. 2017. Highway and Residual Networks learn Unrolled Iterative Estimation.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1612. 07771&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1612.07771v3&quot;&gt;arXiv:1612.07771v3&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.8711656441718&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref160&quot; id=&quot;ftnt160&quot; name=&quot;ftnt160&quot;&gt;[160]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Abdi and Nahavandi. 2017. Multi-Residual Networks: Improving the Speed and Accuracy of Residual Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] 1609.05672&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1609.05672v4&quot;&gt;arXiv:1609.05672v4&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.6569343065693&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref161&quot; id=&quot;ftnt161&quot; name=&quot;ftnt161&quot;&gt;[161]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Targ et al. 2016. Resnet in Resnet: Generalizing Residual Architectures.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1603.08029&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1603.08029v1&quot;&gt;arXiv:1603.08029v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.7733333333333&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref162&quot; id=&quot;ftnt162&quot; name=&quot;ftnt162&quot;&gt;[162]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Wu et al. 2016. Wider or Deeper: Revisiting the ResNet Model for Visual Recognition.&lt;/span&gt;&lt;span class=&quot;c7 c3&quot;&gt; [Online] arXiv: 1611.10080&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1611.10080v1&quot;&gt;arXiv:1611.10080v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;7.8305084745763&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref163&quot; id=&quot;ftnt163&quot; name=&quot;ftnt163&quot;&gt;[163]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Liao and Poggio. 2016. Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1604.03640&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1604.03640v1&quot;&gt;arXiv:1604.03640v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.528&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref164&quot; id=&quot;ftnt164&quot; name=&quot;ftnt164&quot;&gt;[164]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Moniz and Pal. 2016. Convolutional Residual Memory Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1606.05262&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1606.05262v3&quot;&gt;arXiv:1606.05262v3&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.453781512605&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref165&quot; id=&quot;ftnt165&quot; name=&quot;ftnt165&quot;&gt;[165]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Hardt and Ma. 2016. Identity Matters in Deep Learning.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1611.04231&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1611.04231v2&quot;&gt;arXiv:1611.04231v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.637037037037&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref166&quot; id=&quot;ftnt166&quot; name=&quot;ftnt166&quot;&gt;[166]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Shah et al. 2016. Deep Residual Networks with Exponential Linear Unit.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1604.04112&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1604.04112v4&quot;&gt;arXiv:1604.04112v4&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.5625&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref167&quot; id=&quot;ftnt167&quot; name=&quot;ftnt167&quot;&gt;[167]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Shen and Zeng. 2016. Weighted Residuals for Very Deep Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1605.08831&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c12 c14 c24&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1605.08831v1&quot;&gt;arXiv:1605.08831v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;


&lt;div readability=&quot;6.4137931034483&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref170&quot; id=&quot;ftnt170&quot; name=&quot;ftnt170&quot;&gt;[170]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; COCO. 2017. Common Objects in Common Homepage.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online]&lt;/span&gt; &lt;span class=&quot;c3&quot;&gt;Available:&lt;/span&gt; &lt;span class=&quot;c21 c3&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;http://mscoco.org/&quot;&gt;http://mscoco.org/&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c8 c3 a-word-wrap&quot;&gt; [Accessed: 04/01/2017]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;




&lt;div readability=&quot;6.9834254143646&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref175&quot; id=&quot;ftnt175&quot; name=&quot;ftnt175&quot;&gt;[175]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; McCormac et al. 2017. SceneNet RGB-D: 5M Photorealistic Images of Synthetic Indoor Trajectories with Ground Truth.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1612.05079&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;v3. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1612.05079v3&quot;&gt;arXiv:1612.05079v3&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;


&lt;div readability=&quot;6.7894736842105&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref178&quot; id=&quot;ftnt178&quot; name=&quot;ftnt178&quot;&gt;[178]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Guo et al. 2016. MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1607.08221&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1607.08221v1&quot;&gt;arXiv:1607.08221v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div readability=&quot;6.7567567567568&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref180&quot; id=&quot;ftnt180&quot; name=&quot;ftnt180&quot;&gt;[180]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Abu-El-Haija et al. 2016. YouTube-8M: A Large-Scale Video Classification Benchmark.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1609.08675&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1609.08675v1&quot;&gt;arXiv:1609.08675v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;






&lt;div readability=&quot;8.0365296803653&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref187&quot; id=&quot;ftnt187&quot; name=&quot;ftnt187&quot;&gt;[187]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Johnson, A. 2016. Trailbehind/DeepOSM - Train a deep learning net with OpenStreetMap features and satellite imagery.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] Github.com&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c3&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://github.com/trailbehind/DeepOSM&quot;&gt;https://github.com/trailbehind/DeepOSM&lt;/a&gt;&lt;/span&gt;&lt;span class=&quot;c8 c3 a-word-wrap&quot;&gt; [Accessed: 29/03/2017].&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;




&lt;div readability=&quot;6.828025477707&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref192&quot; id=&quot;ftnt192&quot; name=&quot;ftnt192&quot;&gt;[192]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Gordo et al. 2016. Deep Image Retrieval: Learning global representations for image search.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1604.01325&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1604.01325v2&quot;&gt;arXiv:1604.01325v2&lt;/a&gt;&lt;/span&gt;&lt;span&gt; &lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.6666666666667&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref193&quot; id=&quot;ftnt193&quot; name=&quot;ftnt193&quot;&gt;[193]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Wang et al. 2016. Deep Learning for Identifying Metastatic Breast Cancer.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1606.05718&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1606.05718v1&quot;&gt;arXiv:1606.05718v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;






&lt;div readability=&quot;6.5846153846154&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref200&quot; id=&quot;ftnt200&quot; name=&quot;ftnt200&quot;&gt;[200]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Assael et al. 2016. LipNet: End-to-End Sentence-level Lipreading.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1611.01599&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c21 c3&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1611.01599v2&quot;&gt;arXiv:1611.01599v2&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;6.7132867132867&quot;&gt;
&lt;p class=&quot;c11&quot;&gt;&lt;a href=&quot;http://www.themtank.org/a-year-in-computer-vision#ftnt_ref201&quot; id=&quot;ftnt201&quot; name=&quot;ftnt201&quot;&gt;[201]&lt;/a&gt;&lt;span class=&quot;c3&quot;&gt; Albanie et al. 2017. Stopping GAN Violence: Generative Unadversarial Networks.&lt;/span&gt; &lt;span class=&quot;c7 c3&quot;&gt;[Online] arXiv: 1703.02528&lt;/span&gt;&lt;span class=&quot;c3&quot;&gt;. Available:&lt;/span&gt; &lt;span class=&quot;c17 c12 c14&quot;&gt;&lt;a class=&quot;c4 a-word-wrap&quot; href=&quot;https://arxiv.org/abs/1703.02528v1&quot;&gt;arXiv:1703.02528v1&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;


</description>
<pubDate>Mon, 27 Nov 2017 03:09:37 +0000</pubDate>
<dc:creator>Geeshang</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.themtank.org/a-year-in-computer-vision</dc:identifier>
</item>
<item>
<title>The beginning of the end for copper wire</title>
<link>https://potsandpansbyccg.com/2017/11/20/the-beginning-of-the-end-for-copper/</link>
<guid isPermaLink="true" >https://potsandpansbyccg.com/2017/11/20/the-beginning-of-the-end-for-copper/</guid>
<description>&lt;p&gt;&lt;a href=&quot;https://potsandpansbyccg.com/2016/02/08/what-happens-to-unused-caf-ii-funds/copper-cable/#main&quot; rel=&quot;attachment wp-att-4605&quot;&gt;&lt;img data-attachment-id=&quot;4605&quot; data-permalink=&quot;https://potsandpansbyccg.com/2016/02/08/what-happens-to-unused-caf-ii-funds/copper-cable/#main&quot; data-orig-file=&quot;https://potsandpansbyccg.files.wordpress.com/2016/01/copper-cable.jpg&quot; data-orig-size=&quot;600,400&quot; data-comments-opened=&quot;1&quot; data-image-meta=&quot;{&amp;quot;aperture&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;credit&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;camera&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;caption&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;created_timestamp&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;copyright&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;focal_length&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;iso&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;shutter_speed&amp;quot;:&amp;quot;0&amp;quot;,&amp;quot;title&amp;quot;:&amp;quot;&amp;quot;,&amp;quot;orientation&amp;quot;:&amp;quot;0&amp;quot;}&quot; data-image-title=&quot;Copper Cable&quot; data-image-description=&quot;&quot; data-medium-file=&quot;https://potsandpansbyccg.files.wordpress.com/2016/01/copper-cable.jpg?w=300&amp;amp;h=200&quot; data-large-file=&quot;https://potsandpansbyccg.files.wordpress.com/2016/01/copper-cable.jpg?w=600&quot; class=&quot;alignright size-medium wp-image-4605&quot; src=&quot;https://potsandpansbyccg.files.wordpress.com/2016/01/copper-cable.jpg?w=300&amp;amp;h=200&quot; alt=&quot;&quot; width=&quot;300&quot; height=&quot;200&quot; srcset=&quot;https://potsandpansbyccg.files.wordpress.com/2016/01/copper-cable.jpg?w=300&amp;amp;h=200 300w, https://potsandpansbyccg.files.wordpress.com/2016/01/copper-cable.jpg?w=150&amp;amp;h=100 150w, https://potsandpansbyccg.files.wordpress.com/2016/01/copper-cable.jpg 600w&quot; sizes=&quot;(max-width: 300px) 100vw, 300px&quot;/&gt;&lt;/a&gt;The FCC voted last Thursday to relax the rules for retiring copper wiring. This change was specifically aimed at Verizon and AT&amp;amp;T and is going to make it a lot easier for them to tear down old copper wiring.&lt;/p&gt;
&lt;p&gt;The change eliminates some of the notification process to customers and also allows the telcos to eliminate old copper wholesale services like resale. But the big consequence of this change is that many customers will lose voice services. This change reverses rules put in place in 2014 that required that the telcos replace copper with service that is functionally as good as the copper facilities that are being removed.&lt;/p&gt;
&lt;p&gt;Consider what this change will mean. If the telcos tear down copper in towns then customers will lose the option to buy DSL. While cable modems have clobbered DSL in the market there are still between 15% and 25% of broadband customers on DSL in most markets. DSL, while slower, also offers lower cost broadband options which many customers find attractive.&lt;/p&gt;
&lt;p&gt;I don’t envision AT&amp;amp;T and Verizon tearing down huge amounts of copper in towns immediately. But there are plenty of neighborhoods where the copper is dreadful and the telcos can now walk away from that copper without offering an alternative to customers. This will give the cable companies a true monopoly in towns or neighborhoods where the copper is removed. Customers losing low-cost DSL will face a price increase if they want to keep broadband.&lt;/p&gt;
&lt;p&gt;The rural areas are a different story. In most of rural America the copper network is used to deliver telephone service and there are still a lot of rural customers buying telephone service. You might think that people can just change to cellular service if they lose their landlines, but it’s not that simple. There are still plenty of rural places that have copper telephone service where there is no good cellular service. And there are a lot more places where the cellular service is too weak to work indoors and customers need to go outside to find the cellular sweet spots (something we all remember doing in airports a decade ago).&lt;/p&gt;
&lt;p&gt;Of a bigger concern in rural areas will be losing access to 911. A lot of homes still keep landlines just for the 911 capabilities. Under the old rules the carriers had to demonstrate that customers would still have access to reliable 911, but it seems the carriers can now walk away without worrying about this.&lt;/p&gt;
&lt;p&gt;The FCC seems to have accepted the big telcos arguments completely. For instance, Chairman Pai cited a big telco argument that carriers could save $40 to $50 per home per year by eliminating copper. That may be a real number, but the revenue from somebody buying voice service on copper is far greater than the savings. It seems clear that the big telcos want to eliminate what’s left of their rural work force and get out of the residential business.&lt;/p&gt;
&lt;p&gt;This is a change that has been inevitable for years. The copper networks are deteriorating due to age and due even more to neglect. But the last FCC rules forced the telcos to work to find an alternative to copper for customers. Since AT&amp;amp;T and Verizon are cellular companies this largely meant guaranteeing adequate access to cellular service – and that meant beefing up the rural cellular networks where there aren’t a lot of customers. But without the functional equivalency requirement it’s unlikely that the carriers will beef up cellular service in the most remote rural places. And that means that many homes will go dark for voice.&lt;/p&gt;
&lt;p&gt;This same ruling applies to other telcos, but I don’t think there will be any rush to tear down copper in the same manner as AT&amp;amp;T and Verizon. Telcos like Frontier and Windstream still rely heavily on their copper networks and don’t have a cellular product to replace landlines. And I don’t know any smaller telcos that would walk away from customers without first providing an alternative service.&lt;/p&gt;
&lt;p&gt;It’s hard to think that the FCC is embracing a policy that will leave some households with no voice option. The FCC is purposefully turning a blind eye to the issue, but anybody who knows rural America knows this will happen. There are still a lot of rural places where copper is the only communications option today. Our regulators once prided themselves on the fact that we brought telephone service to every place that had electricity. We had a communications network that was the envy of the world, and connecting everybody was a huge boon to the economy. We could still keep those same universal service policies for cellular service if we had the will to do so. But this FCC clearly sides with the big carriers over the public and they are not going to impose any rules that the big telcos and cable companies don’t want.&lt;/p&gt;
&lt;div id=&quot;jp-post-flair&quot; class=&quot;sharedaddy sd-like-enabled sd-sharing-enabled&quot;&gt;
&lt;div class=&quot;sharedaddy sd-sharing-enabled&quot;&gt;
&lt;div class=&quot;robots-nocontent sd-block sd-social sd-social-icon sd-sharing&quot;&gt;
&lt;h3 class=&quot;sd-title&quot;&gt;Share this:&lt;/h3&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded&quot; id=&quot;like-post-wrapper-49312101-6307-5a1c365eefe93&quot; data-src=&quot;//widgets.wp.com/likes/#blog_id=49312101&amp;amp;post_id=6307&amp;amp;origin=potsandpansbyccg.wordpress.com&amp;amp;obj_id=49312101-6307-5a1c365eefe93&quot; data-name=&quot;like-post-frame-49312101-6307-5a1c365eefe93&quot;&gt;
&lt;h3 class=&quot;sd-title&quot;&gt;Like this:&lt;/h3&gt;
&lt;div class=&quot;likes-widget-placeholder post-likes-widget-placeholder&quot;&gt;&lt;span class=&quot;button&quot;&gt;&lt;span&gt;Like&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;loading&quot;&gt;Loading...&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
</description>
<pubDate>Sun, 26 Nov 2017 20:06:27 +0000</pubDate>
<dc:creator>rmason</dc:creator>
<og:type>article</og:type>
<og:title>The Beginning of the End for Copper</og:title>
<og:url>https://potsandpansbyccg.com/2017/11/20/the-beginning-of-the-end-for-copper/</og:url>
<og:description>The FCC voted last Thursday to relax the rules for retiring copper wiring. This change was specifically aimed at Verizon and AT&amp;T and is going to make it a lot easier for them to tear down old …</og:description>
<og:image>https://potsandpansbyccg.files.wordpress.com/2016/01/copper-cable.jpg?w=300</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://potsandpansbyccg.com/2017/11/20/the-beginning-of-the-end-for-copper/</dc:identifier>
</item>
<item>
<title>Bitcoin Blows Past $9,000</title>
<link>https://gizmodo.com/bitcoin-blows-past-9-000-1820747737</link>
<guid isPermaLink="true" >https://gizmodo.com/bitcoin-blows-past-9-000-1820747737</guid>
<description>&lt;div class=&quot;img-wrapper lazy-image&quot;&gt;
&lt;div class=&quot;img-permalink-sub-wrapper&quot;&gt;&lt;span class=&quot;js_lightbox-wrapper lightbox-wrapper&quot;&gt;&lt;img src=&quot;https://i.kinja-img.com/gawker-media/image/upload/s--KUnB_2EU--/c_scale,fl_progressive,q_80,w_800/bf3zwneum2dcu7oryjbn.jpg&quot; class=&quot;lazyload ls-lazy-image-tag&quot; data-sizes=&quot;auto&quot; data-width=&quot;2250&quot; data-chomp-id=&quot;bf3zwneum2dcu7oryjbn&quot; data-format=&quot;jpg&quot;/&gt;&lt;/span&gt;&lt;/div&gt;
Photo: Getty Images&lt;/div&gt;
&lt;p&gt;The Bitcoin bubble continues to stretch to even more grotesque proportions, clearing a value of at least $9,143 per bitcoin and attaining a market cap of $152 billion on Sunday, &lt;a href=&quot;https://techcrunch.com/2017/11/26/bitcoin-is-over-9000/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'External link', 'https://techcrunch.com/2017/11/26/bitcoin-is-over-9000/', {metric25:1})&quot;&gt;TechCrunch reported&lt;/a&gt;. The strong gains more or less put Bitcoin on a path to clear a value of $10,000 by the end of calendar year 2017, and potentially even more.&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;While doomsayers have been predicting a massive crash in the cryptocurrency market for years, Bitcoin has largely continued its trend of explosive growth—for now. What might have cost a pittance a few years ago could be worth &lt;a href=&quot;http://www.chicagotribune.com/bluesky/technology/ct-bitcoin-value-20170524-story.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'External link', 'http://www.chicagotribune.com/bluesky/technology/ct-bitcoin-value-20170524-story.html', {metric25:1})&quot;&gt;tens of millions of dollars today&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Other cryptocurrencies like ether have also become very valuable, and the initial coin offering sector—a more or less totally unregulated form of investment vehicle for crypto-backed startups—&lt;a href=&quot;https://www.bloomberg.com/news/articles/2017-10-16/initial-coin-offerings-rake-in-another-billion-in-under-2-months&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'External link', 'https://www.bloomberg.com/news/articles/2017-10-16/initial-coin-offerings-rake-in-another-billion-in-under-2-months', {metric25:1})&quot;&gt;took in billions of dollars&lt;/a&gt; in 2017. Per the &lt;a href=&quot;http://www.chicagotribune.com/bluesky/technology/ct-bitcoin-value-20170524-story.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'External link', 'http://www.chicagotribune.com/bluesky/technology/ct-bitcoin-value-20170524-story.html', {metric25:1})&quot;&gt;&lt;em&gt;Washington Post&lt;/em&gt;&lt;/a&gt;, even a few high-ranking members of Donald Trump’s administration like “budget director Mick Mulvaney and vice president Mike Pence’s chief economist Mark Calabria” are Bitcoin boosters.&lt;/p&gt;
&lt;div class=&quot;js_ad-mobile-dynamic js_ad-dynamic ad-mobile-dynamic&quot;&gt;
&lt;div class=&quot;ad-unit js_ad-unit hide-contents ad-mobile js_ad-mobile&quot;&gt;
&lt;div class=&quot;ad-mobile-inner&quot;&gt;
&lt;p class=&quot;ad-label proxima&quot;&gt;&lt;small class=&quot;proxima&quot;&gt;Advertisement&lt;/small&gt;&lt;/p&gt;


&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;However, adoption of cryptocurrency by the public at large (or for that matter, by retailers and other firms) &lt;a href=&quot;https://gizmodo.com/survey-most-americans-remain-blissfully-unaware-of-the-1820027828&quot; rel=&quot;nofollow&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'Internal link', 'https://gizmodo.com/survey-most-americans-remain-blissfully-unaware-of-the-1820027828', {metric25:1})&quot;&gt;has been limited&lt;/a&gt;, meaning that most of the growth in bitcoin’s value has been driven by speculation and investors looking to get rich quick. The crypto markets in general are littered with &lt;a href=&quot;https://gizmodo.com/initial-coin-offering-backed-startup-confido-goes-dark-1820629969&quot; rel=&quot;nofollow&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'Internal link', 'https://gizmodo.com/initial-coin-offering-backed-startup-confido-goes-dark-1820629969', {metric25:1})&quot;&gt;shady companies&lt;/a&gt; and &lt;a href=&quot;https://gizmodo.com/guy-who-ran-really-really-huge-scam-warns-icos-are-the-1819756698&quot; rel=&quot;nofollow&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'Internal link', 'https://gizmodo.com/guy-who-ran-really-really-huge-scam-warns-icos-are-the-1819756698', {metric25:1})&quot;&gt;scams&lt;/a&gt;, and bitcoin has a tendency to &lt;a href=&quot;https://gizmodo.com/the-guy-who-oversaw-mt-goxs-catastrophic-meltdown-coul-1820360851&quot; rel=&quot;nofollow&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'Internal link', 'https://gizmodo.com/the-guy-who-oversaw-mt-goxs-catastrophic-meltdown-coul-1820360851', {metric25:1})&quot;&gt;implode in value&lt;/a&gt; every now and then due to occurrences like the crash of trading exchange Mt. Gox, so it’s possible something could quickly knock the value back down a peg.&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;But as TechCrunch noted, Bitcoin shows no signs of slowing down in the immediate future to the point where “news of clearing these incremental price hurdles are going to get old.” One wonders how long these good times could possibly last.&lt;/p&gt;
&lt;p&gt;[&lt;a href=&quot;https://techcrunch.com/2017/11/26/bitcoin-is-over-9000/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; onclick=&quot;window.ga('send', 'event', 'Embedded Url', 'External link', 'https://techcrunch.com/2017/11/26/bitcoin-is-over-9000/', {metric25:1})&quot;&gt;TechCrunch&lt;/a&gt;]&lt;/p&gt;
</description>
<pubDate>Sun, 26 Nov 2017 18:24:20 +0000</pubDate>
<dc:creator>ourmandave</dc:creator>
<og:title>Bitcoin Blows Past $9,000</og:title>
<og:type>article</og:type>
<og:image>https://i.kinja-img.com/gawker-media/image/upload/s--Gd6ht3PX--/c_fill,fl_progressive,g_center,h_900,q_80,w_1600/bf3zwneum2dcu7oryjbn.jpg</og:image>
<og:url>https://gizmodo.com/bitcoin-blows-past-9-000-1820747737</og:url>
<og:description>The Bitcoin bubble continues to stretch to even more grotesque proportions, clearing a value of at least $9,143 per bitcoin and attaining a market cap of $152 billion on Sunday, TechCrunch reported. The strong gains more or less put Bitcoin on a path to clear a value of $10,000 by the end of calendar year 2017, and potentially even more.</og:description>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://gizmodo.com/bitcoin-blows-past-9-000-1820747737</dc:identifier>
</item>
<item>
<title>Facebook Is the Junk Food of Socializing (2015)</title>
<link>http://nautil.us/blog/why-facebook-is-the-junk-food-of-socializing</link>
<guid isPermaLink="true" >http://nautil.us/blog/why-facebook-is-the-junk-food-of-socializing</guid>
<description>&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;H&lt;/span&gt;ave you ever been walking in a dark alley and seen something that you thought was a crouching person, but it turned out to be a garbage bag or something similarly innocuous? Me too.&lt;/p&gt;
&lt;p&gt;Have you ever seen a person crouching in a dark alley and mistaken it for a garbage bag? Me neither. Why does the error go one way and not the other?&lt;/p&gt;
&lt;img src=&quot;http://static.nautil.us/6248_e6a4f65e7355bb8b7671c3a18003b146.jpg&quot; width=&quot;733&quot; alt=&quot;&quot;/&gt;&lt;a href=&quot;http://www.shutterstock.com/pic-276648545/stock-photo-egg-and-white-sugar-in-stainless-bowl-for-make-a-cakes-but-look-like-face.html&quot;&gt;Klattistock via Shutterstock&lt;/a&gt;
&lt;p&gt;&lt;span&gt;Human beings are intensely social animals. We live in hierarchical social environments in which our comfort, reproduction, and very survival depend on our relationships with other people. As a result, we are very good at thinking about things in social ways. In fact, some scientists have argued that the evolutionary arms race for strategic social thinking—either&lt;/span&gt; &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S1090513804000595&quot; target=&quot;_blank&quot;&gt;for competition&lt;/a&gt;&lt;span&gt;, &lt;/span&gt;&lt;a href=&quot;http://rspb.royalsocietypublishing.org/content/early/2012/04/04/rspb.2012.0206&quot; target=&quot;_blank&quot;&gt;for cooperation&lt;/a&gt;&lt;span&gt;, or both—was a large part of why we became so intelligent as a species.&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;This affinity for social reasoning, however, has resulted in systematic quirks in human reasoning about the non-human. This happens in two ways. First, we tend to see humanlike agency where there isn’t any, a common form of &lt;a href=&quot;http://en.wikipedia.org/wiki/Pareidolia&quot; target=&quot;_blank&quot;&gt;pareidolia&lt;/a&gt;. Many people view the sun as happy, for instance, and in religions the world over, diseases are seen as curses cast by witches. This effect has been argued to be one of the main reasons religions exist at all: People imagine that there must be supernatural beings behind the scenes, making the world work the way it does.&lt;sup&gt;1&lt;/sup&gt; Second, we are more prone to believe in explanations when they are couched in terms of the everyday psychology people use to explain and predict people’s behavior. Teachers sometimes take advantage of this, using “anthropomorphic” glosses on natural phenomena to help their students learn (&lt;a href=&quot;http://bit.ly/1JdUphX&quot; target=&quot;_blank&quot;&gt;e.g.&lt;/a&gt;, “the water wants to find its level.”)&lt;/p&gt;
&lt;p&gt;Why would we evolve to have a systematic error like this? Like most biases, it takes advantage of patterns in our environment to help us (or, more accurately, paleolithic people) reproduce and survive. In the environment where humans first evolved, mistaking a log for a lion is much safer than mistaking a lion for a log, favoring the survival of those who err on the side of seeing agency in many places. And for a hunter-gatherer at greater risk from wild animals and interpersonal violence than we face today, living things tend to be more dangerous than non-living things. We tend to see agency in everything, and children have it more than adults, &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/16313665&quot; target=&quot;_blank&quot;&gt;suggesting that it has an inborn element&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are some interesting ramifications of this. In the 1990s, human-computer-interaction researchers Reeves and Nass replicated social psychology experiments, but rather than interacting with other people, &lt;a href=&quot;http://humanityonline.com/docs/the%20media%20equation.pdf&quot; target=&quot;_blank&quot;&gt;participants interacted with computers&lt;/a&gt; (pdf). For example, the researchers put a blue ribbon around a participant’s arm and a blue piece of paper around a computer’s monitor. Participants were told that that computer was on their team, and that another computer, adorned with red paper, was on the other team. Participants believed that the spell checker on the “teammate” computer caught more errors. This is because we think about computers (or characters in fiction, or gods) using the same reasoning processes we do when we reason about other people. That experiment is just one of many fascinating (and often hilarious) examples.&lt;/p&gt;
&lt;p&gt;The other interesting effect of this is that we treat virtual people as real people. Experiments show that, at some level, people tend to think of the characters on their favorite TV shows as personal friends—&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/21750250&quot; target=&quot;_blank&quot;&gt;even if those characters are wizards or vampires&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Similarly, when we interact with “friends” on social-networking sites or through texting, it can feel like we’re getting quality social contact, but we are not. It turns out that face-to-face interaction with other people—real people, right in front of us, not characters on TV or friends we communicate via text messages—is absolutely vital for longevity and happiness. In fact, it is a larger contributor than exercise or diet!&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;We need to remind ourselves of our evolutionary history, where we evolved without exposure to realistic representations of people. Back then, if you saw something that looked like a person, by golly it was a person. When you look at a video of a person, most of your brain thinks it’s real—the fusiform face area of your brain area reacts identically whether you’re looking at a real face or a picture of one (in fact, most experiments investigating this part of the brain do not use real faces at all, but photos or videos of them).&lt;/p&gt;
&lt;p&gt;The errors we make when we view non-human things as human satisfies our desire to interact with other people without giving us many of the benefits. In the moment, watching TV feels good; it satisfies your desire to be with other people. But it’s the visual equivalent of empty calories—delicious but not nutritious.&lt;/p&gt;
&lt;p&gt;Get together with a friend instead. Your brain will thank you. &lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;1. Bering, J. (2011). &lt;em&gt;The Belief Instinct: The Psychology of Souls, Destiny, and the Meaning of Life&lt;/em&gt;. W.W. Norton &amp;amp; Company.&lt;/p&gt;
&lt;p&gt;2. &lt;span&gt;Pinker, S. (2014). &lt;em&gt;The Village Effect: How Face-to-Face Contact Can Make Us Happier and Healthier&lt;/em&gt;. Random House Canada.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;&lt;em&gt;Jim Davies is the author or &lt;/em&gt;&lt;a href=&quot;http://www.amazon.com/Riveted-Science-Movies-Religion-Universe/dp/113727901X&quot; target=&quot;_blank&quot;&gt;Riveted: The Science of Why Jokes Make Us Laugh, Movies Make Us Cry, and Religion Makes Us Feel One with the Universe&lt;/a&gt;. &lt;em&gt;He teaches cognitive science at Carleton University.&lt;/em&gt;&lt;br/&gt;&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Sun, 26 Nov 2017 17:43:46 +0000</pubDate>
<dc:creator>dnetesn</dc:creator>
<og:type>website</og:type>
<og:url>http://nautil.us/blog/why-facebook-is-the-junk-food-of-socializing</og:url>
<og:title>Why Facebook Is the Junk Food of Socializing - Facts So Romantic - Nautilus</og:title>
<og:description>Have you ever been walking in a dark alley and seen something that you thought was a crouching person, but it turned out to be a garbage&amp;#8230;</og:description>
<og:image>http://static.nautil.us/6248_e6a4f65e7355bb8b7671c3a18003b146.jpg</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>http://nautil.us/blog/why-facebook-is-the-junk-food-of-socializing</dc:identifier>
</item>
<item>
<title>Lisp in fewer than 200 lines of C</title>
<link>https://carld.github.io/2017/06/20/lisp-in-less-than-200-lines-of-c.html</link>
<guid isPermaLink="true" >https://carld.github.io/2017/06/20/lisp-in-less-than-200-lines-of-c.html</guid>
<description>&lt;div class=&quot;highlighter-rouge&quot; readability=&quot;8&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;11&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;Title: a brief and simple programming language implementation
Tags: lambda calculus, Lisp, C, programming
Authors:
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;em&gt;Objective: implement a lambda calculus based programming language like LisP, simply and briefly in C&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;After learning some Scheme and Lisp and implementing LispKit and reading about eval/apply and how minimal the evaluator is, I decided to try implement Lisp in as little C as I could.&lt;/p&gt;
&lt;p&gt;Since it’s less than 200 lines of C code I’ll just discuss the code inline:&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;cp&quot;&gt;#include &amp;lt;stdio.h&amp;gt;
&lt;/span&gt;  &lt;span class=&quot;cp&quot;&gt;#include &amp;lt;stdlib.h&amp;gt;
&lt;/span&gt;  &lt;span class=&quot;cp&quot;&gt;#include &amp;lt;string.h&amp;gt;
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Included are standard headers files: &lt;code class=&quot;highlighter-rouge&quot;&gt;stdio.h&lt;/code&gt; gives us &lt;code class=&quot;highlighter-rouge&quot;&gt;printf&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;puts&lt;/code&gt; for printing to stdout, and &lt;code class=&quot;highlighter-rouge&quot;&gt;getchar&lt;/code&gt; for retreving a character from stdin. &lt;code class=&quot;highlighter-rouge&quot;&gt;stdlib.h&lt;/code&gt; provides &lt;code class=&quot;highlighter-rouge&quot;&gt;calloc&lt;/code&gt; for dynamically allocating memory while the program is running. &lt;code class=&quot;highlighter-rouge&quot;&gt;string.h&lt;/code&gt; provides &lt;code class=&quot;highlighter-rouge&quot;&gt;strcmp&lt;/code&gt; for comparing two strings, and &lt;code class=&quot;highlighter-rouge&quot;&gt;strdup&lt;/code&gt; for making a duplicate copy of a string.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;8.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;12&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;cp&quot;&gt;#define debug(m,e) printf(&quot;%s:%d: %s:&quot;,__FILE__,__LINE__,m); print_obj(e,1); puts(&quot;&quot;);
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This &lt;code class=&quot;highlighter-rouge&quot;&gt;debug&lt;/code&gt; macro was used to help troubleshoot the program when it didn’t work. I’d add a line like &lt;code class=&quot;highlighter-rouge&quot;&gt;debug('evaluating', exp)&lt;/code&gt; and it would print out the file, line number, a message, and the Lisp expression representation in a readable form.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;k&quot;&gt;typedef&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;List&lt;/code&gt; structure is the fundamental data structure used to represent code and data. It is a singly linked list with two pointers: &lt;code class=&quot;highlighter-rouge&quot;&gt;next&lt;/code&gt; points to the next item in the list, and &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt; points to either a symbol or another list structure. &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt; could be cast to either a &lt;code class=&quot;highlighter-rouge&quot;&gt;char *&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;List *&lt;/code&gt;. To determine which one keep reading (spoiler: pointer tagging is used).&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The global variable &lt;code class=&quot;highlighter-rouge&quot;&gt;symbols&lt;/code&gt; represents the head of a list of symbols. When symbol is parsed, we’ll look for it in the list of symbols, if it’s not there we’ll add it. This way we can compare two symbols by using the equals comparison operator, &lt;code class=&quot;highlighter-rouge&quot;&gt;==&lt;/code&gt;. It saves a little bit of storage space when the same symbol is repeated many times in a LisP program, but with 8GB of RAM memory in my computer I probably won’t notice the space saving.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;look&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/* look ahead character */&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/* token */&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Because a symbol can contain more than one character, we have a complete symbol when a character that doesn’t belong in a symbol is encountered. Non symbol characters include whitespace (space, tab, newline etc), and syntax characters such as parenthesis, &lt;code class=&quot;highlighter-rouge&quot;&gt;(&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;)&lt;/code&gt;. To determine whether the end of a symbol has been reached we need to look ahead by one character. The &lt;code class=&quot;highlighter-rouge&quot;&gt;look&lt;/code&gt; variable stores the look ahead character. If this character contains a non-symbol character we’ll know to stop reading the symbol. The &lt;code class=&quot;highlighter-rouge&quot;&gt;token&lt;/code&gt; variable is an array of characters, it stores the current symbol that has been read from the input. Note that it has a size of 32, so the maximum length of a symbol will be 31 characters, because the token is a NULL terminated string, so the token is always terminated with a &lt;code class=&quot;highlighter-rouge&quot;&gt;\0&lt;/code&gt; character.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;cp&quot;&gt;#define is_space(x)  (x == ' ' || x == '\n')
&lt;/span&gt;  &lt;span class=&quot;cp&quot;&gt;#define is_parens(x) (x == '(' || x == ')')
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The two macros above are really just a convenience for the sake of readability and possibly maintainability and extensibility of the program. &lt;code class=&quot;highlighter-rouge&quot;&gt;is_space&lt;/code&gt; takes a single character and will return true if that character is a space or a newline. &lt;code class=&quot;highlighter-rouge&quot;&gt;is_parens&lt;/code&gt; takes a single character and will return true if that character is a parenthesis.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;gettoken&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;look&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;look&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getchar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_parens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;look&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;look&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;look&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getchar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;look&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EOF&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;look&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_parens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;look&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;look&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;look&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getchar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sc&quot;&gt;'\0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The function &lt;code class=&quot;highlighter-rouge&quot;&gt;gettoken&lt;/code&gt; is responsible for reading characters from standard input and determining whether parenthesis or a symbol has been discovered. First it will skip over any whitespace. If the &lt;code class=&quot;highlighter-rouge&quot;&gt;look&lt;/code&gt; variable, the look ahead character, is a parenthesis, it is stored in &lt;code class=&quot;highlighter-rouge&quot;&gt;token&lt;/code&gt;, and the next character in the input stream read into &lt;code class=&quot;highlighter-rouge&quot;&gt;look&lt;/code&gt;. If the lookahead character is not a parenthesis, it’s assumed to belong to a symbol. Keep looking ahead and saving the character until either &lt;code class=&quot;highlighter-rouge&quot;&gt;EOF&lt;/code&gt; the end of the file is reached, or the look ahead character is whitespace, or the look ahead character is a parenthesis. &lt;code class=&quot;highlighter-rouge&quot;&gt;index&lt;/code&gt; stores the current position in the &lt;code class=&quot;highlighter-rouge&quot;&gt;token&lt;/code&gt; array so it is incremented every time a character belonging to the symbol is stored. At the end the token is NULL terminated.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;cp&quot;&gt;#define is_pair(x) (((long)x &amp;amp; 0x1) == 0x1)  &lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;/* tag pointer to pair with 0x1 (alignment dependent)*/&lt;/span&gt;&lt;span class=&quot;cp&quot;&gt;
&lt;/span&gt;  &lt;span class=&quot;cp&quot;&gt;#define untag(x)   ((long) x &amp;amp; ~0x1)
&lt;/span&gt;  &lt;span class=&quot;cp&quot;&gt;#define tag(x)     ((long) x | 0x1)
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Above contains a curiosity that can be found in many language implementations. Remember from the &lt;code class=&quot;highlighter-rouge&quot;&gt;List&lt;/code&gt; structure that the &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt; pointer can be either a &lt;code class=&quot;highlighter-rouge&quot;&gt;char *&lt;/code&gt; a symbol, or &lt;code class=&quot;highlighter-rouge&quot;&gt;List *&lt;/code&gt; another List. The way we are indicating the type of pointer is by setting the lowest bit on the pointer on. For example, given a pointer to the address &lt;code class=&quot;highlighter-rouge&quot;&gt;0x100200230&lt;/code&gt;, if it’s a pair we’ll modify that pointer with a bitwise or with 1 so the address becomes &lt;code class=&quot;highlighter-rouge&quot;&gt;0x100200231&lt;/code&gt;. The questionable thing about modifying a pointer in this way is how can we tell a pointer tagged with 1, from a regular untagged address. Well, partly as a performance optimization, many computers and their Operating Systems, allocate memory on set boundaries. It’s referred to as memory alignment, and if for example the alignment is to an 8-bit boundary, it means that when memory is allocated it’s address will be a multiple of 8. For example the next 8 bit boundary for the address &lt;code class=&quot;highlighter-rouge&quot;&gt;0x100200230&lt;/code&gt; is &lt;code class=&quot;highlighter-rouge&quot;&gt;0x100200238&lt;/code&gt;. Memory could be aligned to 16-bits, 32-bits as well. Typically it will be aligned on machine word, which means 32-bits if you have a 32-bit CPU and bus. A more thorough discussion is on wikipedia &lt;a href=&quot;https://en.wikipedia.org/wiki/Data_structure_alignment&quot;&gt;https://en.wikipedia.org/wiki/Data_structure_alignment&lt;/a&gt;. Effectively for us it means that whenever we call &lt;code class=&quot;highlighter-rouge&quot;&gt;calloc&lt;/code&gt; we’ll always get back an address where the lowest bit is off (0), so we can set it on if we want. The macro &lt;code class=&quot;highlighter-rouge&quot;&gt;is_pair&lt;/code&gt; returns non-zero if the address is a pair (which means we’ll need to unset the lowest bit to get the address). It uses a bitwise and with 1 to determine this. The &lt;code class=&quot;highlighter-rouge&quot;&gt;untag&lt;/code&gt; macro switches the lowest bit off, with a bitwise and of the ones complement of 1. The &lt;code class=&quot;highlighter-rouge&quot;&gt;tag&lt;/code&gt; macro switches the lowest bit on with a bitwise or of 1.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;cp&quot;&gt;#define car(x)     (((List*)untag(x))-&amp;gt;data)
&lt;/span&gt;  &lt;span class=&quot;cp&quot;&gt;#define cdr(x)     (((List*)untag(x))-&amp;gt;next)
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;There’s two fundamental primitive operations in a typical Lisp/Scheme, &lt;code class=&quot;highlighter-rouge&quot;&gt;car&lt;/code&gt; which returns the head of a list, and &lt;code class=&quot;highlighter-rouge&quot;&gt;cdr&lt;/code&gt; which returns the tail of the list. They are named after operations on an IBM computer, some information on the history is on Wikipedia &lt;a href=&quot;https://en.wikipedia.org/wiki/CAR_and_CDR&quot;&gt;https://en.wikipedia.org/wiki/CAR_and_CDR&lt;/a&gt;. We could as easily call them head and tail, but since they are so ingrained in Lisp and Scheme conventions they are perpetuated here.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;cp&quot;&gt;#define e_true     cons( intern(&quot;quote&quot;), cons( intern(&quot;t&quot;), 0))
&lt;/span&gt;  &lt;span class=&quot;cp&quot;&gt;#define e_false    0
&lt;/span&gt;&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;e_true&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;e_false&lt;/code&gt; macros are a convenience for defining a what true and false are in this implementation. Basically so long as true is non-zero everything should be ok. It will help if the values they have can be readily printed in human readable form.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;7.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;10&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_pair&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;calloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sizeof&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_pair&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_pair&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Another fundamental Lisp/Scheme operation is &lt;code class=&quot;highlighter-rouge&quot;&gt;cons&lt;/code&gt;. It constructs a pair, which means a pair of pointers, in this implementation the &lt;code class=&quot;highlighter-rouge&quot;&gt;List&lt;/code&gt; structure that holds the &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt; pointer and the &lt;code class=&quot;highlighter-rouge&quot;&gt;next&lt;/code&gt; pointer. &lt;a href=&quot;https://en.wikipedia.org/wiki/Cons&quot;&gt;https://en.wikipedia.org/wiki/Cons&lt;/a&gt; Because pointers to a &lt;code class=&quot;highlighter-rouge&quot;&gt;List&lt;/code&gt; (a pair) must be tagged using the lowest bit, we rely on &lt;code class=&quot;highlighter-rouge&quot;&gt;calloc&lt;/code&gt; to provide memory large enough to hold the &lt;code class=&quot;highlighter-rouge&quot;&gt;List&lt;/code&gt; data structure and that the memory is aligned to an address that does not involve the lowest bit. The &lt;code class=&quot;highlighter-rouge&quot;&gt;cons&lt;/code&gt; function here takes two arguments, the first is an address that will be stored in the &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt; field, and the second an address that will be stored in the &lt;code class=&quot;highlighter-rouge&quot;&gt;next&lt;/code&gt; field. Finally the address where the &lt;code class=&quot;highlighter-rouge&quot;&gt;List&lt;/code&gt; structure is stored is returned, after being tagged as a special kind of pointer.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;8.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;12&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;intern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sym&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_pair&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_pair&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_pair&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strncmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sym&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strdup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sym&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Here’s where a symbol is retreived from the global list of symbols, or added if it is not found. It takes a single string argument. It uses &lt;code class=&quot;highlighter-rouge&quot;&gt;strncmp&lt;/code&gt; to determine if anyone of the symbols are equivalent to the string passed in. If we get to the end of the list of symbols and didnt find a match. The symbol is duplicated with &lt;code class=&quot;highlighter-rouge&quot;&gt;strdup&lt;/code&gt; and added to the head of the list. This is the effect of &lt;code class=&quot;highlighter-rouge&quot;&gt;cons&lt;/code&gt; when given an existing list as the second parameter: a new symbol is pushed onto the list, and a new list head is constructed. The reason &lt;code class=&quot;highlighter-rouge&quot;&gt;strdup&lt;/code&gt; is used, and the string is duplicated, is because we want a more permanent copy of the string. When the program runs, the &lt;code class=&quot;highlighter-rouge&quot;&gt;sym&lt;/code&gt; parameter could be a pointer to the &lt;code class=&quot;highlighter-rouge&quot;&gt;token&lt;/code&gt; global variable which will be modified as symbols are read from the input stream. The function is called &lt;code class=&quot;highlighter-rouge&quot;&gt;intern&lt;/code&gt; out of convention, see &lt;a href=&quot;https://en.wikipedia.org/wiki/String_interning&quot;&gt;https://en.wikipedia.org/wiki/String_interning&lt;/a&gt; for more background on string interning.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Above is a forward declaration of the function &lt;code class=&quot;highlighter-rouge&quot;&gt;getlist&lt;/code&gt; which is defined further down. A forward declaration is needed because the &lt;code class=&quot;highlighter-rouge&quot;&gt;getobj&lt;/code&gt; function can call it, and &lt;code class=&quot;highlighter-rouge&quot;&gt;getlist&lt;/code&gt; can call &lt;code class=&quot;highlighter-rouge&quot;&gt;getobj&lt;/code&gt; which is a chicken and egg kind of problem. The C compiler needs to know that the full signature of this function so it can be used before it is defined.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;getobj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;sc&quot;&gt;'('&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;All &lt;code class=&quot;highlighter-rouge&quot;&gt;getobj&lt;/code&gt; has to do is check if the current token from the input stream was an opening parenthesis, which means a list is being defined, and &lt;code class=&quot;highlighter-rouge&quot;&gt;getlist&lt;/code&gt; can be called to construct the list. Otherwise, the token is treated as a symbol, and &lt;code class=&quot;highlighter-rouge&quot;&gt;intern&lt;/code&gt; is used to either return the single copy, or create a single copy and add it to the list of symbols.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;getlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gettoken&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;sc&quot;&gt;')'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getobj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;());&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The function &lt;code class=&quot;highlighter-rouge&quot;&gt;getlist&lt;/code&gt; reads the next token from the input. If the token is a closing parenthesis it returns 0 (a NULL pointer). Otherwise the token is probably a symbol, so call &lt;code class=&quot;highlighter-rouge&quot;&gt;getobj&lt;/code&gt; and intern that symbol, then use &lt;code class=&quot;highlighter-rouge&quot;&gt;cons&lt;/code&gt; to add that symbol to the head of the list, calling &lt;code class=&quot;highlighter-rouge&quot;&gt;getlist&lt;/code&gt; recursively to get the tail of the list. Take note that the variable &lt;code class=&quot;highlighter-rouge&quot;&gt;tmp&lt;/code&gt; - an abbreviation of temporary - and explicity assigned to the return value of &lt;code class=&quot;highlighter-rouge&quot;&gt;getobj&lt;/code&gt; before the &lt;code class=&quot;highlighter-rouge&quot;&gt;cons&lt;/code&gt;. This is to ensure that the list is constructed in the correct order from head towards tail. Before the &lt;code class=&quot;highlighter-rouge&quot;&gt;cons&lt;/code&gt; function is called, it’s arguments are evaluated, and in this case it’s second argument is a function call to &lt;code class=&quot;highlighter-rouge&quot;&gt;getlist&lt;/code&gt;. So &lt;code class=&quot;highlighter-rouge&quot;&gt;getlist&lt;/code&gt; is called again before &lt;code class=&quot;highlighter-rouge&quot;&gt;cons&lt;/code&gt; is called, and either the end of the list (right parens) is discovered, or the next item in the list is. How this recursive function call works is worthwhile understanding. In C, when functions are called, the arguments to the function, and the variables in the function are pushed on top of a data structure called a stack. A stack is literally a stack of things, like a stack of plates, where the last thing on top is the first thing that will come off. The arguments and variables to the function come off the stack when the function returns, literally where you see &lt;code class=&quot;highlighter-rouge&quot;&gt;return&lt;/code&gt; in the code. With every call to the &lt;code class=&quot;highlighter-rouge&quot;&gt;getlist&lt;/code&gt; function as it comes across items in the list it is processing, the stack grows with another set of variables needed by &lt;code class=&quot;highlighter-rouge&quot;&gt;getlist&lt;/code&gt;. So 3 recursive calls to &lt;code class=&quot;highlighter-rouge&quot;&gt;getlist&lt;/code&gt; means the stack grows by 3 times the &lt;code class=&quot;highlighter-rouge&quot;&gt;getlist&lt;/code&gt; functions storage requirements. The inefficiency here is the longer the list, the taller the stack. Some programming languages have a stack overflow error where the stack has out grown the available memory. Wikipedia has a page about this &lt;a href=&quot;https://en.wikipedia.org/wiki/Stack_overflow&quot;&gt;https://en.wikipedia.org/wiki/Stack_overflow&lt;/a&gt; Programming languages like Scheme implement something called tail call optimization where the language can determine if the variables used by a recursive function call will be needed after it returns and if not, it does not grow the stack. This is a pretty cool feature of a programming language and it would be great to have in this language, and maybe we can add it later on. For more on tail calls, &lt;a href=&quot;https://en.wikipedia.org/wiki/Tail_call&quot;&gt;https://en.wikipedia.org/wiki/Tail_call&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;13&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head_of_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;%s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ob&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;null&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head_of_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;(&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;print_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;print_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;print_obj&lt;/code&gt; function is tremendously useful in that it can print either a symbol, or an entire list, to stdout so that we can read it. If the first argument, &lt;code class=&quot;highlighter-rouge&quot;&gt;object&lt;/code&gt; isn’t the specially tagged pointer, it’s just a symbol so it can be output with &lt;code class=&quot;highlighter-rouge&quot;&gt;printf&lt;/code&gt; using the &lt;code class=&quot;highlighter-rouge&quot;&gt;%s&lt;/code&gt; format specifier, which says that the provided pointer is a null terminated string. Otherwise &lt;code class=&quot;highlighter-rouge&quot;&gt;print_obj&lt;/code&gt; is being asked to print a list, so &lt;code class=&quot;highlighter-rouge&quot;&gt;ob&lt;/code&gt; will be the address of a &lt;code class=&quot;highlighter-rouge&quot;&gt;List&lt;/code&gt; structure, meaning it is somewhere, either the beginning, middle or end, or printing a list. the &lt;code class=&quot;highlighter-rouge&quot;&gt;head_of_list&lt;/code&gt; argument is the giveaway here. If &lt;code class=&quot;highlighter-rouge&quot;&gt;head_of_list&lt;/code&gt; is non-zero, it’s the beginning of a new list, so print the left parenthesis. In any case it has to print the value of the current item (it could either be a symbol or a nested listed) so it calls itself with the value of the current head of the list, &lt;code class=&quot;highlighter-rouge&quot;&gt;car(ob)&lt;/code&gt;. If the tail of the list is non-zero, this means there’s more, so as long as the tail of the list is a pointer to another &lt;code class=&quot;highlighter-rouge&quot;&gt;List&lt;/code&gt; structure, print a space, and then print the tail of the list. Otherwise, the tail of the list is zero, which means we’re at the end of the list, so print the closing parenthesis.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;8.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;12&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fcons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)));&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fcar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fcdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;feq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;      &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e_true&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e_false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fpair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;       &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e_true&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e_false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fsym&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e_true&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e_false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fnull&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;           &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e_true&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e_false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;freadobj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;look&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getchar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gettoken&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getobj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fwriteobj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;print_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;puts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e_true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Above are defined the basic primitive operations required by Lisp, all using the same return value and argument specification. These functions will be referenced in the interpreters environment so they can be used from a Lisp program. Because the Lisp language we’re implementing will know nothing about C and how many arguments and what type they should be in C, the arguments are represented using the linked list structure, which has an equivalent Lisp representation using parenthesis, whitespace and symbols. These functions are prefixed with &lt;code class=&quot;highlighter-rouge&quot;&gt;f&lt;/code&gt; which stands for function. They are called indirectly only when a Lisp program looks one up and wants to apply it.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This is a forward declaraction of &lt;code class=&quot;highlighter-rouge&quot;&gt;eval&lt;/code&gt; the meta-circular evaluator.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;13&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;evlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;untag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Above is the &lt;code class=&quot;highlighter-rouge&quot;&gt;evlist&lt;/code&gt; function, short for “evaluate list”. It takes a list and an environment, and evaluates each item in the list, returning a corresponding list with the evaluation of each input item, maintaining the order. There is use of a pointer to a pointer here which makes this code less immediately obvious, but it means we can walk through the list, creating a parallel list with the evaluated elements in the same order. In “The C Programming Language” by Brian Kernighan and Dennis Ritchie, a pointer is said to be a variable that contains the address of another variable. The &lt;code class=&quot;highlighter-rouge&quot;&gt;*&lt;/code&gt; operator dereferences a pointer, giving the object pointed to. The &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;amp;&lt;/code&gt; operator gives the address of a variable. &lt;code class=&quot;highlighter-rouge&quot;&gt;evlist&lt;/code&gt; iterates through the &lt;code class=&quot;highlighter-rouge&quot;&gt;list&lt;/code&gt; argument in a for loop. Two local variables, a pointer, &lt;code class=&quot;highlighter-rouge&quot;&gt;head&lt;/code&gt;, is initialized to 0, the purpose of &lt;code class=&quot;highlighter-rouge&quot;&gt;head&lt;/code&gt; is to store the head of the list that will be returned. &lt;code class=&quot;highlighter-rouge&quot;&gt;args&lt;/code&gt; is a pointer to a pointer, it is initialied to the address of &lt;code class=&quot;highlighter-rouge&quot;&gt;head&lt;/code&gt;. On each iteration, &lt;code class=&quot;highlighter-rouge&quot;&gt;args&lt;/code&gt; is dereferenced and the resulting pointer is assigned to a newly constructed cell. On the next line, &lt;code class=&quot;highlighter-rouge&quot;&gt;args&lt;/code&gt; is assigned to the address of the &lt;code class=&quot;highlighter-rouge&quot;&gt;next&lt;/code&gt; field in that constructed cell. This means that on the next iteration, &lt;code class=&quot;highlighter-rouge&quot;&gt;args&lt;/code&gt; is a pointer to a pointer to the &lt;code class=&quot;highlighter-rouge&quot;&gt;next&lt;/code&gt; field of the previous element. When it is dereferenced with a single &lt;code class=&quot;highlighter-rouge&quot;&gt;*&lt;/code&gt; and assigned, we are effectively setting the &lt;code class=&quot;highlighter-rouge&quot;&gt;next&lt;/code&gt; field to point to the newly constructed cell in the current iteration.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;9&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;apply_primitive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;primfn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;primfn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;apply_primitive&lt;/code&gt; function does nothing more than cast the &lt;code class=&quot;highlighter-rouge&quot;&gt;primfn&lt;/code&gt; to a pointer to a function that takes a single &lt;code class=&quot;highlighter-rouge&quot;&gt;List *&lt;/code&gt; and returns a &lt;code class=&quot;highlighter-rouge&quot;&gt;List *&lt;/code&gt;, and then calls that function with &lt;code class=&quot;highlighter-rouge&quot;&gt;args&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;18.5&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;32&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)));&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;car&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/* special forms */&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;quote&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;if&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;lambda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/* todo: create a closure and capture free vars */&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;apply&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/* apply function to list */&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evlist&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/* assumes one argument and that it is a list */&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;apply_primitive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/* function call */&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;primop&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_pair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;primop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/* user defined lambda, arg list eval happens in binding  below */&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;primop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;primop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/* built-in primitive */&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;apply_primitive&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;primop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;cm&quot;&gt;/* should be a lambda, bind names into env and eval body */&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;lambda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extenv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))),&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vars&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;names&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vars&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;extenv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vars&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;extenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
          &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;extenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;puts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cannot evaluate expression&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;eval&lt;/code&gt; function is the heart of LiSP. It interprets LisP expressions. If the expression is not a pair (not a &lt;code class=&quot;highlighter-rouge&quot;&gt;List&lt;/code&gt; structure), we look for that value it is associated with in the environment. In other implementations of eval, the equivalent test is if the expression is an &lt;code class=&quot;highlighter-rouge&quot;&gt;atom&lt;/code&gt;. Otherwise the expression must be a list, and then the first element of that list is checked, if that first element is not a &lt;code class=&quot;highlighter-rouge&quot;&gt;List&lt;/code&gt; structure - it is a symbol, or more officially an atom, then the following series of if statements handle it: if the first element is a &lt;code class=&quot;highlighter-rouge&quot;&gt;quote&lt;/code&gt; symbol, the next element is return, that is, the head of the tail of the list; if the first element is an &lt;code class=&quot;highlighter-rouge&quot;&gt;if&lt;/code&gt; symbol, the head of the tail of the list is evaluated, if that returns non-zero, the head of the tail of the tail of the list is evaluated and returned, if it returns zero, the head of the tail of the tail of the tail is evaluated and returned. If the first element is the symbol &lt;code class=&quot;highlighter-rouge&quot;&gt;lambda&lt;/code&gt; the expression is simply returned (maybe this is redundant so may indicate a bug or some optimization that is missing). In a Scheme interpreter, a closure would be created and the free variables in the closure captured using the current environment. If the first symbol is &lt;code class=&quot;highlighter-rouge&quot;&gt;apply&lt;/code&gt; that means, in this interpreter at least, that the next element is a function and the element after that, the third element in this list is a list - the &lt;code class=&quot;highlighter-rouge&quot;&gt;(b c d)&lt;/code&gt; in &lt;code class=&quot;highlighter-rouge&quot;&gt;(apply a (b c d))&lt;/code&gt;. The assumption is that &lt;code class=&quot;highlighter-rouge&quot;&gt;apply&lt;/code&gt; is being used to call one of the basic primitive operations defined above: &lt;code class=&quot;highlighter-rouge&quot;&gt;car&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;cdr&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;cons&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;eq?&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;pair?&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;symbol?&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;null?&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;read&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;write&lt;/code&gt;. If the first symbol did not match any of the prior if statements, we assume a the first symbol is in the environment and is either a user defined function - a lambda, or a primitive function (and apply is not being used to call it). We find out which it is by evaluating that first element, if it’s a pair, it’s a list, i.e. an expression in the form &lt;code class=&quot;highlighter-rouge&quot;&gt;(lambda (arg) (body expressions ...))&lt;/code&gt;. If it’s not a pair we assume it’s a pointer to a function, and use &lt;code class=&quot;highlighter-rouge&quot;&gt;apply_primitive&lt;/code&gt; to invocate that function, evaluating it’s arguments before calling it. The remaining block is the &lt;code class=&quot;highlighter-rouge&quot;&gt;else&lt;/code&gt; which meant the first argument in the expression was a pair - eval was called with a list nested inside a list, i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;((x y z))&lt;/code&gt;, and the only form of nested expression handled, is lambda, e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;((lambda (arg) (body expr ...)) value )&lt;/code&gt;. In this case the names of the arguments in the lambda definition are bound to the corresponding values, and the name value pairs are pushed onto the head of the environment, until there are no more arguments (names) left to bind. The body of the lambda is then evaluated with the extended environment.&lt;/p&gt;
&lt;p&gt;A newer article describing eval is called “The Roots of Lisp” by Paul Graham, and can be downloaded from &lt;a href=&quot;http://www.paulgraham.com/rootsoflisp.html&quot;&gt;http://www.paulgraham.com/rootsoflisp.html&lt;/a&gt; A thorough explanation can be found in “Structure and Interpretation of Computer Programs”, by Harold Ableson and Gerald Jay Sussman. This book can be found online: &lt;a href=&quot;https://mitpress.mit.edu/sicp/full-text/book/book-Z-H-26.html#%_sec_4.1&quot;&gt;https://mitpress.mit.edu/sicp/full-text/book/book-Z-H-26.html#%_sec_4.1&lt;/a&gt; The earliest implementation of eval I have found is in the Lisp 1.5 Programmers Manual.&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-rouge&quot; readability=&quot;24&quot;&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;43&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;
&lt;code&gt;  &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;car&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fcar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cdr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fcdr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cons&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fcons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;eq?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pair?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fpair&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;symbol?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fsym&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;null?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fnull&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;read&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;freadobj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;write&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fwriteobj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;null&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cons&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))))))));&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;look&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getchar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gettoken&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;print_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getobj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;main&lt;/code&gt; is the entry point for this program when it is run. It has one variable, &lt;code class=&quot;highlighter-rouge&quot;&gt;env&lt;/code&gt; which is assigned to a list of lists, effectively just associating a symbol with a primitive function. The remaining lines, look ahead one character, load the first token with &lt;code class=&quot;highlighter-rouge&quot;&gt;gettoken&lt;/code&gt;, and then print with &lt;code class=&quot;highlighter-rouge&quot;&gt;print_obj&lt;/code&gt;, the evaluated object read by &lt;code class=&quot;highlighter-rouge&quot;&gt;getobj&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;That is it a very small and incomplete interpreter… Noticeably there is no garbage collection, or even any explicit free of the memory allocated by &lt;code class=&quot;highlighter-rouge&quot;&gt;calloc&lt;/code&gt;. Neither is there any error handling, so a program with missing or unmatched parenthesis, unresolved symbols, etc will likely just result in something like a segmentation fault.&lt;/p&gt;
&lt;p&gt;Despite the limitations, this interpreter provides enough primitive functions to implement an equivalent eval on itself.&lt;/p&gt;
&lt;p&gt;The complete source code and some tests can be found at &lt;a href=&quot;https://github.com/carld/micro-lisp&quot;&gt;https://github.com/carld/micro-lisp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;An implementaion of eval that runs on the interpreter above can be found in &lt;code class=&quot;highlighter-rouge&quot;&gt;repl.lisp&lt;/code&gt;. It implements a Read Eval Print Loop and it can be run using:&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cat repl.lisp - |./micro-lisp&lt;/code&gt;&lt;/p&gt;
</description>
<pubDate>Sun, 26 Nov 2017 17:23:09 +0000</pubDate>
<dc:creator>jfo</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://carld.github.io/2017/06/20/lisp-in-less-than-200-lines-of-c.html</dc:identifier>
</item>
<item>
<title>Some chip makers have hidden latency and jitter issues from common tests</title>
<link>http://www.badmodems.com/</link>
<guid isPermaLink="true" >http://www.badmodems.com/</guid>
<description>[unable to retrieve full-text content]&lt;p&gt;Article URL: &lt;a href=&quot;http://www.badmodems.com/&quot;&gt;http://www.badmodems.com/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=15781474&quot;&gt;https://news.ycombinator.com/item?id=15781474&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Points: 213&lt;/p&gt;&lt;p&gt;# Comments: 41&lt;/p&gt;</description>
<pubDate>Sun, 26 Nov 2017 16:06:23 +0000</pubDate>
<dc:creator>based2</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.badmodems.com/</dc:identifier>
</item>
<item>
<title>Rust: Enable WebAssembly backend by default</title>
<link>https://www.hellorust.com/news/native-wasm-target.html</link>
<guid isPermaLink="true" >https://www.hellorust.com/news/native-wasm-target.html</guid>
<description>&lt;time pubdate=&quot;pubdate&quot;&gt;Nov 26, 2017&lt;/time&gt; - Jan-Erik Rediger
&lt;p&gt;Yesterday the &lt;a href=&quot;https://github.com/rust-lang/rust/pull/46115&quot;&gt;Pull Request 46115: &quot;rustbuild: Enable WebAssembly backend by default&quot;&lt;/a&gt; was merged into Rust master. With the Nightly build from that night, the &lt;code&gt;wasm32-unknown-unknown&lt;/code&gt; target is natively available.&lt;/p&gt;
&lt;p&gt;Once you install that nightly (or any later one from now on), you can compile to WebAssembly without additional tools:&lt;/p&gt;
&lt;pre&gt;
&lt;span&gt;rustup update
&lt;/span&gt;&lt;span&gt;rustup target add wasm32-unknown-unknown --toolchain nightly
&lt;/span&gt;&lt;span&gt;rustc +nightly --target wasm32-unknown-unknown -O hello.rs
&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;Support &amp;amp; documentation is still a bit limited, but we're working to expand in this area. The Rust compiler also does not have a proper linker just now, so final WebAssembly modules will be quite big. Alex wrote a small tool to reduce the size:&lt;/p&gt;
&lt;pre&gt;
&lt;span&gt;cargo install --git https://github.com/alexcrichton/wasm-gc
&lt;/span&gt;&lt;span&gt;wasm-gc hello.wasm small-hello.wasm
&lt;/span&gt;
&lt;/pre&gt;
&lt;p&gt;You can find the full instruction &lt;a href=&quot;https://www.hellorust.com/setup/wasm-target&quot;&gt;in the &lt;code&gt;wasm-32-unknown-unknown&lt;/code&gt; setup guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want to contribute with examples, documentation, articles or other resources open an issue or pull request on &lt;a href=&quot;https://github.com/badboy/hellorust&quot;&gt;github.com/badboy/hellorust&lt;/a&gt;.&lt;/p&gt;
</description>
<pubDate>Sun, 26 Nov 2017 12:58:42 +0000</pubDate>
<dc:creator>a_humean</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.hellorust.com/news/native-wasm-target.html</dc:identifier>
</item>
<item>
<title>Super Tiny Website Logos in SVG</title>
<link>https://shkspr.mobi/blog/2017/11/super-tiny-website-logos-in-svg/</link>
<guid isPermaLink="true" >https://shkspr.mobi/blog/2017/11/super-tiny-website-logos-in-svg/</guid>
<description>&lt;p&gt;You may not realise it, but bandwidth is expensive. It costs you time, money, and battery power whenever you download a file larger than it needs to be.&lt;/p&gt;
&lt;p&gt;That's why I've become a little bit obsessed with SVG - Scalable Vector Graphics. They're the closest thing to magic that the web has when it comes to image compression. Let me show you what I mean.&lt;/p&gt;
&lt;p&gt;This is the standard Twitter Logo. It's 512 * 512 pixels and, even with hefty PNG compression, weighs in at around 20KB.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://shkspr.mobi/blog/wp-content/uploads/2017/10/Twitter-Logo.png&quot; width=&quot;256&quot; height=&quot;256&quot; class=&quot;alignnone&quot; alt=&quot;The Twitter logo is a bird&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Here's the same logo rendered as an SVG. Because it is a vector graphic it can be magnified infinitely without any loss of fidelity.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://edent.github.io/SuperTinyIcons/tiny/twitter.svg&quot; width=&quot;256&quot; class=&quot;alignnone&quot; title=&quot;Twitter&quot; alt=&quot;The Twitter logo is a bird&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The uncompressed SVG is &lt;strong&gt;a mere 397 Bytes&lt;/strong&gt;. Not a typo. You could fit over 3,000 of these images on a floppy disk.&lt;/p&gt;
&lt;p&gt;That's why I have &lt;a href=&quot;https://github.com/edent/SuperTinyIcons&quot;&gt;released SuperTinyIcons on GitHub&lt;/a&gt;. Eighty of the web's most popular logos - each image is under 1KB.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/edent/SuperTinyIcons&quot;&gt;&lt;img src=&quot;https://shkspr.mobi/blog/wp-content/uploads/2017/10/Screenshot-2017-10-25-edent-SuperTinySocialIcons-fs8.png&quot; alt=&quot;Rows of icons - each one has the size printed next to it.&quot; width=&quot;606&quot; height=&quot;552&quot; class=&quot;aligncenter size-full wp-image-28787&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;These files are suitable for use on the web - just use &lt;code&gt;&amp;lt;img src=&quot;...&lt;/code&gt;. They are supported by all popular browsers. I've also converted them to Android Vector Drawables, so they can be incorporated into your apps.&lt;/p&gt;
&lt;p&gt;I've released them as MIT licensed files - although you should check the original images' licences. Some of these logos may be trademarked.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/edent/SuperTinyIcons&quot;&gt;SuperTinyIcons on GitHub&lt;/a&gt;&lt;/p&gt;
&lt;footer class=&quot;entry-footer&quot;/&gt;</description>
<pubDate>Sun, 26 Nov 2017 11:59:26 +0000</pubDate>
<dc:creator>edent</dc:creator>
<og:type>article</og:type>
<og:title>Super Tiny Website Logos in SVG</og:title>
<og:url>https://shkspr.mobi/blog/2017/11/super-tiny-website-logos-in-svg/</og:url>
<og:description>You may not realise it, but bandwidth is expensive. It costs you time, money, and battery power whenever you download a file larger than it needs to be. That’s why I’ve become a little …</og:description>
<og:image>https://shkspr.mobi/blog/wp-content/uploads/2017/10/Screenshot-2017-10-25-edent-SuperTinySocialIcons-fs8.png</og:image>
<dc:language>en-GB</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://shkspr.mobi/blog/2017/11/super-tiny-website-logos-in-svg/</dc:identifier>
</item>
</channel>
</rss>