<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Why things might have taken so long</title>
<link>https://meteuphoric.wordpress.com/2017/12/31/16417/</link>
<guid isPermaLink="true" >https://meteuphoric.wordpress.com/2017/12/31/16417/</guid>
<description>&lt;p&gt;I asked &lt;a href=&quot;https://meteuphoric.wordpress.com/2017/12/28/why-did-everything-take-so-long/&quot;&gt;why humanity took so long to do anything at the start&lt;/a&gt;, and the Internet gave me its thoughts. Here is my expanded list of hypotheses, summarizing from comments &lt;a href=&quot;https://meteuphoric.wordpress.com/2017/12/28/why-did-everything-take-so-long/#comments&quot;&gt;on the post&lt;/a&gt;, &lt;a href=&quot;http://marginalrevolution.com/marginalrevolution/2017/12/saturday-assorted-links-141.html&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;https://www.lesserwrong.com/posts/mFqG58s4NE3EE68Lq/why-did-everything-take-so-long&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;strong&gt;Inventing is harder than it looks&lt;/strong&gt;&lt;/p&gt;
&lt;ol class=&quot;ol1&quot;&gt;&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;Inventions are usually more ingenious than they seem.&lt;/strong&gt; Relatedly, reality has a &lt;a href=&quot;http://johnsalvatier.org/blog/2017/reality-has-a-surprising-amount-of-detail&quot;&gt;lot of detail&lt;/a&gt;.&lt;/li&gt;
&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;There are lots of apparent paths&lt;/strong&gt;: without hindsight, you have to waste a lot of time on dead ends.&lt;/li&gt;
&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;People are not as inventive as they imagine.&lt;/strong&gt; For instance, I haven’t actually invented anything – why do I even imagine I could invent rope?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Posing the question is a large part of the work.&lt;/strong&gt; If you have never seen rope, it actually doesn’t occur to you that rope would come in handy, or to ask yourself how to make some.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Animals (including humans) mostly think by intuitively recognizing over time what is promising and not among affordances they have&lt;/strong&gt;, and reading what common observations imply. New affordances generally only appear by some outside force e.g. accidentally. To invent a thing, you have to somehow have an affordance to make it even though you have never seen it. And in retrospect it seems so obvious because now you do have the affordance.&lt;/li&gt;
&lt;/ol&gt;&lt;p class=&quot;p1&quot;&gt;&lt;strong&gt;People fifty thousand years ago were not really behaviorally modern&lt;/strong&gt;&lt;/p&gt;
&lt;ol class=&quot;ol1&quot;&gt;&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;People’s brains were actually biologically less functional fifty thousand years ago.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Having concepts in general is a big deal&lt;/strong&gt;. You need a foundation of knowledge and mental models to come up with more of them.&lt;/li&gt;
&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;We lacked a small number of unimaginably basic concepts&lt;/strong&gt; that it is hard to even imagine not having now. For instance ‘abstraction’, or ‘changing the world around you to make it better’.&lt;/li&gt;
&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;Having external thinking tools is a big deal.&lt;/strong&gt; Modern ‘human intelligence’ relies a lot on things like writing and collected data, that aren’t in anyone’s brain.&lt;/li&gt;
&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;The entire mental landscapes of early people was very different&lt;/strong&gt;, as Julian Jaynes suggests.&lt;span class=&quot;Apple-converted-space&quot;&gt; &lt;/span&gt; In particular, they lacked self awareness and the ability to have original thought rather than just repeating whatever they usually repeat.&lt;/li&gt;
&lt;/ol&gt;&lt;p class=&quot;p1&quot;&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/p&gt;
&lt;ol class=&quot;ol1&quot;&gt;&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;Often A isn’t useful without B, and B isn’t useful without A.&lt;/strong&gt; For instance, A is chariots and B is roads.&lt;/li&gt;
&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;A isn’t useful without lots of other things&lt;/strong&gt;, which don’t depend on A, but take longer to accrue than you imagine.&lt;/li&gt;
&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;Lots of ways to solve problems don’t lead to great things in the long run.&lt;/strong&gt; ‘Crude hacks’ get you most of the way there, reducing the value of great inventions.&lt;/li&gt;
&lt;/ol&gt;&lt;p class=&quot;p1&quot;&gt;&lt;strong&gt;Nobody can do much at all&lt;/strong&gt;&lt;/p&gt;
&lt;ol class=&quot;ol1&quot;&gt;&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;People in general are stupid in all domains, even now. &lt;/strong&gt;Everything is always mysteriously a thousand times harder than you might think.&lt;/li&gt;
&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;Have I tried even &lt;em&gt;making&lt;/em&gt; rope from scratch?&lt;/strong&gt; Let alone inventing it?&lt;/li&gt;
&lt;/ol&gt;&lt;p class=&quot;p1&quot;&gt;&lt;strong&gt;People were really busy&lt;/strong&gt;&lt;/p&gt;
&lt;ol class=&quot;ol1&quot;&gt;&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;Poverty traps.&lt;/strong&gt; Inventing only pays off long term, so for anyone to do it you need spare wealth and maybe institutions for capital to fund invention.&lt;/li&gt;
&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;People are just really busy doing and thinking about other things.&lt;/strong&gt; Like mating and dancing and eating and so on.&lt;/li&gt;
&lt;/ol&gt;&lt;p class=&quot;p1&quot;&gt;&lt;strong&gt;Communication and records&lt;/strong&gt;&lt;/p&gt;
&lt;ol class=&quot;ol1&quot;&gt;&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;The early humans did have those things, we just don’t have good records.&lt;/strong&gt; Which is not surprising, because our records of those times are clearly very lacking.&lt;/li&gt;
&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;Things got invented a lot, but communication wasn’t good/common enough to spread them.&lt;/strong&gt; For instance because tribes were small and didn’t interact that much).&lt;/li&gt;
&lt;/ol&gt;&lt;p class=&quot;p1&quot;&gt;&lt;strong&gt;Social costs&lt;/strong&gt;&lt;/p&gt;
&lt;ol class=&quot;ol1&quot;&gt;&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;Technology might have been seen as a sign of weakness or laziness&lt;/strong&gt;&lt;/li&gt;
&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;Making technology might make you stand out rather than fit in&lt;/strong&gt;&lt;/li&gt;
&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;Productivity shames your peers and invites more work from you&lt;/strong&gt;&lt;/li&gt;
&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;Inventions are sometimes against received wisdom&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p class=&quot;p1&quot;&gt;&lt;strong&gt;Population&lt;/strong&gt;&lt;/p&gt;
&lt;ol class=&quot;ol1&quot;&gt;&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;There were very few people in the past&lt;/strong&gt;, so the total thinking occurring between 50k and 28k years ago was less than in the last hundred years.&lt;/li&gt;
&lt;/ol&gt;&lt;p class=&quot;p1&quot;&gt;&lt;strong&gt;Value&lt;/strong&gt;&lt;/p&gt;
&lt;ol class=&quot;ol1&quot;&gt;&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;We didn’t invent things until they became relevant at all, and most of these things aren’t relevant to a hunter-gatherer.&lt;/strong&gt;&lt;/li&gt;
&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;Innovation is risky&lt;/strong&gt;: if you try a new thing, you might die.&lt;/li&gt;
&lt;/ol&gt;&lt;p class=&quot;p1&quot;&gt;&lt;strong&gt;Orders of invention&lt;/strong&gt;&lt;/p&gt;
&lt;ol class=&quot;ol1&quot;&gt;&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;First order inventions&lt;/strong&gt; are those where the raw materials are in your immediate surroundings, and they don’t require huge amounts of skill. My intuition is mostly that first order inventions should have been faster. But maybe we did get very good at first order ones quickly, but it is hard to move to higher orders.&lt;/li&gt;
&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;You need a full-time craftsman to make most basic things to a quality where they are worth having&lt;/strong&gt;, and we couldn’t afford full-time craftsmen for a very long time.&lt;/li&gt;
&lt;li class=&quot;li1&quot;&gt;&lt;strong&gt;Each new layer requires the last layer of innovation be common enough that it is available everywhere&lt;/strong&gt;, for the next person to use.&lt;/li&gt;
&lt;/ol&gt;&lt;div id=&quot;jp-post-flair&quot; class=&quot;sharedaddy sd-like-enabled sd-sharing-enabled&quot;&gt;
&lt;div class=&quot;sharedaddy sd-sharing-enabled&quot;&gt;
&lt;div class=&quot;robots-nocontent sd-block sd-social sd-social-icon sd-sharing&quot;&gt;
&lt;h3 class=&quot;sd-title&quot;&gt;Share this:&lt;/h3&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
</description>
<pubDate>Thu, 04 Jan 2018 03:49:20 +0000</pubDate>
<dc:creator>blonky</dc:creator>
<og:type>article</og:type>
<og:title>Why everything might have taken so long</og:title>
<og:url>https://meteuphoric.wordpress.com/2017/12/31/16417/</og:url>
<og:description>I asked why humanity took so long to do anything at the start, and the Internet gave me its thoughts. Here is my expanded list of hypotheses, summarizing from comments on the post, here, and here. …</og:description>
<og:image>https://secure.gravatar.com/blavatar/aecfd073c78a69a918bba38e17092fdb?s=200&amp;ts=1515081602</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://meteuphoric.wordpress.com/2017/12/31/16417/</dc:identifier>
</item>
<item>
<title>Intel was aware of the chip vulnerability when its CEO sold off company stock</title>
<link>http://www.businessinsider.com/intel-ceo-krzanich-sold-shares-after-company-was-informed-of-chip-flaw-2018-1</link>
<guid isPermaLink="true" >http://www.businessinsider.com/intel-ceo-krzanich-sold-shares-after-company-was-informed-of-chip-flaw-2018-1</guid>
<description>&lt;p&gt;&lt;span class=&quot;KonaFilter image-container display-table image on-image&quot; data-post-image=&quot;&quot;&gt;&lt;img src=&quot;http://static4.businessinsider.com/image/5a4e470ccf698a34008b4ced-2400/gettyimages-630985290.jpg&quot; alt=&quot;Intel CEO Brian Krzanich&quot; data-mce-source=&quot;Getty/David Becker&quot;/&gt;&lt;span class=&quot;caption-source&quot;&gt;&lt;span class=&quot;caption&quot;&gt;Intel CEO Brian Krzanich&lt;/span&gt; &lt;span class=&quot;source&quot;&gt;Getty/David Becker&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Intel CEO Brian Krzanich sold off $24 million worth of stock and options in the company in late November.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The stock sale came after Google had informed Intel of a significant vulnerability in its chips — a flaw that became public only this week.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Intel says the stock sale was unrelated to the vulnerability and came as part of a planned divestiture program. But Krzanich put that stock-sale plan in place in October — several months after Intel was informed of the vulnerability.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;hr/&gt;&lt;p&gt;&lt;br/&gt;Intel CEO Brian Krzanich sold off a large portion of his stake in the company months after Google had informed the chipmaker of a significant security vulnerability in its flagship PC processors — but before the problem was publicly known.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;http://www.businessinsider.com/intel-ceo-google-discovered-the-chip-flaw-months-ago-2018-1&quot;&gt;vulnerability&lt;/a&gt;, which affects processors from Intel, AMD, and ARM and could allow malicious actors to steal passwords and other secret data, became public this week. The disclosure has left processor makers and operating-system vendors including Intel and Microsoft scrambling to get on top of the story and patch their products.&lt;/p&gt;
&lt;p&gt;But while the public is just being informed about the security problem, tech companies have known about it for months. In fact, Google informed Intel of the vulnerability in June, an Intel representative told Business Insider in a statement.&lt;/p&gt;
&lt;p&gt;That means Intel was aware of the problem before Krzanich sold off a big chunk of his holdings. Intel's CEO saw a $24 million windfall November 29 through a combination of selling shares he owned outright and exercising stock options.&lt;/p&gt;
&lt;p&gt;The stock sale &lt;a href=&quot;https://www.fool.com/investing/2017/12/19/intels-ceo-just-sold-a-lot-of-stock.aspx&quot;&gt;raised eyebrows&lt;/a&gt; when it was disclosed, primarily because it left Krzanich with just 250,000 shares of Intel stock — the minimum the company requires him to hold under his employment agreement.&lt;/p&gt;

&lt;p&gt;But &lt;a href=&quot;https://gizmodo.com/intel-says-ceo-dumping-tons-of-stock-last-year-unrelate-1821739988&quot;&gt;the sell-off&lt;/a&gt; could draw even more scrutiny now, given the news about the security vulnerability and the timing of when Intel knew about it.&lt;/p&gt;
&lt;p&gt;A representative for the Securities and Exchange Commission declined to comment on whether it was looking into the stock sales.&lt;/p&gt;
&lt;h2&gt;Intel says the sale was preplanned — but that plan was put in place months after it learned of the chip vulnerability&lt;/h2&gt;
&lt;p&gt;In the statement, the Intel representative said Krzanich's sale had nothing to do with the newly disclosed chip vulnerability and was done as part of a standard stock-sale plan.&lt;/p&gt;
&lt;p&gt;&quot;Brian's sale is unrelated,&quot; the representative said in the statement, adding that Krzanich &quot;continues to hold shares in line with corporate guidelines.&quot;&lt;/p&gt;
&lt;p&gt;To avoid charges of trading on insider knowledge, executives often put in place plans that automatically sell a portion of their stock holdings or exercise some of their options on a predetermined schedule, typically referred to as Rule 10b5-1(c) trading plans. According to an SEC filing, the holdings that Krzanich sold in November — 245,743 shares of stock he owned outright and 644,135 shares he got from exercising his options — were divested under just such a trading plan.&lt;/p&gt;
&lt;p&gt;But Krzanich &lt;a href=&quot;https://www.sec.gov/Archives/edgar/data/50863/000112760217033679/xslF345X03/form4.xml&quot;&gt;put that plan in place only on October 30, according to the filing&lt;/a&gt;. The representative said his decision to set up that plan was &quot;unrelated&quot; to information about the security vulnerability. Still, the timeline raises questions.&lt;/p&gt;
&lt;p&gt;News of the security flaw helped send Intel's stock lower Wednesday. It closed down $1.59, or 3.4%, to $45.26.&lt;/p&gt;
&lt;p class=&quot;tagline&quot;&gt;Get the latest Intel stock price &lt;a href=&quot;http://markets.businessinsider.com/stock/intc-Quote&quot;&gt;here.&lt;/a&gt;&lt;/p&gt;


</description>
<pubDate>Thu, 04 Jan 2018 01:49:14 +0000</pubDate>
<dc:creator>MollyR</dc:creator>
<og:title>Intel was aware of the chip vulnerability when its CEO sold off $24 million in company stock</og:title>
<og:description>Intel CEO Brian Krzanich sold off a major stake in the company in November, months after the chipmaker learned of a significant security flaw in its chips.</og:description>
<og:type>article</og:type>
<og:url>http://www.businessinsider.com/intel-ceo-krzanich-sold-shares-after-company-was-informed-of-chip-flaw-2018-1</og:url>
<og:image>http://static3.businessinsider.com/image/5a4e4730cf698a87008b4cd0-1190-625/intel-was-aware-of-the-chip-vulnerability-when-its-ceo-sold-off-24-million-in-company-stock.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.businessinsider.com/intel-ceo-krzanich-sold-shares-after-company-was-informed-of-chip-flaw-2018-1</dc:identifier>
</item>
<item>
<title>Avoid speculative indirect calls in kernel</title>
<link>https://lkml.org/lkml/2018/1/3/797</link>
<guid isPermaLink="true" >https://lkml.org/lkml/2018/1/3/797</guid>
<description>&lt;pre itemprop=&quot;articleBody&quot;&gt;
On Wed, Jan 3, 2018 at 3:09 PM, Andi Kleen &amp;lt;andi@firstfloor.org&amp;gt; wrote:&lt;br /&gt;&amp;gt; This is a fix for Variant 2 in&lt;br /&gt;&amp;gt; &lt;a href=&quot;https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html&quot;&gt;https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html&lt;/a&gt;&lt;br /&gt;&amp;gt;&lt;br /&gt;&amp;gt; Any speculative indirect calls in the kernel can be tricked&lt;br /&gt;&amp;gt; to execute any kernel code, which may allow side channel&lt;br /&gt;&amp;gt; attacks that can leak arbitrary kernel data.&lt;p&gt;Why is this all done without any configuration options?&lt;/p&gt;&lt;p&gt;A *competent* CPU engineer would fix this by making sure speculation&lt;br /&gt;doesn't happen across protection domains. Maybe even a L1 I$ that is&lt;br /&gt;keyed by CPL.&lt;/p&gt;&lt;p&gt;I think somebody inside of Intel needs to really take a long hard look&lt;br /&gt;at their CPU's, and actually admit that they have issues instead of&lt;br /&gt;writing PR blurbs that say that everything works as designed.&lt;/p&gt;&lt;p&gt;.. and that really means that all these mitigation patches should be&lt;br /&gt;written with &quot;not all CPU's are crap&quot; in mind.&lt;/p&gt;&lt;p&gt;Or is Intel basically saying &quot;we are committed to selling you shit&lt;br /&gt;forever and ever, and never fixing anything&quot;?&lt;/p&gt;&lt;p&gt;Because if that's the case, maybe we should start looking towards the&lt;br /&gt;ARM64 people more.&lt;/p&gt;&lt;p&gt;Please talk to management. Because I really see exactly two possibibilities:&lt;/p&gt;&lt;p&gt;- Intel never intends to fix anything&lt;/p&gt;&lt;p&gt;OR&lt;/p&gt;&lt;p&gt;- these workarounds should have a way to disable them.&lt;/p&gt;&lt;p&gt;Which of the two is it?&lt;/p&gt;&lt;p&gt;Linus&lt;/p&gt;&lt;/pre&gt;</description>
<pubDate>Thu, 04 Jan 2018 00:47:39 +0000</pubDate>
<dc:creator>grover_hartmann</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://lkml.org/lkml/2018/1/3/797</dc:identifier>
</item>
<item>
<title>Mitigations landing for new class of timing attack</title>
<link>https://blog.mozilla.org/security/2018/01/03/mitigations-landing-new-class-timing-attack/</link>
<guid isPermaLink="true" >https://blog.mozilla.org/security/2018/01/03/mitigations-landing-new-class-timing-attack/</guid>
<description>&lt;p&gt;Several recently-published &lt;a href=&quot;https://spectreattack.com/spectre.pdf&quot;&gt;research&lt;/a&gt; &lt;a href=&quot;https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html&quot;&gt;articles&lt;/a&gt; have demonstrated a new class of timing attacks (Meltdown and Spectre) that work on modern CPUs.  Our internal experiments confirm that it is possible to use similar techniques from Web content to read private information between different origins.  The full extent of this class of attack is still under investigation and we are working with security researchers and other browser vendors to fully understand the threat and fixes.  Since this new class of attacks involves measuring precise time intervals, as a partial, short-term, mitigation we are disabling or reducing the precision of several time sources in Firefox.  This includes both explicit sources, like &lt;code&gt;performance.now()&lt;/code&gt;, and implicit sources that allow building high-resolution timers, viz., &lt;code&gt;SharedArrayBuffer&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Specifically, in all release channels, starting with 57:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;The resolution of &lt;code&gt;performance.now()&lt;/code&gt; will be reduced to 20µs.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;SharedArrayBuffer&lt;/code&gt; feature is being disabled by default.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Furthermore, other &lt;a href=&quot;https://gruss.cc/files/fantastictimers.pdf&quot;&gt;timing sources and time-fuzzing techniques&lt;/a&gt; are being worked on.&lt;/p&gt;
&lt;p&gt;In the longer term, we have started experimenting with techniques to remove the information leak closer to the source, instead of just hiding the leak by disabling timers.  This project requires time to understand, implement and test, but might allow us to consider reenabling &lt;code&gt;SharedArrayBuffer&lt;/code&gt; and the other high-resolution timers as these features provide important capabilities to the Web platform.&lt;/p&gt;
</description>
<pubDate>Thu, 04 Jan 2018 00:29:30 +0000</pubDate>
<dc:creator>Nrbelex</dc:creator>
<og:title>Mitigations landing for new class of timing attack</og:title>
<og:url>https://blog.mozilla.org/security/2018/01/03/mitigations-landing-new-class-timing-attack/</og:url>
<og:description>Several recently-published research articles have demonstrated a new class of timing attacks (Meltdown and Spectre) that work on modern CPUs.  Our internal experiments confirm that ...</og:description>
<og:image>https://blog.mozilla.org/security/wp-content/themes/OneMozilla/img/mozilla-wordmark.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.mozilla.org/security/2018/01/03/mitigations-landing-new-class-timing-attack/</dc:identifier>
</item>
<item>
<title>HDR Photography in Microsoft Excel (2017) [video]</title>
<link>https://www.youtube.com/watch?v=bkQJdaGGVM8</link>
<guid isPermaLink="true" >https://www.youtube.com/watch?v=bkQJdaGGVM8</guid>
<description>&lt;p id=&quot;eow-description&quot; class=&quot;&quot;&gt;HDR Photography in Microsoft Excel?! by Kevin Chen&lt;/p&gt;&lt;p&gt;Have you ever taken a photo with areas that are too bright or too dark? As any photographer will tell you, high dynamic range photography is the right way to solve your problem. And, as any businessperson will tell you, Microsoft Excel is the right platform to implement your solution.&lt;/p&gt;&lt;p&gt;In this talk, I’ll explain the algorithm from one of the foundational papers about HDR imaging — no prior image processing knowledge required. Turns out, it’s just a system of linear equations! So, obviously, the next step is to implement HDR in a spreadsheet. Because we can. The end result reveals how this complicated-sounding algorithm boils down to a few simple ideas.&lt;/p&gt;&lt;p&gt;Kevin is an amateur photographer and Microsoft Office enthusiast. He also studies computer science at Columbia University.&lt;/p&gt;
</description>
<pubDate>Wed, 03 Jan 2018 23:19:27 +0000</pubDate>
<dc:creator>rayshan</dc:creator>
<og:url>https://www.youtube.com/watch?v=bkQJdaGGVM8</og:url>
<og:title>!!Con 2017: HDR Photography in Microsoft Excel?! by Kevin Chen</og:title>
<og:image>https://i.ytimg.com/vi/bkQJdaGGVM8/maxresdefault.jpg</og:image>
<og:description>HDR Photography in Microsoft Excel?! by Kevin Chen Have you ever taken a photo with areas that are too bright or too dark? As any photographer will tell you,...</og:description>
<og:type>video</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.youtube.com/watch?v=bkQJdaGGVM8</dc:identifier>
</item>
<item>
<title>Reading privileged memory with a side-channel</title>
<link>https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html</link>
<guid isPermaLink="true" >https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html</guid>
<description>&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Posted by Jann Horn, Project Zero&lt;/span&gt;&lt;/div&gt;
&lt;strong id=&quot;docs-internal-guid-c66856d3-bdac-1e12-91fc-544573d27dc6&quot;&gt;&lt;br/&gt;&lt;/strong&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We have discovered that CPU data cache timing can be abused to efficiently leak information out of mis-speculated execution, leading to (at worst) arbitrary virtual memory read vulnerabilities across local security boundaries in various contexts.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Variants of this issue are known to affect many modern processors, including certain processors by Intel, AMD and ARM. For a few Intel and AMD CPU models, we have exploits that work against real software. We reported this issue to Intel, AMD and ARM on 2017-06-01 [1]&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;So far, there are three known variants of the issue:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Variant 1: bounds check bypass (&lt;/span&gt;&lt;span&gt;CVE-2017-5753)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Variant 2: branch target injection (&lt;/span&gt;&lt;span&gt;CVE-2017-5715)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Variant 3: rogue data cache load (&lt;/span&gt;&lt;span&gt;CVE-2017-5754)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Before the issues described here were publicly disclosed, Daniel Gruss, Moritz Lipp, Yuval Yarom, Paul Kocher, Daniel Genkin, Michael Schwarz, Mike Hamburg, Stefan Mangard, Thomas Prescher and Werner Haas also reported them; their [writeups/blogposts/paper drafts] are at:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;During the course of our research, we developed the following proofs of concept (PoCs):&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ol&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A PoC that demonstrates the basic principles behind variant 1 in userspace on the tested Intel Haswell Xeon CPU, the AMD FX CPU, the AMD PRO CPU and an ARM Cortex A57 [2].&lt;/span&gt; &lt;span&gt;This PoC only tests for the ability to read data inside mis-speculated execution within the same process, without crossing any privilege boundaries.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A PoC for variant 1 that, when running with normal user privileges under a modern Linux kernel with a distro-standard config, can perform arbitrary reads in a 4GiB range [3]&lt;/span&gt; &lt;span&gt;in kernel virtual memory on the Intel Haswell Xeon CPU. If the kernel's BPF JIT is enabled (non-default configuration), it also works on the AMD PRO CPU. On the Intel Haswell Xeon CPU, kernel virtual memory can be read at a rate of around 2000 bytes per second after around 4 seconds of startup time. [4]&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A PoC for variant 2 that, when running with root privileges inside a KVM guest created using virt-manager on the Intel Haswell Xeon CPU, with a specific (now outdated) version of Debian's distro kernel&lt;/span&gt; &lt;span&gt;[5] running on the host, can read host kernel memory at a rate of around 1500 bytes/second, with room for optimization. Before the attack can be performed, some initialization has to be performed that takes roughly between 10 and 30 minutes for a machine with 64GiB of RAM; the needed time should scale roughly linearly with the amount of host RAM. (If 2MB hugepages are available to the guest, the initialization should be much faster, but that hasn't been tested.)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A PoC for variant 3 that, when running with normal user privileges, can read kernel memory on the Intel Haswell Xeon CPU under some precondition. We believe that this precondition is that the targeted kernel memory is present in the L1D cache.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;For interesting resources around this topic, look down into the &quot;Literature&quot; section.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A warning regarding explanations about processor internals in this blogpost: This blogpost contains a lot of speculation about hardware internals based on observed behavior, which might not necessarily correspond to what processors are actually doing.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We have some ideas on possible mitigations and provided some of those ideas to the processor vendors; however, we believe that the processor vendors are in a much better position than we are to design and evaluate mitigations, and we expect them to be the source of authoritative guidance.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The PoC code and the writeups that we sent to the CPU vendors will be made available at a later date.&lt;/span&gt;&lt;/div&gt;

&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz (called &quot;Intel Haswell Xeon CPU&quot; in the rest of this document)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;AMD FX(tm)-8320 Eight-Core Processor (called &quot;AMD FX CPU&quot; in the rest of this document)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;AMD PRO A8-9600 R7, 10 COMPUTE CORES 4C+6G (called &quot;AMD PRO CPU&quot; in the rest of this document)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;An ARM Cortex A57 core of a Google Nexus 5x phone [6]&lt;/span&gt; &lt;span&gt;(called &quot;ARM Cortex A57&quot; in the rest of this document)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;retire:&lt;/span&gt; &lt;span&gt;An instruction retires when its results, e.g. register writes and memory writes, are committed and made visible to the rest of the system. Instructions can be executed out of order, but must always retire in order.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;logical processor core:&lt;/span&gt; &lt;span&gt;A logical processor core is what the operating system sees as a processor core. With hyperthreading enabled, the number of logical cores is a multiple of the number of physical cores.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;cached/uncached data:&lt;/span&gt; &lt;span&gt;In this blogpost, &quot;uncached&quot; data is data that is only present in main memory, not in any of the cache levels of the CPU. Loading uncached data will typically take over 100 cycles of CPU time.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;speculative execution:&lt;/span&gt; &lt;span&gt;A processor can execute past a branch without knowing whether it will be taken or where its target is, therefore executing instructions before it is known whether they should be executed. If this speculation turns out to have been incorrect, the CPU can discard the resulting state without architectural effects and continue execution on the correct execution path. Instructions do not retire before it is known that they are on the correct execution path.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;mis-speculation window:&lt;/span&gt; &lt;span&gt;The time window during which the CPU speculatively executes the wrong code and has not yet detected that mis-speculation has occurred.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section explains the common theory behind all three variants and the theory behind our PoC for variant 1 that, when running in userspace under a Debian distro kernel, can perform arbitrary reads in a 4GiB region of kernel memory in at least the following configurations:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel Haswell Xeon CPU, eBPF JIT is off (default state)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel Haswell Xeon CPU, eBPF JIT is on (non-default state)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;AMD PRO CPU, eBPF JIT is on (non-default state)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The state of the eBPF JIT can be toggled using the net.core.bpf_jit_enable sysctl.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Theoretical explanation&lt;/span&gt;&lt;/h2&gt;

&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Branch prediction predicts the branch target and enables the&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;processor to begin executing instructions long before the branch&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;true execution path is known.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In section 2.3.5.2 (&quot;L1 DCache&quot;):&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Loads can:&lt;/span&gt;&lt;/div&gt;

&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Be carried out speculatively, before preceding branches are resolved.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Take cache misses out of order and in an overlapped manner.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel's Software Developer's Manual&lt;/span&gt; &lt;span&gt;[7] states in Volume 3A, section 11.7 (&quot;Implicit Caching (Pentium 4, Intel Xeon, and P6 family processors&quot;):&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Implicit caching occurs when a memory element is made potentially cacheable, although the element may never have been accessed in the normal von Neumann sequence. Implicit caching occurs on the P6 and more recent processor families due to aggressive prefetching, branch prediction, and TLB miss handling. Implicit caching is an extension of the behavior of existing Intel386, Intel486, and Pentium processor systems, since software running on these processor families also has not been able to deterministically predict the behavior of instruction prefetch.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Consider the code sample below. If&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt; &lt;span&gt;is uncached, the processor can speculatively load data from&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[untrusted_offset_from_caller]&lt;/span&gt;&lt;span&gt;. This is an out-of-bounds read. That should not matter because the processor will effectively roll back the execution state when the branch has executed; none of the speculatively executed instructions will retire (e.g. cause registers etc. to be affected).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned long length;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned char data[];&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array *arr1 = ...;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;unsigned long untrusted_offset_from_caller = ...;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;if (untrusted_offset_from_caller &amp;lt;&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt;&lt;span&gt;) {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned char value =&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[untrusted_offset_from_caller]&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;&lt;/div&gt;


&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;However, in the following code sample, there's an issue. If&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[0x200]&lt;/span&gt; &lt;span&gt;and&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[0x300]&lt;/span&gt; &lt;span&gt;are not cached, but all other accessed data is, and the branch conditions are predicted as true, the processor can do the following speculatively before&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt; &lt;span&gt;has been loaded and the execution is re-steered:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;load&lt;/span&gt; &lt;span&gt;value =&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[&lt;/span&gt;&lt;span&gt;untrusted_offset_from_caller&lt;/span&gt;&lt;span&gt;]&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;start a load from a data-dependent offset in&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data&lt;/span&gt;&lt;span&gt;, loading the corresponding cache line into the L1 cache&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned long length;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned char data[];&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array *arr1 = ...; /* small array */&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array *arr2 = ...; /* array of size 0x400 */&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;/* &amp;gt;0x400&lt;/span&gt; &lt;span&gt;(OUT OF BOUNDS!)&lt;/span&gt; &lt;span&gt;*/&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;unsigned long untrusted_offset_from_caller = ...;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;if (&lt;/span&gt;&lt;span&gt;untrusted_offset_from_caller&lt;/span&gt; &lt;span&gt;&amp;lt;&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt;&lt;span&gt;) {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned char value =&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[&lt;/span&gt;&lt;span&gt;untrusted_offset_from_caller&lt;/span&gt;&lt;span&gt;]&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned long index2 = ((value&amp;amp;1)*0x100)+0x200;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; if (index2 &amp;lt; arr2-&amp;gt;length) {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;   unsigned char value2 =&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[index2]&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;&lt;/div&gt;


&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;After the execution has been returned to the non-speculative path because the processor has noticed that&lt;/span&gt; &lt;span&gt;untrusted_offset_from_caller&lt;/span&gt; &lt;span&gt;is bigger than&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt;&lt;span&gt;, the cache line containing&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[index2]&lt;/span&gt; &lt;span&gt;stays in the L1 cache. By measuring the time required to load&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[0x200]&lt;/span&gt; &lt;span&gt;and&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[0x300]&lt;/span&gt;&lt;span&gt;, an attacker can then determine whether the value of&lt;/span&gt; &lt;span&gt;index2&lt;/span&gt; &lt;span&gt;during speculative execution was 0x200 or 0x300 - which discloses whether&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[&lt;/span&gt;&lt;span&gt;untrusted_offset_from_caller&lt;/span&gt;&lt;span&gt;]&lt;/span&gt;&lt;span&gt;&amp;amp;1&lt;/span&gt; &lt;span&gt;is 0 or 1.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To be able to actually use this behavior for an attack, an attacker needs to be able to cause the execution of such a vulnerable code pattern in the targeted context with an out-of-bounds index. For this, the vulnerable code pattern must either be present in existing code, or there must be an interpreter or JIT engine that can be used to generate the vulnerable code pattern. So far, we have not actually identified any existing, exploitable instances of the vulnerable code pattern; the PoC for leaking kernel memory using variant 1 uses the eBPF interpreter or the eBPF JIT engine, which are built into the kernel and accessible to normal users.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A minor variant of this could be to instead use an out-of-bounds read to a function pointer to gain control of execution in the mis-speculated path. We did not investigate this variant further.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Attacking the kernel&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section describes in more detail how variant 1 can be used to leak Linux kernel memory using the eBPF bytecode interpreter and JIT engine. While there are many interesting potential targets for variant 1 attacks, we chose to attack the Linux in-kernel eBPF JIT/interpreter because it provides more control to the attacker than most other JITs.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The Linux kernel supports eBPF since version 3.18. Unprivileged userspace code can supply bytecode to the kernel that is verified by the kernel and then:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;either interpreted by an in-kernel bytecode interpreter&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;or translated to native machine code that also runs in kernel context using a JIT engine (which translates individual bytecode instructions without performing any further optimizations)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Execution of the bytecode can be triggered by attaching the eBPF bytecode to a socket as a filter and then sending data through the other end of the socket.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Whether the JIT engine is enabled depends on a run-time configuration setting - but at least on the tested Intel processor, the attack works independent of that setting.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Unlike classic BPF, eBPF has data types like data arrays and function pointer arrays into which eBPF bytecode can index. Therefore, it is possible to create the code pattern described above in the kernel using eBPF bytecode.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;eBPF's data arrays are less efficient than its function pointer arrays, so the attack will use the latter where possible.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Both machines on which this was tested have no SMAP, and the PoC relies on that (but it shouldn't be a precondition in principle).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Additionally, at least on the Intel machine on which this was tested, bouncing modified cache lines between cores is slow, apparently because the MESI protocol is used for cache coherence [8]&lt;/span&gt;&lt;span&gt;. Changing the reference counter of an eBPF array on one physical CPU core causes the cache line containing the reference counter to be bounced over to that CPU core, making reads of the reference counter on all other CPU cores slow until the changed reference counter has been written back to memory. Because the length and the reference counter of an eBPF array are stored in the same cache line, this also means that changing the reference counter on one physical CPU core causes reads of the eBPF array's length to be slow on other physical CPU cores (intentional false sharing).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The attack uses two eBPF programs. The first one tail-calls through a page-aligned eBPF function pointer array&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;at a configurable index. In simplified terms, this program is used to determine the address of&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;by guessing the offset from&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;to a userspace address and tail-calling through&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;at the guessed offsets. To cause the branch prediction to predict that the offset is below the length of&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt;&lt;span&gt;, tail calls to an in-bounds index are performed in between. To increase the mis-speculation window, the cache line containing the length of&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;is bounced to another core. To test whether an offset guess was successful, it can be tested whether the userspace address has been loaded into the cache.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Because such straightforward brute-force guessing of the address would be slow, the following optimization is used: 2&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;adjacent userspace memory mappings [9]&lt;/span&gt;&lt;span&gt;, each consisting of 2&lt;/span&gt;&lt;span&gt;4&lt;/span&gt; &lt;span&gt;pages, are created at the userspace address&lt;/span&gt; &lt;span&gt;user_mapping_area&lt;/span&gt;&lt;span&gt;, covering a total area of 2&lt;/span&gt;&lt;span&gt;31&lt;/span&gt; &lt;span&gt;bytes. Each mapping maps the same physical pages, and all mappings are present in the pagetables.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-64YRnkgMujY/Wk064WtrSRI/AAAAAAAACPg/O4ZxlUiWerACL61P490xrVMWSmHTwQflQCEwYBhgL/s1600/image3.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;138&quot; data-original-width=&quot;624&quot; height=&quot;139&quot; src=&quot;https://3.bp.blogspot.com/-64YRnkgMujY/Wk064WtrSRI/AAAAAAAACPg/O4ZxlUiWerACL61P490xrVMWSmHTwQflQCEwYBhgL/s640/image3.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This permits the attack to be carried out in steps of 2&lt;/span&gt;&lt;span&gt;31&lt;/span&gt; &lt;span&gt;bytes. For each step, after causing an out-of-bounds access through&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt;&lt;span&gt;, only one cache line each from the first 2&lt;/span&gt;&lt;span&gt;4&lt;/span&gt; &lt;span&gt;pages of&lt;/span&gt; &lt;span&gt;user_mapping_area&lt;/span&gt; &lt;span&gt;have to be tested for cached memory. Because the L3 cache is physically indexed, any access to a virtual address mapping a physical page will cause all other virtual addresses mapping the same physical page to become cached as well.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;When this attack finds a hit—a cached memory location—the upper 33 bits of the kernel address are known (because they can be derived from the address guess at which the hit occurred), and the low 16 bits of the address are also known (from the offset inside&lt;/span&gt; &lt;span&gt;user_mapping_area&lt;/span&gt; &lt;span&gt;at which the hit was found). The remaining part of the address of&lt;/span&gt; &lt;span&gt;user_mapping_area&lt;/span&gt; &lt;span&gt;is the middle.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-kSdcfy3GVSw/Wk065qWZZaI/AAAAAAAACPs/7HRZEXij3rIqvAf4gq8QpyHKvJsT2pYIQCEwYBhgL/s1600/image5.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;149&quot; data-original-width=&quot;624&quot; height=&quot;152&quot; src=&quot;https://1.bp.blogspot.com/-kSdcfy3GVSw/Wk065qWZZaI/AAAAAAAACPs/7HRZEXij3rIqvAf4gq8QpyHKvJsT2pYIQCEwYBhgL/s640/image5.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The remaining bits in the middle can be determined by bisecting the remaining address space: Map two physical pages to adjacent ranges of virtual addresses, each virtual address range the size of half of the remaining search space, then determine the remaining address bit-wise.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;At this point, a second eBPF program can be used to actually leak data. In pseudocode, this program looks as follows:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t bitmask = &amp;lt;runtime-configurable&amp;gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t bitshift_selector = &amp;lt;runtime-configurable&amp;gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t prog_array_base_offset = &amp;lt;runtime-configurable&amp;gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t secret_data_offset = &amp;lt;runtime-configurable&amp;gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;// index will be bounds-checked by the runtime,&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;// but the bounds check will be bypassed speculatively&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t secret_data = bpf_map_read(array=victim_array, index=secret_data_offset);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;// select a single bit, move it to a specific position, and add the base offset&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t progmap_index = (((secret_data &amp;amp; bitmask) &amp;gt;&amp;gt; bitshift_selector) &amp;lt;&amp;lt; 7) + prog_array_base_offset;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;bpf_tail_call(prog_map, progmap_index);&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This program reads 8-byte-aligned 64-bit values from an eBPF data array &quot;&lt;/span&gt;&lt;span&gt;victim_map&lt;/span&gt;&lt;span&gt;&quot; at a runtime-configurable offset and bitmasks and bit-shifts the value so that one bit is mapped to one of two values that are 2&lt;/span&gt;&lt;span&gt;7&lt;/span&gt; &lt;span&gt;bytes apart (sufficient to not land in the same or adjacent cache lines when used as an array index). Finally it adds a 64-bit offset, then uses the resulting value as an offset into&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;for a tail call.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This program can then be used to leak memory by repeatedly calling the eBPF program with an out-of-bounds offset into&lt;/span&gt; &lt;span&gt;victim_map&lt;/span&gt; &lt;span&gt;that specifies the data to leak and an out-of-bounds offset into&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;that causes&lt;/span&gt; &lt;span&gt;prog_map + offset&lt;/span&gt; &lt;span&gt;to point to a userspace memory area. Misleading the branch prediction and bouncing the cache lines works the same way as for the first eBPF program, except that now, the cache line holding the length of&lt;/span&gt; &lt;span&gt;victim_map&lt;/span&gt; &lt;span&gt;must also be bounced to another core.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section describes the theory behind our PoC for variant 2 that, when running with root privileges inside a KVM guest created using virt-manager on the Intel Haswell Xeon CPU, with a specific version of Debian's distro kernel running on the host, can read host kernel memory at a rate of around 1500 bytes/second.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Basics&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Prior research (see the Literature section at the end) has shown that it is possible for code in separate security contexts to influence each other's branch prediction. So far, this has only been used to infer information about where code is located (in other words, to create interference from the victim to the attacker); however, the basic hypothesis of this attack variant is that it can also be used to redirect execution of code in the victim context (in other words, to create interference from the attacker to the victim; the other way around).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-O6-JHlkvq5U/Wk064bpm2qI/AAAAAAAACPk/rjpYr72vyF0jYASgT-w4NBFQXC3hgIZZwCEwYBhgL/s1600/image2.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;200&quot; data-original-width=&quot;278&quot; src=&quot;https://3.bp.blogspot.com/-O6-JHlkvq5U/Wk064bpm2qI/AAAAAAAACPk/rjpYr72vyF0jYASgT-w4NBFQXC3hgIZZwCEwYBhgL/s1600/image2.png&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The basic idea for the attack is to target victim code that contains an indirect branch whose target address is loaded from memory and flush the cache line containing the target address out to main memory. Then, when the CPU reaches the indirect branch, it won't know the true destination of the jump, and it won't be able to calculate the true destination until it has finished loading the cache line back into the CPU, which takes a few hundred cycles. Therefore, there is a time window of typically over 100 cycles in which the CPU will speculatively execute instructions based on branch prediction.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Haswell branch prediction internals&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Some of the internals of the branch prediction implemented by Intel's processors have already been published; however, getting this attack to work properly required significant further experimentation to determine additional details.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section focuses on the branch prediction internals that were experimentally derived from the Intel Haswell Xeon CPU.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Haswell seems to have multiple branch prediction mechanisms that work very differently:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A generic branch predictor that can only store one target per source address; used for all kinds of jumps, like absolute jumps, relative jumps and so on.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A specialized indirect call predictor that can store multiple targets per source address; used for indirect calls.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;(There is also a specialized return predictor, according to Intel's optimization manual, but we haven't analyzed that in detail yet. If this predictor could be used to reliably dump out some of the call stack through which a VM was entered, that would be very interesting.)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Generic predictor&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The generic branch predictor, as documented in prior research, only uses the lower 31 bits of the address of the last byte of the source instruction for its prediction. If, for example, a branch target buffer (BTB) entry exists for a jump from 0x4141.0004.1000 to 0x4141.0004.5123, the generic predictor will also use it to predict a jump from 0x4242.0004.1000. When the higher bits of the source address differ like this, the higher bits of the predicted destination change together with it—in this case, the predicted destination address will be 0x4242.0004.5123—so apparently this predictor doesn't store the full, absolute destination address.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Before the lower 31 bits of the source address are used to look up a BTB entry, they are folded together using XOR. Specifically, the following bits are folded together:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;
&lt;table&gt;&lt;colgroup&gt;&lt;col width=&quot;*&quot;/&gt;&lt;col width=&quot;*&quot;/&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x100.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x200.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x400.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x800.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x2000.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x4000.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In other words, if a source address is XORed with both numbers in a row of this table, the branch predictor will not be able to distinguish the resulting address from the original source address when performing a lookup. For example, the branch predictor is able to distinguish source addresses 0x100.0000 and 0x180.0000, and it can also distinguish source addresses 0x100.0000 and 0x180.8000, but it can't distinguish source addresses 0x100.0000 and 0x140.2000 or source addresses 0x100.0000 and 0x180.4000. In the following, this will be referred to as aliased source addresses.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;When an aliased source address is used, the branch predictor will still predict the same target as for the unaliased source address. This indicates that the branch predictor stores a truncated absolute destination address, but that hasn't been verified.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Based on observed maximum forward and backward jump distances for different source addresses, the low 32-bit half of the target address could be stored as an absolute 32-bit value with an additional bit that specifies whether the jump from source to target crosses a 2&lt;/span&gt;&lt;span&gt;32&lt;/span&gt; &lt;span&gt;boundary; if the jump crosses such a boundary, bit 31 of the source address determines whether the high half of the instruction pointer should increment or decrement.&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Indirect call predictor&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The inputs of the BTB lookup for this mechanism seem to be:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The low 12 bits of the address of the source instruction (we are not sure whether it's the address of the first or the last byte) or a subset of them.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The branch history buffer state.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;If the indirect call predictor can't resolve a branch, it is resolved by the generic predictor instead. Intel's optimization manual hints at this behavior: &quot;Indirect Calls and Jumps. These may either be predicted as having a monotonic target or as having targets that vary in accordance with recent program behavior.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The branch history buffer (BHB) stores information about the last 29 taken branches - basically a fingerprint of recent control flow - and is used to allow better prediction of indirect calls that can have multiple targets.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The update function of the BHB works as follows (in pseudocode;&lt;/span&gt; &lt;span&gt;src&lt;/span&gt; &lt;span&gt;is the address of the last byte of the source instruction,&lt;/span&gt; &lt;span&gt;dst&lt;/span&gt; &lt;span&gt;is the destination address):&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;void bhb_update(uint58_t *bhb_state, unsigned long src, unsigned long dst) {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state &amp;lt;&amp;lt;= 2;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (dst &amp;amp; 0x3f);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0xc0) &amp;gt;&amp;gt; 6;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0xc00) &amp;gt;&amp;gt; (10 - 2);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0xc000) &amp;gt;&amp;gt; (14 - 4);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0x30) &amp;lt;&amp;lt; (6 - 4);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0x300) &amp;lt;&amp;lt; (8 - 8);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0x3000) &amp;gt;&amp;gt; (12 - 10);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0x30000) &amp;gt;&amp;gt; (16 - 12);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0xc0000) &amp;gt;&amp;gt; (18 - 14);&lt;/span&gt;&lt;/div&gt;

&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Some of the bits of the BHB state seem to be folded together further using XOR when used for a BTB access, but the precise folding function hasn't been understood yet.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The BHB is interesting for two reasons. First, knowledge about its approximate behavior is required in order to be able to accurately cause collisions in the indirect call predictor. But it also permits dumping out the BHB state at any repeatable program state at which the attacker can execute code - for example, when attacking a hypervisor, directly after a hypercall. The dumped BHB state can then be used to fingerprint the hypervisor or, if the attacker has access to the hypervisor binary, to determine the low 20 bits of the hypervisor load address (in the case of KVM: the low 20 bits of the load address of kvm-intel.ko).&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Reverse-Engineering Branch Predictor Internals&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This subsection describes how we reverse-engineered the internals of the Haswell branch predictor. Some of this is written down from memory, since we didn't keep a detailed record of what we were doing.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We initially attempted to perform BTB injections into the kernel using the generic predictor, using the knowledge from prior research that the generic predictor only looks at the lower half of the source address and that only a partial target address is stored. This kind of worked - however, the injection success rate was very low, below 1%. (This is the method we used in our preliminary PoCs for method 2 against modified hypervisors running on Haswell.)&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We decided to write a userspace test case to be able to more easily test branch predictor behavior in different situations.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Based on the assumption that branch predictor state is shared between hyperthreads [10]&lt;/span&gt;&lt;span&gt;, we wrote a program of which two instances are each pinned to one of the two logical processors running on a specific physical core, where one instance attempts to perform branch injections while the other measures how often branch injections are successful. Both instances were executed with ASLR disabled and had the same code at the same addresses. The injecting process performed indirect calls to a function that accesses a (per-process) test variable; the measuring process performed indirect calls to a function that tests, based on timing, whether the per-process test variable is cached, and then evicts it using CLFLUSH. Both indirect calls were performed through the same callsite. Before each indirect call, the function pointer stored in memory was flushed out to main memory using CLFLUSH to widen the speculation time window. Additionally, because of the reference to &quot;recent program behavior&quot; in Intel's optimization manual, a bunch of conditional branches that are always taken were inserted in front of the indirect call.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In this test, the injection success rate was above 99%, giving us a base setup for future experiments.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-RMKau2Jstd0/Wk066oeGPUI/AAAAAAAACP0/sb5X95OHei0JV0xhCQDRhaYhRsLvx9ZmACEwYBhgL/s1600/image7.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;644&quot; data-original-width=&quot;618&quot; height=&quot;640&quot; src=&quot;https://2.bp.blogspot.com/-RMKau2Jstd0/Wk066oeGPUI/AAAAAAAACP0/sb5X95OHei0JV0xhCQDRhaYhRsLvx9ZmACEwYBhgL/s640/image7.png&quot; width=&quot;612&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We then tried to figure out the details of the prediction scheme. We assumed that the prediction scheme uses a global branch history buffer of some kind.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To determine the duration for which branch information stays in the history buffer, a conditional branch that is only taken in one of the two program instances was inserted in front of the series of always-taken conditional jumps, then the number of always-taken conditional jumps (N) was varied. The result was that for N=25, the processor was able to distinguish the branches (misprediction rate under 1%), but for N=26, it failed to do so (misprediction rate over 99%).&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Therefore, the branch history buffer had to be able to store information about at least the last 26 branches.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The code in one of the two program instances was then moved around in memory. This revealed that only the lower 20 bits of the source and target addresses have an influence on the branch history buffer.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Testing with different types of branches in the two program instances revealed that static jumps, taken conditional jumps, calls and returns influence the branch history buffer the same way; non-taken conditional jumps don't influence it; the address of the last byte of the source instruction is the one that counts; IRETQ doesn't influence the history buffer state (which is useful for testing because it permits creating program flow that is invisible to the history buffer).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Moving the last conditional branch before the indirect call around in memory multiple times revealed that the branch history buffer contents can be used to distinguish many different locations of that last conditional branch instruction. This suggests that the history buffer doesn't store a list of small history values; instead, it seems to be a larger buffer in which history data is mixed together.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;However, a history buffer needs to &quot;forget&quot; about past branches after a certain number of new branches have been taken in order to be useful for branch prediction. Therefore, when new data is mixed into the history buffer, this can not cause information in bits that are already present in the history buffer to propagate downwards - and given that, upwards combination of information probably wouldn't be very useful either. Given that branch prediction also must be very fast, we concluded that it is likely that the update function of the history buffer left-shifts the old history buffer, then XORs in the new state (see diagram).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-crbBTenupqw/Wk066CBQ2qI/AAAAAAAACPw/GaUAa5tUDE8dV9tZ6bt-QWGLY5klZM5FgCEwYBhgL/s1600/image6.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;117&quot; data-original-width=&quot;624&quot; height=&quot;118&quot; src=&quot;https://1.bp.blogspot.com/-crbBTenupqw/Wk066CBQ2qI/AAAAAAAACPw/GaUAa5tUDE8dV9tZ6bt-QWGLY5klZM5FgCEwYBhgL/s640/image6.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;If this assumption is correct, then the history buffer contains a lot of information about the most recent branches, but only contains as many bits of information as are shifted per history buffer update about the last branch about which it contains any data. Therefore, we tested whether flipping different bits in the source and target addresses of a jump followed by 32 always-taken jumps with static source and target allows the branch prediction to disambiguate an indirect call. [11]&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;With 32 static jumps in between, no bit flips seemed to have an influence, so we decreased the number of static jumps until a difference was observable. The result with 28 always-taken jumps in between was that bits 0x1 and 0x2 of the target and bits 0x40 and 0x80 of the source had such an influence; but flipping both 0x1 in the target and 0x40 in the source or 0x2 in the target and 0x80 in the source did not permit disambiguation. This shows that the per-insertion shift of the history buffer is 2 bits and shows which data is stored in the least significant bits of the history buffer. We then repeated this with decreased amounts of fixed jumps after the bit-flipped jump to determine which information is stored in the remaining bits.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Reading host memory from a KVM guest&lt;/span&gt;&lt;/h2&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Locating the host kernel&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Our PoC locates the host kernel in several steps. The information that is determined and necessary for the next steps of the attack consists of:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;lower 20 bits of the address of kvm-intel.ko&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;full address of kvm.ko&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;full address of vmlinux&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Looking back, this is unnecessarily complicated, but it nicely demonstrates the various techniques an attacker can use. A simpler way would be to first determine the address of vmlinux, then bisect the addresses of kvm.ko and kvm-intel.ko.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In the first step, the address of kvm-intel.ko is leaked. For this purpose, the branch history buffer state after guest entry is dumped out. Then, for every possible value of bits 12..19 of the load address of kvm-intel.ko, the expected lowest 16 bits of the history buffer are computed based on the load address guess and the known offsets of the last 8 branches before guest entry, and the results are compared against the lowest 16 bits of the leaked history buffer state.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The branch history buffer state is leaked in steps of 2 bits by measuring misprediction rates of an indirect call with two targets. One way the indirect call is reached is from a vmcall instruction followed by a series of N branches whose relevant source and target address bits are all zeroes. The second way the indirect call is reached is from a series of controlled branches in userspace that can be used to write arbitrary values into the branch history buffer.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Misprediction rates are measured as in the section &quot;Reverse-Engineering Branch Predictor Internals&quot;, using one call target that loads a cache line and another one that checks whether the same cache line has been loaded.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-juuq6gYvhn4/Wk065XER9yI/AAAAAAAACPo/EuTi1HzweloKCChZajY7FUDEtHXhs12ZwCEwYBhgL/s1600/image4.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;462&quot; data-original-width=&quot;451&quot; height=&quot;640&quot; src=&quot;https://3.bp.blogspot.com/-juuq6gYvhn4/Wk065XER9yI/AAAAAAAACPo/EuTi1HzweloKCChZajY7FUDEtHXhs12ZwCEwYBhgL/s640/image4.png&quot; width=&quot;624&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;With N=29, mispredictions will occur at a high rate if the controlled branch history buffer value is zero because all history buffer state from the hypercall has been erased. With N=28, mispredictions will occur if the controlled branch history buffer value is one of 0&amp;lt;&amp;lt;(28*2), 1&amp;lt;&amp;lt;(28*2), 2&amp;lt;&amp;lt;(28*2), 3&amp;lt;&amp;lt;(28*2) - by testing all four possibilities, it can be detected which one is right. Then, for decreasing values of N, the four possibilities are {0|1|2|3}&amp;lt;&amp;lt;(28*2) | (history_buffer_for(N+1) &amp;gt;&amp;gt; 2). By repeating this for decreasing values for N, the branch history buffer value for N=0 can be determined.&lt;/span&gt;&lt;/div&gt;

&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-uAWgXXpjs8I/Wk064FS-g3I/AAAAAAAACPc/0nh9nbSn0EkTXgzbbuY1TRROTR02GWIFQCEwYBhgL/s1600/image1.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;315&quot; data-original-width=&quot;457&quot; height=&quot;440&quot; src=&quot;https://4.bp.blogspot.com/-uAWgXXpjs8I/Wk064FS-g3I/AAAAAAAACPc/0nh9nbSn0EkTXgzbbuY1TRROTR02GWIFQCEwYBhgL/s640/image1.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;At this point, the low 20 bits of kvm-intel.ko are known; the next step is to roughly locate kvm.ko.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;For this, the generic branch predictor is used, using data inserted into the BTB by an indirect call from kvm.ko to kvm-intel.ko that happens on every hypercall; this means that the source address of the indirect call has to be leaked out of the BTB.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;kvm.ko will probably be located somewhere in the range from&lt;/span&gt; &lt;span&gt;0xffffffffc0000000&lt;/span&gt; &lt;span&gt;to&lt;/span&gt; &lt;span&gt;0xffffffffc4000000&lt;/span&gt;&lt;span&gt;, with page alignment (0x1000). This means that the first four entries in the table in the section &quot;Generic Predictor&quot; apply; there will be 2&lt;/span&gt;&lt;span&gt;4&lt;/span&gt;&lt;span&gt;-1=15 aliasing addresses for the correct one. But that is also an advantage: It cuts down the search space from 0x4000 to 0x4000/2&lt;/span&gt;&lt;span&gt;4&lt;/span&gt;&lt;span&gt;=1024.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To find the right address for the source or one of its aliasing addresses, code that loads data through a specific register is placed at all possible call targets (the leaked low 20 bits of kvm-intel.ko plus the in-module offset of the call target plus a multiple of 2&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;&lt;span&gt;) and indirect calls are placed at all possible call sources. Then, alternatingly, hypercalls are performed and indirect calls are performed through the different possible non-aliasing call sources, with randomized history buffer state that prevents the specialized prediction from working. After this step, there are 2&lt;/span&gt;&lt;span&gt;16&lt;/span&gt; &lt;span&gt;remaining possibilities for the load address of kvm.ko.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Next, the load address of vmlinux can be determined in a similar way, using an indirect call from vmlinux to kvm.ko. Luckily, none of the bits which are randomized in the load address of vmlinux  are folded together, so unlike when locating kvm.ko, the result will directly be unique. vmlinux has an alignment of 2MiB and a randomization range of 1GiB, so there are still only 512 possible addresses.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Because (as far as we know) a simple hypercall won't actually cause indirect calls from vmlinux to kvm.ko, we instead use port I/O from the status register of an emulated serial port, which is present in the default configuration of a virtual machine created with virt-manager.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The only remaining piece of information is which one of the 16 aliasing load addresses of kvm.ko is actually correct. Because the source address of an indirect call to kvm.ko is known, this can be solved using bisection: Place code at the various possible targets that, depending on which instance of the code is speculatively executed, loads one of two cache lines, and measure which one of the cache lines gets loaded.&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Identifying cache sets&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The PoC assumes that the VM does not have access to hugepages.To discover eviction sets for all L3 cache sets with a specific alignment relative to a 4KiB page boundary, the PoC first allocates 25600 pages of memory. Then, in a loop, it selects random subsets of all remaining unsorted pages such that the expected number of sets for which an eviction set is contained in the subset is 1, reduces each subset down to an eviction set by repeatedly accessing its cache lines and testing whether the cache lines are always cached (in which case they're probably not part of an eviction set) and attempts to use the new eviction set to evict all remaining unsorted cache lines to determine whether they are in the same cache set [12].&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Locating the host-virtual address of a guest page&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Because this attack uses a FLUSH+RELOAD approach for leaking data, it needs to know the host-kernel-virtual address of one guest page. Alternative approaches such as PRIME+PROBE should work without that requirement.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The basic idea for this step of the attack is to use a branch target injection attack against the hypervisor to load an attacker-controlled address and test whether that caused the guest-owned page to be loaded. For this, a gadget that simply loads from the memory location specified by R8 can be used - R8-R11 still contain guest-controlled values when the first indirect call after a guest exit is reached on this kernel build.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We expected that an attacker would need to either know which eviction set has to be used at this point or brute-force it simultaneously; however, experimentally, using random eviction sets works, too. Our theory is that the observed behavior is actually the result of L1D and L2 evictions, which might be sufficient to permit a few instructions worth of speculative execution.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The host kernel maps (nearly?) all physical memory in the physmap area, including memory assigned to KVM guests. However, the location of the physmap is randomized (with a 1GiB alignment), in an area of size 128PiB. Therefore, directly bruteforcing the host-virtual address of a guest page would take a long time. It is not necessarily impossible; as a ballpark estimate, it should be possible within a day or so, maybe less, assuming 12000 successful injections per second and 30 guest pages that are tested in parallel; but not as impressive as doing it in a few minutes.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To optimize this, the problem can be split up: First, brute-force the physical address using a gadget that can load from physical addresses, then brute-force the base address of the physmap region. Because the physical address can usually be assumed to be far below 128PiB, it can be brute-forced more efficiently, and brute-forcing the base address of the physmap region afterwards is also easier because then address guesses with 1GiB alignment can be used.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To brute-force the physical address, the following gadget can be used:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9def:       4c 89 c0                mov    rax,r8&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9df2:       4d 63 f9                movsxd r15,r9d&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9df5:       4e 8b 04 fd c0 b3 a6    mov    r8,QWORD PTR [r15*8-0x7e594c40]&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9dfc:       81&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9dfd:       4a 8d 3c 00             lea    rdi,[rax+r8*1]&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9e01:       4d 8b a4 00 f8 00 00    mov    r12,QWORD PTR [r8+rax*1+0xf8]&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9e08:       00&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This gadget permits loading an 8-byte-aligned value from the area around the kernel text section by setting R9 appropriately, which in particular permits loading&lt;/span&gt; &lt;span&gt;page_offset_base&lt;/span&gt;&lt;span&gt;, the start address of the physmap. Then, the value that was originally in R8 - the physical address guess minus 0xf8 - is added to the result of the previous load, 0xfa is added to it, and the result is dereferenced.&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Cache set selection&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To select the correct L3 eviction set, the attack from the following section is essentially executed with different eviction sets until it works.&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Leaking data&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;At this point, it would normally be necessary to locate gadgets in the host kernel code that can be used to actually leak data by reading from an attacker-controlled location, shifting and masking the result appropriately and then using the result of that as offset to an attacker-controlled address for a load. But piecing gadgets together and figuring out which ones work in a speculation context seems annoying. So instead, we decided to use the eBPF interpreter, which is built into the host kernel - while there is no legitimate way to invoke it from inside a VM, the presence of the code in the host kernel's text section is sufficient to make it usable for the attack, just like with ordinary ROP gadgets.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The eBPF interpreter entry point has the following function signature:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;static unsigned int __bpf_prog_run(void *ctx, const struct bpf_insn *insn)&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The second parameter is a pointer to an array of statically pre-verified eBPF instructions to be executed - which means that&lt;/span&gt; &lt;span&gt;__bpf_prog_run()&lt;/span&gt; &lt;span&gt;will not perform any type checks or bounds checks. The first parameter is simply stored as part of the initial emulated register state, so its value doesn't matter.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The eBPF interpreter provides, among other things:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;multiple emulated 64-bit registers&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;64-bit immediate writes to emulated registers&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;memory reads from addresses stored in emulated registers&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;bitwise operations (including bit shifts) and arithmetic operations&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To call the interpreter entry point, a gadget that gives RSI and RIP control given R8-R11 control and controlled data at a known memory location is necessary. The following gadget provides this functionality:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff81514edd:       4c 89 ce                mov    rsi,r9&lt;/span&gt;&lt;span&gt;&lt;br class=&quot;kix-line-break&quot;/&gt;&lt;/span&gt;&lt;span&gt;ffffffff81514ee0:       41 ff 90 b0 00 00 00    call   QWORD PTR [r8+0xb0]&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Now, by pointing R8 and R9 at the mapping of a guest-owned page in the physmap, it is possible to speculatively execute arbitrary unvalidated eBPF bytecode in the host kernel. Then, relatively straightforward bytecode can be used to leak data into the cache.&lt;/span&gt;&lt;/div&gt;


&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In summary, an attack using this variant of the issue attempts to read kernel memory from userspace without misdirecting the control flow of kernel code. This works by using the code pattern that was used for the previous variants, but in userspace. The underlying idea is that the permission check for accessing an address might not be on the critical path for reading data from memory to a register, where the permission check could have significant performance impact. Instead, the memory read could make the result of the read available to following instructions immediately and only perform the permission check asynchronously, setting a flag in the reorder buffer that causes an exception to be raised if the permission check fails.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We do have a few additions to make to Anders Fogh's blogpost:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Imagine the following instruction executed in usermode&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;mov rax,[somekernelmodeaddress]&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It will cause an interrupt when retired, [...]&quot;&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It is also possible to already execute that instruction behind a high-latency mispredicted branch to avoid taking a page fault. This might also widen the speculation window by increasing the delay between the read from a kernel address and delivery of the associated exception.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;First, I call a syscall that touches this memory. Second, I use the prefetcht0 instruction to improve my odds of having the address loaded in L1.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;When we used prefetch instructions after doing a syscall, the attack stopped working for us, and we have no clue why. Perhaps the CPU somehow stores whether access was denied on the last access and prevents the attack from working if that is the case?&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Fortunately I did not get a slow read suggesting that Intel null’s the result when the access is not allowed.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;That (read from kernel address returns all-zeroes) seems to happen for memory that is not sufficiently cached but for which pagetable entries are present, at least after repeated read attempts. For unmapped memory, the kernel address read does not return a result at all.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We believe that our research provides many remaining research topics that we have not yet investigated, and we encourage other public researchers to look into these.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section contains an even higher amount of speculation than the rest of this blogpost - it contains untested ideas that might well be useless.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Leaking without data cache timing&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It would be interesting to explore whether there are microarchitectural attacks other than measuring data cache timing that can be used for exfiltrating data out of speculative execution.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Other microarchitectures&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Our research was relatively Haswell-centric so far. It would be interesting to see details e.g. on how the branch prediction of other modern processors works and how well it can be attacked.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Other JIT engines&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We developed a successful variant 1 attack against the JIT engine built into the Linux kernel. It would be interesting to see whether attacks against more advanced JIT engines with less control over the system are also practical - in particular, JavaScript engines.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;More efficient scanning for host-virtual addresses and cache sets&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In variant 2, while scanning for the host-virtual address of a guest-owned page, it might make sense to attempt to determine its L3 cache set first. This could be done by performing L3 evictions using an eviction pattern through the physmap, then testing whether the eviction affected the guest-owned page.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The same might work for cache sets - use an L1D+L2 eviction set to evict the function pointer in the host kernel context, use a gadget in the kernel to evict an L3 set using physical addresses, then use that to identify which cache sets guest lines belong to until a guest-owned eviction set has been constructed.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Dumping the complete BTB state&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Given that the generic BTB seems to only be able to distinguish 2&lt;/span&gt;&lt;span&gt;31-8&lt;/span&gt; &lt;span&gt;or fewer source addresses, it seems feasible to dump out the complete BTB state generated by e.g. a hypercall in a timeframe around the order of a few hours. (Scan for jump sources, then for every discovered jump source, bisect the jump target.) This could potentially be used to identify the locations of functions in the host kernel even if the host kernel is custom-built.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The source address aliasing would reduce the usefulness somewhat, but because target addresses don't suffer from that, it might be possible to correlate (source,target) pairs from machines with different KASLR offsets and reduce the number of candidate addresses based on KASLR being additive while aliasing is bitwise.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This could then potentially allow an attacker to make guesses about the host kernel version or the compiler used to build it based on jump offsets or distances between functions.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Variant 2: Leaking with more efficient gadgets&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;If sufficiently efficient gadgets are used for variant 2, it might not be necessary to evict host kernel function pointers from the L3 cache at all; it might be sufficient to only evict them from L1D and L2.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Various speedups&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In particular the variant 2 PoC is still a bit slow. This is probably partly because:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It only leaks one bit at a time; leaking more bits at a time should be doable.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It heavily uses IRETQ for hiding control flow from the processor.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It would be interesting to see what data leak rate can be achieved using variant 2.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Leaking or injection through the return predictor&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;If the return predictor also doesn't lose its state on a privilege level change, it might be useful for either locating the host kernel from inside a VM (in which case bisection could be used to very quickly discover the full address of the host kernel) or injecting return targets (in particular if the return address is stored in a cache line that can be flushed out by the attacker and isn't reloaded before the return instruction).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;However, we have not performed any experiments with the return predictor that yielded conclusive results so far.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Leaking data out of the indirect call predictor&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We have attempted to leak target information out of the indirect call predictor, but haven't been able to make it work.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The following statement were provided to us regarding this issue from the vendors to whom Project Zero disclosed this vulnerability:&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;No current statement provided at this time.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;AMD&lt;/span&gt;&lt;/h2&gt;

&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;ARM&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Arm recognises that the speculation functionality of many modern high-performance processors, despite working as intended, can be used in conjunction with the timing of cache operations to leak some information as described in this blog. Correspondingly, Arm has developed software mitigations that we recommend be deployed.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Arm has included a detailed technical whitepaper as well as links to information from some of Arm’s architecture partners regarding their specific implementations and mitigations.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Note that some of these documents - in particular Intel's documentation - change over time, so quotes from and references to it may not reflect the latest version of Intel's documentation.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Placing data immediately following an indirect branch can cause a performance problem. If the data consists of all zeros, it looks like a long stream of ADDs to memory destinations and this can cause resource conflicts and slow down branch recovery. Also, data immediately following indirect branches may appear as branches to the branch predication [sic] hardware, which can branch off to execute other data pages. This can lead to subsequent self-modifying code problems.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Loads can:[...]Be carried out speculatively, before preceding branches are resolved.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Software should avoid writing to a code page in the same 1-KByte subpage that is being executed or fetching code in the same 2-KByte subpage of that is being written. In addition, sharing a page containing directly or speculatively executed code with another processor as a data page can trigger an SMC condition that causes the entire pipeline of the machine and the trace cache to be cleared. This is due to the self-modifying code condition.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;if mapped as WB or WT, there is a potential for speculative processor reads to bring the data into the caches&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Failure to map the region as WC may allow the line to be speculatively read into the processor caches (via the wrong path of a mispredicted branch).&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1507.06955.pdf&quot;&gt;&lt;span&gt;https://arxiv.org/pdf/1507.06955.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;: The rowhammer.js research by Daniel Gruss, Clémentine Maurice and Stefan Mangard contains information about L3 cache eviction patterns that we reused in the KVM PoC to evict a function pointer.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://www.sophia.re/thesis.pdf&quot;&gt;&lt;span&gt;https://www.sophia.re/thesis.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;: Sophia D'Antoine wrote a thesis that shows that opcode scheduling can theoretically be used to transmit data between hyperthreads.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://gruss.cc/files/kaiser.pdf&quot;&gt;&lt;span&gt;https://gruss.cc/files/kaiser.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;: Daniel Gruss, Moritz Lipp, Michael Schwarz, Richard Fellner, Clémentine Maurice, and Stefan Mangard wrote a paper on mitigating microarchitectural issues caused by pagetable sharing between userspace and the kernel.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;&lt;span&gt;[2]&lt;/span&gt; &lt;span&gt;The precise model names are listed in the section &quot;Tested Processors&quot;. The code for reproducing this is in the writeup_files.tar archive in our bugtracker, in the folders userland_test_x86 and userland_test_aarch64.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;[3]&lt;/span&gt; &lt;span&gt;The attacker-controlled offset used to perform an out-of-bounds access on an array by this PoC is a 32-bit value, limiting the accessible addresses to a 4GiB window in the kernel heap area.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;[4]&lt;/span&gt; &lt;span&gt;This PoC won't work on CPUs with SMAP support; however, that is not a fundamental limitation.&lt;/span&gt;&lt;/div&gt;

&lt;div&gt;&lt;span&gt;[6]&lt;/span&gt; &lt;span&gt;The phone was running an Android build from May 2017.&lt;/span&gt;&lt;/div&gt;


&lt;div&gt;&lt;span&gt;[9]&lt;/span&gt; &lt;span&gt;More than 2&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;mappings would be more efficient, but the kernel places a hard cap of 2&lt;/span&gt;&lt;span&gt;16&lt;/span&gt; &lt;span&gt;on the number of VMAs that a process can have.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;[10]&lt;/span&gt; &lt;span&gt;Intel's optimization manual states that &quot;In the first implementation of HT Technology, the physical execution resources are shared and the architecture state is duplicated for each logical processor&quot;, so it would be plausible for predictor state to be shared. While predictor state could be tagged by logical core, that would likely reduce performance for multithreaded processes, so it doesn't seem likely.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;[11]&lt;/span&gt; &lt;span&gt;In case the history buffer was a bit bigger than we had measured, we added some margin - in particular because we had seen slightly different history buffer lengths in different experiments, and because 26 isn't a very round number.&lt;/span&gt;&lt;/div&gt;



</description>
<pubDate>Wed, 03 Jan 2018 22:28:18 +0000</pubDate>
<dc:creator>ccurrens</dc:creator>
<og:url>https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html</og:url>
<og:title>Reading privileged memory with a side-channel</og:title>
<og:description>Posted by Jann Horn, Project Zero We have discovered that CPU data cache timing can be abused to efficiently leak information out of mi...</og:description>
<og:image>https://3.bp.blogspot.com/-64YRnkgMujY/Wk064WtrSRI/AAAAAAAACPg/O4ZxlUiWerACL61P490xrVMWSmHTwQflQCEwYBhgL/w1200-h630-p-k-no-nu/image3.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html</dc:identifier>
</item>
<item>
<title>Reading privileged memory with a side-channel</title>
<link>https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html</link>
<guid isPermaLink="true" >https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html</guid>
<description>&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Posted by Jann Horn, Project Zero&lt;/span&gt;&lt;/div&gt;
&lt;strong id=&quot;docs-internal-guid-c66856d3-bdac-1e12-91fc-544573d27dc6&quot;&gt;&lt;br/&gt;&lt;/strong&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We have discovered that CPU data cache timing can be abused to efficiently leak information out of mis-speculated execution, leading to (at worst) arbitrary virtual memory read vulnerabilities across local security boundaries in various contexts.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Variants of this issue are known to affect many modern processors, including certain processors by Intel, AMD and ARM. For a few Intel and AMD CPU models, we have exploits that work against real software. We reported this issue to Intel, AMD and ARM on 2017-06-01 [1]&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;So far, there are three known variants of the issue:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Variant 1: bounds check bypass (&lt;/span&gt;&lt;span&gt;CVE-2017-5753)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Variant 2: branch target injection (&lt;/span&gt;&lt;span&gt;CVE-2017-5715)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Variant 3: rogue data cache load (&lt;/span&gt;&lt;span&gt;CVE-2017-5754)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Before the issues described here were publicly disclosed, Daniel Gruss, Moritz Lipp, Yuval Yarom, Paul Kocher, Daniel Genkin, Michael Schwarz, Mike Hamburg, Stefan Mangard, Thomas Prescher and Werner Haas also reported them; their [writeups/blogposts/paper drafts] are at:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;During the course of our research, we developed the following proofs of concept (PoCs):&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ol&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A PoC that demonstrates the basic principles behind variant 1 in userspace on the tested Intel Haswell Xeon CPU, the AMD FX CPU, the AMD PRO CPU and an ARM Cortex A57 [2].&lt;/span&gt; &lt;span&gt;This PoC only tests for the ability to read data inside mis-speculated execution within the same process, without crossing any privilege boundaries.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A PoC for variant 1 that, when running with normal user privileges under a modern Linux kernel with a distro-standard config, can perform arbitrary reads in a 4GiB range [3]&lt;/span&gt; &lt;span&gt;in kernel virtual memory on the Intel Haswell Xeon CPU. If the kernel's BPF JIT is enabled (non-default configuration), it also works on the AMD PRO CPU. On the Intel Haswell Xeon CPU, kernel virtual memory can be read at a rate of around 2000 bytes per second after around 4 seconds of startup time. [4]&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A PoC for variant 2 that, when running with root privileges inside a KVM guest created using virt-manager on the Intel Haswell Xeon CPU, with a specific (now outdated) version of Debian's distro kernel&lt;/span&gt; &lt;span&gt;[5] running on the host, can read host kernel memory at a rate of around 1500 bytes/second, with room for optimization. Before the attack can be performed, some initialization has to be performed that takes roughly between 10 and 30 minutes for a machine with 64GiB of RAM; the needed time should scale roughly linearly with the amount of host RAM. (If 2MB hugepages are available to the guest, the initialization should be much faster, but that hasn't been tested.)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A PoC for variant 3 that, when running with normal user privileges, can read kernel memory on the Intel Haswell Xeon CPU under some precondition. We believe that this precondition is that the targeted kernel memory is present in the L1D cache.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;For interesting resources around this topic, look down into the &quot;Literature&quot; section.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A warning regarding explanations about processor internals in this blogpost: This blogpost contains a lot of speculation about hardware internals based on observed behavior, which might not necessarily correspond to what processors are actually doing.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We have some ideas on possible mitigations and provided some of those ideas to the processor vendors; however, we believe that the processor vendors are in a much better position than we are to design and evaluate mitigations, and we expect them to be the source of authoritative guidance.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The PoC code and the writeups that we sent to the CPU vendors will be made available at a later date.&lt;/span&gt;&lt;/div&gt;

&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel(R) Xeon(R) CPU E5-1650 v3 @ 3.50GHz (called &quot;Intel Haswell Xeon CPU&quot; in the rest of this document)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;AMD FX(tm)-8320 Eight-Core Processor (called &quot;AMD FX CPU&quot; in the rest of this document)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;AMD PRO A8-9600 R7, 10 COMPUTE CORES 4C+6G (called &quot;AMD PRO CPU&quot; in the rest of this document)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;An ARM Cortex A57 core of a Google Nexus 5x phone [6]&lt;/span&gt; &lt;span&gt;(called &quot;ARM Cortex A57&quot; in the rest of this document)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;retire:&lt;/span&gt; &lt;span&gt;An instruction retires when its results, e.g. register writes and memory writes, are committed and made visible to the rest of the system. Instructions can be executed out of order, but must always retire in order.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;logical processor core:&lt;/span&gt; &lt;span&gt;A logical processor core is what the operating system sees as a processor core. With hyperthreading enabled, the number of logical cores is a multiple of the number of physical cores.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;cached/uncached data:&lt;/span&gt; &lt;span&gt;In this blogpost, &quot;uncached&quot; data is data that is only present in main memory, not in any of the cache levels of the CPU. Loading uncached data will typically take over 100 cycles of CPU time.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;speculative execution:&lt;/span&gt; &lt;span&gt;A processor can execute past a branch without knowing whether it will be taken or where its target is, therefore executing instructions before it is known whether they should be executed. If this speculation turns out to have been incorrect, the CPU can discard the resulting state without architectural effects and continue execution on the correct execution path. Instructions do not retire before it is known that they are on the correct execution path.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;mis-speculation window:&lt;/span&gt; &lt;span&gt;The time window during which the CPU speculatively executes the wrong code and has not yet detected that mis-speculation has occurred.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section explains the common theory behind all three variants and the theory behind our PoC for variant 1 that, when running in userspace under a Debian distro kernel, can perform arbitrary reads in a 4GiB region of kernel memory in at least the following configurations:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel Haswell Xeon CPU, eBPF JIT is off (default state)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel Haswell Xeon CPU, eBPF JIT is on (non-default state)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;AMD PRO CPU, eBPF JIT is on (non-default state)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The state of the eBPF JIT can be toggled using the net.core.bpf_jit_enable sysctl.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Theoretical explanation&lt;/span&gt;&lt;/h2&gt;

&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Branch prediction predicts the branch target and enables the&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;processor to begin executing instructions long before the branch&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;true execution path is known.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In section 2.3.5.2 (&quot;L1 DCache&quot;):&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Loads can:&lt;/span&gt;&lt;/div&gt;

&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Be carried out speculatively, before preceding branches are resolved.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Take cache misses out of order and in an overlapped manner.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel's Software Developer's Manual&lt;/span&gt; &lt;span&gt;[7] states in Volume 3A, section 11.7 (&quot;Implicit Caching (Pentium 4, Intel Xeon, and P6 family processors&quot;):&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Implicit caching occurs when a memory element is made potentially cacheable, although the element may never have been accessed in the normal von Neumann sequence. Implicit caching occurs on the P6 and more recent processor families due to aggressive prefetching, branch prediction, and TLB miss handling. Implicit caching is an extension of the behavior of existing Intel386, Intel486, and Pentium processor systems, since software running on these processor families also has not been able to deterministically predict the behavior of instruction prefetch.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Consider the code sample below. If&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt; &lt;span&gt;is uncached, the processor can speculatively load data from&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[untrusted_offset_from_caller]&lt;/span&gt;&lt;span&gt;. This is an out-of-bounds read. That should not matter because the processor will effectively roll back the execution state when the branch has executed; none of the speculatively executed instructions will retire (e.g. cause registers etc. to be affected).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned long length;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned char data[];&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array *arr1 = ...;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;unsigned long untrusted_offset_from_caller = ...;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;if (untrusted_offset_from_caller &amp;lt;&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt;&lt;span&gt;) {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned char value =&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[untrusted_offset_from_caller]&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;&lt;/div&gt;


&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;However, in the following code sample, there's an issue. If&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt;&lt;span&gt;,&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[0x200]&lt;/span&gt; &lt;span&gt;and&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[0x300]&lt;/span&gt; &lt;span&gt;are not cached, but all other accessed data is, and the branch conditions are predicted as true, the processor can do the following speculatively before&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt; &lt;span&gt;has been loaded and the execution is re-steered:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;load&lt;/span&gt; &lt;span&gt;value =&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[&lt;/span&gt;&lt;span&gt;untrusted_offset_from_caller&lt;/span&gt;&lt;span&gt;]&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;start a load from a data-dependent offset in&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data&lt;/span&gt;&lt;span&gt;, loading the corresponding cache line into the L1 cache&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned long length;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned char data[];&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array *arr1 = ...; /* small array */&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;struct array *arr2 = ...; /* array of size 0x400 */&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;/* &amp;gt;0x400&lt;/span&gt; &lt;span&gt;(OUT OF BOUNDS!)&lt;/span&gt; &lt;span&gt;*/&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;unsigned long untrusted_offset_from_caller = ...;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;if (&lt;/span&gt;&lt;span&gt;untrusted_offset_from_caller&lt;/span&gt; &lt;span&gt;&amp;lt;&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt;&lt;span&gt;) {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned char value =&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[&lt;/span&gt;&lt;span&gt;untrusted_offset_from_caller&lt;/span&gt;&lt;span&gt;]&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; unsigned long index2 = ((value&amp;amp;1)*0x100)+0x200;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; if (index2 &amp;lt; arr2-&amp;gt;length) {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;   unsigned char value2 =&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[index2]&lt;/span&gt;&lt;span&gt;;&lt;/span&gt;&lt;/div&gt;


&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;After the execution has been returned to the non-speculative path because the processor has noticed that&lt;/span&gt; &lt;span&gt;untrusted_offset_from_caller&lt;/span&gt; &lt;span&gt;is bigger than&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;length&lt;/span&gt;&lt;span&gt;, the cache line containing&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[index2]&lt;/span&gt; &lt;span&gt;stays in the L1 cache. By measuring the time required to load&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[0x200]&lt;/span&gt; &lt;span&gt;and&lt;/span&gt; &lt;span&gt;arr2-&amp;gt;data[0x300]&lt;/span&gt;&lt;span&gt;, an attacker can then determine whether the value of&lt;/span&gt; &lt;span&gt;index2&lt;/span&gt; &lt;span&gt;during speculative execution was 0x200 or 0x300 - which discloses whether&lt;/span&gt; &lt;span&gt;arr1-&amp;gt;data[&lt;/span&gt;&lt;span&gt;untrusted_offset_from_caller&lt;/span&gt;&lt;span&gt;]&lt;/span&gt;&lt;span&gt;&amp;amp;1&lt;/span&gt; &lt;span&gt;is 0 or 1.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To be able to actually use this behavior for an attack, an attacker needs to be able to cause the execution of such a vulnerable code pattern in the targeted context with an out-of-bounds index. For this, the vulnerable code pattern must either be present in existing code, or there must be an interpreter or JIT engine that can be used to generate the vulnerable code pattern. So far, we have not actually identified any existing, exploitable instances of the vulnerable code pattern; the PoC for leaking kernel memory using variant 1 uses the eBPF interpreter or the eBPF JIT engine, which are built into the kernel and accessible to normal users.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A minor variant of this could be to instead use an out-of-bounds read to a function pointer to gain control of execution in the mis-speculated path. We did not investigate this variant further.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Attacking the kernel&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section describes in more detail how variant 1 can be used to leak Linux kernel memory using the eBPF bytecode interpreter and JIT engine. While there are many interesting potential targets for variant 1 attacks, we chose to attack the Linux in-kernel eBPF JIT/interpreter because it provides more control to the attacker than most other JITs.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The Linux kernel supports eBPF since version 3.18. Unprivileged userspace code can supply bytecode to the kernel that is verified by the kernel and then:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;either interpreted by an in-kernel bytecode interpreter&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;or translated to native machine code that also runs in kernel context using a JIT engine (which translates individual bytecode instructions without performing any further optimizations)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Execution of the bytecode can be triggered by attaching the eBPF bytecode to a socket as a filter and then sending data through the other end of the socket.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Whether the JIT engine is enabled depends on a run-time configuration setting - but at least on the tested Intel processor, the attack works independent of that setting.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Unlike classic BPF, eBPF has data types like data arrays and function pointer arrays into which eBPF bytecode can index. Therefore, it is possible to create the code pattern described above in the kernel using eBPF bytecode.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;eBPF's data arrays are less efficient than its function pointer arrays, so the attack will use the latter where possible.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Both machines on which this was tested have no SMAP, and the PoC relies on that (but it shouldn't be a precondition in principle).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Additionally, at least on the Intel machine on which this was tested, bouncing modified cache lines between cores is slow, apparently because the MESI protocol is used for cache coherence [8]&lt;/span&gt;&lt;span&gt;. Changing the reference counter of an eBPF array on one physical CPU core causes the cache line containing the reference counter to be bounced over to that CPU core, making reads of the reference counter on all other CPU cores slow until the changed reference counter has been written back to memory. Because the length and the reference counter of an eBPF array are stored in the same cache line, this also means that changing the reference counter on one physical CPU core causes reads of the eBPF array's length to be slow on other physical CPU cores (intentional false sharing).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The attack uses two eBPF programs. The first one tail-calls through a page-aligned eBPF function pointer array&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;at a configurable index. In simplified terms, this program is used to determine the address of&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;by guessing the offset from&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;to a userspace address and tail-calling through&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;at the guessed offsets. To cause the branch prediction to predict that the offset is below the length of&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt;&lt;span&gt;, tail calls to an in-bounds index are performed in between. To increase the mis-speculation window, the cache line containing the length of&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;is bounced to another core. To test whether an offset guess was successful, it can be tested whether the userspace address has been loaded into the cache.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Because such straightforward brute-force guessing of the address would be slow, the following optimization is used: 2&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;adjacent userspace memory mappings [9]&lt;/span&gt;&lt;span&gt;, each consisting of 2&lt;/span&gt;&lt;span&gt;4&lt;/span&gt; &lt;span&gt;pages, are created at the userspace address&lt;/span&gt; &lt;span&gt;user_mapping_area&lt;/span&gt;&lt;span&gt;, covering a total area of 2&lt;/span&gt;&lt;span&gt;31&lt;/span&gt; &lt;span&gt;bytes. Each mapping maps the same physical pages, and all mappings are present in the pagetables.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-64YRnkgMujY/Wk064WtrSRI/AAAAAAAACPg/O4ZxlUiWerACL61P490xrVMWSmHTwQflQCEwYBhgL/s1600/image3.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;138&quot; data-original-width=&quot;624&quot; height=&quot;139&quot; src=&quot;https://3.bp.blogspot.com/-64YRnkgMujY/Wk064WtrSRI/AAAAAAAACPg/O4ZxlUiWerACL61P490xrVMWSmHTwQflQCEwYBhgL/s640/image3.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;

&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This permits the attack to be carried out in steps of 2&lt;/span&gt;&lt;span&gt;31&lt;/span&gt; &lt;span&gt;bytes. For each step, after causing an out-of-bounds access through&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt;&lt;span&gt;, only one cache line each from the first 2&lt;/span&gt;&lt;span&gt;4&lt;/span&gt; &lt;span&gt;pages of&lt;/span&gt; &lt;span&gt;user_mapping_area&lt;/span&gt; &lt;span&gt;have to be tested for cached memory. Because the L3 cache is physically indexed, any access to a virtual address mapping a physical page will cause all other virtual addresses mapping the same physical page to become cached as well.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;When this attack finds a hit—a cached memory location—the upper 33 bits of the kernel address are known (because they can be derived from the address guess at which the hit occurred), and the low 16 bits of the address are also known (from the offset inside&lt;/span&gt; &lt;span&gt;user_mapping_area&lt;/span&gt; &lt;span&gt;at which the hit was found). The remaining part of the address of&lt;/span&gt; &lt;span&gt;user_mapping_area&lt;/span&gt; &lt;span&gt;is the middle.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-kSdcfy3GVSw/Wk065qWZZaI/AAAAAAAACPs/7HRZEXij3rIqvAf4gq8QpyHKvJsT2pYIQCEwYBhgL/s1600/image5.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;149&quot; data-original-width=&quot;624&quot; height=&quot;152&quot; src=&quot;https://1.bp.blogspot.com/-kSdcfy3GVSw/Wk065qWZZaI/AAAAAAAACPs/7HRZEXij3rIqvAf4gq8QpyHKvJsT2pYIQCEwYBhgL/s640/image5.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The remaining bits in the middle can be determined by bisecting the remaining address space: Map two physical pages to adjacent ranges of virtual addresses, each virtual address range the size of half of the remaining search space, then determine the remaining address bit-wise.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;At this point, a second eBPF program can be used to actually leak data. In pseudocode, this program looks as follows:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t bitmask = &amp;lt;runtime-configurable&amp;gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t bitshift_selector = &amp;lt;runtime-configurable&amp;gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t prog_array_base_offset = &amp;lt;runtime-configurable&amp;gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t secret_data_offset = &amp;lt;runtime-configurable&amp;gt;;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;// index will be bounds-checked by the runtime,&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;// but the bounds check will be bypassed speculatively&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t secret_data = bpf_map_read(array=victim_array, index=secret_data_offset);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;// select a single bit, move it to a specific position, and add the base offset&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;uint64_t progmap_index = (((secret_data &amp;amp; bitmask) &amp;gt;&amp;gt; bitshift_selector) &amp;lt;&amp;lt; 7) + prog_array_base_offset;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;bpf_tail_call(prog_map, progmap_index);&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This program reads 8-byte-aligned 64-bit values from an eBPF data array &quot;&lt;/span&gt;&lt;span&gt;victim_map&lt;/span&gt;&lt;span&gt;&quot; at a runtime-configurable offset and bitmasks and bit-shifts the value so that one bit is mapped to one of two values that are 2&lt;/span&gt;&lt;span&gt;7&lt;/span&gt; &lt;span&gt;bytes apart (sufficient to not land in the same or adjacent cache lines when used as an array index). Finally it adds a 64-bit offset, then uses the resulting value as an offset into&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;for a tail call.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This program can then be used to leak memory by repeatedly calling the eBPF program with an out-of-bounds offset into&lt;/span&gt; &lt;span&gt;victim_map&lt;/span&gt; &lt;span&gt;that specifies the data to leak and an out-of-bounds offset into&lt;/span&gt; &lt;span&gt;prog_map&lt;/span&gt; &lt;span&gt;that causes&lt;/span&gt; &lt;span&gt;prog_map + offset&lt;/span&gt; &lt;span&gt;to point to a userspace memory area. Misleading the branch prediction and bouncing the cache lines works the same way as for the first eBPF program, except that now, the cache line holding the length of&lt;/span&gt; &lt;span&gt;victim_map&lt;/span&gt; &lt;span&gt;must also be bounced to another core.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section describes the theory behind our PoC for variant 2 that, when running with root privileges inside a KVM guest created using virt-manager on the Intel Haswell Xeon CPU, with a specific version of Debian's distro kernel running on the host, can read host kernel memory at a rate of around 1500 bytes/second.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Basics&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Prior research (see the Literature section at the end) has shown that it is possible for code in separate security contexts to influence each other's branch prediction. So far, this has only been used to infer information about where code is located (in other words, to create interference from the victim to the attacker); however, the basic hypothesis of this attack variant is that it can also be used to redirect execution of code in the victim context (in other words, to create interference from the attacker to the victim; the other way around).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-O6-JHlkvq5U/Wk064bpm2qI/AAAAAAAACPk/rjpYr72vyF0jYASgT-w4NBFQXC3hgIZZwCEwYBhgL/s1600/image2.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;200&quot; data-original-width=&quot;278&quot; src=&quot;https://3.bp.blogspot.com/-O6-JHlkvq5U/Wk064bpm2qI/AAAAAAAACPk/rjpYr72vyF0jYASgT-w4NBFQXC3hgIZZwCEwYBhgL/s1600/image2.png&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The basic idea for the attack is to target victim code that contains an indirect branch whose target address is loaded from memory and flush the cache line containing the target address out to main memory. Then, when the CPU reaches the indirect branch, it won't know the true destination of the jump, and it won't be able to calculate the true destination until it has finished loading the cache line back into the CPU, which takes a few hundred cycles. Therefore, there is a time window of typically over 100 cycles in which the CPU will speculatively execute instructions based on branch prediction.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Haswell branch prediction internals&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Some of the internals of the branch prediction implemented by Intel's processors have already been published; however, getting this attack to work properly required significant further experimentation to determine additional details.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section focuses on the branch prediction internals that were experimentally derived from the Intel Haswell Xeon CPU.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Haswell seems to have multiple branch prediction mechanisms that work very differently:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A generic branch predictor that can only store one target per source address; used for all kinds of jumps, like absolute jumps, relative jumps and so on.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;A specialized indirect call predictor that can store multiple targets per source address; used for indirect calls.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;(There is also a specialized return predictor, according to Intel's optimization manual, but we haven't analyzed that in detail yet. If this predictor could be used to reliably dump out some of the call stack through which a VM was entered, that would be very interesting.)&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Generic predictor&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The generic branch predictor, as documented in prior research, only uses the lower 31 bits of the address of the last byte of the source instruction for its prediction. If, for example, a branch target buffer (BTB) entry exists for a jump from 0x4141.0004.1000 to 0x4141.0004.5123, the generic predictor will also use it to predict a jump from 0x4242.0004.1000. When the higher bits of the source address differ like this, the higher bits of the predicted destination change together with it—in this case, the predicted destination address will be 0x4242.0004.5123—so apparently this predictor doesn't store the full, absolute destination address.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Before the lower 31 bits of the source address are used to look up a BTB entry, they are folded together using XOR. Specifically, the following bits are folded together:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;
&lt;table&gt;&lt;colgroup&gt;&lt;col width=&quot;*&quot;/&gt;&lt;col width=&quot;*&quot;/&gt;&lt;/colgroup&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;

&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x100.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x200.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x400.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x800.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x2000.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;0x4000.0000&lt;/span&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In other words, if a source address is XORed with both numbers in a row of this table, the branch predictor will not be able to distinguish the resulting address from the original source address when performing a lookup. For example, the branch predictor is able to distinguish source addresses 0x100.0000 and 0x180.0000, and it can also distinguish source addresses 0x100.0000 and 0x180.8000, but it can't distinguish source addresses 0x100.0000 and 0x140.2000 or source addresses 0x100.0000 and 0x180.4000. In the following, this will be referred to as aliased source addresses.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;When an aliased source address is used, the branch predictor will still predict the same target as for the unaliased source address. This indicates that the branch predictor stores a truncated absolute destination address, but that hasn't been verified.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Based on observed maximum forward and backward jump distances for different source addresses, the low 32-bit half of the target address could be stored as an absolute 32-bit value with an additional bit that specifies whether the jump from source to target crosses a 2&lt;/span&gt;&lt;span&gt;32&lt;/span&gt; &lt;span&gt;boundary; if the jump crosses such a boundary, bit 31 of the source address determines whether the high half of the instruction pointer should increment or decrement.&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Indirect call predictor&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The inputs of the BTB lookup for this mechanism seem to be:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The low 12 bits of the address of the source instruction (we are not sure whether it's the address of the first or the last byte) or a subset of them.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The branch history buffer state.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;If the indirect call predictor can't resolve a branch, it is resolved by the generic predictor instead. Intel's optimization manual hints at this behavior: &quot;Indirect Calls and Jumps. These may either be predicted as having a monotonic target or as having targets that vary in accordance with recent program behavior.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The branch history buffer (BHB) stores information about the last 29 taken branches - basically a fingerprint of recent control flow - and is used to allow better prediction of indirect calls that can have multiple targets.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The update function of the BHB works as follows (in pseudocode;&lt;/span&gt; &lt;span&gt;src&lt;/span&gt; &lt;span&gt;is the address of the last byte of the source instruction,&lt;/span&gt; &lt;span&gt;dst&lt;/span&gt; &lt;span&gt;is the destination address):&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;void bhb_update(uint58_t *bhb_state, unsigned long src, unsigned long dst) {&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state &amp;lt;&amp;lt;= 2;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (dst &amp;amp; 0x3f);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0xc0) &amp;gt;&amp;gt; 6;&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0xc00) &amp;gt;&amp;gt; (10 - 2);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0xc000) &amp;gt;&amp;gt; (14 - 4);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0x30) &amp;lt;&amp;lt; (6 - 4);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0x300) &amp;lt;&amp;lt; (8 - 8);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0x3000) &amp;gt;&amp;gt; (12 - 10);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0x30000) &amp;gt;&amp;gt; (16 - 12);&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt; *bhb_state ^= (src &amp;amp; 0xc0000) &amp;gt;&amp;gt; (18 - 14);&lt;/span&gt;&lt;/div&gt;

&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Some of the bits of the BHB state seem to be folded together further using XOR when used for a BTB access, but the precise folding function hasn't been understood yet.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The BHB is interesting for two reasons. First, knowledge about its approximate behavior is required in order to be able to accurately cause collisions in the indirect call predictor. But it also permits dumping out the BHB state at any repeatable program state at which the attacker can execute code - for example, when attacking a hypervisor, directly after a hypercall. The dumped BHB state can then be used to fingerprint the hypervisor or, if the attacker has access to the hypervisor binary, to determine the low 20 bits of the hypervisor load address (in the case of KVM: the low 20 bits of the load address of kvm-intel.ko).&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Reverse-Engineering Branch Predictor Internals&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This subsection describes how we reverse-engineered the internals of the Haswell branch predictor. Some of this is written down from memory, since we didn't keep a detailed record of what we were doing.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We initially attempted to perform BTB injections into the kernel using the generic predictor, using the knowledge from prior research that the generic predictor only looks at the lower half of the source address and that only a partial target address is stored. This kind of worked - however, the injection success rate was very low, below 1%. (This is the method we used in our preliminary PoCs for method 2 against modified hypervisors running on Haswell.)&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We decided to write a userspace test case to be able to more easily test branch predictor behavior in different situations.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Based on the assumption that branch predictor state is shared between hyperthreads [10]&lt;/span&gt;&lt;span&gt;, we wrote a program of which two instances are each pinned to one of the two logical processors running on a specific physical core, where one instance attempts to perform branch injections while the other measures how often branch injections are successful. Both instances were executed with ASLR disabled and had the same code at the same addresses. The injecting process performed indirect calls to a function that accesses a (per-process) test variable; the measuring process performed indirect calls to a function that tests, based on timing, whether the per-process test variable is cached, and then evicts it using CLFLUSH. Both indirect calls were performed through the same callsite. Before each indirect call, the function pointer stored in memory was flushed out to main memory using CLFLUSH to widen the speculation time window. Additionally, because of the reference to &quot;recent program behavior&quot; in Intel's optimization manual, a bunch of conditional branches that are always taken were inserted in front of the indirect call.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In this test, the injection success rate was above 99%, giving us a base setup for future experiments.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://2.bp.blogspot.com/-RMKau2Jstd0/Wk066oeGPUI/AAAAAAAACP0/sb5X95OHei0JV0xhCQDRhaYhRsLvx9ZmACEwYBhgL/s1600/image7.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;644&quot; data-original-width=&quot;618&quot; height=&quot;640&quot; src=&quot;https://2.bp.blogspot.com/-RMKau2Jstd0/Wk066oeGPUI/AAAAAAAACP0/sb5X95OHei0JV0xhCQDRhaYhRsLvx9ZmACEwYBhgL/s640/image7.png&quot; width=&quot;612&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We then tried to figure out the details of the prediction scheme. We assumed that the prediction scheme uses a global branch history buffer of some kind.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To determine the duration for which branch information stays in the history buffer, a conditional branch that is only taken in one of the two program instances was inserted in front of the series of always-taken conditional jumps, then the number of always-taken conditional jumps (N) was varied. The result was that for N=25, the processor was able to distinguish the branches (misprediction rate under 1%), but for N=26, it failed to do so (misprediction rate over 99%).&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Therefore, the branch history buffer had to be able to store information about at least the last 26 branches.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The code in one of the two program instances was then moved around in memory. This revealed that only the lower 20 bits of the source and target addresses have an influence on the branch history buffer.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Testing with different types of branches in the two program instances revealed that static jumps, taken conditional jumps, calls and returns influence the branch history buffer the same way; non-taken conditional jumps don't influence it; the address of the last byte of the source instruction is the one that counts; IRETQ doesn't influence the history buffer state (which is useful for testing because it permits creating program flow that is invisible to the history buffer).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Moving the last conditional branch before the indirect call around in memory multiple times revealed that the branch history buffer contents can be used to distinguish many different locations of that last conditional branch instruction. This suggests that the history buffer doesn't store a list of small history values; instead, it seems to be a larger buffer in which history data is mixed together.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;However, a history buffer needs to &quot;forget&quot; about past branches after a certain number of new branches have been taken in order to be useful for branch prediction. Therefore, when new data is mixed into the history buffer, this can not cause information in bits that are already present in the history buffer to propagate downwards - and given that, upwards combination of information probably wouldn't be very useful either. Given that branch prediction also must be very fast, we concluded that it is likely that the update function of the history buffer left-shifts the old history buffer, then XORs in the new state (see diagram).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://1.bp.blogspot.com/-crbBTenupqw/Wk066CBQ2qI/AAAAAAAACPw/GaUAa5tUDE8dV9tZ6bt-QWGLY5klZM5FgCEwYBhgL/s1600/image6.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;117&quot; data-original-width=&quot;624&quot; height=&quot;118&quot; src=&quot;https://1.bp.blogspot.com/-crbBTenupqw/Wk066CBQ2qI/AAAAAAAACPw/GaUAa5tUDE8dV9tZ6bt-QWGLY5klZM5FgCEwYBhgL/s640/image6.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;If this assumption is correct, then the history buffer contains a lot of information about the most recent branches, but only contains as many bits of information as are shifted per history buffer update about the last branch about which it contains any data. Therefore, we tested whether flipping different bits in the source and target addresses of a jump followed by 32 always-taken jumps with static source and target allows the branch prediction to disambiguate an indirect call. [11]&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;With 32 static jumps in between, no bit flips seemed to have an influence, so we decreased the number of static jumps until a difference was observable. The result with 28 always-taken jumps in between was that bits 0x1 and 0x2 of the target and bits 0x40 and 0x80 of the source had such an influence; but flipping both 0x1 in the target and 0x40 in the source or 0x2 in the target and 0x80 in the source did not permit disambiguation. This shows that the per-insertion shift of the history buffer is 2 bits and shows which data is stored in the least significant bits of the history buffer. We then repeated this with decreased amounts of fixed jumps after the bit-flipped jump to determine which information is stored in the remaining bits.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Reading host memory from a KVM guest&lt;/span&gt;&lt;/h2&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Locating the host kernel&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Our PoC locates the host kernel in several steps. The information that is determined and necessary for the next steps of the attack consists of:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;lower 20 bits of the address of kvm-intel.ko&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;full address of kvm.ko&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;full address of vmlinux&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Looking back, this is unnecessarily complicated, but it nicely demonstrates the various techniques an attacker can use. A simpler way would be to first determine the address of vmlinux, then bisect the addresses of kvm.ko and kvm-intel.ko.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In the first step, the address of kvm-intel.ko is leaked. For this purpose, the branch history buffer state after guest entry is dumped out. Then, for every possible value of bits 12..19 of the load address of kvm-intel.ko, the expected lowest 16 bits of the history buffer are computed based on the load address guess and the known offsets of the last 8 branches before guest entry, and the results are compared against the lowest 16 bits of the leaked history buffer state.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The branch history buffer state is leaked in steps of 2 bits by measuring misprediction rates of an indirect call with two targets. One way the indirect call is reached is from a vmcall instruction followed by a series of N branches whose relevant source and target address bits are all zeroes. The second way the indirect call is reached is from a series of controlled branches in userspace that can be used to write arbitrary values into the branch history buffer.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Misprediction rates are measured as in the section &quot;Reverse-Engineering Branch Predictor Internals&quot;, using one call target that loads a cache line and another one that checks whether the same cache line has been loaded.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://3.bp.blogspot.com/-juuq6gYvhn4/Wk065XER9yI/AAAAAAAACPo/EuTi1HzweloKCChZajY7FUDEtHXhs12ZwCEwYBhgL/s1600/image4.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;462&quot; data-original-width=&quot;451&quot; height=&quot;640&quot; src=&quot;https://3.bp.blogspot.com/-juuq6gYvhn4/Wk065XER9yI/AAAAAAAACPo/EuTi1HzweloKCChZajY7FUDEtHXhs12ZwCEwYBhgL/s640/image4.png&quot; width=&quot;624&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;With N=29, mispredictions will occur at a high rate if the controlled branch history buffer value is zero because all history buffer state from the hypercall has been erased. With N=28, mispredictions will occur if the controlled branch history buffer value is one of 0&amp;lt;&amp;lt;(28*2), 1&amp;lt;&amp;lt;(28*2), 2&amp;lt;&amp;lt;(28*2), 3&amp;lt;&amp;lt;(28*2) - by testing all four possibilities, it can be detected which one is right. Then, for decreasing values of N, the four possibilities are {0|1|2|3}&amp;lt;&amp;lt;(28*2) | (history_buffer_for(N+1) &amp;gt;&amp;gt; 2). By repeating this for decreasing values for N, the branch history buffer value for N=0 can be determined.&lt;/span&gt;&lt;/div&gt;

&lt;br/&gt;&lt;div class=&quot;separator&quot;&gt;&lt;a href=&quot;https://4.bp.blogspot.com/-uAWgXXpjs8I/Wk064FS-g3I/AAAAAAAACPc/0nh9nbSn0EkTXgzbbuY1TRROTR02GWIFQCEwYBhgL/s1600/image1.png&quot; imageanchor=&quot;1&quot;&gt;&lt;img border=&quot;0&quot; data-original-height=&quot;315&quot; data-original-width=&quot;457&quot; height=&quot;440&quot; src=&quot;https://4.bp.blogspot.com/-uAWgXXpjs8I/Wk064FS-g3I/AAAAAAAACPc/0nh9nbSn0EkTXgzbbuY1TRROTR02GWIFQCEwYBhgL/s640/image1.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;At this point, the low 20 bits of kvm-intel.ko are known; the next step is to roughly locate kvm.ko.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;For this, the generic branch predictor is used, using data inserted into the BTB by an indirect call from kvm.ko to kvm-intel.ko that happens on every hypercall; this means that the source address of the indirect call has to be leaked out of the BTB.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;kvm.ko will probably be located somewhere in the range from&lt;/span&gt; &lt;span&gt;0xffffffffc0000000&lt;/span&gt; &lt;span&gt;to&lt;/span&gt; &lt;span&gt;0xffffffffc4000000&lt;/span&gt;&lt;span&gt;, with page alignment (0x1000). This means that the first four entries in the table in the section &quot;Generic Predictor&quot; apply; there will be 2&lt;/span&gt;&lt;span&gt;4&lt;/span&gt;&lt;span&gt;-1=15 aliasing addresses for the correct one. But that is also an advantage: It cuts down the search space from 0x4000 to 0x4000/2&lt;/span&gt;&lt;span&gt;4&lt;/span&gt;&lt;span&gt;=1024.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To find the right address for the source or one of its aliasing addresses, code that loads data through a specific register is placed at all possible call targets (the leaked low 20 bits of kvm-intel.ko plus the in-module offset of the call target plus a multiple of 2&lt;/span&gt;&lt;span&gt;20&lt;/span&gt;&lt;span&gt;) and indirect calls are placed at all possible call sources. Then, alternatingly, hypercalls are performed and indirect calls are performed through the different possible non-aliasing call sources, with randomized history buffer state that prevents the specialized prediction from working. After this step, there are 2&lt;/span&gt;&lt;span&gt;16&lt;/span&gt; &lt;span&gt;remaining possibilities for the load address of kvm.ko.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Next, the load address of vmlinux can be determined in a similar way, using an indirect call from vmlinux to kvm.ko. Luckily, none of the bits which are randomized in the load address of vmlinux  are folded together, so unlike when locating kvm.ko, the result will directly be unique. vmlinux has an alignment of 2MiB and a randomization range of 1GiB, so there are still only 512 possible addresses.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Because (as far as we know) a simple hypercall won't actually cause indirect calls from vmlinux to kvm.ko, we instead use port I/O from the status register of an emulated serial port, which is present in the default configuration of a virtual machine created with virt-manager.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The only remaining piece of information is which one of the 16 aliasing load addresses of kvm.ko is actually correct. Because the source address of an indirect call to kvm.ko is known, this can be solved using bisection: Place code at the various possible targets that, depending on which instance of the code is speculatively executed, loads one of two cache lines, and measure which one of the cache lines gets loaded.&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Identifying cache sets&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The PoC assumes that the VM does not have access to hugepages.To discover eviction sets for all L3 cache sets with a specific alignment relative to a 4KiB page boundary, the PoC first allocates 25600 pages of memory. Then, in a loop, it selects random subsets of all remaining unsorted pages such that the expected number of sets for which an eviction set is contained in the subset is 1, reduces each subset down to an eviction set by repeatedly accessing its cache lines and testing whether the cache lines are always cached (in which case they're probably not part of an eviction set) and attempts to use the new eviction set to evict all remaining unsorted cache lines to determine whether they are in the same cache set [12].&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Locating the host-virtual address of a guest page&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Because this attack uses a FLUSH+RELOAD approach for leaking data, it needs to know the host-kernel-virtual address of one guest page. Alternative approaches such as PRIME+PROBE should work without that requirement.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The basic idea for this step of the attack is to use a branch target injection attack against the hypervisor to load an attacker-controlled address and test whether that caused the guest-owned page to be loaded. For this, a gadget that simply loads from the memory location specified by R8 can be used - R8-R11 still contain guest-controlled values when the first indirect call after a guest exit is reached on this kernel build.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We expected that an attacker would need to either know which eviction set has to be used at this point or brute-force it simultaneously; however, experimentally, using random eviction sets works, too. Our theory is that the observed behavior is actually the result of L1D and L2 evictions, which might be sufficient to permit a few instructions worth of speculative execution.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The host kernel maps (nearly?) all physical memory in the physmap area, including memory assigned to KVM guests. However, the location of the physmap is randomized (with a 1GiB alignment), in an area of size 128PiB. Therefore, directly bruteforcing the host-virtual address of a guest page would take a long time. It is not necessarily impossible; as a ballpark estimate, it should be possible within a day or so, maybe less, assuming 12000 successful injections per second and 30 guest pages that are tested in parallel; but not as impressive as doing it in a few minutes.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To optimize this, the problem can be split up: First, brute-force the physical address using a gadget that can load from physical addresses, then brute-force the base address of the physmap region. Because the physical address can usually be assumed to be far below 128PiB, it can be brute-forced more efficiently, and brute-forcing the base address of the physmap region afterwards is also easier because then address guesses with 1GiB alignment can be used.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To brute-force the physical address, the following gadget can be used:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9def:       4c 89 c0                mov    rax,r8&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9df2:       4d 63 f9                movsxd r15,r9d&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9df5:       4e 8b 04 fd c0 b3 a6    mov    r8,QWORD PTR [r15*8-0x7e594c40]&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9dfc:       81&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9dfd:       4a 8d 3c 00             lea    rdi,[rax+r8*1]&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9e01:       4d 8b a4 00 f8 00 00    mov    r12,QWORD PTR [r8+rax*1+0xf8]&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff810a9e08:       00&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This gadget permits loading an 8-byte-aligned value from the area around the kernel text section by setting R9 appropriately, which in particular permits loading&lt;/span&gt; &lt;span&gt;page_offset_base&lt;/span&gt;&lt;span&gt;, the start address of the physmap. Then, the value that was originally in R8 - the physical address guess minus 0xf8 - is added to the result of the previous load, 0xfa is added to it, and the result is dereferenced.&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Cache set selection&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To select the correct L3 eviction set, the attack from the following section is essentially executed with different eviction sets until it works.&lt;/span&gt;&lt;/div&gt;
&lt;h3 dir=&quot;ltr&quot;&gt;&lt;span&gt;Leaking data&lt;/span&gt;&lt;/h3&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;At this point, it would normally be necessary to locate gadgets in the host kernel code that can be used to actually leak data by reading from an attacker-controlled location, shifting and masking the result appropriately and then using the result of that as offset to an attacker-controlled address for a load. But piecing gadgets together and figuring out which ones work in a speculation context seems annoying. So instead, we decided to use the eBPF interpreter, which is built into the host kernel - while there is no legitimate way to invoke it from inside a VM, the presence of the code in the host kernel's text section is sufficient to make it usable for the attack, just like with ordinary ROP gadgets.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The eBPF interpreter entry point has the following function signature:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;static unsigned int __bpf_prog_run(void *ctx, const struct bpf_insn *insn)&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The second parameter is a pointer to an array of statically pre-verified eBPF instructions to be executed - which means that&lt;/span&gt; &lt;span&gt;__bpf_prog_run()&lt;/span&gt; &lt;span&gt;will not perform any type checks or bounds checks. The first parameter is simply stored as part of the initial emulated register state, so its value doesn't matter.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The eBPF interpreter provides, among other things:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;multiple emulated 64-bit registers&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;64-bit immediate writes to emulated registers&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;memory reads from addresses stored in emulated registers&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;bitwise operations (including bit shifts) and arithmetic operations&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;To call the interpreter entry point, a gadget that gives RSI and RIP control given R8-R11 control and controlled data at a known memory location is necessary. The following gadget provides this functionality:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;ffffffff81514edd:       4c 89 ce                mov    rsi,r9&lt;/span&gt;&lt;span&gt;&lt;br class=&quot;kix-line-break&quot;/&gt;&lt;/span&gt;&lt;span&gt;ffffffff81514ee0:       41 ff 90 b0 00 00 00    call   QWORD PTR [r8+0xb0]&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Now, by pointing R8 and R9 at the mapping of a guest-owned page in the physmap, it is possible to speculatively execute arbitrary unvalidated eBPF bytecode in the host kernel. Then, relatively straightforward bytecode can be used to leak data into the cache.&lt;/span&gt;&lt;/div&gt;


&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In summary, an attack using this variant of the issue attempts to read kernel memory from userspace without misdirecting the control flow of kernel code. This works by using the code pattern that was used for the previous variants, but in userspace. The underlying idea is that the permission check for accessing an address might not be on the critical path for reading data from memory to a register, where the permission check could have significant performance impact. Instead, the memory read could make the result of the read available to following instructions immediately and only perform the permission check asynchronously, setting a flag in the reorder buffer that causes an exception to be raised if the permission check fails.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We do have a few additions to make to Anders Fogh's blogpost:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Imagine the following instruction executed in usermode&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;mov rax,[somekernelmodeaddress]&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It will cause an interrupt when retired, [...]&quot;&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It is also possible to already execute that instruction behind a high-latency mispredicted branch to avoid taking a page fault. This might also widen the speculation window by increasing the delay between the read from a kernel address and delivery of the associated exception.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;First, I call a syscall that touches this memory. Second, I use the prefetcht0 instruction to improve my odds of having the address loaded in L1.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;When we used prefetch instructions after doing a syscall, the attack stopped working for us, and we have no clue why. Perhaps the CPU somehow stores whether access was denied on the last access and prevents the attack from working if that is the case?&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Fortunately I did not get a slow read suggesting that Intel null’s the result when the access is not allowed.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;That (read from kernel address returns all-zeroes) seems to happen for memory that is not sufficiently cached but for which pagetable entries are present, at least after repeated read attempts. For unmapped memory, the kernel address read does not return a result at all.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We believe that our research provides many remaining research topics that we have not yet investigated, and we encourage other public researchers to look into these.&lt;/span&gt;&lt;/div&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This section contains an even higher amount of speculation than the rest of this blogpost - it contains untested ideas that might well be useless.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Leaking without data cache timing&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It would be interesting to explore whether there are microarchitectural attacks other than measuring data cache timing that can be used for exfiltrating data out of speculative execution.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Other microarchitectures&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Our research was relatively Haswell-centric so far. It would be interesting to see details e.g. on how the branch prediction of other modern processors works and how well it can be attacked.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Other JIT engines&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We developed a successful variant 1 attack against the JIT engine built into the Linux kernel. It would be interesting to see whether attacks against more advanced JIT engines with less control over the system are also practical - in particular, JavaScript engines.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;More efficient scanning for host-virtual addresses and cache sets&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In variant 2, while scanning for the host-virtual address of a guest-owned page, it might make sense to attempt to determine its L3 cache set first. This could be done by performing L3 evictions using an eviction pattern through the physmap, then testing whether the eviction affected the guest-owned page.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The same might work for cache sets - use an L1D+L2 eviction set to evict the function pointer in the host kernel context, use a gadget in the kernel to evict an L3 set using physical addresses, then use that to identify which cache sets guest lines belong to until a guest-owned eviction set has been constructed.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Dumping the complete BTB state&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Given that the generic BTB seems to only be able to distinguish 2&lt;/span&gt;&lt;span&gt;31-8&lt;/span&gt; &lt;span&gt;or fewer source addresses, it seems feasible to dump out the complete BTB state generated by e.g. a hypercall in a timeframe around the order of a few hours. (Scan for jump sources, then for every discovered jump source, bisect the jump target.) This could potentially be used to identify the locations of functions in the host kernel even if the host kernel is custom-built.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The source address aliasing would reduce the usefulness somewhat, but because target addresses don't suffer from that, it might be possible to correlate (source,target) pairs from machines with different KASLR offsets and reduce the number of candidate addresses based on KASLR being additive while aliasing is bitwise.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;This could then potentially allow an attacker to make guesses about the host kernel version or the compiler used to build it based on jump offsets or distances between functions.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Variant 2: Leaking with more efficient gadgets&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;If sufficiently efficient gadgets are used for variant 2, it might not be necessary to evict host kernel function pointers from the L3 cache at all; it might be sufficient to only evict them from L1D and L2.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Various speedups&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;In particular the variant 2 PoC is still a bit slow. This is probably partly because:&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It only leaks one bit at a time; leaking more bits at a time should be doable.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It heavily uses IRETQ for hiding control flow from the processor.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;It would be interesting to see what data leak rate can be achieved using variant 2.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Leaking or injection through the return predictor&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;If the return predictor also doesn't lose its state on a privilege level change, it might be useful for either locating the host kernel from inside a VM (in which case bisection could be used to very quickly discover the full address of the host kernel) or injecting return targets (in particular if the return address is stored in a cache line that can be flushed out by the attacker and isn't reloaded before the return instruction).&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;However, we have not performed any experiments with the return predictor that yielded conclusive results so far.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Leaking data out of the indirect call predictor&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;We have attempted to leak target information out of the indirect call predictor, but haven't been able to make it work.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;The following statement were provided to us regarding this issue from the vendors to whom Project Zero disclosed this vulnerability:&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Intel&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;No current statement provided at this time.&lt;/span&gt;&lt;/div&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;AMD&lt;/span&gt;&lt;/h2&gt;

&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;ARM&lt;/span&gt;&lt;/h2&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Arm recognises that the speculation functionality of many modern high-performance processors, despite working as intended, can be used in conjunction with the timing of cache operations to leak some information as described in this blog. Correspondingly, Arm has developed software mitigations that we recommend be deployed.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;
&lt;br/&gt;&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Arm has included a detailed technical whitepaper as well as links to information from some of Arm’s architecture partners regarding their specific implementations and mitigations.&lt;/span&gt;&lt;/div&gt;

&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;Note that some of these documents - in particular Intel's documentation - change over time, so quotes from and references to it may not reflect the latest version of Intel's documentation.&lt;/span&gt;&lt;/div&gt;
&lt;br/&gt;&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Placing data immediately following an indirect branch can cause a performance problem. If the data consists of all zeros, it looks like a long stream of ADDs to memory destinations and this can cause resource conflicts and slow down branch recovery. Also, data immediately following indirect branches may appear as branches to the branch predication [sic] hardware, which can branch off to execute other data pages. This can lead to subsequent self-modifying code problems.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Loads can:[...]Be carried out speculatively, before preceding branches are resolved.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Software should avoid writing to a code page in the same 1-KByte subpage that is being executed or fetching code in the same 2-KByte subpage of that is being written. In addition, sharing a page containing directly or speculatively executed code with another processor as a data page can trigger an SMC condition that causes the entire pipeline of the machine and the trace cache to be cleared. This is due to the self-modifying code condition.&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;if mapped as WB or WT, there is a potential for speculative processor reads to bring the data into the caches&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;span&gt;&quot;Failure to map the region as WC may allow the line to be speculatively read into the processor caches (via the wrong path of a mispredicted branch).&quot;&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1507.06955.pdf&quot;&gt;&lt;span&gt;https://arxiv.org/pdf/1507.06955.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;: The rowhammer.js research by Daniel Gruss, Clémentine Maurice and Stefan Mangard contains information about L3 cache eviction patterns that we reused in the KVM PoC to evict a function pointer.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://www.sophia.re/thesis.pdf&quot;&gt;&lt;span&gt;https://www.sophia.re/thesis.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;: Sophia D'Antoine wrote a thesis that shows that opcode scheduling can theoretically be used to transmit data between hyperthreads.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;
&lt;div dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://gruss.cc/files/kaiser.pdf&quot;&gt;&lt;span&gt;https://gruss.cc/files/kaiser.pdf&lt;/span&gt;&lt;/a&gt;&lt;span&gt;: Daniel Gruss, Moritz Lipp, Michael Schwarz, Richard Fellner, Clémentine Maurice, and Stefan Mangard wrote a paper on mitigating microarchitectural issues caused by pagetable sharing between userspace and the kernel.&lt;/span&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;li dir=&quot;ltr&quot;&gt;

&lt;/li&gt;
&lt;/ul&gt;

&lt;div&gt;&lt;span&gt;[2]&lt;/span&gt; &lt;span&gt;The precise model names are listed in the section &quot;Tested Processors&quot;. The code for reproducing this is in the writeup_files.tar archive in our bugtracker, in the folders userland_test_x86 and userland_test_aarch64.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;[3]&lt;/span&gt; &lt;span&gt;The attacker-controlled offset used to perform an out-of-bounds access on an array by this PoC is a 32-bit value, limiting the accessible addresses to a 4GiB window in the kernel heap area.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;[4]&lt;/span&gt; &lt;span&gt;This PoC won't work on CPUs with SMAP support; however, that is not a fundamental limitation.&lt;/span&gt;&lt;/div&gt;

&lt;div&gt;&lt;span&gt;[6]&lt;/span&gt; &lt;span&gt;The phone was running an Android build from May 2017.&lt;/span&gt;&lt;/div&gt;


&lt;div&gt;&lt;span&gt;[9]&lt;/span&gt; &lt;span&gt;More than 2&lt;/span&gt;&lt;span&gt;15&lt;/span&gt; &lt;span&gt;mappings would be more efficient, but the kernel places a hard cap of 2&lt;/span&gt;&lt;span&gt;16&lt;/span&gt; &lt;span&gt;on the number of VMAs that a process can have.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;[10]&lt;/span&gt; &lt;span&gt;Intel's optimization manual states that &quot;In the first implementation of HT Technology, the physical execution resources are shared and the architecture state is duplicated for each logical processor&quot;, so it would be plausible for predictor state to be shared. While predictor state could be tagged by logical core, that would likely reduce performance for multithreaded processes, so it doesn't seem likely.&lt;/span&gt;&lt;/div&gt;
&lt;div&gt;&lt;span&gt;[11]&lt;/span&gt; &lt;span&gt;In case the history buffer was a bit bigger than we had measured, we added some margin - in particular because we had seen slightly different history buffer lengths in different experiments, and because 26 isn't a very round number.&lt;/span&gt;&lt;/div&gt;



</description>
<pubDate>Wed, 03 Jan 2018 22:20:48 +0000</pubDate>
<dc:creator>brandon</dc:creator>
<og:url>https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html</og:url>
<og:title>Reading privileged memory with a side-channel</og:title>
<og:description>Posted by Jann Horn, Project Zero We have discovered that CPU data cache timing can be abused to efficiently leak information out of mi...</og:description>
<og:image>https://3.bp.blogspot.com/-64YRnkgMujY/Wk064WtrSRI/AAAAAAAACPg/O4ZxlUiWerACL61P490xrVMWSmHTwQflQCEwYBhgL/w1200-h630-p-k-no-nu/image3.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html</dc:identifier>
</item>
<item>
<title>Degraded performance after forced reboot due to AWS instance maintenance</title>
<link>https://forums.aws.amazon.com/thread.jspa?threadID=269858</link>
<guid isPermaLink="true" >https://forums.aws.amazon.com/thread.jspa?threadID=269858</guid>
<description>&lt;td valign=&quot;top&quot; width=&quot;99%&quot;&gt;
&lt;div id=&quot;jive-flatpage&quot;&gt;
&lt;table summary=&quot;Message&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot; width=&quot;100%&quot; readability=&quot;8.0604908399585&quot;&gt;&lt;tr id=&quot;jive-message-820266&quot; valign=&quot;top&quot; readability=&quot;16.120981679917&quot;&gt;&lt;td class=&quot;jive-first&quot; valign=&quot;top&quot;&gt;
&lt;table class=&quot;jive-thread-messagebox&quot; width=&quot;100%&quot; readability=&quot;9.8898725212465&quot;&gt;&lt;tr readability=&quot;3.2242424242424&quot;&gt;&lt;td valign=&quot;top&quot; width=&quot;1%&quot;/&gt;
&lt;td width=&quot;100%&quot; valign=&quot;top&quot; readability=&quot;3.7211538461538&quot;&gt;&lt;span class=&quot;jive-subject&quot;&gt; Degraded performance after forced reboot due to AWS instance maintenance&lt;/span&gt;
&lt;div readability=&quot;5.3975903614458&quot;&gt;

&lt;p&gt;Posted on: &lt;span&gt;Dec 19, 2017 11:15 AM&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;td align=&quot;right&quot; valign=&quot;top&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;1.5&quot;&gt;&lt;td valign=&quot;top&quot; colspan=&quot;3&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;11.85192234488&quot;&gt;&lt;td colspan=&quot;3&quot;&gt;
&lt;div id=&quot;jive-question-box&quot; class=&quot;jive-infobox&quot;&gt;
&lt;table cellpadding=&quot;2&quot; cellspacing=&quot;0&quot; border=&quot;0&quot;&gt;&lt;tbody readability=&quot;1&quot;&gt;&lt;tr valign=&quot;top&quot; readability=&quot;2&quot;&gt;&lt;td width=&quot;1&quot;&gt;&lt;img src=&quot;https://dn1lvoe5mgzdm.cloudfront.net/images/question-resolved-24x24.gif&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td width=&quot;99%&quot;&gt;This question is &lt;span class=&quot;ans&quot;&gt;answered&lt;/span&gt;.&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt;
&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; width=&quot;100%&quot; class=&quot;jive-message-body-wrapper&quot; readability=&quot;28.689230769231&quot;&gt;&lt;tr readability=&quot;11.84&quot;&gt;&lt;td&gt;
&lt;div class=&quot;jive-message-body&quot; readability=&quot;47.815384615385&quot;&gt;
&lt;div readability=&quot;45.538461538462&quot;&gt;Five days ago I received email from AWS (see below for full text) which informed me that a reboot of one of my instances was necessary due to &quot;updates&quot;. To pre-empt auto-reboot on 5th Jan I manually rebooted 3 days ago. &lt;strong&gt;Immediately following the reboot my server running on this instance started to suffer from cpu stress.&lt;/strong&gt; Looking at cpu stats there was a very clear change in daily cpu usage pattern, despite continuing normal traffic to my server. I performed extensive review of what might have changed on my server configuration but drew a complete blank - configuration of the server did not change.&lt;p&gt;It is simply as if the instance (m1.medium) was somehow degraded to a lesser performing one following the reboot. I simply can't find any explanation other than a change to the instance capability that took effect when I rebooted.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;What could possible be causing this? I'm at wits end trying to understand what happened? Is it possible that AWS maintenance is responsible for this degradation?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;See attached file showing changed cpu pattern that started on 15th Dec immediately following the reboot.&lt;/p&gt;&lt;p&gt;==============================&lt;br class=&quot;jive-newline&quot;/&gt;
Full email from AWS announcing need to reboot:&lt;/p&gt;&lt;p&gt;Dear Amazon EC2 Customer,&lt;/p&gt;&lt;p&gt;One or more of your Amazon EC2 instances in the ap-southeast-2 region requires important security and operational updates which will require a reboot of your instance. A maintenance window has been scheduled between Sat, 6 Jan 2018 03:00:00 GMT and Sat, 6 Jan 2018 05:00:00 GMT during which the EC2 service will automatically perform the required reboot. During the maintenance window, the affected instance will be unavailable for a short period of time as it reboots. You may instead choose to reboot the instance yourself at any time before the maintenance window. If you choose to do this, the maintenance will be marked as completed and no reboot will occur during the maintenance window. For more information on EC2 maintenance please see our documentation here: &lt;a class=&quot;jive-link-external&quot; href=&quot;https://aws.amazon.com/maintenance-help/&quot;&gt;https://aws.amazon.com/maintenance-help/&lt;/a&gt;. More details on rebooting your instances yourself can be found here:&lt;br class=&quot;jive-newline&quot;/&gt;&lt;a class=&quot;jive-link-external&quot; href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-reboot.html&quot;&gt;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-reboot.html&lt;/a&gt;&lt;/p&gt;&lt;p&gt;To see which of your instances are impacted please visit the 'Events' page on the EC2 console to view your instances that are scheduled for maintenance:&lt;/p&gt;&lt;p&gt;&lt;a class=&quot;jive-link-external&quot; href=&quot;https://ap-southeast-2.console.aws.amazon.com/ec2/v2/home?region=ap-southeast-2#Events&quot;&gt;https://ap-southeast-2.console.aws.amazon.com/ec2/v2/home?region=ap-southeast-2#Events&lt;/a&gt;&lt;/p&gt;&lt;p&gt;If you have any questions or concerns, you can contact the AWS Support Team on the community forums and via AWS Premium Support at: &lt;a class=&quot;jive-link-external&quot; href=&quot;https://aws.amazon.com/support&quot;&gt;https://aws.amazon.com/support&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Edited by: ajnaware on Dec 19, 2017 11:19 AM&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;div id=&quot;jive-message-holder&quot;&gt;
&lt;div class=&quot;jive-message-list&quot;&gt;
&lt;div class=&quot;jive-table&quot;&gt;
&lt;div class=&quot;jive-messagebox&quot;&gt;
&lt;table summary=&quot;Message&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot; width=&quot;100%&quot; readability=&quot;3.8159509202454&quot;&gt;&lt;tr id=&quot;jive-message-820492&quot; valign=&quot;top&quot; readability=&quot;7.6319018404908&quot;&gt;&lt;td class=&quot;jive-first&quot; valign=&quot;top&quot;&gt;
&lt;table class=&quot;jive-thread-messagebox&quot; width=&quot;100%&quot; readability=&quot;5.2469325153374&quot;&gt;&lt;tr readability=&quot;3.3406593406593&quot;&gt;&lt;td valign=&quot;top&quot; width=&quot;1%&quot;/&gt;
&lt;td width=&quot;100%&quot; valign=&quot;top&quot; readability=&quot;3.849710982659&quot;&gt;&lt;span class=&quot;jive-subject&quot;&gt; Re: Degraded performance after forced reboot due to AWS instance maintenanc&lt;/span&gt;
&lt;div readability=&quot;5.9381443298969&quot;&gt;

&lt;p&gt;Posted on: &lt;span&gt;Dec 20, 2017 10:54 AM&lt;/span&gt;&lt;/p&gt;


&lt;/div&gt;
&lt;/td&gt;
&lt;td align=&quot;right&quot; valign=&quot;top&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; colspan=&quot;3&quot;&gt;
&lt;table width=&quot;100%&quot;&gt;&lt;tr&gt;&lt;td width=&quot;80%&quot; valign=&quot;top&quot;/&gt;
&lt;td valign=&quot;top&quot; align=&quot;right&quot;&gt;
&lt;div class=&quot;jive-helpful-rating&quot;&gt;
&lt;table cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot;&gt;&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;https://dn1lvoe5mgzdm.cloudfront.net/images/answer-helpful-16x16.gif&quot; title=&quot;5 points&quot; border=&quot;0&quot; alt=&quot;&quot;/&gt;&lt;/td&gt;
&lt;td nowrap=&quot;nowrap&quot;&gt;Helpful&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;7&quot;&gt;&lt;td colspan=&quot;3&quot;&gt;
&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; width=&quot;100%&quot; class=&quot;jive-message-body-wrapper&quot; readability=&quot;28.5&quot;&gt;&lt;tr readability=&quot;7&quot;&gt;&lt;td&gt;
&lt;div class=&quot;jive-message-body&quot; readability=&quot;36.5&quot;&gt;
&lt;div readability=&quot;18&quot;&gt;We are experiencing the same thing across all roles in our fleet.&lt;p&gt;Attached is a CPU graph (statistic: average, period: 1 hour) for one instance type. The arrows point at reboot events. blue lines are systems that have been rebooted at some point in this graph. red lines are systems that have not been rebooted.&lt;/p&gt;&lt;p&gt;These hosts are all behind the same ELB handing uniform traffic patterns throughout this graphed time period.&lt;/p&gt;&lt;p&gt;Edited by: sfdanb on Dec 20, 2017 10:55 AM&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jive-message-list&quot;&gt;
&lt;div class=&quot;jive-table&quot;&gt;
&lt;div class=&quot;jive-messagebox&quot;&gt;
&lt;table summary=&quot;Message&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot; width=&quot;100%&quot; readability=&quot;7.8999272197962&quot;&gt;&lt;tr id=&quot;jive-message-820608&quot; valign=&quot;top&quot; readability=&quot;15.799854439592&quot;&gt;&lt;td class=&quot;jive-first&quot; valign=&quot;top&quot;&gt;
&lt;table class=&quot;jive-thread-messagebox&quot; width=&quot;100%&quot; readability=&quot;9.2940320232897&quot;&gt;&lt;tr readability=&quot;3.4782608695652&quot;&gt;&lt;td valign=&quot;top&quot; width=&quot;1%&quot;/&gt;
&lt;td width=&quot;100%&quot; valign=&quot;top&quot; readability=&quot;4.0263157894737&quot;&gt;&lt;span class=&quot;jive-subject&quot;&gt; Re: Degraded performance after forced reboot due to AWS instance maintenanc&lt;/span&gt;
&lt;div readability=&quot;6.3157894736842&quot;&gt;

&lt;p&gt;Posted on: &lt;span&gt;Dec 20, 2017 3:47 PM&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;td align=&quot;right&quot; valign=&quot;top&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; colspan=&quot;3&quot;&gt;
&lt;table width=&quot;100%&quot;&gt;&lt;tr&gt;&lt;td width=&quot;80%&quot; valign=&quot;top&quot;/&gt;
&lt;td valign=&quot;top&quot; align=&quot;right&quot;&gt;
&lt;div class=&quot;jive-helpful-rating&quot;&gt;
&lt;table cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot;&gt;&lt;tr&gt;&lt;td&gt;&lt;img src=&quot;https://dn1lvoe5mgzdm.cloudfront.net/images/answer-helpful-16x16.gif&quot; title=&quot;5 points&quot; border=&quot;0&quot; alt=&quot;&quot;/&gt;&lt;/td&gt;
&lt;td nowrap=&quot;nowrap&quot;&gt;Helpful&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;14.990033222591&quot;&gt;&lt;td colspan=&quot;3&quot;&gt;
&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; width=&quot;100%&quot; class=&quot;jive-message-body-wrapper&quot; readability=&quot;30.916943521595&quot;&gt;&lt;tr readability=&quot;14.990033222591&quot;&gt;&lt;td&gt;
&lt;div class=&quot;jive-message-body&quot; readability=&quot;39.348837209302&quot;&gt;
&lt;div readability=&quot;27.169435215947&quot;&gt;Hi,&lt;br class=&quot;jive-newline&quot;/&gt;
The update that is being applied to a portion of EC2 instances can, in some corner cases, require additional CPU resources. We always attempt to make updates and maintenance smooth and non-disruptive for customers, and in the vast majority of cases we are able to apply updates without scheduling maintenance events like instance reboots. For this update, we have attempted to find and eliminate as many of the corner cases that influence performance as possible.&lt;p&gt;For some time we have recommended that customers use our latest generation instances with HVM AMIs to get the best performance from EC2 (see &lt;a class=&quot;jive-link-external&quot; href=&quot;http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/virtualization_types.html&quot;&gt;http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/virtualization_types.html&lt;/a&gt;). If moving to a HVM based AMI is not easy, changing your instance size to m3.medium, which provides more compute than m1.medium at a lower price, may be a workaround.&lt;/p&gt;&lt;p&gt;As the notice points out, the update that is being applied is important to maintain the high security and operational aspects for your instances. We want to make every effort to make this as non-disruptive as possible. If this information does not help you resolve the CPU utilization issue you're experiencing, please reach out again.&lt;/p&gt;&lt;p&gt;Kind regards,&lt;/p&gt;&lt;p&gt;Matt&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jive-message-list&quot;&gt;
&lt;div class=&quot;jive-table&quot;&gt;
&lt;div class=&quot;jive-messagebox&quot;&gt;
&lt;table summary=&quot;Message&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot; width=&quot;100%&quot; readability=&quot;3.3560606060606&quot;&gt;&lt;tr id=&quot;jive-message-820598&quot; valign=&quot;top&quot; readability=&quot;6.7121212121212&quot;&gt;&lt;td class=&quot;jive-first&quot; valign=&quot;top&quot;&gt;
&lt;table class=&quot;jive-thread-messagebox&quot; width=&quot;100%&quot; readability=&quot;4.7943722943723&quot;&gt;&lt;tr readability=&quot;3.5220125786164&quot;&gt;&lt;td valign=&quot;top&quot; width=&quot;1%&quot;/&gt;
&lt;td width=&quot;100%&quot; valign=&quot;top&quot; readability=&quot;4.08&quot;&gt;&lt;span class=&quot;jive-subject&quot;&gt; Re: Degraded performance after forced reboot due to AWS instance maintenanc&lt;/span&gt;
&lt;div readability=&quot;6.4864864864865&quot;&gt;

&lt;p&gt;Posted on: &lt;span&gt;Dec 20, 2017 6:20 PM&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;td align=&quot;right&quot; valign=&quot;top&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; colspan=&quot;3&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;6&quot;&gt;&lt;td colspan=&quot;3&quot;&gt;
&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; width=&quot;100%&quot; class=&quot;jive-message-body-wrapper&quot; readability=&quot;28&quot;&gt;&lt;tr readability=&quot;6&quot;&gt;&lt;td&gt;
&lt;div class=&quot;jive-message-body&quot; readability=&quot;33.5&quot;&gt;
&lt;div readability=&quot;12&quot;&gt;Fortunately we had been working on a path to upgrade our fleet to HVM. This upgrade forced our hand, so we can confirm that on the same instance type (c3.xlarge) and using the same code we have returned to an acceptable performance level on affected hosts with HVM AMIs.&lt;p&gt;It was a very long couple days..&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jive-message-list&quot;&gt;
&lt;div class=&quot;jive-table&quot;&gt;
&lt;div class=&quot;jive-messagebox&quot;&gt;
&lt;table summary=&quot;Message&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot; width=&quot;100%&quot; readability=&quot;6.3978293413174&quot;&gt;&lt;tr id=&quot;jive-message-820640&quot; valign=&quot;top&quot; readability=&quot;12.795658682635&quot;&gt;&lt;td class=&quot;jive-first&quot; valign=&quot;top&quot;&gt;
&lt;table class=&quot;jive-thread-messagebox&quot; width=&quot;100%&quot; readability=&quot;7.874251497006&quot;&gt;&lt;tr readability=&quot;3.4782608695652&quot;&gt;&lt;td valign=&quot;top&quot; width=&quot;1%&quot;/&gt;
&lt;td width=&quot;100%&quot; valign=&quot;top&quot; readability=&quot;4.0263157894737&quot;&gt;&lt;span class=&quot;jive-subject&quot;&gt; Re: Degraded performance after forced reboot due to AWS instance maintenanc&lt;/span&gt;
&lt;div readability=&quot;6.3157894736842&quot;&gt;

&lt;p&gt;Posted on: &lt;span&gt;Dec 20, 2017 6:22 PM&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;td align=&quot;right&quot; valign=&quot;top&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; colspan=&quot;3&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;12&quot;&gt;&lt;td colspan=&quot;3&quot;&gt;
&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; width=&quot;100%&quot; class=&quot;jive-message-body-wrapper&quot; readability=&quot;31&quot;&gt;&lt;tr readability=&quot;12&quot;&gt;&lt;td readability=&quot;9&quot;&gt;
&lt;div class=&quot;jive-message-body&quot; readability=&quot;42&quot;&gt;
&lt;p&gt;Thanks for the detailed reply. I guess you are essentially confirming that the instance maintenance was likely to be the reason for the major change to cpu usage, and that I am one of those edge cases, and that the only solution now is to change instance type. Of course I am not entirely happy about this. I bought a 3-year reserved instance 2 years ago, and now have to hope I can sell the remaining year for a reasonable amount (which may be a stretch given that I am apparently using legacy instance type), and then purchase new reserved instance after upgrading. I will likely only buy 1 year reserved instances henceforth, given that there is apparently no guarantee that the instance will actually remain viable for the full duration, should you undertake any future maintenance causing similar issues. Also as I am not personally expert enough to carry out the new instance selection and update myself, I am having to pay for hired expert assistance to do this. Also the cpu max-outs I've experienced have caused some grief regarding my own user relations. So all in all I'm pretty disappointed about this issue. If I manage to sort it all out I'll mark as answered.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jive-message-list&quot;&gt;
&lt;div class=&quot;jive-table&quot;&gt;
&lt;div class=&quot;jive-messagebox&quot;&gt;
&lt;table summary=&quot;Message&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot; width=&quot;100%&quot; readability=&quot;8.8892107168718&quot;&gt;&lt;tr id=&quot;jive-message-820668&quot; valign=&quot;top&quot; readability=&quot;17.778421433744&quot;&gt;&lt;td class=&quot;jive-first&quot; valign=&quot;top&quot;&gt;
&lt;table class=&quot;jive-thread-messagebox&quot; width=&quot;100%&quot; readability=&quot;10.37074583635&quot;&gt;&lt;tr readability=&quot;3.5668789808917&quot;&gt;&lt;td valign=&quot;top&quot; width=&quot;1%&quot;/&gt;
&lt;td width=&quot;100%&quot; valign=&quot;top&quot; readability=&quot;4.1351351351351&quot;&gt;&lt;span class=&quot;jive-subject&quot;&gt; Re: Degraded performance after forced reboot due to AWS instance maintenanc&lt;/span&gt;
&lt;div readability=&quot;6.6666666666667&quot;&gt;

&lt;p&gt;Posted on: &lt;span&gt;Dec 20, 2017 7:55 PM&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;td align=&quot;right&quot; valign=&quot;top&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; colspan=&quot;3&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;17&quot;&gt;&lt;td colspan=&quot;3&quot;&gt;
&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; width=&quot;100%&quot; class=&quot;jive-message-body-wrapper&quot; readability=&quot;33.5&quot;&gt;&lt;tr readability=&quot;17&quot;&gt;&lt;td readability=&quot;11.5&quot;&gt;
&lt;div class=&quot;jive-message-body&quot; readability=&quot;47&quot;&gt;
&lt;p&gt;We are in the exact same situation with an m1.medium instance that has a bit over a year to go on a three year reserved instance. As our business is primarily online, we have now suffered significant losses. In our case, Amazon tried to say that our problem was not the same as what everyone else is reporting, and instead was due to our running an old kernel and Linux distribution in general, despite the fact that we had exactly the same symptoms. We have now upgraded our distro, but are having the same problem. It really sounds as if Amazon screwed up with an inadequately tested upgrade and are now trying to avoid responsibility. That is very unfortunate, as we could accept the admission of an honest mistake, but not these excuses with no attempt on their part to fix the problem. We may have to upgrade now, but we'll certainly be looking to move to different hosting. I suspect they are afraid to admit liability, or maybe they just don't care about the people who will be affected, as I suspect it is mostly smaller customers. If that is the case, then they may have even known this would be the result, and just made a decision that the resulting loss of goodwill and probable lawsuits was worth the tradeoff.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jive-message-list&quot;&gt;
&lt;div class=&quot;jive-table&quot;&gt;
&lt;div class=&quot;jive-messagebox&quot;&gt;
&lt;table summary=&quot;Message&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot; width=&quot;100%&quot; readability=&quot;3.8613138686131&quot;&gt;&lt;tr id=&quot;jive-message-820635&quot; valign=&quot;top&quot; readability=&quot;7.7226277372263&quot;&gt;&lt;td class=&quot;jive-first&quot; valign=&quot;top&quot;&gt;
&lt;table class=&quot;jive-thread-messagebox&quot; width=&quot;100%&quot; readability=&quot;5.3093065693431&quot;&gt;&lt;tr readability=&quot;3.525&quot;&gt;&lt;td valign=&quot;top&quot; width=&quot;1%&quot;/&gt;
&lt;td width=&quot;100%&quot; valign=&quot;top&quot; readability=&quot;4.0827814569536&quot;&gt;&lt;span class=&quot;jive-subject&quot;&gt; Re: Degraded performance after forced reboot due to AWS instance maintenanc&lt;/span&gt;
&lt;div readability=&quot;6.5066666666667&quot;&gt;

&lt;p&gt;Posted on: &lt;span&gt;Dec 20, 2017 10:05 PM&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;td align=&quot;right&quot; valign=&quot;top&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; colspan=&quot;3&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;7&quot;&gt;&lt;td colspan=&quot;3&quot;&gt;
&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; width=&quot;100%&quot; class=&quot;jive-message-body-wrapper&quot; readability=&quot;28.5&quot;&gt;&lt;tr readability=&quot;7&quot;&gt;&lt;td readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;jive-message-body&quot; readability=&quot;37&quot;&gt;
&lt;p&gt;For what it's worth, we also had reserved instances (c3.xlarge) and we were able to relaunch our instances on that same instance type with HVM. So while I'm sure it's small consolation (as it was for us) at least you don't need to immediately deal with reselling the RIs. You just need to figure out how to migrate to HVM and relaunch the instances... which is no small task, to be sure.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jive-message-list&quot;&gt;
&lt;div class=&quot;jive-table&quot;&gt;
&lt;div class=&quot;jive-messagebox&quot;&gt;
&lt;table summary=&quot;Message&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot; width=&quot;100%&quot; readability=&quot;2.752427184466&quot;&gt;&lt;tr id=&quot;jive-message-820602&quot; valign=&quot;top&quot; readability=&quot;5.504854368932&quot;&gt;&lt;td class=&quot;jive-first&quot; valign=&quot;top&quot;&gt;
&lt;table class=&quot;jive-thread-messagebox&quot; width=&quot;100%&quot; readability=&quot;3.2111650485437&quot;&gt;&lt;tr readability=&quot;3.5696202531646&quot;&gt;&lt;td valign=&quot;top&quot; width=&quot;1%&quot;/&gt;
&lt;td width=&quot;100%&quot; valign=&quot;top&quot; readability=&quot;4.1375838926174&quot;&gt;&lt;span class=&quot;jive-subject&quot;&gt; Re: Degraded performance after forced reboot due to AWS instance maintenanc&lt;/span&gt;
&lt;div readability=&quot;6.6849315068493&quot;&gt;

&lt;p&gt;Posted on: &lt;span&gt;Dec 20, 2017 10:14 PM&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;td align=&quot;right&quot; valign=&quot;top&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; colspan=&quot;3&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td colspan=&quot;3&quot;&gt;
&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; width=&quot;100%&quot; class=&quot;jive-message-body-wrapper&quot; readability=&quot;26.5&quot;&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td readability=&quot;4.5&quot;&gt;
&lt;div class=&quot;jive-message-body&quot; readability=&quot;33&quot;&gt;
&lt;p&gt;Unfortunately, M1 instances do not support HVM.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jive-message-list&quot;&gt;
&lt;div class=&quot;jive-table&quot;&gt;
&lt;div class=&quot;jive-messagebox&quot;&gt;
&lt;table summary=&quot;Message&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot; width=&quot;100%&quot; readability=&quot;5.2431318681319&quot;&gt;&lt;tr id=&quot;jive-message-820645&quot; valign=&quot;top&quot; readability=&quot;10.486263736264&quot;&gt;&lt;td class=&quot;jive-first&quot; valign=&quot;top&quot;&gt;
&lt;table class=&quot;jive-thread-messagebox&quot; width=&quot;100%&quot; readability=&quot;6.1964285714286&quot;&gt;&lt;tr readability=&quot;3.5696202531646&quot;&gt;&lt;td valign=&quot;top&quot; width=&quot;1%&quot;/&gt;
&lt;td width=&quot;100%&quot; valign=&quot;top&quot; readability=&quot;4.1375838926174&quot;&gt;&lt;span class=&quot;jive-subject&quot;&gt; Re: Degraded performance after forced reboot due to AWS instance maintenanc&lt;/span&gt;
&lt;div readability=&quot;6.6849315068493&quot;&gt;

&lt;p&gt;Posted on: &lt;span&gt;Dec 20, 2017 10:19 PM&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;jive-message-info&quot;&gt;&lt;a href=&quot;https://forums.aws.amazon.com/message.jspa?messageID=820602#820602&quot; title=&quot;in response to: lvms&quot;&gt;&lt;img src=&quot;https://forums.aws.amazon.com/images/up-10x10.gif&quot; width=&quot;10&quot; height=&quot;10&quot; border=&quot;0&quot; alt=&quot;in response to: lvms&quot; title=&quot;in response to: lvms&quot;/&gt;&lt;/a&gt; in response to: &lt;a href=&quot;https://forums.aws.amazon.com/message.jspa?messageID=820602#820602&quot; title=&quot;Go to message&quot;&gt;lvms&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td align=&quot;right&quot; valign=&quot;top&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; colspan=&quot;3&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;9&quot;&gt;&lt;td colspan=&quot;3&quot;&gt;
&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; width=&quot;100%&quot; class=&quot;jive-message-body-wrapper&quot; readability=&quot;29.5&quot;&gt;&lt;tr readability=&quot;9&quot;&gt;&lt;td&gt;
&lt;div class=&quot;jive-message-body&quot; readability=&quot;33&quot;&gt;
&lt;div readability=&quot;11&quot;&gt;Hi,&lt;p&gt;All instance types can now run HVM AMIs for any operating system. Previously only Windows HVM AMIs could be used in HVM mode on M1, M2, C1, and T1 instances. This is no longer the case.&lt;/p&gt;&lt;p&gt;Kind regards,&lt;/p&gt;&lt;p&gt;Matt&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jive-message-list&quot;&gt;
&lt;div class=&quot;jive-table&quot;&gt;
&lt;div class=&quot;jive-messagebox&quot;&gt;
&lt;table summary=&quot;Message&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot; width=&quot;100%&quot; readability=&quot;4.7848360655738&quot;&gt;&lt;tr id=&quot;jive-message-820646&quot; valign=&quot;top&quot; readability=&quot;9.5696721311475&quot;&gt;&lt;td class=&quot;jive-first&quot; valign=&quot;top&quot;&gt;
&lt;table class=&quot;jive-thread-messagebox&quot; width=&quot;100%&quot; readability=&quot;6.2202868852459&quot;&gt;&lt;tr readability=&quot;3.4814814814815&quot;&gt;&lt;td valign=&quot;top&quot; width=&quot;1%&quot;/&gt;
&lt;td width=&quot;100%&quot; valign=&quot;top&quot; readability=&quot;4.0294117647059&quot;&gt;&lt;span class=&quot;jive-subject&quot;&gt; Re: Degraded performance after forced reboot due to AWS instance maintenanc&lt;/span&gt;
&lt;div readability=&quot;6.3376623376623&quot;&gt;

&lt;p&gt;Posted on: &lt;span&gt;Dec 20, 2017 10:38 PM&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;td align=&quot;right&quot; valign=&quot;top&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; colspan=&quot;3&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;9&quot;&gt;&lt;td colspan=&quot;3&quot;&gt;
&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; width=&quot;100%&quot; class=&quot;jive-message-body-wrapper&quot; readability=&quot;29.5&quot;&gt;&lt;tr readability=&quot;9&quot;&gt;&lt;td&gt;
&lt;div class=&quot;jive-message-body&quot; readability=&quot;34.5&quot;&gt;
&lt;div readability=&quot;14&quot;&gt;Hi,&lt;p&gt;Before selling your RI, can you try running your workload on a HVM AMI running on a m1.medium instance?&lt;/p&gt;&lt;p&gt;I am also disappointed that we have fallen short in making this maintenance completely painless for you, despite our continuing efforts. We will follow up directly to make sure your issues are resolved.&lt;/p&gt;&lt;p&gt;Kind regards,&lt;/p&gt;&lt;p&gt;Matt&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jive-message-list&quot;&gt;
&lt;div class=&quot;jive-table&quot;&gt;
&lt;div class=&quot;jive-messagebox&quot;&gt;
&lt;table summary=&quot;Message&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot; width=&quot;100%&quot; readability=&quot;6.4376058723885&quot;&gt;&lt;tr id=&quot;jive-message-820682&quot; valign=&quot;top&quot; readability=&quot;12.875211744777&quot;&gt;&lt;td class=&quot;jive-first&quot; valign=&quot;top&quot;&gt;
&lt;table class=&quot;jive-thread-messagebox&quot; width=&quot;100%&quot; readability=&quot;7.9232072275551&quot;&gt;&lt;tr readability=&quot;3.5696202531646&quot;&gt;&lt;td valign=&quot;top&quot; width=&quot;1%&quot;/&gt;
&lt;td width=&quot;100%&quot; valign=&quot;top&quot; readability=&quot;4.1375838926174&quot;&gt;&lt;span class=&quot;jive-subject&quot;&gt; Re: Degraded performance after forced reboot due to AWS instance maintenanc&lt;/span&gt;
&lt;div readability=&quot;6.6849315068493&quot;&gt;

&lt;p&gt;Posted on: &lt;span&gt;Dec 20, 2017 11:41 PM&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;jive-message-info&quot;&gt;&lt;a href=&quot;https://forums.aws.amazon.com/message.jspa?messageID=820668#820668&quot; title=&quot;in response to: lvms&quot;&gt;&lt;img src=&quot;https://forums.aws.amazon.com/images/up-10x10.gif&quot; width=&quot;10&quot; height=&quot;10&quot; border=&quot;0&quot; alt=&quot;in response to: lvms&quot; title=&quot;in response to: lvms&quot;/&gt;&lt;/a&gt; in response to: &lt;a href=&quot;https://forums.aws.amazon.com/message.jspa?messageID=820668#820668&quot; title=&quot;Go to message&quot;&gt;lvms&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;td align=&quot;right&quot; valign=&quot;top&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; colspan=&quot;3&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;12&quot;&gt;&lt;td colspan=&quot;3&quot;&gt;
&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; width=&quot;100%&quot; class=&quot;jive-message-body-wrapper&quot; readability=&quot;31&quot;&gt;&lt;tr readability=&quot;12&quot;&gt;&lt;td&gt;
&lt;div class=&quot;jive-message-body&quot; readability=&quot;41.5&quot;&gt;
&lt;div readability=&quot;28&quot;&gt;Hi,&lt;p&gt;I have reviewed support case 4743634091 regarding what you're experiencing on your instance. You're correct that what you are seeing is not the same issue as what others are reporting. In the first correspondence from support they correctly pointed out that the kernel in your instance is encountering an out of memory (OOM) condition and made suggestions about how to adjust the configuration within your instance to avoid the OOM processor killer from kicking in.&lt;/p&gt;&lt;p&gt;The update that is being applied to instances that have scheduled reboot maintenance can cause slight changes to system resources available to paravirtualized instances, including a small reduction in usable memory. This can cause smaller instances, like m1.medium, that run workloads that were previously just fitting within the usable memory available to the instance to trigger out of memory conditions. Adding a swap file (as no swap is configured in your instance) or reducing the number of processes may resolve the issue on your existing PV instance.&lt;/p&gt;&lt;p&gt;Replacing your instance with one started from a HVM AMI will provide more system resources than PV instances. In either case (adjusting your configuration or moving to HVM), you should be able to run your existing workload on a m1.medium instance if you do not want to change to a different size.&lt;/p&gt;&lt;p&gt;I'm sorry that the additional information that would have provided a better explanation for the recommendations made in the case was not originally included, and that this important update is requiring additional effort beyond the reboot for your workload and configuration.&lt;/p&gt;&lt;p&gt;Kind regards,&lt;/p&gt;&lt;p&gt;Matt&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jive-message-list&quot;&gt;
&lt;div class=&quot;jive-table&quot;&gt;
&lt;div class=&quot;jive-messagebox&quot;&gt;
&lt;table summary=&quot;Message&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot; width=&quot;100%&quot; readability=&quot;7.3975276512687&quot;&gt;&lt;tr id=&quot;jive-message-821900&quot; valign=&quot;top&quot; readability=&quot;14.795055302537&quot;&gt;&lt;td class=&quot;jive-first&quot; valign=&quot;top&quot;&gt;
&lt;table class=&quot;jive-thread-messagebox&quot; width=&quot;100%&quot; readability=&quot;8.8770331815224&quot;&gt;&lt;tr readability=&quot;3.4782608695652&quot;&gt;&lt;td valign=&quot;top&quot; width=&quot;1%&quot;/&gt;
&lt;td width=&quot;100%&quot; valign=&quot;top&quot; readability=&quot;4.0263157894737&quot;&gt;&lt;span class=&quot;jive-subject&quot;&gt; Re: Degraded performance after forced reboot due to AWS instance maintenanc&lt;/span&gt;
&lt;div readability=&quot;6.3157894736842&quot;&gt;

&lt;p&gt;Posted on: &lt;span&gt;Dec 30, 2017 9:23 AM&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;td align=&quot;right&quot; valign=&quot;top&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; colspan=&quot;3&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;14&quot;&gt;&lt;td colspan=&quot;3&quot;&gt;
&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; width=&quot;100%&quot; class=&quot;jive-message-body-wrapper&quot; readability=&quot;32&quot;&gt;&lt;tr readability=&quot;14&quot;&gt;&lt;td&gt;
&lt;div class=&quot;jive-message-body&quot; readability=&quot;45&quot;&gt;
&lt;div readability=&quot;35&quot;&gt;I have now moved to an m3.medium instance which brought typical cpu loads down from about 50% (with many max-outs) to about 15%.&lt;p&gt;As there was no simple migration path from my m1.medium instance to HVI AMI I had to re-install server software from scratch, which is a lengthy process. I did not test m1.medium HVI AMI because I couldn't afford to waste time testing configurations. I just needed a solution that would allow my server to run reliably, henceforth, and based on your advice m3.medium seemed the safest bet.&lt;/p&gt;&lt;p&gt;I have put my m1.medium reserved instance up for sale for the recommended $470. However, given that it is an obsolete instance type I am not optimistic that it will sell.&lt;/p&gt;&lt;p&gt;Overall, in direct monetary terms I would estimate its going to cost me at least $1000 for the wasted reserved instance plus contractor time. And that figure doesn't include my own time and stress, nor the intangible loss of user confidence that came from the problems when my instance started maxing out in cpu.&lt;/p&gt;&lt;p&gt;So while I appreciate the &quot;better late than never&quot; advice that you gave after I posted the problem here, which has subsequently allowed me to find a solution &lt;strong&gt;at my own effort and expense&lt;/strong&gt;, given that the problem arose entirely due to AWS actions causing the service to degrade, I would expect something a bit more proportionate from you than just a verbal expression of regret.&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jive-message-list&quot;&gt;
&lt;div class=&quot;jive-table&quot;&gt;
&lt;div class=&quot;jive-messagebox&quot;&gt;
&lt;table summary=&quot;Message&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot; width=&quot;100%&quot; readability=&quot;3.2745098039216&quot;&gt;&lt;tr id=&quot;jive-message-822540&quot; valign=&quot;top&quot; readability=&quot;6.5490196078431&quot;&gt;&lt;td class=&quot;jive-first&quot; valign=&quot;top&quot;&gt;
&lt;table class=&quot;jive-thread-messagebox&quot; width=&quot;100%&quot; readability=&quot;3.7422969187675&quot;&gt;&lt;tr readability=&quot;3.4355828220859&quot;&gt;&lt;td valign=&quot;top&quot; width=&quot;1%&quot;/&gt;
&lt;td width=&quot;100%&quot; valign=&quot;top&quot; readability=&quot;3.974025974026&quot;&gt;&lt;span class=&quot;jive-subject&quot;&gt; Re: Degraded performance after forced reboot due to AWS instance maintenance&lt;/span&gt;
&lt;div readability=&quot;6.1298701298701&quot;&gt;

&lt;p&gt;Posted on: &lt;span&gt;Jan 4, 2018 4:33 AM&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;td align=&quot;right&quot; valign=&quot;top&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; colspan=&quot;3&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td colspan=&quot;3&quot;&gt;
&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; width=&quot;100%&quot; class=&quot;jive-message-body-wrapper&quot; readability=&quot;27&quot;&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td&gt;
&lt;div class=&quot;jive-message-body&quot; readability=&quot;33&quot;&gt;
&lt;div readability=&quot;11&quot;&gt;This just happened to us today on a c3.large. The cost to us to move the platform to new hardware and the lost confidence from our customers is huge.&lt;p&gt;Edited by: jbeaumont1 on Jan 4, 2018 4:34 AM&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jive-message-list&quot;&gt;
&lt;div class=&quot;jive-table&quot;&gt;
&lt;div class=&quot;jive-messagebox&quot;&gt;
&lt;table summary=&quot;Message&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot; width=&quot;100%&quot; readability=&quot;2.3069498069498&quot;&gt;&lt;tr id=&quot;jive-message-822633&quot; valign=&quot;top&quot; readability=&quot;4.6138996138996&quot;&gt;&lt;td class=&quot;jive-first&quot; valign=&quot;top&quot;&gt;
&lt;table class=&quot;jive-thread-messagebox&quot; width=&quot;100%&quot; readability=&quot;2.7683397683398&quot;&gt;&lt;tr readability=&quot;3.4968553459119&quot;&gt;&lt;td valign=&quot;top&quot; width=&quot;1%&quot;/&gt;
&lt;td width=&quot;100%&quot; valign=&quot;top&quot; readability=&quot;4.05&quot;&gt;&lt;span class=&quot;jive-subject&quot;&gt; Re: Degraded performance after forced reboot due to AWS instance maintenanc&lt;/span&gt;
&lt;div readability=&quot;6.3783783783784&quot;&gt;

&lt;p&gt;Posted on: &lt;span&gt;Jan 4, 2018 7:28 AM&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;td align=&quot;right&quot; valign=&quot;top&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; colspan=&quot;3&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td colspan=&quot;3&quot;&gt;
&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; width=&quot;100%&quot; class=&quot;jive-message-body-wrapper&quot; readability=&quot;26&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td readability=&quot;4&quot;&gt;
&lt;div class=&quot;jive-message-body&quot; readability=&quot;32&quot;&gt;
&lt;p&gt;M1 instances do not support HVM AMIs. Please show us an example of 1 that can be ran in any region.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jive-message-list&quot;&gt;
&lt;div class=&quot;jive-table&quot;&gt;
&lt;div class=&quot;jive-messagebox&quot;&gt;
&lt;table summary=&quot;Message&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot; width=&quot;100%&quot; readability=&quot;8.8312&quot;&gt;&lt;tr id=&quot;jive-message-822619&quot; valign=&quot;top&quot; readability=&quot;17.6624&quot;&gt;&lt;td class=&quot;jive-first&quot; valign=&quot;top&quot;&gt;
&lt;table class=&quot;jive-thread-messagebox&quot; width=&quot;100%&quot; readability=&quot;10.2256&quot;&gt;&lt;tr readability=&quot;3.4567901234568&quot;&gt;&lt;td valign=&quot;top&quot; width=&quot;1%&quot;/&gt;
&lt;td width=&quot;100%&quot; valign=&quot;top&quot; readability=&quot;4&quot;&gt;&lt;span class=&quot;jive-subject&quot;&gt; Re: Degraded performance after forced reboot due to AWS instance maintenance&lt;/span&gt;
&lt;div readability=&quot;6.2105263157895&quot;&gt;

&lt;p&gt;Posted on: &lt;span&gt;Jan 4, 2018 7:45 AM&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;td align=&quot;right&quot; valign=&quot;top&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; colspan=&quot;3&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;16.843457943925&quot;&gt;&lt;td colspan=&quot;3&quot;&gt;
&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; width=&quot;100%&quot; class=&quot;jive-message-body-wrapper&quot; readability=&quot;31.815420560748&quot;&gt;&lt;tr readability=&quot;16.843457943925&quot;&gt;&lt;td&gt;
&lt;div class=&quot;jive-message-body&quot; readability=&quot;42.108644859813&quot;&gt;
&lt;div readability=&quot;32.751168224299&quot;&gt;I'm not sure if you've been following the news (e.g., &lt;a class=&quot;jive-link-external&quot; href=&quot;http://fortune.com/2018/01/04/meltdown-spectre-intel-amd-arm-security-bug-apple-microsoft-google-apple-amazon/&quot;&gt;http://fortune.com/2018/01/04/meltdown-spectre-intel-amd-arm-security-bug-apple-microsoft-google-apple-amazon/&lt;/a&gt;) but this is the result of an industry-wide security crisis. It is as close to computing Armageddon as we've come. It isn't something of AWS' doing, or of them having a choice in the matter. All cloud providers were rushing to patch their infrastructure before the vulnerabilities were disclosed. I know people who pretty much haven't seen their families since Thanksgiving as they worked to patch systems.&lt;p&gt;Now that the issues were prematurely leaked, software suppliers are rushing to patch operating systems and other software (e.g., browsers). Microsoft had to accelerate release of fixes for Windows by a week. Patches for the Linux kernel are rolling out. I believe OS X has a Meltdown fix out as well. Google has Android and Chrome OS updates, and is recommending users turn on an experimental feature in the Chrome browser as a mitigation for Spectre. You are going to want to deploy those on your VMs, bare metal servers, personal computers, tablets, and phones asap. It's a real mess.&lt;/p&gt;&lt;p&gt;These are Intel hardware bugs and Intel/AMD/ARM architectural issues that allow information leak across supposedly protected boundaries, including between virtual machines. They had to be addressed, and the mitigations have performance impact. The impact is worse for some workloads, and for older (pre-2010) processors and Paravirtualization. So some people will have no work to do other than install patches, while others will have real work to do (or make other cost tradeoffs) to recover performance lost by having to workaround these hardware issues.&lt;/p&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;jive-message-list&quot;&gt;
&lt;div class=&quot;jive-table&quot;&gt;
&lt;div class=&quot;jive-messagebox&quot;&gt;
&lt;table summary=&quot;Message&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; border=&quot;0&quot; width=&quot;100%&quot; readability=&quot;4.375&quot;&gt;&lt;tr id=&quot;jive-message-822603&quot; valign=&quot;top&quot; readability=&quot;8.75&quot;&gt;&lt;td class=&quot;jive-first&quot; valign=&quot;top&quot;&gt;
&lt;table class=&quot;jive-thread-messagebox&quot; width=&quot;100%&quot; readability=&quot;5.8333333333333&quot;&gt;&lt;tr readability=&quot;3.5&quot;&gt;&lt;td valign=&quot;top&quot; width=&quot;1%&quot;/&gt;
&lt;td width=&quot;100%&quot; valign=&quot;top&quot; readability=&quot;4.0529801324503&quot;&gt;&lt;span class=&quot;jive-subject&quot;&gt; Re: Degraded performance after forced reboot due to AWS instance maintenance&lt;/span&gt;
&lt;div readability=&quot;6.3783783783784&quot;&gt;

&lt;p&gt;Posted on: &lt;span&gt;Jan 4, 2018 7:56 AM&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;td align=&quot;right&quot; valign=&quot;top&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; colspan=&quot;3&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;8&quot;&gt;&lt;td colspan=&quot;3&quot;&gt;
&lt;table border=&quot;0&quot; cellpadding=&quot;0&quot; cellspacing=&quot;0&quot; width=&quot;100%&quot; class=&quot;jive-message-body-wrapper&quot; readability=&quot;29&quot;&gt;&lt;tr readability=&quot;8&quot;&gt;&lt;td readability=&quot;7&quot;&gt;
&lt;div class=&quot;jive-message-body&quot; readability=&quot;38&quot;&gt;
&lt;p&gt;Yes, the news of cpu security issues is finally out now. However you seem to be missing the point that AWS knew that implementing these updates would cause loss of capability to some customers (like myself and others in this thread) and yet failed to either 1) notify us in advance of possible problems, so that we could take pre-emptive action to avoid damage to our own business, nor 2) provide any other mitigating workaround (like pre-emptively upgrading capability for affected customers) which could have prevented business damage to affected customers.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;&lt;td valign=&quot;top&quot; width=&quot;1%&quot;&gt;
&lt;div id=&quot;sideNav&quot;&gt;
&lt;div id=&quot;subNavB&quot;&gt;
&lt;div class=&quot;decoratedBox&quot;&gt;
&lt;p&gt;
&lt;h4&gt;Available Actions&lt;/h4&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&quot;sideNav&quot;&gt;
&lt;div id=&quot;subNavB&quot;&gt;
&lt;div class=&quot;decoratedBox&quot;&gt;
&lt;p&gt;
&lt;h4&gt;Icon Legend&lt;/h4&gt;
&lt;/p&gt;
&lt;div class=&quot;body&quot;&gt;
&lt;table readability=&quot;1&quot;&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; class=&quot;jive-icon&quot;&gt;&lt;img src=&quot;https://forums.aws.amazon.com/images/question-resolved-16x16.gif&quot; width=&quot;16&quot; height=&quot;16&quot; border=&quot;0&quot; alt=&quot;&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;jive-icon-label&quot;&gt;Answered question&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td valign=&quot;top&quot; class=&quot;jive-icon&quot;&gt;&lt;img src=&quot;https://forums.aws.amazon.com/images/question-pts-available-16x16.gif&quot; width=&quot;16&quot; height=&quot;16&quot; border=&quot;0&quot; alt=&quot;&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;jive-icon-label&quot;&gt;Unanswered question with answer points still available&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td valign=&quot;top&quot; class=&quot;jive-icon&quot;&gt;&lt;img src=&quot;https://forums.aws.amazon.com/images/question-16x16.gif&quot; width=&quot;16&quot; height=&quot;16&quot; border=&quot;0&quot; alt=&quot;&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;jive-icon-label&quot;&gt;Unanswered question&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/td&gt;
</description>
<pubDate>Wed, 03 Jan 2018 20:13:12 +0000</pubDate>
<dc:creator>woliveirajr</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://forums.aws.amazon.com/thread.jspa?threadID=269858</dc:identifier>
</item>
<item>
<title>Intel Responds to Security Research Findings</title>
<link>https://newsroom.intel.com/news/intel-responds-to-security-research-findings/</link>
<guid isPermaLink="true" >https://newsroom.intel.com/news/intel-responds-to-security-research-findings/</guid>
<description>&lt;p&gt;Intel and other technology companies have been made aware of new security research describing software analysis methods that, when used for malicious purposes, have the potential to improperly gather sensitive data from computing devices that are operating as designed. Intel believes these exploits do not have the potential to corrupt, modify or delete data.&lt;/p&gt;
&lt;p&gt;Recent reports that these exploits are caused by a “bug” or a “flaw” and are unique to Intel products are incorrect. Based on the analysis to date, many types of computing devices — with many different vendors’ processors and operating systems — are susceptible to these exploits.&lt;/p&gt;
&lt;p&gt;Intel is committed to product and customer security and is working closely with many other technology companies, including AMD, ARM Holdings and several operating system vendors, to develop an industry-wide approach to resolve this issue promptly and constructively. Intel has begun providing software and firmware updates to mitigate these exploits. Contrary to some reports, any performance impacts are workload-dependent, and, for the average computer user, should not be significant and will be mitigated over time.&lt;/p&gt;
&lt;p&gt;Intel is committed to the industry best practice of responsible disclosure of potential security issues, which is why Intel and other vendors had planned to disclose this issue next week when more software and firmware updates will be available. However, Intel is making this statement today because of the current inaccurate media reports.&lt;/p&gt;
&lt;p&gt;Check with your operating system vendor or system manufacturer and apply any available updates as soon as they are available. Following good security practices that protect against malware in general will also help protect against possible exploitation until updates can be applied.&lt;/p&gt;
&lt;p&gt;Intel believes its products are the most secure in the world and that, with the support of its partners, the current solutions to this issue provide the best possible security for its customers.&lt;/p&gt;
</description>
<pubDate>Wed, 03 Jan 2018 20:06:10 +0000</pubDate>
<dc:creator>runesoerensen</dc:creator>
<og:type>article</og:type>
<og:title>Intel Responds to Security Research Findings</og:title>
<og:description>Intel Corporation and other technology companies have been made aware of new security research describing software analysis methods that, when used for malicious purposes, have the potential to improperly gather sensitive data from computing devices that are operating as designed. Intel believes these exploits do not have the potential to corrupt, modify or delete data.</og:description>
<og:url>https://newsroom.intel.com/news/intel-responds-to-security-research-findings/</og:url>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://newsroom.intel.com/news/intel-responds-to-security-research-findings/</dc:identifier>
</item>
<item>
<title>GIMPS Project Discovers Largest Known Prime Number</title>
<link>https://www.mersenne.org/primes/press/M77232917.html</link>
<guid isPermaLink="true" >https://www.mersenne.org/primes/press/M77232917.html</guid>
<description>&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt;&lt;meta name=&quot;description&quot; content=&quot;Discovery of the 50th known Mersenne Prime.&quot; /&gt;&lt;meta name=&quot;keywords&quot; content=&quot;World Record Largest Mersenne Prime Number&quot; /&gt;&lt;meta name=&quot;GENERATOR&quot; content=&quot;Microsoft FrontPage 6.0&quot; /&gt;&lt;title&gt;50th Known Mersenne Prime Discovered&lt;/title&gt;&lt;h2 align=&quot;center&quot;&gt;&lt;a href=&quot;http://www.mersenne.org/&quot;&gt;GIMPS&lt;/a&gt; Project Discovers&lt;br /&gt;Largest Known Prime Number: 2&lt;span&gt;&lt;sup&gt;77,232,917&lt;/sup&gt;&lt;/span&gt;-1&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;RALEIGH, NC., January 3, 2018&lt;/strong&gt; -- The &lt;a href=&quot;http://mersenne.org&quot;&gt;Great Internet Mersenne Prime Search (GIMPS)&lt;/a&gt; has discovered the largest known prime number, 2&lt;sup&gt;77,232,917&lt;/sup&gt;-1, having &lt;a href=&quot;http://www.mersenne.org/primes/digits/M77232917.zip&quot;&gt;23,249,425 digits&lt;/a&gt;. A computer volunteered by Jonathan Pace made the find on December 26, 2017. Jonathan is one of thousands of volunteers using free GIMPS software available at &lt;a href=&quot;http://www.mersenne.org/download/&quot;&gt;www.mersenne.org/download/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The new prime number, also known as M77232917, is calculated by multiplying together 77,232,917 twos, and then subtracting one. It is nearly one million digits larger than the &lt;a href=&quot;http://www.mersenne.org/primes/?press=M74207281&quot;&gt;previous record prime number&lt;/a&gt;, in a special class of extremely rare prime numbers known as Mersenne primes. It is only the 50th known Mersenne prime ever discovered, each increasingly difficult to find. Mersenne primes were named for the French monk &lt;a href=&quot;http://www-groups.dcs.st-and.ac.uk/~history/Mathematicians/Mersenne.html&quot;&gt;Marin Mersenne&lt;/a&gt;, who studied these numbers more than 350 years ago. GIMPS, founded in 1996, has discovered the last 16 Mersenne primes. Volunteers &lt;a href=&quot;http://www.mersenne.org/download/&quot;&gt;download a free program&lt;/a&gt; to search for these primes, with a cash award offered to anyone lucky enough to find a new prime. Prof. Chris Caldwell maintains an authoritative web site on &lt;a href=&quot;http://www.utm.edu/research/primes/largest.html&quot;&gt;the largest known primes&lt;/a&gt;, and has an excellent &lt;a href=&quot;http://primes.utm.edu/mersenne/index.html&quot;&gt;history of Mersenne primes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The primality proof took six days of non-stop computing on a PC with an Intel i5-6600 CPU. To prove there were no errors in the prime discovery process, the new prime was independently verified using four different programs on four different hardware configurations.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Aaron Blosser verified it using &lt;a href=&quot;http://www.mersenne.org/download/&quot;&gt;Prime95&lt;/a&gt; on an Intel Xeon server in 37 hours.&lt;/li&gt;
&lt;li&gt;David Stanfill verified it using &lt;a href=&quot;http://www.mersenneforum.org/showthread.php?t=22204&quot;&gt;gpuOwL&lt;/a&gt; on an AMD RX Vega 64 GPU in 34 hours.&lt;/li&gt;
&lt;li&gt;Andreas Höglund verified the prime using &lt;a href=&quot;http://www.mersenneforum.org/showthread.php?t=16142&quot;&gt;CUDALucas&lt;/a&gt; running on NVidia Titan Black GPU in 73 hours.&lt;/li&gt;
&lt;li&gt;Ernst Mayer also verified it using his own program &lt;a href=&quot;http://www.mersenneforum.org/mayer/README.html&quot;&gt;Mlucas&lt;/a&gt; on 32-core Xeon server in 82 hours. Andreas Höglund also confirmed using Mlucas running on an Amazon AWS instance in 65 hours.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Jonathan Pace is a 51-year old Electrical Engineer living in Germantown, Tennessee. Perseverance has finally paid off for Jon - he has been hunting for big primes with GIMPS for over 14 years. The discovery is eligible for a $3,000 GIMPS research discovery award.&lt;/p&gt;
&lt;p&gt;GIMPS Prime95 client software was developed by founder George Woltman. Scott Kurowski wrote the PrimeNet system software that coordinates GIMPS' computers. Aaron Blosser is now the system administrator, upgrading and maintaining PrimeNet as needed. Volunteers have a chance to earn &lt;a href=&quot;http://www.mersenne.org/legal/#awards&quot;&gt;research discovery awards of $3,000 or $50,000&lt;/a&gt; if their computer discovers a new Mersenne prime. GIMPS' next major goal is to win the &lt;a href=&quot;http://www.eff.org/awards/coop-prime-info.php&quot;&gt;$150,000 award administered by the Electronic Frontier Foundation&lt;/a&gt; offered for finding a 100 million digit prime number.&lt;/p&gt;
&lt;p&gt;Credit for this prime goes not only to Jonathan Pace for running the Prime95 software, Woltman for writing the software, Kurowski and Blosser for their work on the Primenet server, but also the thousands of GIMPS volunteers that sifted through millions of non-prime candidates.  In recognition of all the above people, official credit for this discovery goes to &quot;J. Pace, G. Woltman, S. Kurowski, A. Blosser, et al.&quot;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;About Mersenne.org's Great Internet Mersenne Prime Search&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;http://www.mersenne.org/&quot;&gt;Great Internet Mersenne Prime Search (GIMPS)&lt;/a&gt; was formed in January 1996 by George Woltman to discover new world record size Mersenne primes. In 1997 Scott Kurowski enabled GIMPS to automatically harness the power of thousands of ordinary computers to search for these &quot;needles in a haystack&quot;. Most GIMPS members join the search for the thrill of possibly discovering a record-setting, rare, and historic new Mersenne prime. The search for more Mersenne primes is already under way. There may be smaller, as yet undiscovered Mersenne primes, and there almost certainly are larger Mersenne primes waiting to be found. Anyone with a reasonably powerful PC can join GIMPS and become a big prime hunter, and possibly earn a cash research discovery award. All the necessary software can be downloaded for free at &lt;a href=&quot;http://www.mersenne.org/download/&quot;&gt;www.mersenne.org/download/&lt;/a&gt;. GIMPS is organized as Mersenne Research, Inc., a 501(c)(3) science research charity. Additional information may be found at &lt;a href=&quot;http://www.mersenneforum.org/&quot;&gt;www.mersenneforum.org&lt;/a&gt; and &lt;a href=&quot;http://www.mersenne.org&quot;&gt;www.mersenne.org&lt;/a&gt;; donations are welcome.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For More Information on Mersenne Primes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Prime numbers have long fascinated both amateur and professional mathematicians. An integer greater than one is called a prime number if its only divisors are one and itself. The first prime numbers are 2, 3, 5, 7, 11, etc. For example, the number 10 is not prime because it is divisible by 2 and 5. A Mersenne prime is a prime number of the form 2&lt;sup&gt;P&lt;/sup&gt;-1. The first Mersenne primes are 3, 7, 31, and 127 corresponding to P = 2, 3, 5, and 7 respectively. There are now 50 known Mersenne primes.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.utm.edu/research/primes/mersenne.shtml&quot;&gt;Mersenne primes&lt;/a&gt; have been central to number theory since they were first discussed by Euclid about 350 BC. The man whose name they now bear, the French monk &lt;a href=&quot;http://www-groups.dcs.st-and.ac.uk/~history/Mathematicians/Mersenne.html&quot;&gt;Marin Mersenne&lt;/a&gt; (1588-1648), made a famous conjecture on which values of P would yield a prime. It took 300 years and several important discoveries in mathematics to settle his conjecture.&lt;/p&gt;
&lt;p&gt;At present there are few practical uses for this new large prime, prompting some to ask &quot;why search for these large primes&quot;? Those same doubts existed a few decades ago until important cryptography algorithms were developed based on prime numbers. For seven more good reasons to search for large prime numbers, &lt;a href=&quot;http://primes.utm.edu/notes/faq/why.html&quot;&gt;see here.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Previous GIMPS Mersenne prime discoveries were made by members in various countries.&lt;br /&gt;   In January 2016, Curtis Cooper et al. discovered the &lt;a href=&quot;http://www.mersenne.org/74207281.htm&quot;&gt;49th known Mersenne prime&lt;/a&gt; in the U.S.&lt;br /&gt;   In January 2013, Curtis Cooper et al. discovered the &lt;a href=&quot;http://www.mersenne.org/57885161.htm&quot;&gt;48th known Mersenne prime&lt;/a&gt; in the U.S.&lt;br /&gt;   In April 2009, Odd Magnar Strindmo et al. discovered the &lt;a href=&quot;http://www.mersenne.org/various/42643801.htm&quot;&gt;47th known Mersenne prime&lt;/a&gt; in Norway.&lt;br /&gt;   In September 2008, Hans-Michael Elvenich et al. discovered the &lt;a href=&quot;http://www.mersenne.org/m45and46.htm&quot;&gt;46th known Mersenne prime&lt;/a&gt; in Germany.&lt;br /&gt;   In August 2008, Edson Smith et al. discovered the &lt;a href=&quot;http://www.mersenne.org/m45and46.htm&quot;&gt;45th known Mersenne prime&lt;/a&gt; in the U.S.&lt;br /&gt;   In September 2006, Curtis Cooper and Steven Boone et al. discovered the &lt;a href=&quot;http://www.mersenne.org/various/32582657.htm&quot;&gt;44th known Mersenne prime&lt;/a&gt; in the U.S.&lt;br /&gt;   In December 2005, Curtis Cooper and Steven Boone et al. discovered the &lt;a href=&quot;http://www.mersenne.org/various/30402457.htm&quot;&gt;43rd known Mersenne prime&lt;/a&gt; in the U.S.&lt;br /&gt;   In February 2005, Dr. Martin Nowak et al. discovered the &lt;a href=&quot;http://www.mersenne.org/various/25964951.htm&quot;&gt;42nd known Mersenne prime&lt;/a&gt; in Germany.&lt;br /&gt;   In May 2004, Josh Findley et al. discovered the &lt;a href=&quot;http://www.mersenne.org/various/24036583.htm&quot;&gt;41st known Mersenne prime&lt;/a&gt; in the U.S.&lt;br /&gt;   In November 2003, Michael Shafer et al. discovered the &lt;a href=&quot;http://www.mersenne.org/various/20996011.htm&quot;&gt;40th known Mersenne prime&lt;/a&gt; in the U.S.&lt;br /&gt;   In November 2001, Michael Cameron et al. discovered the &lt;a href=&quot;http://www.mersenne.org/various/13466917.htm&quot;&gt;39th Mersenne prime&lt;/a&gt; in Canada.&lt;br /&gt;   In June 1999, Nayan Hajratwala et al. discovered the &lt;a href=&quot;http://www.mersenne.org/various/6972593.htm&quot;&gt;38th Mersenne prime&lt;/a&gt; in the U.S.&lt;br /&gt;   In January 1998, Roland Clarkson et al. discovered the &lt;a href=&quot;http://www.mersenne.org/various/3021377.htm&quot;&gt;37th Mersenne prime&lt;/a&gt; in the U.S.&lt;br /&gt;   In August 1997, Gordon Spence et al. discovered the &lt;a href=&quot;http://www.mersenne.org/various/2976221.htm&quot;&gt;36th Mersenne prime&lt;/a&gt; in the U.K.&lt;br /&gt;   In November 1996, Joel Armengaud et al. discovered the &lt;a href=&quot;http://www.mersenne.org/various/1398269.htm&quot;&gt;35th Mersenne prime&lt;/a&gt; in France.&lt;/p&gt;
&lt;p&gt;Euclid proved that every Mersenne prime generates a perfect number. A perfect number is one whose proper divisors add up to the number itself. The smallest perfect number is 6 = 1 + 2 + 3 and the second perfect number is 28 = 1 + 2 + 4 + 7 + 14. Euler (1707-1783) proved that all even perfect numbers come from Mersenne primes. The newly discovered perfect number is 2&lt;sup&gt;77,232,916&lt;/sup&gt; x (2&lt;sup&gt;77,232,917&lt;/sup&gt;-1). This number is over &lt;a href=&quot;http://www.mersenne.org/primes/perfect/perfect77232917.zip&quot;&gt;46 million digits&lt;/a&gt; long! It is still unknown if any odd perfect numbers exist.&lt;/p&gt;
&lt;p&gt;There is a unique history to the arithmetic algorithms underlying the GIMPS project. The programs that found the recent big Mersenne primes are based on a special algorithm. In the early 1990's, the late &lt;a href=&quot;http://en.wikipedia.org/wiki/Richard_Crandall&quot;&gt;Richard Crandall&lt;/a&gt;, Apple Distinguished Scientist, discovered ways to double the speed of what are called convolutions -- essentially big multiplication operations. The method is applicable not only to prime searching but other aspects of computation. During that work he also patented the Fast Elliptic Encryption system, now owned by Apple Computer, which uses Mersenne primes to quickly encrypt and decrypt messages. George Woltman implemented Crandall's algorithm in assembly language, thereby producing a prime-search program of unprecedented efficiency, and that work led to the successful GIMPS project.&lt;/p&gt;
&lt;p&gt;School teachers from elementary through high-school grades have used GIMPS to get their students excited about mathematics. Students who run the free software are contributing to mathematical research. David Stanfill's and Ernst Mayer's verification computations for this discovery was donated by &lt;span&gt;Squirrels LLC (&lt;a href=&quot;http://www.airsquirrels.com&quot;&gt;http://www.airsquirrels.com&lt;/a&gt;) which services K-12 education and other customers&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;em&gt;Science (American Association for the Advancement of Science), May 6, 2005 p810.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;
</description>
<pubDate>Wed, 03 Jan 2018 19:01:58 +0000</pubDate>
<dc:creator>seycombi</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.mersenne.org/primes/press/M77232917.html</dc:identifier>
</item>
</channel>
</rss>