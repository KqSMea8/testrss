<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>NSA Deletes “Honesty” and “Openness” from Core Values</title>
<link>https://theintercept.com/2018/01/24/nsa-core-values-honesty-deleted/</link>
<guid isPermaLink="true" >https://theintercept.com/2018/01/24/nsa-core-values-honesty-deleted/</guid>
<description>&lt;p&gt;&lt;u&gt;The National Security Agency&lt;/u&gt; maintains a page on its website that outlines its mission statement. But earlier this month, the agency made a discreet change: It removed “honesty” as its top priority.&lt;/p&gt;
&lt;p&gt;Since at least May 2016, the surveillance agency had featured honesty as the first of four “core values” listed on NSA.gov, alongside “respect for the law,” “integrity,” and “transparency.” The agency vowed on the site to “be truthful with each other.”&lt;/p&gt;
&lt;p&gt;On January 12, however, the NSA removed the mission statement page – which can &lt;a href=&quot;https://web.archive.org/web/20171214073007/https://www.nsa.gov/about/mission-strategy/&quot;&gt;still be viewed&lt;/a&gt; through the Internet Archive – and replaced it with &lt;a href=&quot;https://www.nsa.gov/about/mission-values/&quot;&gt;a new version&lt;/a&gt;. Now, the parts about honesty and the pledge to be truthful have been deleted. The agency’s new top value is “commitment to service,” which it says means “excellence in the pursuit of our critical mission.”&lt;/p&gt;
&lt;p&gt;Those are not the only striking alterations. In its old core values, the NSA explained that it would strive to be deserving of the “great trust” placed in it by national leaders and American citizens. It said that it would “honor the public’s need for openness.” But those phrases are now gone; all references to “trust,” “honor,” and “openness” have disappeared.&lt;/p&gt;
&lt;p&gt;The agency previously stated on its website that it embraced transparency and claimed that all of its activities were aimed at “ensuring the safety, security, and liberty of our fellow citizens.” That has also been discarded. The agency still says it is committed to transparency on the updated website, but the transparency is now described as being for the benefit of “those who authorize and oversee NSA’s work on behalf of the American people.” The definition of “integrity” has been edited, too. The agency formerly said its commitment to integrity meant it would “behave honorably and apply good judgment.” The phrase “behave honorably” has now been dropped in favor of “communicating honestly and directly, acting ethically and fairly and carrying out our mission efficiently and effectively.”&lt;/p&gt;
&lt;p&gt;The new list of values includes the additions “respect for people” and “accountability.” But the section on respecting people is a reference to diversity within the NSA workforce, not a general commitment to members of the public. Accountability is defined as taking “responsibility for our decisions.” The one core value that remains essentially unchanged is “respect for the law,” which the agency says means it is “grounded in our adherence to the U.S. Constitution and compliance with the U.S. laws, regulations and policies that govern our activities.”&lt;/p&gt;
&lt;p&gt;In response to questions from The Intercept on Tuesday, the NSA played down the alterations. Thomas Groves, a spokesperson for the agency, said: “It’s nothing more than a website update, that’s all it is.”&lt;/p&gt;
&lt;p class=&quot;caption&quot;&gt;Top photo: Adm. Mike Rogers, director of the National Security Agency, testifies about the Fiscal Year 2018 budget request for U.S. Cyber Command during a House Armed Services Committee hearing on Capitol Hill in Washington, D.C., on May 23, 2017.&lt;/p&gt;
</description>
<pubDate>Wed, 24 Jan 2018 12:51:53 +0000</pubDate>
<dc:creator>etiam</dc:creator>
<og:url>https://theintercept.com/2018/01/24/nsa-core-values-honesty-deleted/</og:url>
<og:description>The spy agency has quietly altered the mission statement on its website, removing a series of commitments.</og:description>
<og:image>https://cdn01.theintercept.com/wp-uploads/sites/1/2018/01/NSA-michael-roger-1516652208-feature-hero.jpg</og:image>
<og:type>article</og:type>
<og:title>NSA Deletes “Honesty” and “Openness” From Core Values</og:title>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://theintercept.com/2018/01/24/nsa-core-values-honesty-deleted/</dc:identifier>
</item>
<item>
<title>EU fines Qualcomm $1.2B over LTE chip dominance in the iPhone</title>
<link>https://techcrunch.com/2018/01/24/eu-slaps-1-23b-fine-on-qualcomm-over-lte-chip-dominance-in-the-iphone/</link>
<guid isPermaLink="true" >https://techcrunch.com/2018/01/24/eu-slaps-1-23b-fine-on-qualcomm-over-lte-chip-dominance-in-the-iphone/</guid>
<description>&lt;img src=&quot;https://tctechcrunch2011.files.wordpress.com/2017/09/iphone-x-tweet-newsletter.jpg?w=640&quot; class=&quot;&quot;/&gt;&lt;p id=&quot;speakable-summary&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;http://qualcomm.com&quot; rel=&quot;noopener&quot;&gt;Qualcomm’s&lt;/a&gt; longstanding dominance in LTE chipsets for smartphones, and specifically with Apple’s iPhone, is getting a major hit today.&lt;/p&gt;&lt;p&gt;The European Commission today &lt;a target=&quot;_blank&quot; href=&quot;http://europa.eu/rapid/press-release_IP-18-421_en.htm&quot; rel=&quot;noopener&quot;&gt;announced&lt;/a&gt; that it would be fining the company €997 million, or $1.23 billion, for abusing its market position between 2011 and 2016, related to its relationship with Apple. The figure works out to 4.9 percent of Qualcomm’s revenues in 2017. (And the 4.9 percent take was worked out based on the five+ year period of violation.)&lt;/p&gt;
&lt;blockquote readability=&quot;19&quot;&gt;
&lt;p&gt;“Qualcomm illegally shut out rivals from the market for LTE baseband chipsets for over five years, thereby cementing its market dominance,” said Competition Commissioner Margrethe Vestager, in a statement. “Qualcomm paid billions of US Dollars to a key customer, Apple, so that it would not buy from rivals. These payments were not just reductions in price — they were made on the condition that Apple would exclusively use Qualcomm’s baseband chipsets in all its iPhones and iPads.&lt;/p&gt;
&lt;p&gt;“This meant that no rival could effectively challenge Qualcomm in this market, no matter how good their products were. Qualcomm’s behaviour denied consumers and other companies more choice and innovation – and this in a sector with a huge demand and potential for innovative technologies. This is illegal under EU antitrust rules and why we have taken today’s decision.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Qualcomm says it will appeal the case and that the decision “does not relate to Qualcomm’s licensing business and has no impact on ongoing operations”:&lt;/p&gt;
&lt;blockquote readability=&quot;11&quot;&gt;
&lt;p&gt;“We are confident this agreement did not violate EU competition rules or adversely affect market competition or European consumers,” said Don Rosenberg, executive vice president and general counsel of Qualcomm, in a statement provided to TechCrunch. “We have a strong case for judicial review and we will immediately commence that process.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Going forward, the big things to watch for are whether Qualcomm chooses to appeal this decision, and how it might affect the company’s current deal with Broadcom, which is trying to buy Qualcomm &lt;a target=&quot;_blank&quot; href=&quot;http://investors.broadcom.com/phoenix.zhtml?c=203541&amp;amp;p=irol-newsArticle&amp;amp;ID=2314458&quot; rel=&quot;noopener&quot;&gt;for $130 billon&lt;/a&gt;. Reportedly, partners like Microsoft and Google are &lt;a target=&quot;_blank&quot; href=&quot;http://www.cnbc.com/2017/12/08/microsoft-google-concerned-about-broadcom-qualcomm-deal.html?__source=twitter%7Cmain&quot; rel=&quot;noopener&quot;&gt;against&lt;/a&gt; the deal.&lt;/p&gt;
&lt;p&gt;Intel, which was one of the key losers in that arrangement, has been scrambling to &lt;a target=&quot;_blank&quot; href=&quot;https://venturebeat.com/2015/10/16/intel-has-1000-people-working-on-chips-for-the-iphone/&quot; rel=&quot;noopener&quot;&gt;catch up for years&lt;/a&gt;. Now, the iPhone is made using chipsets from both vendors, but all is not completely resolved.&lt;/p&gt;
&lt;p&gt;There may be more than simple anticompetitive practices at issue here, since &lt;a target=&quot;_blank&quot; href=&quot;https://www.macrumors.com/2017/12/01/qualcomm-iphone-x-still-faster-than-intel/&quot; rel=&quot;noopener&quot;&gt;some tests appear to show&lt;/a&gt; that iPhones running on Intel’s chips are performing slower than their Qualcomm-based counterparts. This could prove to be grounds for an appeal down the line.&lt;/p&gt;
&lt;p&gt;And to add a further twist to this story, there may be a case for Qualcomm to appeal its own fine based on another regulatory case involving — yes — that same Intel. The latter company last September had a &lt;a target=&quot;_blank&quot; href=&quot;https://www.bloomberg.com/news/articles/2017-09-06/intel-gets-boost-in-1-26-billion-fight-as-case-sent-for-review&quot; rel=&quot;noopener&quot;&gt;vote in its favor&lt;/a&gt; in its (so far) nine-year-long battle to appeal its own $1 billion antitrust fine from the EU, when the &lt;a target=&quot;_blank&quot; href=&quot;https://techcrunch.com/2017/09/06/intel-antitrust-decision-sent-for-review-by-europes-top-court/&quot; rel=&quot;noopener&quot;&gt;European Courts of Justice&lt;/a&gt; said that the Commission needed to provide more proof in its case against the company.&lt;/p&gt;
&lt;p&gt;“The Commission seems to have followed a ‘creative reading’ of the Intel ruling, remaining consistent with it while allowing some leeway on interpretation,” said Assimakis Komninos, a partner at global law firm White &amp;amp; Case, in a statement. “It will be interesting to see how the EU courts will react.”&lt;/p&gt;
&lt;p&gt;Indeed, Qualcomm, which says that it “strongly disagrees with the decision,” said it will be making its appeal to the General Court of the European Union “immediately.”&lt;/p&gt;
&lt;p&gt;We are reaching out to both companies for comment and will update this post as and if we get it.&lt;/p&gt;
&lt;p&gt;The baseband chipsets at the center of this case are what enable smartphones to connect to cellular networks for voice and data services.&lt;/p&gt;

&lt;p&gt;Qualcomm, as the EU points out, is the world’s biggest LTE supplier, making it a target for regulators looking into those who abuse their market dominance.&lt;/p&gt;
&lt;p&gt;This is not the first time that Qualcomm has been under regulatory scrutiny. The company faced an &lt;a target=&quot;_blank&quot; href=&quot;https://www.reuters.com/article/us-qualcomm-antitrust/south-korea-fines-qualcomm-854-million-for-violating-competition-laws-idUSKBN14H062&quot; rel=&quot;noopener&quot;&gt;$854 million fine&lt;/a&gt; in South Korea over anticompetitive practices, forcing vendors to pay royalties on its patents.&lt;/p&gt;
&lt;p&gt;And vendor financing, which is the practice by which companies essentially pay customers to take on big orders of their equipment, is not a new thing — it was particularly strong in the late Nineties and early 00’s when telecoms networks were rapidly updating legacy infrastructure and building new mobile and digital networks.&lt;/p&gt;
&lt;p&gt;Qualcomm itself had a strong game in it (as you can see in &lt;a target=&quot;_blank&quot; href=&quot;https://books.google.co.uk/books?id=JcH4C2eAsJEC&amp;amp;pg=PA128&amp;amp;lpg=PA128&amp;amp;dq=qualcomm+%27vendor+finance%27&amp;amp;source=bl&amp;amp;ots=ANUTeDm6_F&amp;amp;sig=BgRTj7cMfdgRVzrCt7QmxLniE84&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ved=0ahUKEwjbpeL5wvDYAhXRasAKHa50DgQQ6AEILDAB#v=onepage&amp;amp;q=qualcomm%20'vendor%20finance'&amp;amp;f=false&quot; rel=&quot;noopener&quot;&gt;this excerpt&lt;/a&gt; from the book &lt;em&gt;The Qualcomm Equation&lt;/em&gt;), and prior to this, Qualcomm was also &lt;a target=&quot;_blank&quot; href=&quot;https://www.wsj.com/articles/SB124834616769075465&quot; rel=&quot;noopener&quot;&gt;fined in Korea to the tune of $208 million&lt;/a&gt; over its CDMA dominance back in 2009.&lt;/p&gt;
&lt;p&gt;Qualcomm’s deal with Apple, in which it committed payments to Apple to secure exclusivity, started in 2011 and then were renewed in 2013. The investigation by the EU started in 2015 out of the Commission’s own examination of market information, including conditions in contracts between Qualcomm and its customers that appeared to harm competition.&lt;/p&gt;
&lt;p&gt;“The agreement made clear that Qualcomm would cease these payments, if Apple commercially launched a device with a chipset supplied by a rival,” the EC notes in its statement. “Furthermore, for most of the time the agreement was in place, Apple would have had to return to Qualcomm a large part of the payments it had received in the past, if it decided to switch suppliers. This meant that Qualcomm’s rivals were denied the possibility to compete effectively for Apple’s significant business, no matter how good their products were. They were also denied business opportunities with other customers that could have followed from securing Apple as a customer.&lt;/p&gt;
&lt;p&gt;“In fact, internal documents show that Apple gave serious consideration to switching part of its baseband chipset requirements to Intel. Qualcomm’s exclusivity condition was a material factor why Apple decided against doing so, until the agreement came to an end.&lt;/p&gt;
&lt;p&gt;“Then, in September 2016, when the agreement was about to expire and the cost of switching under its terms was limited, Apple started to source part of its baseband chipset requirements from Intel. But until then, Qualcomm’s practices denied consumers and other companies the benefits of effective competition, namely more choice and innovation.”&lt;/p&gt;
&lt;p&gt;We have confirmed that there were no formal complaints that led to the investigation, and that Apple, although one of the central players in this case, is not liable in this investigation.&lt;/p&gt;
&lt;p&gt;There is an &lt;a target=&quot;_blank&quot; href=&quot;https://www.fool.com/investing/2017/11/08/this-might-be-the-real-reason-apple-inc-wants-qual.aspx&quot; rel=&quot;noopener&quot;&gt;interesting op-ed here&lt;/a&gt; exploring why Apple might be moving away from Qualcomm, which isn’t directly relevant to the EU case, but underscores some of the challenges facing the company, should it prove accurate.&lt;/p&gt;
&lt;p&gt;Qualcomm is down 10 cents on its opening price this morning.&lt;/p&gt;
&lt;p&gt;We’ll update this post as we learn more.&lt;/p&gt;

</description>
<pubDate>Wed, 24 Jan 2018 11:31:17 +0000</pubDate>
<dc:creator>rbanffy</dc:creator>
<og:title>Qualcomm to appeal $1.23B fine from EU over LTE chip dominance in the iPhone</og:title>
<og:description>Qualcomm's longstanding dominance in LTE chipsets for smartphones, and specifically with Apple's iPhone, is getting a major hit today. The European..</og:description>
<og:image>https://tctechcrunch2011.files.wordpress.com/2017/09/iphone-x-tweet-newsletter.jpg</og:image>
<og:url>http://social.techcrunch.com/2018/01/24/eu-slaps-1-23b-fine-on-qualcomm-over-lte-chip-dominance-in-the-iphone/</og:url>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://techcrunch.com/2018/01/24/eu-slaps-1-23b-fine-on-qualcomm-over-lte-chip-dominance-in-the-iphone/</dc:identifier>
</item>
<item>
<title>Building a Lightroom PC</title>
<link>https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc</link>
<guid isPermaLink="true" >https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc</guid>
<description>&lt;header class=&quot;entry-title&quot;&gt;
&lt;h3 class=&quot;subtitle&quot;&gt;Why I switched to Windows and built a water-cooled 5.2GHz 6-core editing machine&lt;/h3&gt;
&lt;/header&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8402-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.4992503748125936&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8402-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;79.544835868695&quot;&gt;&lt;p class=&quot;larger&quot;&gt;If there's one thing that will make even the most powerful computer feel like a 7 year old rig, it's Adobe Lightroom paired with RAW files from any high-megapixel camera.&lt;/p&gt;
&lt;p&gt;In my case, I spent over a year of spare time editing 848GB worth of 11,000+ 42-megapixel RAW photos and 4K videos from my New Zealand trip and making &lt;a href=&quot;https://paulstamatiou.com/photos/new-zealand/&quot; title=&quot;New Zealand - Paul Stamatiou&quot;&gt;these nine photosets&lt;/a&gt;. I quickly realized that my two year old iMac was not up to the challenge.&lt;/p&gt;
&lt;p&gt;In 2015 I took a stab at solving my photo storage problem with a &lt;a href=&quot;https://paulstamatiou.com/storage-for-photographers-part-2/&quot; title=&quot;Storage for Photographers (Part 2) - How a 12TB Synology NAS changed my digital life&quot;&gt;cloud-backed 12TB Synology NAS&lt;/a&gt;. That setup is still running great. Now I just need to keep up with the performance requirements of having the latest camera gear with absurd file sizes.&lt;/p&gt;
&lt;p&gt;I decided it was time to upgrade to something a bit more powerful. This time I decided to build a PC and switch to Windows 10 for my heavy computing tasks. &lt;strong&gt;Yes, I switched to &lt;em&gt;Windows&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;hr class=&quot;full&quot;/&gt;&lt;h6&gt;A note to the reader&lt;/h6&gt;
&lt;p&gt;This is a long blog post. The longest I've written on this site—over 32,000 words—and consumed many of my weekends for about 4 months. Typically these &quot;I built a computer&quot; posts are rather useless a few months down the line when new hardware comes out and it's nothing but an old parts list. While I can't avoid that, I aimed to provide enough information about my reasoning for why I chose certain parts or how I configured things so that this post may still be helpful a year or three down the line. &lt;strong&gt;Enjoy!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you like this post, please share it with your friends, followers or anyone that might be interested.&lt;/p&gt;
&lt;div id=&quot;toc&quot;&gt;
&lt;h6&gt;Table of contents&lt;/h6&gt;
&lt;/div&gt;
&lt;h4&gt;What I use my computers for&lt;/h4&gt;
&lt;p&gt;For the last few years I have more or less had some variant of the same setup: a beefy desktop computer for heavy lifting and a small laptop for travel and casual use. My desktop usage, in order from most to least frequent, is largely comprised of Adobe Lightroom, web development for this website, Adobe Premiere Pro and some occasional gaming.&lt;/p&gt;
&lt;p&gt;While I did love my 5K iMac, I hated that the only way to upgrade a year or two later was just to replace the entire thing. I hated that even the newest models were typically behind Intel's release schedule and you couldn't get the absolute latest and greatest hardware, much less be able to overclock them a bit for even more performance.&lt;/p&gt;
&lt;p&gt;Apple has failed to provide the option for high-performance, user-upgradeable machines for years and even the new iMac Pro continues that trend. Perhaps the rumored upcoming Mac Pro will be different but I just don't see a world where you'll ever be able to hear about the latest Intel chipset and processor launch, immediately buy a new processor and motherboard and upgrade your Mac that weekend.&lt;/p&gt;
&lt;p&gt;I'm not the only one with this mindset. More and more creative professionals that demand the most from their machines are getting over Apple for their high-end computing needs. Filmmaker Philip Bloom recently &lt;a href=&quot;https://www.youtube.com/watch?v=2sGtefBwv8Y&quot; title=&quot;TIME TO MAKE THE SWITCH FROM MAC TO PC FOR PREMIERE PRO EDITING?&quot;&gt;moved to a Windows machine&lt;/a&gt;. Photographer Trey Ratcliff &lt;a href=&quot;https://www.stuckincustoms.com/2017/02/10/switching-from-mac-to-windows/&quot; title=&quot;Apple Is Dead To Me… I’m Switching To Windows!&quot;&gt;did the same&lt;/a&gt; and I've been seeing more and more friends in the creative space do the same.&lt;/p&gt;
&lt;hr class=&quot;full&quot;/&gt;&lt;p&gt;&lt;strong&gt;&lt;span class=&quot;dropcap&quot;&gt;N&lt;/span&gt;othing is really holding me to macOS on the desktop.&lt;/strong&gt; I go in there, edit some photos or do another large task then I retreat to my 13&quot; Macbook Pro.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-1&quot; id=&quot;r1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; The Adobe suite works on Windows and there is now official Linux support via WSL on Windows 10 so I can run my development environment easily.&lt;/p&gt;
&lt;p&gt;I would be lying if I didn't mention one of the main reasons I wanted to build a PC: finally having a modern full-size graphics card, for both GPU acceleration in creative applications as well as for gaming. With my iMac I casually played a few games on Steam, but with paltry settings. Even if I were to purchase a new high-end Mac, you just can't get the best graphics card on the market (even with the new iMac Pro&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-2&quot; id=&quot;r2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; ). Much less just be able to easily swap it out with a better card a year later.&lt;/p&gt;
&lt;p&gt;When I began planning this new build around April 2017, I considered making it a dual-boot &lt;a href=&quot;https://paulstamatiou.com/hackintosh-computer/&quot; title=&quot;My first Hackintosh&quot;&gt;Hackintosh&lt;/a&gt; and Windows 10 PC. At the time a hackintosh build sounded promising: Kaby Lake processor support and Nvidia drivers for Pascal GPUs for macOS had just been announced.&lt;/p&gt;
&lt;p&gt;Then I began thinking of how I actually use my computer. The idea of constantly rebooting to hop into Windows for a bit to play a game, then reboot to go back to macOS seemed like a major inconvenience. It also meant that I couldn't just upgrade to the newest hardware — I would have to wait for hackintosh support to arrive. Not to mention the associated hackintosh annoyances I've dealt with in the past: tricky software updates and reliability issues. I knew what I needed to do.&lt;/p&gt;
&lt;p class=&quot;larger&quot;&gt;&lt;strong&gt;The goal&lt;/strong&gt;: Build a fast, yet quiet and understated desktop PC with a healthy overclock aimed at improving my photo workflow while giving me the ability to upgrade parts of it later on.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;section-title&quot; id=&quot;lightroom_fast&quot;&gt;&lt;h2&gt;&lt;span&gt;What makes Lightroom fast?&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;But first, let's talk about what Lightroom needs to thrive.&lt;/h3&gt;
&lt;/section&gt;&lt;section readability=&quot;47.406637613141&quot;&gt;&lt;p&gt;One thing to note, and this kind of defeats the purpose of this whole post: I've never seen any hardware improvement, no matter how drastic, turn Lightroom into a pure &lt;em&gt;speed demon&lt;/em&gt; when dealing with the kind of huge RAW files I work with. I might experience single digit to low double digit percentage improvements in certain tasks, but nothing that would blow my socks off. Nothing instant. If someone claims their Lightroom setup is instant, they're lying or they're working with tiny 12-megapixel JPGs. &lt;a href=&quot;https://www.pugetsystems.com/labs/articles/How-Much-Faster-is-a-Modern-Workstation-for-Adobe-Lightroom-CC-2015-8-901/&quot; title=&quot;How Much Faster is a Modern Workstation for Adobe Lightroom CC 2015.8?&quot;&gt;Here's more on the topic&lt;/a&gt; if you're interested.&lt;/p&gt;
&lt;p&gt;In addition, any performance improvements gained on new hardware are often then negated when upgrading to newer and newer cameras that shoot higher megapixel photos and higher resolution and bitrate videos. &lt;strong&gt;It's a vicious cycle.&lt;/strong&gt; I have even thought about downgrading to a lower megapixel camera to make editing easier; but I love being able to have room to crop photos and videos. And the extra megapixels helps when I &lt;a href=&quot;https://twitter.com/Stammy/status/918297218246885376&quot;&gt;frame some of my photos&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;What would make Lightroom really fast is the software itself receiving dramatic optimization and performance updates. It has been around for ages, I'd imagine there is quite a bit of code cruft that Adobe would love to refactor and rethink. Adobe has even &lt;a href=&quot;https://theblog.adobe.com/on-lightroom-performance/&quot; title=&quot;On Lightroom Performance&quot;&gt;stated that they know Lightroom is slow&lt;/a&gt; and they're working on it. Nothing I can do here but cross my fingers and wait for software updates.&lt;/p&gt;
&lt;h4&gt;How I use Lightroom&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;Here's what I do in Lightroom that can feel slow.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I spend a lot of time in Lightroom.&lt;/strong&gt; What exactly do I do to my photos? Increasingly less and less (more on that below), but there's still quite a few tasks from culling to pick the best shots out of hundreds or thousands all the way to numerous adjustments made individually on each photo.&lt;/p&gt;
&lt;p&gt;I have been interested in photography for over a decade but didn't really start taking it seriously until I built out my &lt;a href=&quot;https://paulstamatiou.com/photos/&quot;&gt;photoblog&lt;/a&gt; and started crafting photosets of trips. At first I enjoyed making photos seem surreal and dramatic. I was all too eager to yank the saturation and clarity sliders and even use programs like Photomatix Pro and Aurora HDR that started out basically encouraging the creation overly gaudy HDR images.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-3&quot; id=&quot;r3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Over the years I have tried to hone my photography aesthetic to be more realistic and only edit to try to capture what it was like to be there and see something with your own eyes — recovering highlights and shadows, removing spots created by a dirty lens in a long exposure, adjusting color temperature to communicate the warmth of that day, remove noise to share the clear night with bright stars and so on. And sure, sometimes that vibrance slider might find it's way to +15 to accentuate some glacial blue water, but I rarely touch the saturation slider these days.&lt;/p&gt;
&lt;p class=&quot;larger&quot;&gt;And like designing&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-4&quot; id=&quot;r4&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; a product interface, there's just as much work if not more that goes into keeping things simple and have it communicate effectively. Sometimes I'll spend the most time leveling a shot and finding a good crop.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7184-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.529051987767584&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7184-1000.jpg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7189-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.3477088948787062&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7189-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;Sony A7R III with &lt;a href=&quot;https://paulstamatiou.com/photos/gear/&quot; title=&quot;Paul Stamatiou Camera Gear&quot;&gt;some of my lenses&lt;/a&gt;.&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;11.728155339806&quot;&gt;&lt;p&gt;I love working in Lightroom on a high-res display in full-screen mode. I often zoom 100% into one of my &lt;a href=&quot;https://paulstamatiou.com/photos/gear/&quot;&gt;Sony a7R III's&lt;/a&gt; massive 42MP RAW photos to find the sharpest and most in focus of several similar shots.&lt;/p&gt;
&lt;p class=&quot;larger&quot;&gt;Unfortunately, all three of these behaviors incur a significant performance cost right off the bat.&lt;/p&gt;
&lt;p&gt;If you're familiar with Lightroom you probably know about the different modules of the app. I spend the vast majority of my time in &lt;strong&gt;Develop&lt;/strong&gt; module and some of my time in &lt;strong&gt;Library&lt;/strong&gt; module. The different modules act as tabs — changing between them brings up a new set of functionality and contextual side panels.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-pstam-lightroom-classic-cc-fullscreen-nz-mtcook-2500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.777&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-pstam-lightroom-classic-cc-fullscreen-nz-mtcook-1500.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;Lightroom Classic CC fullscreened at 3840x2160 in the Develop module working on an HDR image.&lt;br/&gt;Photo: &lt;a href=&quot;https://paulstamatiou.com/photos/new-zealand/mount-cook-to-christchurch/&quot; title=&quot;Paul Stamatiou - Mount Cook, New Zealand&quot;&gt;Mount Cook, New Zealand&lt;/a&gt;&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;52.501214329083&quot;&gt;&lt;p&gt;When I’m doing basic culling, I &lt;em&gt;try&lt;/em&gt; to stick around in the Library module where there are performance benefits at the expense of not being able to do any real editing to the shots. It's possible to make filmstrip scrolling and browsing in the Library module fairly speedy by generating previews either manually or on import.&lt;/p&gt;
&lt;p&gt;Generating previews in advance means that Lightroom doesn't have to fire up the Camera Raw engine to process and then cache a large compressed RAW file each time you click on a photo, an action that can take up to 3-5 seconds per photo on a large screen.&lt;/p&gt;
&lt;p&gt;There are several kinds of previews in Lightroom, but I generally have &lt;strong&gt;1:1&lt;/strong&gt; previews created when I import a new set of photos. They're processed, full-size versions of the RAW photo. It takes a lot of time to generate them but it's done all at once. I don't mind that upfront cost as I can just go make a coffee, &lt;a href=&quot;https://paulstamatiou.com/reading-more-kindle-oasis/&quot; title=&quot;Reading more -- Why I got a Kindle and set a goal to read 24 books in 2017&quot;&gt;read&lt;/a&gt; and come back in 30 minutes.&lt;/p&gt;
&lt;hr class=&quot;full&quot;/&gt;&lt;p&gt;However, I often hop over to the Develop module while culling to see what the photo could look like with some basic adjustments or a crop to see if the shot is worth keeping. Unfortunately, 1:1 previews are not utilized in the Develop module and even if I had generated Smart Previews which are used in the Develop module, they only create previews up to a max size of 2540px on the longest edge of each photo.&lt;/p&gt;
&lt;p&gt;So where does this leave me? Spending the majority of my time in the Develop module where generated previews won't help on a large display with frequent 100% zooming. The only savior we have here is that the Develop module is the only part of Lightroom with GPU acceleration:&lt;/p&gt;
&lt;blockquote readability=&quot;12.223880597015&quot;&gt;
&lt;p&gt;Lr can now use graphics processors (GPUs) to accelerate interactive image editing in Develop. A big reason that we started here is the recent development and increased availability of high-res displays, such as 4K and 5K monitors. To give you some numbers: a standard HD screen is 2 megapixels (MP), a MacBook Retina Pro 15&quot; is 5 MP, a 4K display is 8 MP, and a 5K display is a whopping 15 MP. This means on a 4K display we need to render and display 4 times as many pixels as on a standard HD display. Using the GPU can provide a significant speedup (10x or more) on high-res displays. The bigger the screen, the bigger the win.&lt;/p&gt;
&lt;span class=&quot;author&quot;&gt;&lt;a href=&quot;https://forums.adobe.com/thread/1828580&quot; title=&quot;GPU notes for Lightroom CC (2015)&quot;&gt;&lt;strong&gt;Adobe&lt;/strong&gt;, GPU notes for Lightroom CC (2015)&lt;/a&gt;&lt;/span&gt;&lt;/blockquote&gt;
&lt;p&gt;Even with that, it's still very early for Lightroom GPU hardware acceleration and it leaves much to be desired. GPU acceleration can make most Develop controls quicker but it seems that can come at the slight expense of two things: the time it takes to load full-resolution images as well as moving from image to image. Also, actions like panorama stitching, HDR photo merging, the adjustment brush and spot removal tools do not seem to get any boost here.&lt;/p&gt;
&lt;h4&gt;Inside the Develop module&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;Okay, here's my typical Lightroom workflow&lt;/p&gt;
&lt;p&gt;After I have mostly completed the culling process and selected the better shots to keep in my collection,&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-5&quot; id=&quot;r5&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; I go over each photo with a series adjustments as needed. I most commonly visit these settings:&lt;/p&gt;
&lt;ul readability=&quot;31.350083986562&quot;&gt;&lt;li readability=&quot;8.8237454100367&quot;&gt;
&lt;p&gt;&lt;strong&gt;Camera Calibration → Profile:&lt;/strong&gt; The profile determines how Lightroom processes the RAW and serves as a basis for all your adjustments. Depending on the camera you use and Lightroom's support for it, you will see different options here. I believe the goal from camera manufacturers is to have the profile mimic the camera's own creative style settings had you had any enabled and shot a JPG; those settings don't affect the actual RAW.&lt;/p&gt;
&lt;p&gt;I rarely use the default Adobe Standard profile and have Lightroom configured to use Camera Standard as the new default profile. Depending on the photo I may use something like Camera Landscape for more contrast and color but often find it too saturated and have to manually compensate for that. There's a wealth of information about Camera Profiles out there, but &lt;a href=&quot;https://petapixel.com/2016/11/29/always-set-profile-lightroom-start-editing/&quot; title=&quot;Why You Should Always Set the Profile in Lightroom Before You Start Editing&quot;&gt;here's a starter&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1.9956989247312&quot;&gt;
&lt;p&gt;&lt;strong&gt;Remove Chromatic Aberration &amp;amp; Enable Profile Corrections:&lt;/strong&gt; I tend to have these on by default. The latter will load a lens profile if one exists to correct any distortion (like barrel or pincushion) with your lens. One school of thought is to rarely use profile corrections as they can reduce detail and also lead to some minor cropping at times. But I find this to be a bit nitpicky and won't readily be able to discern a significant loss of detail by enabling it.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-6&quot; id=&quot;r6&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;strong&gt;Transform:&lt;/strong&gt; On certain occassions, like having a shot of a building that was taken at a slight skew, it can come in handy to enable a perspective correction. With more complex subjects, such as trying to tame two similar but slightly off leading lines in a photo, Lightroom has the guided transform feature. However, I try to only use these if the effect does not do &lt;em&gt;too&lt;/em&gt; much. It can look pretty unnatural in those cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;9&quot;&gt;
&lt;p&gt;&lt;strong&gt;Basics:&lt;/strong&gt; The essential controls that I fiddle with on every shot: Exposure, Temperature, Tint, Highlights, Shadows, Whites, Blacks and to a much lesser extent clarity and vibrance. At times I will jump directly to the Tone Curve but I often really only go there to tweak one or two RGB curves a bit, not everything at once.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;4&quot;&gt;
&lt;p&gt;&lt;strong&gt;HSL:&lt;/strong&gt; Sparingly, I'll find myself wanting to reduce the luminance, saturation and rarely hue of a particular color in a shot. Most commonly I'll use it to decrease the prominence of a particular color in a scene I find distracting. Like if I adjusted the color temperature of a photo to be a bit warmer and it's making some yellow/orange foliage in a shot look obviously too saturated. Or if I wanted to adjust the luminance of blue to make a body of water darker or brighter to maybe compensate for other adjustments that may have made it appear a bit off.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;20&quot;&gt;
&lt;p&gt;&lt;strong&gt;Spot removal, adjustment brush, graduated filter:&lt;/strong&gt;&lt;br/&gt;Spot removal gets a good amount of use. Most frequently to clean up anything caused by a dirty lens. When you're out shooting all day you tend to get some dust specks, mist and other tiny debris that only becomes obvious when capturing long exposures. I use the &quot;Visualize spots&quot; mode of the spot removal tool to easily track down and remove these spots.&lt;/p&gt;
&lt;p&gt;I use the adjustment brush much less, but in recent memory I used it to select a mountain range in the distance that had decreased visibility due to clouds/fog and increased the clarity and contrast a tad. But I'm using it less and less these days.&lt;/p&gt;
&lt;p&gt;Graduated filter rarely gets used anymore, but in the past I liked placing it above the horizon to make the top of the sky a bit darker, reduce highlights, boost contrast and clarity to make clouds pop.&lt;/p&gt;
&lt;p&gt;Lightroom Classic CC has a new range mask modifier for these actions that makes them easier to control that I've used a few times.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-spot-removal-visualize-spots.jpg&quot;&gt;&lt;img data-ratio=&quot;1.316&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-spot-removal-visualize-spots.jpg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-spot-removal-visualize-spots-corrected.jpg&quot;&gt;&lt;img data-ratio=&quot;1.316&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-spot-removal-visualize-spots-corrected.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;The visualize spots mode for spot removal. Left: Dirty lens from shooting near the water a lot one day with lots of spots obvious in the sky (the largest spot was actually a tiny scratch in my polarizer). Right: With corrections applied.&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;131.1684039453&quot;&gt;&lt;ul readability=&quot;0&quot;&gt;&lt;li readability=&quot;2.8748137108793&quot;&gt;
&lt;p&gt;&lt;strong&gt;Merge HDR:&lt;/strong&gt; When necessary depending on the scene, I will turn on bracketing and shoot a ton of 3-shot brackets. I've done several 5 shot brackets but didn't find enough value in the difference to make up for all the extra storage and time required for those. In Lightroom I will stack each 3-shot bracket then select a few of the stacks and begin them in parallel with the &lt;a href=&quot;https://twitter.com/Stammy/status/869765824294207488&quot;&gt;headless HDR processing mode&lt;/a&gt;. Just press CTRL (windows) or Cmd (mac) + Shift + H when you have a few stacks selected. This is only good if your HDRs are from the same scene and fairly similar as the headless mode skips the HDR Merge Preview dialog and just goes with whatever setting you last used.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Even though I consider this &lt;em&gt;light&lt;/em&gt; editing, that's still a ton of actions to do on one photo. Even something seemingly as simple as a profile correction can end up increasing the number of calculations Lightroom has to do on all subsequent actions. Adobe even &lt;a href=&quot;https://helpx.adobe.com/lightroom/kb/optimize-performance-lightroom.html&quot; title=&quot;Optimize performance&quot;&gt;recommends a particular order of operations&lt;/a&gt; in the Develop module to speed things up:&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;The best order of Develop operations to increase performance is as follows:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Spot healing.&lt;/li&gt;
&lt;li&gt;Geometry corrections, such as Lens Correction profiles and Manual corrections, including keystone corrections using the Vertical slider.&lt;/li&gt;
&lt;li&gt;Global non-detail corrections, such as Exposure and White Balance. These corrections can also be done first if desired.&lt;/li&gt;
&lt;li&gt;Local corrections, such as Gradient Filter and Adjustment Brush strokes.&lt;/li&gt;
&lt;li&gt;Detail corrections, such as Noise Reduction and Sharpening.&lt;/li&gt;
&lt;/ol&gt;&lt;br/&gt;&lt;strong&gt;Note:&lt;/strong&gt; Performing spot healing first improves the accuracy of the spot healing, and ensures the boundaries of the healed areas match the spot location.&lt;/blockquote&gt;
&lt;p&gt;Once I've adjusted each shot to my heart's content — and gone back and forth over each shot multiple times — I happily initiate an full-size JPG export. This takes a long time but I don't care as much compared to speed in the Develop module; I use the opportunity to take a break and do something else while the computer works.&lt;/p&gt;
&lt;h4&gt;A word about presets&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;Why don't you just have a few presets to pick from instead of adjusting everything manually?&lt;/p&gt;
&lt;p&gt;Having a robust set of custom presets tailored to your personal photography aesthetic can save a ton of time when faced with a new set of imported photos. But that's not really my thing. &lt;strong&gt;I simply don't like using presets as one-click-and-done filters.&lt;/strong&gt; I always want to manually adjust things to see what a certain photo is capable of and not &quot;leave anything at the table&quot; by just using a preset I have lying around. It could get the job done but wouldn't be what I would have ended up with had I started from scratch.&lt;/p&gt;
&lt;p&gt;The thing that speeds up my workflow more than anything is not presets, but actually just a quick way to copy and paste develop settings with shortcut keys. I use VSCO Keys to do this using &lt;code class=&quot;inline&quot;&gt;.&lt;/code&gt; and &lt;code class=&quot;inline&quot;&gt;,&lt;/code&gt; hotkeys. It's quick and effective — I fiddle with settings on one photo to my liking, copy and use that as a base for any subsequent photos that are similar.&lt;/p&gt;
&lt;p&gt;Many folks use presets in a similar way to speed up basic, repetitive things you typically do then stack them to provide a quick base to work from. You might have a handful of presets to do things like boost contrast, increase shadows and so on. And by making them for distinct actions and not set any other values, you can stack them by continuing to click on other such presets. Some folks love this flow but I never really got into it.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-7&quot; id=&quot;r7&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h4&gt;Hardware considerations&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;What do you need most? Disk I/O, GHz, CPU cores, GPU?&lt;/p&gt;
&lt;p&gt;As you know there are a few main levers that affect the majority of a computer’s performance: storage, RAM, GPU and CPU. To be more precise: storage throughput, RAM size, RAM speed as well as the number of CPU cores and clock speed. In the case of Lightroom, CPU plays the most important role in overall application performance and to a much lesser extent GPU.&lt;/p&gt;
&lt;h6&gt;Storage&lt;/h6&gt;
&lt;p&gt;Surprisingly, storage speed is not of the utmost importance to Lightroom as long as you have something decent. It's especially a non-issue if you have some kind of SSD and store everything on it. That can be rather expensive and you may opt to have a smaller SSD that only stores the Camera Raw cache, previews and catalog and then a regular hard drive to store the images themselves. Even with that setup the Lightroom performance difference is fairly indistinguisable. There are &lt;a href=&quot;https://www.pugetsystems.com/labs/articles/Adobe-Lightroom-2015-8-Storage-Performance-Analysis-875/&quot; title=&quot;Adobe Lightroom 2015.8 Storage Performance Analysis&quot;&gt;minimal benefits between a regular SSD and a superfast NVMe SSD&lt;/a&gt; as far as Lightroom is concerned.&lt;/p&gt;
&lt;p&gt;Whatever your storage solution, you'll want a lot of storage space when dealing with hefty RAWs. Or &lt;a href=&quot;https://paulstamatiou.com/storage-for-photographers-part-2/&quot; title=&quot;Storage for Photographers (Part 2)&quot;&gt;consider investing in a NAS setup&lt;/a&gt; to archive shots when you're done with them.&lt;/p&gt;
&lt;h6&gt;RAM&lt;/h6&gt;
&lt;p&gt;As for RAM, you probably don't &lt;em&gt;need&lt;/em&gt; more than 16GB for Lightroom. However, if you aggressively multitask and/or use Adobe Premiere Pro you'll want at least 32GB. You can definitely exceed that amount and go with the maximum amount supported by your motherboard.&lt;/p&gt;
&lt;p&gt;But there are some things to keep in mind. First, RAM is not as cheap these days as it used to be for a variety of reasons, and low latency RAM is even pricier. Second, if you care about bleeding edge performance and overclocking, you should find your ideal amount of RAM in 2 sticks, not 4. It's harder to maintain an aggressive overclock with 4 channels of RAM putting a larger strain on the integrated memory controller, especially if the CPU only has dual-channel support like the Intel i7-8700K and if the CPU itself is running an aggressive overclock.&lt;/p&gt;
&lt;h6&gt;GPU&lt;/h6&gt;
&lt;p&gt;Lightroom &lt;em&gt;can&lt;/em&gt; use a good graphics card for hardware acceleration but it really doesn't take the most advantage of it. You might notice the benefit if you are using a 4K or better display and do basic actions in the Develop module.&lt;/p&gt;
&lt;p&gt;On lower resolution displays I've heard that having GPU acceleration enabled can actually hurt performance as your computer spends time sending data between the CPU and GPU that the CPU could have just done on its own in a shorter amount of time.&lt;/p&gt;
&lt;p&gt;Hopefully Lightroom will make better use of high-end graphics cards with future software updates. You definitely don't need a top of the line card for Lightroom but if you're going to get one anyways you'll want to learn towards an Nvidia card. Adobe software seems to be more optimized for Nvidia graphics cards.&lt;/p&gt;
&lt;h6&gt;CPU&lt;/h6&gt;
&lt;p&gt;Adobe's recent upgrade to Lightroom Classic CC brought some performance improvements, largely related to increased multi-core performance for generating previews. Overall though, &lt;strong&gt;Lightroom does not make the best use of many CPU cores.&lt;/strong&gt; The Develop module can to a degree but the performance makes it obvious that it's not terribly efficient. This is a theme with all of Lightroom in regards to performance: it could always be better.&lt;/p&gt;
&lt;p class=&quot;larger&quot;&gt;For my needs Lightroom loves the highest clock speed it can get, as opposed to a ton of lower clocked cores.&lt;/p&gt;
&lt;p&gt;Having more cores in Lightroom can help you if you care more about exporting images and generating previews. That is not something I care about as it happens so infrequently compared to me fiddling with sliders in Develop. Otherwise, you're better off with fewer cores with a very high clock speed. This will help with:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Scrolling through photos in the Develop module&lt;/li&gt;
&lt;li&gt;Performance and responsiveness for adjustments in Develop&lt;/li&gt;
&lt;li&gt;Converting images to DNG&lt;/li&gt;
&lt;li&gt;Merging HDR images and stitching panoramas&lt;/li&gt;
&lt;/ul&gt;&lt;blockquote readability=&quot;13.72754491018&quot;&gt;
&lt;p&gt;If you shoot a large amount of photos and hate waiting for images to export or previews to generate, then a higher core count CPU like the Core i7 7820X 8 Core, Intel Core i9 7900X 10 Core, or even the Core i9 7940X 14 Core may be a great choice depending on your budget. You certainly give up general editing performance as you get into the higher core counts, but a 30-40% reduction in the time it takes to export and generate previews can be a massive time saver. However, if this isn't a major consideration and you just want the smoothest editing experience possible, then the Intel Core i7 8700K is still our go-to recommendation for Lightroom.&lt;/p&gt;
&lt;span class=&quot;author&quot;&gt;— &lt;strong&gt;&lt;a href=&quot;https://www.pugetsystems.com/labs/articles/Lightroom-Classic-CC-is-it-faster-than-CC-2015-1065/&quot; title=&quot;Lightroom Classic CC: is it faster than CC 2015?&quot;&gt;Puget Systems&lt;/a&gt;&lt;/strong&gt;&lt;/span&gt;&lt;/blockquote&gt;
&lt;h4&gt;What about video editing?&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;Premiere Pro is great at using multiple cores and a beefy GPU&lt;/p&gt;
&lt;p&gt;I wouldn't consider myself anywhere near savvy in any video editing apps, but I have been shooting more and more video footage on my trips. I've gone from making edits in iMovie to Final Cut Pro X then After Effects and finally to using Premiere Pro. A good chunk of my footage is now captured in 4K. Better Premiere Pro performance is a nice to have for me but not a priority compared to Lightroom performance.&lt;/p&gt;
&lt;p&gt;On the completely opposite end of the spectrum compared to Lightroom, Adobe Premiere Pro has much, much better multicore efficiency. It also seems to &lt;a href=&quot;https://helpx.adobe.com/premiere-pro/using/effects.html#gpu_accelerated_effects&quot; title=&quot;List of GPU accelerated effects in Premiere Pro&quot;&gt;put a powerful graphics card to good use&lt;/a&gt; and will most definitely make use of as much RAM you can give it.&lt;/p&gt;
&lt;p&gt;My common tasks in Premiere Pro — rendering previews, warp stabilization, Lumetri color adjustments and exporting — all take advantage of additional processor cores. And the faster the clock of each core, the better.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;section-title&quot; id=&quot;why_build&quot;&gt;&lt;h2&gt;&lt;span&gt;Why build a PC?&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;Building a PC is easier than ever today&lt;/h3&gt;
&lt;/section&gt;&lt;section readability=&quot;31.557093425606&quot;&gt;&lt;p class=&quot;larger&quot;&gt;I'm no stranger to building a computer from scratch; I have built dozens by now&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-8&quot; id=&quot;r8&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; . However, quite a few things have changed from the last time I built a computer.&lt;/p&gt;
&lt;p&gt;These things are now fairly commonplace and welcomed additions to the building process:&lt;/p&gt;
&lt;ul readability=&quot;22.5&quot;&gt;&lt;li readability=&quot;15&quot;&gt;
&lt;p&gt;&lt;strong&gt;AIO (all-in-one) liquid cooling systems&lt;/strong&gt;&lt;br/&gt;Back in the day if you wanted a high-performance and relatively quiet cooling option for your overclocked processor, you would have to source a radiator, fans, pump, reservoir, tubing, CPU/GPU/Northbridge waterblock and assemble it yourself. You'd have to fill it up, put some anti-algae chemicals in, get all the bubbles out, do hours of leak testing and then change the liquid coolant out every 6 months or so. It was a huge hassle.&lt;/p&gt;
&lt;p&gt;Now you can just buy an all-in-one system that comes with everything you need in a simple closed loop. Most AIO units these days even have a USB connection and some desktop software to help monitor and automatically ramp up the fans and pump depending on CPU or GPU activity.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;strong&gt;Fully-modular PSUs&lt;/strong&gt;&lt;br/&gt;Why should a computer with a million internal drives and accessories have the same number of power supply cables as a basic setup with just one SSD? There's no need to have 20 extra power cables taking up space in your case if you don't need them.&lt;/p&gt;
&lt;p&gt;Modular power supplies let you connect only the cables you need, reducing clutter from your case. And most offer attractive sleeving styles to boot. Just don't ever mix and match cables from other PSUs — there is no standard pin layout for the connections &lt;em&gt;on&lt;/em&gt; the PSU and rookie PC builders often burn their PC by accidentally keeping old cables in there when switching to another modular PSU.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;strong&gt;Operating systems are sold on USB sticks now.&lt;/strong&gt; No longer do you need to buy a cheap optical drive just to install the operating system and never use it again. Windows 10 is now sold on a tiny USB stick.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;3&quot;&gt;
&lt;p&gt;&lt;strong&gt;Cases with no 3.5&quot; and 5.25&quot; drive bays!&lt;/strong&gt; And on that note, why should you have a case with unsightly internal and external drive bays you may never use? Case manufacturers have started offering cases entirely devoid of 5.25&quot; optical drive bays as well as 3.5&quot; racks, or they have removable racks.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;strong&gt;M.2 NVMe SSDs:&lt;/strong&gt; Probably the biggest innovation for me personally. SSDs in a small PCIe stick that offer a tremendous performance advantage over even regular 2.5&quot; SSDs. Something like 3-5x faster.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;strong&gt;Case windows use real glass now.&lt;/strong&gt; Modern high-end computer cases actually use real glass instead of scratch-prone and flimsy lucite or plexiglass. I still think windows in computer cases are kind of silly though.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;3&quot;&gt;
&lt;p&gt;&lt;strong&gt;Wireless mice are actually good now.&lt;/strong&gt; Okay, I'm really dating myself here but for the longest time wireless mice were laggy. Noticeably laggy cursor. Impossible to use for even the most basic gaming and they came with horrible battery life. I have been using &lt;em&gt;wired&lt;/em&gt; Logitech mice for about a decade... but I recently switched to a Logitech MX Master 2S. Wireless mice with great battery life and nice customizability are finally here.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;In the past you sort of had to wing it when it came to picking parts for your build. You would have to read a bunch of reviews for motherboards, graphics cards, RAM and so on. Then you might need to actively participate in a computer forum to see what folks were running, if there were any compatibility concerns, order everything and then hope everything worked as expected.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://pcpartpicker.com/&quot; title=&quot;Pick Parts. Build Your PC. Compare And Share.&quot;&gt;PCPartPicker.com&lt;/a&gt; is one relatively new resource that I have found to be invaluable. People share their build lists, photos and more there. It helped me answer very specific questions on numerous occasions:&lt;/p&gt;
&lt;ul readability=&quot;1&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Does this graphics card fit in this case?&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Will this watercooler fit?&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;What does this case look like with these parts?&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;What is the smallest case that will fit this motherboard?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Chances are someone out there has built a machine identical to what you want to build and you can just look up pictures of that rig.&lt;/p&gt;
&lt;p&gt;In addition, I've found a few other handy resources while building: the active Reddit &lt;a href=&quot;http://reddit.com/r/buildapc&quot;&gt;r/buildapc&lt;/a&gt; and some popular YouTube channels like &lt;a href=&quot;https://www.youtube.com/channel/UCftcLVz-jtPXoH3cWUUDwYw&quot;&gt;Bitwit&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/user/LinusTechTips&quot;&gt;Linus Tech Tips&lt;/a&gt; and &lt;a href=&quot;https://www.youtube.com/user/Jayztwocents&quot;&gt;JayzTwoCents&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Though I would be remiss of me if I did not mention why it may not be a great time to build such a PC: RAM prices and graphics card prices have skyrocketed in the last year. The latter mainly due to insane high-end graphics card demand from cryptocurrency mining.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;section-title&quot; id=&quot;case&quot;&gt;&lt;h2&gt;&lt;span&gt;The case&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;Finding a good case will never be easy&lt;/h3&gt;
&lt;/section&gt;&lt;section readability=&quot;46.456387665198&quot;&gt;&lt;p class=&quot;larger&quot;&gt;Unfortunately, one thing has not become easier over time. Finding an attractive, understated and simple case. Case manufacturers seem to only cater to the gamer stereotype of excess and gaudy.&lt;/p&gt;
&lt;p&gt;I'm in my 30s, I'm a designer... I want something simple but that doesn't mean I don't want to have the best hardware, support for large water-cooling radiators and expected case ammenities like thumb-screws, anti-vibration features and other noise considerations. I could care less about LED fans, weird intake designs and other questionable aesthetic choices.&lt;/p&gt;
&lt;p&gt;In the past I tended to like small form factor computers, having &lt;a href=&quot;https://paulstamatiou.com/how-to-build-microsoft-windows-7-intel-core-i7-pc/&quot;&gt;made quite a few Shuttle SFF computers&lt;/a&gt;. So I started there, thinking I could get a micro-ATX motherboard. This would prove to be a challenge given my desire to have a long full-size high-end graphics card.&lt;/p&gt;
&lt;p&gt;There were a few that were somewhat close to what I was looking for like the Define Mini C and Corsair Air 240, at least size-wise. Then I found a Kickstarter for a &lt;em&gt;ridiculously&lt;/em&gt; small case called the DAN Cases A4-SFX that used a micro-ITX motherboard and could house a full-size graphics card. It was dubbed the &quot;world's smallest gaming tower&quot; at roughly the size of a shoebox. Sure, it had some tradeoffs (non-ATX PSU, limited motherboard selection, limited heatsink-fan options and not the best cooling in general) but it seemed perfect.&lt;/p&gt;
&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/dancases-a4sfxv2.jpg&quot;&gt;&lt;img data-ratio=&quot;3.164&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/dancases-a4sfxv2.jpg&quot;/&gt;&lt;/a&gt;
&lt;p&gt;Unfortunately, it was sold out everywhere. A &lt;a href=&quot;https://www.kickstarter.com/projects/33753221/dan-cases-a4-sfx-v2-an-ultra-compact-sff-case/&quot; title=&quot;DAN Cases A4-SFX v2 - an ultra-compact SFF case&quot;&gt;second version eventually launched&lt;/a&gt; on Kickstarter and I ordered it. Though it won't arrive for a while. Maybe I'll use that for a separate build later on.&lt;/p&gt;
&lt;hr class=&quot;full&quot;/&gt;&lt;p&gt;The more I thought about it, I wanted a case large enough for me to pick a motherboard with great overclocking capabilities, the ability to have a water-cooling setup and the room to expand to two graphics cards via SLI if I so decided in the future&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-9&quot; id=&quot;r9&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; .&lt;/p&gt;
&lt;p&gt;The searching continued for a case that could accomodate an ATX motherboard as well as a large 280mm radiator. I'll spare you the details, but after looking at a bunch of cases (ones from NZXT, Define, Corsair and Lian Li mainly), I landed on the &lt;strong&gt;NZXT S340 Elite VR&lt;/strong&gt;. NZXT also has a newer model called the H700i that seems interesting and has a bit better internal cable management but I'm not a fan of some of the perforated panels it has on the top.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02565-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.1904761904761905&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02565-1000.jpg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02599-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.24875&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02599-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;Shot these photos myself! Got &lt;a href=&quot;https://twitter.com/Stammy/status/943355030127693825&quot;&gt;a studio lighting setup&lt;/a&gt; just for this article.&lt;/small&gt;&lt;/section&gt;&lt;section class=&quot;section-title&quot; id=&quot;parts&quot; readability=&quot;10&quot;&gt;&lt;h2&gt;&lt;span&gt;The parts&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;What parts I chose and why&lt;/h3&gt;
&lt;p&gt;I first built this computer in April 2017 with a quad-core i7 7700K and Z270 chipset motherboard. But later that year Intel released six-core i7 8700K processor and Z370 chipset. I ended up upgrading both the CPU and motherboard at that time.&lt;/p&gt;
&lt;h4&gt;Processor&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;Intel Core i7 8700K&lt;/p&gt;
&lt;p&gt;I went with the &lt;strong&gt;hexa-core&lt;/strong&gt; Coffee Lake Intel Core i7 8700K processor running at 3.7GHz (4.7GHz with Turbo Boost). At the time I built this computer, the 8700K was the best processor for gaming as well as performing well in Lightroom compared to other chips. It's not the best for Adobe Premiere Pro but it's better than a 7700K with fewer cores.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC05376-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.5&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC05376-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;42.921532846715&quot;&gt;&lt;p class=&quot;larger&quot;&gt;Why not go for more than 6 cores?&lt;/p&gt;
&lt;p&gt;There are processors with more cores — from both Intel and AMD — but I don't think they would be better for what's important to me: Lightroom and gaming. Two uses that traditionally prefer higher clocks and don't make good use of too many cores. Typically when you get a processor with more cores, the lower the clock speed per core.&lt;/p&gt;
&lt;p&gt;For example, each core in the ridiculous $2,000 18-core Intel Core i9 7980XE has a mere base clock speed of 2.6GHz but with a Turbo Boost (v3.0) up to 4.4GHz. There's some extra clarification to be made here: Turbo Boost does not mean every core gets that speed. In this example, only two cores get 4.4GHz. If there was some magical processor that had a ton of cores where each core had a very high clock speed as well, then the case may be different.&lt;/p&gt;
&lt;p&gt;This does also apply to the 8700K. While the Turbo Boost is listed at 4.7GHz that's only for one core. If the computer decides two cores should be boosted, then they are each at 4.6GHz. That continues down to all cores running at 4.3GHz with Turbo Boost. Compare to the 7700K that has a Turbo Boost of 4.5GHz for one core and 4.4GHz for all four cores. So yes, the comparable all core Turbo Boost speed of the 8700K is slightly slower than the 7700K chip.&lt;/p&gt;
&lt;p&gt;Then why did I get the 8700K if that's the case? Because I'm going to overclock the heck out of all cores on the 8700K — and a bit of a spoiler but I got a good chip and was able to overclock higher with the 7700K.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-10&quot; id=&quot;r10&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; And as a nice secondary benefit the extra two cores means I have better performance for applications that make good use of multiple cores and many threads, like Premiere Pro.&lt;/p&gt;
&lt;p&gt;I would try to explain more of the current landscape of Intel processors... but it would take way too long to even begin to explain and it doesn't really matter.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-11&quot; id=&quot;r11&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h4&gt;CPU cooling&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;Corsair Hydro H115i AIO liquid cooler&lt;/p&gt;
&lt;p&gt;As I mentioned above, all-in-one liquid cooling options are affordable and highly performant alternatives to creating your own water-cooling loop, not to mention the related hassle and maintenence. These systems are easy to use, as long as your case is large enough to support the radiator size you want.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02723-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.0&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02723-1000.jpg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02735-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.0&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02735-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;39.901137800253&quot;&gt;&lt;p&gt;I picked for one of the larger ones: the Corsair H115i has dual 140mm fans for its 280mm radiator. Corsair recently released a newer version called the &lt;a href=&quot;https://www.amazon.com/Corsair-Radiator-Advanced-Lighting-CW-9060032-WW/dp/B077G3C6HH/ref=as_li_ss_tl?ie=UTF8&amp;amp;qid=1515996230&amp;amp;sr=8-1&amp;amp;keywords=h115i+pro&amp;amp;linkCode=ll1&amp;amp;tag=paulstamatiou-20&amp;amp;linkId=65d2ab01cb4357add6790ed6ceb082d0&quot; title=&quot;Corsair Hydro Series H115i PRO RGB 280mm Radiator Dual 140mm ML Series PWM Fans Advanced RGB Lighting Liquid CPU Cooler&quot;&gt;H115i PRO RGB&lt;/a&gt; but the main differences seem to be a pump head with RGB LED lights and mag lev fans that aim to be quieter (which I will probably order separately and swap out my current fans).&lt;/p&gt;
&lt;p&gt;While I got the largest and easiest to use Corsair 280mm AIO system, there are a few other options if you're feeling more adventurous and wish to build a custom loop or have a larger case and want something more performant. If you plan to watercool your graphics card and don't want to have to deal with placing a second radiator from an AIO kit, a custom loop is a good way to go.&lt;/p&gt;
&lt;h4&gt;Motherboard&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;ASUS ROG Maximus X Hero&lt;/p&gt;
&lt;p&gt;Aside from the basic need that it support LGA1151 processors and used the Intel Z370 chipset, I had a few requirements when I began searching for the motherboard:&lt;/p&gt;
&lt;ul readability=&quot;10.5&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;strong&gt;Onboard 802.11ac Wi-Fi:&lt;/strong&gt; Because I really don't want to have to get an additional card to add Wi-Fi capability.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;strong&gt;Two M.2 slots:&lt;/strong&gt; I didn't want to go the route of a traditional SATA SSD with this build and wanted to go with a tiny and speedy M.2 card slot SSD. As for why I wanted two — more on that in the section below.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;strong&gt;First-class overclocking support:&lt;/strong&gt; There are a few things I like to see in a board I plan to overclock with, even a bit:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Large built-in heatsinks over vital parts of the chipset, especially the power management components above and to the left of the CPU socket.&lt;/li&gt;
&lt;li&gt;An easy way to reset or diagnose why the computer doesn't boot (external rear restart buttons are nice, as well as an onboard display to indicate error codes).&lt;/li&gt;
&lt;li&gt;A solid UEFI BIOS that lets me control everything related to overclocking. While modern motherboards also have companion Windows software to let you control this on the fly, it's nice to have more control in the UEFI BIOS itself.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li readability=&quot;3&quot;&gt;
&lt;p&gt;&lt;strong&gt;Aesthetics:&lt;/strong&gt; Definitely further down on the list of wants, but I'd like something fairly discrete without a ton of bright red RAM and PCIe slots. Nowadays everything (motherboards, graphics cards...) has a ton of LEDs on it but fortunately they are all customizable so I can turn them off.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;strong&gt;Full ATX form factor:&lt;/strong&gt; Adequately spaced PCIe slots to accommodate large graphics cards with non-standard height coolers and future expansion cards I may plug in.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;4&quot;&gt;
&lt;p&gt;&lt;strong&gt;Strong, reinforced PCIe slot:&lt;/strong&gt; Graphics cards are so heavy these days with massive coolers that I'm always worried I'm going to damage the PCIe slot with all the weight. While I plan on getting a graphics card brace to help with this, it would definitely be nice to have if the motherboard had a stronger PCIe &quot;SafeSlot&quot; as Asus calls theirs.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;This may sound like a laundry list but if we're talking about fairly high-end boards, there are a lot that meet these needs. I ended up going with &lt;a href=&quot;https://rog.asus.com/articles/maximus-motherboards/rog-introduces-new-z370-gaming-motherboards-for-coffee-lake/&quot;&gt;one of Asus' many Z370 options&lt;/a&gt;. In fact, the one I went with was the lowest end model of this high-end ASUS ROG Maximus line that caters to the gaming and overclocking crowd.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-12&quot; id=&quot;r12&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;If you're looking for something with similar functionality but a few less boxes checked, any Z370 motherboard from the enthusiast Asus ROG Strix line below this Maximus line would be a solid choice.&lt;/p&gt;
&lt;h4&gt;Graphics&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;ASUS ROG STRIX Nvidia GeForce GTX 1080 Ti&lt;/p&gt;
&lt;p&gt;For the graphics card, there was really no beating the just-released (at the time) Nvidia GTX 1080 Ti. The much more expensive $1,200 Titan Xp came out shortly after but had marginally better performance (around 5-7%) — gains that could mostly be achieved by mildly overclocking the GTX 1080 Ti. And as I had mentioned earlier, Lightroom and other Adobe applications I use frequently are more optimized for Nvidia cards at this time.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-13&quot; id=&quot;r13&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p class=&quot;larger&quot;&gt;You might be thinking.. holy crap, $750+ for a graphics card!?! That's almost double the price of the CPU.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02661-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.607717041800643&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02661-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02671-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.607717041800643&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02671-1000.jpg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02679-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.3333333333333333&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02679-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;48.952941176471&quot;&gt;&lt;p&gt;First off, this card gets you solid VR and 4K-and-beyond gaming, and should continue doing its job well into the next generation of VR gear. With a clock speed nearing 1.6GHz, 11GB of GDDR5X VRAM, 3,584 CUDA cores, 11.3 teraflops and a whopping thermal dissipation of around 250W (more than double the i7 8700K CPU's TDP), the GTX 1080 Ti is a beast. If you care more about the details, this &lt;a href=&quot;https://www.anandtech.com/show/11180/the-nvidia-geforce-gtx-1080-ti-review&quot; title=&quot;The NVIDIA GeForce GTX 1080 Ti Founder's Edition Review: Bigger Pascal for Better Performance&quot;&gt;Anandtech review&lt;/a&gt; should be more than enough.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-14&quot; id=&quot;r14&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;High-end graphics cards have matured significantly since the last time I purchased one for a build. With their ridiculous number of cores excelling at highly parallelizable tasks, modern graphics cards have also found a life beyong gaming with the rise of general-purpose computing tasks on GPUs (GPGPU) like cryptocurrency mining.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-15&quot; id=&quot;r15&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;However, knowing I wanted the GTX 1080 Ti wasn't enough. I had to pick out which of the many different models I wanted. I knew I wanted to avoid the standard &quot;blower&quot; type reference design cards. The single-fan blower style cards tend to be fairly loud and lack some ideal thermal characteristics that I'd want for such a card, especially one I will overclock. I ended up with this triple fan ASUS model that also featured a huge heatsink requiring a 2.5-slot height. There are way more options for GTX 1080 Ti cards at the time of publishing compared to when I started building this PC; for the most part look for anything with a huge cooler and you should be good.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;There's one caveat though.&lt;/strong&gt; The GeForce line of graphics cards don't output 10-bit color graphics (40-bit RGBA) to a 10-bit monitor unless you're in a DirectX 11 fullscreen mode, which is basically only for gaming. It seems that Nvidia blocks their consumer line of cards from outputting 10-bit color for professional applications and prefers that you buy a card from their &lt;em&gt;much&lt;/em&gt; more expensive Quadro line (the top Quadro cards range in price from $2,000 to $7,500!).&lt;/p&gt;
&lt;h6&gt;Why get only one card?&lt;/h6&gt;
&lt;p&gt;I initially considered going for a dual-card SLI setup. After some research I discovered two things:&lt;/p&gt;
&lt;p&gt;As such, it doesn't seem worth pursuing a dual-card setup. I can already game at 4K 60fps no problem with this single GTX 1080 Ti. However, I expect the need for and adoption of SLI support from developers to change as the need for even more performance grows with future high-resolution, 240Hz DisplayPort 1.X+ monitors and VR HMDs.&lt;/p&gt;
&lt;h4&gt;Storage&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;Dual 1TB Samsung 960 EVO M.2 SSDs&lt;/p&gt;
&lt;p&gt;I had heard so much about the crazy performance of these tiny new PCIe NVMe M.2 SSDs that I had to try one. Some of the latest high-end M.2 SSDs boast speeds more than 3-5x faster compared to their SATA counterparts.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7705-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.3157894736842106&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7705-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;960 EVO at home in its M.2 card slot.&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;60.522064466616&quot;&gt;&lt;p class=&quot;larger&quot;&gt;But first.. what the heck does PCIe NVMe M.2 mean?&lt;/p&gt;
&lt;ul readability=&quot;6.5&quot;&gt;&lt;li readability=&quot;4&quot;&gt;
&lt;p&gt;&lt;strong&gt;PCIe:&lt;/strong&gt; The high-speed serial expansion bus that connects to a bunch of peripherals like graphics cards and some types of storage (not SATA). You might have heard about a CPU/chipset supporting a particular number of PCIe lanes. That roughly refers to how much bandwidth (each lane equates to 4 physical wires — two to send, two to receive) a particular device may require. For example, a modern graphics card usually wants a PCIe x16 slot to get 16 lanes for more bandwidth while current M.2 SSDs only require 4 lanes.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;strong&gt;NVMe:&lt;/strong&gt; Short for NVM Express (which is short for something even longer), NVMe is just a specification for interfacing with non-volatile storage attached via PCIe. It's like an API for these new SSDs. Previously, PCIe-attached SSDs had their own custom way to talk to the chipset and that lead to requiring custom drivers. NVMe is now the standard and was designed with SSDs in mind, compared to the precursor protocol AHCI that was made with spinning disks in mind.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;strong&gt;M.2:&lt;/strong&gt; And this is simply the name of the connector for the expansion card itself. You might see them called M.2 2242 or 2280. That refers to the length of the card: 22mm wide and either 42mm or 80mm long.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;When I began researching M.2 SSDs for this build, there seemed to be only two options when it came to no-holds-barred performance: the Samsung 960 EVO and the Samsung 960 PRO. The EVO model uses TLC V-NAND with some smart uses of two kinds of SLC caches to increase performance. The PRO model on the other hand uses superior MLC V-NAND flash memory.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-16&quot; id=&quot;r16&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Despite the difference in NAND types used between the 960 PRO and 960 EVO, the performance isn't too dissimilar (likely thanks to the EVO's great use of SLC caching):&lt;/p&gt;
&lt;ul readability=&quot;2&quot;&gt;&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;&lt;strong&gt;960 PRO —&lt;/strong&gt; 3,500 MB/s seq. read, 2,100 MB/s seq. write&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;&lt;strong&gt;960 EVO —&lt;/strong&gt; 3,200 MB/s seq. read, 1,900 MB/s seq. write&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;I went with the 960 EVO to save a bit of money given that I probably would not be able to tell the difference between those two in terms of speed. I was not concerned with lifespan as I would likely upgrade long before I saw any diminishing performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I got two 1TB EVO 960's.&lt;/strong&gt; I started this build thinking I would have a dual-boot Windows 10 and macOS hackintosh machine. I ended up deciding against that for a variety of reasons, including having to limit my initial hardware choices to only things that would be friendly for a hackintosh setup. Then I thought maybe I would just RAID 0 the two SSDs but decided against that (more on that later). I just ended up making one a dedicated scratch disk for Lightroom to store photos I'm currently working on. It feels safer that way in case I do something that somehow nukes my main OS drive (though I always have the photos backed up to the NAS and Backblaze so it wouldn't matter much).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A note about 3D XPoint:&lt;/strong&gt; Intel has a promising new type of memory technology called 3D XPoint memory that they have started selling under &lt;a href=&quot;https://www.anandtech.com/show/12136/the-intel-optane-ssd-900p-480gb-review/2&quot; title=&quot;The Intel Optane SSD 900p 480GB Review: Diving Deeper Into 3D XPoint&quot;&gt;their new Optane SSD brand&lt;/a&gt;. It's really expensive for the time being but very fast and something to keep an eye on in the future.&lt;/p&gt;
&lt;p class=&quot;larger&quot;&gt;Uh, this is a Lightroom PC and you only have 2TB of storage??&lt;/p&gt;
&lt;p&gt;If I didn't have &lt;a href=&quot;https://paulstamatiou.com/storage-for-photographers-part-2/&quot; title=&quot;Storage for Photographers (Part 2) -- How a 12TB Synology NAS changed my digital life&quot;&gt;my 12TB Synology NAS&lt;/a&gt; to archive photos I was done editing, I would have opted to also get large internal mechanical hard drive like this &lt;a href=&quot;https://www.amazon.com/dp/B01MQWHTBE/ref=as_li_ss_tl?_encoding=UTF8&amp;amp;psc=1&amp;amp;linkCode=ll1&amp;amp;tag=paulstamatiou-20&amp;amp;linkId=33d665dd42c90f7a0362ccd20207b781&quot; title=&quot;WD Black 6TB Performance Desktop Hard Disk Drive - 7200 RPM SATA 6 Gb/s 128MB Cache 3.5 Inch &quot;&gt;Western Digital Black series drive&lt;/a&gt; if I was only going to use it alone. If I was going to use it in a RAID array, I'd get several of the &lt;a href=&quot;https://www.amazon.com/3-5-Inch-7200rpm-128MB-Internal-WD6002FFWX/dp/B01CHP20MG/ref=as_li_ss_tl?s=electronics&amp;amp;ie=UTF8&amp;amp;qid=1510444537&amp;amp;sr=1-2&amp;amp;keywords=western+digital+red+pro&amp;amp;dpID=51qT5x3noHL&amp;amp;preST=_SY300_QL70_&amp;amp;dpSrc=srch&amp;amp;th=1&amp;amp;linkCode=ll1&amp;amp;tag=paulstamatiou-20&amp;amp;linkId=432464e7bb591b6b302e61b4336c535b&quot; title=&quot;WD Red Pro drive for NAS/RAID&quot;&gt;WD Red Pro&lt;/a&gt; or &lt;a href=&quot;https://www.amazon.com/Seagate-IronWolf-3-5-Inch-Internal-ST8000VN0022/dp/B01M4FU8Y3/ref=as_li_ss_tl?ie=UTF8&amp;amp;qid=1516000017&amp;amp;sr=8-2&amp;amp;keywords=seagate+ironwolf&amp;amp;th=1&amp;amp;linkCode=ll1&amp;amp;tag=paulstamatiou-20&amp;amp;linkId=e9a8dab4e2d3c84015dd8a0617bbfa88&quot; title=&quot;Seagate 10TB IronWolf Pro 7200RPM SATA 6Gb/s 256MB Cache 3.5-Inch NAS Hard Disk Drive &quot;&gt;Seagate IronWolf Pro&lt;/a&gt; drives (they have NAS/RAID specific features like TLER).&lt;/p&gt;
&lt;h4&gt;RAM&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;2 x 16GB G.SKILL Trident Z DDR4-3200 CL14&lt;/p&gt;
&lt;p&gt;I feel like RAM is an often overlooked piece of vital computer hardware for all but the more experienced computer enthusiasts. When it comes to RAM it's not just about picking enough so that your applications have enough room to play and don't need to unncessarily keep paging to your SSD.&lt;/p&gt;
&lt;p&gt;You at least need good enough RAM so you don't have stability issues. Bad RAM can lead to a myriad of stability issues and odd computer behavior. I will just reiterate that this is not an area you want to cheap out on. Unfortunately, RAM prices are so high these days&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-17&quot; id=&quot;r17&quot;&gt;17&lt;/a&gt;&lt;/sup&gt; it's actually pretty hard to &quot;cheap out&quot; in this space.&lt;/p&gt;
&lt;p&gt;I began by &lt;strong&gt;looking for only 2 sticks of RAM&lt;/strong&gt; instead of 4 for a few reasons. First, I plan to overclock a bit so I wanted to only use 2 sticks to reduce strain on the integrated memory controller. Second, the Z370 chipset on this motherboard only supports dual channel so there would be no performance benefit going with 4 sticks. Not to mention the extra heat created with 4 sticks crammed right next to each other.&lt;/p&gt;
&lt;p&gt;I wanted two low latency matched 16GB ram sticks for for a total of 32GB. While I definitely wouldn't mind having more RAM, 32GB is more than sufficient for my needs and performance is a higher concern for me. And well, it's really not possible to find really fast, low latency RAM in anything larger than 16GB sticks; even that is a challenge. The very high speed RAM kits tend to only come in 8GB sticks.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7219-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.4245014245014245&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7219-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;104.03362831858&quot;&gt;&lt;p class=&quot;larger&quot;&gt;When it comes to RAM, there's a lot more to look at beyond just the number of gigabytes. Speed and latency play a very large and interconnected role.&lt;/p&gt;
&lt;p&gt;Overclocked RAM can have a sizable performance improvement when it comes to frames per second while playing some CPU-bound games. The performance variance for general system tasks &lt;a href=&quot;https://www.techpowerup.com/reviews/Intel/Core_i7_8700K_Coffee_Lake_Memory_Performance_Benchmark_Analysis/10.html&quot; title=&quot;Intel i7-8700K Coffee Lake Memory Benchmark Analysis&quot;&gt;seems to be much less pronounced&lt;/a&gt; on an Intel machine: Intel machines are much less picky with RAM than AMD Ryzen machines.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-18&quot; id=&quot;r18&quot;&gt;18&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The Intel Core i7 8700K with a Z370 motherboard supports a Coffee Lake DDR4 reference speed of 2666MHz. However, even if you have DDR4-2666 or faster installed, you won't get this speed out of the box without any configuration. It will run at 2133MHz due to the base JEDEC DDR4 specification. Fortunately, all you have to do is enable a memory setting in the UEFI settings called XMP (Extreme Memory Profile) — this will automatically bring your memory up to their rated speed and memory timings, adding a bit more voltage if necessary.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-19&quot; id=&quot;r19&quot;&gt;19&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;It's rather easy these days to overclock RAM on its own, separate from any CPU overclock. While the performance benefits on an Intel system outside of gaming probably don't make it worth your while to go overboard with extremely pricey RAM, just setting your RAM to its rated XMP settings can get you on your way quickly.&lt;/p&gt;
&lt;p&gt;When I was overclocking long ago, the memory controller resided in the northbridge chip of the chipset and overclocking was frequently done by just increasing the front-side bus speed (in addition to the CPU multiplier if it was unlocked for the CPU) that links the processor and RAM, usually with a 1:1 memory divider if it would work. With modern Intel machines the memory controller resides inside the processor itself and it's much less common to overclock the base clock (BCLK) when RAM speed can be manipulated entirely on its own easily. Well that and because BCLK overclocking is tricky and can easily cause system-wide instability from RAM to devices on the PCIe bus.&lt;/p&gt;
&lt;h6&gt;Nerdy bits about RAM frequency and latency&lt;/h6&gt;
&lt;p&gt;When you shop for RAM, you typically see 3 things: size in GB, speed in MHz (like DDR4-3200 for 3200MHz) and finally latency or timings, typically shown with four numbers like 14-14-14-34. You may also see the latency listed as a CAS latency or CL value, that simply refers to the first and most important number for us of those four timing numbers.&lt;/p&gt;
&lt;p&gt;In general, faster speeds and lower latencies are always better. But they are interconnected when it comes time to measure the absolute latency. Lets talk about what that means.&lt;/p&gt;
&lt;p&gt;CAS latency (CL) does not represent a time value. Rather, it refers to the number of clock cycles it takes from the time the CPU (well, integrated memory controller inside the CPU to be more accurate) requests some data from the RAM to the time the RAM can supply that data back. For example, RAM with a CAS latency of 14 will take 14 clock cycles to return that data and CL16 RAM at the same speed would take 2 more cycles to get the same number of operations done.&lt;/p&gt;
&lt;p&gt;So what is this clock cycle? The frequency at which the RAM operates is the number of operations per second the RAM can achieve. In the case of 3200MHz DDR4 RAM this is 3.2 billion cycles per second. A single cycle is the smallest unit of time a computer can recognize. There's one more wrinkle in this: we're talking about DDR, which stands for Double Data Rate. This kind of modern RAM processes 2 piece of data per cycle, so the DDR4-3200 we've been talking about is actually only clocked at 1600MHz but effectively operates at 3200MHz.&lt;/p&gt;
&lt;p&gt;Now that we know how latency and frequency are related, we can begin to calculate absolute latency and see how it varies as RAM frequency increases.&lt;/p&gt;
&lt;p&gt;Let's say we have DDR4-2666 RAM with a CAS latency of 14. First, we need to use the true data rate, which is half of 2666MHz. To get the absolute latency we plug it into this: &lt;code class=&quot;inline&quot;&gt;1/(data rate/2) * CAS latency&lt;/code&gt;. That would be &lt;code class=&quot;inline&quot;&gt;1/(1333MHz) * 14&lt;/code&gt; = &lt;code class=&quot;inline&quot;&gt;10.5 nanoseconds&lt;/code&gt; to complete an operation requested by the CPU.&lt;/p&gt;
&lt;p&gt;If you run a few different types of RAM through that equation, you can see the difference in absolute latencies. I only plotted a few RAM speeds and latencies in there, but it's possible to buy RAM at frequencies as high as &lt;a href=&quot;https://www.anandtech.com/show/12079/gskill-launches-ddr4-4000-32gb-dimm-kits&quot; title=&quot;G.Skill&quot; shrinks=&quot;&quot; latencies=&quot;&quot; of=&quot;&quot; kits=&quot;&quot; for=&quot;&quot; coffee=&quot;&quot; lake:=&quot;&quot; ddr4-4266=&quot;&quot; cl17=&quot;&quot;&gt;DDR4-4266 CL17&lt;/a&gt; (as far as I've seen).&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-20&quot; id=&quot;r20&quot;&gt;20&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;*&lt;/strong&gt;While I wasn't able to find RAM for sale at those CAS latencies, it might be possible to increase voltage and overclock the RAM to achieve some of those lower latencies. For example, I ended up running my DDR4-3200 CL14 at 3333 CL14.)&lt;/p&gt;
&lt;p&gt;As you can see here, having the lowest latency doesn't mean much when it's not referring to absolute latency, which takes into account the number of cycles happening per second. Makes sense — the faster it's going, the less time each individual cycle takes, so at some point much faster RAM can make up for slightly higher CL timings. Here's another example: DDR4-2400 CL12 has the same 10ns absolute latency as DDR4-3200 CL16.&lt;/p&gt;
&lt;p&gt;Does this mean you're better off getting cheaper RAM and overclocking it to the desired speed? Well, not quite. First off, there is no guarantee that your cheaper 2400MHz RAM could actually reach an overclock like 3200MHz. It might 1) not be possible, 2) require extra voltage, or 3) only be possible with significantly loosened higher CL timings, which defeats the purpose. As such, it's a good idea to always get the lowest latency RAM you can find, even if you plan to run it overclocked with higher timings. Better to get CL14 RAM and run it at CL15 or CL16 when overclocked much higher, than get CL16 RAM at the same speed but only be able to get it to work overclocked at CL18 or higher.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02740-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.5479876160990713&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02740-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;32.479338842975&quot;&gt;&lt;p&gt;For all those reasons above, &lt;strong&gt;I ended up going with 2x16GB DDR4-3200 CL14 RAM&lt;/strong&gt;. It's among the highest frequency and lowest latency RAM you can find.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-21&quot; id=&quot;r21&quot;&gt;21&lt;/a&gt;&lt;/sup&gt; This should give me solid headroom to overclock the RAM past 3200MHz and still have a low CL even if I have to loosen it up a bit. In addition, this G.SKILL RAM uses Samsung's B-die chips which are reputable for their performance and overclocking ability.&lt;/p&gt;
&lt;p&gt;And as a minor point, I was looking for RAM that would feel more at home in my mostly black PC and wasn't some obnoxious bright color.&lt;/p&gt;
&lt;h4&gt;PSU&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;Corsair AX860&lt;/p&gt;
&lt;p&gt;Back when I was just getting into building computers some 15+ years ago, power supplies felt like they were largely overlooked by the DIY computer building community. The thinking was something like: just get something that's 300 watts or so with a big, heavy heatsink and a fan that isn't too loud and you were probably good to go.&lt;/p&gt;
&lt;p&gt;In reality, the power supply is one of the most important parts of a stable and performant rig. Cheaping out on the power supply can result in random computer stability issues and restarts. In a worst case scenario a bad PSU could damage or even kill some of your components. The criteria for picking a good power supply has become a bit more stringent with power hungry graphics cards (modern graphics cards can use much more power than the CPU) in the last few years.&lt;/p&gt;
&lt;p&gt;In addition, overclocking is no longer some mystical dark art — motherboard manufacturers cater exclusively to this crowd with high quality capacitors, PWMs and VRMS along with UEFIs featuring comprehensive voltage and control settings for just about everything.&lt;/p&gt;
&lt;p&gt;When it came time to pick my power supply, I looked at a few things in particular:&lt;/p&gt;
&lt;ul readability=&quot;14.599211563732&quot;&gt;&lt;li readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;strong&gt;Quiet:&lt;/strong&gt; These days it's easier to find a PSU with a larger single fan (120 to 140mm in size) instead of the louder dual 80mm fans you used to find in power supplies. However, some more advanced power supplies have what they call a zero RPM mode — they don't even need to spin the PSU fan(s) until the load reaches some percentage of total output. Even then the fan only speeds up incrementally as needed. As such, it might be worth getting a more powerful PSU than you need, just so you can stay closer to that zero RPM mode with your regular idle/light use load.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;9.8932926829268&quot;&gt;
&lt;p&gt;&lt;strong&gt;Efficient:&lt;/strong&gt; Again, fairly common these days to find PSUs with the &lt;a href=&quot;https://en.wikipedia.org/wiki/80_Plus&quot;&gt;80 Plus&lt;/a&gt; efficiency designation, of which there are now a bunch of tiers: bronze, silver, gold, platinum and titanium. The higher the efficiency of the PSU, the less power that turns into heat instead of becoming DC current for your computer. This usually also means less heat that the PSU needs to deal with and pump out of your case. For example, if you have a 1000W PSU with 80% efficiency, then your PSU will likely pull around 1250W from the wall outlet to generate the 1000W peak output. That'll cost you extra on your electricity bill compared to a PSU with a higher efficiency rating.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;3.9839034205231&quot;&gt;
&lt;p&gt;&lt;strong&gt;Fully modular:&lt;/strong&gt; A fully modular PSU has fully detachable cables. So if you have only a few interal drives and peripherals, you just plug in the cables you need and don't have to worry about where to hide the unused cables inside your case. This makes cable management much, much easier. And also improves airflow from having fewer cables obstructing it. Just make sure you're getting a PSU with enough connections (and wattage) to support the number of devices you need to power inside your case.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-22&quot; id=&quot;r22&quot;&gt;22&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;4.538188277087&quot;&gt;
&lt;p&gt;&lt;strong&gt;Sufficient wattage with a bit of headroom:&lt;/strong&gt; The best way to figure out how much wattage your build will need is with &lt;a href=&quot;http://www.coolermaster.com/power-supply-calculator/&quot; title=&quot;How much wattage I need in my PSU for the PC build? This tool will help you to select a suitable power supply unit for your system.&quot;&gt;a power supply calculator&lt;/a&gt;. You select your exact parts and it'll estimate max load wattage and provide a recommendation (about 10% more wattage). The nice thing about this calculator in particular is that it lets you estimate usage if you overclock your CPU and GPU as well. In my case, it said my rig would use close to 600W when overclocked a bit.&lt;/p&gt;
&lt;p&gt;There's also the whole &lt;a href=&quot;http://www.jonnyguru.com/forums/showthread.php?t=3990&quot;&gt;single vs multiple 12V rail&lt;/a&gt; discussion to be had if you're really curious.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02714-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.4285714285714286&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02714-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;18.478090255069&quot;&gt;&lt;p&gt;After a bit of research I ended up going with the &lt;strong&gt;Corsair AX860&lt;/strong&gt;. It has the zero RPM fan mode I was talking about, is fully modular and has an 80 Plus Platinum rating. Corsair also has AX860i model that has more functionality (there's a desktop app to control it) but there are some mixed reviews about fan issues and buggy software so I decided to avoid it.&lt;/p&gt;
&lt;p&gt;At 860W this PSU is considerably more powerful PSU than I need right now. I opted for something like this to provide enough headroom for high CPU and GPU overclocks and to future proof myself a bit in case I ever decide to add a second graphics card or do something crazy like upgrade to an overclocked 10+ core processor with a much higher TDP. Had that not been the case I could have gone with a power supply in the 650-750W range.&lt;/p&gt;
&lt;h4&gt;Keyboard &amp;amp; mouse&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;Apple Magic Keyboard, Logitech MX Master 2S &amp;amp; Evoluent VerticalMouse 4&lt;/p&gt;
&lt;p&gt;I've more or less always used and loved Apple keyboards and the slim new Magic Keyboard is no exception. It's bluetooth and it's possible to configure it to work as expected with Windows 10. The new Magic Keyboard has keys with limited travel and some folks may not feel comfortable typing on it. You'll have to try it for yourself. One thing is for certain though: I absolutely hate loud clicky-style mechanical keyboards with long key travel. So no keyboards with Cherry MX mechanical switches for me.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8376-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.4598540145985401&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8376-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;16.288288288288&quot;&gt;&lt;p&gt;As for the two mice, I often switch between a regular mouse and a vertical mouse to allay some RSI wrist pain from time to time. More detail on my &lt;a href=&quot;https://paulstamatiou.com/stuff-i-use/&quot;&gt;Stuff I use&lt;/a&gt; page.&lt;/p&gt;
&lt;h4&gt;Speakers&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;Bose SoundLink Mini II&lt;/p&gt;
&lt;p&gt;This one probably seems the most out of place compared to everything else on this list. Yes, it's a tiny portable speaker that I'm using for my desktop computer. I just didn't want a large multiple speaker setup taking up space on my desk, especially one requiring some bulky power adapter and multiple cables.&lt;/p&gt;
&lt;p&gt;It's only for light use like watching videos on the web, basic Spotify background music or casual gaming. — I have a much larger and powerful Sonos system for when I really want to play music. And as for sound while gaming, that's not a priority for me. I make due with just this or plugging in headphones.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8264-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.639344262295082&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8264-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;7.531984691088&quot;&gt;&lt;p&gt;There's a newer model of the SoundLink but it's not directional and didn't seem like what I wanted. The SoundLink Mini II is tiny but packs a good punch and can be powered via micro-USB and connect via bluetooth or a standard 3.5mm audio aux cable. I have a micro-USB cable hidden under a cable management shelf under my desk that I can pull out whenever I need to charge this or my Logitech MX Master 2S mouse.&lt;/p&gt;
&lt;h4&gt;Operating System&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;Microsoft Windows 10 Home, USB flash drive&lt;/p&gt;
&lt;p&gt;Nothing much to say here, Windows 10 Home. I have little use for any of the features included in Windows 10 Pro.&lt;/p&gt;
&lt;hr class=&quot;full&quot;/&gt;&lt;/section&gt;&lt;section class=&quot;section-title&quot; id=&quot;the_display&quot;&gt;&lt;h2&gt;&lt;span&gt;The display&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;Dell UP2718Q 27&quot; 4K display&lt;/h3&gt;
&lt;/section&gt;&lt;section readability=&quot;77.760583678616&quot;&gt;&lt;p&gt;I was definitely spoiled coming from a 5K iMac. It has a stellar display with great color accuracy and incredible brightness. They even tossed out the industry's way of advertising monitors based on sRGB or Adobe RGB color space accuracy and have instead slightly adapted the DCI-P3 color space, originally meant for projectors, to be used for device displays and called it Display P3. Apple is seemingly ahead of the game here (well, it's &lt;a href=&quot;http://www.colourspace.xyz/the-new-apple-imac-and-the-dci-p3-colour-gamut/&quot; title=&quot;The New Apple iMac and the DCI P3 colour gamut&quot;&gt;debated if P3 is good move for consumers&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I had a daunting challenge ahead of me if I was to find a quality replacement for that display.&lt;/p&gt;
&lt;p class=&quot;larger&quot;&gt;I'm &lt;em&gt;not&lt;/em&gt; a professional photographer. I'm merely a hobbyist.&lt;/p&gt;
&lt;p&gt;That means I don't get paid to take photos or do anything where color accuracy is mission critical, such as shooting and post-processing portraits with accurate skin tones or working with print. That means I don't care quite enough to use a pricey 10-bit Nvidia Quadro graphics card paired to a 10-bit professional monitor with internal LUTs for calibration with great homogeneity and extremely high color accuracy across the board like some of the &lt;a href=&quot;http://www.necdisplay.com/p/desktop-monitors/pa322uhd-bk-sv&quot; title=&quot;32-inch UHD color accurate desktop monitor w/ SpectraViewII&quot;&gt;NEC PA MultiSync models&lt;/a&gt; or &lt;a href=&quot;http://www.eizo.com/products/coloredge/coloredge_cg.html/&quot; title=&quot;Eizo color management monitors - premium line for professionals in photography&quot;&gt;Eizo ColorEdge displays&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It also means I don't plan to buy and meticulously use a color calibration device. That means I don't typically share my images with others in large lossless formats. That means I don't care to limit myself to a 1920x1080 or 2560x1440 resolution display for increased Lightroom performance.&lt;/p&gt;
&lt;p&gt;I, on the other hand, spend my time publishing my photos online. I will knowingly compress and sacrifice a good bit of image quality to make it easier for people to load my shots in a photoset. I know people view my compressed shots on a myriad of displays and devices where photos could look slightly different than how I might have intended. I also don't quite care enough to embed file-size-increasing ICC color profiles and save different versions of my files and &lt;a href=&quot;https://webkit.org/blog/6682/improving-color-on-the-web/&quot; title=&quot;Improving Color on the Web&quot;&gt;serve up an sRGB version or a wide-gamut version&lt;/a&gt; depending on the device.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-23&quot; id=&quot;r23&quot;&gt;23&lt;/a&gt;&lt;/sup&gt; Whatever, I'm fine with that. (Okay, I &lt;em&gt;do&lt;/em&gt; want to spend some time to research serving up wide-gamut images on my site at some point..)&lt;/p&gt;
&lt;h4&gt;So what am I looking for?&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;What matters for my hobbyist photography use&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;4K or 5K resolution:&lt;/strong&gt; Yes, I know this comes at the expense of Lightroom speed but I just love having more space.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Good color accuracy:&lt;/strong&gt; There's a lot of ways you can define good, but for me this just means something exceeding the sRGB color space and ideally covering a solid portion of either the the DCI-P3 or Adobe RGB color spaces.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-24&quot; id=&quot;r24&quot;&gt;24&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sub-10ms response time&lt;/strong&gt; for &lt;em&gt;occasional&lt;/em&gt; gaming&lt;/li&gt;
&lt;li&gt;Ability to use the display with both Macs and PCs easily&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;What this meant was that right off the bat I was &lt;strong&gt;not looking for fast 144Hz gaming monitors&lt;/strong&gt; that were not 4K, had a questionable physical appearance geared towards gamers and subpar color accuracy. Which is great because I won't have to describe Nvidia G-Sync and AMD FreeSync adaptive sync technologies meant to reduce screen tearing while gaming.&lt;/p&gt;
&lt;p&gt;At first I did a lot of &lt;strong&gt;searching for a 5K display&lt;/strong&gt; (did you know Dell has an &lt;em&gt;8K&lt;/em&gt; monitor out now too?). I was going to breakdown a list of all the current 5K monitors on the market and what was not good about each of them but I'll spare you the details.. I just don't think there are any exceptional 5K displays on the market at this moment that satisfy my above criteria. I talk about this in more detail below, but this is changing and it may be a good time to wait a bit longer. For example, LG has some 21:9 5K Nano IPS displays with great color space coverage coming out this year.&lt;/p&gt;
&lt;p&gt;We're in this weird time of a transition away from DisplayPort 1.2 to 1.3/1.4 so if you want to run 5K at 60Hz on a PC now, you're likely going to need to use two cables. And then there's the mixed bag of trying to output video via Thunderbolt on a PC for certain displays. For example, if you want to run the LG Ultrafine 5K monitor (the one made for Macs) at 5K 60Hz as intended, you have to use a &lt;a href=&quot;https://shantanujoshi.github.io/lg-5k-windows/&quot; title=&quot;Running the LG Ultrafine 5k on Windows 10 &amp;amp; Linux&quot;&gt;very particular motherboard and PCIe combination&lt;/a&gt; to passthrough the graphics to a valid Thunderbolt 3 signal. It's all just a hassle right now. I'll wait.&lt;/p&gt;
&lt;h6&gt;I got a cheap Dell 4K.. and I didn't like it.&lt;/h6&gt;
&lt;p&gt;Given the current state of 5K monitors and my primary use of just publishing sRGB photos on the web, I thought I would be fine with a placeholder 4K display for now. Just something to hold me over for a year or two until a great 5K display came out. I got the 27-inch 4K Dell P2715Q display for about $500 (the slightly updated &lt;a href=&quot;https://www.amazon.com/Dell-27-Inch-LED-lit-Monitor-U2718Q/dp/B073VYVX5S/ref=as_li_ss_tl?ie=UTF8&amp;amp;linkCode=ll1&amp;amp;tag=paulstamatiou-20&amp;amp;linkId=2fa9a18e6369dd171384070b6265c8ce&quot; title=&quot;Dell U Series 27-Inch Screen LED-lit Monitor (U2718Q)&quot;&gt;Dell U2718Q&lt;/a&gt; is its successor).&lt;/p&gt;
&lt;p&gt;It wasn't the most attractive monitor, didn't have the best brightness or even color space coverage. It only did about 79% accuracy for Adobe RGB. There did not seem to be many 4K displays with Adobe RGB coverage in the 90% range without going over the $1,000.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-25&quot; id=&quot;r25&quot;&gt;25&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Even though it seemed fine at first and definitely did exceed the sRGB color space, I began to want more control over my photos and be more future-proofed. In short: even if I end up converting and publishing to sRGB, I still want to see my photos as close to how they were captured as possible and have control over the proofing.&lt;/p&gt;
&lt;p&gt;More importantly: I think this notion that sRGB is the color space of the web is quickly changing for the folks that visit my website and tend to have Macs with wider gamuts than most or recent phones with OLED displays and so on. &lt;strong&gt;If I'm going to keep my display for 3-5 years, I should be ready to publish my photos with larger color profiles&lt;/strong&gt; in short order.&lt;/p&gt;
&lt;p class=&quot;larger&quot;&gt;I put the Dell P2715Q to the side and replaced it with the &lt;a href=&quot;https://www.amazon.com/Dell-Ultrasharp-LED-Lit-Monitor-UP2718Q/dp/B0728K6YVB/ref=as_li_ss_tl?ie=UTF8&amp;amp;linkCode=ll1&amp;amp;tag=paulstamatiou-20&amp;amp;linkId=04b8c9ff6839fb719a4be01e3146c0df&quot; title=&quot;Dell Ultrasharp 27-inch Screen LED-Lit Monitor Black (UP2718Q)&quot;&gt;Dell UltraSharp UP2718Q&lt;/a&gt;.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8244-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.4184397163120568&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8244-1000.jpg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8256-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.4347202295552368&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8256-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;I ended up with the $1500 Dell UP2718Q that I mounted on a Humanscale M8 VESA arm.&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;144.24370683894&quot;&gt;&lt;p&gt;At $1,500 this display is not cheap but as part of Dell's UltraSharp line, it has some great color accuracy: &lt;strong&gt;100% sRGB, Adobe RGB and Rec 709, along with 97.7% DCI-P3.&lt;/strong&gt; In addition it is a 10-bit panel, has HDR10 support and a ridiculous peak brightness of up to 1000 nits. And as a supremely nice benefit, it supports DDC/CI which means its settings can be controlled via the OS. Long story short, I was able to script it so that I could &lt;strong&gt;control the monitor brightness with my keyboard&lt;/strong&gt;.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-26&quot; id=&quot;r26&quot;&gt;26&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h6&gt;You don't need 10-bit right now&lt;/h6&gt;
&lt;p&gt;While this display does have a 10-bit (per color) panel, I'm not using it with a 10-bit graphics card. Yes, you can get way more colors with a 10-bit setup: 1.07B compared to 16.7M for 8-bit. But it's going to cost you, a lot.&lt;/p&gt;
&lt;p&gt;If you want to take advantage of 10-bit in applications like Lightroom in Windows, you have to use a &lt;em&gt;different&lt;/em&gt; graphics card such as one from the Nvidia Quadro line. They're pricey workstation cards, not gaming graphics cards. And once you have that, you need an even more pricey 10-bit display. If you find an affordable 10-bit display, it's probably not real 10-bit but something called 8-bit + FRC (Frame Rate Control) that fakes 10-bit output by flashing two colors very quickly to mimic another color it can't natively reproduce.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-27&quot; id=&quot;r27&quot;&gt;27&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h6&gt;Don't buy into the HDR hype yet&lt;/h6&gt;
&lt;p&gt;Do not buy into the current HDR hype with computer monitors. It's just a huge bunch of gotchas. Some displays like the cheaper Dell U2718Q (not the UP2718Q) that boast HDR functionality only have it work on the HDMI port and not the DisplayPort port.&lt;/p&gt;
&lt;p&gt;In Windows 10, HDR support has to be manually turned on and then all the colors go dull as colors get remapped to the Rec. 2020 color space and the display lowers the brightness. As such, it's not something you will use for your daily computing and only for a select few games and supported applications.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The state of HDR for your desktop is horrible right now.&lt;/strong&gt; But at least now there is a new &quot;DisplayHDR&quot; standard to help you identify what kind of display you're working with. Currently there are a few levels: DisplayHDR 400, 600 and 1000. The numbers refer to the minimum required nits for brightness and each level requires a minimum color space accuracy and global display dimming functionality. Next, we'll just need operating systems to allow for HDR to be automatic depending on the application.&lt;/p&gt;
&lt;h6&gt;What about OLED?&lt;/h6&gt;
&lt;p&gt;OLED displays in particular are very intriguing. Dell released their first 4K OLED display last year, albeit for $3,500. OLED displays bring a lot to the table: a ridiculously fast response rate, an insanely high contrast ratio and impressive color accuracy including very dark blacks. This seems to make OLED displays ideal running the gamut from gamers to creative professionals.&lt;/p&gt;
&lt;p&gt;That's the hope at least. OLED displays currently have issues with color shift over time as well as image burn-in (remember old plasma TVs? I definitely had got some burn-in on mine). There are some new technologies that aim to address those issues so we can only hope the tech matures in a few years. At the moment, there are not many affordable 4K or 5K displays on the market.&lt;/p&gt;
&lt;p&gt;At the same time, nascent Micro LED displays are an area to watch. Like OLED displays they require no separate backlight and have great brightness and contrast (Micro LED can be even brighter) with the benefit of no burn-in or decreased performance over time.&lt;/p&gt;
&lt;h4&gt;On color spaces&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;A primer on sRGB, Adobe RGB, P3 and more...&lt;/p&gt;
&lt;p&gt;I've mentioned color spaces a few times now so let me provide a somewhat brief description of what they are and what to look for in a monitor. The best way to describe it is probably to start off by showing you this chart. The colorful horseshoe shape behind everything represents the range of colors visible to humans. This is one of the more popular chromaticity diagrams called the CIE 1931 color space. There are a &lt;a href=&quot;https://en.wikipedia.org/wiki/CIELUV&quot;&gt;few of these&lt;/a&gt; that calculate things differently based on things like the light source, but the purpose is the same.&lt;/p&gt;
&lt;div class=&quot;center&quot;&gt;&lt;img alt=&quot;CIE1931 color space comparisons: ProPhoto RGB, Rec. 2020, Adobe RGB, DCI-P3, sRGB&quot; src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-cie1931-custom-chart.jpg&quot;/&gt;&lt;/div&gt;
&lt;p&gt;The triangles shown on top of this visible spectrum represent other color spaces. When it comes to talking about a display or photo-editing workflow, &lt;strong&gt;gamut&lt;/strong&gt; is the range of a certain color space that can be reproduced. &lt;strong&gt;There is no display in the world that can reproduce every color humans can see.&lt;/strong&gt; Gamut refers to the range of colors that can be displayed, not that can be observed. So as much as I would like to plot a high-end camera on this diagram, it &lt;a href=&quot;http://www.color-image.com/2012/08/a-digital-camera-does-not-have-a-color-gamut/&quot;&gt;doesn't work like that&lt;/a&gt;. But to give you a general idea: yes, your camera can capture colors you can't see.&lt;/p&gt;
&lt;p&gt;Let's start with the smallest and most restrictive color space shown here, &lt;strong&gt;sRGB&lt;/strong&gt;. The sRGB color space has for a long time been the &quot;default color space of the web&quot; because while it's fairly limited compared to other color spaces, it more or less represents the lowest common denominator with respect to what various computer and device displays out there can reproduce. That's why historically SVG defaults to sRGB, and CSS only supports sRGB (&lt;a href=&quot;https://www.w3.org/TR/css-color-4/#icc-colors&quot; title=&quot;CSS Color Module Level 4&quot;&gt;that's just changing now&lt;/a&gt;). That means that if you edited and converted your photo to sRGB, it should look close to how you intended on the vast majority of devices.&lt;/p&gt;
&lt;p&gt;Compared to another color space shown here, &lt;strong&gt;Adobe RGB&lt;/strong&gt;, you can say sRGB has a smaller or narrower gamut. Adobe RGB provides a much larger color space—more than 50% of the visible spectrum—and was originally designed for people working with RGB on computers to be able to match the colors capable by CYMK printers. Most monitors &lt;strong&gt;do not&lt;/strong&gt; display anything near 100% Adobe RGB. It's getting better as the years go on, but it doesn't have the best coverage like sRGB. Displays that can reproduce or exceed Adobe RGB (P3 as well) are considered &lt;strong&gt;wide-gamut&lt;/strong&gt;.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-28&quot; id=&quot;r28&quot;&gt;28&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Next up we've got &lt;strong&gt;DCI-P3&lt;/strong&gt;. While it has roughly the same gamut size as Adobe RGB, P3 gladly sacrifices a few saturated blues and greens in favor of some reds and yellows. That's because the P3 color space was made in 2007 for high-end digital cinema projectors. Only recently has this been applied towards computer displays, not projectors.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-29&quot; id=&quot;r29&quot;&gt;29&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;But if DCI-P3 was originally intended for the cinema, why should we care about it and why did Apple even push forward with it for their hardware? Perhaps they wanted to hop on the digital video bandwagon as P3 might be the next standard gamut for movies as we transition beyond the current Rec. 709 color space on the way to Rec. 2020 for UltraHD content.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-30&quot; id=&quot;r30&quot;&gt;30&lt;/a&gt;&lt;/sup&gt; Maybe they wanted to cater even more to digital video content creators? Or maybe they just wanted to go with a color space that more uniformly augments the sRGB color space compared to Adobe RGB.&lt;/p&gt;
&lt;p&gt;Then we have &lt;strong&gt;Rec. 2020&lt;/strong&gt;. This one aims to represent the gamut for upcoming display technologies — both HDR10 displays (also Rec. 2100) and UHDTV 8K televisions. If this is the new standard gamut for such high-end televisions, one can only hope it will make its way to some kinds of professional computer displays meant for creatives. There is also a use for this gamut outside of the context of a display and more for video workflows (almost like ProPhoto RGB below) but there's not much point to diving even deeper into that now.&lt;/p&gt;
&lt;hr class=&quot;full&quot;/&gt;&lt;p&gt;Finally, I wanted to point out a color space called &lt;strong&gt;ProPhoto RGB&lt;/strong&gt;. This is not like the other color spaces mentioned here. It goes &lt;em&gt;way&lt;/em&gt; outside the visible spectrum. And for a reason; ProPhoto RGB is a massive 16 bit per channel color space used in Adobe Lightroom. It's not meant to be a display gamut. It's just a safe working space that won't clip or compress of the colors captured by your camera when shooting in RAW, providing ample headroom while post-processing your shots.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-31&quot; id=&quot;r31&quot;&gt;31&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;You might have seen ProPhoto RGB listed in Lightroom if you go to edit a file you're working on in another app or plugin. There's a little dialog asking what color space and bit depth to send the file as. However, it gets a bit more complex behind the scenes. RAW camera files have a gamma of 1.0, so Lightroom has decided to do all of its calculations at this gamma in the ProPhoto RGB color space. What you end up seeing as a preview in the Develop module (not the Library module or filmstrip — those use Adobe RGB) actually uses a gamma close to that of sRGB at 2.2. I believe this modified color space is internally called Melissa RGB at Adobe, named after one of their engineers. Confused yet? Great, so am I.&lt;/p&gt;
&lt;p&gt;While you work in this large ProPhoto RGB space while manipulating your photos in Lightroom, you later export your files and have them converted to your desired color space. This process remaps the colors to fit within your desired destination color space. If you've ever saved an image to another color space and seen terms like &quot;perceptual&quot; and &quot;relative&quot;, they often define how to map colors and how to deal with colors that are outside the gamut of your destination color space.&lt;/p&gt;
&lt;p&gt;And when it comes to doing this in Lightroom you can always preview what this may look like by &lt;a href=&quot;https://helpx.adobe.com/lightroom/help/develop-module-options.html#soft_proof_images&quot;&gt;soft-proofing your images in Lightroom&lt;/a&gt;:&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-classic-cc-profile-proofing-2000.jpg&quot;&gt;&lt;img data-ratio=&quot;1.329&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-classic-cc-profile-proofing-1280.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;An extreme example of soft proofing in Lightroom. These photos may look the same if your display can't show more than sRGB. The right photo (limited to sRGB) lacks certain saturated colors.&lt;br/&gt;Photo: Off the coast of Grand Cayman Island near Rum Point&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;11.294820717131&quot;&gt;&lt;h4&gt;What it all means&lt;/h4&gt;
&lt;p&gt;That was a ton of detail to get one point across — when selecting a monitor, you should at least be aware of what coverage the monitor has in the color space you care about: likely Adobe RGB or P3 if you're doing a lot of photography. Since it's hard to find regular prosumer monitors right now that even list their P3 accuracy, you'll probably just want to find something as close to 100% Adobe RGB as possible. At the end of the day, having more coverage means for the most part you'll be able to reproduce more saturated colors, assuming you're working with color managed software.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-32&quot; id=&quot;r32&quot;&gt;32&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class=&quot;build-list build-list-sm&quot; readability=&quot;9.4206549118388&quot;&gt;
&lt;h6 class=&quot;smaller&quot;&gt;Alternatives&lt;/h6&gt;
&lt;p&gt;I will refrain from recommending any particular display options aside from &lt;a href=&quot;https://www.amazon.com/Dell-Ultrasharp-LED-Lit-Monitor-UP2718Q/dp/B0728K6YVB/ref=as_li_ss_tl?ie=UTF8&amp;amp;qid=1516414568&amp;amp;sr=8-1&amp;amp;keywords=dell+up2718q&amp;amp;linkCode=ll1&amp;amp;tag=paulstamatiou-20&amp;amp;linkId=931408c3cd774ea000e2eb17eea6400e&quot; title=&quot;Dell Ultrasharp 27-inch Screen LED-Lit Monitor Black (UP2718Q)&quot;&gt;the one I purchased&lt;/a&gt; right now. There are a ton of new ones coming out this year, but I don't think you can go wrong with displays from Dell's UltraSharp line. There are also some interesting &lt;a href=&quot;https://www.theverge.com/circuitbreaker/2017/12/21/16804850/lg-hdr-5k-ultrawide-monitor-ces-specs&quot; title=&quot;LG announces '5K ultrawide' HDR monitor&quot;&gt;&quot;Nano IPS&quot; displays coming out from LG&lt;/a&gt; this year that also have Nvidia G-Sync and a great 98% P3 color space coverage.&lt;/p&gt;
&lt;/div&gt;
&lt;/section&gt;&lt;section class=&quot;section-title&quot; id=&quot;the_build&quot;&gt;&lt;h2&gt;&lt;span&gt;The build&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;Putting it all together&lt;/h3&gt;
&lt;/section&gt;&lt;section readability=&quot;12.646373056995&quot;&gt;&lt;p&gt;And now the fun begins. I started ordering parts here and there, slowly accumulating everything over a week or two. I didn't quite jump right into building the PC as soon as everything arrived. I actually ordered a &lt;a href=&quot;https://paulstamatiou.com/photos/gear/&quot; title=&quot;Paul Stamatiou - Camera Gear&quot;&gt;studio lighting setup&lt;/a&gt; to try my hand at taking some shots of the parts and computer against a white backdrop as you saw in some of the parts list earlier.&lt;/p&gt;
&lt;h6&gt;Case&lt;/h6&gt;
&lt;p&gt;I began by unboxing the NZXT S340 Elite case and removing its side panels, which was uneventful with its avid use of thumbscrews that remain attached even when unscrewed. I found the case internals to be laid out well, providing for some great channels to hide the PSU and various cables. The case also had some 2.5&quot; drive cages pre-installed. I removed those as I won't be using any SATA devices.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02565-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.1904761904761905&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC02565-1000.jpg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7781-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.4005602240896358&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7781-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;11.856191744341&quot;&gt;&lt;h6&gt;PSU &amp;amp; custom cables&lt;/h6&gt;
&lt;p&gt;I installed the Corsair AX860 power supply and connected a few of the cables I knew I would need soon: 24-pin ATX motherboard power cable, 8-pin CPU power cable and two 8-pin PCIe power cables for the graphics card.&lt;/p&gt;
&lt;p&gt;I did not use the standard cables that came with the PSU though. I wanted something a bit more aesthetically pleasing and went with individually wrapped custom cables from &lt;a href=&quot;https://www.ensourced.net/&quot; title=&quot;custom modular power cables for diy computers&quot;&gt;Ensourced&lt;/a&gt;. They let you pick exactly what color paracord is used for each pin on the cable, but I opted for the same gray pattern on all of them. Since the cables are entirely custom I was able to specify the exact length of each one. I went with 50cm for the PCIe cables, 70cm for the CPU power cable and 60cm for the motherboard power cable.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7644-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.824&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7644-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;34&quot;&gt;&lt;h6&gt;Water-cooling&lt;/h6&gt;
&lt;p&gt;Then I installed the Corsair h115i AIO liquid cooler. I removed the front plate of the S340 and removed the handy dust filter that was magnetically affixed to the front intake. One of the main reasons I got this case was because it supports a 2x140mm radiator. It was a bit of a close fit but ended up working out. I screwed the fans and radiator in, then snaked the power cables for the fans and pump to the back.&lt;/p&gt;
&lt;h6&gt;Motherboard&lt;/h6&gt;
&lt;p&gt;The motherboard came next, but first I had to install the supplied motherboard backplate for the CPU. The h115i has some pretty stiff tubes coming off the waterblock and it takes a good amount of force to fasten it down, so the backplate helps spread that force across the motherboard. Compare that to my old days of PC building where a large and heavy heatsink-fan would bend the back of the motherboard.&lt;/p&gt;
&lt;h6&gt;RAM&lt;/h6&gt;
&lt;p&gt;Then I installed the two sticks of G.SKILL DDR4-3200 RAM. And now this is the point where I remind you to make sure to make sure they are installed in slots of the same color. This ensures that dual-channel mode will be activated. Apart from that there is one thing new about DDR4 that took me a second to realize: only one side of the RAM slot opens up. I'm not sure if this is a DDR4 spec thing, or just a motherboard-specific feature.&lt;/p&gt;
&lt;p&gt;The last time I built a desktop PC when you inserted ram, you would nudge the two locking tab levers to their position away from the RAM, then place the ram straight down to lock both tabs on place. But now with DDR4 the bottom lever does not move and is locked in place so you have to put that side in first, cantilever the RAM down to the other side and then lock the lever in place. Sounds more complex than it is, it's just different from how I was accustomed. Also, if you look closely at the bottom of DDR4 RAM you'll notice it has a slightly curved edge to reduce required insertion force with all the extra pins compared to previous generations of RAM.&lt;/p&gt;
&lt;h6&gt;CPU&lt;/h6&gt;
&lt;p&gt;With the brace installed and motherboard screwed in to the case, it was time for the delicate act of installing the CPU, applying thermal paste and then fastening the h115i's pump head.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7269-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.422475106685633&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7269-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;51.619950535861&quot;&gt;&lt;p class=&quot;larger&quot;&gt;But this isn't any regular i7 8700K CPU. I had it &lt;strong&gt;delidded&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The big metal thing you see attached to the green PCB on a processor is not the processor itself; it's a heatspreader or just IHS for Integrated Heat Spreader. Desktop processors have this IHS to take the grunt of the force that comes from attaching a large heatsink and helps prevent amateur PC builders from inadvertently cracking the die. For years the CPU die under there had solder attaching the die to the IHS, thus providing for highly effective heat transfer.&lt;/p&gt;
&lt;p&gt;At some point, desktop processors began shipping with thermal paste instead of solder to connect the CPU die to the IHS. And with that change came a larger distance between the die and the IHS, due to a thick application of glue on the sides of the IHS. This means more distance for the heat to travel and through a less effective heat transfer agent. Depending on the particular CPU, that means &lt;strong&gt;higher CPU temperatures and likely worse overclocking potential&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Delidding fixes that.&lt;/strong&gt; Delidding is the process of forcibly removing the IHS by applying so much lateral pressure that the IHS glue breaks and the IHS can be removed. Then meticulously cleaning the die and surrounding area, carefully applying a better and much thinner layer of &quot;liquid metal&quot; thermal compound then gluing the IHS back on, but this time with less compound to ensure the die sits closer to the IHS.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-33&quot; id=&quot;r33&quot;&gt;33&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Delidding can be done with a specialized tool that holds the CPU in place while pushing the IHS off. Given that it would be my first time doing this, I opted to have a professional do the job for me. Especially at a time when the 8700K was impossible to find in stock and I got lucky even getting one shortly after launch. I didn't have time to try to find a replacement if I ended up cracking this one.&lt;/p&gt;
&lt;p&gt;After some research on various forums, I found &lt;a href=&quot;https://siliconlottery.com/&quot; title=&quot;Silicon Lottery - CPU delidding and binning&quot;&gt;Silicon Lottery&lt;/a&gt;. These folks provide CPU delidding and binning services. I had them do both and I shipped them my 8700K and they sent it back out in a day or two.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-34&quot; id=&quot;r34&quot;&gt;34&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-conversation=&quot;none&quot; data-lang=&quot;en&quot; readability=&quot;7.0260869565217&quot;&gt;
&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;CPU has returned after getting lost by UPS for a bit. Won the silicon lottery so to speak, turns out this chip is among the top ~15% of i7 8700Ks and can overclock to 5.2GHz stable now that it has been delidded (assuming my other hardware is up to the challenge) 🤓 &lt;a href=&quot;https://t.co/l9hvzKUSAa&quot;&gt;pic.twitter.com/l9hvzKUSAa&lt;/a&gt;&lt;/p&gt;
— Paul Stamatiou 📷 (@Stammy) &lt;a href=&quot;https://twitter.com/Stammy/status/938555957826568192?ref_src=twsrc%5Etfw&quot;&gt;December 6, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;The CPU and binning results came back in a great state: &lt;strong&gt;this particular 8700K can sustain a very admirable 5.2GHz overclock&lt;/strong&gt; with a healthy dose of extra voltage: 1.425V. There's a small caveat in that it runs with an AVX offset, which downclocks for certain workloads that make use of Intel Advanced Vector Extensions instructions which can be brutal on a machine. A bit of a note on AVX from Intel:&lt;/p&gt;
&lt;blockquote readability=&quot;9&quot;&gt;
&lt;p&gt;Because Intel AVX instructions generally consume more power, frequency reductions can occur to keep the processor operating within TDP limits. Intel is including additional AVX base and turbo frequency specifcations to provide more clarity for these Intel AVX instructions. Performance of workloads optimized for Intel AVX instructions can be signifcantly greater than workloads that do not use Intel AVX instructions even when the processor is operating at a slightly lower frequency.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr class=&quot;full&quot;/&gt;&lt;p&gt;CPU in hand, I put it in the LGA 1151 socket on the motherboard and slowly closed the socket. I didn't expect closing the socket lever to require so much force but my concerns were quickly allayed after some frantic Googling. The next step was the apply a thin layer of thermal paste to the CPU. The h115i ships with a questionable thermal pad affixed to it, so I cleaned that off first.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7301-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.122334455667789&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7301-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7708-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.6260162601626016&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7708-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;10&quot;&gt;&lt;p&gt;The most stressful part of the build was now out of the way. Installing the M.2 SSDs was up next. This motherboard has two M.2 slots but only one has its own heatsink. I wanted to place the more active drive that I would install the OS on there, and leave the less active Lightroom scratch drive in the standalone M.2 slot.&lt;/p&gt;
&lt;p&gt;I couldn't quickly ascertain how each slot was identified in the UEFI and I didn't want to mistake installing Windows on the wrong drive. To solve this I only installed the SSD under the heatsink first and would install the other one after I had Windows up and running.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a title=&quot;Samsung 960 EVO M.2 SSD in place but before heatsink installation.&quot; href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7345-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.4992503748125936&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7345-1000.jpg&quot; alt=&quot;Samsung 960 EVO M.2 SSD in place but before heatsink installation.&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;Samsung 960 EVO M.2 SSD in place but before heatsink installation. That USB cable on the Corsair pump head annoys the crap out of me. I need to find a slimmer cable I can hide more easily.&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;14&quot;&gt;&lt;h6&gt;Graphics card&lt;/h6&gt;
&lt;p&gt;The massive GTX 1080 Ti was next up to bat. When I picked the case and graphics card I had to make sure that I would have enough room in the front for the radiator. Fortunately, that was not an issue here. But the one thing I knew would be an issue: so-called &lt;strong&gt;GPU sag&lt;/strong&gt;. Graphics cards like this one are very heavy — especially this one with a larger than average heatsink — and have two bulky 8-pin power cables adding even more weight. As such, when mounted in a vertical case like this the card tends to sag down, applying a ton of stress on the PCIe slot with it.&lt;/p&gt;
&lt;p&gt;While it's probably nothing to be terribly worried about as this motherboard has a reinforced PCIe slot, I also installed a GPU support bracket to be safe. Though I had one unexpected snag: this card has fans along the length of the card so there weren't any great bracing points for the support bracket to push against. I managed to place it at the very end which doesn't provide the best support.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7582-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.466275659824047&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7582-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;11.594713656388&quot;&gt;&lt;h6&gt;Cable management&lt;/h6&gt;
&lt;p&gt;With the main components installed it was time to connect the remaining power cables, attach the front panel LED and power switch cables, the front panel USB cable as well as a micro-USB cable for the Corsair AIO pump.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-35&quot; id=&quot;r35&quot;&gt;35&lt;/a&gt;&lt;/sup&gt; I wanted to hide some of these USB cables and give me another port so I installed an &lt;a href=&quot;https://www.amazon.com/NZXT-Internal-Controller-Black-AC-IUSBH-M1/dp/B01IFGFTJ2/ref=as_li_ss_tl?ie=UTF8&amp;amp;linkCode=ll1&amp;amp;tag=paulstamatiou-20&amp;amp;linkId=3bac0154b80ceea06ba7d8cee9decfa0&quot; title=&quot;NZXT Internal USB Hub Controller, Black (AC-IUSBH-M1)&quot;&gt;internal NZXT USB hub&lt;/a&gt; and hid it in the back.&lt;/p&gt;
&lt;p&gt;With that out of the way I meticulously zip tied just about everything on the back. Not that it mattered much; they wouldn't be visible with the side panel on. Then I put the glass side panel back on. This took some work as the Corsair h115i tubes are very rigid and didn't want to stay inside the case at first.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;section-title&quot; id=&quot;finished_build&quot;&gt;&lt;h2&gt;&lt;span&gt;The finished product&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;A closer look&lt;/h3&gt;
&lt;/section&gt;&lt;section readability=&quot;8&quot;&gt;&lt;p&gt;Finally, here's the finished computer and desk setup! While I was initially concerned this case might be a bit larger than I wanted, I ended up being rather pleased with it, especially the mostly black/gray theme with the internals. The S340 Elite has enough room to make hiding cables easy and allow for just about any component I want without size or thermal restrictions.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7512-2000.jpg&quot;&gt;&lt;img data-ratio=&quot;1.064&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7512-1500.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7498-2000.jpg&quot;&gt;&lt;img data-ratio=&quot;1.222&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7498-1500.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7417-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.223&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7417-1500.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;

&lt;/section&gt;&lt;section readability=&quot;5&quot;&gt;&lt;p&gt;The can does attract fingerprints though not nearly as much as a glossy surface would. The side panels do scuff easily as well, but on the plus side I've been surprised at how well the front dust filter does at keeping dust out.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7795-2000.jpg&quot;&gt;&lt;img data-ratio=&quot;1.1918951132300357&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7795-1500.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;3&quot;&gt;&lt;p&gt;The next challenge was ensuring my desk cable management situation was at least decent to complement the tidy internals of the PC. I went with a three fold approach:&lt;/p&gt;
&lt;ul readability=&quot;1.5&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;strong&gt;Humanscale M8&lt;/strong&gt; adjustable monitor arm&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;&lt;strong&gt;Humanscale NeatTech&lt;/strong&gt; cable tray that screws under the desk to hide chargers and miscellaneous cables. I always hide micro-USB, USB-C and Lightning cables under there as well as MacBook Pro charger for when I connect my work laptop to the display when working from home.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Cable sleeving and velcro ties for taming PC cables under the desk&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8386-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.4992503748125936&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8386-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8378-2000.jpg&quot;&gt;&lt;img data-ratio=&quot;1.402524544179523&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8378-1500.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8266-2000.jpg&quot;&gt;&lt;img data-ratio=&quot;0.7727975270479135&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8266-1500.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8318-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;0.7468259895444361&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8318-1000.jpg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8333-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;0.675219446320054&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8333-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8349-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.493273542600897&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8349-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;14&quot;&gt;&lt;h5&gt;First boot&lt;/h5&gt;
&lt;p&gt;I plugged everything in — including a different keyboard directly into the USB socket labeled BIOS on the motherboard — and nervously pressed the power button for the first time. The computer quietly whirred to life, the motherboard and graphics card lit up their numerous animated LEDs and the motherboard's two digit Q-code display showed various codes before successfully POSTing.&lt;/p&gt;
&lt;p&gt;As for the sound of the machine, it's nowhere near as completely silent as an idle iMac but not too much louder with the dual 140mm fans of the Corsair AIO liquid cooler in the default silent mode. However, the top 140mm and rear 120mm case fans could be quieter. I ended up undervolting the top fan to 7 volts with an adapter to spin a bit slower. Though I do want to look into either quieter Noctua fans, &quot;be quiet!&quot; brand fans or the new maglev Corsair ML line of fans.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7885-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.3477088948787062&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7885-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;PC on (after I disabled all the bright animating LEDs)&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;13.968217934166&quot;&gt;&lt;p&gt;The first thing I did was enter the BIOS&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-36&quot; id=&quot;r36&quot;&gt;36&lt;/a&gt;&lt;/sup&gt; to do a quick runthrough of the settings. I wasn't concerned with overclocking just yet but did setup the boot drive order and disable SATA as I didn't have any SATA devices connected. I also enabled the XMP memory settings so the RAM would run at its rated 3200MHz. After Windows finished installing I went back to install the second M.2 SSD and then go back into the BIOS to ensure it was running at PCIe x4 speed (the default was x2).&lt;/p&gt;
&lt;p&gt;The BIOS on this Asus Maximus X Hero is insanely detailed and overclocker oriented. You can tweak just about every voltage or timing you'd ever dream of fiddling with. After saving the settings I changed, I put in the Windows 10 USB stick and rebooted to the installer.&lt;/p&gt;
&lt;p&gt;There was nothing particularly noteworthy about the Windows 10 installation process, it did it's thing and went by pretty quickly.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;section-title&quot; id=&quot;windows_setup&quot;&gt;&lt;h2&gt;&lt;span&gt;Windows 10&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;Setup &amp;amp; first impressions&lt;/h3&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-first-use-3000.jpg&quot;&gt;&lt;img src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-ratio=&quot;1.777&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-first-use-2000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;Windows 10 after installation. 4K display means screenshots will be rather tiny, even with 125% scaling&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;16.955671447197&quot;&gt;&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;T&lt;/span&gt;he first thing you notice about Windows 10 is the dark theme. It's a rather bold choice for Microsoft to set as the default for everyone, but something about it does feel modern, sleek and precise. However, as I quickly noticed throughout my entire Windows 10 setup experience, &lt;strong&gt;everything is customizable&lt;/strong&gt;. You can change the accent color, adjust how you want it to appear in the title bar and so on.&lt;/p&gt;
&lt;p&gt;I have some mixed feelings about the Start Menu though.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-37&quot; id=&quot;r37&quot;&gt;37&lt;/a&gt;&lt;/sup&gt; Right after having installed Windows and opening the Start Menu for the first time, it’s a more than a bit daunting in its default incarnation — there's a ton of stuff pinned to it making it this rather large monstrosity that only draws attention to all the unwanted, preinstalled applications and games.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-preinstalled-stuff.jpg&quot;&gt;&lt;img src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-ratio=&quot;1.387&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-preinstalled-stuff.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;Ahhhh what is all this junk??&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;8.974025974026&quot;&gt;&lt;p&gt;I had to spend a few minutes going through and uninstalling a bunch of stuff. It's only then that the Start Menu began to feel more humble and minimal. Some folks may like the live tiles for glanceable weather updates but I prefer something more basic.&lt;/p&gt;
&lt;hr class=&quot;full&quot;/&gt;&lt;p&gt;Coming from macOS there were definitely a few things that feel familiar. There's the expandable sidebar for notifications and Action Center on the right side of the screen. Similarly some Windows 10 has some pop-up notifications in the bottom right corner when apps or the OS need to tell me something. Then there's macOS Expose/Spaces-like TaskView that supports multiple virtual desktops (in addition to the expected Alt+Tab switcher).&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-38&quot; id=&quot;r38&quot;&gt;38&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;img src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-ratio=&quot;1.412&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-action-center.jpg&quot;/&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;63.625377643505&quot;&gt;&lt;p&gt;Windows has had a neat window snapping functionality to allow quick resizing and snapping of windows to either full-screen, half-screen or corner quarter-screen sizes for a while now. While I appreciate its existence, it's largely only useful for devices with smaller displays — with a 4K display I really don't need my windows to be as large as 50% of the screen as it wants to do for me.&lt;/p&gt;
&lt;p&gt;Unlike macOS, typography on Windows leaves a lot to be desired, even after fiddling with Microsoft ClearType settings. Many typefaces throughout the OS just feel like they are lacking weight and commonly used ones like in the Chrome address bar seem hairline thin. There is a third party font rasterizer called &lt;a href=&quot;https://github.com/snowie2000/mactype&quot;&gt;MacType&lt;/a&gt; but I haven't had much luck with it and uninstalled it.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-39&quot; id=&quot;r39&quot;&gt;39&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Another area this manifests itself is high-DPI display support. Some applications can display blurry text when mixed with UI scaling — I run my 4K display with the 125% scaling set, which seems to keep the native resolution but only scale certain parts of application chrome and text as necessary, which I do like.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-40&quot; id=&quot;r40&quot;&gt;40&lt;/a&gt;&lt;/sup&gt; However, this particular issue may just be a transition period thing until more Windows 10 applications are served as &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/uwp/get-started/whats-a-uwp&quot; title=&quot;What's a Universal Windows Platform (UWP) app?&quot;&gt;UWP (Universal Windows Platform) apps&lt;/a&gt; that can run on any Windows 10 device.&lt;/p&gt;
&lt;p&gt;This push for UWP apps makes sense for the grander vision for Windows 10. There are so many kinds of devices running Windows 10, especially convertible 2-in-1 tablet/laptop hybrids like the Surface Pro and Surface Book 2, that Windows has invested quite a bit in making the experience on any device smooth. For example, there's a tablet mode you can enter to make it easier to use with a touchscreen notebook.&lt;/p&gt;
&lt;hr class=&quot;full&quot;/&gt;&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;C&lt;/span&gt;ortana is Microsoft's &quot;truly personal digital assistant.&quot; It has lofty goals of being able to do stuff like Alexa and Siri—things it can only do by knowing more about your online habits, contacts, location, calendar, emails, et cetera. Instead of being a relatively hidden and streamlined-when-you-need-it part of the OS, Cortana seems to be more like an overbearing parasite grabbing every surface area it can. For now, that means there's a few more items in your Start Menu: Cortana Notebook, Cortana speaker (even if you don't have one there's a reserved menu item for it), Cortana Reminders and Collections.&lt;/p&gt;
&lt;p&gt;Hopefully, the rumors will come to fruition and Cortana will find a &lt;a href=&quot;https://www.windowscentral.com/microsoft-moving-cortana-out-search-and-action-center-windows-10&quot; title=&quot;Microsoft is moving Cortana out of Windows 10 search and into the Action Center&quot;&gt;new home in the Action Center&lt;/a&gt; where it can be safely ignored. I know that's a bit harsh, but I don't see the value from Cortana yet and I'm not sure how I could—I use Google Chrome, I use G Suite Gmail and Calendar instead of native clients and so on. There's not too many ways for Cortana to learn about me aside from just what I use Cortana for: finding files and launching applications.&lt;/p&gt;
&lt;p&gt;Even if you only use the regular Cortana search the default screen for that is always trying to get you to do something else and trying to show you what else it can search for. I get it, Microsoft wants to be aggressive with this and find ways for Cortana to grab a hold of your daily needs to fit into your life somehow. It just comes off as adding complexity to everything. I disabled it as much as I could. Maybe I'll find a use for it one day, but for now I need Cortana to recede.&lt;/p&gt;
&lt;p&gt;Other significant additions to the Windows 10 experience include the fast new Edge browser, the renamed and revamped Microsoft Store as well as less popular but noteworthy functionality like automatic facial recognition login with Windows Hello (though nowhere near as advanced as Face ID) and Dynamic Lock to log you out when you're not near the computer with a Bluetooth-paired phone.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-msft-store-2000.jpg&quot;&gt;&lt;img src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-ratio=&quot;1.566&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-msft-store-1500.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;The Windows store became the Microsoft Store and now features media for purchase.&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;6&quot;&gt;&lt;p&gt;As for the overall design of Windows 10, there's a definite feeling of inconsistency. Some parts feel more refined and modern while others seems like a relic of the past. For example, take a look at the entirely different aesthetic of these two settings-related windows:&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-old-control-panels.jpg&quot;&gt;&lt;img src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-ratio=&quot;1.4&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-old-control-panels.jpg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-new-settings-panel.jpg&quot;&gt;&lt;img src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-ratio=&quot;1.4&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-new-settings-panel.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;10.401500938086&quot;&gt;&lt;p class=&quot;larger&quot;&gt;&lt;strong&gt;But that is changing — and quickly.&lt;/strong&gt; Microsoft is beginning to incorporate their &lt;a href=&quot;https://fluent.microsoft.com/&quot; title=&quot;Microsoft Fluent Design&quot;&gt;Fluent design&lt;/a&gt; system to replace the older Metro style that exists in parts of Windows 10.&lt;/p&gt;

&lt;p&gt;Fluent design has a few areas of focus for how they're thinking about the system: light, depth, motion, material and scale. &lt;strong&gt;Material&lt;/strong&gt; was the first aspect of Fluent design that I noticed in parts of the current Windows 10 release. There's this &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/uwp/design/style/acrylic&quot; title=&quot;Fluent Design - Acrylic is a type of Brush that creates a partially transparent texture&quot;&gt;acrylic material&lt;/a&gt; in certain menus and panes that is like frosted glass with translucency and a strong background blur.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;img data-ratio=&quot;1.123&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-fluent-photos-app-trim-video.jpg&quot;/&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;Probably not the best example of Fluent design.. but you get the idea :)&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;10.595402298851&quot;&gt;&lt;p&gt;Facets of &lt;strong&gt;light&lt;/strong&gt; can also already be seen, mostly in the hover states for various elements with &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/uwp/design/style/reveal&quot; title=&quot;Fluent Design - Reveal is a lighting effect that helps bring depth and focus to your app's interactive elements.&quot;&gt;reveal highlight&lt;/a&gt;. They now dynamically adjust the lighting of the container based on where your cursor is over the element. It's like what happens when you move your cursor above an inactive Google Chrome tab.&lt;/p&gt;
&lt;p&gt;However, until Fluent design permeates more of those legacy surface areas, we'll have to deal with some repulsive stuff like this:&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;img data-ratio=&quot;3.835&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-file-explorer-confusing.jpg&quot;/&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;I have no idea how to use this menu.&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;29.64552238806&quot;&gt;&lt;p&gt;While my first impressions of Windows 10 might have come off rather negative, I do really like the OS. Sure I'm dismayed that various parts of Windows have some clutter (I'm looking at you Cortana, OneDrive and Quick Access) but that's just the default. Similar to how many parts of Windows can be personalized, other items can often be changed and simplified to your liking with enough motivation. For every minor annoyance I've had, it only took a few minutes to find out how to customize it enough to make it more acceptable.&lt;/p&gt;
&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;img data-ratio=&quot;1.061&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-uninstall-preinstalled-apps.jpg&quot;/&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;First step: uninstalling many preinstalled programs I don't want.&lt;/small&gt;&lt;/section&gt;&lt;p&gt;After a few customization and cleanup tasks that I'll dive into later, Windows itself felt like it began to recede and let me focus on my tasks at hand. It's a faster and &lt;strong&gt;more capable beast than the Windows versions I remember&lt;/strong&gt;.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-41&quot; id=&quot;r41&quot;&gt;41&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Fluent design sounds exciting but it'll take some time to see what the fully realized vision will do for Windows 10. The good news is that Windows feels like it's constantly being updated. Instead of waiting for larger annual tentpole releases, there are more frequent large updates like the recent Fall &quot;Creators Update&quot; that brings entirely new functionality, not just bug fixes. The Windows &quot;Service Pack&quot; updates are long gone.&lt;/p&gt;
&lt;p&gt;And if you want to see new features being tested, you can easily sign up for Windows 10 Insider Preview builds. For example, the last big release in December 2017 featured two new workflow and window management features: &lt;a href=&quot;https://blogs.windows.com/windowsexperience/2017/12/19/announcing-windows-10-insider-preview-build-17063-pc/&quot;&gt;Timeline and Sets&lt;/a&gt;.&lt;/p&gt;
&lt;p class=&quot;larger&quot;&gt;There's one more thing. &lt;strong&gt;You can run now Linux &lt;em&gt;on&lt;/em&gt; Windows!&lt;/strong&gt; And not in some slow VM. This is absolutely huge news.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;section-title&quot; id=&quot;windows_setup_2&quot;&gt;&lt;h2&gt;&lt;span&gt;Windows 10&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;Installing apps &amp;amp; drivers&lt;/h3&gt;
&lt;/section&gt;&lt;section readability=&quot;33.83042394015&quot;&gt;&lt;p&gt;After the Windows 10 installation completed, I had to first install some basic motherboard drivers to get online. Only the Ethernet port was working out of the box so I had to connect that to my router first. I'll spare you the exact details, but I went to the Asus site and had to download a ton of drivers from Wi-Fi to Bluetooth, and then some motherboard specific programs like AI Suite 3 from Asus to manage overclocking and advanced energy use settings. Then I downloaded the Asus Aura Sync program to be able to disable the animating LEDs on the motherboard and graphics card.&lt;/p&gt;
&lt;p&gt;I then installed all Windows updates, a motherboard BIOS update, Nvidia GeForce drivers, Dell Display Manager software, Logitech Options for the MX Master 2S mouse, Corsair Link software for the AIO cooler and Samsung Magician for the 960 EVO SSDs. A reboot or two later and I had all required software installed. Definitely not as easy as turning on a Mac for the first time, but not difficult.&lt;/p&gt;
&lt;p&gt;The vast majority of programs I wanted to use had Windows versions too.. which is a really funny thing to say because 15 years ago I would have been complaining that my favorite apps from Windows weren't on OS X. The largest exception was my preferred note-taking app &lt;a href=&quot;http://www.bear-writer.com/&quot; title=&quot;Bear is a beautiful, flexible writing app for crafting notes and prose for iPhone, iPad, and Mac&quot;&gt;Bear&lt;/a&gt;; its iOS/macOS apps are iCloud-based and they don't have a web version. However, I'm currently testing out &lt;a href=&quot;https://www.notion.so/&quot; title=&quot;A unified &amp;amp; collaborative workspace for your notes, wikis, and tasks.&quot;&gt;Notion&lt;/a&gt; which has a web support and a Windows 10 app.&lt;/p&gt;
&lt;p&gt;Then I installed my essentials:&lt;/p&gt;
&lt;ul readability=&quot;3.5&quot;&gt;&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;Adobe Creative Cloud: Lightroom Classic CC, Premiere Pro CC, Photoshop CC&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Atom text editor&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Backblaze&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Dropbox&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spotify&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;Steam (CS:GO, Call of Duty: WWII, PUBG)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Origin (Battlefield 1)&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;Blizzard (Destiny 2, Overwatch)&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Oculus (Robo Recall and misc VR games)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;hr class=&quot;full&quot;/&gt;&lt;p&gt;The &lt;strong&gt;Corsair Link&lt;/strong&gt; software controls the settings of the CPU liquid cooler. You can create and set different profiles that determine what speed the radiator fans and pump should be running at for each temperature range. It also lets you glance at temperatures for the motherboard, CPU, graphics card and drives.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-corsair-link-app-configure-2000.jpg&quot;&gt;&lt;img data-ratio=&quot;1.922&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-corsair-link-app-configure-1280.jpg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-corsair-link-app-stats-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.333&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-corsair-link-app-stats-750.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;12&quot;&gt;&lt;p&gt;I find the &lt;strong&gt;Samsung Magician&lt;/strong&gt; software much more interesting. Aside from letting you run firmware updates and performance benchmarks, it also gives you an easy way to enable &lt;strong&gt;over provisioning&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;As mentioned during the SSD selection process earlier, there are different kinds of NAND memory from QLC to SLC. They all have some kind of maximum number of read/write cycles. For the average consumer like myself, I don't really need to worry at all about this, especially with TRIM and modern SSDs. Last I checked you'd have to write on the order of a few hundred TB of data to the SSD before you experience any kind of errors; though you'd get some performance degradation along the way. I've been running these two SSDs half a year so far and have only put about 8TB of writes between both drives. But there are some technologies at play to help extend the SSD lifespan.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;
&lt;/section&gt;&lt;section readability=&quot;32.262981574539&quot;&gt;&lt;p&gt;The Samsung Magician app links you to the Windows drive optimization dialog but this is largely unnecessary. For one, it's already done on a schedule by default and second, Windows 10 has great TRIM command support. When TRIM is enabled, every time you delete a file Windows tells your SSD that a particular set of LBA data blocks are no longer being used by the OS and can be erased immediately.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-42&quot; id=&quot;r42&quot;&gt;42&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Without TRIM, the SSD will retain the contents of those LBAs until they are overwritten by another action. In actuality it's a bit more complex than this with the SSD firmware doing some automated wear leveling and garbage collection in association with TRIM, but long story short TRIM is good and helps reduce write amplification and increase write speed overall.&lt;/p&gt;
&lt;p&gt;As for over provisioning, this lets you specify a percentage of the drive to go unused by the OS and give to the SSD to help maintain the performance and extend the lifespan of the drive. Seagate has &lt;a href=&quot;https://www.seagate.com/tech-insights/ssd-over-provisioning-benefits-master-ti/&quot; title=&quot;SSD Over-Provisioning And Its Benefits&quot;&gt;a good primer on SSD over provisioning&lt;/a&gt; if you'd like to learn more:&lt;/p&gt;
&lt;blockquote readability=&quot;9&quot;&gt;
&lt;p&gt;Note that in this case, as the amount of over-provisioning increases, the gain in performance is quite significant. Just moving from 0% over-provisioning (OP) to 7% OP improves performance by nearly 30%.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It's for these reasons—easy firmware updates, ability to set over-provisioning and TRIM support—that I opted to not use my two 1TB 960 EVO SSDs in a RAID array. While TRIM in a RAID array is technically possible (I've heard Intel RST can do it), I didn't want to have another potential thing to debug for a new build. Also, if I were to upgrade my processor and motherboard down the line, the new motherboard may not recognize the array. And these things are so fast already, I'm not sure I'd see a huge real world performance gain for the added risk.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;section-title&quot; id=&quot;tweaking_windows&quot;&gt;&lt;h2&gt;&lt;span&gt;Tweaking Windows&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;Making it feel like home&lt;/h3&gt;
&lt;/section&gt;&lt;section readability=&quot;3.9814385150812&quot;&gt;&lt;p&gt;With the computer now up and running with all required drivers, it was time to tweak a few things that I wanted to feel more natural to me coming from macOS or just fix things that annoyed me:&lt;/p&gt;
&lt;ul readability=&quot;0&quot;&gt;&lt;li readability=&quot;2.9747899159664&quot;&gt;
&lt;p&gt;&lt;strong&gt;Disable User Account Control:&lt;/strong&gt; Every time I install a new program in Windows 10, I get this really annoying confirmation dialog that takes over the full screen. While this is great for less tech savvy Windows users, it's annoying for me.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-43&quot; id=&quot;r43&quot;&gt;43&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7373-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.806509945750452&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7373-1000.jpg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-adjust-uac.jpg&quot;&gt;&lt;img data-ratio=&quot;1.348&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-adjust-uac.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section&gt;&lt;ul readability=&quot;4.7960199004975&quot;&gt;&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;strong&gt;Install 7-Zip:&lt;/strong&gt; I'm just really not a fan of how Windows 10's native compressed folders work for unzipping things. I also find it way slower than 7-Zip. 7-Zip lets me right-click on any compressed archive and extract right in place.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;8.522639068564&quot;&gt;
&lt;p&gt;&lt;strong&gt;Use &lt;a href=&quot;https://keytweak.en.softonic.com/&quot; title=&quot;Redefine Your Keyboard Input Signals &quot; rel=&quot;nofollow&quot;&gt;KeyTweak&lt;/a&gt; to remap keys to feel more like the Mac keyboard layout:&lt;/strong&gt; While I was able to easily pair the &lt;a href=&quot;https://www.amazon.com/Apple-Magic-Keyboard-MLA22LL-A/dp/B016QO64FI/ref=as_li_ss_tl?s=pc&amp;amp;ie=UTF8&amp;amp;qid=1514580297&amp;amp;sr=1-5&amp;amp;keywords=apple+keyboard&amp;amp;linkCode=ll1&amp;amp;tag=paulstamatiou-20&amp;amp;linkId=73da0cb043a79030c757ea6db7320633&quot; title=&quot;Apple Magic Keyboard (MLA22LL/A)&quot;&gt;new wireless Apple Magic Keyboard&lt;/a&gt; to Windows 10, I did not have any functioning media keys. In addition, I wanted to remap the Windows key to use the current Control key instead. That way I could still use the same Cmd (Mac)/Ctrl (Win) placement. That means that things like opening a new tab in Chrome would use the same finger position for me if I was on my MacBook Pro or my Windows machine. It would make going between the two machines much less annoying.&lt;/p&gt;
&lt;p&gt;KeyTweak is a rather complicated piece of software that took some time to get used to, but I was able to remap those keys after some poking around. I have not had any issues after setting it up the one time.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-keytweak-remap-keys.jpg&quot;&gt;&lt;img src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-ratio=&quot;1.524&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-keytweak-remap-keys.jpg&quot;/&gt;&lt;/a&gt;
&lt;ul readability=&quot;2.3721399730821&quot;&gt;&lt;li readability=&quot;2.8387096774194&quot;&gt;
&lt;p&gt;&lt;strong&gt;Install Apple BootCamp keyboard software:&lt;/strong&gt; I wanted the Apple-style volume HUD and related features so I installed the Apple Boot Camp software. But since Boot Camp is intended for Macs that have Windows installed, by default it will want to install a bunch of other hardware drivers that I don't need. I &lt;a href=&quot;https://www.youtube.com/watch?v=D31uzS1Szek&quot; title=&quot;How to use an Apple Keyboard Volume Keys on Windows 10 - YouTube&quot;&gt;followed these steps&lt;/a&gt; to only install the keyboard-specific software.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1.9027027027027&quot;&gt;
&lt;p&gt;&lt;strong&gt;Install Xmeters:&lt;/strong&gt; For years I have gotten used to seeing CPU and network activity at a glance in my macOS menubar with &lt;a href=&quot;https://bjango.com/mac/istatmenus/&quot; title=&quot;An advanced Mac system monitor for your menubar&quot;&gt;iStat Menus&lt;/a&gt;. It's just some nice peace of mind to know if my machine is doing something (or isn't doing something) that I'm expecting. &lt;a href=&quot;https://entropy6.com/xmeters&quot; title=&quot;Taskbar System Stats for Windows&quot;&gt;Xmeters&lt;/a&gt; is the closest equivalent I could find for Windows. It lets you put a myriad of system stats in your taskbar.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-configure-xmeters.jpg&quot;&gt;&lt;img src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-ratio=&quot;1.587&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-configure-xmeters.jpg&quot;/&gt;&lt;/a&gt;
&lt;ul readability=&quot;-0.49285714285714&quot;&gt;&lt;li readability=&quot;1.9714285714286&quot;&gt;
&lt;p&gt;&lt;strong&gt;Install &lt;a href=&quot;http://1218.io/&quot; title=&quot;A Windows quick look tool. Acts just like the one in macOS, but Seer is more powerful, customizable and faster.&quot;&gt;Seer&lt;/a&gt; for macOS-like &quot;Quick Look&quot; spacebar file previews:&lt;/strong&gt; Because I've gotten way too used to selecting a photo and hitting the spacebar in macOS to preview it. There are two versions of Seer, a free one and a more advanced paid one. I went with the older free one for now.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-install-seer-tool.jpg&quot;&gt;&lt;img src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-ratio=&quot;1.487&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-install-seer-tool.jpg&quot;/&gt;&lt;/a&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-file-explorer-customization-before.jpg&quot;&gt;&lt;img data-ratio=&quot;1.47&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-file-explorer-customization-before.jpg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-file-explorer-customization-done.jpg&quot;&gt;&lt;img data-ratio=&quot;1.47&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-file-explorer-customization-done.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;5.8036529680365&quot;&gt;&lt;ul readability=&quot;0&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;strong&gt;Remove the Recycle Bin from the desktop:&lt;/strong&gt; I prefer to have a clean desktop. Instructions on how to do that &lt;a href=&quot;https://support.microsoft.com/en-us/help/15057/windows-show-hide-recycle-bin&quot; title=&quot;Show or hide the Recycle Bin&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;strong&gt;Rename the PC:&lt;/strong&gt; So that I don't keep seeing some random PC name on the network or announced by my Bluetooth speaker when I connect. This setting can be found in Settings → System → About&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-rename-pc.jpg&quot;&gt;&lt;img src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-ratio=&quot;1.447&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-rename-pc.jpg&quot;/&gt;&lt;/a&gt;
&lt;ul readability=&quot;16.523471800068&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;&lt;strong&gt;Move the task bar to the top:&lt;/strong&gt; I'm just used to having it up on top.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1.8272425249169&quot;&gt;
&lt;p&gt;&lt;strong&gt;Install Virtual Desktop Manager:&lt;/strong&gt; The good news is that Windows 10 has native virtual desktops. I often use two with one dedicated to full-screen Adobe Lightroom. While doing this a few times I kept running into issues with the standard windows hotkey to switch desktops sometimes not working while I was editing a photo. I also didn't like that the native animation to switch desktops felt a bit slow. I installed the lightweight app &lt;a href=&quot;https://github.com/m0ngr31/VirtualDesktopManager&quot; title=&quot;Virtual Desktop Manager&quot;&gt;Virtual Desktop Manager&lt;/a&gt; to change the hotkey and eliminate the slow switching animation. If you need a bit more control there's also &lt;a href=&quot;https://github.com/sdias/win-10-virtual-desktop-enhancer&quot; title=&quot;An application that enhances the Windows 10 multiple desktops feature by adding additional keyboard shortcuts and support for multiple wallpapers&quot;&gt;Virtual Desktop Enhancer&lt;/a&gt; and &lt;a href=&quot;https://peachapp.net/&quot; title=&quot;Keep your ideas in their own spaces with Virtual Desktops&quot;&gt;Peach&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;10.810752688172&quot;&gt;
&lt;p&gt;&lt;strong&gt;Install Tiny Hot Corners to enable basic hotcorner functionality like macOS:&lt;/strong&gt; After years of using Exposé/Mission Control, I wanted to bring some of that behavior to Windows to allow me to easily throw my mouse in a corner of the screen to show all windows to quickly find something I'm looking for (I also used it to quickly sleep the computer or show the desktop). Windows 8 had hot corners but apparently the implementation was more annoying and it was removed with Windows 10.&lt;/p&gt;
&lt;p&gt;After a lot of searching I found some really bad apps that offered this kind of functionality. Then I stumbled on &lt;a title=&quot;Tiny Hot Corners&quot; href=&quot;https://github.com/taviso/hotcorner&quot;&gt;Tiny Hot Corners&lt;/a&gt;. It's an impressively minimal, no frills executable. You can only specify the coordinates of the hot corner zone, the action to be done on activation, the delay before it triggers and so on. But it does the job of opening up Task View for me when I toss my mouse to the corner and it uses very little system resources.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;10.820895522388&quot;&gt;
&lt;p&gt;&lt;strong&gt;Configure keyboard to control monitor brightness using &lt;a href=&quot;https://autohotkey.com/&quot; title=&quot;Powerful. Easy to learn. The ultimate automation scripting language for Windows&quot;&gt;AutoHotkey&lt;/a&gt;:&lt;/strong&gt; I got the monitor brightness buttons on my Mac keyboard working. One benefit to running the Dell UP2718Q is that it can be controlled via the Dell Display Manager app to adjust settings instead of fiddling with the hardware buttons on the display. This is thanks to it supporting DDC/CI.&lt;/p&gt;
&lt;p&gt;I also realized the Dell Display Manager executable accepts command line arguments, letting me easily script something basic using AutoHotkey.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-44&quot; id=&quot;r44&quot;&gt;44&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;pre class=&quot;hide-label noexpand&quot;&gt;
/* AutoHotkey .ahk script compiled to .exe then autorun by Windows Task Scheduler */
#NoEnv
SetWorkingDir, C:\Program Files (x86)\Dell\Dell Display Manager
NumpadSub::run ddm.exe /DecControl 10 10
NumpadAdd::run ddm.exe /IncControl 10 10
&lt;/pre&gt;&lt;/li&gt;
&lt;li readability=&quot;1.9903381642512&quot;&gt;
&lt;p&gt;&lt;strong&gt;Install &lt;a href=&quot;http://www.getwox.com/&quot; title=&quot;A full-featured launcher, access programs and web contents as you type. Be more productive ever since.&quot;&gt;Wox&lt;/a&gt; as a better launcher over Cortana:&lt;/strong&gt; As a general app launcher Cortana is fine. I can quickly click the search box in the taskbar or hit the Windows key and start typing searching. But it's not great. By default the search results are cluttered with suggestions from the Microsoft Store or the web. There are some rudimentary search filters but it doesn't appear that they can be set as the default. There is a way to permanently disable those suggestions in Cortana settings. When that's done the search field placeholder text says &quot;Type here to search&quot; instead of &quot;Ask me anything&quot; and results look like this:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-disable-cortana-suggestions.jpg&quot;&gt;&lt;img src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-ratio=&quot;1.306&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-disable-cortana-suggestions.jpg&quot;/&gt;&lt;/a&gt;
&lt;div readability=&quot;15.946902654867&quot;&gt;
&lt;p&gt;While the results are filled with fewer suggestions now, I don't like having the expanded search box taking up space in the taskbar and if I hide it, I'll still be pressing a hotkey to open up search. If I'm doing that, I might as well look for a better alternative. One that's even a bit simpler.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Enter &lt;a href=&quot;https://github.com/Wox-launcher/Wox&quot; title=&quot;Launcher for Windows, an alternative to Alfred and Launchy.&quot;&gt;Wox&lt;/a&gt;.&lt;/strong&gt; It's a basic Spotlight/Alfred equivalent for Windows. I use it to launch applications and search for local files easily. To do the latter I have to install a separate indexing service called &lt;a href=&quot;https://www.voidtools.com/&quot; title=&quot;Locate files and folders by name instantly.&quot;&gt;Everything&lt;/a&gt;. It can do a bit more than that with various plugins you can configure. &lt;strong&gt;But&lt;/strong&gt; this may be temporary — it seems like search on Windows is &lt;a href=&quot;https://arstechnica.com/gadgets/2017/11/windows-10-search-could-soon-look-just-like-macoss/&quot; title=&quot;New Windows search interface borrows heavily from macOS&quot;&gt;gearing up for a nice upgrade&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-install-wox-search.jpg&quot;&gt;&lt;img src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-ratio=&quot;1.394&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-install-wox-search.jpg&quot;/&gt;&lt;/a&gt;&lt;/section&gt;&lt;section class=&quot;section-title&quot; id=&quot;linux_setup&quot;&gt;&lt;h2&gt;&lt;span&gt;Developer mode + Linux!&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;Enabling the Windows Subsystem for Linux&lt;/h3&gt;
&lt;/section&gt;&lt;section readability=&quot;15.762968299712&quot;&gt;&lt;p&gt;It is now possible to run a full Linux environment right inside Windows. This means you can install Ubuntu or another distro and get access to the same bash prompt you'd expect inside Ubuntu. It was this new Linux functionality (that I read about on &lt;a href=&quot;https://char.gd/blog/2017/why-i-left-mac-for-windows-apple-has-given-up&quot; title=&quot;How to set up the perfect modern dev environment on Windows&quot;&gt;Owen's blog several times&lt;/a&gt;) that was partially responsible for my initial curiosity in Windows 10 and building a new PC. It meant I could also easily carry out my basic web developement tasks to maintain and publish to this site. For me that means a simple Ruby and Node development environment.&lt;/p&gt;
&lt;blockquote class=&quot;huge bound&quot; readability=&quot;10.565789473684&quot;&gt;
&lt;p&gt;The Windows Subsystem for Linux lets developers run Linux environments -- including most command-line tools, utilities, and applications -- directly on Windows, unmodified, without the overhead of a virtual machine.&lt;/p&gt;
&lt;span class=&quot;author&quot;&gt;—&lt;a href=&quot;https://docs.microsoft.com/en-us/windows/wsl/faq&quot; title=&quot;Windows Subsystem for Linux Documentation&quot;&gt;Microsoft&lt;/a&gt;&lt;/span&gt;&lt;/blockquote&gt;
&lt;p&gt;I really can't understate the magnitude of this. There are some quirks and not everything is smooth sailing but I've been able to adapt my workflow to it just fine. Your mileage may vary.&lt;/p&gt;
&lt;hr class=&quot;full&quot;/&gt;&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;F&lt;/span&gt;irst I needed to enable Developer Mode and the WSL feature. I also found some nifty settings on the &quot;For developers&quot; page to &lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-file-explorer-settings-for-devs.jpg&quot;&gt;change several File Explorer settings&lt;/a&gt; like showing the full path in the title bar by default as well as displaying file extensions and hidden files. The entire setup process is pretty straightforward and &lt;a href=&quot;https://docs.microsoft.com/en-us/windows/wsl/install-win10&quot; title=&quot;WSL Windows 10 Installation Guide&quot;&gt;documented on Microsoft's site&lt;/a&gt;: pick and install a Linux distro then create a UNIX user account.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;
&lt;/section&gt;&lt;section readability=&quot;32.173622704508&quot;&gt;&lt;p&gt;After that's done you can now just type &lt;code class=&quot;inline&quot;&gt;bash&lt;/code&gt; inside any command prompt to get access to your Linux distro's shell.&lt;/p&gt;
&lt;h4&gt;Install Hyper terminal&lt;/h4&gt;
&lt;p&gt;It didn't take long for me to dislike the included Command Prompt and PowerShell command-line shells in Windows 10. I wanted something more customizable. I went with &lt;a href=&quot;https://github.com/zeit/hyper&quot; title=&quot;A terminal built on web technologies&quot;&gt;Hyper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are a few ways to install Hyper but I used &lt;a href=&quot;https://chocolatey.org/install&quot; title=&quot;The package manager for Windows&quot;&gt;Chocolatey&lt;/a&gt; to quickly install it for me. Chocolately is like the Homebrew package manager on macOS or apt on Ubuntu. Chocolately also has a &lt;a href=&quot;https://chocolatey.org/packages/ChocolateyGUI&quot; title=&quot;Chocolatey GUI is a nice GUI on top of the Chocolatey command line tool.&quot;&gt;GUI&lt;/a&gt; you can install if that's more your style. A few moments later I had the lovely Hyper terminal up and running.&lt;/p&gt;
&lt;p&gt;It's rather novel compared to other terminals I have used in that it's built with web technologies.. you can even open up a web inspector for the terminal itself! When you want to add a plugin, you just type in the name in the &lt;code class=&quot;inline&quot;&gt;.hyper.js&lt;/code&gt; preferences file and when you save the plugin is automatically downloaded and installed behind the scenes with npm. It's super easy to get started and there are &lt;a href=&quot;https://github.com/bnb/awesome-hyper&quot;&gt;a ton of plugins&lt;/a&gt; and &lt;a href=&quot;https://hyperthemes.matthi.coffee/&quot;&gt;themes&lt;/a&gt;. Unfortunately, despite all my tinkering, it seems like it's not possible to get a transparent or translucent terminal background for the Windows version of Hyper.&lt;/p&gt;
&lt;p&gt;After spending too much time checking out various Hyper plugins and copying over some of my &lt;code class=&quot;inline&quot;&gt;.bash_profile&lt;/code&gt; alias and tweaks, I was ready to get back to work.&lt;/p&gt;
&lt;p&gt;By default Hyper uses the Windows system prompt. This means that whenever you want to access your Ubuntu bash prompt you need to type &lt;code class=&quot;inline&quot;&gt;bash&lt;/code&gt; at the prompt. This got annoying pretty quickly so I configured Hyper to make that my default prompt. There's just a line you can uncomment in the preferences file:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;shell: 'C:\\Windows\\System32\\bash.exe',&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;I also installed my preferred terminal font &quot;&lt;a href=&quot;https://www.fontsquirrel.com/fonts/m-1m&quot; rel=&quot;nofollow&quot; title=&quot;M+ 1m monospaced typeface&quot;&gt;M+ 1m&lt;/a&gt;&quot; and set it as the typeface for Hyper.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-install-hyper-terminal.jpg&quot;&gt;&lt;img data-ratio=&quot;1.6&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-install-hyper-terminal.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;Hyper after a few tweaks. I like a fairly simple and dark style. Used &lt;a href=&quot;http://jmd.im/black&quot; title=&quot;BLACK - A wallpaper series using shapes and lights.&quot; rel=&quot;nofollow&quot;&gt;one of these wallpapers&lt;/a&gt; for the background.&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;11.318725099602&quot;&gt;&lt;h4&gt;Setting up my dev environment&lt;/h4&gt;
&lt;p&gt;My site is all static flat files and based on the static site generator Jekyll so I needed to set up a ruby environment. I also work with my site on my laptop here and there so I prefer to install the exact same ruby version on both machines. I find it easiest to manage ruby versions with rbenv. I installed rbenv and ruby by following along &lt;a href=&quot;https://www.digitalocean.com/community/tutorials/how-to-install-ruby-on-rails-with-rbenv-on-ubuntu-16-04&quot; title=&quot;How To Install Ruby on Rails with rbenv on Ubuntu 16.04&quot;&gt;with parts of this guide&lt;/a&gt; (or any basic Ubuntu ruby setup guide) and then running a &lt;code class=&quot;inline&quot;&gt;bundle install&lt;/code&gt; in my Jekyll directory to install the gems I use.&lt;/p&gt;
&lt;p&gt;That was pretty much it! I also installed ImageMagick and grunt, which I use for a few things. I also resize my photos a few times, compress them and convert some to WebP. I have always done the former with a grunt script but I had a Mac app I used for WebP conversion. I started looking around for a Windows equivalent to let me batch convert photos to WebP and found the lovely &lt;a href=&quot;https://www.xnview.com/en/xnconvert/&quot; title=&quot;Image Batch Converter for Everyone&quot;&gt;XnConvert&lt;/a&gt;. &lt;a href=&quot;https://saerasoft.com/caesium/&quot; title=&quot;Caesium Image Compressor for Windows&quot;&gt;Caesium Image Compressor&lt;/a&gt; is also a solid runner-up but I found XnConvert to be more capable.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-final-dev-env.jpg&quot;&gt;&lt;img data-ratio=&quot;1.8175&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-final-dev-env.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;10.460377358491&quot;&gt;&lt;p&gt;There are a few gotchas associated with this WSL setup. The main one is this: you can't edit a file that originates from the Linux userland inside Windows. My workaround was to pull down a git repo in Windows, edit it in Atom on Windows and have Jekyll in Linux work with the files. The catch was that I had to ensure I had git &lt;a href=&quot;https://github.com/Microsoft/WSL/issues/184&quot; title=&quot;git status shows all files as modified - microsoft WSL&quot;&gt;force convert line endings&lt;/a&gt; for me:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;git config --global core.autocrlf true&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;But you won't run into that issue as long as you do all your Linux stuff inside Linux and all your Windows stuff inside Windows.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;section-title&quot; id=&quot;lightroom_config&quot;&gt;&lt;h2&gt;&lt;span&gt;Configuring Lightroom&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;What I do after a clean install&lt;/h3&gt;
&lt;/section&gt;&lt;section readability=&quot;13.383108935129&quot;&gt;&lt;p&gt;The first thing I do with any new Lightroom installation is move the catalog to my Dropbox folder. I have done this for years, initially to sync catalogs when I used them interchangeably between my Macs as I mentioned in &lt;a href=&quot;https://paulstamatiou.com/storage-for-photographers-part-2/&quot;&gt;Storage for Photographers (Part 2)&lt;/a&gt;. But with this new PC, I pretty much only do my editing here so syncing is less of a requirement now.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-45&quot; id=&quot;r45&quot;&gt;45&lt;/a&gt;&lt;/sup&gt; But I digress, I like having the catalog backed up in case I mess something up I can quickly revert.&lt;/p&gt;
&lt;p&gt;Since I move the RAWs I'm no longer actively editing to my NAS, I went to File Explorer and mapped the NAS as a network drive. I set it as &lt;code class=&quot;inline&quot;&gt;Z:\&lt;/code&gt; and went into Lightroom and updated the locations of archived sets to the new drive path. When I'm done editing a photoset I just drag the local folder to the NAS network drive inside the Lightroom Library module.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-lightroom-classic-cc-window-2500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.367&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-lightroom-classic-cc-window-1500.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;&lt;a href=&quot;https://paulstamatiou.com/photos/new-zealand/mount-cook-to-christchurch/&quot; title=&quot;Mount Cook to Christchurch - Paul Stamatiou Photos&quot;&gt;Lake Pukaki&lt;/a&gt;, Mount Cook, New Zealand&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;4.9266862170088&quot;&gt;&lt;p&gt;There's not much I need to do aside from tweak a few settings and install one thing to get Lightroom to my liking. I don't really use presets or plugins.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-46&quot; id=&quot;r46&quot;&gt;46&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;ul readability=&quot;0.98174442190669&quot;&gt;&lt;li readability=&quot;4.9087221095335&quot;&gt;
&lt;p&gt;&lt;strong&gt;Install &lt;a href=&quot;https://github.com/vsco/keys&quot; title=&quot;VSCO Keys is an open-source keyboard shortcut tool for Adobe Lightroom that reduces time spent editing photos.&quot;&gt;VSCO Keys&lt;/a&gt;:&lt;/strong&gt; Once a paid app, VSCO Keys is now free and open source (though not updated anymore) shortcut tool I use to speed up my editing workflow. I mainly use it to copy and paste develop settings across photos quickly: I just tap &lt;code class=&quot;inline&quot;&gt;,&lt;/code&gt; to copy develop settings on a photo and &lt;code class=&quot;inline&quot;&gt;.&lt;/code&gt; to paste. It can also map keys to some other functions. It has been said that VSCO Keys was broken with Adobe Lightroom Classic CC but I was able to get it to work, so maybe that was only for the Mac version.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-lightroom-vscokeys.jpg&quot;&gt;&lt;img src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-ratio=&quot;1.334&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-windows-10-lightroom-vscokeys.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;Some of the many customizable hotkeys.&lt;/small&gt;&lt;/section&gt;&lt;ul readability=&quot;14.288267875126&quot;&gt;&lt;li readability=&quot;3&quot;&gt;
&lt;p&gt;&lt;strong&gt;Keep NVIDIA drivers up to date and ensure GPU acceleration is enabled:&lt;/strong&gt; While Lightroom doesn't put the GPU to the best use, it is particularly helpful if you're on 4K display. It was enabled for me by default, but good to double-check.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;strong&gt;Save presets inside catalog:&lt;/strong&gt; While I don't use many presets, I like having them stored inside the catalog and backed up. Handy if you have any synced catalogs across computers.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;strong&gt;Increase Camera Raw cache to 20GB or more&lt;/strong&gt;: As instructed by Adobe &lt;a href=&quot;https://helpx.adobe.com/lightroom/kb/optimize-performance-lightroom.html&quot; title=&quot;Optimize Adobe lightroom performance&quot;&gt;in their optimization guide&lt;/a&gt;. I went to 50GB given that I'm working with much larger 42MP RAW files so I might hit 20GB more readily.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;3&quot;&gt;
&lt;p&gt;&lt;strong&gt;Set JPEG preview to full size for DNG creation&lt;/strong&gt; I don't convert my RAWs to DNG too much. There is said to be some performance gain but I can't really tell the difference for it to be worth the lengthy upfront conversion time. But if I end up changing my workflow in the future to use more third-party culling apps, I would want larger JPEG previews baked into the DNG. I might dabble with this more this year.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;12.973056994819&quot;&gt;

&lt;p&gt;&lt;strong&gt;Set Default Develop Settings:&lt;/strong&gt; Whenever I import new photos from my camera, I'd like to have my default develop settings applied automatically. I don't have many default settings — just Remove Chromatic Aberration, Enable Profile Corrections&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-47&quot; id=&quot;r47&quot;&gt;47&lt;/a&gt;&lt;/sup&gt; and set a camera profile other than the default Adobe Standard. As I mentioned above, the list you see in the Camera Calibration » Profile dropdown will vary depending on your camera manufacturer. For my Sony A7R III with Version 4 processing, the profiles feel a bit different from my A7R II so I'm still trying to see what I want to set as my default. Likely Camera Standard or Camera Neutral.&lt;/p&gt;
&lt;p&gt;To change these defaults, go to the Develop module and change any settings on a particular photo that you would like to have as the new default for every photo from that camera. When you're doing tweaking, hold down the &lt;code class=&quot;inline&quot;&gt;Alt&lt;/code&gt; key on Windows then click &quot;Set Default...&quot; in the bottom right and accept the dialog that comes up:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-classic-cc-set-cam-dev-defaults.jpg&quot;&gt;&lt;img data-ratio=&quot;2.632&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-classic-cc-set-cam-dev-defaults.jpg&quot;/&gt;&lt;/a&gt;
&lt;p&gt;And now I just fullscreen Lightroom and get to work:&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-pstam-lightroom-classic-cc-fullscreen-nz-2500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.777&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-pstam-lightroom-classic-cc-fullscreen-nz-1500.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;I usually have Lightroom fullscreened in its own virtual desktop.&lt;br/&gt;Akaroa, New Zealand&lt;/small&gt;&lt;/section&gt;&lt;section/&gt;&lt;section class=&quot;section-title&quot; id=&quot;performance&quot;&gt;&lt;h2&gt;&lt;span&gt;Performance&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;Overclocking &amp;amp; putting the new build to work&lt;/h3&gt;
&lt;/section&gt;&lt;section readability=&quot;143.75786096257&quot;&gt;&lt;p class=&quot;larger&quot;&gt;I set out to build a speedy Windows 10 PC mainly for my Lightroom photo editing work. One that would be easily upgradeable. How did I do on that goal?&lt;/p&gt;
&lt;p&gt;Well there's still one more thing to do — overclock! I built this computer with the intention of getting some extra performance by overclocking the CPU, RAM and GPU a bit. Especially with many of the parts I purchased being geared towards the overclocker enthusiast. With a delidded processor and liquid cooling system with a large dual-140mm radiator, I should be able to achieve a high and stable overclock to run 24/7 for this processor.&lt;/p&gt;
&lt;p&gt;I spend the majority of my time in the Develop module, a part of Lightroom that does not really put extra processors cores to work efficiently. As such, I opted for relatively fewer CPU cores (compared to going with 8, 10 or more), but with a very high clock speed compared to tons of cores with a lower clock speed.&lt;/p&gt;
&lt;p&gt;But why did I opt for 6 cores instead of 4 cores? It seems like I would be able to achieve a better overclock with the 6-core i7 8700K than I would with the 4-core 7700K, meaning I could have my cake (extra cores) and eat it too (overclocked for a high clock speed). This wouldn't be the case for an 8-core chip where I wouldn't be able to reach 5GHz+ on all CPU cores and where the extra cores wouldn't be put to use well with inefficient Lightroom.&lt;/p&gt;
&lt;p&gt;And I got lucky. The Intel i7 8700K I have happens to be made from better silicon than other 8700Ks and I can get away with a stable 5.2GHz overclock on all cores. Compare that to the stock setup which would only ever run all cores at a Turbo Boost of 4.3GHz (the advertised 4.7GHz Turbo Boost is just for one core). As a secondary benefit, the two extra cores markedly improve my Premiere Pro video editing performance; that was not my goal as I use Premiere Pro much less, but it's nice to have. Going with 6 cores seemed like the right choice given the current Lightroom implementation. If Lightroom keeps getting better at multi-core performance for Develop actions, then that would not be the case.&lt;/p&gt;
&lt;hr class=&quot;full&quot;/&gt;&lt;h4&gt;Overclocking&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;How and what did I overclock?&lt;/p&gt;
&lt;p&gt;There are two main ways to overclock the CPU: in the UEFI or using a Windows program like the one provided by the motherboard manufacturer. While you can tweak the overclocking basics quickly with the Asus AI Suite 3 in Windows, it doesn't contain every piece of functionality. The controls exposed by the UEFI for this Asus Maximus X Hero motherboard can be very daunting at first. There are pages and pages of discrete settings, voltages, frequencies and more that you can adjust and test.&lt;/p&gt;
&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-asus-ai-suite-gpu-tweak-overclocking-2000.jpg&quot;&gt;&lt;img data-ratio=&quot;1.111&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-asus-ai-suite-gpu-tweak-overclocking-1000.jpg&quot;/&gt;&lt;/a&gt;
&lt;p&gt;As much as I would like to provide a guide to overclocking here, that would add many pages to an already long post, and more important: I'm not an overclocking expert. In a nutshell, it is lots of trial and error: increase the CPU Vcore voltage a tiny bit, increase the CPU multipier a few steps, see if it boots and is stable, repeat until unstable, then back down until stable and lower voltage as necessary.&lt;/p&gt;
&lt;p class=&quot;larger&quot;&gt;Fortunately, overclocking is much easier than it was in the past.&lt;/p&gt;
&lt;p&gt;Many enthusiast motherboards have simple two-digit error code displays directly on them to help you understand why your machine is not booting. And if you attempt some crazy settings, the motherboard will most likely catch the error and simply reboot for you with safe settings. There are other related motherboard safeguards too, like if you somehow corrupt your BIOS and you can't POST at all, you can use BIOS (again, it's UEFI but half the stuff is still labeled BIOS) flashback feature to plug in a USB port with a firmware file to flash or update your BIOS. Back in the day a corrupt BIOS flash would have just meant a dead board.&lt;/p&gt;
&lt;p&gt;Even with those new debugging tools and safeguards, your first time overclocking can still be a nerve-racking experience. Intel now lets you purchase a &lt;a href=&quot;https://click.intel.com/tuningplan/&quot; title=&quot;Allows a single replacement for your qualified boxed processor, in addition to your standard 3 year warranty.&quot;&gt;performance tuning protection plan&lt;/a&gt;. I think it's a bit gimmicky but it's insurance: Intel will replace your CPU once if you kill it. I've only killed a processor from overclocking once — a 3.06GHz Northwood Intel Pentium 4 that I sent too much voltage to — and that was in 2002 and Intel sent me a free replacement anyway. I do applaud Intel for being so overclocker-friendly these days though. Of course, I already voided my warranty by delidding the CPU so that protection plan is not an option for me.&lt;/p&gt;
&lt;p&gt;Fortunately, it's easy to find great overclocking guides for your exact hardware as well as general rules of thumb:&lt;/p&gt;
&lt;ul readability=&quot;6.5848888888889&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://rog.asus.com/forum/showthread.php?93536-Strix-1080Ti-Overclocking-Guide&quot; title=&quot;ASUS ROG STRIX 1080TI OVERCLOCKING GUIDE&quot;&gt;GTX 1080 Ti graphics card overclocking&lt;/a&gt;:&lt;/strong&gt; A good starter guide on using the ASUS GPU Tweak II software to eke out some extra performance from their ROG Strix GTX 1080 Ti.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=dG2Az0PclQA&quot; title=&quot;Nvidia GTX 1080 Ti Overclocking Tutorial (stock cooler)&quot;&gt;GTX 1080 Ti Overclocking Tutorial&lt;/a&gt;:&lt;/strong&gt; An easy to follow video guide on overclocking the GTX 1080 Ti but this time using the MSI Afterburner software which I tend to prefer as well.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1.6320754716981&quot;&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://overclocking.guide/ultimate-rog-strix-gtx-1080-ti-overclocking-guide/&quot; title=&quot;Ultimate ROG Strix GTX 1080 TI Overclocking Guide&quot;&gt;GTX 1080 Ti extreme overclocking guide:&lt;/a&gt;&lt;/strong&gt; For those that that are comfortable flashing their card to an XOC BIOS for more voltage controls, or even extreme liquid nitrogen cooling and hardware voltage mod tweaking.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0.86554621848739&quot;&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://edgeup.asus.com/2017/kaby-lake-overclocking-guide/5/&quot; title=&quot;The Kaby Lake overclocking guide - ASUS&quot;&gt;The Kaby Lake overclocking guide&lt;/a&gt;:&lt;/strong&gt; Yea I know this is not a Coffee Lake guide for an 8700K but it's a thorough guide written by ASUS themselves that is a fantastic primer for navigating around the UEFI BIOS on ASUS Maximus boards and more.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=CoUtA7DKXhU&quot; title=&quot;ASUS MAXIMUS X HERO - Overclocking Test and Guide 8700K (en)&quot;&gt;8700K + Maximus X Hero overclocking guide&lt;/a&gt;:&lt;/strong&gt; And a great video tutorial walking through the UEFI BIOS and explaining what things do as it's being overclocked using my exact hardware.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0.88965517241379&quot;&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://linustechtips.com/main/topic/773966-comprehensive-memory-overclocking-guide/&quot; title=&quot;Comprehensive Memory Overclocking Guide&quot;&gt;RAM overclocking&lt;/a&gt;:&lt;/strong&gt; While memory overclocking is covered sufficiently in other guides, here's a much deeper dive on the topic for those interested.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;h6&gt;Results&lt;/h6&gt;
&lt;p&gt;I'm still finalizing my exact settings but it's looking like this is a stable overclock &lt;a href=&quot;https://valid.x86.fr/2quwqg&quot; title=&quot;Paul Stamatiou - CPU-Z validated Intel Core i7 8700K @ 5198.75 MHz&quot;&gt;(CPU-Z validated)&lt;/a&gt; for me:&lt;/p&gt;
&lt;ul readability=&quot;1.984496124031&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;&lt;strong&gt;CPU:&lt;/strong&gt; 5.2GHz (52 CPU multiplier, 47 uncore multiplier) on all cores at 1.42V with -2 AVX offset&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;&lt;strong&gt;GPU:&lt;/strong&gt; 2012MHz GPU clock, 5602MHz memory clock (11,204MHz DDR effective clock) at 1.06V with 120% power limit&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-0.96363636363636&quot;&gt;
&lt;p&gt;&lt;strong&gt;RAM:&lt;/strong&gt; DDR4-3333 at CL14 at 1.4V (using Maximus Mode 2)&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-48&quot; id=&quot;r48&quot;&gt;48&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The graphics card overclock isn't terribly necessary given that Lightroom doesn't use the GPU too much and it's already such a fast card and I won't see any real gain in games as I'm already exceeding 60fps with 4K gaming.&lt;/p&gt;
&lt;p&gt;Given that this is quite an overclock, I should probably mention the idle and load temperatures. With the Corsair liquid cooler set to silent mode, the idle temperature runs somewhere between 29-32°C depending on the ambient temperature in my room. Under Lightroom loads with the fans kicked up, it will approach 55°C. Under complete 100% load like with a benchmark or Premiere Pro rendering a video, loads it could reach up to 72°C.&lt;/p&gt;
&lt;p&gt;If I keep the fans in silent mode, I hit around 79°C at load. Given that this is with a delidded CPU running higher voltage than normal &lt;em&gt;and&lt;/em&gt; on water, it seems unlikely that such a high overclock could be sustained without a good liquid cooling system. Compare to the max load temperature of around 52-56°C that I saw at stock clock (4.3GHz Turbo Boost on all cores).&lt;/p&gt;
&lt;h4&gt;Benchmarking&lt;/h4&gt;
&lt;p class=&quot;sublarge&quot;&gt;What do six 5.2GHz CPU cores mean for Lightroom?&lt;/p&gt;
&lt;p&gt;Now that everything is finally and up running as I had intended, it's time to see how this new PC stands up to my typical Lightroom workflow. First off, Lightroom Classic CC opens up quickly in about 5 seconds thanks to the 960 EVO M.2 SSD.&lt;/p&gt;
&lt;p&gt;The actions I care about are all in the Develop module: things like the responsiveness of dragging around the spot removal tool or adjustment brush, as well as simply scrolling through the filmstrip with Lightroom fullscreened on my 4K display. &lt;strong&gt;Unfortunately, I do not know of an easy way to benchmark actions in the Develop module.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Anecdotally, I can say everything in the Develop module is faster. I wouldn't say it's instantaneous or snappy — I mean we're still dealing with massive RAWs using relatively unoptimized software. But it's a marked improvement. I'd like to think this is the best performance I'd be able to achieve in the Develop module with any number of cores; only a higher clock would help more.&lt;/p&gt;
&lt;p&gt;Then there's the items that are easier to benchmark: import, export, DNG creation and HDR merging. They are mostly ones that can benefit from more cores and to varying degrees.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-49&quot; id=&quot;r49&quot;&gt;49&lt;/a&gt;&lt;/sup&gt;&lt;strong&gt;While I did benchmark these tasks&lt;/strong&gt;, they're not really the type of task I was aiming to optimize with this build, so showing these results is a bit moot.&lt;/p&gt;
&lt;hr class=&quot;full&quot;/&gt;&lt;p&gt;I selected a set of 350 42MP RAWs (~15GB) from my recent visit to the &lt;a href=&quot;https://twitter.com/Stammy/status/936017494174765057&quot; title=&quot;Snorkeled and fed some stingrays today. Did not expect stingrays to be so ridiculously soft. 😎🌞🌊&quot;&gt;Cayman Islands&lt;/a&gt; and ran them though importing (with the copy option), 1:1 preview generation, DNG conversion and exporting. I ran these all multiple times and averaged everything. Since I initially built this machine with a quad-core Intel 7700K and Asus Maximus IX Code motherboard before upgrading to the 8700K, I also ran the benchmarks on that rig. In addition, I benchmarked each without an overclock and with. For the 7700K I was able to get to 5GHz stable and with the 8700K I'm at 5.2GHz on all six cores.&lt;/p&gt;
&lt;p&gt;I'm not a professional benchmarker so I wouldn't say these results are totally accurate, but directionally accurate. For example, I think on my earliest 7700K benchmarks, I didn't wait long enough after import to start building previews and Lightroom was automatically applying my default camera profile corrections at the same time. That probably would have made that time a tad faster.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-classic-cc-previews-benching-2500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.777&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-classic-cc-previews-benching-1280.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;Building 1:1 previews on 8700K @ 5GHz (set to 5.2Ghz but Lightroom counts as an AVX process so it's at a -2 offset)&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;7&quot;&gt;&lt;p&gt;Below you'll see two screenshots of the 8700K at work building 1:1 previews (left) and exporting photos (right). When I talk about Lightroom being efficent or not efficient with multiple cores, &lt;strong&gt;you can see the difference in the CPU usage&lt;/strong&gt; in task manager between the two tasks. While neither are perfectly using all of the CPU, exporting images is much more efficient. Building previews on the other hand is comparatively all over the place.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-classic-cc-efficiency-2500.jpg&quot;&gt;&lt;img data-ratio=&quot;2.183&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-classic-cc-efficiency-1500.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;Building 1:1 previews (left) and exporting images (right). Notice the difference in CPU usage.&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;30&quot;&gt;&lt;p&gt;For the merge to HDR benchmarks, I selected 15 RAWs (5 3-bracket photos) and timed merging a single 3-bracket stack to an HDR in headless HDR mode. I did this many times for each shot to find consistent times, then averaged all the times together. I have less faith in the HDR numbers as I could see dramatically different times by doing things like simply closing Lightroom then reopening and running the HDR again, despite clearing cache and so on.&lt;/p&gt;
&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-classic-cc-hdr-benching-2000.jpg&quot;&gt;&lt;img data-ratio=&quot;1.507&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-classic-cc-hdr-benching-1280.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;5 stacks of 3 bracketed shots during headless HDR benchmarking.&lt;/small&gt;&lt;/section&gt;&lt;p&gt;Some thoughts on these numbers: first off, the import numbers are kind of cheating. The iMac and MacBook Pro were copying from and to the same SSD whereas the PC had two SSDs that I was copying between as I was moving the files to the dedicated SSD I use for Lightroom.&lt;/p&gt;
&lt;p&gt;Second, maybe I'm doing something wrong but the times on the Macs were faster than I was expecting. My hypothesis is that Lightroom Classic CC for macOS is more efficient at certain tasks compared to the Windows version. It definitely wasn't the case for me with Develop actions and felt sluggish there.&lt;/p&gt;
&lt;p&gt;And third, I was expecting a more significant gain with HDR merging between the machines but they were fairly minimal.&lt;/p&gt;
&lt;hr class=&quot;full&quot;/&gt;&lt;p&gt;I also ran the Premiere Pro PPBM H.264 encoding benchmark. Obviously this is a benchmark that puts the graphics card and every core to very efficient use. Again, not something I was optimizing for with this build but a nice secondary benefit from my move to 6 cores. What I'm showing here is different kinds of GPU acceleration: CUDA (Nvidia GPU), Software (no GPU acceleration) as well as OpenCL and Metal acceleration for the Macs.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-premiere-pro-extreme-overclock-testing-2500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.777&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-premiere-pro-extreme-overclock-testing-1500.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;An early 5.2GHz Premiere Pro benchmark while I was still finding my ideal stable overclock. This was without AVX and set using the Asus AI Suite overclocking app. I was using extreme voltage here. I absolutely do not recommend going anywhere past 1.4V Vcore. I went a bit overboard here.&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;5&quot;&gt;&lt;p&gt;As expected, the Premiere Pro encoding times went down considerably with the addition of more cores and faster cores, and was monumentally faster with GPU acceleration enabled.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;section-title&quot; id=&quot;whats_next&quot;&gt;&lt;h2&gt;&lt;span&gt;What's next?&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;Nothing's ever really finished.&lt;/h3&gt;
&lt;/section&gt;&lt;section readability=&quot;8&quot;&gt;&lt;p&gt;I've been really happy with this machine so far. First off, I just love the aesthetics and the dark theme inside the case. While my goal was not to make a gaudy PC with a window to show off everything, I think this build was tastefully done. I do kind of wish the case was a tad smaller, but I wouldn't compromise for a smaller radiator or limit my motherboard options to get that.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8386-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.4992503748125936&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8386-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7513-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.18&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7513-500.jpg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7498-2000.jpg&quot;&gt;&lt;img data-ratio=&quot;1.222&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC7498-1500.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/section&gt;&lt;section readability=&quot;67.056149124801&quot;&gt;&lt;p&gt;This PC also does exceedingly well at 4K and VR gaming which I was going to talk about in this post but decided to leave out. I also got an Oculus Rift and an extra sensor for 360° play and was thoroughly amazed the first time I used it and went through the &lt;a href=&quot;https://www.oculus.com/experiences/rift/1217155751659625/&quot;&gt;First Contact&lt;/a&gt; demo. And I got that same feeling when I played &lt;a href=&quot;https://www.youtube.com/watch?v=MIK4D0kVlIs&quot; title=&quot;Robo Recall&quot;&gt;Robo Recall&lt;/a&gt; for the first time. When I played &lt;a href=&quot;https://www.youtube.com/watch?v=CSQyTPzzTRk&quot; title=&quot;Crytek The Climb&quot;&gt;The Climb&lt;/a&gt; for the first time. And again when I used &lt;a href=&quot;https://www.youtube.com/watch?v=SCrkZOx5Q1M&quot; title=&quot;Google Earth VR&quot;&gt;Google Earth VR&lt;/a&gt; for the first time. And when I fired up a massive virtual computer display in front of me in VR with &lt;a href=&quot;http://bigscreenvr.com/&quot; title=&quot;Watch your favorite movies, TV shows, sports, and more with your friends from around the world. No matter where you are, like you’re sitting next to each other&quot;&gt;BigScreenVR&lt;/a&gt;. But many hours spent in VR over several months, I started to become annoyed at the low resolution of the Rift (nothing ever feels sharp, especially text) and the so-called god rays. Maybe I'll give it another shot with the next generation of higher-resolution HMD hardware.&lt;/p&gt;
&lt;p&gt;After a lot of tweaking Windows 10 to get it to my liking, I've really come to like it — though to be frank I'm not sure it will ever feel as natural as macOS to me. It can do everything I need no problem, but some of the Metro/Fluent design inconsistencies and very involved ways of getting certain tasks done (try using Task Scheduler) make it clear that there are definitely parts of Windows 10 that were swept under the rug.&lt;/p&gt;
&lt;p&gt;The big surprise for me was how good the current state of the Windows Subsystem for Linux is and how well it can take care of my web development needs.&lt;/p&gt;
&lt;hr class=&quot;full&quot;/&gt;&lt;p class=&quot;larger&quot;&gt;But for how fast the PC is, it only makes me realize how much I'm being held back by Adobe's subpar Lightroom optimization.&lt;/p&gt;
&lt;p&gt;It's like I have a fast hypercar but I can only to use bald tires and a worn clutch that can't put all the power to use. I have been editing photos in Lightroom for years. I even remember when &lt;a href=&quot;https://paulstamatiou.com/its-true-adobe-lightroom-the-aperture-killer/&quot;&gt;the beta of the first version came out&lt;/a&gt; in 2006! At that time photographers had Photoshop, Camera Raw and Bridge to do all their RAW photo editing. It wasn't the best workflow and Photoshop had become an unwieldy behemoth to photographers wanting to focus on more bulk, basic image edits.&lt;/p&gt;
&lt;p&gt;I don't see myself entirely leaving Lightroom for one of the many competitors: notably &lt;a href=&quot;https://www.phaseone.com/en/Products/Software/Capture-One-Pro/Tutorials.aspx&quot; title=&quot;Capture One Pro&quot;&gt;Capture One&lt;/a&gt; but there's also &lt;a href=&quot;http://www.dxo.com/us/photography/photo-software/dxo-photolab&quot; title=&quot;Advanced Photo Editing Solution&quot;&gt;DxO PhotoLab&lt;/a&gt;, &lt;a href=&quot;https://affinity.serif.com/en-us/photo/&quot; title=&quot;Professional photo editing software&quot;&gt;Affinity Photo&lt;/a&gt;, &lt;a href=&quot;https://macphun.com/luminar&quot; title=&quot;Luminar 2018. The all-new photo editor.&quot;&gt;Luminar&lt;/a&gt; and &lt;a href=&quot;https://www.on1.com/&quot; title=&quot;Your entire workflow in one app. The perfect plug-in or alternative to Lightroom &amp;amp; Photoshop.&quot;&gt;ON1 Photo RAW&lt;/a&gt;. I've given other applications a shot but always come back to Lightroom.&lt;/p&gt;
&lt;p&gt;While I'm really tied to the Develop module in Lightroom, maybe there are other parts of my workflow I can optimize. The best candidate would be trying to do my photo &lt;strong&gt;culling outside of Lightroom&lt;/strong&gt;. That would mean fewer photos imported, fewer photos that need to have previews generated and so on. Even with 1:1 previews generated in Lightroom, navigating around photos in the Library module fullscreen view is not lightning quick.&lt;/p&gt;
&lt;p&gt;There are two programs that come to mind for quick RAW viewing and culling: &lt;a href=&quot;http://www.camerabits.com/tour-v5/&quot; title=&quot;Photo Mechanic by Camera Bits&quot;&gt;Photo Mechanic&lt;/a&gt; and &lt;a href=&quot;https://www.fastrawviewer.com/&quot; title=&quot;The Essential Workflow Tool for Every RAW Shooter&quot;&gt;Fast Raw Viewer&lt;/a&gt;. Photo Mechanic has a pretty powerful culling workflow and related file management functionality. However, if I shoot RAWs alone it only views the small embedded JPEG inside the RAW file, which in the case of my Sony A7r III seems to be 1616x1080. &lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-50&quot; id=&quot;r50&quot;&gt;50&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Fast Raw Viewer lacks some of the advanced culling features and is more focused on being a RAW viewer. It views the embedded JPEG inside the RAW file, but you can also set it to process the RAW and render that original image instead of the small embedded JPEG. The latter is not quite as fast as just displaying the JPEG but is handy when you want to zoom all the way in.&lt;/p&gt;
&lt;p&gt;With Photo Mechanic you open your folder of RAW files then browse through them and tap a key to tag (select) any shot you want to end up importing into Lightroom. You can also assign a rating or color value during this process. When you're done culling, you can have Photo Mechanic only show the selected items, then you can simply drag them into Lightroom. Lightroom will pop open an import dialog and only the photos you culled will get added to Lightroom. And as long as you specified to have the photos added and not copied, any rating or other metadata you added in Photo Mechanic is visible in Lightroom as well.&lt;/p&gt;
&lt;p&gt;If your Lightroom catalog allows for changes to automatically be written into XMP, any rating and metadata updated in Lightroom will be reflected in Photo Mechanic instantly. Unfortunately, it does not appear to work the other way around automatically: you'd have to go to the Metadata menu and select &quot;Read metadata from file&quot; to have it update in Lightroom.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-photomechanic-2000.jpg&quot;&gt;&lt;img data-ratio=&quot;1.342&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-photomechanic-1500.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;&lt;strong&gt;Photo Mechanic 5:&lt;/strong&gt; Powerful though not the most aesthetically appealing UI to work in..&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;47.973741794311&quot;&gt;&lt;p&gt;The benefit of simply viewing the embedded JPEG is that loading and going between photos truly is instant. Though for my exact needs, it leaves me wanting more. My typical culling process is not based on viewing the overall composition. I usually want to zoom 100% into the photo to see if the focus is sharp — for example if I have many similar shots of the same scene and only want to keep the one with the best focus, especially when I shoot with manual focus. Furthermore, sometimes I don't know whether I should keep a photo until I tinker with basic Develop settings in Lightroom like tone and cropping.&lt;/p&gt;
&lt;p&gt;For the first issue about not being able to see the original, I reached out to the maker of Photo Mechanic. They offered one workaround for not being able to render RAWs as an option in the Windows version: just telling my camera to shoot RAW+JPEG. Photo Mechanic then displays the pair as one item and lets me view the full-size image quickly. Seems interesting but that would significantly increase my storage needs while I'm traveling. If my New Zealand trip exceeded 800GB I can't imagine how many more SD cards I'd have to carry along if I had RAW+JPEG enabled.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-51&quot; id=&quot;r51&quot;&gt;51&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Then there's my desire for some basic editing functionality during culling just to see if a shot is worth keeping. Ideally, I would be able tag photos in Photo Mechanic from a set of photos I had already imported into Lightroom, and be able to see the new selections automatically appear in a rating filter I had in Lightroom. That would let me select a photo in Photo Mechanic, see it appear in Lightroom and then tinker with Develop settings or further zoom into the photo if it's one of those shots where I'm not sure I want to keep it yet. Like a two-way sync.&lt;/p&gt;
&lt;p&gt;I do recognize that there is tremendous value in only having a quick preview functionality and I'll keep tinkering to try to change or improve my workflow with it in some way.&lt;/p&gt;
&lt;hr class=&quot;full&quot;/&gt;&lt;p class=&quot;larger&quot;&gt;But here we are in 2018 and Lightroom feels like a sluggish, unwieldy behemoth. Is history set to repeat itself?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It looks that way.&lt;/strong&gt; Adobe recently spun off a simpler version called &lt;strong&gt;Lightroom CC&lt;/strong&gt;. That's &lt;em&gt;not&lt;/em&gt; the Lightroom Classic CC I've been referring to in this article. If I had to guess, it seems like Adobe has some different goals for Lightroom CC:&lt;/p&gt;
&lt;ul readability=&quot;3&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Entice new photographers and inspire casual photographers to try out the Lightroom ecosystem.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;3&quot;&gt;
&lt;p&gt;Enable entirely new and simple synced and mobile workflows for the casual user, functionality enabled by requiring all photos to be backed up to Adobe's cloud. Also, remove the traditional pain point of storage management.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;Make some extra money in the process by requiring everyone to pay for each terabyte of cloud storage. (Though it appears that the most you could ever have is 10TB which excludes most prosumer/professional photographers.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Lightroom CC is a native application for your computer along with companion mobile apps. It has a much simpler interface. There are no module tabs like in Lightroom Classic CC, just a pane on one side that is like a basic Library module for browsing and managing your imports. And then there is a pane on the right that can expand to show Develop-like functionality, now called Edit. It's definitely more pleasant on the eyes. Labels and sliders are larger and easier to use on 4K display.&lt;/p&gt;
&lt;p&gt;Upon importing a set of photos, Lightroom CC immediately goes to work uploading all of those massive RAW files. You can pause it temporarily, but eventually they will need to be uploaded. You can specify how much of your local drive to give up for the photo cache and Lightroom CC will selectively remove and download photos when that cache gets filled.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;photoset&quot;&gt;&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-cc-nelsonlakes-newzealand-2500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.466&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-cc-nelsonlakes-newzealand-1500.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;div class=&quot;photoset-row&quot;&gt;&lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-cc-grid-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;1.435&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-cc-grid-1280.jpg&quot;/&gt;&lt;/a&gt; &lt;a href=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-cc-nelsonlakes-newzealand-closeup-1500.jpg&quot;&gt;&lt;img data-ratio=&quot;0.917&quot; src=&quot;https://turbo.paulstamatiou.com/assets/gray-squircles-loader-3.gif&quot; data-src=&quot;https://turbo.paulstamatiou.com/uploads/2018/01/copyright-paulstamatiou_com-lightroom-cc-nelsonlakes-newzealand-closeup-1000.jpg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;small class=&quot;caption&quot;&gt;Top and bottom right: Nelson Lakes National Park, New Zealand.&lt;br/&gt;Bottom left: Wharariki Beach and the Archway Islands, New Zealand.&lt;/small&gt;&lt;/section&gt;&lt;section readability=&quot;36.231211317418&quot;&gt;&lt;p&gt;Performance-wise, it felt identical to Lightroom Classic CC in my brief back and forth comparison between the two applications. I think it was just the less cluttered and simpler interface that tricked me into liking Lightroom CC more initially and thinking it felt faster than it actually was.&lt;/p&gt;
&lt;p&gt;If I like the UI of Lightroom CC more, then why don't I just use it? It &lt;a href=&quot;https://www.lightroomqueen.com/lightroom-cc-vs-classic-features&quot; title=&quot;Lightroom CC vs. Lightroom Classic – Which Do I Need?&quot;&gt;definitely lacks a few things&lt;/a&gt;. Well, I shouldn't say &lt;em&gt;lack&lt;/em&gt; because the targeted audience for Lightroom CC probably doesn't care about these things:&lt;/p&gt;
&lt;ul readability=&quot;2.3840579710145&quot;&gt;&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Ability to merge HDRs or stitch panoramas. This is a big one.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-0.78378378378378&quot;&gt;
&lt;p&gt;Ability to split original files across various drives (I &lt;a href=&quot;https://paulstamatiou.com/storage-for-photographers-part-2/&quot;&gt;love my NAS flow&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Support for multiple catalogs&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;Support for tethering and watched folders (Though I've only ever used this functionality once for a portrait studio)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Color labeling&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Ability to set custom sort order&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Range masking&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;That list used to include presets and tone curve adjustments but they were added recently.&lt;/p&gt;
&lt;p&gt;Another part of the Lightroom CC story is around using machine learning — Adobe calls their tech Sensei and it appears to be used in two ways so far. First, the &quot;auto&quot; button now takes the shot into account and tries to find ideal settings for it compared to other shots.&lt;sup&gt;&lt;a rel=&quot;footnote&quot; href=&quot;https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/#footnote-52&quot; id=&quot;r52&quot;&gt;52&lt;/a&gt;&lt;/sup&gt; Second, Sensei enables easier searching by automatically tagging photos with keywords.&lt;/p&gt;
&lt;hr class=&quot;full&quot;/&gt;&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;W&lt;/span&gt;hile I like the pitch and how bold Adobe is being with the cloud and machine learning approach, I won't be leaving Lightroom Classic CC anytime soon. Classic is for the high volume photographers that can handle and prefer to manage their own asset storage (and have numerous backups) and who aren't keen on doing any kind of real editing on a mobile device.&lt;/p&gt;
&lt;p&gt;But Lightroom CC is good for one thing: it shows me that &lt;strong&gt;Adobe knows what they're doing&lt;/strong&gt; and can make some good software. I hope that this will trickle its way down into future Classic improvements. And if one day Lightroom puts multiple cores to use efficiently, you can be sure I'll come back here with another long post about building a 24+ core machine optimized for Lightroom.&lt;/p&gt;
&lt;h6&gt;Please share :)&lt;/h6&gt;
&lt;p&gt;If you enjoyed this post, please share it with your friends and followers. It took me several months of spare time to write this and is currently my longest article out of 1,200+ on this site.&lt;/p&gt;
&lt;/section&gt;&lt;footer class=&quot;postex-meta clearfix&quot;&gt;&lt;aside id=&quot;post-content&quot; class=&quot;clearfix&quot;&gt;&lt;a href=&quot;https://twitter.com/intent/tweet?text=I%20enjoyed%20this%20post:%20&amp;quot;Building%20a%20Lightroom%20PC&amp;quot;&amp;amp;url=http%3A%2F%2Fpaulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/&amp;amp;via=Stammy&quot; target=&quot;_blank&quot; id=&quot;twttr_btn&quot;&gt;Like it? Tweet&lt;/a&gt; &lt;time class=&quot;date&quot; datetime=&quot;2018-01-22 08:00:00 -0800&quot;&gt;Published &lt;span&gt;22 Jan 2018&lt;/span&gt;&lt;/time&gt;&lt;/aside&gt;&lt;div class=&quot;fluid-notice&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;fluid-notice-wrap in-post&quot; readability=&quot;32&quot;&gt;
&lt;p&gt;Get new articles via email &lt;span&gt;I only publish once every few months.&lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/footer&gt;&lt;section&gt;&lt;ul id=&quot;post-grid&quot; class=&quot;after-content-list post-grid-wide list-none clearfix&quot;&gt;&lt;li class=&quot;clearfix&quot;&gt;
&lt;header class=&quot;entry-title&quot; readability=&quot;26&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;32&quot;&gt;
&lt;div class=&quot;hero-wrap&quot;&gt;&lt;img class=&quot;hero-img&quot; src=&quot;https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8402-750.jpg&quot;/&gt;&lt;/div&gt;
&lt;h2&gt;Building a Lightroom PC&lt;/h2&gt;
&lt;p class=&quot;oneliner&quot;&gt;Why I switched to Windows and built a water-cooled 5.2GHz 6-core editing machine&lt;/p&gt;
&lt;/div&gt;
&lt;/header&gt;&lt;/li&gt;
&lt;li class=&quot;clearfix&quot;&gt;
&lt;header class=&quot;entry-title&quot; readability=&quot;26&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;32&quot;&gt;
&lt;div class=&quot;hero-wrap&quot;&gt;&lt;img class=&quot;hero-img&quot; src=&quot;https://turbo.paulstamatiou.com/uploads/2017/01/copyright-paulstamatiou_com-DSC02218-500.jpg&quot;/&gt;&lt;/div&gt;
&lt;h2&gt;Reading more&lt;/h2&gt;
&lt;p class=&quot;oneliner&quot;&gt;Why I got a Kindle and set a goal to read 24 books in 2017&lt;/p&gt;
&lt;/div&gt;
&lt;/header&gt;&lt;/li&gt;
&lt;li class=&quot;clearfix&quot;&gt;
&lt;header class=&quot;entry-title&quot; readability=&quot;26&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;32&quot;&gt;
&lt;div class=&quot;hero-wrap&quot;&gt;&lt;img class=&quot;hero-img&quot; src=&quot;https://turbo.paulstamatiou.com/uploads/2016/06/pstam-rpi-photo-frame-DSC00108-500.jpg&quot;/&gt;&lt;/div&gt;
&lt;h2&gt;Getting started with Raspberry Pi&lt;/h2&gt;
&lt;p class=&quot;oneliner&quot;&gt;What you can do with a tiny $35 computer and how I built a digital photo frame&lt;/p&gt;
&lt;/div&gt;
&lt;/header&gt;&lt;/li&gt;
&lt;li class=&quot;clearfix&quot;&gt;
&lt;header class=&quot;entry-title&quot; readability=&quot;26&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;32&quot;&gt;
&lt;div class=&quot;hero-wrap&quot;&gt;&lt;img class=&quot;hero-img&quot; src=&quot;https://turbo.paulstamatiou.com/uploads/2015/05/DSC01751-500.jpg&quot;/&gt;&lt;/div&gt;
&lt;h2&gt;Storage for Photographers (Part 2)&lt;/h2&gt;
&lt;p class=&quot;oneliner&quot;&gt;How 12TB of network-attached storage changed my digital life and why you need it too.&lt;/p&gt;
&lt;/div&gt;
&lt;/header&gt;&lt;/li&gt;
&lt;li class=&quot;clearfix&quot;&gt;
&lt;header class=&quot;entry-title&quot; readability=&quot;26&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;32&quot;&gt;
&lt;div class=&quot;hero-wrap&quot;&gt;&lt;img class=&quot;hero-img&quot; src=&quot;https://turbo.paulstamatiou.com/uploads/2015/01/pstam-gopro-folegandros-mtn-500.jpg&quot;/&gt;&lt;/div&gt;
&lt;h2&gt;Traveling and photography (Part 1)&lt;/h2&gt;
&lt;p class=&quot;oneliner&quot;&gt;How and why I create photo stories.&lt;/p&gt;
&lt;/div&gt;
&lt;/header&gt;&lt;/li&gt;
&lt;li class=&quot;clearfix&quot;&gt;
&lt;header class=&quot;entry-title&quot; readability=&quot;26.5&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;33&quot;&gt;
&lt;div class=&quot;hero-wrap&quot;&gt;&lt;img class=&quot;hero-img&quot; src=&quot;https://turbo.paulstamatiou.com/uploads/2014/07/DSC00711-500.jpg&quot;/&gt;&lt;/div&gt;
&lt;h2&gt;Getting started with drones&lt;/h2&gt;
&lt;p class=&quot;oneliner&quot;&gt;Everything you need to know about how drones work, how to fly them and modify them.&lt;/p&gt;
&lt;/div&gt;
&lt;/header&gt;&lt;/li&gt;
&lt;li class=&quot;clearfix&quot;&gt;
&lt;header class=&quot;entry-title&quot; readability=&quot;26.5&quot;&gt;
&lt;div class=&quot;content&quot; readability=&quot;33&quot;&gt;
&lt;div class=&quot;hero-wrap&quot;&gt;&lt;img class=&quot;hero-img&quot; src=&quot;https://turbo.paulstamatiou.com/uploads/2013/04/pstam_simplify_500.jpg&quot;/&gt;&lt;/div&gt;
&lt;h2&gt;Simplify.&lt;/h2&gt;
&lt;p class=&quot;oneliner&quot;&gt;Wherein I realize how to own less, worry less and live more.&lt;/p&gt;
&lt;/div&gt;
&lt;/header&gt;&lt;/li&gt;
&lt;li&gt;
&lt;div class=&quot;view-more-posts&quot;&gt;

&lt;small&gt;1200 posts since '05&lt;/small&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/section&gt;&lt;div id=&quot;tweet_it&quot; class=&quot;animated-long&quot; readability=&quot;5.7087378640777&quot;&gt;&lt;a href=&quot;javascript:void(0)&quot; id=&quot;tweet_compose_close&quot; class=&quot;close_it&quot;&gt;&lt;img src=&quot;https://turbo.paulstamatiou.com/assets/x.png&quot;/&gt;&lt;/a&gt;
&lt;h4&gt;Like it? Tweet it.&lt;/h4&gt;
&lt;p&gt;&quot;Building a Lightroom PC&quot; by @Stammy&lt;/p&gt;

&lt;footer&gt;&lt;h4&gt;Get new articles via email&lt;/h4&gt;

&lt;/footer&gt;&lt;/div&gt;</description>
<pubDate>Wed, 24 Jan 2018 04:23:04 +0000</pubDate>
<dc:creator>chambo622</dc:creator>
<og:title>Building a Lightroom PC</og:title>
<og:url>https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/</og:url>
<og:type>article</og:type>
<og:image>https://turbo.paulstamatiou.com/uploads/2017/12/copyright-paulstamatiou_com-DSC8402-750.jpg</og:image>
<og:description>Why I switched to Windows 10 from macOS and built a water-cooled and overclocked 5.2GHz 6-core PC for photo editing with Adobe Lightroom Classic CC.</og:description>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://paulstamatiou.com/building-a-windows-10-lightroom-photo-editing-pc/</dc:identifier>
</item>
<item>
<title>The new microcode from Intel and AMD adds three new features</title>
<link>https://lkml.org/lkml/2018/1/22/598</link>
<guid isPermaLink="true" >https://lkml.org/lkml/2018/1/22/598</guid>
<description>&lt;pre itemprop=&quot;articleBody&quot;&gt;
On Sun, 2018-01-21 at 14:27 -0800, Linus Torvalds wrote:&lt;br /&gt;&amp;gt; On Sun, Jan 21, 2018 at 2:00 PM, David Woodhouse &amp;lt;dwmw2@infradead.org&amp;gt; wrote:&lt;br /&gt;&amp;gt; &amp;gt;&amp;gt;&lt;br /&gt;&amp;gt; &amp;gt;&amp;gt; The patches do things like add the garbage MSR writes to the kernel&lt;br /&gt;&amp;gt; &amp;gt;&amp;gt; entry/exit points. That's insane. That says &quot;we're trying to protect&lt;br /&gt;&amp;gt; &amp;gt;&amp;gt; the kernel&quot;.  We already have retpoline there, with less overhead.&lt;br /&gt;&amp;gt; &amp;gt;&lt;br /&gt;&amp;gt; &amp;gt; You're looking at IBRS usage, not IBPB. They are different things.&lt;br /&gt;&amp;gt; &lt;br /&gt;&amp;gt; Ehh. Odd intel naming detail.&lt;br /&gt;&amp;gt; &lt;br /&gt;&amp;gt; If you look at this series, it very much does that kernel entry/exit&lt;br /&gt;&amp;gt; stuff. It was patch 10/10, iirc. In fact, the patch I was replying to&lt;br /&gt;&amp;gt; was explicitly setting that garbage up.&lt;br /&gt;&amp;gt; &lt;br /&gt;&amp;gt; And I really don't want to see these garbage patches just mindlessly&lt;br /&gt;&amp;gt; sent around.&lt;p&gt;I think we've covered the technical part of this now, not that you like&lt;br /&gt;it — not that any of us *like* it. But since the peanut gallery is&lt;br /&gt;paying lots of attention it's probably worth explaining it a little&lt;br /&gt;more for their benefit.&lt;/p&gt;&lt;p&gt;This is all about Spectre variant 2, where the CPU can be tricked into&lt;br /&gt;mispredicting the target of an indirect branch. And I'm specifically&lt;br /&gt;looking at what we can do on *current* hardware, where we're limited to&lt;br /&gt;the hacks they can manage to add in the microcode.&lt;/p&gt;&lt;p&gt;The new microcode from Intel and AMD adds three new features.&lt;/p&gt;&lt;p&gt;One new feature (IBPB) is a complete barrier for branch prediction.&lt;br /&gt;After frobbing this, no branch targets learned earlier are going to be&lt;br /&gt;used. It's kind of expensive (order of magnitude ~4000 cycles).&lt;/p&gt;&lt;p&gt;The second (STIBP) protects a hyperthread sibling from following branch&lt;br /&gt;predictions which were learned on another sibling. You *might* want&lt;br /&gt;this when running unrelated processes in userspace, for example. Or&lt;br /&gt;different VM guests running on HT siblings.&lt;/p&gt;&lt;p&gt;The third feature (IBRS) is more complicated. It's designed to be&lt;br /&gt;set when you enter a more privileged execution mode (i.e. the kernel).&lt;br /&gt;It prevents branch targets learned in a less-privileged execution mode,&lt;br /&gt;BEFORE IT WAS MOST RECENTLY SET, from taking effect. But it's not just&lt;br /&gt;a 'set-and-forget' feature, it also has barrier-like semantics and&lt;br /&gt;needs to be set on *each* entry into the kernel (from userspace or a VM&lt;br /&gt;guest). It's *also* expensive. And a vile hack, but for a while it was&lt;br /&gt;the only option we had.&lt;/p&gt;&lt;p&gt;Even with IBRS, the CPU cannot tell the difference between different&lt;br /&gt;userspace processes, and between different VM guests. So in addition to&lt;br /&gt;IBRS to protect the kernel, we need the full IBPB barrier on context&lt;br /&gt;switch and vmexit. And maybe STIBP while they're running.&lt;/p&gt;&lt;p&gt;Then along came Paul with the cunning plan of &quot;oh, indirect branches&lt;br /&gt;can be exploited? Screw it, let's not have any of *those* then&quot;, which&lt;br /&gt;is retpoline. And it's a *lot* faster than frobbing IBRS on every entry&lt;br /&gt;into the kernel. It's a massive performance win.&lt;/p&gt;&lt;p&gt;So now we *mostly* don't need IBRS. We build with retpoline, use IBPB&lt;br /&gt;on context switches/vmexit (which is in the first part of this patch&lt;br /&gt;series before IBRS is added), and we're safe. We even refactored the&lt;br /&gt;patch series to put retpoline first.&lt;/p&gt;&lt;p&gt;But wait, why did I say &quot;mostly&quot;? Well, not everyone has a retpoline&lt;br /&gt;compiler yet... but OK, screw them; they need to update.&lt;/p&gt;&lt;p&gt;Then there's Skylake, and that generation of CPU cores. For complicated&lt;br /&gt;reasons they actually end up being vulnerable not just on indirect&lt;br /&gt;branches, but also on a 'ret' in some circumstances (such as 16+ CALLs&lt;br /&gt;in a deep chain).&lt;/p&gt;&lt;p&gt;The IBRS solution, ugly though it is, did address that. Retpoline&lt;br /&gt;doesn't. There are patches being floated to detect and prevent deep&lt;br /&gt;stacks, and deal with some of the other special cases that bite on SKL,&lt;br /&gt;but those are icky too. And in fact IBRS performance isn't anywhere&lt;br /&gt;near as bad on this generation of CPUs as it is on earlier CPUs&lt;br /&gt;*anyway*, which makes it not quite so insane to *contemplate* using it&lt;br /&gt;as Intel proposed.&lt;/p&gt;&lt;p&gt;That's why my initial idea, as implemented in this RFC patchset, was to&lt;br /&gt;stick with IBRS on Skylake, and use retpoline everywhere else. I'll&lt;br /&gt;give you &quot;garbage patches&quot;, but they weren't being &quot;just mindlessly&lt;br /&gt;sent around&quot;. If we're going to drop IBRS support and accept the&lt;br /&gt;caveats, then let's do it as a conscious decision having seen what it&lt;br /&gt;would look like, not just drop it quietly because poor Davey is too&lt;br /&gt;scared that Linus might shout at him again. :)&lt;/p&gt;&lt;p&gt;I have seen *hand-wavy* analyses of the Skylake thing that mean I'm not&lt;br /&gt;actually lying awake at night fretting about it, but nothing concrete&lt;br /&gt;that really says it's OK.&lt;/p&gt;&lt;p&gt;If you view retpoline as a performance optimisation, which is how it&lt;br /&gt;first arrived, then it's rather unconventional to say &quot;well, it only&lt;br /&gt;opens a *little* bit of a security hole but it does go nice and fast so&lt;br /&gt;let's do it&quot;.&lt;/p&gt;&lt;p&gt;But fine, I'm content with ditching the use of IBRS to protect the&lt;br /&gt;kernel, and I'm not even surprised. There's a *reason* we put it last&lt;br /&gt;in the series, as both the most contentious and most dispensable part.&lt;br /&gt;I'd be *happier* with a coherent analysis showing Skylake is still OK,&lt;br /&gt;but hey-ho, screw Skylake.&lt;/p&gt;&lt;p&gt;The early part of the series adds the new feature bits and detects when&lt;br /&gt;it can turn KPTI off on non-Meltdown-vulnerable Intel CPUs, and also&lt;br /&gt;supports the IBPB barrier that we need to make retpoline complete. That&lt;br /&gt;much I think we definitely *do* want. There have been a bunch of us&lt;br /&gt;working on this behind the scenes; one of us will probably post that&lt;br /&gt;bit in the next day or so.&lt;/p&gt;&lt;p&gt;I think we also want to expose IBRS to VM guests, even if we don't use&lt;br /&gt;it ourselves. Because Windows guests (and RHEL guests; yay!) do use it.&lt;/p&gt;&lt;p&gt;If we can be done with the shouty part, I'd actually quite like to have&lt;br /&gt;a sensible discussion about when, if ever, we do IBPB on context switch&lt;br /&gt;(ptraceability and dumpable have both been suggested) and when, if&lt;br /&gt;ever, we set STIPB in userspace.&lt;/p&gt;&lt;p&gt;[unhandled content-type:application/x-pkcs7-signature]
&lt;/p&gt;&lt;/pre&gt;</description>
<pubDate>Wed, 24 Jan 2018 03:10:39 +0000</pubDate>
<dc:creator>DoreenMichele</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://lkml.org/lkml/2018/1/22/598</dc:identifier>
</item>
<item>
<title>People leave managers, not companies</title>
<link>https://blog.intercom.com/people-leave-managers-not-companies/</link>
<guid isPermaLink="true" >https://blog.intercom.com/people-leave-managers-not-companies/</guid>
<description>&lt;div readability=&quot;48.959116022099&quot;&gt;
&lt;p class=&quot;opening_paragraph&quot;&gt;Do people really leave managers, rather than companies? And if so, how do you fix the problem?&lt;/p&gt;
&lt;p&gt;The data suggests bad management is a real and significant issue. According to &lt;a href=&quot;http://www.gallup.com/services/182216/state-american-manager-report.aspx&quot;&gt;a study by Gallup&lt;/a&gt;, one in two people admitted to having left a job to get away from a bad manager. In fact, 70% of the factors that contribute to your happiness at work are directly related to your manager.&lt;/p&gt;
&lt;p&gt;Yet few managers ask themselves the hard questions: Am I the reason people are leaving? Was it because of something I did, or something I didn’t do? In my experience, managers suffer something akin to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect&quot;&gt;Dunning-Kruger effect&lt;/a&gt;. They assume they’re not the problem, but that their employees are.&lt;/p&gt;
&lt;p&gt;Just look at the data to see where managers are laying blame. A &lt;a href=&quot;http://www.leadershipiq.com/blogs/leadershipiq/35354241-why-new-hires-fail-emotional-intelligence-vs-skills&quot;&gt;survey of 5,247 hiring managers&lt;/a&gt; who’d hired 20,000 employees said that after 18 months “46% of newly-hired employees” failed and only 20% achieved success.&lt;/p&gt;
&lt;/div&gt;&lt;div readability=&quot;32&quot;&gt;
&lt;p&gt;The biggest causes for failure were:&lt;/p&gt;
&lt;/div&gt;&lt;div readability=&quot;151.31871298223&quot;&gt;
&lt;p&gt;And what was the biggest takeaway for managers interviewed? They needed &lt;strong&gt;better interview processes&lt;/strong&gt; to weed out the failures before they got into their teams.&lt;/p&gt;
&lt;p&gt;To any rational person, this makes no sense. The common denominator isn’t the 46% of employees. &lt;strong&gt;It’s the managers&lt;/strong&gt;. Is the problem really that half of the people couldn’t learn or care about their new job? Or was it that a smaller number of managers overestimated their own abilities to teach, connect and inspire their new hires?&lt;/p&gt;
&lt;p&gt;I’ve led teams of engineers for almost a decade but when I look back at my own management career I regularly thought I was a “good manager”, at times even a great one. I attributed any problems I encountered to the people I managed rather than myself. Looking back now it’s clear I was actually a blindingly naive, over-confident, under-skilled, inexperienced manager who made lots of mistakes.&lt;/p&gt;
&lt;p&gt;This management overconfidence is a trap I see many other falling into, but thankfully it can be avoided by following a few of the steps below.&lt;/p&gt;
&lt;h2&gt;1. Always ask for advice&lt;/h2&gt;
&lt;p&gt;As a software engineer, no matter how senior you are, you always seek code reviews before deploying new code to production. YOLO code changes are only for emergencies 😀&lt;/p&gt;
&lt;p&gt;The same is true for managing people. You should always be asking for peer review on a people management problem before you take action, whether that’s from your own manager, your HR team, etc. The bigger the potential consequence of your action, the more thorough you should be about getting help and peer review before making it.&lt;/p&gt;
&lt;p&gt;When I have a performance review to write I talk it through with my peers or my boss before delivering it. If I have someone on my team who’s not performing as well as I’d like I ask for help on how to coach them differently. You can do this for pretty much everything, people related or not, and have a better outcome as a result.&lt;/p&gt;
&lt;p class=&quot;quote&quot;&gt;Once you think you’ve really nailed your management skills is when you’re at your most vulnerable to failure.&lt;/p&gt;
&lt;h2&gt;2. Don’t blame, own&lt;/h2&gt;
&lt;p&gt;Ownership is one of our engineering team’s values and it applies equally to managing people as it does to engineering. It’s not in our culture to say “that’s not my problem” or “that’s not my fault”.&lt;/p&gt;
&lt;p&gt;Yet as a manager it’s all too easy to abdicate responsibility and blame the person on the other side of the table when they are not doing as well as you’d like. The fact is every person you manage is different. Just because one management style worked with the previous three team members and it’s not “working” with the person in front of me doesn’t necessarily mean it’s their fault. Like a good sports coach, managers aren’t judged on the performance of individual players, but the team’s effort. So the onus is on you to make sure the right management style is being used for all of your team.&lt;/p&gt;
&lt;p&gt;I have to constantly check myself to make sure I’m taking ownership for the relationships in my team and help do everything I can to make things better. Instead of assigning blame to others when things go wrong, I try and figure out out my own role in the situation. What are the things that I have done, or not done, that have contributed to the problems at hand?&lt;/p&gt;
&lt;p&gt;If I have any trouble identifying these, I go back back to step 1 above, and ask for advice from my peers. Often after doing this exercise I discover the right next step is not to give “constructive feedback” to someone on my team about their mistakes; it’s to get feedback on how I can better support them in future.&lt;/p&gt;
&lt;h2&gt;3. Give feedback with empathy&lt;/h2&gt;
&lt;p&gt;Giving feedback is your most powerful tool for growing your people and your team. It’s also one of the hardest to get right. Do it wrong, and you’ll turn your most proactive tool into something that causes unwanted collateral damage.&lt;/p&gt;
&lt;p&gt;Before I deliver any feedback I set aside some time to understand how the other person might receive the feedback. Is it likely to be high-fives all round? Or might they feel hurt or upset? Try to pay particular attention to the wider implications of the feedback you’re giving. Candid feedback could be motivating for a seasoned engineer looking to step up to the next level, but crippling for a recent hire who’s finding it hard to settle in. Don’t give feedback in the moment if the tone or timing isn’t right. Use your emotional intelligence and empathy. You can always save your feedback for another time, and a more private or less stressful setting.&lt;/p&gt;
&lt;h2&gt;4. Savor your success&lt;/h2&gt;
&lt;p&gt;One of the best things about being an engineer at a startup are the feedback loops. You can prioritize, design, build, and ship into the hands of users, all within a week, often even a day or two. These feedback loops are fuel for your team’s morale; they have a positive influence on your team’s productivity and power each team member’s happiness and job satisfaction.&lt;/p&gt;
&lt;p&gt;Unfortunately it’s easy for managers to get caught up with looking forward all the time. Or to focus exclusively on problem areas. When you do this it’s easy for your team’s shields to drop. They feel unvalued, like just another cog in the machine.&lt;/p&gt;
&lt;p&gt;So make sure you celebrate your team’s success and make sure team members know they’re having impact. You don’t have to hang bunting from the ceiling. It could be as simple as giving someone detailed, timely feedback on something they just did really well, passing along some positive feedback you received from your boss about your team’s recent work or calling someone’s work out positively in a Slack channel.&lt;/p&gt;
&lt;h2&gt;Great managers build great companies&lt;/h2&gt;
&lt;p&gt;When you stay humble, ask for advice, own rather than blame and give feedback with empathy, you’re on the path to being a good manager. But as we’ve seen, success as a manager breeds complacency, and it’s all too easy to fall into bad habits of overconfidence. Once you think you’ve really nailed your management skills is when you’re at your most vulnerable to failure. You need to constantly revisit what it takes to be a good manager, or risk losing your best people for good.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;If this sounds like a management philosophy that you’d like to live and work by, we’re hiring a &lt;a href=&quot;http://grnh.se/vhec3n1&quot;&gt;Software Engineering Manager&lt;/a&gt; in Dublin 😀&lt;/p&gt;
&lt;/div&gt;</description>
<pubDate>Wed, 24 Jan 2018 02:50:54 +0000</pubDate>
<dc:creator>theyeti</dc:creator>
<og:type>article</og:type>
<og:title>People leave managers, not companies. Don't let that manager be you. - Inside Intercom</og:title>
<og:description>One in two people admit to leaving a job to get away from a bad manager. So what does it take to be a good manager, and make sure good people don't leave?</og:description>
<og:url>https://blog.intercom.com/people-leave-managers-not-companies/</og:url>
<og:image>https://blog.intercomassets.com/wp-content/uploads/2017/03/07115039/People_Leave_Managers_Not_Companies_Logo.jpg</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.intercom.com/people-leave-managers-not-companies/</dc:identifier>
</item>
<item>
<title>The Google Lunar X Prize’s Race to the Moon Is Over. Nobody Won</title>
<link>https://www.nytimes.com/2018/01/23/science/google-lunar-x-prize-moon.html</link>
<guid isPermaLink="true" >https://www.nytimes.com/2018/01/23/science/google-lunar-x-prize-moon.html</guid>
<description>&lt;p itemprop=&quot;articleBody&quot;&gt;Today, we announce that after consulting our teams over the last few months, that there will not be a launch by March 31st, 2018, and our grand prize will go unclaimed. We are exploring a number of ways to proceed, to continue to support our teams: &lt;a href=&quot;https://t.co/n2jQ8lKWcX&quot;&gt;https://t.co/n2jQ8lKWcX&lt;/a&gt;&lt;/p&gt;</description>
<pubDate>Wed, 24 Jan 2018 00:26:24 +0000</pubDate>
<dc:creator>IntronExon</dc:creator>
<og:url>https://www.nytimes.com/2018/01/23/science/google-lunar-x-prize-moon.html</og:url>
<og:type>article</og:type>
<og:title>The Google Lunar X Prize’s Race to the Moon Is Over. Nobody Won.</og:title>
<og:description>None of the remaining competitors for the $20 million award will be able to get off the ground by March 31, a deadline that had already been extended multiple times.</og:description>
<og:image>https://static01.nyt.com/images/2018/01/24/science/24XPRIZE-promo/24XPRIZE-promo-facebookJumbo.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nytimes.com/2018/01/23/science/google-lunar-x-prize-moon.html</dc:identifier>
</item>
<item>
<title>Branchless Doom</title>
<link>https://github.com/xoreaxeaxeax/movfuscator/tree/master/validation/doom</link>
<guid isPermaLink="true" >https://github.com/xoreaxeaxeax/movfuscator/tree/master/validation/doom</guid>
<description>&lt;h3&gt;README.md&lt;/h3&gt;
&lt;article class=&quot;markdown-body entry-content&quot; itemprop=&quot;text&quot;&gt;&lt;h2&gt;A branchless DOOM&lt;/h2&gt;
&lt;p&gt;This directory provides a branchless, mov-only version of the classic DOOM video game.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/xoreaxeaxeax/movfuscator/blob/master/validation/doom/doom.png&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://github.com/xoreaxeaxeax/movfuscator/raw/master/validation/doom/doom.png&quot; alt=&quot;mov doom&quot;/&gt;&lt;/a&gt;&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;&lt;em&gt;DOOM, running with only mov instructions.&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;This is thought to be entirely secure against the Meltdown and Spectre CPU vulnerabilities, which require speculative execution on branch instructions.&lt;/p&gt;
&lt;p&gt;To build and run a branchless, mov-only, exploit-hardened DOOM:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;# Download the DOOM source code
git clone https://github.com/id-Software/DOOM

# Download a DOOM WAD file
wget http://distro.ibiblio.org/pub/linux/distributions/slitaz/sources/packages/d/doom1.wad -P ./DOOM/linuxdoom-1.10

# Apply the M/o/Vfuscator patches
# These make DOOM compatible with the mov-compiler
patch -s -p0 &amp;lt; doom.patch

# M/o/Vfuscate DOOM
export MOVCC=~/movfuscator/  # your movfuscator source directory
cd DOOM/linuxdoom-1.10
mkdir linux
make

# Play DOOM
./linux/linuxxdoom -episode 0
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The mov-only DOOM renders approximately one frame every 7 hours, so playing this version requires somewhat increased patience.&lt;/p&gt;
&lt;h3&gt;Requirements&lt;/h3&gt;
&lt;p&gt;DOOM requires a 256 color desktop. Setting this up will vary depending on your flavor of Linux, but one approach is proposed below.&lt;/p&gt;
&lt;p&gt;Start an 8-bit X desktop:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;sudo startx -- :1 -depth 8 vt8
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Switch to that desktop with ctrl-alt-f8.&lt;/p&gt;
&lt;h3&gt;Patches&lt;/h3&gt;
&lt;p&gt;Some small patches are applied to DOOM first to fix various build issues. This set of patches corrects some issues in DOOM, replaces some missing library functions like alloca, works around the C warnings that LCC (the compiler frontend) treats as errros, and fixes a specific bug with floating point casts in M/o/Vfuscator.&lt;/p&gt;
&lt;h3&gt;Notes&lt;/h3&gt;
&lt;p&gt;Tested on i386 Debian 7.11.&lt;/p&gt;
&lt;p&gt;This project is entirely tongue-in-cheek, and only serves as a correctness test for the M/o/Vfuscator compiler.&lt;/p&gt;
&lt;/article&gt;</description>
<pubDate>Tue, 23 Jan 2018 23:24:39 +0000</pubDate>
<dc:creator>strangecasts</dc:creator>
<og:image>https://avatars0.githubusercontent.com/u/12904366?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>xoreaxeaxeax/movfuscator</og:title>
<og:url>https://github.com/xoreaxeaxeax/movfuscator</og:url>
<og:description>movfuscator - The single instruction C compiler</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/xoreaxeaxeax/movfuscator/tree/master/validation/doom</dc:identifier>
</item>
<item>
<title>Ursula Le Guin has died</title>
<link>https://www.nytimes.com/2018/01/23/obituaries/ursula-k-le-guin-acclaimed-for-her-fantasy-fiction-is-dead-at-88.html</link>
<guid isPermaLink="true" >https://www.nytimes.com/2018/01/23/obituaries/ursula-k-le-guin-acclaimed-for-her-fantasy-fiction-is-dead-at-88.html</guid>
<description>&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;316&quot; data-total-count=&quot;2231&quot; id=&quot;story-continues-3&quot;&gt;“If you cannot or will not imagine the results of your actions, there’s no way you can act morally or responsibly,” she told The Guardian in an interview in 2005. “Little kids can’t do it; babies are morally monsters — completely greedy. Their imagination has to be trained into foresight and empathy.”&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;145&quot; data-total-count=&quot;2376&quot;&gt;The writer’s “pleasant duty,” she said, is to ply the reader’s imagination with “the best and purest nourishment that it can absorb.”&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;392&quot; data-total-count=&quot;2768&quot;&gt;She was born Ursula Kroeber in Berkeley, Calif., on Oct. 21, 1929, the youngest of four children and the only daughter of two anthropologists, Alfred L. Kroeber and Theodora Quinn Kroeber. Her father was an expert on the Native Americans of California, and her mother wrote an acclaimed book, “Ishi in Two Worlds” (1960), about the life and death of California’s “last wild Indian.”&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;430&quot; data-total-count=&quot;3198&quot;&gt;At a young age, Ms. Le Guin immersed herself in books about mythology, among them James Frazer’s “The Golden Bough,” classic fantasies like Lord Dunsany’s “A Dreamer’s Tales,” and the science-fiction magazines of the day. But in early adolescence she lost interest in science fiction, because, she recalled, the stories “seemed to be all about hardware and soldiers: White men go forth and conquer the universe.”&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;301&quot; data-total-count=&quot;3499&quot;&gt;She graduated from Radcliffe College in 1951, earned a master’s degree in romance literature of the Middle Ages and Renaissance from Columbia University in 1952, and won a Fulbright fellowship to study in Paris. There she met and married another Fulbright scholar, Charles Le Guin, who survives her.&lt;/p&gt;
&lt;span class=&quot;visually-hidden&quot;&gt;Photo&lt;/span&gt;
&lt;div class=&quot;image&quot;&gt;&lt;img src=&quot;https://static01.nyt.com/images/2018/02/14/books/14leguin-alpha/14leguin-alpha-jumbo-v2.jpg&quot; alt=&quot;&quot; class=&quot;media-viewer-candidate&quot; data-mediaviewer-src=&quot;https://static01.nyt.com/images/2018/02/14/books/14leguin-alpha/14leguin-alpha-superJumbo-v2.jpg&quot; data-mediaviewer-caption=&quot;Author Ursula K. Le Guin in July 1996.&quot; data-mediaviewer-credit=&quot;Jill Krementz, All Rights Reserved&quot; itemprop=&quot;url&quot; itemid=&quot;https://static01.nyt.com/images/2018/02/14/books/14leguin-alpha/14leguin-alpha-jumbo-v2.jpg&quot;/&gt;&lt;/div&gt;
&lt;span class=&quot;caption-text&quot;&gt;Author Ursula K. Le Guin in July 1996.&lt;/span&gt; &lt;span class=&quot;credit&quot; itemprop=&quot;copyrightHolder&quot;&gt;&lt;span class=&quot;visually-hidden&quot;&gt;Credit&lt;/span&gt; Jill Krementz, All Rights Reserved&lt;/span&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;199&quot; data-total-count=&quot;3698&quot;&gt;On their return to the United States, she abandoned her graduate studies to raise a family; the Le Guins eventually settled in Portland, where Mr. Le Guin taught history at Portland State University.&lt;/p&gt;
&lt;div id=&quot;story-ad-2&quot; class=&quot;story-ad ad ad-placeholder nocontent robots-nocontent&quot;&gt;

&lt;a class=&quot;visually-hidden skip-to-text-link&quot; href=&quot;https://www.nytimes.com/2018/01/23/obituaries/ursula-k-le-guin-acclaimed-for-her-fantasy-fiction-is-dead-at-88.html#story-continues-4&quot;&gt;Continue reading the main story&lt;/a&gt;&lt;/div&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;170&quot; data-total-count=&quot;3868&quot; id=&quot;story-continues-4&quot;&gt;Besides her husband and son, Ms. Le Guin is survived by two daughters, Caroline and Elisabeth Le Guin; two brothers, Theodore and Clifton Kroeber; and four grandchildren.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;220&quot; data-total-count=&quot;4088&quot;&gt;By the early 1960s Ms. Le Guin had written five unpublished novels, mostly set in an imaginary Central European country called Orsinia. Eager to find a more welcoming market, she decided to try her hand at genre fiction.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;262&quot; data-total-count=&quot;4350&quot;&gt;Her first science-fiction novel, “Rocannon’s World,” came out in 1966. Two years later she published “A Wizard of Earthsea,” the first in a series about a made-up world where the practice of magic is as precise as any science, and as morally ambiguous.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;279&quot; data-total-count=&quot;4629&quot;&gt;The first three Earthsea books — the other two were “The Tombs of Atuan” (1971) and “The Farthest Shore” (1972) — were written, at the request of her publisher, for young adults. But their grand scale and elevated style betray no trace of writing down to an audience.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;322&quot; data-total-count=&quot;4951&quot;&gt;The magic of Earthsea is language-driven: Wizards gain power over people and things by knowing their “true names.” Ms. Le Guin took this discipline seriously in naming her own characters. “I must find the right name or I cannot get on with the story,” she said. “I cannot write the story if the name is wrong.”&lt;/p&gt;
&lt;span class=&quot;visually-hidden&quot;&gt;Photo&lt;/span&gt;
&lt;div class=&quot;image&quot;&gt;&lt;img src=&quot;https://static01.nyt.com/images/2018/01/24/obituaries/24leguin1/24leguin1-master675.jpg&quot; alt=&quot;&quot; class=&quot;media-viewer-candidate&quot; data-mediaviewer-src=&quot;https://static01.nyt.com/images/2018/01/24/obituaries/24leguin1/24leguin1-superJumbo.jpg&quot; data-mediaviewer-caption=&quot;Ms. Le Guin speaking in 2014 at the University of Oregon.&quot; data-mediaviewer-credit=&quot;Jack Liu&quot; itemprop=&quot;url&quot; itemid=&quot;https://static01.nyt.com/images/2018/01/24/obituaries/24leguin1/24leguin1-master675.jpg&quot;/&gt;&lt;/div&gt;
&lt;span class=&quot;caption-text&quot;&gt;Ms. Le Guin speaking in 2014 at the University of Oregon.&lt;/span&gt; &lt;span class=&quot;credit&quot; itemprop=&quot;copyrightHolder&quot;&gt;&lt;span class=&quot;visually-hidden&quot;&gt;Credit&lt;/span&gt; Jack Liu&lt;/span&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;314&quot; data-total-count=&quot;5265&quot;&gt;The Earthsea series was clearly influenced by J. R. R. Tolkien’s “The Lord of the Rings” trilogy. But instead of a holy war between Good and Evil, Ms. Le Guin’s stories are organized around a search for “balance” among competing forces — a concept she adapted from her lifelong study of Taoist texts.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;182&quot; data-total-count=&quot;5447&quot;&gt;She returned to Earthsea later in her career, extending and deepening the trilogy with books like “Tehanu” (1990) and “The Other Wind” (2001), written for a general audience.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;382&quot; data-total-count=&quot;5829&quot;&gt;“The Left Hand of Darkness,” published in 1969, takes place on a planet called Gethen, where people are neither male nor female but assume the attributes of either sex during brief periods of reproductive fervor. Speaking with an anthropological dispassion, Ms. Le Guin later referred to her novel as a “thought experiment” designed to explore the nature of human societies.&lt;/p&gt;
&lt;div id=&quot;story-ad-3&quot; class=&quot;story-ad ad ad-placeholder nocontent robots-nocontent&quot;&gt;

&lt;a class=&quot;visually-hidden skip-to-text-link&quot; href=&quot;https://www.nytimes.com/2018/01/23/obituaries/ursula-k-le-guin-acclaimed-for-her-fantasy-fiction-is-dead-at-88.html#story-continues-5&quot;&gt;Continue reading the main story&lt;/a&gt;&lt;/div&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;75&quot; data-total-count=&quot;5904&quot; id=&quot;story-continues-5&quot;&gt;“I eliminated gender to find out what was left,” she told The Guardian.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;289&quot; data-total-count=&quot;6193&quot;&gt;But there is nothing dispassionate about the relationship at the core of the book, between an androgynous native of Gethen and a human male from Earth. The book won the two major prizes in science fiction, the Hugo and Nebula awards, and is widely taught in secondary schools and colleges.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;368&quot; data-total-count=&quot;6561&quot;&gt;Much of Ms. Le Guin’s science fiction has a common background: a loosely knit confederation of worlds known as the Ekumen. This was founded by an ancient people who seeded humans on habitable planets throughout the galaxy — including Gethen, Earth and the twin worlds of her most ambitious novel, “The Dispossessed,” subtitled “An Ambiguous Utopia” (1974).&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;408&quot; data-total-count=&quot;6969&quot;&gt;As the subtitle implies, “The Dispossessed” contrasts two forms of social organization: a messy but vibrant capitalist society, which oppresses its underclass, and a classless “utopia” (partly based on the ideas of the Russian anarchist Peter Kropotkin), which turns out to be oppressive in its own conformist way. Ms. Le Guin leaves it up to the reader to find a comfortable balance between the two.&lt;/p&gt;


&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;254&quot; data-total-count=&quot;7223&quot;&gt;“The Lathe of Heaven” (1971) offers a very different take on utopian ambitions. A man whose dreams can alter reality falls under the sway of a psychiatrist, who usurps this power to conjure his own vision of a perfect world, with unfortunate results.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;223&quot; data-total-count=&quot;7446&quot;&gt;“The Lathe of Heaven” was among the few books by Ms. Le Guin that have been adapted for film or television. There were two made-for-television versions, one on PBS in 1980 and the other on the A&amp;amp;E cable channel in 2002.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;177&quot; data-total-count=&quot;7623&quot;&gt;Among the other adaptations of her work were the 2006 Japanese animated feature “Tales From Earthsea” and a 2004 mini-series on the Sci Fi channel, “Legend of Earthsea.”&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;99&quot; data-total-count=&quot;7722&quot;&gt;With the exception of the 1980 “Lathe of Heaven,” she had little good to say about any of them.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;318&quot; data-total-count=&quot;8040&quot;&gt;Ms. Le Guin always considered herself a feminist, even when genre conventions led her to center her books on male heroes. Her later works, like the additions to the Earthsea series and such Ekumen tales as “Four Ways to Forgiveness” (1995) and “The Telling” (2000), are mostly told from a female point of view.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;243&quot; data-total-count=&quot;8283&quot;&gt;In some of her later books, she gave in to a tendency toward didacticism, as if she were losing patience with humanity for not learning the hard lessons — about the need for balance and compassion — that her best work so astutely embodies.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;331&quot; data-total-count=&quot;8614&quot;&gt;At the 2014 National Book Awards, Ms. Le Guin was given the Medal for Distinguished Contribution to American Letters. She accepted the medal on behalf of her fellow writers of fantasy and science fiction, who, she said, had been “excluded from literature for so long” while literary honors went to the “so-called realists.”&lt;/p&gt;
&lt;div id=&quot;story-ad-4&quot; class=&quot;story-ad ad ad-placeholder nocontent robots-nocontent&quot;&gt;

&lt;a class=&quot;visually-hidden skip-to-text-link&quot; href=&quot;https://www.nytimes.com/2018/01/23/obituaries/ursula-k-le-guin-acclaimed-for-her-fantasy-fiction-is-dead-at-88.html#story-continues-6&quot;&gt;Continue reading the main story&lt;/a&gt;&lt;/div&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;78&quot; data-total-count=&quot;8692&quot; id=&quot;story-continues-6&quot;&gt;She also urged publishers and writers not to put too much emphasis on profits.&lt;/p&gt;
&lt;p class=&quot;story-body-text story-content&quot; data-para-count=&quot;169&quot; data-total-count=&quot;8861&quot;&gt;“I have had a long career and a good one,” she said, adding, “Here at the end of it, I really don’t want to watch American literature get sold down the river.”&lt;/p&gt;
&lt;div id=&quot;addenda&quot; class=&quot;addenda&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;story-addendum story-content theme-correction&quot; readability=&quot;34&quot;&gt;&lt;strong&gt;Correction: January 24, 2018&lt;/strong&gt;&lt;p&gt;An earlier version of this obituary misspelled the surname of the social anthropologist who wrote “The Golden Bough.” He was James Frazer, not Frazier.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;a class=&quot;visually-hidden skip-to-text-link&quot; href=&quot;https://www.nytimes.com/2018/01/23/obituaries/ursula-k-le-guin-acclaimed-for-her-fantasy-fiction-is-dead-at-88.html#whats-next&quot;&gt;Continue reading the main story&lt;/a&gt;</description>
<pubDate>Tue, 23 Jan 2018 22:29:02 +0000</pubDate>
<dc:creator>sampo</dc:creator>
<og:url>https://www.nytimes.com/2018/01/23/obituaries/ursula-k-le-guin-acclaimed-for-her-fantasy-fiction-is-dead-at-88.html</og:url>
<og:type>article</og:type>
<og:title>Ursula K. Le Guin, Acclaimed for Her Fantasy Fiction, Is Dead at 88</og:title>
<og:description>Ms. Le Guin brought literary elegance and a feminist sensibility to science fiction and fantasy tales, drawing millions of readers around the world.</og:description>
<og:image>https://static01.nyt.com/images/2018/02/14/books/14leguin-alpha/14leguin-alpha-facebookJumbo-v2.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nytimes.com/2018/01/23/obituaries/ursula-k-le-guin-acclaimed-for-her-fantasy-fiction-is-dead-at-88.html</dc:identifier>
</item>
<item>
<title>Reducing Memory Usage in Ruby</title>
<link>https://tenderlovemaking.com/2018/01/23/reducing-memory-usage-in-ruby.html</link>
<guid isPermaLink="true" >https://tenderlovemaking.com/2018/01/23/reducing-memory-usage-in-ruby.html</guid>
<description>&lt;p&gt;I’ve been working on building a compacting garbage collector in Ruby for a while now, and one of the biggest hurdles for implementing a compacting GC is updating references. For example, if Object A points to Object B, but the compacting GC moves Object B, how do we make sure that Object A points to the new location?&lt;/p&gt;
&lt;p&gt;Solving this problem has been fairly straight forward for most objects. Ruby’s garbage collector knows about the internals of most Ruby Objects, so after the compactor runs, it just walks through all objects and updates their internals to point at new locations for any moved objects. If the GC &lt;em&gt;doesn’t&lt;/em&gt; know about the internals of some object (for example an Object implemented in a C extension), it doesn’t allow things referred to by that object to move. For example, Object A points to Object B. If the GC doesn’t know how to update the internals of Object A, it won’t allow Object B to move (I call this “pinning” an object).&lt;/p&gt;
&lt;p&gt;Of course, the more objects we allow to move, the better.&lt;/p&gt;
&lt;p&gt;Earlier I wrote that updating references for most objects is fairly straight forward. Unfortunately there has been one thorn in my side for a while, and that has been Instruction Sequences.&lt;/p&gt;
&lt;h2 id=&quot;instruction-sequences&quot;&gt;Instruction Sequences&lt;/h2&gt;
&lt;p&gt;When your Ruby code is compiled, it is turned in to instruction sequence objects, and those objects are Ruby objects. Typically you don’t interact with these Ruby objects, but they are there. These objects store byte code for your Ruby program, any literals in your code, and some other miscellaneous information about the code that was compiled (source location, coverage info, etc).&lt;/p&gt;
&lt;p&gt;Internally, these instruction sequence objects are referred to as “IMEMO” objects. There are multiple sub-types of IMEMO objects, and the instruction sequence sub-type is “iseq”. If you are using Ruby 2.5, and you dump the heap using &lt;code&gt;ObjectSpace&lt;/code&gt;, you’ll see the dump now contains these IMEMO subtypes. Lets look at an example.&lt;/p&gt;
&lt;p&gt;I’ve been using the following code to dump the heap in a Rails application:&lt;/p&gt;
&lt;div class=&quot;language-ruby highlighter-coderay&quot;&gt;
&lt;div class=&quot;CodeRay&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;code&quot; readability=&quot;9&quot;&gt;
&lt;pre&gt;
require &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;objspace&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;'&lt;/span&gt;&lt;/span&gt;
require &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;config/environment&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;'&lt;/span&gt;&lt;/span&gt;

&lt;span class=&quot;constant&quot;&gt;File&lt;/span&gt;.open(&lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;output.txt&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;'&lt;/span&gt;&lt;/span&gt;) &lt;span class=&quot;keyword&quot;&gt;do&lt;/span&gt; |f|
  &lt;span class=&quot;constant&quot;&gt;ObjectSpace&lt;/span&gt;.dump_all(&lt;span class=&quot;key&quot;&gt;output&lt;/span&gt;: f)
&lt;span class=&quot;keyword&quot;&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The above code outputs all objects in memory to a file called “output.txt” in &lt;a href=&quot;http://jsonlines.org&quot;&gt;JSON lines&lt;/a&gt; format. Here are a couple IMEMO records from a Rails heap dump:&lt;/p&gt;
&lt;div class=&quot;language-json highlighter-coderay&quot;&gt;
&lt;div class=&quot;CodeRay&quot; readability=&quot;10.5&quot;&gt;
&lt;div class=&quot;code&quot; readability=&quot;16&quot;&gt;
&lt;pre&gt;
{
  &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;0x7fc89d00c400&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;,
  &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;IMEMO&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;,
  &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;class&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;0x7fc89e95c130&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;,
  &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;imemo_type&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;ment&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;,
  &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;memsize&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;integer&quot;&gt;40&lt;/span&gt;,
  &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: {
    &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;wb_protected&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;value&quot;&gt;true&lt;/span&gt;,
    &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;old&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;value&quot;&gt;true&lt;/span&gt;,
    &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;uncollectible&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;value&quot;&gt;true&lt;/span&gt;,
    &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;marked&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;value&quot;&gt;true&lt;/span&gt;
  }
}
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;language-json highlighter-coderay&quot;&gt;
&lt;div class=&quot;CodeRay&quot; readability=&quot;12.5&quot;&gt;
&lt;div class=&quot;code&quot; readability=&quot;20&quot;&gt;
&lt;pre&gt;
{
  &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;address&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;0x7fc89d00c2e8&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;,
  &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;IMEMO&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;,
  &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;imemo_type&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;iseq&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;,
  &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;references&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: [
    &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;0x7fc89d00c270&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;,
    &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;0x7fc89e989a68&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;,
    &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;0x7fc89e989a68&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;,
    &lt;span class=&quot;string&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;0x7fc89d00ef48&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;
  ],
  &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;memsize&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;integer&quot;&gt;40&lt;/span&gt;,
  &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;flags&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: {
    &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;wb_protected&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;value&quot;&gt;true&lt;/span&gt;,
    &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;old&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;value&quot;&gt;true&lt;/span&gt;,
    &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;uncollectible&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;value&quot;&gt;true&lt;/span&gt;,
    &lt;span class=&quot;key&quot;&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;content&quot;&gt;marked&lt;/span&gt;&lt;span class=&quot;delimiter&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;: &lt;span class=&quot;value&quot;&gt;true&lt;/span&gt;
  }
}
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This example came from Ruby 2.5, so both records contain an &lt;code&gt;imemo_type&lt;/code&gt; field. The first example is a “ment” or “method entry”, and the second example is an “iseq” or an “instruction sequence”. Today we’ll look at instruction sequences.&lt;/p&gt;
&lt;h2 id=&quot;format-of-instruction-sequence&quot;&gt;Format of Instruction Sequence&lt;/h2&gt;
&lt;p&gt;The instruction sequences are the result of compiling our Ruby code. The instruction sequences are a binary representation of our Ruby code. These instructions are stored on the instruction sequence object, specifically &lt;a href=&quot;https://github.com/ruby/ruby/blob/36d91068ed9297cb792735f93f31d0bf186afeec/vm_core.h#L309&quot;&gt;this &lt;code&gt;iseq_encoded&lt;/code&gt; field&lt;/a&gt; (&lt;code&gt;iseq_size&lt;/code&gt; is the length of the &lt;code&gt;iseq_encoded&lt;/code&gt; field).&lt;/p&gt;
&lt;p&gt;If you were to examine &lt;code&gt;iseq_encoded&lt;/code&gt;, you’ll find it’s just a list of numbers. The list of numbers is virtual machine instructions as well as parameters (operands) for the instructions.&lt;/p&gt;
&lt;p&gt;If we examine the &lt;code&gt;iseq_encoded&lt;/code&gt; list, it might look something like this:&lt;/p&gt;
&lt;table class=&quot;table&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt; &lt;/th&gt;
&lt;th&gt;Address&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0x00000001001cddad&lt;/td&gt;
&lt;td&gt;Instruction (0 operands)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0x00000001001cdeee&lt;/td&gt;
&lt;td&gt;Instruction (2 operands)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0x00000001001cdf1e&lt;/td&gt;
&lt;td&gt;Operand&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0x000000010184c400&lt;/td&gt;
&lt;td&gt;Operand&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0x00000001001cdeee&lt;/td&gt;
&lt;td&gt;Instruction (2 operands)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0x00000001001c8040&lt;/td&gt;
&lt;td&gt;Operand&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;0x0000000100609e40&lt;/td&gt;
&lt;td&gt;Operand&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0x0000000100743d10&lt;/td&gt;
&lt;td&gt;Instruction (1 operand)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;0x00000001001c8040&lt;/td&gt;
&lt;td&gt;Operand&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;0x0000000100609e50&lt;/td&gt;
&lt;td&gt;Instruction (1 operand)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;0x0000000100743d38&lt;/td&gt;
&lt;td&gt;Operand&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Each element of the list corresponds to either an instruction, or the operands for an instruction. All of the operands for an instruction follow that instruction in the list. The operands are anything required for executing the corresponding instruction, including Ruby objects. In other words, some of these addresses could be addresses for Ruby objects.&lt;/p&gt;
&lt;p&gt;Since some of these addresses could be Ruby objects, it means that instruction sequences reference Ruby objects. But, if instruction sequences reference Ruby objects, how do the instruction sequences prevent those Ruby objects from getting garbage collected?&lt;/p&gt;
&lt;h2 id=&quot;liveness-and-code-compilation&quot;&gt;Liveness and Code Compilation&lt;/h2&gt;
&lt;p&gt;As I said, instruction sequences are the result of compiling your Ruby code. During compilation, some parts of your code are converted to Ruby objects and then the addresses for those objects are embedded in the byte code. Lets take a look at an example of when a Ruby object will be embedded in instruction sequences, then look at how those objects are kept alive.&lt;/p&gt;
&lt;p&gt;Our sample code is just going to be &lt;code&gt;puts &quot;hello world&quot;&lt;/code&gt;. We can use &lt;code&gt;RubyVM::InstructionSequence&lt;/code&gt; to compile the code, then disassemble it. Disassembly decodes &lt;code&gt;iseq_encoded&lt;/code&gt; and prints out something more readable.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;&amp;gt;&amp;gt; insns = RubyVM::InstructionSequence.compile 'puts &quot;hello world&quot;'
=&amp;gt; &amp;lt;RubyVM::InstructionSequence:&amp;lt;compiled&amp;gt;@&amp;lt;compiled&amp;gt;&amp;gt;
&amp;gt;&amp;gt; puts insns.disasm
== disasm: #&amp;lt;ISeq:&amp;lt;compiled&amp;gt;@&amp;lt;compiled&amp;gt;&amp;gt;================================
0000 trace            1                                               (   1)
0002 putself          
0003 putstring        &quot;hello world&quot;
0005 opt_send_without_block &amp;lt;callinfo!mid:puts, argc:1, FCALL|ARGS_SIMPLE&amp;gt;, &amp;lt;callcache&amp;gt;
0008 leave            
=&amp;gt; nil
&amp;gt;&amp;gt;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Instruction &lt;code&gt;003&lt;/code&gt; is the &lt;code&gt;putstring&lt;/code&gt; instruction. Lets look at the definition of the &lt;code&gt;putstring&lt;/code&gt; instruction &lt;a href=&quot;https://github.com/ruby/ruby/blob/36d91068ed9297cb792735f93f31d0bf186afeec/insns.def#L345-L353&quot;&gt;which can be found in &lt;code&gt;insns.def&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;/* put string val. string will be copied. */
DEFINE_INSN
putstring
(VALUE str)
()
(VALUE val)
{
    val = rb_str_resurrect(str);
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;When the virtual machine executes, it will jump to the location of the &lt;code&gt;putstring&lt;/code&gt; instruction, decode operands, and provide those operands to the instruction. In this case, the &lt;code&gt;putstring&lt;/code&gt; instruction has one operand called &lt;code&gt;str&lt;/code&gt; which is of type &lt;code&gt;VALUE&lt;/code&gt;, and one return value called &lt;code&gt;val&lt;/code&gt; which is also of type &lt;code&gt;VALUE&lt;/code&gt;. The instruction body itself simply calls &lt;code&gt;rb_str_resurrect&lt;/code&gt;, passing in &lt;code&gt;str&lt;/code&gt;, and assigning the return value to &lt;code&gt;val&lt;/code&gt;. &lt;code&gt;rb_str_resurrect&lt;/code&gt; &lt;a href=&quot;https://github.com/ruby/ruby/blob/36d91068ed9297cb792735f93f31d0bf186afeec/string.c#L1520-L1525&quot;&gt;just duplicates a Ruby string&lt;/a&gt;. So this instruction takes a Ruby object (a string which has been stored in the instruction sequences), duplicates that string, then the virtual machines pushes that duplicated string on to the stack. For a fun exercise, try going through this process with &lt;code&gt;puts &quot;hello world&quot;.freeze&lt;/code&gt; and take a look at the difference.&lt;/p&gt;
&lt;p&gt;Now, how does the string “hello world” stay alive until this instruction is executed? Something must mark the string object so the garbage collector knows that a reference is being held.&lt;/p&gt;
&lt;p&gt;The way the instruction sequences keep these objects alive is through the use of what it calls a “mark array”. As the compiler converts your code in to instruction sequences, it will allocate a string for “hello world”, then push that string on to an array. Here is an excerpt &lt;a href=&quot;https://github.com/ruby/ruby/blob/47d9ee39df63f0b87e041d46160e429eea19f3c6/compile.c#L2042-L2049&quot;&gt;from &lt;code&gt;compile.c&lt;/code&gt;&lt;/a&gt; that does this:&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-coderay&quot;&gt;
&lt;div class=&quot;CodeRay&quot; readability=&quot;7&quot;&gt;
&lt;div class=&quot;code&quot; readability=&quot;9&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;keyword&quot;&gt;case&lt;/span&gt; TS_VALUE:    
{
    VALUE v = operands[j];
    generated_iseq[code_index + &lt;span class=&quot;integer&quot;&gt;1&lt;/span&gt; + j] = v;
    
    iseq_add_mark_object(iseq, v);
    &lt;span class=&quot;keyword&quot;&gt;break&lt;/span&gt;;
}
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;All &lt;code&gt;iseq_add_mark_object&lt;/code&gt; does is push the &lt;code&gt;VALUE&lt;/code&gt; on to an array which is stored on the instruction sequence object. &lt;code&gt;iseq&lt;/code&gt; is the instruction sequence object, and &lt;code&gt;v&lt;/code&gt; is the &lt;code&gt;VALUE&lt;/code&gt; we want to keep alive (in this case the string “hello world”). If you look in &lt;code&gt;vm_core.h&lt;/code&gt;, you can &lt;a href=&quot;https://github.com/ruby/ruby/blob/47d9ee39df63f0b87e041d46160e429eea19f3c6/vm_core.h#L410&quot;&gt;find the location of that mark array&lt;/a&gt; with a comment that says:&lt;/p&gt;
&lt;div class=&quot;language-c highlighter-coderay&quot;&gt;
&lt;div class=&quot;CodeRay&quot;&gt;
&lt;div class=&quot;code&quot;&gt;
&lt;pre&gt;
VALUE mark_ary;     
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&quot;instruction-sequence-references-and-compaction&quot;&gt;Instruction Sequence References and Compaction&lt;/h2&gt;
&lt;p&gt;So, instruction sequences contain two references to a string literal: one in the instructions in &lt;code&gt;iseq_encoded&lt;/code&gt;, and one via the mark array. If the string literal moves, then both locations will need to be updated. Updating array internals is fairly trivial: it’s just a list. Updating instruction sequences on the other hand is not so easy.&lt;/p&gt;
&lt;p&gt;To update references in the instruction sequences, we have to disassemble the instructions, locate any &lt;code&gt;VALUE&lt;/code&gt; operands, and update those locations. There wasn’t any code to walk these instructions, so I introduced a function that would &lt;a href=&quot;https://github.com/ruby/ruby/blob/36d91068ed9297cb792735f93f31d0bf186afeec/iseq.c#L170-L194&quot;&gt;disassemble instructions and call a function pointer with those objects&lt;/a&gt;. This allows us to find new locations of Ruby objects and update the instructions. But what if we could use this function for something more?&lt;/p&gt;
&lt;h2 id=&quot;reducing-memory&quot;&gt;Reducing Memory&lt;/h2&gt;
&lt;p&gt;Now we’re finally on to the part about saving memory. The point of the mark arrays stored on the instruction sequence objects is to keep any objects referred to by instruction sequences alive:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tenderlovemaking.com/literal_marking.png&quot; alt=&quot;ISeq and Array marking paths&quot;/&gt;&lt;/p&gt;
&lt;p&gt;We can reuse the “update reference” function to mark references contained directly in instruction sequences. This means we can reduce the size of the mark array:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://tenderlovemaking.com/iseq_mark_literals.png&quot; alt=&quot;Mark Literals via disasm&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Completely eliminating the mark array is a different story as there are things stored in the mark array that aren’t just literals. However, if we directly mark objects from the instruction sequences, then we rarely have to grow the array. The amount of memory we save is the size of the array &lt;a href=&quot;https://twitter.com/tenderlove/status/951204087382491136&quot;&gt;plus any unused extra capacity in the array&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’ve made a patch that implements this strategy, and you can find it on the &lt;a href=&quot;https://github.com/github/ruby/pull/39&quot;&gt;GitHub fork of Ruby&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I found that this saves approximately 3% memory on a basic Rails application set to production mode. Of course, the more code you load, the more memory you save. I expected the patch to impact GC performance because disassembling instructions and iterating through them should be harder than just iterating an array. However, since instruction sequences get old, and we have a generational garbage collector, the impact to real apps is very small.&lt;/p&gt;
&lt;p&gt;I’m working to upstream this patch to Ruby, and you can follow along and read more information about the analysis &lt;a href=&quot;https://bugs.ruby-lang.org/issues/14370&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Anyway, I hope you found this blurgh post informative, and please have a good day!&lt;/p&gt;
&lt;p&gt;&amp;lt;3 &amp;lt;3 &amp;lt;3&lt;/p&gt;
</description>
<pubDate>Tue, 23 Jan 2018 22:23:06 +0000</pubDate>
<dc:creator>craigkerstiens</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://tenderlovemaking.com/2018/01/23/reducing-memory-usage-in-ruby.html</dc:identifier>
</item>
<item>
<title>Forging a Swiss Lens: How Zurich&amp;#039;s tech scene changed my view of SV</title>
<link>https://nextrends.swissnexsanfrancisco.org/forging-a-swiss-lens-3-ways-zurich-changed-my-view-of-silicon-valley/</link>
<guid isPermaLink="true" >https://nextrends.swissnexsanfrancisco.org/forging-a-swiss-lens-3-ways-zurich-changed-my-view-of-silicon-valley/</guid>
<description>&lt;p&gt;&lt;em&gt;Guest contributor Matthew Daiter reflects on life at a Swiss startup — and his return to Silicon Valley.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There we were, floating down the Limmatplatz on our makeshift inflatable pizza raft. One by one we peeled off the sides, shooting alpine water at one another while ensuring our raft didn’t pop. Stares accompanied our shrieks while we nearly capsized our boat through a turbine. This would only serve as a minimal precursor for the wasps, sunburn, and my almost-lost passport that accompanied our virgin outing of our “Italian Crusader I”.&lt;/p&gt;
&lt;p&gt;Our outing was an end to my time living in Zurich, and working in the Swiss tech sphere. Starkly contrasting the rain and stress that had greeted me on my first day in Switzerland, this day felt lighthearted. Drinking beer on a pizza, followed by eating a pizza, and eventually simulating a pizza at Seat 3, Gate 75 in Zurich Airport, all distracted me from my recruiter’s lingering responses to my questions at at my new job in Silicon Valley.&lt;/p&gt;
&lt;p&gt;“Intense,” my recruiter responded to my question about my new hours. “And with a lot of responsibility. Occasionally people work weekends here.” He added, “We’re composed of small teams. If you pull hard, people will notice; but be warned that if you mess up, people know where to point the finger.”&lt;/p&gt;
&lt;p&gt;People work weekends? Even during crunch-time at our startup in Switzerland, the office was vacant outside of workdays.&lt;/p&gt;
&lt;p&gt;“Is there over-time pay for working weekends?” I asked.&lt;br/&gt;“No overtime. Vacation days are given out on occasion, but no guarantees.”&lt;/p&gt;
&lt;p&gt;I couldn’t help but question my own actions pertaining to my return back to the States.&lt;/p&gt;
&lt;p&gt;On one hand, living in Switzerland felt artificial and forced. My hasty departure from the States landed me in an awkward visa situation, granting me permission to live in Luxembourg and long-term sublet in Switzerland. The spoken language carries a heavily localised dialect, rendering it difficult to pick up without costly courses. My nationality hampered my ability to obtain medical treatment or open a bank account in Zurich.&lt;/p&gt;
&lt;p&gt;But on the other hand, living in Zurich forced completely unanticipated personal growth. Weekends once filled with work and JIRA tickets were now occupied with impulsive SCUBA trips off the Italian coast, ibex-spotting excursions in southeast Switzerland, and under-the-bridge “nature raves” a quick train-ride away from Zurich proper. Being able to remove myself from the constant specter of work made me more creative and driven; in fact, this replenished focus led to developing the research that landed me and my co-publishers a spot at the European Conference of Computer Vision in 2016. Zurich, with its initially cold and unwelcoming air, proved to be filled with incredibly inviting and skilled individuals that force-fed life into our startup. Tucked away in the Swiss Alps happened to be a large village of congregated intellectuals pushing the frills of science without the warping magnet of fiscal motivation eroding personal development.&lt;/p&gt;
&lt;p&gt;Would returning to the States — with weekends working, long commutes, and a culture of outworking others as a sense of social validation — be worth the effort?&lt;/p&gt;
&lt;h2&gt;The Return&lt;/h2&gt;
&lt;p&gt;Returning felt almost foreign. My smile dropped when I left my apartment for my first commute to my job in Sunnyvale. In front of me, a homeless man pleaded for help to cross the street. Bodies rushed past him, urgent to catch fleeting express trains to their six-figure jobs in South Bay. Tripping over the impoverished to make thousands per week symbolized one of many culture shocks on my return to the States: polarization seeps into the foundation propping up San Francisco. This is the system I support?&lt;/p&gt;
&lt;p&gt;My first train ride would commence my stark re-entry to American Reality. For five days a week, a three-hour roundtrip commute and a ten-hour workday would become standard. Money replaced personal time: from DoorDash’ed desk dinners to Ubers for when public transport closed, money became an excuse for the sacrilege of sleep and after-work personal space. While I hammered away at code for 12 hours a day, my far-sight eye muscles relaxed and my long-term vision diluted to a two-week gaze.&lt;/p&gt;
&lt;p&gt;My (multi-thousand kilometer) leap of faith revealed far more about Silicon Valley than I could have imagined. There’s a lot of emerging discussion around Hacker News and the Bay Area that spawned from various domestic American events about immigrating to another country to pursue founding a startup. Switzerland tectonically shifted my view of entrepreneurship in Silicon Valley, debased American Reality for me, and redefined my idea of success. Invariably the right move for me at the time, I hope this article sheds light on the process of immigration and starting-up abroad. Here are three ways my “Swiss lens” changed my vision of Silicon Valley.&lt;/p&gt;
&lt;h2&gt;1. Less choice leads to more concentration&lt;/h2&gt;
&lt;p&gt;Routine seeps into Swiss culture. Trash must be disposed within certain types of bags. Trains must depart on time. Assigned laundry days are commonplace. The streets are cleanly swept and washed down daily. Like clockwork, Switzerland moves to an implicit schedule expressed throughout the perceivable culture.&lt;/p&gt;
&lt;p&gt;Tuning out the outside world proved to be one of the main advantages of Zurich. The lack of decision-making prodding for attention let our research team more whole-heartedly focus on the tasks at hand. My laundry would be done on Friday night at 6. The 33 bus for home always came to the office at 11:35 (I’m a late sleeper) and left at 00:06. My groceries rarely became unavailable or varied in price, making it easy to budget out monthly costs. All of this culminated in an easy commute to the office, focus on work, and leave without fretting about fiscal or scheduling issues.&lt;/p&gt;
&lt;p&gt;When returning to the Bay Area, grocery shopping was one of many trivialities that caught me off-guard. Twenty variations of yogurt lined the grocery shelves, each with different price points and niches. At the top were premium and exclusive foreign yogurts. Scanning further down the aisle came variations in yogurt sale points. “No sugar added”. “Icelandic rations”. “Cashew-milk-made”. Overwhelmed by options, I found myself choosing products not due to a rationale on the product itself, but on the advertised price point and deals that accompanied it. As the only common variable between so many dimensions of a product I thought I had known, this was my fallback mechanism.&lt;/p&gt;
&lt;p&gt;This was just for yogurt.&lt;/p&gt;
&lt;p&gt;Noise — visual, auditory, cultural — can empower or dampen the focus and amplitude of executing your idea. Growing up in America normalized one extreme of this; Switzerland, the other. San Francisco is filled with all degrees of opportunity; you need to find your place and hone in on what you want. Switzerland can be quiet enough to hear a cigarette crackle at night. You need to construct what you want.&lt;/p&gt;
&lt;p&gt;While having less choice was initially off-putting, it allowed me to tune out parts of my life that I found didn’t matter, leading to a more efficient work process and faster results.&lt;/p&gt;
&lt;h2&gt;2. Strong public investment can outperform the private sector in creating better individual experiences.&lt;/h2&gt;
&lt;p&gt;I had always assumed that private companies simply outperformed Bay Area public services because of market pressure and their higher cost. Owning a car or using a ride-share app would get you to a destination faster and more comfortably than using public transport; private schools typically delivered better educational results than the public system (unless you lived in a wealthy school district); and private healthcare was so expensive because its quality was incredible compared to public clinics. Data existed to reinforce this: the MUNI in San Francisco has an &lt;a href=&quot;http://sfgov.org/scorecards/transit-time-performance&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;on-time rate of 57%&lt;/a&gt;. The New York Times published &lt;a href=&quot;https://www.nytimes.com/interactive/2016/04/29/upshot/money-race-and-success-how-your-school-district-compares.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;extensive data&lt;/a&gt; correlating wealth and educational development; and for the hefty price of Cigna or Kaiser, San Francisco hospitals mostly &lt;a href=&quot;https://health.usnews.com/health-care/best-hospitals/articles/best-hospitals-honor-roll-and-overview&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;outrank&lt;/a&gt; other national offerings.&lt;/p&gt;
&lt;p&gt;Switzerland blew my logic out of the water.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transportation&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In 2013, 87.5% of passengers reached their destination within three minutes of the advertised time within Switzerland. Compare this to the MUNI in San Francisco, which has an on-time percentage rate of 57%. The MUNI’s lack of predictability contributes to the creation of fallbacks onto private infrastructure filling the vacuum for an in-efficient public transportation system in San Francisco, such as ride sharing services like Uber or Lyft. These makeshift props for those that can bear the cost cause the general public to lose out and fluctuations in budgeting transportation for the month, leading to more induced stress and worse predictability.&lt;/p&gt;
&lt;p&gt;Whether visiting Luxembourg from Switzerland, going on a weekend beach vacation to Italy, or bumming off to the Swiss wilderness, the expansive and well- maintained public transportation system makes daisy-chaining transport links easy and predictable. 1.5 hours of train, bus and cable-car travel can get you to the side of an alp in Amden or a lake in Lucerne. And for this reason, co-workers and I hiked on the weekend and explored Switzerland and surrounding countries. Getting to Yosemite National Park from San Francisco takes around 3.5 hours by car, and even more time by multiple non-coordinated legs of public transportation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Education&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.ethz.ch/de.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ETH Zurich&lt;/a&gt; and &lt;a href=&quot;https://www.epfl.ch/index.en.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;EPFL&lt;/a&gt; (Switzerland’s flagship technical universities and two of the &lt;a href=&quot;http://lenews.ch/2017/09/07/two-swiss-universities-make-top-50-global-ranking/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;top engineering schools in the world&lt;/a&gt;) provided top-notch education for all Swiss high-school graduates at an affordable price-point of 580 CHF a semester. Comparable schools in the Bay Area (Stanford and UC Berkeley) offer tuition for $16,329 a semester and $13,900 a semester, respectively, and only admit a select few. Not only did these easily accessible, meritocratic Swiss universities produce a wealth of qualified and ambitious young scientists and engineers, but having ETH Zurich twenty minutes away created a backdoor into one of the most qualified talent pools in continental Europe for scaling our startup. Recruiting events like Startup Speed Dating flooded our resume bank with highly capable applicants. With Switzerland’s popular percentage-based work system, these students could easily split work and research, and we could directly profit off of this talent pool.&lt;/p&gt;
&lt;p&gt;Furthermore, ETH’s free public schooling system for auditors meant that a world-caliber education was only a short bus ride away. In an industry where uncommon educational trajectories are normalized, the Swiss educational system encourages continual educational development without the particular commitment of obtaining a diploma. When performing 3D-reconstruction research, sitting in on classes and hammering top-notch professors with our questions allowed us to move faster throughout our development cycles.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Healthcare&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Finally, healthcare was not only cheap, but also of incredible quality. Under the Swiss system, having healthcare is mandatory; however, average healthcare in Switzerland costs only 200-400CHF ($201-$402) a month. With this, all basic services are provided at a fixed, up-front cost under national law. No insurance company can profit off of these basic services. For an average-salaried Swiss worker ($60,124 in 2016), self-financing healthcare is approachable.&lt;/p&gt;
&lt;p&gt;When I woke up one morning with an illness, I immediately rushed off to ETH’s hospital for treatment. While I was turned down due to my nationality and lack of Swiss-specific living permit, I was able to obtain treatment at the free clinic within the train station. Although needing to pay out-of-pocket for treatment, Luxembourgish health care covered all of my costs from the social system through expedited invoices. Over the coming months, the clinic urged me to return for checkups (completely covered by social services) while taking high-grade and fast-acting pharmaceuticals (also completely covered by social services) to ensure a full recovery. Normally, clinics in the Bay Area are for the uninsured, with an attached stigma of lesser quality and optionality. Clinics in Switzerland allowed for quick and easy high-quality recuperation.&lt;/p&gt;
&lt;p&gt;Heavy public investments created reliable, maintainable, and quality services. Zurich proved that a system optimized to the needs of the public instead of the individual can offer alternate avenues to provide a better experience that services everyone’s needs.&lt;/p&gt;
&lt;h2&gt;3. Adjustable work schedules boosted employee retention and easier recruitment of dynamic labor pools&lt;/h2&gt;
&lt;p&gt;In Switzerland, employees can usually decide the percentage of a week they’d like to work in return for the corresponding percentage of a complete full-time salary. Having this concept integrated as a societal norm mitigated outside factors causing burnout and stress.&lt;/p&gt;
&lt;p&gt;When I became hospitalized, not only were my medical bills completely taken care of, but my teammates emphasized that my return would be on my own schedule with my own ramp-up period. This became integral when returning to my role at Nomoko. In America, employers often limit sick days and can fire employees who can’t promptly return. American companies aren’t required by federal law to provide paid sick days to employees, leaving more than a third of Americans with absolutely no sick days.&lt;/p&gt;
&lt;p&gt;When we needed to let go of my co-worker, she was legally entitled to receive 70% of her previous compensation for 6 months. Switzerland has an incredible unemployment system: if you ever become unemployed, the Swiss government will provide 70-80% of your prior compensation for up to 18 months. While this may seem overly generous, this doesn’t actually affect the unemployment rate significantly; in fact, Switzerland has the same unemployment rate compared to the United States. A massive societal safety net helped employees feel enabled to take a risk working for our startup.&lt;/p&gt;
&lt;p&gt;Finally, vacation time is plentiful and without stigma. My American work- weekends were replaced with email-free Alp hikes. When I decided to take my (legal-minimum) four-week holiday on short notice, my boss encouraged me to go to avoid burnout. When I told my coworkers in Silicon Valley that I was going on a two-week vacation to Morocco, one team member shot back “we might not be able to find you, but email can!” American law doesn’t enforce employers to give vacation time off. 43% of Americans working at small businesses cited &lt;a href=&quot;https://www.projecttimeoff.com/sites/default/files/StateofAmericanVacation2017.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;heavy workloads&lt;/a&gt; as a primary barrier to taking time off. While many tech startups in the Bay offer &lt;a href=&quot;https://www.fastcompany.com/3052926/we-offered-unlimited-vacation-for-one-year-heres-what-we-learned&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;unlimited vacation&lt;/a&gt; as a perk, this can quickly be manipulated to ensure no large payouts become necessary for when employees leave, while disabling employees from going on vacation at all. &lt;a href=&quot;https://www.buzzfeed.com/carolineodonovan/at-kickstarter-flexible-vacation-time-is-no-more&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Stigmatizing vacation&lt;/a&gt; doesn’t lead to more work getting done; it leads to disgruntled employees, output deceleration, and high workforce throughput.&lt;/p&gt;
&lt;p&gt;Constant work-sprints in America led to slower results and resentment against my employer. Being able to pause led to higher spikes in creativity and further concentration within the workplace.&lt;/p&gt;
&lt;h2&gt;A Swiss Mindset&lt;/h2&gt;
&lt;p&gt;When leaving for Switzerland, packing my life into a suitcase proved to be surprisingly easy. Irreplaceability trumped all else: I tucked away my typography books and electronics into my check-in bag, along with just a single hoodie for the approaching winter. A quick leaf over my valuables—passport, birth certificate with apostille, flight information—and then I departed.&lt;/p&gt;
&lt;p&gt;Unpacking my life back in America has been difficult. Caught between multiple cultures, the culmination of working in Switzerland showed me first-hand that cultures tackle and optimise for various social and economic facets and that this active research on what the collective human effort can provide for its citizens benefits humanity as a whole. While I missed the Bay Area for its undying friendliness and creativity, some of its blemishes have become apparent only from gaining distance. Even while undergoing my reintegration to America, my mind still lingers in Switzerland.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Shameless plug: if you’re currently looking to make an actual virtual reality Swiss camera (lens included), Nomoko is currently hiring for software, electrical and mechanical engineering positions. They helped change my lens of reality; maybe they’ll help change yours too.&lt;/em&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;em&gt;Photo of Amden, courtesy of Matt Daiter: “My favorite place to hike.” &lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;addtoany_share_save_container addtoany_content addtoany_content_bottom&quot;&gt;
&lt;div class=&quot;a2a_kit a2a_kit_size_16 addtoany_list&quot; data-a2a-url=&quot;https://nextrends.swissnexsanfrancisco.org/forging-a-swiss-lens-3-ways-zurich-changed-my-view-of-silicon-valley/&quot; data-a2a-title=&quot;Forging a Swiss Lens: 3 Ways Zurich Changed My View of Silicon Valley&quot;&gt;&lt;a class=&quot;a2a_dd addtoany_share_save addtoany_share&quot; href=&quot;https://www.addtoany.com/share&quot;&gt;&lt;img src=&quot;https://static.addtoany.com/buttons/favicon.png&quot; alt=&quot;Share&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Tue, 23 Jan 2018 20:37:02 +0000</pubDate>
<dc:creator>msd81257</dc:creator>
<og:type>article</og:type>
<og:title>Forging a Swiss Lens: 3 Ways Zurich Changed My View of Silicon Valley</og:title>
<og:url>https://nextrends.swissnexsanfrancisco.org/forging-a-swiss-lens-3-ways-zurich-changed-my-view-of-silicon-valley/</og:url>
<og:description>An American working in Switzerland returns to Silicon Valley with a fresh perspective.</og:description>
<og:image>https://nextrends.swissnexsanfrancisco.org/wp-content/uploads/2018/01/daitermountain.jpg</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://nextrends.swissnexsanfrancisco.org/forging-a-swiss-lens-3-ways-zurich-changed-my-view-of-silicon-valley/</dc:identifier>
</item>
</channel>
</rss>