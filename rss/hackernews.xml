<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>You probably don&amp;#039;t need AI/ML. You can make do with well written SQL scripts</title>
<link>https://threadreaderapp.com/thread/987602838594445312.html</link>
<guid isPermaLink="true" >https://threadreaderapp.com/thread/987602838594445312.html</guid>
<description>&lt;p&gt;&quot;I feel so full,&quot; Jungkook moaned. The boy often emphasised his act in order to put on an amazing show, but he sincerely felt so pleasurable right now, especially with the low voiced male in mind. The boy pushed in further while thrusting his hips up, trying to create friction.&lt;/p&gt;
&lt;p&gt;&quot;You looks s-so good... Fuck,&quot; Yoongi responded. Precum oozed from his slit as his hand moved faster up and down, causing the pit in his stomach to grow. He groaned loudly while closing his eyes, focused on all the sounds; the moans, vibrating sounds, wet smacks and throaty sighs&lt;/p&gt;
&lt;p&gt;&quot;You... You sound so good,&quot; Jungkook responded, not entirely sure how to address the other, which he acknowledged with a moan. The boy's free hand started to shake as he wasn't sure what to do with it, he felt so good and wanted to touch himself, yet didn't want it to end.&lt;/p&gt;
</description>
<pubDate>Sun, 22 Apr 2018 21:56:43 +0000</pubDate>
<dc:creator>passenger</dc:creator>
<og:title>Thread by @cyberomin: &quot;It's always fun when I speak to founders and potential founders and they are quick to tell me how they want to use AI/ML to improve customer […]&quot;</og:title>
<og:image>https://threadreaderapp.com/images/screenshots/thread/987602838594445312.jpg</og:image>
<og:url>https://threadreaderapp.com/thread/987602838594445312.html</og:url>
<og:description>Thread by @cyberomin: &quot;It's always fun when I speak to founders and potential founders and they are quick to tell me how they want to use AI/ML to improve customer retention and improve LTV. Truth is, they don't even need ML. A properly writte […]&quot;</og:description>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://threadreaderapp.com/thread/987602838594445312.html</dc:identifier>
</item>
<item>
<title>A blockchain is a specific set of choices suitable for a narrow set of use-cases</title>
<link>https://threadreaderapp.com/thread/987266940887535616.html</link>
<guid isPermaLink="true" >https://threadreaderapp.com/thread/987266940887535616.html</guid>
<description>&lt;div readability=&quot;27.778481012658&quot;&gt;So I left &lt;a class=&quot;entity-hashtag&quot; href=&quot;https://threadreaderapp.com/hashtag/postdoc&quot;&gt;#postdoc&lt;/a&gt;, and I left &lt;a class=&quot;entity-hashtag&quot; href=&quot;https://threadreaderapp.com/hashtag/adjunct&quot;&gt;#adjunct&lt;/a&gt; and went into &lt;a class=&quot;entity-hashtag&quot; href=&quot;https://threadreaderapp.com/hashtag/industry&quot;&gt;#industry&lt;/a&gt;. There are two things I wish I knew when making this transition, and I hope it may help you...&lt;/div&gt;&lt;div readability=&quot;35.592760180995&quot;&gt;...I didn’t know what buzzwords to put on my resume to make it reflect, in &lt;a class=&quot;entity-hashtag&quot; href=&quot;https://threadreaderapp.com/hashtag/industry&quot;&gt;#industry&lt;/a&gt; terms, my skills. For &lt;a class=&quot;entity-hashtag&quot; href=&quot;https://threadreaderapp.com/hashtag/STEM&quot;&gt;#STEM&lt;/a&gt; academics, I’ve learned that a lot of what we do includes systems engineering, and project management...&lt;/div&gt;&lt;div readability=&quot;34.90243902439&quot;&gt;...now this may be obvious to some of you, esp the E part of &lt;a class=&quot;entity-hashtag&quot; href=&quot;https://threadreaderapp.com/hashtag/stem&quot;&gt;#stem&lt;/a&gt;, but I didn’t know this. I recommend looking up buzzwords for sys eng and project management...&lt;/div&gt;</description>
<pubDate>Sun, 22 Apr 2018 19:23:49 +0000</pubDate>
<dc:creator>BerislavLopac</dc:creator>
<og:title>Thread by @clemensv: &quot;I've talked to a lot of distributed systems engineers (who build cloud-scale stuff) from across the industry about blockchain. While most pl […]&quot;</og:title>
<og:image>https://threadreaderapp.com/images/screenshots/thread/987266940887535616.jpg</og:image>
<og:url>https://threadreaderapp.com/thread/987266940887535616.html</og:url>
<og:description>Thread by @clemensv: &quot;I've talked to a lot of distributed systems engineers (who build cloud-scale stuff) from across the industry about blockchain. While most platform folks I talked to are perfectly happy to help with frameworks that help s […]&quot;</og:description>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://threadreaderapp.com/thread/987266940887535616.html</dc:identifier>
</item>
<item>
<title>Parsing JSON is a Minefield</title>
<link>http://seriot.ch/parsing_json.php</link>
<guid isPermaLink="true" >http://seriot.ch/parsing_json.php</guid>
<description>&lt;p&gt;&lt;strong&gt;[2016-10-26]&lt;/strong&gt; First version of the article&lt;br /&gt;&lt;strong&gt;[2016-10-28]&lt;/strong&gt; Presentation at Soft-Shake Conference, Geneva (&lt;a href=&quot;http://seriot.ch/json/20161028_softshake_parsing_json.pdf&quot;&gt;slides&lt;/a&gt;)&lt;br /&gt;&lt;strong&gt;[2016-11-01]&lt;/strong&gt; Article and comments in &lt;a href=&quot;http://m.theregister.co.uk/2016/11/01/json_parsers_tested/&quot;&gt;The Register&lt;/a&gt;&lt;br /&gt;&lt;strong&gt;[2017-11-16]&lt;/strong&gt; Presentation at Black Alps Security Conference, Yverdon (&lt;a href=&quot;http://seriot.ch/resources/talks_papers/20171116_parsing_json_black_alps.pdf&quot;&gt;slides&lt;/a&gt;)&lt;br /&gt;&lt;strong&gt;[2018-03-09]&lt;/strong&gt; Presentation at Toulouse Hacking Conference (&lt;a href=&quot;http://seriot.ch/resources/talks_papers/20180309_json_toulouse.pdf&quot;&gt;slides&lt;/a&gt;)&lt;br /&gt;&lt;strong&gt;[2018-03-30]&lt;/strong&gt; Updated this article considering &lt;a href=&quot;https://tools.ietf.org/html/rfc8259&quot;&gt;RFC 8259&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Feel free to comment on &lt;a href=&quot;https://news.ycombinator.com/item?id=12796556&quot;&gt;Hacker News&lt;/a&gt; or &lt;a href=&quot;https://www.reddit.com/r/programming/comments/59htn7/parsing_json_is_a_minefield/&quot;&gt;reddit&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Session Description&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;JSON is the de facto standard when it comes to (un)serialising and exchanging data in web and mobile programming. But how well do you really know JSON? We'll read the specifications and write test cases together. We'll test common JSON libraries against our test cases. I'll show that JSON is not the easy, idealised format as many do believe. Indeed, I did not find two libraries that exhibit the very same behaviour. Moreover, I found that edge cases and maliciously crafted payloads can cause bugs, crashes and denial of services, mainly because JSON libraries rely on specifications that have evolved over time and that left many details loosely specified or not specified at all.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Table of Contents&lt;/h3&gt;
&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;http://seriot.ch/parsing_json.php#1&quot;&gt;JSON Specifications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://seriot.ch/parsing_json.php#2&quot;&gt;Parsing Tests&lt;/a&gt;&lt;br /&gt;2.1 &lt;a href=&quot;http://seriot.ch/parsing_json.php#21&quot;&gt;Structure&lt;/a&gt;&lt;br /&gt;2.2 &lt;a href=&quot;http://seriot.ch/parsing_json.php#22&quot;&gt;Numbers&lt;/a&gt;&lt;br /&gt;2.3 &lt;a href=&quot;http://seriot.ch/parsing_json.php#23&quot;&gt;Arrays&lt;/a&gt;&lt;br /&gt;2.4 &lt;a href=&quot;http://seriot.ch/parsing_json.php#24&quot;&gt;Objects&lt;/a&gt;&lt;br /&gt;2.5 &lt;a href=&quot;http://seriot.ch/parsing_json.php#25&quot;&gt;Strings&lt;/a&gt;&lt;br /&gt;2.6 &lt;a href=&quot;http://seriot.ch/parsing_json.php#26&quot;&gt;RFC 7159 Ambiguities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://seriot.ch/parsing_json.php#3&quot;&gt;Testing Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://seriot.ch/parsing_json.php#4&quot;&gt;Parsing Tests Results&lt;/a&gt;&lt;br /&gt;4.1 &lt;a href=&quot;http://seriot.ch/parsing_json.php#41&quot;&gt;Full Results&lt;/a&gt;&lt;br /&gt;4.2 &lt;a href=&quot;http://seriot.ch/parsing_json.php#42&quot;&gt;C Parsers&lt;/a&gt;&lt;br /&gt;4.3 &lt;a href=&quot;http://seriot.ch/parsing_json.php#43&quot;&gt;Objective-C Parsers&lt;/a&gt;&lt;br /&gt;4.4 &lt;a href=&quot;http://seriot.ch/parsing_json.php#44&quot;&gt;Apple (NS)JSONSerialization&lt;/a&gt;&lt;br /&gt;4.5 &lt;a href=&quot;http://seriot.ch/parsing_json.php#45&quot;&gt;Freddy (Swift)&lt;/a&gt;&lt;br /&gt;4.6 &lt;a href=&quot;http://seriot.ch/parsing_json.php#46&quot;&gt;Bash JSON.sh&lt;/a&gt;&lt;br /&gt;4.7 &lt;a href=&quot;http://seriot.ch/parsing_json.php#47&quot;&gt;Other Parsers&lt;/a&gt;&lt;br /&gt;4.8 &lt;a href=&quot;http://seriot.ch/parsing_json.php#48&quot;&gt;JSON Checker&lt;/a&gt;&lt;br /&gt;4.9 &lt;a href=&quot;http://seriot.ch/parsing_json.php#49&quot;&gt;Regex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://seriot.ch/parsing_json.php#5&quot;&gt;Parsing Contents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://seriot.ch/parsing_json.php#6&quot;&gt;STJSON&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://seriot.ch/parsing_json.php#7&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://seriot.ch/parsing_json.php#8&quot;&gt;Appendix&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;h3&gt; 1. JSON Specifications&lt;/h3&gt;
&lt;p&gt;JSON is the de facto serialization standard when it comes to sending data over HTTP, the &lt;em&gt;lingua franca&lt;/em&gt; used to exchange data between heterogeneous software, both in modern web sites and mobile applications.&lt;/p&gt;
&lt;p&gt;&quot;Discovered&quot; in 2001 &lt;a href=&quot;https://en.wikipedia.org/wiki/Douglas_Crockford&quot;&gt;Douglas Crockford&lt;/a&gt;, JSON specification is so short and simple that Crockford created business cards with the whole JSON grammar on their back.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://www.seriot.ch/json/json_business_card.png&quot; alt=&quot;JSON Business Card&quot; border=&quot;1&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Pretty much all Internet users and programmers use JSON, yet few do actually agree on how JSON should actually work. The conciseness of the grammar leaves many aspects undefined. On top of that, several specifications exist, and their various interpretations tend to be murky.&lt;/p&gt;
&lt;p&gt;Crockford &lt;a href=&quot;https://www.computer.org/csdl/mags/co/2012/04/mco2012040006.html&quot;&gt;chose&lt;/a&gt; not to version JSON definition:&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;Probably the boldest design decision I made was to not put a version number on JSON so there is no mechanism for revising it. We are stuck with JSON: whatever it is in its current form, that’s it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Yet JSON is defined in at least seven different documents:&lt;/p&gt;
&lt;ol readability=&quot;2.7503924646782&quot;&gt;&lt;li&gt;2002 - &lt;a href=&quot;http://www.json.org&quot;&gt;json.org&lt;/a&gt;, and the business card&lt;/li&gt;
&lt;li&gt;2006 - IETF &lt;a href=&quot;https://tools.ietf.org/html/rfc4627&quot;&gt;RFC 4627&lt;/a&gt;, which set the &lt;code&gt;application/json&lt;/code&gt; MIME media type&lt;/li&gt;
&lt;li&gt;2011 - &lt;a href=&quot;http://www.ecma-international.org/ecma-262/5.1/#sec-15.12&quot;&gt;ECMAScript 262, section 15.12&lt;/a&gt;&lt;/li&gt;
&lt;li readability=&quot;2.2727272727273&quot;&gt;
&lt;p&gt;2013 - &lt;a href=&quot;http://www.ecma-international.org/publications/standards/Ecma-404.htm&quot;&gt;ECMA 404&lt;/a&gt; according to Tim Bray (RFC 7159 editor), &lt;a href=&quot;https://www.tbray.org/ongoing/When/201x/2014/03/05/RFC7159-JSON&quot;&gt;ECMA rushed out to release it&lt;/a&gt; because:&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;&quot;Someone told the ECMA working group that the IETF had gone crazy and was going to rewrite JSON with no regard for compatibility and break the whole Internet and something had to be done urgently about this terrible situation. (...) It doesn’t address any of the gripes that were motivating the IETF revision.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li readability=&quot;5.8509316770186&quot;&gt;
&lt;p&gt;2014 - IETF &lt;a href=&quot;https://tools.ietf.org/html/rfc7158&quot;&gt;RFC 7158&lt;/a&gt; makes the specification &quot;Standard Tracks&quot; instead of &quot;Informational&quot;, allows scalars (anything other than arrays and objects) such as &lt;code&gt;123&lt;/code&gt; and &lt;code&gt;true&lt;/code&gt; at the root level as ECMA does, warns about bad practices such as duplicated keys and broken Unicode strings, without explicitely forbidding them, though.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;2014 - IETF &lt;a href=&quot;https://tools.ietf.org/html/rfc7159&quot;&gt;RFC 7159&lt;/a&gt; was released to fix a typo in RFC 7158, which was dated from &quot;March 2013&quot; instead of &quot;March 2014&quot;.&lt;/li&gt;
&lt;li&gt;2017 - IETF &lt;a href=&quot;https://tools.ietf.org/html/rfc8259&quot;&gt;RFC 8259&lt;/a&gt; was released in December 2017. It basically adds two things: 1) outside of closed eco-systems, JSON MUST be encoded in UTF-8 and 2) JSON text that is not networked transmitted MAY now add the byte ordrer mark &lt;code&gt;U+FEFF&lt;/code&gt;, although this is not stated explicitely.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Despite the clarifications they bring, RFC 7159 and 8259 contain several approximations and leaves many details loosely specified.&lt;/p&gt;
&lt;p&gt;For instance, RFC 8259 &lt;a href=&quot;https://tools.ietf.org/html/rfc8259#section-1&quot;&gt;mentions&lt;/a&gt; that a design goal of JSON was to be &quot;a subset of JavaScript&quot;, but it's actually not. Specifically, JSON allows the Unicode line terminators &lt;code&gt;U+2028 LINE SEPARATOR&lt;/code&gt; and &lt;code&gt;U+2029 PARAGRAPH SEPARATOR&lt;/code&gt; to appear unescaped. But JavaScript specifies that strings cannot contains line terminators (&lt;a href=&quot;http://www.ecma-international.org/ecma-262/5.1/#sec-7.8.4&quot;&gt;ECMA-262 - 7.8.4 String Literals&lt;/a&gt;), and line terminators include... &lt;code&gt;U+2028&lt;/code&gt; and &lt;code&gt;U+2029&lt;/code&gt; (&lt;a href=&quot;http://www.ecma-international.org/ecma-262/5.1/#sec-7.3&quot;&gt;7.3 Line Terminators&lt;/a&gt;). The single fact that these two characters are allowed without escape in JSON strings while they are not in JavaScript implies that JSON is &lt;strong&gt;not&lt;/strong&gt; a subset of JavaScript, despite the JSON design goals.&lt;/p&gt;
&lt;p&gt;Also, RFC 7159 is unclear about how a JSON parser should treat extreme number values, malformed Unicode strings, similar objects or handle recursion depth. Some corner cases are explicitely left free to implementations, while others suffer from contradictory statements.&lt;/p&gt;
&lt;p&gt;To illustrate the poor precision of RFC 7159, I wrote a corpus of JSON test files and documented how selected JSON parsers chose to handle these files. You'll see that deciding if a test file should be parsed or not is not always straightforward. In my findings, there were no two parsers that exhibited the same behaviour, which may cause serious interoperability issues.&lt;/p&gt;
&lt;h3&gt; 2. Parsing Tests&lt;/h3&gt;
&lt;p&gt;In this section, I explain how to create test files to validate parsers behaviour, discuss some interesting tests, and the rationale to decide if they should be accepted or rejected by RFC 7159 compliant parsers, or if parsers should be free to accept or reject the contents.&lt;/p&gt;
&lt;p&gt;File names start with a letter which tells the expected result: &lt;code&gt;y&lt;/code&gt; (yes) for parsing success, &lt;code&gt;n&lt;/code&gt; (no) for parsing error, and &lt;code&gt;i&lt;/code&gt; for implementation defined. They also give clues about which component of the parser is specifically tested.&lt;/p&gt;
&lt;p&gt;For instance, &lt;code&gt;n_string_unescaped_tab.json&lt;/code&gt; contains &lt;code&gt;[&quot;&lt;u&gt;09&lt;/u&gt;&quot;]&lt;/code&gt;, which is an array containing a string, which consists in the &lt;code&gt;TAB 0x09&lt;/code&gt; character, which MUST be u-escaped according to JSON specifications. Note how the underlined values represent the hex values of the bytes. This file specifically tests string parsing, hence the &lt;code&gt;string&lt;/code&gt; in file name, and not &lt;code&gt;structure&lt;/code&gt;, &lt;code&gt;array&lt;/code&gt; or &lt;code&gt;object&lt;/code&gt;. According to RFC 7159, this is not a valid JSON string, hence the &lt;code&gt;n&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Note that since several parsers don't allow scalars at the top level (&lt;code&gt;&quot;test&quot;&lt;/code&gt;), I embed strings into arrays (&lt;code&gt;[&quot;test&quot;]&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;You'll find more that 300 tests in the &lt;a href=&quot;https://github.com/nst/JSONTestSuite&quot;&gt;JSONTestSuite GitHub repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The test files were mostly handcrafted while reading specifications, trying to pay attention to edge cases and ambiguous parts. I also tried to reuse other test suites found on the Internet (mainly &lt;a href=&quot;https://code.google.com/archive/p/json-test-suite/&quot;&gt;json-test-suite&lt;/a&gt; and &lt;a href=&quot;http://www.json.org/JSON_checker/&quot;&gt;JSON Checker&lt;/a&gt;), but I found that most test suites did only cover basic cases.&lt;/p&gt;
&lt;p&gt;Finally, I also generated JSON files with the fuzzing software &lt;a href=&quot;http://lcamtuf.coredump.cx/afl/&quot;&gt;American Fuzzy Lop&lt;/a&gt;. I then removed redundant tests that produced the same set of results, and then reduced the remaining ones to the keep the least number of characters that triggered these results (see &lt;a href=&quot;http://seriot.ch/parsing_json.php#3&quot;&gt;section 3&lt;/a&gt;).&lt;/p&gt;
&lt;h4&gt; 2.1 Structure&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Scalars&lt;/strong&gt; - Clearly, scalars such as &lt;code&gt;123&lt;/code&gt; or &lt;code&gt;&quot;asd&quot;&lt;/code&gt; must be parsed. In practice, many popular parsers do still implement RFC 4627 and won't parse lonely values. So there are basic tests such as:&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;1&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;y_structure_lonely_string.json&lt;/td&gt;
&lt;td&gt;&quot;asd&quot;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;Trailing commas&lt;/strong&gt; - Trailing commas such as in &lt;code&gt;[123,]&lt;/code&gt; or &lt;code&gt;{&quot;a&quot;:1,}&lt;/code&gt; are not part of the grammar, so these files should not pass, right? The thing is that RFC 8259 allows parsers to support &quot;extensions&quot; (&lt;a href=&quot;https://tools.ietf.org/html/rfc8259#section-9&quot;&gt;section 9&lt;/a&gt;), although it does not elaborates about extensions. In practice, allowing trailing commas is a common extension. Since it's not part of JSON grammar, parser &lt;em&gt;don't have&lt;/em&gt; to support it, so the file name starts with &lt;code&gt;n&lt;/code&gt;.&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;2&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_object_trailing_comma.json&lt;/td&gt;
&lt;td&gt;{&quot;id&quot;:0,}&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_object_several_trailing_commas.json&lt;/td&gt;
&lt;td&gt;{&quot;id&quot;:0,,,,,}&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;Comments&lt;/strong&gt; - Comments are not part of the grammar. Crockford &lt;a href=&quot;https://plus.google.com/+DouglasCrockfordEsq/posts/RK8qyGVaGSr&quot;&gt;removed&lt;/a&gt; them from early specifications. Yet, they are still another common extension. Some parsers allow trailing comments &lt;code&gt;[1]//xxx&lt;/code&gt;, or even inline comments &lt;code&gt;[1,/*xxx*/2]&lt;/code&gt;.&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;2&quot;&gt;&lt;tr&gt;&lt;td class=&quot;fixedWidth&quot;&gt;y_string_comments.json&lt;/td&gt;
&lt;td&gt;[&quot;a/*b*/c/*d//e&quot;]&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_object_trailing_comment.json&lt;/td&gt;
&lt;td&gt;{&quot;a&quot;:&quot;b&quot;}/**/&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_structure_object_with_comment.json&lt;/td&gt;
&lt;td&gt;{&quot;a&quot;:/*comment*/&quot;b&quot;}&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;Unclosed Structures&lt;/strong&gt; - These tests cover everything that is opened and not closed or the opposite, such as &lt;code&gt;[&lt;/code&gt; or &lt;code&gt;[1,{,3]&lt;/code&gt;. They are clearly invalid and must fail.&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;2&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_structure_object_unclosed_no_value.json&lt;/td&gt;
&lt;td&gt;{&quot;&quot;:&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_structure_object_followed_by_closing_object.json&lt;/td&gt;
&lt;td&gt;{}}&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;Nested Structures&lt;/strong&gt; - Structures may contain other structures. An array may contain other arrays. The first element can be an array, whose first element is also an array, etc, like russian dolls &lt;code&gt;[[[[[]]]]]&lt;/code&gt;. RFC 8259 allows parsers to set limits to the maximum depth of nesting (&lt;a href=&quot;https://tools.ietf.org/html/rfc8259#section-9&quot;&gt;section 9&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In practice, several parsers don't set a depth limit and crash after a certain threshold. For example, Xcode itself will crash when opening a &lt;code&gt;.json&lt;/code&gt; file made the character &lt;code&gt;[&lt;/code&gt; repeated 10000 times, most probably because the JSON syntax highlighter does not implement a depth limit.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ python -c &quot;print('['*100000)&quot; &amp;gt; ~/x.json
$ ./Xcode ~/x.json
Segmentation fault: 11
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;White Spaces&lt;/strong&gt; - RFC 7159 grammar defines white spaces as &lt;code&gt;0x20&lt;/code&gt; (space), &lt;code&gt;0x09&lt;/code&gt; (tab), &lt;code&gt;0x0A&lt;/code&gt; (line feed) and &lt;code&gt;0x0D&lt;/code&gt; (carriage return). It allows white spaces before and after &quot;structural characters&quot; &lt;code&gt;[]{}:,&lt;/code&gt;. So, we'll write passing tests like &lt;code&gt;&lt;u&gt;20&lt;/u&gt;[&lt;u&gt;090A&lt;/u&gt;]&lt;u&gt;0D&lt;/u&gt;&lt;/code&gt; and failing ones including all kinds of white spaces that are not explicitely allowed, such as &lt;code&gt;0x0C&lt;/code&gt; form feed or &lt;code&gt;[&lt;u&gt;E281A0&lt;/u&gt;]&lt;/code&gt;, which is the UTF-8 encoding for &lt;code&gt;U+2060 WORD JOINER&lt;/code&gt;.&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;2&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_structure_whitespace_formfeed.json&lt;/td&gt;
&lt;td&gt;[&lt;u&gt;0C&lt;/u&gt;]&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_structure_whitespace_U+2060_word_joiner.json&lt;/td&gt;
&lt;td&gt;[&lt;u&gt;E281A0&lt;/u&gt;]&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_structure_no_data.json&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;h4&gt; 2.2 Numbers&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;NaN and Infinity&lt;/strong&gt; - Strings that describe special numbers such as &lt;code&gt;NaN&lt;/code&gt; or &lt;code&gt;Infinity&lt;/code&gt; are not part of the JSON grammar. However, several parsers accept them, which can be considered as an &quot;extension&quot; (&lt;a href=&quot;https://tools.ietf.org/html/rfc8259#section-9&quot;&gt;section 9&lt;/a&gt;). Test files also test the negative forms &lt;code&gt;-NaN&lt;/code&gt; and &lt;code&gt;-Infinity&lt;/code&gt;.&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;1&quot;&gt;&lt;tr&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_number_NaN.json&lt;/td&gt;
&lt;td&gt;[NaN]&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_number_minus_infinity.json&lt;/td&gt;
&lt;td&gt;[-Infinity]&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;Hex Numbers&lt;/strong&gt; - RFC 7159 doesn't allow hex numbers. Tests will include numbers such as &lt;code&gt;0xFF&lt;/code&gt;, and these files should not be parsed.&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;1&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_number_hex_2_digits.json&lt;/td&gt;
&lt;td&gt;[0x42]&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;Range and Precision&lt;/strong&gt; - What about numbers with a huge amount of digits? According to RFC 8259, &quot;A JSON parser MUST accept all texts that conform to the JSON grammar&quot; (&lt;a href=&quot;https://tools.ietf.org/html/rfc8259#section-9&quot;&gt;section 9&lt;/a&gt;). However, according to the same paragraph, &quot;An implementation may set limits on the range and precision of numbers.&quot;. So, it is unclear to me whether parsers are allowed to raise errors when they meet extreme values such &lt;code&gt;1e9999&lt;/code&gt; or &lt;code&gt;0.0000000000000000000000000000001&lt;/code&gt;.&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;2&quot;&gt;&lt;tr readability=&quot;4&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;i_number_very_big_negative_int.json&lt;/td&gt;
&lt;td&gt;[-237462374673276894279832(...)&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;[Update 2016-11-02]&lt;/strong&gt; The original version of this article classified the &quot;Range and Precision&quot; tests as &lt;code&gt;y_&lt;/code&gt; (must pass). This classification was &lt;a href=&quot;https://github.com/nst/JSONTestSuite/issues/51&quot;&gt;challenged&lt;/a&gt; and I eventually changed the tests into &lt;code&gt;i_&lt;/code&gt; (implementation defined).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exponential Notation&lt;/strong&gt; - Parsing exponential notation can be surprisingly hard (see the results section). Here are some valid contents &lt;code&gt;[0E0]&lt;/code&gt;, &lt;code&gt;[0e+1]&lt;/code&gt; and invalid ones &lt;code&gt;[1.0e+]&lt;/code&gt;, &lt;code&gt;[0E]&lt;/code&gt; and &lt;code&gt;[1eE2]&lt;/code&gt;.&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;2&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_number_0_capital_E+.json&lt;/td&gt;
&lt;td&gt;[0E+]&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_number_.2e-3.json&lt;/td&gt;
&lt;td&gt;[.2e-3]&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;y_number_double_huge_neg_exp.json&lt;/td&gt;
&lt;td&gt;[123.456e-789]&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;h4&gt; 2.3 Arrays&lt;/h4&gt;
&lt;p&gt;Most edge cases regarding arrays are opening/closing issues and nesting limit. These cases were discussed in section &lt;a href=&quot;http://seriot.ch/parsing_json.php#21&quot;&gt;2.1 Structure&lt;/a&gt;. Passing tests will include &lt;code&gt;[[],[[]]]&lt;/code&gt;, while failing tests will be like &lt;code&gt;]&lt;/code&gt; or &lt;code&gt;[[]]]&lt;/code&gt;.&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;3&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_array_comma_and_number.json&lt;/td&gt;
&lt;td&gt;[,1]&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_array_colon_instead_of_comma.json&lt;/td&gt;
&lt;td&gt;[&quot;&quot;: 1]&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_array_unclosed_with_new_lines.json&lt;/td&gt;
&lt;td&gt;[1,&lt;u&gt;0A&lt;/u&gt;1&lt;u&gt;0A&lt;/u&gt;,1&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;h4&gt; 2.4 Objects&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Duplicated Keys&lt;/strong&gt; - &lt;a href=&quot;https://tools.ietf.org/html/rfc8259#section-4&quot;&gt;RFC 8259 section 4&lt;/a&gt; says that &quot;The names within an object should be unique.&quot;. It does not prevent parsing objects where the same key does appear several times &lt;code&gt;{&quot;a&quot;:1,&quot;a&quot;:2}&lt;/code&gt;, but lets parsers decide what to do in this case. The same section 4 even mentions that &quot;(some) implementations report an error or fail to parse the object&quot;, without telling clearly if failing to parse such objects is compliant or not with the RFC and especially &lt;a href=&quot;https://tools.ietf.org/html/rfc8259#section-9&quot;&gt;section 9&lt;/a&gt;: &quot;A JSON parser MUST accept all texts that conform to the JSON grammar.&quot;.&lt;/p&gt;
&lt;p&gt;Variants of this special case include same key - same value &lt;code&gt;{&quot;a&quot;:1,&quot;a&quot;:1}&lt;/code&gt;, and similar keys or values, where the similarity depends on how you compare strings. For example, the keys may be binary different but equivalent according to Unicode NFC normalization, such as in &lt;code&gt;{&quot;&lt;u&gt;C3A9&lt;/u&gt;:&quot;NFC&quot;,&quot;&lt;u&gt;65CC81&lt;/u&gt;&quot;:&quot;NFD&quot;}&lt;/code&gt; where boths keys encode &quot;é&quot;. Tests will also include &lt;code&gt;{&quot;a&quot;:0,&quot;a&quot;:-0}&lt;/code&gt;.&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;5&quot;&gt;&lt;tr&gt;&lt;td class=&quot;fixedWidth&quot;&gt;y_object_empty_key.json&lt;/td&gt;
&lt;td&gt;{&quot;&quot;:0}&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;y_object_duplicated_key_and_value.json&lt;/td&gt;
&lt;td&gt;{&quot;a&quot;:&quot;b&quot;,&quot;a&quot;:&quot;b&quot;}&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_object_double_colon.json&lt;/td&gt;
&lt;td&gt;{&quot;x&quot;::&quot;b&quot;}&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_object_key_with_single_quotes.json&lt;/td&gt;
&lt;td&gt;{key: 'value'}&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_object_missing_key.json&lt;/td&gt;
&lt;td&gt;{:&quot;b&quot;}&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_object_non_string_key.json&lt;/td&gt;
&lt;td&gt;{1:1}&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;h4&gt; 2.5 Strings&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;File Encoding&lt;/strong&gt; - Former &lt;a href=&quot;https://tools.ietf.org/html/rfc7159#section-8.1&quot;&gt;RFC 7159&lt;/a&gt; did only recommend UTF-8, and said that &quot;JSON text SHALL be encoded in UTF-8, UTF-16, or UTF-32&quot;.&lt;/p&gt;
&lt;p&gt;Now RFC 8259 &lt;a href=&quot;https://tools.ietf.org/html/rfc8259#section-8.1&quot;&gt;section 8.1&lt;/a&gt; says that &quot;JSON text exchanged between systems that are not part of a closed ecosystem MUST be encoded using UTF-8&quot;.&lt;/p&gt;
&lt;p&gt;Still, passing tests should include text encoded in these three encodings. UTF-16 and UTF-32 texts should also include both their big-endian and little-endian variants.&lt;/p&gt;
&lt;p&gt;The parsing of invalid UTF-8 will be implementation defined.&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;1&quot;&gt;&lt;tr&gt;&lt;td class=&quot;fixedWidth&quot;&gt;y_string_utf16.json&lt;/td&gt;
&lt;td&gt;&lt;u&gt;FFFE&lt;/u&gt;[&lt;u&gt;00&lt;/u&gt;&quot;&lt;u&gt;00E900&lt;/u&gt;&quot;&lt;u&gt;00&lt;/u&gt;]&lt;u&gt;00&lt;/u&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;i_string_iso_latin_1.json&lt;/td&gt;
&lt;td&gt;[&quot;&lt;u&gt;E9&lt;/u&gt;&quot;]&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;[Update 2016-11-04]&lt;/strong&gt; The first version of this article considered invalid UTF-8 as &lt;code&gt;n_&lt;/code&gt; tests. This classification was &lt;a href=&quot;https://github.com/nst/JSONTestSuite/issues/30&quot;&gt;challenged&lt;/a&gt; and I eventually changed these tests into &lt;code&gt;i_&lt;/code&gt; tests.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Byte Order Mark&lt;/strong&gt; - Former RFC 8259 &lt;a href=&quot;https://tools.ietf.org/html/rfc8259#section-8.1&quot;&gt;section 8.1&lt;/a&gt; stated &quot;Implementations MUST NOT add a byte order mark to the beginning of a JSON text&quot;, &quot;implementations (...) MAY ignore the presence of a byte order mark rather than treating it as an error&quot;.&lt;/p&gt;
&lt;p&gt;Now, RFC 8259 &lt;a href=&quot;https://tools.ietf.org/html/rfc8259#section-8.1&quot;&gt;section 8.1&lt;/a&gt; adds: &quot;Implementations MUST NOT add a byte order mark (&lt;code&gt;U+FEFF&lt;/code&gt;) to the beginning &lt;em&gt;of a networked-transmitted JSON text&lt;/em&gt;.&quot;, which seems to imply that implemenatations may now add a BOM when JSON is not sent over the network.&lt;/p&gt;
&lt;p&gt;Tests with implementation defined will include a plain UTF-8 BOM with no other content, a UTF-8 BOM with a UTF-8 string, but also a UTF-8 BOM with a UTF-16 string, and a UTF-16 BOM with a UTF-8 string.&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;3&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_structure_UTF8_BOM_no_data.json&lt;/td&gt;
&lt;td&gt;&lt;u&gt;EFBBBF&lt;/u&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_structure_incomplete_UTF8_BOM.json&lt;/td&gt;
&lt;td&gt;&lt;u&gt;EFBB&lt;/u&gt;{}&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;i_structure_UTF-8_BOM_empty_object.json&lt;/td&gt;
&lt;td&gt;&lt;u&gt;EFBBBF&lt;/u&gt;{}&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;Control Characters&lt;/strong&gt; - Control characters must be escaped, and are defined as &lt;code&gt;U+0000&lt;/code&gt; through &lt;code&gt;U+001F&lt;/code&gt; (&lt;a href=&quot;https://tools.ietf.org/html/rfc8259#section-7&quot;&gt;section 7&lt;/a&gt;). This range does not include &lt;code&gt;0x7F DEL&lt;/code&gt;, which may be part of other definitions of control characters (see &lt;a href=&quot;http://seriot.ch/parsing_json.php#46&quot;&gt;section 4.6 Bash JSON.sh&lt;/a&gt;). That is why passing tests include &lt;code&gt;[&quot;&lt;u&gt;7F&lt;/u&gt;&quot;]&lt;/code&gt;.&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;2&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_string_unescaped_ctrl_char.json&lt;/td&gt;
&lt;td&gt;[&quot;a&lt;u&gt;09&lt;/u&gt;a&quot;]&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;y_string_unescaped_char_delete.json&lt;/td&gt;
&lt;td&gt;[&quot;&lt;u&gt;7F&lt;/u&gt;&quot;]&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_string_escape_x.json&lt;/td&gt;
&lt;td&gt;[&quot;\x00&quot;]&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;Escape&lt;/strong&gt; - &quot;All characters may be escaped&quot; (&lt;a href=&quot;https://tools.ietf.org/html/rfc8259#section-7&quot;&gt;section 7&lt;/a&gt;) like &lt;code&gt;\uXXXX&lt;/code&gt;, but some MUST be escaped: quotation mark, reverse solidus and control characters. Failing tests should include the escape character without the escaped value, or with an incomplete escaped value. Examples: &lt;code&gt;[&quot;\&quot;]&lt;/code&gt;, &lt;code&gt;[&quot;\&lt;/code&gt;, &lt;code&gt;[\&lt;/code&gt;.&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;2&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;y_string_allowed_escapes.json&lt;/td&gt;
&lt;td&gt;[&quot;\&quot;\\/\b\f\n\r\t&quot;]&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_structure_bad_escape.json&lt;/td&gt;
&lt;td&gt;[&quot;\&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;The escape character can be used to represent codepoints in the Basic Multilingual Plane (&lt;code&gt;\u005C&lt;/code&gt;). Passing tests will include the zero character &lt;code&gt;\u0000&lt;/code&gt;, which may cause issues in C-based parsers. Failing tests will include capital U &lt;code&gt;\U005C&lt;/code&gt;, non-hexadecimal escaped values &lt;code&gt;\u123Z&lt;/code&gt; and incomplete escaped values &lt;code&gt;\u123&lt;/code&gt;.&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;3&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;y_string_backslash_and_u_escaped_zero.json&lt;/td&gt;
&lt;td&gt;[&quot;\u0000&quot;]&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_string_invalid_unicode_escape.json&lt;/td&gt;
&lt;td&gt;[&quot;\uqqqq&quot;]&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_string_incomplete_escaped_character.json&lt;/td&gt;
&lt;td&gt;[&quot;\u00A&quot;]&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;Escaped Invalid Characters&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Codepoints outside of the BMP are represented by their escaped UTF-16 surrogates: &lt;code&gt;U+1D11E&lt;/code&gt; becomes &lt;code&gt;\uD834\uDD1E&lt;/code&gt;. Passing tests will include single surrogates, since they are valid JSON according to the grammar. RFC 7159 &lt;a href=&quot;https://www.rfc-editor.org/errata_search.php?rfc=7159&amp;amp;eid=3984&quot;&gt;errata 3984&lt;/a&gt; raised the issue of grammatically correct escaped codepoints that don't encode Unicode characters.&lt;/p&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;The ABNF cannot at the same time allow non conformant Unicode codepoints (section 7) and states conformance to Unicode (section 1).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The editors considered that the grammar should not be restricted, and that warning users about the fact that parsers behaviour was &quot;unpredictable&quot; (&lt;a href=&quot;https://tools.ietf.org/html/rfc8259#section-8.2&quot;&gt;RFC 8259 section 8.2&lt;/a&gt;) was enough. In other words, parsers MUST parse u-escaped invalid codepoints, but the result is undefined, hence the &lt;code&gt;i_&lt;/code&gt; (implementation definded) prefix in the file name. According to the Unicode standard, invalid codepoints should be replaced by &lt;code&gt;U+FFFD REPLACEMENT CHARACTER&lt;/code&gt;. People familiar with &lt;a href=&quot;http://seriot.ch/resources/talks_papers/i_love_unicode_softshake.pdf&quot;&gt;Unicode complexity&lt;/a&gt; won't be surprised that this replacement is not mandatory, and can be done in several ways (see &lt;a href=&quot;http://unicode.org/review/pr-121.html&quot;&gt;Unicode PR #121: Recommended Practice for Replacement Characters&lt;/a&gt;). So several parsers use replacement characters, while other keep the escaped form or produce an non-Unicode character (see &lt;a href=&quot;http://seriot.ch/parsing_json.php#5&quot;&gt;Section 5 - Parsing Contents&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[Update 2016-11-03]&lt;/strong&gt; In the first version of this article, I treated non-characters such as &lt;code&gt;U+FDD0&lt;/code&gt; to &lt;code&gt;U+10FFFE&lt;/code&gt; the same was as invalid codepoints (&lt;code&gt;i_&lt;/code&gt; tests). This classification was &lt;a href=&quot;https://github.com/nst/JSONTestSuite/issues/52&quot;&gt;challenged&lt;/a&gt; and I eventually changed the non-characters tests into &lt;code&gt;y_&lt;/code&gt; tests.&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;6&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;y_string_accepted_surrogate_pair.json&lt;/td&gt;
&lt;td&gt;[&quot;\uD801\udc37&quot;]&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_string_incomplete_escaped_character.json&lt;/td&gt;
&lt;td&gt;[&quot;\u00A&quot;]&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;i_string_incomplete_surrogates_escape_valid.json&lt;/td&gt;
&lt;td&gt;[&quot;\uD800\uD800\n&quot;]&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;i_string_lone_second_surrogate.json&lt;/td&gt;
&lt;td&gt;[&quot;\uDFAA&quot;]&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;i_string_1st_valid_surrogate_2nd_invalid.json&lt;/td&gt;
&lt;td&gt;[&quot;\uD888\u1234&quot;]&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;i_string_inverted_surrogates_U+1D11E.json&lt;/td&gt;
&lt;td&gt;[&quot;\uDd1e\uD834&quot;]&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;Raw non-Unicode Characters&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The previous section discussed non-Unicode codepoints that appear in strings, such as &lt;code&gt;&quot;\uDEAD&quot;&lt;/code&gt;, which is valid Unicode in its u-escaped form, but doesn't decode into a Unicode character.&lt;/p&gt;
&lt;p&gt;Parsers also have to handle raw bytes that don't encode Unicode characters. For instance, the byte &lt;code&gt;&lt;u&gt;FF&lt;/u&gt;&lt;/code&gt; does not represent a Unicode character in UTF-8. As a consequence, a string containing &lt;code&gt;&lt;u&gt;FF&lt;/u&gt;&lt;/code&gt; is not an UTF-8 string. In this case, parsers should simply refuse to parse the string, because &quot;A string is a sequence of zero or more Unicode characters&quot; &lt;a href=&quot;https://tools.ietf.org/html/rfc8259#section-1&quot;&gt;RFC 8259 section 1&lt;/a&gt; and &quot;JSON text SHALL be encoded in Unicode &lt;a href=&quot;https://tools.ietf.org/html/rfc7159#section-8.1&quot;&gt;RFC 7159 section 8.1&lt;/a&gt;.&lt;/p&gt;
&lt;table class=&quot;monospace&quot; readability=&quot;2&quot;&gt;&lt;tr&gt;&lt;td class=&quot;fixedWidth&quot;&gt;y_string_utf8.json&lt;/td&gt;
&lt;td&gt;[&quot;€𝄞&quot;]&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_string_invalid_utf-8.json&lt;/td&gt;
&lt;td&gt;[&quot;&lt;u&gt;FF&lt;/u&gt;&quot;]&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td class=&quot;fixedWidth&quot;&gt;n_array_invalid_utf8.json&lt;/td&gt;
&lt;td class=&quot;grey&quot;&gt;[&lt;u&gt;FF&lt;/u&gt;]&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;RFC 8259 Ambiguities&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Beyond the specific cases we just went through, finding out if a parser is RFC 8259 compliant or not is next to impossible because of &lt;a href=&quot;http://seriot.ch/parsing_json.php&quot;&gt;section 9 &quot;Parsers&quot;&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;A JSON parser MUST accept all texts that conform to the JSON grammar. A JSON parser MAY accept non-JSON forms or extensions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To this point, I perfectly understand the RFC. All grammatically correct inputs MUST be parsed, and parsers are free to accept other contents as well.&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;An implementation may set limits on the size of texts that it accepts. An implementation may set limits on the maximum depth of nesting. An implementation may set limits on the range and precision of numbers. An implementation may set limits on the length and character contents of strings.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;All these limitations sound reasonable (except maybe the one about &quot;character contents&quot;), but contradict the &quot;MUST&quot; from the previous sentence. &lt;a href=&quot;https://tools.ietf.org/html/rfc2119&quot;&gt;RFC 2119&lt;/a&gt; is crystal-clear about the meaning of &quot;MUST&quot;:&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;MUST - This word, or the terms &quot;REQUIRED&quot; or &quot;SHALL&quot;, mean that the definition is an absolute requirement of the specification.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;RFC 7159 allows restrictions, but does not set minimal requirements, so technically speaking, a parser that cannot parse strings longer than 3 characters is still compliant with RFC 7159.&lt;/p&gt;
&lt;p&gt;Also, RFC 7159 section 9 should require the parsers to document the restrictions clearly, and/or allow configuration by the user. These configurations would still cause interoperability issues, that's why minimal requirements should be preferred.&lt;/p&gt;
&lt;p&gt;This lack of precision regarding allowed restrictions makes it almost impossible to say if a parser is actually RFC 7159 compliant. Indeed, parsing contents that don't match the grammar is not wrong (it's an &quot;extension&quot;) and rejecting contents that does match the grammar is allowed (it's a parser &quot;limit&quot;).&lt;/p&gt;
&lt;h3&gt; 3. Testing Architecture&lt;/h3&gt;
&lt;p&gt;Independently from how parsers should behave, I wanted to observe how they actually behave, so I picked several JSON parsers and set up things so that I could feed them with my test files.&lt;/p&gt;
&lt;p&gt;As I'm a Cocoa developer, I included mostly Swift and Objective-C parsers, but also C, Python, Ruby, R, Lua, Perl, Bash and Rust parsers, chosen pretty arbitrarily. I mainly tried to achieve diversity in age, popularity and languages.&lt;/p&gt;
&lt;p&gt;Several parsers have options to increase or decrease strictness, tweak Unicode support or allow specific extensions. I strived to always configure the parsers so that they behave as close as possible to the most strict interpretation of RFC 8259.&lt;/p&gt;
&lt;p&gt;A Python script &lt;code&gt;run_tests.py&lt;/code&gt; runs each parser with each test file (or a single test when the file is passed as an argument). The parsers are generally wrapped so that the process returns 0 in case of success, 1 in case of parsing error, yet another status in case of crash, a 5-second delay being considered as a timeout. Basically, I turned JSON parsers into JSON validators.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;run_tests.py&lt;/code&gt; compares the return value of each test with the expected result indicated by the file name prefix. When the value doesn't match, or when this prefix is &lt;code&gt;i&lt;/code&gt; (implementation defined), &lt;code&gt;run_tests.py&lt;/code&gt; writes a line in a log file (&lt;code&gt;results/logs.txt&lt;/code&gt;) in a specific format such as:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;Python 2.7.10   SHOULD_HAVE_FAILED  n_number_infinity.json
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;img src=&quot;http://www.seriot.ch/json/run_tests.svg&quot; width=&quot;600&quot; alt=&quot;Testing Architecture&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;run_tests.py&lt;/code&gt; then reads the log file and generates HTML tables with the results (&lt;code&gt;results/parsing.html&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The results show one row per file, one column per parser, and one color per unexpected result. They also show detailed results by parser.&lt;/p&gt;
CrashTimeout
&lt;table class=&quot;monospace&quot; readability=&quot;5&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td bgcolor=&quot;#CC6600&quot;&gt;parsing should have succeeded but failed&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td bgcolor=&quot;#FFCC33&quot;&gt;parsing should have failed but succeeded&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td bgcolor=&quot;#66CCFF&quot;&gt;result undefined, parsing succeeded&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;3&quot;&gt;&lt;td bgcolor=&quot;#0066FF&quot;&gt;result undefined, parsing failed&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td bgcolor=&quot;#FF3333&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td bgcolor=&quot;#666666&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;Tests are sorted by results equality, making easy to spot sets of similar results and remove redundant tests.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://www.seriot.ch/json/similar_results.png&quot; border=&quot;1&quot; width=&quot;600&quot; alt=&quot;Sets of tests with similar results&quot; /&gt;&lt;/p&gt;
&lt;h3&gt; 4. Results and Comments&lt;/h3&gt;
&lt;p&gt;4.1 &lt;a href=&quot;http://seriot.ch/parsing_json.php#41&quot;&gt;Full Results&lt;/a&gt;&lt;br /&gt;4.2 &lt;a href=&quot;http://seriot.ch/parsing_json.php#42&quot;&gt;C Parsers&lt;/a&gt;&lt;br /&gt;4.3 &lt;a href=&quot;http://seriot.ch/parsing_json.php#43&quot;&gt;Obj-C Parsers&lt;/a&gt;&lt;br /&gt;4.4 &lt;a href=&quot;http://seriot.ch/parsing_json.php#44&quot;&gt;Apple (NS)JSONSerialization&lt;/a&gt;&lt;br /&gt;4.5 &lt;a href=&quot;http://seriot.ch/parsing_json.php#45&quot;&gt;Swift Freddy&lt;/a&gt;&lt;br /&gt;4.6 &lt;a href=&quot;http://seriot.ch/parsing_json.php#46&quot;&gt;Bash&lt;/a&gt;&lt;br /&gt;4.7 &lt;a href=&quot;http://seriot.ch/parsing_json.php#47&quot;&gt;Other Parsers&lt;/a&gt;&lt;br /&gt;4.8 &lt;a href=&quot;http://seriot.ch/parsing_json.php#48&quot;&gt;JSON Checker&lt;/a&gt;&lt;br /&gt;4.9 &lt;a href=&quot;http://seriot.ch/parsing_json.php#49&quot;&gt;Regex&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt; 4.1 Full Results&lt;/h4&gt;
&lt;p&gt;Full results are presented in &lt;a href=&quot;http://seriot.ch/json/parsing.html&quot;&gt;http://seriot.ch/json/parsing.html&lt;/a&gt;. The tests are vertically sorted by similar results, so it is easy to prune similar tests. An option in &lt;code&gt;run_tests.py&lt;/code&gt; will produce &quot;pruned results&quot;: when a set of tests yields the same results, only the first one is kept. Pruned results HTML file is available here: &lt;a href=&quot;http://www.seriot.ch/json/parsing_pruned.html&quot;&gt;http://www.seriot.ch/json/parsing_pruned.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The most serious issues are crashes (in red), since parsing an uncontrolled input may put the whole process at risk. The &quot;should have passed&quot; tests (in brown) are also very dangerous, because an uncontrolled input may prevent the parser to parse a whole document. The &quot;should have failed&quot; tests (in yellow) are more benign. They indicate a JSON &quot;extension&quot; that can be parsed. Everything will work fine, until the parser is replaced with another parser which does not parse the same &quot;extensions&quot;...&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://seriot.ch/json/pruned_results.png&quot;&gt;&lt;img src=&quot;http://seriot.ch/json/pruned_results.png&quot; border=&quot;1&quot; width=&quot;600&quot; alt=&quot;JSON Parsing Tests&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This section highlights and comments some noticeable results.&lt;/p&gt;
&lt;h4&gt; 4.2 C Parsers&lt;/h4&gt;
&lt;p&gt;Here are the five C parsers considered:&lt;/p&gt;
&lt;p&gt;And here is a quick comparison between them:&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;
&lt;th class=&quot;fixedWidthSmall&quot;&gt;jsmn&lt;/th&gt;
&lt;th class=&quot;fixedWidthSmall&quot;&gt;jansson&lt;/th&gt;
&lt;th class=&quot;fixedWidthSmall&quot;&gt;ccan&lt;/th&gt;
&lt;th class=&quot;fixedWidthSmall&quot;&gt;cJSON&lt;/th&gt;
&lt;th class=&quot;fixedWidthSmall&quot;&gt;json-parser&lt;/th&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Parses &lt;code&gt;[&quot;\u0000&quot;]&lt;/code&gt;&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;YES&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;YES&lt;/td&gt;
&lt;td class=&quot;SHOULD_HAVE_PASSED&quot;&gt;NO&lt;/td&gt;
&lt;td class=&quot;SHOULD_HAVE_PASSED&quot;&gt;NO&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;YES&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Too liberal&lt;/td&gt;
&lt;td class=&quot;SHOULD_HAVE_FAILED&quot;&gt;YES&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;td class=&quot;SHOULD_HAVE_FAILED&quot;&gt;YES&lt;/td&gt;
&lt;td class=&quot;SHOULD_HAVE_FAILED&quot;&gt;YES&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Crash on nested structs.&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;td class=&quot;CRASH&quot;&gt;YES&lt;/td&gt;
&lt;td class=&quot;CRASH&quot;&gt;YES&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Rejects big numbers&lt;/td&gt;
&lt;td class=&quot;SHOULD_HAVE_PASSED&quot;&gt;YES&lt;/td&gt;
&lt;td class=&quot;SHOULD_HAVE_PASSED&quot;&gt;YES&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;You can refer to the full results for more details.&lt;/p&gt;
&lt;h4&gt; 4.3 Objective-C Parsers&lt;/h4&gt;
&lt;p&gt;Here are a couple of Objective-C parsers that used to be very popular in the early days of iOS development, especially because Apple waited until iOS 5 to release NSJSONSerialization. They are still interesting to test, since they are used in production in many applications. Let's consider:&lt;/p&gt;
&lt;p&gt;And here is a quick comparison between them:&lt;/p&gt;
&lt;table readability=&quot;4&quot;&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;
&lt;th class=&quot;fixedWidthSmall&quot;&gt;JSONKit&lt;/th&gt;
&lt;th class=&quot;fixedWidthSmall&quot;&gt;TouchJSON&lt;/th&gt;
&lt;th class=&quot;fixedWidthSmall&quot;&gt;SBJSON&lt;/th&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Crash on nested structs.&lt;/td&gt;
&lt;td class=&quot;CRASH&quot;&gt;YES&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;td class=&quot;CRASH&quot;&gt;YES&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Crash on invalid UTF-8&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;td class=&quot;CRASH&quot;&gt;YES&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;Parses trailing garbage &lt;code&gt;[]x&lt;/code&gt;&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;td class=&quot;SHOULD_HAVE_FAILED&quot;&gt;YES&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Rejects big numbers&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;td class=&quot;SHOULD_HAVE_PASSED&quot;&gt;YES&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;Parses bad numbers &lt;code&gt;[0.e1]&lt;/code&gt;&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;td class=&quot;SHOULD_HAVE_FAILED&quot;&gt;YES&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;Treats &lt;code&gt;0x0C FORM FEED&lt;/code&gt; as white space&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;td class=&quot;SHOULD_HAVE_FAILED&quot;&gt;YES&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;NO&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;Parses non-char. &lt;code&gt;[&quot;\uFFFF&quot;]&lt;/code&gt;&lt;/td&gt;
&lt;td class=&quot;SHOULD_HAVE_PASSED&quot;&gt;NO&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;YES&lt;/td&gt;
&lt;td class=&quot;PASS&quot;&gt;YES&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;SBJSON survived after the arrival of NSJSONSerialization, is still maintained and is available through CocoaPods, so I reported the crash when parsing non UTF-8 strings such as &lt;code&gt;[&quot;&lt;u&gt;FF&lt;/u&gt;&quot;]&lt;/code&gt; in &lt;a href=&quot;https://github.com/stig/json-framework/issues/219&quot;&gt;issue #219&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;*** Assertion failure in -[SBJson4Parser parserFound:isValue:], SBJson4Parser.m:150
*** Terminating app due to uncaught exception 'NSInternalInconsistencyException', reason: 'Invalid parameter not satisfying: obj'
*** First throw call stack:
(
    0   CoreFoundation                      0x00007fff95f4b4f2 __exceptionPreprocess + 178
    1   libobjc.A.dylib                     0x00007fff9783bf7e objc_exception_throw + 48
    2   CoreFoundation                      0x00007fff95f501ca +[NSException raise:format:arguments:] + 106
    3   Foundation                          0x00007fff9ce86856 -[NSAssertionHandler handleFailureInMethod:object:file:lineNumber:description:] + 198
    4   test_SBJSON                         0x00000001000067e5 -[SBJson4Parser parserFound:isValue:] + 309
    5   test_SBJSON                         0x00000001000073f3 -[SBJson4Parser parserFoundString:] + 67
    6   test_SBJSON                         0x0000000100004289 -[SBJson4StreamParser parse:] + 2377
    7   test_SBJSON                         0x0000000100007989 -[SBJson4Parser parse:] + 73
    8   test_SBJSON                         0x0000000100005d0d main + 221
    9   libdyld.dylib                       0x00007fff929ea5ad start + 1
)
libc++abi.dylib: terminating with uncaught exception of type NSException
&lt;/code&gt;
&lt;/pre&gt;
&lt;h4&gt; 4.4 Apple (NS)JSONSerialization&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://developer.apple.com/reference/foundation/nsjsonserialization&quot;&gt;https://developer.apple.com/reference/foundation/nsjsonserialization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;NSJSONSerialization was introduced with iOS 5 and is the standard JSON parser on OS X and iOS since then. It is available in Objective-C, and was rewritten in Swift: &lt;a href=&quot;https://github.com/apple/swift-corelibs-foundation/blob/master/Foundation/NSJSONSerialization.swift&quot;&gt;NSJSONSerialization.swift&lt;/a&gt;. The NS prefix was then &lt;a href=&quot;https://github.com/apple/swift-corelibs-foundation/commit/b914527d4d560602afc90da29254e1f1571672dd#diff-959321c08d3a9f8d385f2daca75a80b2&quot;&gt;dropped&lt;/a&gt; in Swift 3.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Restrictions and Extensions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;JSONSerialization has the following, undocumented restrictions:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;it won't parse big numbers: &lt;code&gt;[123123e100000]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;it won't parse u-escaped invalid codepoints: &lt;code&gt;[&quot;\ud800&quot;]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;JSONSerialization has the following, undocumented extension:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;it does parse trailing commas: &lt;code&gt;[1,]&lt;/code&gt; and &lt;code&gt;{&quot;a&quot;:0,}&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;I find the restriction about invalid codepoints to be especially problematic, especially in such a high-profile parser, because trying to parse uncontrolled contents may result in a parsing failure.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Crash on Serialization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This article is more about JSON parsing than JSON producing, yet I wanted to mention this crash that I found in JSONSerialization when writing &lt;code&gt;Double.nan&lt;/code&gt;. Remember that &lt;code&gt;NaN&lt;/code&gt; does not conform to JSON grammar, so in this case, JSONSerialization should throw an error, but not crash the whole process.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;do {
    let a = [Double.nan]
    let data = try JSONSerialization.data(withJSONObject: a, options: [])
} catch let e {
}

SIGABRT

*** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: 'Invalid number value (NaN) in JSON write'
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;[Update 2016-10-27]&lt;/strong&gt; The original version of this article erroneously said that &lt;code&gt;JSONSerialization.isValidJSONObject([&quot;x&quot;:&quot;x&quot;, &quot;x&quot;:&quot;x&quot;])&lt;/code&gt; would crash because of a bug in the method. &lt;a href=&quot;https://twitter.com/H2CO3_iOS/status/791344089807675392&quot;&gt;@H2CO3_iOS found&lt;/a&gt; that the crash is not related to JSONSerialization but to Swift dictionaries, that just cannot be build with the same key appearing twice.&lt;/p&gt;
&lt;h4&gt; 4.5 Freddy (Swift)&lt;/h4&gt;
&lt;p&gt;Freddy (&lt;a href=&quot;https://github.com/bignerdranch/Freddy&quot;&gt;https://github.com/bignerdranch/Freddy&lt;/a&gt;) is a real JSON Parser written is Swift 3. I say real because several GitHub projects claim to be Swift JSON parsers, but actually use Apple JSONSerialization and just map JSON contents with model objects.&lt;/p&gt;
&lt;p&gt;Freddy is interesting because it is written by a famous organization of Cocoa developers, and does leverage Swift type safety by using a Swift enum to represent the different kind of JSON nodes (Array, Dictionary, Double, Int, String, Bool and Null).&lt;/p&gt;
&lt;p&gt;But, being &lt;a href=&quot;https://www.bignerdranch.com/blog/introducing-freddy-an-open-source-framework-for-parsing-json-in-swift&quot;&gt;released in January 2016&lt;/a&gt;, Freddy is still young, and buggy. My test suite showed that unclosed structures such as &lt;code&gt;[1,&lt;/code&gt; and &lt;code&gt;{&quot;a&quot;:&lt;/code&gt; used to crash the parser, as well as a string with a single space &lt;code&gt;&quot; &quot;&lt;/code&gt;, so I opened &lt;a href=&quot;https://github.com/bignerdranch/Freddy/issues/199&quot;&gt;issue #199&lt;/a&gt; that was fixed within 1 day!&lt;/p&gt;
&lt;p&gt;Additionnally, I found that &lt;code&gt;&quot;0e1&quot;&lt;/code&gt; was incorrectly rejected by the parser, so I opened &lt;a href=&quot;https://github.com/bignerdranch/Freddy/issues/198&quot;&gt;issue #198&lt;/a&gt;, which was also fixed within 1 day.&lt;/p&gt;
&lt;p&gt;However, Freddy does still crash on 2016-10-18 when parsing &lt;code&gt;[&quot;\&lt;/code&gt;. I reported the bug in (&lt;a href=&quot;https://github.com/bignerdranch/Freddy/issues/206&quot;&gt;issue #206&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The following table does summarize the evolution of Freddy's behaviour:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.seriot.ch/json/freddy_results.png&quot;&gt;&lt;img src=&quot;http://www.seriot.ch/json/freddy_results.png&quot; border=&quot;1&quot; width=&quot;600&quot; alt=&quot;Freddy JSON Parser Tests Results&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt; 4.6 Bash JSON.sh&lt;/h4&gt;
&lt;p&gt;I tested &lt;a href=&quot;https://github.com/dominictarr/JSON.sh/&quot;&gt;https://github.com/dominictarr/JSON.sh/&lt;/a&gt; from 2016-08-12.&lt;/p&gt;
&lt;p&gt;This Bash parser relies on a regex to find the control characters, which MUST be backslash-escaped according to RFC 8259. But Bash and JSON don't share the same definition of control characters.&lt;/p&gt;
&lt;p&gt;The regex uses the &lt;code&gt;:cntlr:&lt;/code&gt; syntax to match control characters, which is a shorthand for &lt;code&gt;[\x00-\x1F\x7F]&lt;/code&gt;. But according to JSON grammar, &lt;code&gt;0x7F DEL&lt;/code&gt; is not part of control characters, and may appear unescaped.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt; 00 nul   01 soh   02 stx   03 etx   04 eot   05 enq   06 ack   07 bel
 08 bs    09 ht    0a nl    0b vt    0c np    0d cr    0e so    0f si
 10 dle   11 dc1   12 dc2   13 dc3   14 dc4   15 nak   16 syn   17 etb
 18 can   19 em    1a sub   1b esc   1c fs    1d gs    1e rs    1f us
 20 sp    21  !    22  &quot;    23  #    24  $    25  %    26  &amp;amp;    27  '
 28  (    29  )    2a  *    2b  +    2c  ,    2d  -    2e  .    2f  /
 30  0    31  1    32  2    33  3    34  4    35  5    36  6    37  7
 38  8    39  9    3a  :    3b  ;    3c  &amp;lt;    3d  =    3e  &amp;gt;    3f  ?
 40  @    41  A    42  B    43  C    44  D    45  E    46  F    47  G
 48  H    49  I    4a  J    4b  K    4c  L    4d  M    4e  N    4f  O
 50  P    51  Q    52  R    53  S    54  T    55  U    56  V    57  W
 58  X    59  Y    5a  Z    5b  [    5c  \    5d  ]    5e  ^    5f  _
 60  `    61  a    62  b    63  c    64  d    65  e    66  f    67  g
 68  h    69  i    6a  j    6b  k    6c  l    6d  m    6e  n    6f  o
 70  p    71  q    72  r    73  s    74  t    75  u    76  v    77  w
 78  x    79  y    7a  z    7b  {    7c  |    7d  }    7e  ~    7f del
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;As a consequence, JSON.sh cannot parse &lt;code&gt;[&quot;&lt;u&gt;7F&lt;/u&gt;&quot;]&lt;/code&gt;. I reported this bug in &lt;a href=&quot;https://github.com/dominictarr/JSON.sh/issues/46&quot;&gt;issue #46&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Additionally, JSON.sh does not limit the nesting level, and will crash when parsing 10000 times the open array character &lt;code&gt;[&lt;/code&gt;. I reported this bug in &lt;a href=&quot;https://github.com/dominictarr/JSON.sh/issues/47&quot;&gt;issue #47&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;$ python -c &quot;print('['*100000)&quot; | ./JSON.sh 
./JSON.sh: line 206: 40694 Done                    tokenize
     40695 Segmentation fault: 11  | parse
&lt;/code&gt;
&lt;/pre&gt;
&lt;h4&gt; 4.7 Other Parsers&lt;/h4&gt;
&lt;p&gt;Besides C / Objective-C and Swift, I also tested parsers from other environments. Here is a synthetic review of their extensions and restrictions, with a subset of the &lt;a href=&quot;http://seriot.ch/json/parsing.html&quot;&gt;full tests results&lt;/a&gt;. The goal of this table is to demonstrate that there are no two parsers that agree on what is wrong and what is right.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.seriot.ch/json/other_parsers.png&quot;&gt;&lt;img src=&quot;http://www.seriot.ch/json/other_parsers.png&quot; border=&quot;1&quot; width=&quot;600&quot; alt=&quot;JSON Parsers Differences&quot; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here are the references for the tested parsers:&lt;/p&gt;
&lt;p&gt;Upon popular request, I also added the following Java parsers, which are not shown on this image but that appear in the full results:&lt;/p&gt;
&lt;p&gt;The Python JSON module will parse &lt;code&gt;NaN&lt;/code&gt; or &lt;code&gt;-Infinity&lt;/code&gt; as numbers. While this behaviour can be fixed by setting the &lt;code&gt;parse_constant&lt;/code&gt; options to a function that will raise an Exception as shown below, it's such an uncommon practice that I didn't use it in the tests, and let the parser erroneously parse these number constants.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;def f_parse_constant(o):
    raise ValueError

o = json.loads(data, parse_constant=f_parse_constant)
&lt;/code&gt;
&lt;/pre&gt;
&lt;h4&gt; 4.8 JSON Checker&lt;/h4&gt;
&lt;p&gt;A JSON parser transforms a JSON document into another representation. If the input is invalid JSON, the parser returns an error.&lt;/p&gt;
&lt;p&gt;Some programs don't transform their input, but just tell if the JSON is valid or not. These programs are JSON validators.&lt;/p&gt;
&lt;p&gt;json.org has a such a program, written in C, called JSON_Checker &lt;a href=&quot;http://www.json.org/JSON_checker/&quot;&gt;http://www.json.org/JSON_checker/&lt;/a&gt;, that even comes with a (small) test suite:&lt;/p&gt;
&lt;blockquote readability=&quot;8&quot;&gt;
&lt;p&gt;JSON_Checker is a Pushdown Automaton that very quickly determines if a JSON text is syntactically correct. It could be used to filter inputs to a system, or to verify that the outputs of a system are syntactically correct. It could be adapted to produce a very fast JSON parser.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Even if JSON_Checker is not a formal reference implementation, one could expect JSON_Checker to clarify JSON specifications or at least implement them correctly.&lt;/p&gt;
&lt;p&gt;Unfortunately, JSON_Checker violates the specifications defined on same web site. Indeed, JSON_Checker will parse the following inputs: &lt;code&gt;[1.]&lt;/code&gt;, &lt;code&gt;[0.e1]&lt;/code&gt;, which do not match JSON grammar.&lt;/p&gt;
&lt;p&gt;Moreover, JSON_Checker will reject &lt;code&gt;[0e1]&lt;/code&gt; which is a perfectly valid JSON number. This last bug is even more serious because a whole document can be rejected as long as it contains the number &lt;code&gt;0e1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The elegance of the JSON_Checker implementation as a pushdown automaton doesn't prevent the code from being wrong, but at least the state transition table makes it easy to spot the errors, especially when you add the states onto the schema of what is a number.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://www.seriot.ch/json/json_grammar_number.png&quot; border=&quot;0&quot; width=&quot;600&quot; alt=&quot;JSON Grammar - Numbers&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bug 1: rejection of 0e1&lt;/strong&gt; In the code, the state &lt;code&gt;ZE&lt;/code&gt;, reached after parsing &lt;code&gt;0&lt;/code&gt;, just lacks transitions to &lt;code&gt;E1&lt;/code&gt; by reading &lt;code&gt;e&lt;/code&gt; or &lt;code&gt;E&lt;/code&gt;. We can fix this case by adding the two missing transitions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bug 2: acceptance of [1.]&lt;/strong&gt; In one case, like after &lt;code&gt;0.&lt;/code&gt;, the grammar requires a digit. In the other case, like after &lt;code&gt;0.1&lt;/code&gt; the grammar doesn't. And yet JSON_Checker defines a single state &lt;code&gt;FR&lt;/code&gt; instead of two. We can fix this case by replacing the &lt;code&gt;FR&lt;/code&gt; state in red on the schema with a new state &lt;code&gt;F0&lt;/code&gt; or &lt;code&gt;frac0&lt;/code&gt;. With this fix, the parser will require a digit after &lt;code&gt;1.&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://www.seriot.ch/json/json_checker_state_transition_table.jpg&quot; width=&quot;600&quot; border=&quot;0&quot; alt=&quot;JSON Checker Fix&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Several other parsers (Obj-C TouchJSON, PHP, R rjson, Rust json-rust, Bash JSON.sh, C jsmn and Lua dkjson) will also erroneously parse &lt;code&gt;[1.]&lt;/code&gt;. One may wonder if, at least in some cases, this bug may have spread from JSON_Checker because parser developers and testers used it as a reference, as advised on json.org.&lt;/p&gt;
&lt;p&gt;[Update 2017-11-18] The aforementioned bugs have been fixed, and JSON Checker is now published on &lt;a href=&quot;https://github.com/douglascrockford/JSON-c&quot;&gt;Douglas Crockford's GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt; 4.9 Regex&lt;/h4&gt;
&lt;p&gt;We may wonder if a regex can validate the conformance to JSON grammar of a given input. See for instance this attempt to find the shortest regex on &lt;a href=&quot;http://codegolf.stackexchange.com/questions/474/write-a-json-validator&quot;&gt;StackExchange: Write a JSON Validator&lt;/a&gt;. The problem is that it is very difficult to know if a regex does succeed or not without a solid test suite.&lt;/p&gt;
&lt;p&gt;I found this &lt;a href=&quot;http://stackoverflow.com/questions/2583472/regex-to-validate-json&quot;&gt;Ruby regex to validate JSON&lt;/a&gt; on StackOverflow to be the best one:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;JSON_VALIDATOR_RE = /(
    # define subtypes and build up the json syntax, BNF-grammar-style
    # The {0} is a hack to simply define them as named groups here but not match on them yet
    # I added some atomic grouping to prevent catastrophic backtracking on invalid inputs
    (?&amp;lt;number&amp;gt;  -?(?=[1-9]|0(?!\d))\d+(\.\d+)?([eE][+-]?\d+)?){0}
    (?&amp;lt;boolean&amp;gt; true | false | null ){0}
    (?&amp;lt;string&amp;gt;  &quot; (?&amp;gt;[^&quot;\\\\]* | \\\\ [&quot;\\\\bfnrt\/] | \\\\ u [0-9a-f]{4} )* &quot; ){0}
    (?&amp;lt;array&amp;gt;   \[ (?&amp;gt; \g&amp;lt;json&amp;gt; (?: , \g&amp;lt;json&amp;gt; )* )? \s* \] ){0}
    (?&amp;lt;pair&amp;gt;    \s* \g&amp;lt;string&amp;gt; \s* : \g&amp;lt;json&amp;gt; ){0}
    (?&amp;lt;object&amp;gt;  \{ (?&amp;gt; \g&amp;lt;pair&amp;gt; (?: , \g&amp;lt;pair&amp;gt; )* )? \s* \} ){0}
    (?&amp;lt;json&amp;gt;    \s* (?&amp;gt; \g&amp;lt;number&amp;gt; | \g&amp;lt;boolean&amp;gt; | \g&amp;lt;string&amp;gt; | \g&amp;lt;array&amp;gt; | \g&amp;lt;object&amp;gt; ) \s* ){0}
    )
    \A \g&amp;lt;json&amp;gt; \Z
    /uix
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Yet, it fails to parse valid JSON, such as:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;u-escaped codepoints, including valid ones: &lt;code&gt;[&quot;\u002c&quot;]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;backslash-escaped backslash: &lt;code&gt;[&quot;\\a&quot;]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Also, it does parse the following extensions, which is just a bug for a JSON validator:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;a capitalized True: &lt;code&gt;[True]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;an unescaped control character: &lt;code&gt;[&quot;&lt;u&gt;09&lt;/u&gt;&quot;]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3&gt; 5. Parsing Contents&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://tools.ietf.org/html/rfc8259#section-9&quot;&gt;RFC 8259 Section 9&lt;/a&gt; says:&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;A JSON parser transforms a JSON text into another representation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;All of the above testing architecture will only tell if a parser would parse a JSON document or not, but doesn't say anything about the representation of the resulting contents.&lt;/p&gt;
&lt;p&gt;For example, a parser may parse the u-escaped invalid Unicode character (&lt;code&gt;&quot;\uDEAD&quot;&lt;/code&gt;) without error, but what will the result be like? a replacement character, or something else, who knows? RFC 8259 is silent about it.&lt;/p&gt;
&lt;p&gt;Similarily, extreme numbers such as &lt;code&gt;0.00000000000000000000001&lt;/code&gt; or &lt;code&gt;-0&lt;/code&gt; can be parsed, but what should the result be? RFC 8259 doesn't make a distinction between integers and doubles, or zero and -zero. It doesn't even say if numbers can be converted into strings or not.&lt;/p&gt;
&lt;p&gt;And what about objects with the same keys (&lt;code&gt;{&quot;a&quot;:1,&quot;a&quot;:2}&lt;/code&gt;)? Or same keys and same values (&lt;code&gt;{&quot;a&quot;:1,&quot;a&quot;:1}&lt;/code&gt;)? And how should a parser compare object keys?? Should it use binary comparison or a Unicode normal form such as NFC? Here again, RFC is silent&lt;/p&gt;
&lt;p&gt;In all these cases, parsers are free to output whatever they want, leading to interoperability issues (think of what could go wrong when you decide to change your usual JSON parser with another one).&lt;/p&gt;
&lt;p&gt;With that in mind, let's create tests for which the representation after parsing is not clearly defined. These tests serve only to document how parsers output may differ.&lt;/p&gt;
&lt;p&gt;Contrary to the parsing tests, these tests are hard to automate. Instead, the results shown here were observed via log statements and/or debuggers.&lt;/p&gt;
&lt;p&gt;Below is an inexhaustive list of some striking differences between resulting representations after parsing. All results are shown in appendix &quot;&lt;a href=&quot;http://seriot.ch/json/transform.html&quot;&gt;Parsing Contents&lt;/a&gt;&quot;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Numbers&lt;/strong&gt;&lt;/p&gt;
&lt;ul readability=&quot;5.5&quot;&gt;&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;code&gt;1.000000000000000005&lt;/code&gt; is generally converted into the float &lt;code&gt;1.0&lt;/code&gt;, but Rust 1.12.0 / json 0.10.2 will keep the original precision and use the number &lt;code&gt;1.000000000000000005&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;&lt;code&gt;1E-999&lt;/code&gt; is generally converted into float or double &lt;code&gt;0.0&lt;/code&gt;, but Swift Freddy yields the string &lt;code&gt;&quot;1E-999&quot;&lt;/code&gt;. Swift Apple JSONSerializattion and Obj-C JSONKit will simply refuse to parse it and return an error.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;code&gt;10000000000000000999&lt;/code&gt; may be converted into a double (Swift Apple JSONSerialization), an unsigned long long (Objective-C JSONKit) or a string (Swift Freddy). It is to be noted that C cJSON will parse this number as a double, but loses precision in the process, resulting in a new number &lt;code&gt;10000000000000002048&lt;/code&gt; (note the last four digits).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Objects&lt;/strong&gt;&lt;/p&gt;
&lt;ul readability=&quot;18.5&quot;&gt;&lt;li readability=&quot;4&quot;&gt;
&lt;p&gt;In &lt;code&gt;{&quot;&lt;u&gt;C3A9&lt;/u&gt;:&quot;NFC&quot;,&quot;&lt;u&gt;65CC81&lt;/u&gt;&quot;:&quot;NFD&quot;}&lt;/code&gt; keys are NFC and NFD representations of &quot;é&quot;. Most parsers will yield the two keys, except Swift parsers Apple JSONSerialization and Freddy, where dictionaries first normalize keys before testing them for equality.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;16&quot;&gt;
&lt;p&gt;&lt;code&gt;{&quot;a&quot;:1,&quot;a&quot;:2}&lt;/code&gt; does generally result in &lt;code&gt;{&quot;a&quot;:2}&lt;/code&gt; (Freddy, SBJSON, Go, Python, JavaScript, Ruby, Rust, Lua dksjon), but may also result in &lt;code&gt;{&quot;a&quot;:1}&lt;/code&gt; (Obj-C Apple NSJSONSerialization, Swift Apple JSONSerialization, Swift Freddy), or &lt;code&gt;{&quot;a&quot;:1,&quot;a&quot;:2}&lt;/code&gt; (cJSON, R, Lua JSON).&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;4&quot;&gt;
&lt;p&gt;&lt;code&gt;{&quot;a&quot;:1,&quot;a&quot;:1}&lt;/code&gt; does generally result in &lt;code&gt;{&quot;a&quot;:1}&lt;/code&gt;, but is parsed as &lt;code&gt;{&quot;a&quot;:1,&quot;a&quot;:1}&lt;/code&gt; in cJSON, R and Lua JSON.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;7&quot;&gt;
&lt;p&gt;&lt;code&gt;{&quot;a&quot;:0,&quot;a&quot;:-0}&lt;/code&gt; is generally parsed as &lt;code&gt;{&quot;a&quot;:0}&lt;/code&gt;, but can also be parsed as &lt;code&gt;{&quot;a&quot;:-0}&lt;/code&gt; (Obj-C JSONKit, Go, JavaScript, Lua) or even &lt;code&gt;{&quot;a&quot;:0, &quot;a&quot;:0}&lt;/code&gt; (cJSON, R).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Strings&lt;/strong&gt;&lt;/p&gt;
&lt;ul readability=&quot;12.612903225806&quot;&gt;&lt;li readability=&quot;4&quot;&gt;
&lt;p&gt;&lt;code&gt;[&quot;A\u0000B&quot;]&lt;/code&gt; contains the u-escaped form of the &lt;code&gt;0x00 NUL&lt;/code&gt; character, which is likely to cause problems in C-based JSON parsers. Most parsers handle this payload gracefully, but JSONKit and cJSON won't parse it. Interestingly, Freddy yields only &lt;code&gt;[&quot;A&quot;]&lt;/code&gt; (the string stop after unescaping byte &lt;code&gt;0x00&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;code&gt;[&quot;\uD800&quot;]&lt;/code&gt; is the u-escaped form of &lt;code&gt;U+D800&lt;/code&gt;, an invalid lone UTF-16 surrogate. Many parsers will fail and return an error, despite the string being perfectly valid according to JSON grammar. Python leaves the string untouched and yields &lt;code&gt;[&quot;\uD800&quot;]&lt;/code&gt;. Go and JavaScript replace the offending chacracter with &quot;�&quot; &lt;code&gt;U+FFFD REPLACEMENT CHARACTER&lt;/code&gt; &lt;code&gt;[&quot;&lt;u&gt;EFBFBD&lt;/u&gt;&quot;]&lt;/code&gt;, R rjson and Lua dkjson simply translate the codepoint into its UTF-8 representation &lt;code&gt;[&quot;&lt;u&gt;EDA080&lt;/u&gt;&quot;]&lt;/code&gt;. R jsonlite and Lua JSON 20160728.17 replace the offending codepoint with a question mark &lt;code&gt;[&quot;?&quot;]&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;9.2105263157895&quot;&gt;
&lt;p&gt;&lt;code&gt;[&quot;EDA080&quot;]&lt;/code&gt; is the non-escaped, UTF-8 form or &lt;code&gt;U+D800&lt;/code&gt;, the invalid lone UTF-16 surrogate discussed in previous point. This string is not valid UTF-8 and should be rejected (see &lt;a href=&quot;http://seriot.ch/parsing_json.php#25&quot;&gt;section 2.5 Strings - Raw non-Unicode Characters&lt;/a&gt;). In practice however, several parsers leave the string untouched &lt;code&gt;[&quot;EDA080&quot;]&lt;/code&gt; such as cJSON, R rjson and jsonlite, Lua JSON, Lua dkjson and Ruby. Go and Javacript yield &lt;code&gt;[&quot;&lt;u&gt;EFBFBDEFBFBDEFBFBD&lt;/u&gt;&quot;]&lt;/code&gt; that is three replacement characters (one per byte). Interestingly, Python 2 converts the sequence into its unicode-escaped form &lt;code&gt;[&quot;\ud800&quot;]&lt;/code&gt;, while Python 3 thows a &lt;code&gt;UnicodeDecodeError&lt;/code&gt; exception.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;&lt;code&gt;[&quot;\uD800\uD800&quot;]&lt;/code&gt; makes some parsers go nuts. R jsonlite yields &lt;code&gt;[&quot;\U00010000&quot;]&lt;/code&gt;, while Ruby parser yields &lt;code&gt;[&quot;&lt;u&gt;F0908080&lt;/u&gt;&quot;]&lt;/code&gt;. I still don't get where this value comes from.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;[Update 2017-11-18]&lt;/strong&gt; A &lt;a href=&quot;https://justi.cz/security/2017/11/14/couchdb-rce-npm.html&quot;&gt;RCE vulnerability was found in CouchDB&lt;/a&gt; because two JSON parsers handle duplicate key differently. The same JSON object, when parsed in JavaScript, contains &lt;code&gt;&quot;roles&quot;: []'&lt;/code&gt;, but when parsed in Erlang it contains &lt;code&gt;&quot;roles&quot;: [&quot;_admin&quot;]&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt; 6. STJSON&lt;/h3&gt;
&lt;p&gt;STJSON is a Swift 3, 600+ lines JSON parser I wrote to see what it took to consider all pitfalls and pass all tests.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/nst/STJSON&quot;&gt;https://github.com/nst/STJSON&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;STJSON API is very simple:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;var p = STJSONParser(data: data)

do {
    let o = try p.parse()
    print(o)
} catch let e {
    print(e)
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;STJSON can be instantiated with additional parameters:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;var p = STJSON(data:data,
               maxParserDepth:1024,
               options:[.useUnicodeReplacementCharacter])
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In fact, there is only one test where STJSON fails: &lt;code&gt;y_string_utf16.json&lt;/code&gt;. This is because, as in nearly all other parsers, STJSON does not support non UTF-8 encodings, even though it should not be very difficult to add, and I may do so in the future if needed. At least, STJSON does raise appropriate errors when a file starts with a UTF-16 or UTF-32 byte order mark.&lt;/p&gt;
&lt;h3&gt; 7. Conclusion&lt;/h3&gt;
&lt;p&gt;In conclusion, JSON is not a data format you can rely on blindly. I've demonstrated this by showing that the standard definition is spread out over at least seven different documents (&lt;a href=&quot;http://seriot.ch/parsing_json.php#1&quot;&gt;section 1&lt;/a&gt;), that the latest and most complete document, RFC-8259, is imprecise and contradictory (&lt;a href=&quot;http://seriot.ch/parsing_json.php#2&quot;&gt;section 2&lt;/a&gt;), and by crafting test files that out of over 30 parsers, no two parsers parsed the same set of documents the same way (&lt;a href=&quot;http://seriot.ch/parsing_json.php#4&quot;&gt;section 4&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In the process of inspecting parser results, I also discovered that json_checker.c from json.org did reject valid JSON &lt;code&gt;[0e1]&lt;/code&gt; (&lt;a href=&quot;http://seriot.ch/json.php#424&quot;&gt;section 4.24&lt;/a&gt;), which certainly doesn't help users to know what's right or wrong. In a general way, many parsers authors like to brag about how right is their parsers (including myself), but there's no way to assess their quality since references are debatable and existing test suites are generally poor.&lt;/p&gt;
&lt;p&gt;So, I wrote yet another JSON parser (&lt;a href=&quot;http://seriot.ch/json.php#6&quot;&gt;section 6&lt;/a&gt;) which will parse or reject JSON document according to my understanding of RFC 7159. Feel free to comment, open issues and pull requests.&lt;/p&gt;
&lt;p&gt;This work may be continued by:&lt;/p&gt;
&lt;ul readability=&quot;5.7209165687427&quot;&gt;&lt;li readability=&quot;0&quot;&gt;
&lt;p&gt;Documenting the behaviour of &lt;strong&gt;more parsers&lt;/strong&gt;, especially parsers that run in non-Apple environments.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1.8103975535168&quot;&gt;
&lt;p&gt;Investigating &lt;strong&gt;JSON generation&lt;/strong&gt;. I extensively assessed what parsers do or do not parse. (&lt;a href=&quot;http://seriot.ch/parsing_json.php#4&quot;&gt;section 4&lt;/a&gt;). I briefly assessed the contents that parsers yield when the parsing is successful (&lt;a href=&quot;http://seriot.ch/json.php#5&quot;&gt;section 5&lt;/a&gt;). I'm pretty sure that several parsers do generate grammatically invalid JSON or even crash in some circumstances (see &lt;a href=&quot;http://seriot.ch/parsing_json.php#421&quot;&gt;Section 4.2.1&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-1&quot;&gt;
&lt;p&gt;Investigating differences in the way &lt;strong&gt;JSON mappers&lt;/strong&gt; maps JSON contents to model objects.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;-0.84883720930233&quot;&gt;
&lt;p&gt;&lt;strong&gt;Finding exploits&lt;/strong&gt; in existing software stacks (check out my &lt;a href=&quot;http://seriot.ch/resources/talks_papers/20141106_asfws_unicode_hacks.pdf&quot;&gt;Unicode Hacks&lt;/a&gt; presentation)&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;3.0756972111554&quot;&gt;
&lt;p&gt;Investigating potential interoperability issues in &lt;strong&gt;other serialization formats&lt;/strong&gt; such as YAML, &lt;a href=&quot;http://bsonspec.org/&quot;&gt;BSON&lt;/a&gt; or &lt;a href=&quot;https://developers.google.com/protocol-buffers/&quot;&gt;ProtoBuf&lt;/a&gt;, which may be a potential successor to JSON. Indeed, Apple already has a Swift implementation &lt;a href=&quot;https://github.com/apple/swift-protobuf-plugin&quot;&gt;https://github.com/apple/swift-protobuf-plugin&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;As a final word, I keep on wondering why &quot;fragile&quot; formats such as HTML, CSS and JSON, or &quot;dangerous&quot; languages such as PHP or JavaScript became so immensely popular. This is probably because they are easy to start with by tweaking contents in a text editor, because of too liberal parsers or interpreters, and seemingly simple specifications. But sometimes, simple specifications just mean hidden complexity.&lt;/p&gt;
&lt;h3&gt; 8. Appendix&lt;/h3&gt;
&lt;ol&gt;&lt;li&gt;Parsing Results &lt;a href=&quot;http://seriot.ch/json/parsing.html&quot;&gt;http://seriot.ch/json/parsing.html&lt;/a&gt;, generated automatically for &lt;a href=&quot;http://seriot.ch/parsing_json.php#4&quot;&gt;section 4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tranform Results &lt;a href=&quot;http://seriot.ch/json/transform.html&quot;&gt;http://seriot.ch/json/transform.html&lt;/a&gt;, created manually for &lt;a href=&quot;http://seriot.ch/parsing_json.php#6&quot;&gt;section 6&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;JSONTestSuite &lt;a href=&quot;https://github.com/nst/JSONTestSuite&quot;&gt;https://github.com/nst/JSONTestSuite&lt;/a&gt;, contains all tests and code&lt;/li&gt;
&lt;li&gt;STJSON &lt;a href=&quot;https://github.com/nst/STJSON&quot;&gt;https://github.com/nst/STJSON&lt;/a&gt;, contains my Swift 3 JSON parser&lt;/li&gt;
&lt;/ol&gt;&lt;h3&gt;Acknowledgments&lt;/h3&gt;
&lt;p&gt;Many thanks to @Reversity, GEndignoux, @ccorsano, @BalestraPatrick and @iPlop.&lt;/p&gt;

</description>
<pubDate>Sun, 22 Apr 2018 16:58:16 +0000</pubDate>
<dc:creator>moks</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://seriot.ch/parsing_json.php</dc:identifier>
</item>
<item>
<title>Gitlab 10.7 Released</title>
<link>https://about.gitlab.com/2018/04/22/gitlab-10-7-released/</link>
<guid isPermaLink="true" >https://about.gitlab.com/2018/04/22/gitlab-10-7-released/</guid>
<description>&lt;div readability=&quot;134.73411002804&quot;&gt;
&lt;div readability=&quot;22.145234493192&quot;&gt;

&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;With this release, we continue to iterate on our &lt;a href=&quot;https://about.gitlab.com/direction/#portfolio-management-and-issue-management&quot;&gt;portfolio management&lt;/a&gt; feature of Epics. Just like with issues, you can now comment on epics, and even start standalone, threaded discussions in an epic, just like in issues and merge requests. This allows you to have a conversation with your teammates, in an epic directly, at a higher abstraction level, before necessarily drilling down in an issue, or even creating one.&lt;/p&gt;
&lt;p&gt;This new feature is also supported in &lt;a href=&quot;https://docs.gitlab.com/ee/api/discussions.html&quot;&gt;the API&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Email notifications and todos are not yet supported in epics, and &lt;a href=&quot;https://gitlab.com/groups/gitlab-org/-/epics/148&quot;&gt;we are working on them right now&lt;/a&gt;.&lt;/p&gt;
&lt;img alt=&quot;Comments in epics&quot; class=&quot;shadow&quot; src=&quot;https://about.gitlab.com/images/10_7/epic-comment-thread.png&quot;/&gt;&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/user/group/epics/#comment-or-start-a-discussion&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Epics&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;hidden-lg&quot; readability=&quot;14.193548387097&quot;&gt;
&lt;h3 id=&quot;custom-additional-text-for-all-emails&quot;&gt; Custom additional text for all emails&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Often, organizations need to add a disclaimer or other related text to all email communications, for various operational or compliance requirements.&lt;/p&gt;
&lt;p&gt;With this release, we’ve added this exact feature. GitLab admins can now go into email settings and paste in any custom text. That text will then appear at the bottom of all emails sent by GitLab.&lt;/p&gt;
&lt;img alt=&quot;Custom additional text for all emails&quot; class=&quot;shadow&quot; src=&quot;https://about.gitlab.com/images/10_7/custom-text-emails.png&quot;/&gt;&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/user/admin_area/settings/email.html&quot; target=&quot;_blank&quot;&gt;Read through the documentation on custom text for emails&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;19.899772209567&quot;&gt;
&lt;h3 id=&quot;subgroup-issues-in-group-issue-board&quot;&gt; Subgroup issues in Group Issue Board&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Group issue boards are a powerful feature to help you manage issues across multiple projects across a single group, all in one interface. This is helpful for teams where their work might come from multiple different code repositories (and thus GitLab projects).&lt;/p&gt;
&lt;p&gt;Prior to this release, the issues in a group issue board only came from immediate child projects of that one group. With this release, all issues in projects in all subgroups of the current group also appear in that one group board. So if your work is structured in a group and project hierarchy with multiple levels reflecting your organization or software product, then this update will extend that hierarchy to the group issue board now, giving you more fine-grained control.&lt;/p&gt;
&lt;img alt=&quot;Subgroup issues in Group Issue Board&quot; class=&quot;shadow&quot; src=&quot;https://about.gitlab.com/images/10_7/board-subgroup-issues.png&quot;/&gt;&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/user/project/issue_board.html&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Issue Boards&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;hidden-lg&quot; readability=&quot;30.920741989882&quot;&gt;
&lt;h3 id=&quot;assigning-and-filtering-by-subgroup-labels&quot;&gt; Assigning and filtering by subgroup labels&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Subgrouping is a powerful feature in GitLab, and we want to extend that functionality to how labels are applied in GitLab. With this release, we’ve made it extremely flexible to apply group labels to issues and merge requests at various subgroup levels.&lt;/p&gt;
&lt;p&gt;In particular, for a given issue or merge request, you can now apply any group label from that object’s ancestor groups. This flexibility means that you can create a label at a given group level, and all objects in its subgroups will be able to use it.&lt;/p&gt;
&lt;p&gt;Since in group issue lists and group merge request lists, you can already see objects belonging to subgroups, we’ve now also made it possible to filter on those lists by group labels that belong to both ancestor and descendant groups of the given group, since all those objects can have those labels now. In other words, GitLab gives you the power and flexibility to filter by any possible label assignable to those objects.&lt;/p&gt;
&lt;p&gt;This filtering capability is also possible in group issue boards for both the filter bar and the board config.&lt;/p&gt;
&lt;img alt=&quot;Assigning and filtering by subgroup labels&quot; src=&quot;https://about.gitlab.com/images/10_7/ancestor-descendent-group-labels.png&quot;/&gt;&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/user/project/labels.html&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Labels&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;12.517730496454&quot;&gt;
&lt;h3 id=&quot;project-badges&quot;&gt; Project Badges&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Many projects use badges, such as the &lt;a href=&quot;https://docs.gitlab.com/ee/user/project/pipelines/settings.html#pipeline-badges&quot;&gt;GitLab CI/CD&lt;/a&gt; and &lt;a href=&quot;https://shields.io&quot;&gt;shields.io&lt;/a&gt; to reflect status of build and code quality. Typically these are added to the project README.&lt;/p&gt;
&lt;p&gt;Now badges are a first-class citizen and can be displayed prominently below the project description, and can be templated at the group level too.&lt;/p&gt;
&lt;img alt=&quot;Project Badges&quot; class=&quot;shadow&quot; src=&quot;https://about.gitlab.com/images/10_7/project_badges.png&quot;/&gt;&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/user/project/badges.html&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Project Badges&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;hidden-lg&quot; readability=&quot;18.751043115438&quot;&gt;
&lt;h3 id=&quot;protected-branch-unprotect-permissions&quot;&gt; Protected branch unprotect permissions&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Protected branches allow push and merge permissions to be restricted by branch, like preventing pushes directly to &lt;code&gt;master&lt;/code&gt;, but any Master can bypass these rules by unprotecting the branch. The new unprotect restriction can be used to limit which users, groups and roles are allowed to unprotect a branch.&lt;/p&gt;
&lt;p&gt;Unprotect restrictions can only be set using the API in 10.7, but will be &lt;a href=&quot;https://gitlab.com/gitlab-org/gitlab-ee/issues/5496&quot;&gt;available in the interface&lt;/a&gt; in an upcoming release. The admin access level (&lt;code&gt;60&lt;/code&gt;) may be removed in a future release, as we are currently evaluating restrictions to the Owner role as an alternative.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/api/protected_branches.html#protect-repository-branches&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Branch Protection&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;17.411475409836&quot;&gt;
&lt;h3 id=&quot;issue-weight-in-issue-board-card&quot;&gt; Issue weight in Issue Board card&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;You can now view the weight in an issue board card. Previously, when using an issue board, to see the weight of an issue, you’d have to click on the issue, and see the weight in the sidebar that slides in. With this change, you can now see it in the board card itself. This makes it much more easy to quickly scan a board and see the weights of issues, giving you a rough sense of the work required in a given list of issues, which is helpful in methodologies such as &lt;a href=&quot;https://about.gitlab.com/2018/03/05/gitlab-for-agile-software-development/&quot;&gt;Agile&lt;/a&gt;.&lt;/p&gt;
&lt;img alt=&quot;Issue weight in Issue Board card&quot; class=&quot;shadow&quot; src=&quot;https://about.gitlab.com/images/10_7/board-card-issue-weight.png&quot;/&gt;&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/user/project/issue_board.html&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Issue Boards&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;hidden-lg&quot; readability=&quot;18.99043062201&quot;&gt;
&lt;h3 id=&quot;gitlab-plugins&quot;&gt; GitLab Plugins&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Being open source means GitLab can always be improved by anyone, but not all customers want to upstream their changes, or they may want to iterate on them privately first. Up to now, this was only possible by running a fork of GitLab, which is hard to keep up to date.&lt;/p&gt;
&lt;p&gt;Plugins allow you to respond to &lt;a href=&quot;https://about.gitlab.com/2018/04/22/gitlab-10-7-released/http//docs.gitlab.com/ee/system_hooks/system_hooks.html&quot;&gt;GitLab system hooks&lt;/a&gt; with a script stored on the GitLab server, so you can more easily extend GitLab to meet your needs, like automatically configuring custom protected branch rules whenever a new project is created.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/administration/plugins.html&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Plugins&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;20.433815350389&quot;&gt;
&lt;h3 id=&quot;https-git-protocol-always-available-for-cicd-jobs&quot;&gt; HTTP(s) Git protocol always available for CI/CD jobs&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;With GitLab you can use either SSH or HTTP(s) to access your repositories. Sometimes GitLab administrators prefer to block HTTP(s) access for security reasons. For example, blocking HTTP(s) prevents users from disclosing their credentials due to an insecure client setup. However, blocking HTTP(S) also stopped GitLab Runner from cloning the repository, making CI/CD not work as expected.&lt;/p&gt;
&lt;p&gt;Starting with GitLab 10.7, the HTTP(s) protocol will be allowed for clone/fetch requests coming from GitLab Runner, even if the same access is explicitly forbidden for users. This is not susceptible to the same type of inseucre client vulnerability because GitLab Runner always uses OTP tokens that cannot be leveraged to perform attacks.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/user/admin_area/settings/visibility_and_access_controls.html&quot; target=&quot;_blank&quot;&gt;Read through the documentation on visibility and access controls&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;hidden-lg&quot; readability=&quot;15.215648854962&quot;&gt;
&lt;h3 id=&quot;support-for-json-web-token-omniauth&quot;&gt; Support for JSON Web Token OmniAuth&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;GitLab uses OmniAuth to allow users to sign into GitLab using popular services like Twitter and Google, as well as standard authentication solutions like OAuth2. In Gitlab 10.7, support for JSON Web Token (JWT) OmniAuth has been added.&lt;/p&gt;
&lt;p&gt;JSON Web Token is a compact and self-contained way to securely transmit information, and is commonly used for authentication between services.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/administration/auth/jwt.html&quot; target=&quot;_blank&quot;&gt;Read through the documentation on JWT OmniAuth provider&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;15.997350993377&quot;&gt;
&lt;h3 id=&quot;automatic-background-verification-of-geo-replicas&quot;&gt; Automatic background verification of Geo replicas&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Automatic background verification of Geo replicas will now occur when Geo is enabled, to make sure that replicas remain consistent with the primary. This is important when using Geo for Disaster Recovery so that you can confidently fail over to a secondary, knowing that it is a a complete replica of your primary GitLab instance.&lt;/p&gt;
&lt;p&gt;GitLab calculates a checksum for each Git repository using &lt;code&gt;heads&lt;/code&gt; and &lt;code&gt;tags&lt;/code&gt; and verifies that the checksum from the primary matches the checksum on each secondary. Verification will be expanded in upcoming releases to also include &lt;a href=&quot;https://gitlab.com/gitlab-org/gitlab-ee/issues/5591&quot;&gt;uploads&lt;/a&gt; and &lt;a href=&quot;https://gitlab.com/gitlab-org/gitlab-ee/issues/5196&quot;&gt;&lt;code&gt;keep-around&lt;/code&gt; refs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/administration/geo/disaster_recovery/background_verification.html&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Geo background verification&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;hidden-lg&quot; readability=&quot;12.571428571429&quot;&gt;
&lt;h3 id=&quot;group-project-creation-in-starter&quot;&gt; Group project creation in Starter&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;As part of providing additional flexibility around our permission model, this will allow group or server admins to set an option that will give users with Developer role the ability to create projects.&lt;/p&gt;
&lt;p&gt;This feature was previously available in GitLab Premium only.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/user/group/index.html#default-project-creation-level&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Groups&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;13.194103194103&quot;&gt;
&lt;h3 id=&quot;project-exports-include-lfs-objects&quot;&gt; Project exports include LFS objects&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Project exports allow you to move projects between GitLab instances conveniently, including issues, merge requests, labels, wikis and uploads. Project exports now include LFS objects so that repositories with LFS objects can also be transferred using project exports.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/user/project/settings/import_export.html&quot; target=&quot;_blank&quot;&gt;Read through the documentation on project exports&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;hidden-lg&quot; readability=&quot;16.485714285714&quot;&gt;
&lt;h3 id=&quot;dependency-scanning-is-now-an-independent-feature&quot;&gt; Dependency Scanning is now an independent feature&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Before this release, security checks on external dependencies and libraries used by your application were done along with SAST. Even if they are related, we think that they should be clearly identified as two separate features.&lt;/p&gt;
&lt;p&gt;GitLab 10.7 introduces Dependency Scanning as a first-class citizen in the Security reports, giving you information about vulnerable libraries that should be updated. Results will be available both in the merge request and in the pipeline view.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/user/project/merge_requests/#dependency-scanning&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Dependency Scanning&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;16.205882352941&quot;&gt;
&lt;h3 id=&quot;runner-specific-job-timeout&quot;&gt; Runner-specific job timeout&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;GitLab currently defines at the project level how long a CI/CD job can run. If a job execution exceeds this duration, it is automatically stopped and reported as failed.&lt;/p&gt;
&lt;p&gt;GitLab 10.7 adds a new timeout setting on the runner itself, which applies to all jobs it runs. If this value is less than the project-level setting, it will override it. This is particularly helpful for shared runners, in order to prevent potential abuse with a project that has set long timeouts.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/ci/runners/#setting-maximum-job-timeout-for-a-runner&quot; target=&quot;_blank&quot;&gt;Read through the documentation on runner-specific job timeout&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;hidden-lg&quot; readability=&quot;14.828343313373&quot;&gt;
&lt;h3 id=&quot;easily-get-failure-reasons-for-cicd-jobs&quot;&gt; Easily get failure reasons for CI/CD jobs&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;When a CI/CD job fails, users normally want to check what happened and commit a fix to make it succeed as expected. Before this release, they had to go into the job details and look at the logs.&lt;/p&gt;
&lt;p&gt;To make the debugging easier and faster, GitLab 10.7 introduces the failure reason as part of the status badges, make it accessible on mouseover.&lt;/p&gt;
&lt;img alt=&quot;Easily get failure reasons for CI/CD jobs&quot; class=&quot;shadow&quot; src=&quot;https://about.gitlab.com/images/10_7/failure_reason.png&quot;/&gt;&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/ci/pipelines.html#seeing-the-failure-reason-for-jobs&quot; target=&quot;_blank&quot;&gt;Read through the documentation on failure reasons for CI/CD jobs&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;hidden-lg&quot; readability=&quot;13.629173989455&quot;&gt;
&lt;h3 id=&quot;improvements-to-restoring-gitlab-backups&quot;&gt; Improvements to restoring GitLab backups&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;GitLab 10.7 now includes support for restoring to custom nested directories. For example if your registry was located at &lt;code&gt;/var/mypath/gitlab/registry&lt;/code&gt;, the restore will now succeed. Previously the task would try to rename the existing directory to &lt;code&gt;&amp;lt;name&amp;gt;.&amp;lt;timestamp&amp;gt;&lt;/code&gt;, but it would not have permission. Now it will create a &lt;code&gt;tmp&lt;/code&gt; folder in the backup directory, and move any existing files there prior to restoring the backup.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/raketasks/backup_restore.html#restore-for-omnibus-installations&quot; target=&quot;_blank&quot;&gt;Read through the documentation on backup and restore&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;13.982238010657&quot;&gt;
&lt;h3 id=&quot;gitlab-pages-automatic-https-redirect&quot;&gt; GitLab Pages automatic HTTPS redirect&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;GitLab Pages can provide static websites via HTTP or HTTPS protocols. HTTPS is normally preferred since it encrypts all the traffic, protecting the content while it is transferred over the net.&lt;/p&gt;
&lt;p&gt;In the case that both are available, in GitLab 10.7 users can force their projects to redirect HTTP requests on the related HTTPS url to improve security and guarantee no data is transferred in clear text.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://gitlab.com/gitlab-org/gitlab-pages/#https-only-domains&quot; target=&quot;_blank&quot;&gt;Read through the documentation on GitLab Pages automatic HTTPS redirect&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;hidden-lg&quot; readability=&quot;13.882797731569&quot;&gt;
&lt;h3 id=&quot;automatic-renewal-of-gitlab-lets-encrypt-certificate&quot;&gt; Automatic renewal of GitLab Let's Encrypt certificate&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In GitLab 10.5, we made it easy to enable HTTPS for your GitLab instance by &lt;a href=&quot;https://about.gitlab.com/2018/02/22/gitlab-10-5-released/#instant-ssl-with-lets-encrypt-for-gitlab&quot;&gt;integrating with Let’s Encrypt&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With GitLab 10.7, we are making it &lt;em&gt;even easier&lt;/em&gt; by no longer requiring it to be explicitly enabled as well as automating the renewal process. All you need to do to enable HTTPS now is set your &lt;code&gt;external_url&lt;/code&gt; to start with &lt;code&gt;https://&lt;/code&gt;, and that’s it!&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/omnibus/settings/ssl.html#automatic-renewal&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Let's Encrypt automatic renewal&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;hidden-lg&quot; readability=&quot;16.320785597381&quot;&gt;
&lt;h3 id=&quot;improvements-to-the-environment-metrics-dashboard&quot;&gt; Improvements to the environment metrics dashboard&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Summary statistics are now available on the environment metrics dashboard, displaying the average and maximum values of each series within the timespan. For example it is now possible to quickly see the average response time for the past eight hours, to understand the experience most users are seeing.&lt;/p&gt;
&lt;p&gt;Total Pod CPU and Memory consumption are now also displayed, providing insight into the resource usage of a particular environment in the cluster.&lt;/p&gt;
&lt;img alt=&quot;Improvements to the environment metrics dashboard&quot; class=&quot;shadow&quot; src=&quot;https://about.gitlab.com/images/10_7/perf_dashboard.png&quot;/&gt;&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/user/project/integrations/prometheus.html#monitoring-ci-cd-environments&quot; target=&quot;_blank&quot;&gt;Read through the documentation on monitoring environments&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div readability=&quot;7.1688311688312&quot;&gt;
&lt;h3 id=&quot;gitlab-runner-107&quot;&gt; GitLab Runner 10.7&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We’re also releasing GitLab Runner 10.7 today! GitLab Runner is the open source project that is used to run your CI/CD jobs and send the results back to GitLab.&lt;/p&gt;
&lt;h5 id=&quot;most-interesting-changes&quot;&gt;Most interesting changes:&lt;/h5&gt;
&lt;p&gt;List of all changes can be found in GitLab Runner’s &lt;a href=&quot;https://gitlab.com/gitlab-org/gitlab-runner/blob/v10.7.0/CHANGELOG.md&quot;&gt;CHANGELOG&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/runner&quot; target=&quot;_blank&quot;&gt;Read through the documentation on GitLab Runner&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;hidden-lg&quot; readability=&quot;5.9004424778761&quot;&gt;
&lt;h3 id=&quot;omnibus-improvements&quot;&gt; Omnibus improvements&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;&lt;li&gt;GitLab &lt;a href=&quot;https://about.mattermost.com/releases/mattermost-4-8/&quot;&gt;Mattermost 4.8.1&lt;/a&gt; includes several platform improvements, including an iOS endpoint that enables users to upload files larger than 20MB, plus much more.&lt;/li&gt;
&lt;li&gt;The default &lt;code&gt;ssl_ciphers&lt;/code&gt; list for NGINX has been updated, excluding &lt;code&gt;ECDHE-RSA-DES-CBC3-SHA&lt;/code&gt; and &lt;code&gt;DES-CBC3-SHA&lt;/code&gt; to address &lt;a href=&quot;https://nvd.nist.gov/vuln/detail/CVE-2016-2183&quot;&gt;Sweet32&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;redis_exporter has been updated to 0.17.1.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/omnibus/README.html&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Omnibus GitLab&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;&lt;div readability=&quot;73.380364288671&quot;&gt;
&lt;div class=&quot;visible-lg&quot; readability=&quot;14.193548387097&quot;&gt;
&lt;h3 id=&quot;custom-additional-text-for-all-emails&quot;&gt; Custom additional text for all emails&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Often, organizations need to add a disclaimer or other related text to all email communications, for various operational or compliance requirements.&lt;/p&gt;
&lt;p&gt;With this release, we’ve added this exact feature. GitLab admins can now go into email settings and paste in any custom text. That text will then appear at the bottom of all emails sent by GitLab.&lt;/p&gt;
&lt;img alt=&quot;Custom additional text for all emails&quot; class=&quot;shadow&quot; src=&quot;https://about.gitlab.com/images/10_7/custom-text-emails.png&quot;/&gt;&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/user/admin_area/settings/email.html&quot; target=&quot;_blank&quot;&gt;Read through the documentation on custom text for emails&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;visible-lg&quot; readability=&quot;30.920741989882&quot;&gt;
&lt;h3 id=&quot;assigning-and-filtering-by-subgroup-labels&quot;&gt; Assigning and filtering by subgroup labels&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Subgrouping is a powerful feature in GitLab, and we want to extend that functionality to how labels are applied in GitLab. With this release, we’ve made it extremely flexible to apply group labels to issues and merge requests at various subgroup levels.&lt;/p&gt;
&lt;p&gt;In particular, for a given issue or merge request, you can now apply any group label from that object’s ancestor groups. This flexibility means that you can create a label at a given group level, and all objects in its subgroups will be able to use it.&lt;/p&gt;
&lt;p&gt;Since in group issue lists and group merge request lists, you can already see objects belonging to subgroups, we’ve now also made it possible to filter on those lists by group labels that belong to both ancestor and descendant groups of the given group, since all those objects can have those labels now. In other words, GitLab gives you the power and flexibility to filter by any possible label assignable to those objects.&lt;/p&gt;
&lt;p&gt;This filtering capability is also possible in group issue boards for both the filter bar and the board config.&lt;/p&gt;
&lt;img alt=&quot;Assigning and filtering by subgroup labels&quot; src=&quot;https://about.gitlab.com/images/10_7/ancestor-descendent-group-labels.png&quot;/&gt;&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/user/project/labels.html&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Labels&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;visible-lg&quot; readability=&quot;18.751043115438&quot;&gt;
&lt;h3 id=&quot;protected-branch-unprotect-permissions&quot;&gt; Protected branch unprotect permissions&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Protected branches allow push and merge permissions to be restricted by branch, like preventing pushes directly to &lt;code&gt;master&lt;/code&gt;, but any Master can bypass these rules by unprotecting the branch. The new unprotect restriction can be used to limit which users, groups and roles are allowed to unprotect a branch.&lt;/p&gt;
&lt;p&gt;Unprotect restrictions can only be set using the API in 10.7, but will be &lt;a href=&quot;https://gitlab.com/gitlab-org/gitlab-ee/issues/5496&quot;&gt;available in the interface&lt;/a&gt; in an upcoming release. The admin access level (&lt;code&gt;60&lt;/code&gt;) may be removed in a future release, as we are currently evaluating restrictions to the Owner role as an alternative.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/api/protected_branches.html#protect-repository-branches&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Branch Protection&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;visible-lg&quot; readability=&quot;18.99043062201&quot;&gt;
&lt;h3 id=&quot;gitlab-plugins&quot;&gt; GitLab Plugins&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Being open source means GitLab can always be improved by anyone, but not all customers want to upstream their changes, or they may want to iterate on them privately first. Up to now, this was only possible by running a fork of GitLab, which is hard to keep up to date.&lt;/p&gt;
&lt;p&gt;Plugins allow you to respond to &lt;a href=&quot;https://about.gitlab.com/2018/04/22/gitlab-10-7-released/http//docs.gitlab.com/ee/system_hooks/system_hooks.html&quot;&gt;GitLab system hooks&lt;/a&gt; with a script stored on the GitLab server, so you can more easily extend GitLab to meet your needs, like automatically configuring custom protected branch rules whenever a new project is created.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/administration/plugins.html&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Plugins&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;visible-lg&quot; readability=&quot;15.215648854962&quot;&gt;
&lt;h3 id=&quot;support-for-json-web-token-omniauth&quot;&gt; Support for JSON Web Token OmniAuth&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;GitLab uses OmniAuth to allow users to sign into GitLab using popular services like Twitter and Google, as well as standard authentication solutions like OAuth2. In Gitlab 10.7, support for JSON Web Token (JWT) OmniAuth has been added.&lt;/p&gt;
&lt;p&gt;JSON Web Token is a compact and self-contained way to securely transmit information, and is commonly used for authentication between services.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/administration/auth/jwt.html&quot; target=&quot;_blank&quot;&gt;Read through the documentation on JWT OmniAuth provider&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;visible-lg&quot; readability=&quot;12.571428571429&quot;&gt;
&lt;h3 id=&quot;group-project-creation-in-starter&quot;&gt; Group project creation in Starter&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;As part of providing additional flexibility around our permission model, this will allow group or server admins to set an option that will give users with Developer role the ability to create projects.&lt;/p&gt;
&lt;p&gt;This feature was previously available in GitLab Premium only.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/user/group/index.html#default-project-creation-level&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Groups&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;visible-lg&quot; readability=&quot;16.485714285714&quot;&gt;
&lt;h3 id=&quot;dependency-scanning-is-now-an-independent-feature&quot;&gt; Dependency Scanning is now an independent feature&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Before this release, security checks on external dependencies and libraries used by your application were done along with SAST. Even if they are related, we think that they should be clearly identified as two separate features.&lt;/p&gt;
&lt;p&gt;GitLab 10.7 introduces Dependency Scanning as a first-class citizen in the Security reports, giving you information about vulnerable libraries that should be updated. Results will be available both in the merge request and in the pipeline view.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/user/project/merge_requests/#dependency-scanning&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Dependency Scanning&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;visible-lg&quot; readability=&quot;14.828343313373&quot;&gt;
&lt;h3 id=&quot;easily-get-failure-reasons-for-cicd-jobs&quot;&gt; Easily get failure reasons for CI/CD jobs&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;When a CI/CD job fails, users normally want to check what happened and commit a fix to make it succeed as expected. Before this release, they had to go into the job details and look at the logs.&lt;/p&gt;
&lt;p&gt;To make the debugging easier and faster, GitLab 10.7 introduces the failure reason as part of the status badges, make it accessible on mouseover.&lt;/p&gt;
&lt;img alt=&quot;Easily get failure reasons for CI/CD jobs&quot; class=&quot;shadow&quot; src=&quot;https://about.gitlab.com/images/10_7/failure_reason.png&quot;/&gt;&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/ci/pipelines.html#seeing-the-failure-reason-for-jobs&quot; target=&quot;_blank&quot;&gt;Read through the documentation on failure reasons for CI/CD jobs&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;visible-lg&quot; readability=&quot;13.629173989455&quot;&gt;
&lt;h3 id=&quot;improvements-to-restoring-gitlab-backups&quot;&gt; Improvements to restoring GitLab backups&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;GitLab 10.7 now includes support for restoring to custom nested directories. For example if your registry was located at &lt;code&gt;/var/mypath/gitlab/registry&lt;/code&gt;, the restore will now succeed. Previously the task would try to rename the existing directory to &lt;code&gt;&amp;lt;name&amp;gt;.&amp;lt;timestamp&amp;gt;&lt;/code&gt;, but it would not have permission. Now it will create a &lt;code&gt;tmp&lt;/code&gt; folder in the backup directory, and move any existing files there prior to restoring the backup.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/raketasks/backup_restore.html#restore-for-omnibus-installations&quot; target=&quot;_blank&quot;&gt;Read through the documentation on backup and restore&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;visible-lg&quot; readability=&quot;13.882797731569&quot;&gt;
&lt;h3 id=&quot;automatic-renewal-of-gitlab-lets-encrypt-certificate&quot;&gt; Automatic renewal of GitLab Let's Encrypt certificate&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In GitLab 10.5, we made it easy to enable HTTPS for your GitLab instance by &lt;a href=&quot;https://about.gitlab.com/2018/02/22/gitlab-10-5-released/#instant-ssl-with-lets-encrypt-for-gitlab&quot;&gt;integrating with Let’s Encrypt&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With GitLab 10.7, we are making it &lt;em&gt;even easier&lt;/em&gt; by no longer requiring it to be explicitly enabled as well as automating the renewal process. All you need to do to enable HTTPS now is set your &lt;code&gt;external_url&lt;/code&gt; to start with &lt;code&gt;https://&lt;/code&gt;, and that’s it!&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/omnibus/settings/ssl.html#automatic-renewal&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Let's Encrypt automatic renewal&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;visible-lg&quot; readability=&quot;16.320785597381&quot;&gt;
&lt;h3 id=&quot;improvements-to-the-environment-metrics-dashboard&quot;&gt; Improvements to the environment metrics dashboard&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Summary statistics are now available on the environment metrics dashboard, displaying the average and maximum values of each series within the timespan. For example it is now possible to quickly see the average response time for the past eight hours, to understand the experience most users are seeing.&lt;/p&gt;
&lt;p&gt;Total Pod CPU and Memory consumption are now also displayed, providing insight into the resource usage of a particular environment in the cluster.&lt;/p&gt;
&lt;img alt=&quot;Improvements to the environment metrics dashboard&quot; class=&quot;shadow&quot; src=&quot;https://about.gitlab.com/images/10_7/perf_dashboard.png&quot;/&gt;&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/ee/user/project/integrations/prometheus.html#monitoring-ci-cd-environments&quot; target=&quot;_blank&quot;&gt;Read through the documentation on monitoring environments&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;visible-lg&quot; readability=&quot;5.9004424778761&quot;&gt;
&lt;h3 id=&quot;omnibus-improvements&quot;&gt; Omnibus improvements&lt;/h3&gt;
&lt;div class=&quot;badge-container dark&quot;&gt;
&lt;div class=&quot;top-row&quot;&gt;
&lt;p&gt;CORE&lt;/p&gt;

&lt;p&gt;STARTER&lt;/p&gt;

&lt;p&gt;PREMIUM&lt;/p&gt;

&lt;p&gt;ULTIMATE&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;bottom-row&quot;&gt;
&lt;p&gt;FREE&lt;/p&gt;

&lt;p&gt;BRONZE&lt;/p&gt;

&lt;p&gt;SILVER&lt;/p&gt;

&lt;p&gt;GOLD&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;&lt;li&gt;GitLab &lt;a href=&quot;https://about.mattermost.com/releases/mattermost-4-8/&quot;&gt;Mattermost 4.8.1&lt;/a&gt; includes several platform improvements, including an iOS endpoint that enables users to upload files larger than 20MB, plus much more.&lt;/li&gt;
&lt;li&gt;The default &lt;code&gt;ssl_ciphers&lt;/code&gt; list for NGINX has been updated, excluding &lt;code&gt;ECDHE-RSA-DES-CBC3-SHA&lt;/code&gt; and &lt;code&gt;DES-CBC3-SHA&lt;/code&gt; to address &lt;a href=&quot;https://nvd.nist.gov/vuln/detail/CVE-2016-2183&quot;&gt;Sweet32&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;redis_exporter has been updated to 0.17.1.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;https://docs.gitlab.com/omnibus/README.html&quot; target=&quot;_blank&quot;&gt;Read through the documentation on Omnibus GitLab&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
<pubDate>Sun, 22 Apr 2018 16:48:09 +0000</pubDate>
<dc:creator>jbergstroem</dc:creator>
<og:title>GitLab 10.7 released with open source Web IDE and SAST for Go and C/C++!</og:title>
<og:type>article</og:type>
<og:url>https://about.gitlab.com/2018/04/22/gitlab-10-7-released/</og:url>
<og:image>https://about.gitlab.com/images/tweets/gitlab-10-7-released.png</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://about.gitlab.com/2018/04/22/gitlab-10-7-released/</dc:identifier>
</item>
<item>
<title>After Facebook scrutiny, is Google next?</title>
<link>https://phys.org/news/2018-04-facebook-scrutiny-google.html</link>
<guid isPermaLink="true" >https://phys.org/news/2018-04-facebook-scrutiny-google.html</guid>
<description>&lt;div class=&quot;first-block&quot; readability=&quot;25&quot;&gt;
&lt;div class=&quot;image-block-ins&quot;&gt;&lt;img src=&quot;https://3c1703fe8d.site.internapcdn.net/newman/csz/news/800/2018/afterfaceboo.jpg&quot; alt=&quot;After Facebook scrutiny, is Google next?&quot;/&gt;&lt;/div&gt;
This photo combo of images shows, clockwise, from upper left: a Google sign, the Twitter app, YouTube TV logo and the Facebook app. Facebook has taken the lion's share of scrutiny from Congress and the media for its data-handling practices that allow savvy marketers and political agents to target specific audiences, but it's far from alone. YouTube, Google and Twitter also have giant platforms awash in more videos, posts and pages than any set of human eyes could ever check. Their methods of serving ads against this sea of content may come under the microscope next. (AP Photo)
&lt;p&gt;Facebook has taken the lion's share of scrutiny from Congress and the media about data-handling practices that allow savvy marketers and political agents to target specific audiences, but it's far from alone. YouTube, Google and Twitter also have giant platforms awash in more videos, posts and pages than any set of human eyes could ever check. Their methods of serving ads against this sea of content may come under the microscope next.&lt;/p&gt;
&lt;/div&gt;&lt;section class=&quot;article-banner first-banner&quot;&gt;
&lt;/section&gt;&lt;p&gt;Advertising and privacy experts say a backlash is inevitable against a &quot;Wild West&quot; internet that has escaped scrutiny before. There continues to be a steady barrage of new examples where unsuspecting advertisers had their brands associated with extremist content on major platforms.&lt;/p&gt;
&lt;p&gt;In the latest discovery, CNN reported that it found more than 300 retail brands, government agencies and technology companies had their ads run on YouTube channels that promoted white nationalists, Nazis, conspiracy theories and North Korean propaganda.&lt;/p&gt;
&lt;p&gt;Child advocates have also raised alarms about the ease with which smartphone-equipped children are exposed to inappropriate videos and deceptive advertising.&lt;/p&gt;
&lt;p&gt;&quot;I absolutely think that Google is next and long overdue,&quot; said Josh Golin, director of the Boston-based Campaign for a Commercial-Free Childhood, which asked the Federal Trade Commission to investigate Google-owned YouTube's advertising and data collection practices earlier this month.&lt;/p&gt;
&lt;p&gt;YouTube has repeatedly outlined the ways it attempts to flag and delete hateful, violent, sexually explicit or harmful videos, but its screening efforts have often missed the mark.&lt;/p&gt;
&lt;p&gt;It also allows advertisers avoid running ads on sensitive content—like news or politics—that don't violate YouTube guidelines but don't fit with a company's brand. Those methods appear to have failed.&lt;/p&gt;
&lt;p&gt;&quot;YouTube has once again failed to correctly filter channels out of our marketing buys,&quot; said a statement Friday from 20th Century Fox Film, which learned that its ads were running on videos posted by a self-described Nazi. YouTube has since deleted the offending channel, but the Hollywood firm says it has unanswered questions about how it happened in the first place.&lt;/p&gt;
&lt;p&gt;&quot;All of our filters were in place in order to ensure that this did not happen,&quot; Fox said, adding it has asked for a refund of any money shared with the &quot;abhorrent channel.&quot;&lt;/p&gt;
&lt;p&gt;YouTube said Friday that it has made &quot;significant changes to how we approach monetization&quot; with &quot;stricter policies, better controls and greater transparency&quot; and said it allows advertisers to exclude certain channels from ads. It also removes ads when it's notified of problems running beside content that doesn't comply with its policies. &quot;We are committed to working with our advertisers and getting this right.&quot;&lt;/p&gt;

&lt;p&gt;So far, just one major advertiser—Baltimore-based retailer Under Armour—had said it had withdrawn its advertising in the wake of the CNN report, though the lull lasted only a few days last week when it was first notified of the problem. After its shoe commercial turned up on a channel known for espousing white nationalist beliefs, Under Armour worked with YouTube to expand its filters to exclude certain topics and keywords.&lt;/p&gt;
&lt;p&gt;On the other hand, Procter &amp;amp; Gamble, which had kept its ads off of YouTube since March 2017, said it had come back to the platform but drastically pared back the channels it would advertise on to under 10,000. It has worked on its own, with third parties, and with YouTube to create its restrictive list.&lt;/p&gt;
&lt;p&gt;That's just a fraction of the some 3 million YouTube channels in the U.S. that accept ads, and is even more stringent than YouTube's &quot;Google Preferred&quot; lineup that focuses on the top most popular 5 percent of videos.&lt;/p&gt;
&lt;p&gt;The CNN report was &quot;an illustration of exactly why we needed to go above and beyond just what YouTube's plans were and why we needed to take more control of where our ads were showing up,&quot; said P&amp;amp;G spokeswoman Tressie Rose.&lt;/p&gt;
&lt;p&gt;The big problem, experts say, is that advertisers lured by the reach and targeting capability of online platforms can mistakenly expect the same standards for decency on network TV will apply online. In the same way, broadcast TV rules that require transparency about political ad buyers are absent on the web.&lt;/p&gt;
&lt;p&gt;&quot;There have always been regulations regarding appropriate conduct in content,&quot; says Robert Passikoff, president of Brand Keys Inc., a New York customer research firm. Regulating content on the internet is one area &quot;that has gotten away from everyone.&quot;&lt;/p&gt;
&lt;p&gt;Also absent from the internet are many of the rules that govern children's programming on television sets. TV networks, for instance, are allowed to air commercial breaks but cannot use kids' characters to advertise products. Such &quot;host-selling&quot; runs rampant on internet services such as YouTube.&lt;/p&gt;
&lt;p&gt;Action to remove ads from inappropriate content is mostly reactive because of lack of upfront control of what gets uploaded, and it generally takes the mass threat of boycott to get advertisers to demand changes, according to BrandSimple consultant Allen Adamson. &quot;The social media backlash is what you're worried about,&quot; he said.&lt;/p&gt;
&lt;p&gt;At the same time, politicians are having trouble keeping up with the changing landscape, evident by how ill-informed many senators and congresspeople appeared during questioning of Facebook CEO Mark Zuckerberg earlier this month.&lt;/p&gt;
&lt;p&gt;&quot;We're in the early stages of trying to figure out what kind of regulation makes sense here,&quot; said Larry Chiagouris, professor of marketing at Pace University in New York. &quot;It's going to take quite some time to sort that out.&quot;&lt;/p&gt;
&lt;p class=&quot;news-relevant&quot; itemscope=&quot;&quot; itemtype=&quot;http://schema.org/WebPage&quot;&gt;&lt;a href=&quot;https://phys.org/news/2018-04-facebook-scrutiny-google.html#&quot; id=&quot;inl-rel-href&quot;&gt;&lt;img class=&quot;toolsicon ic-rel&quot; src=&quot;https://cf3e497594.site.internapcdn.net/tmpl/v5/img/1x1.gif&quot; width=&quot;14&quot; height=&quot;16&quot; alt=&quot;&quot;/&gt;&lt;/a&gt; &lt;strong&gt;Explore further:&lt;/strong&gt; &lt;a itemprop=&quot;relatedLink&quot; href=&quot;https://phys.org/news/2018-01-youtube-toughens-videos-ads.html&quot;&gt;YouTube toughens rules regarding which videos get ads&lt;/a&gt;&lt;/p&gt;
&lt;footer class=&quot;post-floor clearfix&quot; readability=&quot;19.901234567901&quot;&gt;
&lt;div class=&quot;post-rating&quot; id=&quot;rank&quot;&gt;
&lt;div id=&quot;flip-box&quot;&gt;
&lt;div class=&quot;flip&quot;&gt;
&lt;p&gt;&lt;span id=&quot;shares&quot;&gt;370&lt;/span&gt; shares &lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;section class=&quot;copyright&quot; readability=&quot;2&quot;&gt;&lt;p&gt;© 2018 The Associated Press. All rights reserved.&lt;br/&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/footer&gt;</description>
<pubDate>Sun, 22 Apr 2018 16:28:17 +0000</pubDate>
<dc:creator>dnetesn</dc:creator>
<og:title>After Facebook scrutiny, is Google next?</og:title>
<og:description>Facebook has taken the lion's share of scrutiny from Congress and the media about data-handling practices that allow savvy marketers and political agents to target specific audiences, but it's far from alone. YouTube, Google and Twitter also have giant platforms awash in more videos, posts and pages than any set of human eyes could ever check. Their methods of serving ads against this sea of content may come under the microscope next.</og:description>
<og:image>https://3c1703fe8d.site.internapcdn.net/newman/gfx/news/2018/afterfaceboo.jpg</og:image>
<og:type>article</og:type>
<og:url>https://phys.org/news/2018-04-facebook-scrutiny-google.html</og:url>
<dc:format>text/html</dc:format>
<dc:identifier>https://phys.org/news/2018-04-facebook-scrutiny-google.html</dc:identifier>
</item>
<item>
<title>Why Stanislaw Lem’s futurism deserves attention (2015)</title>
<link>http://nautil.us/issue/28/2050/the-book-no-one-read</link>
<guid isPermaLink="true" >http://nautil.us/issue/28/2050/the-book-no-one-read</guid>
<description>&lt;p&gt;&lt;span class=&quot;dropcap&quot;&gt;I&lt;/span&gt; remember well the first time my certainty of a bright future evaporated, when my confidence in the panacea of technological progress was shaken. It was in 2007, on a warm September evening in San Francisco, where I was relaxing in a cheap motel room after two days covering The Singularity Summit, an annual gathering of scientists, technologists, and entrepreneurs discussing the future obsolescence of human beings.&lt;/p&gt;
&lt;p&gt;In math, a “singularity” is a function that takes on an infinite value, usually to the detriment of an equation’s sense and sensibility. In physics, the term usually refers to a region of infinite density and infinitely curved space, something thought to exist inside black holes and at the very beginning of the Big Bang. In the rather different parlance of Silicon Valley, “The Singularity” is an inexorably-approaching event in which humans ride an accelerating wave of technological progress to somehow create superior artificial intellects—intellects which with predictable unpredictability then explosively make further disruptive innovations so powerful and profound that our civilization, our species, and perhaps even our entire planet are rapidly transformed into some scarcely imaginable state. Not long after The Singularity’s arrival, argue its proponents, humanity’s dominion over the Earth will come to an end.&lt;/p&gt;
&lt;p&gt;I had encountered a wide spectrum of thought in and around the conference. Some attendees overflowed with exuberance, awaiting the arrival of machines of loving grace to watch over them in a paradisiacal post-scarcity utopia, while others, more mindful of history, dreaded the possible demons new technologies could unleash. Even the self-professed skeptics in attendance sensed the world was poised on the cusp of some massive technology-driven transition. A typical conversation at the conference would refer at least once to some exotic concept like whole-brain emulation, cognitive enhancement, artificial life, virtual reality, or molecular nanotechnology, and many carried a cynical sheen of eschatological hucksterism: Climb aboard, don’t delay, invest right now, and you, too, may be among the chosen who rise to power from the ashes of the former world!&lt;/p&gt;
&lt;p&gt;Over vegetarian hors d’oeuvres and red wine at a Bay Area villa, I had chatted with the billionaire venture capitalist Peter Thiel, who planned to adopt an “aggressive” strategy for investing in a “positive” Singularity, which would be “the biggest boom ever,” if it doesn’t first “blow up the whole world.” I had talked with the autodidactic artificial-intelligence researcher Eliezer Yudkowsky about his fears that artificial minds might, once created, rapidly destroy the planet. At one point, the inventor-turned-proselytizer  Ray Kurzweil teleconferenced in to discuss, among other things, his plans for becoming transhuman, transcending his own biology to  achieve some sort of  eternal life. Kurzweil  believes this is possible,  even probable, provided he can just live to see  The Singularity’s dawn,  which he has pegged at  sometime in the middle of the 21st century. To this end, he reportedly consumes some 150 vitamin supplements a day.&lt;/p&gt;
&lt;blockquote class=&quot;pull-quote&quot;&gt;
&lt;p&gt;If our technological civilization is to avoid falling into decay, human obsolescence in one form or another is unavoidable.&lt;/p&gt;
&lt;div class=&quot;reco&quot;&gt;
&lt;article class=&quot;issue-article&quot;&gt;&lt;div&gt;&lt;a href=&quot;http://nautil.us/issue/26/Color/the-terrifying-uncertainty-in-jeff-vandermeers-sci_fi&quot; class=&quot;obnd_lnk&quot; data-trval=&quot;the-terrifying-uncertainty-in-jeff-vandermeers-sci_fi&quot; data-trlbl=&quot;foc_rec&quot; data-tract=&quot;internal_art&quot;&gt;&lt;img src=&quot;http://static.nautil.us/6761_8ab70731b1553f17c11a3bbc87e0b605.png&quot; alt=&quot;Sapolsky_TH-F1&quot; width=&quot;314&quot; height=&quot;177&quot;/&gt;&lt;/a&gt;&lt;/div&gt;


&lt;/article&gt;&lt;/div&gt;
&lt;/blockquote&gt;
&lt;p&gt;Returning to my motel room exhausted each night, I unwound by reading excerpts from an old book, &lt;em&gt;Summa Technologiae&lt;/em&gt;. The late Polish author Stanislaw Lem had written it in the early 1960s, setting himself the lofty goal of forging a secular counterpart to the 13th-century &lt;em&gt;Summa Theologica&lt;/em&gt;, Thomas Aquinas’s landmark compendium exploring the foundations and limits of Christian theology. Where Aquinas argued for the certainty of a Creator, an immortal soul, and eternal salvation as based on scripture, Lem concerned himself with the uncertain future of intelligence and technology throughout the universe, guided by the tenets of modern science.&lt;/p&gt;
&lt;img src=&quot;http://static.nautil.us/4206_769ac34a4012ab69c069de0bab7d9e81.png&quot; width=&quot;733&quot; alt=&quot;&quot;/&gt;&lt;p&gt;To paraphrase Lem himself, the book was an investigation of the thorns of technological roses that had yet to bloom. And yet, despite Lem’s later observation that “nothing ages as fast as the future,” to my surprise most of the book’s nearly half-century-old prognostications concerned the very same topics I had encountered during my days at the conference, and felt just as fresh. Most surprising of all, in subsequent conversations I confirmed my suspicions that among the masters of our technological universe gathered there in San Francisco to forge a transhuman future, very few were familiar with the book or, for that matter, with Lem. I felt like a passenger in a car who discovers a blindspot in the central focus of the driver’s view.&lt;/p&gt;
&lt;p&gt;Such blindness was, perhaps, understandable. In 2007, only fragments of &lt;em&gt;Summa Technologiae&lt;/em&gt; had appeared in English, via partial translations undertaken independently by the literary scholar Peter Swirski and a German software developer named Frank Prengel. These fragments were what I read in the motel. The first complete English translation, by the media researcher Joanna Zylinska, only appeared in 2013. By Lem’s own admission, from the start the book was a commercial and a critical failure that “sank without a trace” upon its first appearance in print. Lem’s terminology and dense, baroque style is partially to blame—many of his finest points were made in digressive parables, allegories, and footnotes, and he coined his own neologisms for what were, at the time, distinctly over-the-horizon fields. In Lem’s lexicon, virtual reality was “phantomatics,” molecular nanotechnology was “molectronics,” cognitive enhancement was “cerebromatics,” and biomimicry and the creation of artificial life was “imitology.” He had even coined a term for search-engine optimization, a la Google: “ariadnology.” The path to advanced artificial intelligence he called the “technoevolution” of “intellectronics.”&lt;/p&gt;
&lt;p&gt;Even now, if Lem is known at all to the vast majority of the English-speaking world, it is chiefly for his authorship of &lt;em&gt;Solaris&lt;/em&gt;, a popular 1961 science-fiction novel that spawned two critically acclaimed film adaptations, one by Andrei Tarkovsky and another by Steven Soderbergh. Yet to say the prolific author only wrote science fiction would be foolishly dismissive. That so much of his output can be classified as such is because so many of his intellectual wanderings took him to the outer frontiers of knowledge.&lt;/p&gt;
&lt;p&gt;Lem was a polymath, a voracious reader who devoured not only the classic literary canon, but also a plethora of research journals, scientific periodicals, and popular books by leading researchers. His genius was in standing on the shoulders of scientific giants to distill the essence of their work, flavored with bittersweet insights and thought experiments that linked their mathematical abstractions to deep existential mysteries and the nature of the human condition. For this reason alone, reading Lem is an education, wherein one may learn the deep ramifications of breakthroughs such as Claude Shannon’s development of information theory, Alan Turing’s work on computation, and John von Neumann’s exploration of game theory. Much of his best work entailed constructing analyses based on logic with which anyone would agree, then showing how these eminently reasonable premises lead to astonishing conclusions. And the fundamental urtext for all of it, the wellspring from which the remainder of his output flowed, is &lt;em&gt;Summa Technologiae&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The core of the book is a heady mix of evolutionary biology, thermodynamics—the study of energy flowing through a system—and cybernetics, a diffuse field pioneered in the 1940s by Norbert Wiener studying how feedback loops can automatically regulate the behavior of machines and organisms. Considering a planetary civilization this way, Lem posits a set of feedbacks between the stability of a society and its degree of technological development. In its early stages, Lem writes, the development of technology is a self-reinforcing process that promotes homeostasis, the ability to maintain stability in the face of continual change and increasing disorder. That is, incremental advances in technology tend to progressively increase a society’s resilience against disruptive environmental forces such as pandemics, famines, earthquakes, and asteroid strikes. More advances lead to more protection, which promotes more advances still.&lt;/p&gt;
&lt;blockquote class=&quot;pull-quote&quot;&gt;
&lt;p&gt;The result is a disconcerting paradox: To maintain control of our own fate, we must yield our agency to minds exponentially more powerful than our own.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And yet, Lem argues, that same technology-driven positive feedback loop is also an Achilles heel for planetary civilizations, at least for ours here on Earth. As advances in science and technology accrue and the pace of discovery continues its acceleration, our society will approach an “information barrier” beyond which our brains—organs blindly, stochastically shaped by evolution for vastly different purposes—can no longer efficiently interpret and act on the deluge of information.&lt;/p&gt;
&lt;p&gt;Past this point, our civilization should reach the end of what has been a period of exponential growth in science and technology. Homeostasis will break down, and without some major intervention, we will collapse into a “developmental crisis” from which we may never fully recover. Attempts to simply muddle through, Lem writes, would only lead to a vicious circle of boom-and-bust economic bubbles as society meanders blindly down a random, path-dependent route of scientific discovery and technological development. “Victories, that is, suddenly appearing domains of some new wonderful activity,” he writes, “will engulf us in their sheer size, thus preventing us from noticing some other opportunities—which may turn out to be even more valuable in the long run.”&lt;/p&gt;
&lt;p&gt;Lem thus concludes that if our technological civilization is to avoid falling into decay, human obsolescence in one form or another is unavoidable. The sole remaining option for continued progress would then be the “automatization of cognitive processes” through development of algorithmic “information farms” and superhuman artificial intelligences. This would occur via a sophisticated plagiarism, the virtual simulation of the mindless, brute-force natural selection we see acting in biological evolution, which, Lem dryly notes, is the only technique known in the universe to construct philosophers, rather than mere philosophies.&lt;/p&gt;
&lt;img src=&quot;http://static.nautil.us/7110_7da9fd85999f583e3906f99a3ee58911.jpg&quot; width=&quot;733&quot; alt=&quot;&quot;/&gt;&lt;span class=&quot;caption&quot;&gt;&lt;strong&gt;star power:&lt;/strong&gt; George Clooney plays the role of Dr. Chris Kelvin in the 2002 film adaption of Lem’s 1961 novel, &lt;em&gt;Solaris&lt;/em&gt;.&lt;/span&gt;&lt;span class=&quot;credit&quot;&gt;Courtesy of 20th Century Fox&lt;/span&gt;
&lt;p&gt;&lt;span&gt;The result is a disconcerting paradox, which Lem expresses early in the book: To maintain control of our own fate, we must yield our agency to minds exponentially more powerful than our own, created through processes we cannot entirely understand, and hence potentially unknowable to us. This is the basis for Lem’s explorations of The Singularity, and in describing its consequences he reaches many conclusions that most of its present-day acolytes would share. But there is a difference between the typical modern approach and Lem’s, not in degree, but in kind.&lt;/span&gt;&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Unlike the commodified futurism now so common in the bubble-worlds of Silicon Valley billionaires, Lem’s forecasts weren’t really about seeking personal enrichment from market fluctuations, shiny new gadgets, or simplistic ideologies of “disruptive innovation.” In &lt;em&gt;Summa Technologiae&lt;/em&gt; and much of his subsequent work, Lem instead sought to map out the plausible answers to questions that today are too often passed over in silence, perhaps because they fail to neatly fit into any TED Talk or startup business plan: Does technology control humanity, or does humanity control technology? Where are the absolute limits for our knowledge and our achievement, and will these boundaries be formed by the fundamental laws of nature or by the inherent limitations of our psyche? If given the ability to satisfy nearly any material desire, what is it that we actually would want?&lt;/p&gt;
&lt;p&gt;Lem’s explorations of these questions are dominated by his obsession with chance, the probabilistic tension between chaos and order as an arbiter of human destiny. He had a deep appreciation for entropy, the capacity for disorder to naturally, spontaneously arise and spread, cursing some while sparing others. It was an appreciation born from his experience as a young man in Poland before, during, and after World War II, where he saw chance’s role in the destruction of countless dreams, and where, perhaps by pure chance alone, his Jewish heritage did not result in his death. “We were like ants bustling in an anthill over which the heel of a boot is raised,” he wrote in &lt;em&gt;Highcastle&lt;/em&gt;, an autobiographical memoir. “Some saw its shadow, or thought they did, but everyone, the uneasy included, ran about their usual business until the very last minute, ran with enthusiasm, devotion—to secure, to appease, to tame the future.” From the accumulated weight of those experiences, Lem wrote in the &lt;em&gt;New Yorker&lt;/em&gt; in 1986, he had “come to understand the fragility that all systems have in common,” and “how human beings behave under extreme conditions—how their behavior when they are under enormous pressure is almost impossible to predict.”&lt;/p&gt;
&lt;p&gt;To Lem (and, to their credit, a sizeable number of modern thinkers), the Singularity is less an opportunity than a question mark, a multidimensional crucible in which humanity’s future will be forged.&lt;/p&gt;
&lt;p&gt;I couldn’t help thinking of Lem’s question mark that summer in 2007. Within and around the gardens surrounding the neoclassical Palace of Fine Arts Theater where the Singularity Summit was taking place, dark and disruptive shadows seemed to loom over the plans and aspirations of the gathered well-to-do. But they had precious little to do with malevolent superintelligences or runaway nanotechnology. Between my motel and the venue, panhandlers rested along the sidewalk, or stood with empty cups at busy intersections, almost invisible to everyone. Walking outside during one break between sessions, I stumbled across a homeless man defecating between two well-manicured bushes. Even within the context of the conference, hints of desperation sometimes tinged the not-infrequent conversations about raising capital; the subprime mortgage crisis was already unfolding that would, a year later, spark the near-collapse of the world’s financial system. While our society’s titans of technology were angling for advantages to create what they hoped would be the best of all possible futures, the world outside reminded those who would listen that we are barely in control even today.&lt;/p&gt;
&lt;blockquote class=&quot;pull-quote&quot;&gt;
&lt;p&gt;In Lem’s view, humans, as imperfect as we are, shall always strive to progress and improve.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I attended two more Singularity Summits, in 2008 and 2009, and during that three-year period, all the much-vaunted performance gains in various technologies seemed paltry against a more obvious yet less-discussed pattern of accelerating change: the rapid, incessant growth in global ecological degradation, economic inequality, and societal instability. Here, forecasts tend to be far less rosy than those for our future capabilities in information technology. They suggest, with some confidence, that when and if we ever breathe souls into our machines, most of humanity will not be dreaming of transcending their biology, but of fresh water, a full belly, and a warm, safe bed. How useful would a superintelligent computer be if it was submerged by storm surges from rising seas or dis- connected from a steady supply of electricity? Would biotech-boosted personal longevity be worthwhile in a world ravaged by armed, angry mobs of starving, displaced people? More than once I have wondered why so many high technologists are more concerned by as- yet-nonexistent threats than the much more mundane and all-too-real ones literally right before their eyes.&lt;/p&gt;
&lt;p&gt;Lem was able to speak to my experience of the world outside the windows of the Singularity conference. A thread of humanistic humility runs through his work, a hard-gained certainty that technological development too often takes place only in service of our most primal urges, rewarding individual greed over the common good. He saw our world as exceedingly fragile, contingent upon a truly astronomical number of coincidences, where the vagaries of the human spirit had become the most volatile variables of all.&lt;/p&gt;
&lt;p&gt;It is here that we find Lem’s key strength as a futurist. He refused to discount human nature’s influence on transhuman possibilities, and believed that the still-incomplete task of understanding our strengths and weaknesses as human beings was a crucial prerequisite for all speculative pathways to any post-Singularity future. Yet this strength also leads to what may be Lem’s great weakness, one which he shares with today’s hopeful transhumanists: an all-too-human optimism that shines through an otherwise-dispassionate darkness, a fervent faith that, when faced with the challenge of a transhuman future, we will heroically plunge headlong into its depths. In Lem’s view, humans, as imperfect as we are, shall always strive to progress and improve, seeking out all that is beautiful and possible rather than what may be merely convenient and profitable, and through this we may find salvation. That we might instead succumb to complacency, stagnation, regression, and extinction is something he acknowledges but can scarcely countenance. In the end, Lem, too, was seduced—though not by quasi-religious notions of personal immortality, endless growth, or cosmic teleology, but instead by the notion of an indomitable human spirit.&lt;/p&gt;
&lt;p&gt;Like many other ideas from &lt;em&gt;Summa Technologiae&lt;/em&gt;, this one finds its best expression in one of Lem’s works of fiction, his 1981 novella &lt;em&gt;Golem XIV&lt;/em&gt;, in which a self-programming military supercomputer that has bootstrapped itself into sentience delivers a series of lectures critiquing evolution and humanity. Some would say it is foolish to seek truth in fiction, or to draw equivalence between an imaginary character’s thoughts and an author’s genuine beliefs, but for me the conclusion is inescapable. When the novella’s artificial philosopher makes its pronouncements through a connected vocoder, it is the human voice of Lem that emerges, uttering a prophecy of transcendence that is at once his most hopeful—and perhaps, in light of trends today, his most erroneous:&lt;/p&gt;
&lt;p&gt;“I feel that you are entering an age of metamorphosis; that you will decide to cast aside your entire history, your entire heritage and all that remains of natural humanity—whose image, magnified into beautiful tragedy, is the focus of the mirrors of your beliefs; that you will advance (for there is no other way), and in this, which for you is now only a leap into the abyss, you will find a challenge, if not a beauty; and that you will proceed in your own way after all, since in casting off man, man will save himself.”&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Freelance writer Lee Billings is the author of&lt;/em&gt; Five Billion Years of Solitude: The Search for Life Among the Stars.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Photograph by Forum/UIG/Getty Images&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This article was originally published online in our “Genius” issue in October, 2014.&lt;/em&gt;&lt;/p&gt;
&lt;section class=&quot;leaderboard-ad-belt&quot;&gt;&lt;div class=&quot;leaderboard-ad-belt-inner adarticle&quot;&gt;&lt;div id=&quot;div-gpt-ad-1380044019755-0&quot; class=&quot;leaderboard-ad&quot;/&gt;
&lt;/div&gt;
&lt;/section&gt;</description>
<pubDate>Sun, 22 Apr 2018 16:27:32 +0000</pubDate>
<dc:creator>dnetesn</dc:creator>
<og:type>website</og:type>
<og:url>http://nautil.us/issue/28/2050/the-book-no-one-read</og:url>
<og:title>The Book No One Read - Issue 28: 2050 - Nautilus</og:title>
<og:description>I remember well the first time my certainty of a bright future evaporated, when my confidence in the panacea of technological progress&amp;#8230;</og:description>
<og:image>http://static.nautil.us/4203_7e05295a468401ec66e8c337855022ed.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>http://nautil.us/issue/28/2050/the-book-no-one-read</dc:identifier>
</item>
<item>
<title>Blind since birth, writing code at Amazon since 2013</title>
<link>https://blog.aboutamazon.com/working-at-amazon/blind-since-birth-writing-code-at-amazon-since-2013</link>
<guid isPermaLink="true" >https://blog.aboutamazon.com/working-at-amazon/blind-since-birth-writing-code-at-amazon-since-2013</guid>
<description>&lt;div class=&quot;RichTextArticleBody mood-color&quot;&gt;
&lt;div class=&quot;RichTextArticleBody-body&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://youtu.be/eqtIrzpa3jk&quot; target=&quot;_blank&quot; data-cms-ai=&quot;0&quot;&gt;Click for an extended edition of the video with audio descriptions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Michael Forzano loves when his teammates ask him for help. There’s no ego involved – far from it. For Forzano, it’s the pure satisfaction that he is right where he belongs, writing code at Amazon.&lt;/p&gt;
&lt;p&gt;“I do feel like I have to prove myself a lot in life,” said Forzano, a 26-year-old software engineer on the retail accessibility team. “But not at Amazon. People have been so open minded here.”&lt;/p&gt;
&lt;p&gt;Forzano has been blind since birth as the result of a genetic condition called Norrie disease. “I definitely had a pretty normal childhood despite my blindness,” said Forzano. “My parents always tried to make sure I was able to do the same things that anyone else would do.”&lt;/p&gt;
&lt;div class=&quot;Enhancement&quot; data-align-center=&quot;&quot;&gt;
&lt;div class=&quot;Enhancement-item&quot;&gt;
&lt;div class=&quot;Quote&quot;&gt;&lt;span class=&quot;Quote-icon-top mood-color&quot;&gt;“&lt;/span&gt;
&lt;blockquote&gt;
&lt;p&gt;I remember when I told my mom that I got the (job) offer, she started crying right there on the phone.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;Quote-attribution&quot;&gt;Amazon software engineer Michael Forzano&lt;/div&gt;
&lt;span class=&quot;Quote-icon-bottom mood-color&quot;&gt;”&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Forzano also started losing his hearing at the age of five and uses cochlear implants to hear. As a teenager, he became interested in audio games, which are audio-based computer games, and taught himself how to program. He went on to Binghamton University, where he played sax in the pep band and earned a bachelor's degree in computer science.&lt;/p&gt;
&lt;p&gt;Right out of college, he interviewed to become a software engineer at Amazon. He walked in the door, revealed to the interviewers that he was blind, and earned himself a job by impressing them with the code he wrote on his laptop.&lt;/p&gt;
&lt;p&gt;“I remember when I told my mom that I got the (job) offer, she started crying right there on the phone,” said Forzano. At Amazon, he writes code that helps other teams make shopping on Amazon more accessible to people with disabilities.&lt;/p&gt;
&lt;p&gt;Today, the New York native lives with his guide dog, Delta, in downtown Seattle, where he relishes his independence.&lt;/p&gt;
&lt;div class=&quot;Enhancement&quot; data-align-right=&quot;&quot;&gt;
&lt;div class=&quot;Enhancement-item&quot;&gt;
&lt;div class=&quot;VideoEnhancement-title&quot;&gt;&lt;span class=&quot;VideoEnhancement&quot; data-video-disable-history=&quot;&quot;&gt;How Forzano writes code&lt;/span&gt;&lt;/div&gt;
&lt;div class=&quot;VideoEnhancement-player&quot;&gt;
&lt;div class=&quot;YouTubeVideoPlayer&quot; data-video-player=&quot;&quot; data-player-id=&quot;f3743a6875a8a41c1a02a62f73b488a54&quot; data-video-id=&quot;57P_dCEPtRw&quot; data-video-title=&quot;How Forzano writes code&quot;&gt;&lt;span class=&quot;VideoEnhancement&quot; data-video-disable-history=&quot;&quot;&gt;&lt;iframe id=&quot;YouTubeVideoPlayer-f3743a6875a8a41c1a02a62f73b488a54&quot; allowfullscreen=&quot;&quot; src=&quot;https://www.youtube.com/embed/57P_dCEPtRw?enablejsapi=1&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Erik Wang, a fellow software developer at Amazon, said Forzano &quot;reads and writes code even faster than me. He has superpowers to spot flaws in the code.”&lt;/p&gt;
&lt;p&gt;Forzano said he has a good “mental map of the structure of the code,” which allows him to help colleagues and provide unique feedback to his team.&lt;/p&gt;
&lt;p&gt;“I feel really lucky to be here at Amazon, just being able to live the same kind of life that anyone else would. Not letting my blindness hold me back is really empowering.”&lt;/p&gt;
&lt;p&gt;Forzano works on a standard laptop with screen-reader software, which translates every aspect of using a computer into audio cues.&lt;/p&gt;
&lt;p&gt;“I think it's really important for other blind people to know what I have done,” Forzano said. “There are probably a lot of blind people out there wondering how far they'll go and what they will be able to do. I definitely do all I can to make myself available, as a role model, and let the world know.”&lt;/p&gt;

&lt;p&gt;&lt;em&gt;On becoming interested in computers (transcript)&lt;/em&gt;: &quot;So I got interested in computers when I was in high school. I knew a community of blind people online and some of them were developers, really just as a hobby not as a career. But one of them introduced me to programming, specifically programming games. I was very interested in programming audio games, which were basically games that use sound effects and are controlled using the keyboard and allow blind people to play them. I met this person who was a game developer and introduced me to programming, and I just took it from there and taught myself.&quot;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;&lt;em&gt;On his job interview at Amazon (transcript)&lt;/em&gt;: &quot;I sent in my application. I was interviewed on campus. They sent some developers out to do interviews, and I went in there not expecting much. I had not told them in advance that I was blind. I just brought in my laptop and said, 'Hey, I'm blind. Can I use my laptop instead of a whiteboard to write my code for the interview?' And they were like, sure no problem. So I did my thing and was extended an offer to come to Seattle. My parents are really proud. I remember when I told my mom that I got the offer, she started crying right there on the phone.&quot;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;&lt;em&gt;On coding (transcript)&lt;/em&gt;: &quot;At Amazon I work on the Retail Accessibility team. We build tools that help other teams who are building the features on the website to make sure that they are accessible to customers with disabilities. I know that my co-workers often ask me, 'Can you tell me how this works?' Because I have a pretty good mental map of the structure of the code and where things are and what part of the system this particular component is in, or the overall architecture of the system. I've got it in my head. I can tell someone how something works, where something is, whereas I feel like a lot of my coworkers are relying on white-boarding and drawing diagrams, which is pretty typical, I would say, for people with vision, because they are often visual learners, and just visually oriented people. I’ve never had that, so I've been able to, I guess, use my brain power to do things non-visually.&quot;&lt;br/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Sun, 22 Apr 2018 14:12:24 +0000</pubDate>
<dc:creator>dominotw</dc:creator>
<og:title>Blind since birth, writing code at Amazon since 2013</og:title>
<og:url>https://blog.aboutamazon.com/working-at-amazon/blind-since-birth-writing-code-at-amazon-since-2013</og:url>
<og:image>https://d39w7f4ix9f5s9.cloudfront.net/dims4/default/faa7e2f/2147483647/strip/true/crop/960x502+0+69/resize/1200x628!/quality/90/?url=https%3A%2F%2Fd39w7f4ix9f5s9.cloudfront.net%2F30%2Fc3%2Fc9a57b4d4b11961f60e6bab35ef4%2Fforzano-social-promo.jpg</og:image>
<og:description>Michael Forzano said he has a good “mental map of the structure of the code,” which allows him to help colleagues and provide unique feedback to his team.</og:description>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.aboutamazon.com/working-at-amazon/blind-since-birth-writing-code-at-amazon-since-2013</dc:identifier>
</item>
<item>
<title>Artificial intelligence accelerates discovery of metallic glass</title>
<link>https://news.northwestern.edu/stories/2018/april/artificial-intelligence-accelerates-discovery-of-metallic-glass/</link>
<guid isPermaLink="true" >https://news.northwestern.edu/stories/2018/april/artificial-intelligence-accelerates-discovery-of-metallic-glass/</guid>
<description>&lt;p&gt; With new approach, predictions closely match actual experimental data. Credit: SLAC
&lt;/p&gt;&lt;div readability=&quot;136.74608610568&quot;&gt;
&lt;p&gt;If you combine two or three metals together, you will get an alloy that usually looks and acts like a metal, with its atoms arranged in rigid geometric patterns.&lt;/p&gt;
&lt;p&gt;But once in a while, under just the right conditions, you get something entirely new: a futuristic alloy called metallic glass. The amorphous material’s atoms are arranged every which way, much like the atoms of the glass in a window. Its glassy nature makes it stronger and lighter than today’s best steel, and it stands up better to corrosion and wear.&lt;/p&gt;
&lt;p&gt;Although metallic glass shows a lot of promise as a protective coating and alternative to steel, only a few thousand of the millions of possible combinations of ingredients have been evaluated over the past 50 years, and only a handful developed to the point that they may become useful.&lt;/p&gt;
&lt;p&gt;Now a group led by scientists at Northwestern University, the Department of Energy’s &lt;a title=&quot;More about SLAC&quot; href=&quot;https://www6.slac.stanford.edu&quot; target=&quot;_blank&quot;&gt;SLAC National Accelerator Laboratory&lt;/a&gt; and the &lt;a href=&quot;https://www.nist.gov/&quot; target=&quot;_blank&quot;&gt;National Institute of Standards and Technology&lt;/a&gt; (NIST) has reported a shortcut for discovering and improving metallic glass — and, by extension, other elusive materials — at a fraction of the time and cost. &lt;/p&gt;
&lt;p&gt;The research group took advantage of a system at SLAC’s Stanford Synchrotron Radiation Lightsource (SSRL) that combines machine learning — a form of artificial intelligence where computer algorithms glean knowledge from enormous amounts of data — with experiments that quickly make and screen hundreds of sample materials at a time. This allowed the team to discover three new blends of ingredients that form metallic glass, and to do it 200 times faster than it could be done before.&lt;/p&gt;
&lt;p&gt;The &lt;a title=&quot;Read the study&quot; href=&quot;http://advances.sciencemag.org/content/4/4/eaaq1566&quot; target=&quot;_blank&quot;&gt;study was published today&lt;/a&gt;, April 13, in Science Advances.&lt;/p&gt;
&lt;p&gt;“It typically takes a decade or two to get a material from discovery to commercial use,” said &lt;a href=&quot;http://www.mccormick.northwestern.edu/research-faculty/directory/profiles/wolverton-chris.html&quot; target=&quot;_blank&quot;&gt;Chris Wolverton&lt;/a&gt;, the Jerome B. Cohen Professor of Materials Science and Engineering in Northwestern’s McCormick School of Engineering, who is an early pioneer in using computation and AI to predict new materials. “This is a big step in trying to squeeze that time down. You could start out with nothing more than a list of properties you want in a material and, using AI, quickly narrow the huge field of potential materials to a few good candidates.” &lt;/p&gt;
&lt;p&gt;The ultimate goal, said Wolverton, who led the paper’s machine learning work, is to get to the point where a scientist can scan hundreds of sample materials, get almost immediate feedback from machine learning models and have another set of samples ready to test the next day — or even within the hour.&lt;/p&gt;
&lt;p&gt;Over the past half century, scientists have investigated about 6,000 combinations of ingredients that form metallic glass. Added paper co-author Apurva Mehta, a staff scientist at SSRL: “We were able to make and screen 20,000 in a single year.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Just getting started&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While other groups have used machine learning to come up with predictions about where different kinds of metallic glass can be found, Mehta said, “The unique thing we have done is to rapidly verify our predictions with experimental measurements and then repeatedly cycle the results back into the next round of machine learning and experiments.”&lt;/p&gt;
&lt;p&gt;There’s plenty of room to make the process even speedier, he added, and eventually automate it to take people out of the loop altogether so scientists can concentrate on other aspects of their work that require human intuition and creativity. “This will have an impact not just on synchrotron users, but on the whole materials science and chemistry community,” Mehta said.&lt;/p&gt;
&lt;p&gt;The team said the method will be useful in all kinds of experiments, especially in searches for materials like metallic glass and catalysts whose performance is strongly influenced by the way they’re manufactured, and those where scientists don’t have theories to guide their search. With machine learning, no previous understanding is needed. The algorithms make connections and draw conclusions on their own, which can steer research in unexpected directions.&lt;/p&gt;
&lt;p&gt;“One of the more exciting aspects of this is that we can make predictions so quickly and turn experiments around so rapidly that we can afford to investigate materials that don’t follow our normal rules of thumb about whether a material will form a glass or not,” said paper co-author Jason Hattrick-Simpers, a materials research engineer at NIST. “AI is going to shift the landscape of how materials science is done, and this is the first step.” &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Experimenting with data&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In the metallic glass study, the research team investigated thousands of alloys that each contain three cheap, nontoxic metals.&lt;/p&gt;
&lt;p&gt;They started with a &lt;a href=&quot;https://en.wikipedia.org/wiki/Landolt%E2%80%93B%C3%B6rnstein&quot; target=&quot;_blank&quot;&gt;trove of materials data&lt;/a&gt; dating back more than 50 years, including the results of 6,000 experiments that searched for metallic glass. The team combed through the data with advanced machine learning algorithms developed by Wolverton and Logan Ward, a graduate student in Wolverton’s laboratory who served as co-first author of the paper.&lt;/p&gt;
&lt;p&gt;Based on what the algorithms learned in this first round, the scientists crafted two sets of sample alloys using two different methods, allowing them to test how manufacturing methods affect whether an alloy morphs into a glass. An SSRL x-ray beam scanned both sets of alloys, then researchers fed the results into a database to generate new machine learning results, which were used to prepare new samples that underwent another round of scanning and machine learning.&lt;/p&gt;
&lt;p&gt;By the experiment’s third and final round, Mehta said, the group’s success rate for finding metallic glass had increased from one out of 300 or 400 samples tested to one out of two or three samples tested. The metallic glass samples they identified represented three different combinations of ingredients, two of which had never been used to make metallic glass before.&lt;/p&gt;
&lt;p&gt;The study was funded by the US Department of Energy (award number FWP-100250), the &lt;a title=&quot;More about ChiMAD&quot; href=&quot;http://chimad.northwestern.edu&quot; target=&quot;_blank&quot;&gt;Center for Hierarchical Materials Design&lt;/a&gt; and the National Institute of Standards and Technology (award number 70NANB14H012).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;— From an article by Glennda Chui, SLAC National Accelerator Laboratory&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;</description>
<pubDate>Sun, 22 Apr 2018 10:33:07 +0000</pubDate>
<dc:creator>rbanffy</dc:creator>
<og:title>Artificial intelligence accelerates discovery of metallic glass - Northwestern Now</og:title>
<og:url>https://news.northwestern.edu/stories/2018/april/artificial-intelligence-accelerates-discovery-of-metallic-glass/</og:url>
<og:image>https://news.northwestern.edu/assets/Stories/2018/04/_resampled/ScaleWidthWyI0ODAiXQ/wolverton-metallic-glass.jpg</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://news.northwestern.edu/stories/2018/april/artificial-intelligence-accelerates-discovery-of-metallic-glass/</dc:identifier>
</item>
<item>
<title>Interns with toasters: how I taught people about load balancers</title>
<link>https://rachelbythebay.com/w/2018/04/21/lb/</link>
<guid isPermaLink="true" >https://rachelbythebay.com/w/2018/04/21/lb/</guid>
<description>&lt;h2&gt;Interns with toasters: how I taught people about load balancers&lt;/h2&gt;
&lt;p&gt;Several years ago, I wrote a description of a problem that could happen in a large load-balanced environment. When it landed on certain web sites, some of the commenters dismissed either it (&quot;impossible&quot;) or me (&quot;doesn't know anything about load balancing&quot;). Those are both wrong. I suspect what happened is that they didn't understand the problem, and so resorted to ineffective means to steer the attention away from their own inadequacies.&lt;/p&gt;
&lt;p&gt;I've since come up with another way to tell the story of what happened. It was refined over many years of teaching new employees about outages. I will now attempt to present a sanitized version of it here.&lt;/p&gt;
&lt;p&gt;One day, someone decided to chase down a weird pattern on a graph. There was a strange &quot;quadruple-hump&quot; figure in the pattern of failed requests to the site. First, they narrowed it down to a specific region, then a specific group of machines. Within that group, they tried to narrow it down more, and attempted to select it out by the load balancer reporting the problem. It didn't help: all of them were reporting it equally.&lt;/p&gt;
&lt;p&gt;Next, they grouped the graph by the web servers which reported the failed requests. This also did not expose the quadruple-hump pattern which was desired, but there was another interesting thing which was then exposed. One of the web servers was reporting perhaps 200 times as many failed requests as every other one of its friends in that location.&lt;/p&gt;
&lt;p&gt;The question I used to ask the class is: how can this happen? How can a big sophisticated operation have a single machine out of many become an outlier in this regard? What's going on?&lt;/p&gt;
&lt;p&gt;It was at this point I asked them to roll with me, and imagine a scenario I would then describe. First, let's say that we get 100 interns to test this, because interns are great and you can use them to test anything. You tell them to report to a room much like the classroom, and have them line up and await further instructions.&lt;/p&gt;
&lt;p&gt;Upon entering the room, they would find 100 small tables I had previously procured. Atop each table was a toaster, since I had gone out and bought every toaster I could find, by hitting Target, Walmart, CVS, and any other place that sells small appliances. Finally, I had the facilities team bring in a whole bunch of extra power feeds, because we were going to need a lot. We wanted to run all 100 toasters &lt;em&gt;at the same time&lt;/em&gt;. That's a lot of juice!&lt;/p&gt;
&lt;p&gt;So the interns come in, find their stations, and are waiting to find out what happens. They're chattering about what this could be for. That's the point when I enter the room carrying a massive bag of bread. I'm talking about something comically sized, like an oversized beach ball, and it's packed with slices of bread.&lt;/p&gt;
&lt;p&gt;I go up to the first intern, and hand them two of the slices. They put them in their toaster, push it down, wait a minute or two, and ... hey, toast. (What did you think would happen at this point in the story? It's just a toaster. This isn't Harry Potter.)&lt;/p&gt;
&lt;p&gt;Meanwhile, naturally, I had gone on to the second intern, who got their bread, and put it in their toaster, and the third, and the fourth, and the fifth, and so on down the line until everyone had bread.&lt;/p&gt;
&lt;p&gt;At various points, a toaster would finish and would pop up. Toasters are all slightly different, so they wouldn't take the same amount of time. I'd notice that they were done and would run over to give them more bread. Oh, you're done! Have some more. Oh, you too! Here you go. Ready for more? Yep, here it is. Over here now. Gotcha. Oh and there? Okay!&lt;/p&gt;
&lt;p&gt;This is how the load balancers work. The whole time (oh, you're done, have some more), they are constantly looking to see who's not busy (two more for you, got it) and then send more traffic that way. I'm the human load balancer in this thought experiment, handing bread to my friends the interns.&lt;/p&gt;
&lt;p&gt;Trouble is, one of the interns wasn't playing by the rules.&lt;/p&gt;
&lt;p&gt;Instead, they'd take the bread from me, and they'd look at it. &quot;That's some nice bread&quot;, they'd think. They'd continue, &quot;it would look even better &lt;em&gt;over there&lt;/em&gt;&quot;, and they'd throw it away without even putting it anywhere near the toaster. Of course, I didn't notice this. I wasn't even watching for it.&lt;/p&gt;
&lt;p&gt;But hey, I'd see they were ready for more, so I'd give them two more slices of bread. They'd take them, size them up again, and *foop* throw them away. The pile behind them grew.&lt;/p&gt;
&lt;p&gt;This would go on over and over and over. It would go on forever if nothing stopped us.&lt;/p&gt;
&lt;p&gt;This is what happened when one bad web server decided it was going to fail all of its requests, and would do so while incurring the absolute minimum amount of load on itself. Maybe it never even got the request anywhere near the normal processing guts. It became a failure machine, turning out those HTTP 500s just as fast as it was handed requests.&lt;/p&gt;
&lt;p&gt;Because it never got backlogged or busy, and was always available for more work, the load balancers just kept sending traffic that way. In so doing, one machine out of a very large group managed to scarf down a substantial fraction of the traffic bound for that entire area.&lt;/p&gt;
&lt;p&gt;I told my students the story this way because I wanted them to take it with them through their career, no matter where they ended up. Some day, if they see some problem with one widget out of thousands or millions &quot;capturing&quot; all of the traffic, maybe they'll remember me and my little story about interns and toasters.&lt;/p&gt;
&lt;p&gt;Of course, when I taught that class, I was there in person, physically moving around to denote the size of the line of interns, picking spots in the audience to be intern #1, #2, #3, and so on, all the while &quot;interrupting&quot; myself to say &quot;oh you're done, have another&quot;. (I did this above with parentheticals. Did you catch it?) When I got to the point about the one intern not playing by the rules, that's when I'd break out some props and start flinging pens or markers over my shoulder, making sure they audibly caromed off something (like the wall). It worked great. Students loved it.&lt;/p&gt;
&lt;p&gt;I do miss giving those classes. But hey, there's a way forward from here. Maybe I'll start doing this stuff in the real world, bringing these lessons to life on stage somehow. Would people want to see that?&lt;/p&gt;
</description>
<pubDate>Sun, 22 Apr 2018 06:17:28 +0000</pubDate>
<dc:creator>deafcalculus</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://rachelbythebay.com/w/2018/04/21/lb/</dc:identifier>
</item>
<item>
<title>Software Testing Anti-patterns</title>
<link>http://blog.codepipes.com/testing/software-testing-antipatterns.html</link>
<guid isPermaLink="true" >http://blog.codepipes.com/testing/software-testing-antipatterns.html</guid>
<description>&lt;span class=&quot;post-date&quot;&gt;21 Apr 2018&lt;/span&gt;
&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;There are several articles out there that talk about testing anti-patterns in the software development process. Most of them however deal with the low level details of the programming code, and almost always they focus on a specific technology or programming language.&lt;/p&gt;
&lt;p&gt;In this article I wanted to take a step back and catalog some high-level testing anti-patterns that are technology agnostic. Hopefully you will recognize some of these patterns regardless of your favorite programming language.&lt;/p&gt;
&lt;h3 id=&quot;terminology&quot;&gt;Terminology&lt;/h3&gt;
&lt;p&gt;Unfortunately, testing terminology has not reached a common consensus yet. If you ask 100 developers what is the difference between an integration test, a component test and an end-to-end test you might get 100 different answers. For the purposes of this article I will focus on the definition of the test pyramid as presented below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/testing-pyramid.png&quot; alt=&quot;The Testing pyramid&quot; /&gt;&lt;/p&gt;
&lt;p&gt;If you have never encountered the testing pyramid before, I would urge you to become familiar with it first before going on. Some good starting points are:&lt;/p&gt;
&lt;p&gt;The testing pyramid deserves a whole discussion on its own, especially on the topic of the amount of tests needed for each category. For the current article I am just referencing the pyramid in order to define the two lowest test categories. Notice that in this article User Interface Tests (the top part of the pyramid) are &lt;em&gt;not&lt;/em&gt; mentioned (mainly for brevity reasons and because UI tests come with their own specific anti-patterns).&lt;/p&gt;
&lt;p&gt;Therefore the two major test categories mentioned as &lt;em&gt;unit&lt;/em&gt; and &lt;em&gt;integration&lt;/em&gt; tests from now on are:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Tests&lt;/th&gt;
&lt;th&gt;Focus on&lt;/th&gt;
&lt;th&gt;Require&lt;/th&gt;
&lt;th&gt;Speed&lt;/th&gt;
&lt;th&gt;Complexity&lt;/th&gt;
&lt;th&gt;Setup needed&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody readability=&quot;1&quot;&gt;&lt;tr&gt;&lt;td&gt;Unit tests&lt;/td&gt;
&lt;td&gt;a class/method&lt;/td&gt;
&lt;td&gt;the source code&lt;/td&gt;
&lt;td&gt;very fast&lt;/td&gt;
&lt;td&gt;low&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;Integration tests&lt;/td&gt;
&lt;td&gt;a component/service&lt;/td&gt;
&lt;td&gt;part of the running system&lt;/td&gt;
&lt;td&gt;slow&lt;/td&gt;
&lt;td&gt;medium&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;Unit tests&lt;/strong&gt; are the category of tests that have wider acceptance regarding the naming and what they mean. They are the tests that accompany the source code and have direct access to it. Usually they are executed with an &lt;a href=&quot;https://en.wikipedia.org/wiki/XUnit&quot;&gt;xUnit framework&lt;/a&gt; or similar library. These tests work directly on the source code and have full view of everything. A single class/method/function is tested (or whatever is the smallest possible working unit for that particular business feature) and anything else is mocked/stubbed.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Integration tests&lt;/strong&gt; (also called service tests, or even component tests) focus on a whole component. A component can be a set of classes/methods/functions, a module, a subsystem or even the application itself. They examine the component by passing input data and examinining the output data it produces. Usually some kind of deployment/bootstrap/setup is required first. External systems can be mocked completely, replaced (e.g. using an in-memory database instead of a real one), or the real external dependency might be used depending on the business case. Compared to unit tests they may require more specialized tools either for preparing the test environment, or for interacting/verifying it.&lt;/p&gt;
&lt;p&gt;The second category suffers from a blurry definition and most naming controversies regarding testing start here. The “scope” for integration tests is also highly controversial and especially the nature of access to the application (&lt;a href=&quot;https://en.wikipedia.org/wiki/Black-box_testing&quot;&gt;black&lt;/a&gt; or &lt;a href=&quot;https://en.wikipedia.org/wiki/White-box_testing&quot;&gt;white&lt;/a&gt; box testing and whether &lt;a href=&quot;https://en.wikipedia.org/wiki/Mock_object&quot;&gt;mocking&lt;/a&gt; is allowed or not).&lt;/p&gt;
&lt;p&gt;As a basic rule of thumb if&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;a test uses a database&lt;/li&gt;
&lt;li&gt;a test uses the network to call another component/application&lt;/li&gt;
&lt;li&gt;a test uses an external system (e.g. a queue or a mail server)&lt;/li&gt;
&lt;li&gt;a test reads/writes files or performs other I/O&lt;/li&gt;
&lt;li&gt;a test does not rely on the source code but instead it uses the deployed binary of the app&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;…then it is an integration test and not a unit test.&lt;/p&gt;
&lt;p&gt;With the naming out of the way, we can dive into the list. The order of anti-patterns roughly follows their appearance in the wild. Frequent problems are gathered in the top positions.&lt;/p&gt;
&lt;h3 id=&quot;software-testing-anti-pattern-list&quot;&gt;Software Testing Anti-Pattern List&lt;/h3&gt;
&lt;ol&gt;&lt;li&gt;&lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-1---having-unit-tests-without-integration-tests&quot;&gt;Having unit tests without integration tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-2---having-integration-tests-without-unit-tests&quot;&gt;Having integration tests without unit tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-3---having-the-wrong-kind-of-tests&quot;&gt;Having the wrong kind of tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-4---testing-the-wrong-functionality&quot;&gt;Testing the wrong functionality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-5---testing-internal-implementation&quot;&gt;Testing internal implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-6---paying-excessive-attention-to-test-coverage&quot;&gt;Paying excessive attention to test coverage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-7---having-flaky-or-slow-tests&quot;&gt;Having flaky or slow tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-8---running-tests-manually&quot;&gt;Running tests manually&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-9---treating-test-code-as-a-second-class-citizen&quot;&gt;Treating test code as a second class citizen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-10---not-converting-production-bugs-to-tests&quot;&gt;Not converting production bugs to tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-11---treating-tdd-as-a-religion&quot;&gt;Treating TDD as a religion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-12---writing-tests-without-reading-documentation-first&quot;&gt;Writing tests without reading documentation first&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-13---giving-testing-a-bad-reputation-out-of-ignorance&quot;&gt;Giving testing a bad reputation out of ignorance&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;h3 id=&quot;anti-pattern-1---having-unit-tests-without-integration-tests&quot;&gt;Anti-Pattern 1 - Having unit tests without integration tests&lt;/h3&gt;
&lt;p&gt;This problem is a classic one with small to medium companies. The application that is being developed in the company has only unit tests (the base of the pyramid) and nothing else. Usually lack of integration tests is caused by any of the following issues:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;The company has no senior developers. The team has only junior developers fresh out of college who have only seen unit tests&lt;/li&gt;
&lt;li&gt;Integration tests existed at one point but were abandoned because they caused more trouble than their worth. Unit tests were much more easy to maintain and so they prevailed.&lt;/li&gt;
&lt;li&gt;The running environment of the application is very “challenging” to setup. Features are “tested” in production.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;I cannot really say anything about the first issue. Every effective team should have at least some kind of mentor/champion that can show good practices to the other members. The second issue is covered in detail in anti-patterns &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-5---testing-internal-implementation&quot;&gt;5&lt;/a&gt;, &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-7---having-flaky-or-slow-tests&quot;&gt;7&lt;/a&gt; and &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-8---running-tests-manually&quot;&gt;8&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This brings us to the last issue - difficulty in setting up a test environment. Now don’t get me wrong, there are indeed some applications that are &lt;em&gt;really&lt;/em&gt; hard to test. Once I had to work with a set of REST applications that actually required special hardware on their host machine. This hardware existed only in production, making integration tests very challenging. But this is a corner case.&lt;/p&gt;
&lt;p&gt;For the run-of-the-mill web or back-end application that the typical company creates, setting up a test environment should be a non-issue. With the appearance of Virtual Machines and lately Containers this is more true than ever. Basically if you are trying to test an application that is hard to setup, you need to fix the setup process first before dealing with the tests themselves.&lt;/p&gt;
&lt;p&gt;But why are integration tests essential in the first place?&lt;/p&gt;
&lt;p&gt;The truth here is that there are some types of issues that &lt;em&gt;only&lt;/em&gt; integration tests can detect. The canonical example is everything that has to do with database operations. Database transactions, database triggers and any stored procedures can only be examined with integration tests that touch them. Any connections to other modules either developed by you or external teams need integration tests (a.k.a. contract tests). Any tests that need to verify performance, are integration tests by definition. Here is a summary on why we need integration tests:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Type of issue&lt;/th&gt;
&lt;th&gt;Detected by Unit tests&lt;/th&gt;
&lt;th&gt;Detected by Integration tests&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody readability=&quot;5&quot;&gt;&lt;tr&gt;&lt;td&gt;Basic business logic&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;Component integration problems&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Transactions&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;Database triggers/procedures&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;Wrong Contracts with other modules/APIs&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;Wrong Contracts with other systems&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Performance/Timeouts&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Deadlocks/Livelocks&lt;/td&gt;
&lt;td&gt;maybe&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;Cross-cutting Security Concerns&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Basically any cross-cutting concern of your application will require integration tests. With the recent microservice craze integration tests become even more important as you now have contracts between your own services. If those services are developed by other teams, you need an automatic way to verify that interface contracts are not broken. This can only be covered with integration tests.&lt;/p&gt;
&lt;p&gt;To sum up, unless you are creating something extremely isolated (e.g. a command line linux utility), you really &lt;strong&gt;need&lt;/strong&gt; integration tests to catch issues not caught by unit tests.&lt;/p&gt;
&lt;h3 id=&quot;anti-pattern-2---having-integration-tests-without-unit-tests&quot;&gt;Anti-Pattern 2 - Having integration tests without unit tests&lt;/h3&gt;
&lt;p&gt;This is the inverse of the previous anti-pattern. This anti-pattern is more common in large companies and large enterprise projects. Almost always the history behind this anti-pattern involves developers who believe that unit tests have no real value and only integration tests can catch regressions. There is a large majority of experienced developers who consider unit tests a waste of time. Usually if you probe them with questions, you will discover that at some point in the past, upper management had forced them to increase code coverage (See &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-6---paying-excessive-attention-to-test-coverage&quot;&gt;anti-pattern 6&lt;/a&gt;) forcing them to write trivial unit tests.&lt;/p&gt;
&lt;p&gt;It is true that in theory you &lt;em&gt;could&lt;/em&gt; have only integration tests in a software project. But in practice this would become very expensive to test (both in developer time and in build time). We saw in the table of the previous section that integration tests can also find business logic errors after each run, and so they could “replace” unit tests in that manner. But is this strategy viable in the long run?&lt;/p&gt;
&lt;h4 id=&quot;integration-tests-are-complex&quot;&gt;Integration tests are complex&lt;/h4&gt;
&lt;p&gt;Let’s look at an example. Assume that you have a service with the following 4 methods/classes/functions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/just-unit-tests.png&quot; alt=&quot;Cyclomatic complexity for 4 modules&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The number on each module denotes its &lt;a href=&quot;https://en.wikipedia.org/wiki/Cyclomatic_complexity&quot;&gt;cyclomatic complexity&lt;/a&gt; or in other words the separate code paths this module can take.&lt;/p&gt;
&lt;p&gt;Mary “by the book” Developer wants to write unit tests for this service (because she understands that unit tests &lt;em&gt;do&lt;/em&gt; have value). How many tests does she need to write in order to get full coverage of all possible scenarios?&lt;/p&gt;
&lt;p&gt;It should be obvious that one can write 2 + 5 + 3 + 2 = 12 isolated unit tests that cover fully the &lt;strong&gt;business logic&lt;/strong&gt; of these modules. Remember that this number is just for a single service, and the application Mary is working on, has multiple services.&lt;/p&gt;
&lt;p&gt;Joe “Grumpy” developer on the other hand does not believe in the value of unit tests. He thinks that unit tests are a waste of time and he decides to write only integration tests for this module. How many integration tests should he write? He starts looking at all the possible paths a request can take in that service.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/just-integration-tests.png&quot; alt=&quot;Examining code paths in a service&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Again it should be obvious that all possible scenarios of codepaths are 2 * 5 * 3 * 2 = 60. Does that mean that Joe will actually write 60 integration tests? Of course not! He will try and cheat. He will try to select a subset of integration tests that feel “representative”. This “representative” subset of tests will give him enough coverage with the minimum amount of effort.&lt;/p&gt;
&lt;p&gt;This sounds easy enough in theory, but can quickly become problematic. The reality is that these 60 code paths are not created equally. Some of them are corner cases. For example if we look at module C we see that is has 3 different code paths. One of them is a very special case, that can only be recreated if C gets a special input from component B, which is itself a corner case and can only be obtained by a special input from component A. This means that this particular scenario might require a very complex setup in order to select the inputs that will trigger the special condition on the component C.&lt;/p&gt;
&lt;p&gt;Mary on the other hand, can just recreate the corner case with a simple unit test, with no added complexity at all.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/unit-test-corner-case.png&quot; alt=&quot;Basic unit test&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Does that mean that Mary will &lt;em&gt;only&lt;/em&gt; write unit tests for this service? After all that will lead her to &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-1---having-unit-tests-without-integration-tests&quot;&gt;anti-pattern 1&lt;/a&gt;. To avoid this, she will write &lt;em&gt;both&lt;/em&gt; unit &lt;em&gt;and&lt;/em&gt; integration tests. She will keep all unit tests for the actual business logic and then she will write 1 or 2 integration tests that make sure that the rest of the system works as expected (i.e. the parts that help these modules do their job)&lt;/p&gt;
&lt;p&gt;The integration tests needed in this system should focus on the rest of the components. The business logic itself can be handled by the unit tests. Mary’s integration tests will focus on testing serialization/deserialization and with the communication to the queue and the database of the system.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/correct-integration-tests.png&quot; alt=&quot;correct Integration tests&quot; /&gt;&lt;/p&gt;
&lt;p&gt;In the end, the number of integration tests will be much smaller than the number of unit tests (matching the shape of the test pyramid described in the first section of this article).&lt;/p&gt;
&lt;h4 id=&quot;integration-tests-are-slow&quot;&gt;Integration tests are slow&lt;/h4&gt;
&lt;p&gt;The second big issue with integration tests apart from their complexity is their speed. Usually an integration test is one order of magnitute slower than a unit test. Unit tests need just the source code of the application and nothing else. They are almost always CPU bound. Integration tests on the other hand can perform I/O with external systems making them much more difficult to run in an effective manner.&lt;/p&gt;
&lt;p&gt;Just to get an idea on the difference for the running time let’s assume the following numbers.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Each unit test takes 60ms (on average)&lt;/li&gt;
&lt;li&gt;Each integration test takes 800ms (on average)&lt;/li&gt;
&lt;li&gt;The application has 40 services like the one shown in the previous section&lt;/li&gt;
&lt;li&gt;Mary is writing 10 unit tests and 2 integration tests for each service&lt;/li&gt;
&lt;li&gt;Joe is writing 12 integration tests for each service&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Now let’s do the calculations. Notice that I assume that Joe has found the perfect subset of integration tests that give him the same code coverage as Mary (which would not be true in a real application).&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Time to run&lt;/th&gt;
&lt;th&gt;Having only integration tests (Joe)&lt;/th&gt;
&lt;th&gt;Having both Unit and Integration tests (Mary)&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Just Unit tests&lt;/td&gt;
&lt;td&gt;N/A&lt;/td&gt;
&lt;td&gt;24 seconds&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Just Integration tests&lt;/td&gt;
&lt;td&gt;6.4 minutes&lt;/td&gt;
&lt;td&gt;64 seconds&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;All tests&lt;/td&gt;
&lt;td&gt;6.4 minutes&lt;/td&gt;
&lt;td&gt;1.4 minutes&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The difference in total running time is enormous. Waiting for 1 minute after each code change is vastly different than waiting for 6 minutes. The 800ms I assumed for each integration test is vastly conservative. I have seen integration test suites where a single test can take several minutes on its own.&lt;/p&gt;
&lt;p&gt;In summary, trying to use &lt;em&gt;only&lt;/em&gt; integration tests to cover business logic is a huge time sink. Even if you automate the tests with CI, your feedback loop (time from commit to getting back the test result) will be very long.&lt;/p&gt;
&lt;h4 id=&quot;integration-tests-are-harder-to-debug-than-unit-tests&quot;&gt;Integration tests are harder to debug than unit tests&lt;/h4&gt;
&lt;p&gt;The last reason why having only integration tests (without any unit tests) is an anti-pattern is the amount of time spent to debug a failed test. Since an integration test is testing multiple software components (by definition), when it breaks, the failure can come from &lt;em&gt;any&lt;/em&gt; of the tested components. Pinpointing the problem can be a hard task depending on the number of components involved.&lt;/p&gt;
&lt;p&gt;When an integration tests fails you need to be able to understand why it failed and how to fix it. The complexity and breadth of integration tests make them extremely difficult to debug. Again, as an example let’s say that your application only has integration tests. The application you are developing is the typical e-shop.&lt;/p&gt;
&lt;p&gt;A developer in your team (or even you) creates a new commit, which triggers the integration tests with the following result:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/integration-tests-break.png&quot; alt=&quot;breakage of integration tests&quot; /&gt;&lt;/p&gt;
&lt;p&gt;As a developer you look at the test result and see that the integration test named “Customer buys item” is broken. In the context of an e-shop application this is not very helpful. There are many reasons why this test might be broken.&lt;/p&gt;
&lt;p&gt;There is no way to know why the test broke without diving into the logs and metrics of the test environment (assuming that they can pinpoint the problem). In several cases (and more complex applications) the only way to truly debug an integration test is to checkout the code, recreate the test environment locally, then run the integration tests and see it fail in the local development environment.&lt;/p&gt;
&lt;p&gt;Now imagine that you work with Mary on this application so you have both integration and unit tests. Your team makes some commits, you run all the tests and get the following:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/both-tests-break.png&quot; alt=&quot;breakage of both kinds of tests&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Now two tests are broken:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;“Customer buys item” is broken as before (integration test)&lt;/li&gt;
&lt;li&gt;“Special discount test” is also broken (unit test)&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;It is now very easy to see the where to start looking for the problem. You can go directly to the source code of the &lt;em&gt;Discount&lt;/em&gt; functionality, locate the bug and fix it and in 99% of the cases the integration test will be fixed as well.&lt;/p&gt;
&lt;p&gt;Having unit tests break &lt;em&gt;before&lt;/em&gt; or &lt;em&gt;with&lt;/em&gt; integration tests is a much more painless process when you need to locate a bug.&lt;/p&gt;
&lt;h5 id=&quot;quick-summary-of-why-you-need-unit-tests&quot;&gt;Quick summary of why you need unit tests&lt;/h5&gt;
&lt;p&gt;This is the longest section of this article, but I consider it very important. In summary while &lt;em&gt;in theory&lt;/em&gt; you could only have integration tests, &lt;em&gt;in practice&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Unit tests are easier to maintain&lt;/li&gt;
&lt;li&gt;Unit tests can easily replicate corner cases and not-so-frequent scenario&lt;/li&gt;
&lt;li&gt;Unit tests run much faster than integration tests&lt;/li&gt;
&lt;li&gt;Broken unit tests are easier to fix than broken integration tests&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;If you only have integration tests, you waste developer time and company money. You need &lt;strong&gt;both&lt;/strong&gt; unit and integration tests are the same time. They are not mutually exclusive. There are several articles on the internet that advocate using only one type of tests. All these articles are misinformed. Sad but true.&lt;/p&gt;
&lt;h3 id=&quot;anti-pattern-3---having-the-wrong-kind-of-tests&quot;&gt;Anti-Pattern 3 - Having the wrong kind of tests&lt;/h3&gt;
&lt;p&gt;Now that we have seen why we need both kinds of tests (unit &lt;em&gt;and&lt;/em&gt; integration), we need to decide on &lt;em&gt;how many&lt;/em&gt; tests we need from each category.&lt;/p&gt;
&lt;p&gt;There is no hard and fast rule here, it depends on your application. The important point is that you need to spend some time to understand what type of tests add the most value to &lt;em&gt;your&lt;/em&gt; application. The test pyramid is only a suggestion on the amount of tests that you should create. It assumes that you are writing a commercial web application, but that is not always the case. Let’s see some examples:&lt;/p&gt;
&lt;h4 id=&quot;example---linux-command-line-utility&quot;&gt;Example - Linux command line utility&lt;/h4&gt;
&lt;p&gt;Your application is a command line utility. It reads one special format of a file (let’s say a CSV) and exports another format (let’s say JSON) after doing some transformations. The application is self-contained, does not communicate with any other system or use the network. The transformations are complex mathematical processes that are critical for the correct functionality of the application (it should always be correct even if it slow).&lt;/p&gt;
&lt;p&gt;In this contrived example you would need:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Lots and lots of unit tests for the mathematical equations.&lt;/li&gt;
&lt;li&gt;Some integration tests for the CSV reading and JSON writing&lt;/li&gt;
&lt;li&gt;No GUI tests because there is no GUI.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Here is the breakdown of tests for this project:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/pyramid1.png&quot; alt=&quot;Test pyramid example&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Unit tests dominate in this example and the shape is &lt;strong&gt;not&lt;/strong&gt; a pyramid.&lt;/p&gt;
&lt;h4 id=&quot;example---payment-management&quot;&gt;Example - Payment Management&lt;/h4&gt;
&lt;p&gt;You are adding a new application that will be inserted into an existing big collection of enterprise systems. The application is a payment gateway that processes payment information for an external system. This new application should keep a log of all transactions to an external DB, it should communicate with external payment providers (e.g. Paypal, Stripe, WorldPay) and it should also send payment details to another system that prepares invoices.&lt;/p&gt;
&lt;p&gt;In this contrived example you would need&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Almost no unit tests because there is no business logic&lt;/li&gt;
&lt;li&gt;Lots and lots of integration tests for the external communications, the db storage, the invoice system&lt;/li&gt;
&lt;li&gt;No UI Tests because there is a no UI&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Here is the breakdown of tests for this project:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/pyramid2.png&quot; alt=&quot;Test pyramid example&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Integrations tests dominate in this example and the shape is &lt;strong&gt;not&lt;/strong&gt; a pyramid.&lt;/p&gt;
&lt;h4 id=&quot;example---website-creator&quot;&gt;Example - Website creator&lt;/h4&gt;
&lt;p&gt;You are working on this brand new startup that will revolutionize the way people create websites, by offering a one-of-a-kind way to create web applications from within the browser.&lt;/p&gt;
&lt;p&gt;The application is a graphical designer with a toolbox of all the possible HTML elements that can be added on a web page along with library of premade templates. There is also the ability to get new templates from a marketplace. The website creator works in a very friendly way by allowing you to drag and drop components on the page, resize them, edit their properties and change their colors and appearance.&lt;/p&gt;
&lt;p&gt;In this contrived example you would need&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Almost no unit tests because there is no business logic&lt;/li&gt;
&lt;li&gt;Some integration tests for the marketplace&lt;/li&gt;
&lt;li&gt;Lots and lots of UI tests that make sure the user experience is as advertised&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Here is the breakdown of tests for this project:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/pyramid3.png&quot; alt=&quot;Test pyramid example&quot; /&gt;&lt;/p&gt;
&lt;p&gt;UI tests dominate here and the shape is &lt;strong&gt;not&lt;/strong&gt; a pyramid.&lt;/p&gt;
&lt;p&gt;I used some extreme examples to illustrate the point that you need to understand what your application needs and focus only on the tests that give you value. I have personally seen “payment management” applications with no integration tests and “website creator” applications with no UI tests.&lt;/p&gt;
&lt;p&gt;There are several articles on the web (I am not going to link them) that talk about a specific amount on integration/unit/UI tests that you need or don’t need. All these articles are based on assumptions that may &lt;em&gt;not&lt;/em&gt; be true in your case.&lt;/p&gt;
&lt;h3 id=&quot;anti-pattern-4---testing-the-wrong-functionality&quot;&gt;Anti-Pattern 4 - Testing the wrong functionality&lt;/h3&gt;
&lt;p&gt;In the previous sections we have outlined the types and amount of tests you need to have for your application. The next logical step is to explain what functionality you actually need to test.&lt;/p&gt;
&lt;p&gt;In theory, getting 100% code coverage in an application is the ultimate goal. In practice this goal is not only difficult to achieve but also it doesn’t guarantee a bug free application.&lt;/p&gt;
&lt;p&gt;There are some cases where indeed it is possible to test &lt;em&gt;all&lt;/em&gt; functionality of your application. If you start on a green-field project, if you work in a small team that is well behaved and takes into account the effort required for tests, it is perfectly fine to write new tests for all new functionality you add (because the existing code already has tests).&lt;/p&gt;
&lt;p&gt;But not all developers are lucky like this. In most cases you inherit an existing application that has a minimal amount of tests (or even none!). If you are part of a big and established company, working with legacy code is mostly the rule rather than the exception.&lt;/p&gt;
&lt;p&gt;Ideally you would have enough development time to write tests for both new and existing code for a legacy application. This is a romantic idea that will probably be rejected by the average project manager who is mostly interested on adding new features rather then testing/refactoring. You have to pick your battles and find a fine balance between adding new functionality (as requested by the business) and expanding the existing test suite.&lt;/p&gt;
&lt;p&gt;So what do you test? Where do you focus your efforts? Several times I have seen developers wasting valuable testing time by writing “unit tests” that add little or no value to the overall stability of the application. The canonical example of useless testing is trivial tests that verify the application data model.&lt;/p&gt;
&lt;p&gt;Code coverage is analyzed in detail in its own &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-6---paying-excessive-attention-to-test-coverage&quot;&gt;anti-pattern&lt;/a&gt; section. In the present section however we will talk about code “severity” and how it relates to your tests.&lt;/p&gt;
&lt;p&gt;If you ask any developer to show you the source code of any application, he/she will probably open an IDE or code repository browser and show you the individual folders.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/source-code-physical.png&quot; alt=&quot;Source code physical model&quot; /&gt;&lt;/p&gt;
&lt;p&gt;This representation is the physical model of the code. It defines the folders in the filesystem that contain the source code. While this hierarchy of folders is great for working with the code itself, unfortunately it doesn’t define the importance of each code folder. A flat list of code folders implies that all code components contained in them are of equal importance.&lt;/p&gt;
&lt;p&gt;This is not true as different code components have a different impact in the overall functionality of the application. As a quick example let’s say that you are writing an eshop application and two bugs appear in production:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Customers cannot check-out their cart halting all sales&lt;/li&gt;
&lt;li&gt;Customers get wrong recommendations when they browse products.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Even though both bugs should be fixed, it is obvious that the first one has higher priority. Therefore if you inherit an eshop application with zero tests, you should write new tests the directly validate the check-out functionality rather than the recommendation engine. Despite the fact that the recommendation engine and the check-out process might exist on sibling folders in the filesystem, their importance is different when it comes to testing.&lt;/p&gt;
&lt;p&gt;To generalize this example, if you work for some time in any medium/large application you will soon need to think about code using a different representation - the mental model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/code-mental-model.png&quot; alt=&quot;Source code mental model&quot; /&gt;&lt;/p&gt;
&lt;p&gt;I am showing here 3 layers of code, but depending on the size of your application it might have more. These are:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Critical code - This is the code that breaks often, gets most of new features and has a big impact on application users&lt;/li&gt;
&lt;li&gt;Core code - This is the code that breaks sometimes, gets few new features and has medium impact on the application users&lt;/li&gt;
&lt;li&gt;Other code - This is code that rarely changes, rarely gets new features and has minimal impact on application users.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;This mental mode should be your guiding principle whenever you write a new software test. Ask yourself if the functionality you are writing tests for now belongs to the &lt;em&gt;critical&lt;/em&gt; or &lt;em&gt;core&lt;/em&gt; categories. If yes, then write a software test. If no, then maybe your development time should better be spent elsewhere (e.g. in another bug).&lt;/p&gt;
&lt;p&gt;The concept of having code with different severity categories is also great when you need to answer the age old question of how much code coverage is enough for an application. To answer this question you need to either know the severity layers of the application or ask somebody that does. Once you have this information at hand the answer is obvious:&lt;/p&gt;
&lt;p&gt;Try to write tests that work towards 100% coverage &lt;strong&gt;of critical code&lt;/strong&gt;. If you have already done this, then try to write tests that work towards 100% &lt;strong&gt;of core code&lt;/strong&gt;. Trying however to get 100% coverage on &lt;em&gt;total&lt;/em&gt; code &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-6---paying-excessive-attention-to-test-coverage&quot;&gt;is not recommended&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The important thing to notice here is that the critical code in an application is always a small subset of the overall code. So if in an application critical code is let’s say 20% of the overall code, then getting just 20% overall code coverage is a good first step for reducing bugs in production.&lt;/p&gt;
&lt;p&gt;In summary, write unit and integration tests for code that&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;breaks often&lt;/li&gt;
&lt;li&gt;changes often&lt;/li&gt;
&lt;li&gt;is critical to the business&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;If you have the time luxury to further expand the test suite, make sure that you understand the diminishing returns before wasting time on tests with little or no value.&lt;/p&gt;
&lt;h3 id=&quot;anti-pattern-5---testing-internal-implementation&quot;&gt;Anti-Pattern 5 - Testing internal implementation&lt;/h3&gt;
&lt;p&gt;More tests are always a good thing. Right?&lt;/p&gt;
&lt;p&gt;Wrong! You also need to make sure that the tests are actually structured in a correct way. Having tests that are written in the wrong manner is bad in two ways.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;They waste precious development time the first time they are written&lt;/li&gt;
&lt;li&gt;They waste even more time when they need to be refactored (when a new feature is added)&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Strictly speaking, &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-9---treating-test-code-as-a-second-class-citizen&quot;&gt;test code is like any other type of code&lt;/a&gt;. You will need to refactor it at some point in order to improve it in a gradual way. But if you find yourself routinely changing existing tests just to make them pass when a new feature is added then &lt;em&gt;your tests are not testing what they should be testing&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;I have seen several companies that started new projects and thinking that they will get it right this time, they started writing a &lt;em&gt;big&lt;/em&gt; number of tests to cover the functionality of the application. After a while, a new feature got added and several existing tests needed to change in order to make them pass again. Then another new feature was added and &lt;em&gt;more&lt;/em&gt; tests needed to be updated. Soon the amount of effort spent refactoring/fixing the existing tests was actually larger than the time needed to implement the feature itself.&lt;/p&gt;
&lt;p&gt;In such situations, several developers just accept defeat. They declare software tests a waste of time and abandon completely the existing test suite in order to focus fully on new features. In some extreme scenarios some changes might even be held back because of the amount of tests that break.&lt;/p&gt;
&lt;p&gt;The problem here is of course the bad quality of tests. Tests that need to be refactored all the time suffer from tight coupling with the main code. Unfortunately, you need some basic testing experience to understand which tests are written in this “wrong” way.&lt;/p&gt;
&lt;p&gt;Having to change a big number of existing tests when a new feature is introduced shows the &lt;em&gt;symptom&lt;/em&gt;. The actual problem is that tests were instructed to verify internal implementation which is always a recipe for disaster. There are several software testing resources online that attempt to explain this concept, but very few of them show some solid examples.&lt;/p&gt;
&lt;p&gt;I promised in the beginning of this article that I will not speak about a particular programming language and I intend to keep that promise. In this section the illustrations show the data structure of your favorite programming language. Think of them as structs/objects/classes that contain fields/values.&lt;/p&gt;
&lt;p&gt;Let’s say that the customer object in an e-shop application is the following:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/coupled-testing.png&quot; alt=&quot;Tight coupling of tests&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The customer type has only two values where &lt;code class=&quot;highlighter-rouge&quot;&gt;0&lt;/code&gt; means “guest user” and &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt; means “registered user”. Developers look at the object and write 10 unit tests that verify various cases of guests users and 10 cases of registered user. And when I say “verify” I mean that tests &lt;strong&gt;are looking at this particular field in this particular object&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Time passes by and business decides that a new customer type with value &lt;code class=&quot;highlighter-rouge&quot;&gt;2&lt;/code&gt; is needed for affiliates. Developers add 10 more tests that deal with affiliates. Finally another type of user called “premium customer” is added and developers add 10 more tests.&lt;/p&gt;
&lt;p&gt;At this point, we have 40 tests in 4 categories that all look at this particular field. (These numbers are imaginary. This contrived example exists only for demonstration purposes. In a real project you might have 10 interconnected fields within 6 nested objects and 200 tests).&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/coupled-testing-example.png&quot; alt=&quot;Tight coupling of tests example&quot; /&gt;&lt;/p&gt;
&lt;p&gt;If you are a seasoned developer you can always imagine what happens next. New requirements come that say:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;For registered users, their email should also be stored&lt;/li&gt;
&lt;li&gt;For affiliate users, their company should also be stored&lt;/li&gt;
&lt;li&gt;Premium users can now gather reward points.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;The customer object now changes as below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/coupled-testing-broken.png&quot; alt=&quot;Tight coupling of tests broken&quot; /&gt;&lt;/p&gt;
&lt;p&gt;You now have 4 objects connected with foreign keys and all 40 tests are instantly broken because the field they were checking no longer exists.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Of course in this trivial example one could simply keep the existing field to not break backwards compatibility with tests&lt;/em&gt;. In a real application this is not always possible. Sometimes backwards compatibility might essentially mean that you need to keep both old and new code (before/after the new feature) resulting in a huge bloat. Also notice that having to keep old code around just to make unit tests pass is a huge anti-pattern on its own.&lt;/p&gt;
&lt;p&gt;In a real application when this happens, developers ask from management some extra time to fix the tests. Project managers then declare that unit testing is a waste of time because they seem to hinder new features. The whole team then abandons the test suite by quickly disabling the failing tests.&lt;/p&gt;
&lt;p&gt;The big problem here is not testing, but instead the way the tests were constructed. Instead of testing internal implementation they should instead expected behavior. In our simple example instead of testing directly the internal structure of the customer they should instead check the exact business requirement of each case. Here is how these same tests should be handled instead.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/coupled-testing-fixed.png&quot; alt=&quot;Tests that test behavior&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The tests do not really care about the internal structure of the customer object. They only care about its interactions with other objects/methods/functions. The other objects/method/functions should be mocked when needed on a case to case basis. Notice that each type of tests directly maps to a business need rather than a technical implementation (which is always a good practice.)&lt;/p&gt;
&lt;p&gt;If the internal implementation of the &lt;em&gt;Customer&lt;/em&gt; object changes, the verification code of the tests remains the same. The only thing that might change is the setup code for each test, which should be centralized in a single helper function called &lt;code class=&quot;highlighter-rouge&quot;&gt;createSampleCustomer()&lt;/code&gt; or something similar (more on this in &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-9---treating-test-code-as-a-second-class-citizen&quot;&gt;AntiPattern 9&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Of course in theory it is possible for the verified objects themselves to change. In practice it is not realistic for changes to happen at &lt;code class=&quot;highlighter-rouge&quot;&gt;loginAsGuest()&lt;/code&gt; &lt;em&gt;and&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;register()&lt;/code&gt; &lt;em&gt;and&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;showAffiliateSales()&lt;/code&gt; &lt;em&gt;and&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;getPremiumDiscount()&lt;/code&gt; &lt;strong&gt;at the same time&lt;/strong&gt;. In a realistic scenario you would have to refactor 10 tests instead of 40.&lt;/p&gt;
&lt;p&gt;In summary, if you find yourself continuously fixing existing tests as you add new features, it means that your tests are tightly coupled to internal implementation.&lt;/p&gt;
&lt;h3 id=&quot;anti-pattern-6---paying-excessive-attention-to-test-coverage&quot;&gt;Anti-Pattern 6 - Paying excessive attention to test coverage&lt;/h3&gt;
&lt;p&gt;Code coverage is a favorite metric among software stakeholders. &lt;a href=&quot;https://softwareengineering.stackexchange.com/questions/1380/how-much-code-coverage-is-enough&quot;&gt;Endless discussions&lt;/a&gt; &lt;a href=&quot;https://martinfowler.com/bliki/TestCoverage.html&quot;&gt;have&lt;/a&gt; &lt;a href=&quot;https://testing.googleblog.com/2010/07/code-coverage-goal-80-and-no-less.html&quot;&gt;happened&lt;/a&gt; (and will continue to happen) among developers and project managers on the amount of code coverage a project needs.&lt;/p&gt;
&lt;p&gt;The reason why everybody likes to talk about code coverage is because it is a metric that is easy to understand and quantify. There are several easily accessible tools that output this metric for most programming languages and test frameworks.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Let me tell you a little secret:&lt;/em&gt; Code coverage is completely useless as a metric. There is no “correct” code coverage number. This is a trap question. You can have a project with 100% code coverage that still has bugs and problems. The real metrics that you should monitor are the well-known CTM.&lt;/p&gt;
&lt;h5 id=&quot;the-codepipes-testing-metrics-ctm&quot;&gt;The Codepipes Testing Metrics (CTM)&lt;/h5&gt;
&lt;p&gt;Here is their definition if you have never seen them before:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Metric Name&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Ideal value&lt;/th&gt;
&lt;th&gt;Usual value&lt;/th&gt;
&lt;th&gt;Problematic value&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody readability=&quot;4&quot;&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;PDWT&lt;/td&gt;
&lt;td&gt;% of Developers writing tests&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;td&gt;20%-70%&lt;/td&gt;
&lt;td&gt;Anything less than 100%&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;PBCNT&lt;/td&gt;
&lt;td&gt;% of bugs that create new tests&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;td&gt;0%-5%&lt;/td&gt;
&lt;td&gt;Anything less than 100%&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;PTVB&lt;/td&gt;
&lt;td&gt;% of tests that verify behavior&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;td&gt;10%&lt;/td&gt;
&lt;td&gt;Anything less than 100%&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;PTD&lt;/td&gt;
&lt;td&gt;% of tests that are deterministic&lt;/td&gt;
&lt;td&gt;100%&lt;/td&gt;
&lt;td&gt;50%-80%&lt;/td&gt;
&lt;td&gt;Anything less than 100%&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;strong&gt;PDWT&lt;/strong&gt; (Percent of Developers who Write Tests) is probably the most important metric of all. There is no point in talking about software testing anti-patterns if you have zero tests in the first place. All developers in the team should write tests. A new feature should be declared &lt;em&gt;done&lt;/em&gt; only when it is accompanied by one or more tests.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PBCNT&lt;/strong&gt; (Percent of Bugs that Create New tests). Every bug that slips into production is a great excuse for writing a new software test that verifies the respective fix. A bug that appears in production should only appear once. If your project suffers from bugs that appear multiple times in production even after their original “fix”, then your team will really benefit from this metric. More details on this topic in &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-10---not-converting-production-bugs-to-tests&quot;&gt;Antipattern 10&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PTVB&lt;/strong&gt; (Percent of Tests that Verify Behavior and not implementation). Tightly coupled tests are a huge time sink when the main code is refactored. This topic was already discussed in &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-5---testing-internal-implementation&quot;&gt;Antipattern 5&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PTD&lt;/strong&gt; (Percent of Tests that are Deterministic to total tests). Tests should only fail when something is wrong with the business code. Having tests that fail intermittently for no apparent reason is a huge problem that is discussed in &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-7---having-flaky-or-slow-tests&quot;&gt;Antipattern 7&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If after reading about these metrics, you still insist on setting a hard number as a goal for code coverage, I will give you the number &lt;strong&gt;20%&lt;/strong&gt;. This number should be used as a rule of thumb and it is based on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Pareto_principle&quot;&gt;Pareto principle&lt;/a&gt;. 20% of your code is causing 80% of your bugs, so if you really want to start writing tests you could do well by starting with that code first. This advice also ties well with &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-4---testing-the-wrong-functionality&quot;&gt;Anti-pattern 4&lt;/a&gt; where I suggest that you should write tests for your critical code first.&lt;/p&gt;
&lt;p&gt;Do &lt;em&gt;not&lt;/em&gt; try to achieve 100% total code coverage. Achieving 100% code coverage sounds good in theory but almost always is a waste of time:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;you have wasted a lost of effort as getting from 80% to 100% is much more difficult than getting from 0% to 20%&lt;/li&gt;
&lt;li&gt;Increasing code coverage has diminishing returns&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;In any non trivial application there are certain scenarios that needs complex unit tests in order to trigger. The effort required to write these tests will usually be more than the risk involved if these particular scenarios ever fail in production (if ever).&lt;/p&gt;
&lt;p&gt;If you have worked with any big application you should know by now that after reaching 70% or 80% code coverage, it is getting very hard to write useful tests for the code that is still untested.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/code-coverage-effort.png&quot; alt=&quot;Code Coverage Effort&quot; /&gt;&lt;/p&gt;
&lt;p&gt;On a similar note, as we already saw in the section for &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-4---testing-the-wrong-functionality&quot;&gt;Antipattern 4&lt;/a&gt;, there are some code paths that never actually fail in production, and therefore writing tests for them is not recommended. The time spent on getting them covered should be better spent on actual features.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/code-coverage-value.jpg&quot; alt=&quot;Code Coverage Value&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Projects that need a specific code coverage percentage as a delivery requirement usually force developers to test trivial code in order or write tests that just verify the underlying programming language. This is a huge waste of time and as a developer you have the duty to complain to management who has such unreasonable demands.&lt;/p&gt;
&lt;p&gt;In summary, code coverage is a metric that should &lt;strong&gt;not&lt;/strong&gt; be used as a representation for quality of a software project.&lt;/p&gt;
&lt;h3 id=&quot;anti-pattern-7---having-flaky-or-slow-tests&quot;&gt;Anti-Pattern 7 - Having flaky or slow tests&lt;/h3&gt;
&lt;p&gt;This particular anti-pattern has &lt;a href=&quot;https://martinfowler.com/articles/nonDeterminism.html&quot;&gt;already&lt;/a&gt; &lt;a href=&quot;https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html&quot;&gt;been&lt;/a&gt; &lt;a href=&quot;https://testing.googleblog.com/2017/04/where-do-our-flaky-tests-come-from.html&quot;&gt;documented&lt;/a&gt; &lt;a href=&quot;https://semaphoreci.com/community/tutorials/how-to-deal-with-and-eliminate-flaky-tests&quot;&gt;heavily&lt;/a&gt; so I am just including it here for completeness.&lt;/p&gt;
&lt;p&gt;Since software tests act as an early warning against regressions, they should always work in a reliable way. A failing test should be a cause of concern and the person(s) that triggered the respective build should investigate why the test failed right away.&lt;/p&gt;
&lt;p&gt;This approach can only work with tests that fail in a deterministic manner. A test that sometimes fails and sometimes passes (without any code changes in between) is unreliable and undermines the whole testing suite. The negative effects are two fold&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Developers do not trust tests anymore and soon ignore them&lt;/li&gt;
&lt;li&gt;Even when non-flaky tests actually fail, it is hard to detect them in a sea of flaky tests&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;A failing test should be easily recognizable by everybody in your team as it changes the status of the whole build. On the other hand if you have flaky tests it is hard to understand if new failures are truly new or they stem from the existing flaky tests.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/flaky-tests.png&quot; alt=&quot;Flaky tests&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Even a small number of flaky tests in enough to destroy the credibility of the rest of test suite. If you have 5 flaky tests for example, run the build and get 3 failures it is not immediately evident if everything is fine (because the failures were coming from the flaky tests) or if you just introduced 3 regressions.&lt;/p&gt;
&lt;p&gt;A similar problem is having tests that are really really slow. Developers need a quick feedback on the result of each commit (also discussed in the next section) so slow tests will eventually be ignored or even not run at all.&lt;/p&gt;
&lt;p&gt;In practice flaky and slow tests are almost always integration tests and/or UI tests. As we go up in the testing pyramid, the probabilities of flaky tests are greatly increasing. Tests that deal with browser events are notoriously hard to get right all the time. Flakiness in integration tests can come from many factors but the usual suspect is the test environment and its requirements.&lt;/p&gt;
&lt;p&gt;The primary defense against flaky and slow tests is to isolate them in their own test suite (assuming that they are not fixable). You can easily find more abundant resources on how to fix flaky tests for your favorite programming language by searching online so there is no point in me explaining the fixes here.&lt;/p&gt;
&lt;p&gt;In summary, you should have a reliable test suite (even if it is a subset of the whole test suite) that is rock solid. A test that fails in this suite means that something is really really wrong with the code and any failure means that the code must not be promoted to production.&lt;/p&gt;
&lt;h3 id=&quot;anti-pattern-8---running-tests-manually&quot;&gt;Anti-Pattern 8 - Running tests manually&lt;/h3&gt;
&lt;p&gt;Depending on your organization you might actually have several types of tests in place. Unit tests, Load tests, User acceptance tests are common categories of test suites that &lt;em&gt;might&lt;/em&gt; be executed before the code goes into production.&lt;/p&gt;
&lt;p&gt;Ideally all your tests should run automatically without any human intervention. If that is not possible at the very least all tests that deal with correctness of code (i.e. unit and integration tests) &lt;strong&gt;must&lt;/strong&gt; run in an automatic manner. This way developers get feedback on the code in the most timely manner. It is very easy to fix a feature when the code is fresh in your mind and you haven’t switched context yet to an unrelated feature.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/test-feedback.png&quot; alt=&quot;Test feedback loop tests&quot; /&gt;&lt;/p&gt;
&lt;p&gt;In the past the most lengthy step of the software lifecycle was the deployment of the application. With the move into cloud infrastructure where machines can be created on demand (either in the form of VMs or containers) the time to provision a new machine has been reduced to minutes or seconds. This paradigm shift has caught a lot of companies by surprise as they were not ready to handle daily or even hourly deployments. Most of the existing practices were centered around lengthy release cycles. Waiting for a specific time in the release to “pass QA” with manual approval is one of those obsolete practices that is no longer applicable if a company wants to deploy as fast as possible.&lt;/p&gt;
&lt;p&gt;Deploying as fast as possible implies that you trust each deployment. Trusting an automatic deployment requires a high degree of confidence in the code that gets deployed. While there are several ways of getting this confidence, the first line of defense should be your software tests. However, having a test suite that can catch regressions quickly is only half part of the equation. The other half is running the tests automatically (possibly after every commit).&lt;/p&gt;
&lt;p&gt;A lot of companies &lt;em&gt;think&lt;/em&gt; that they practice continuous delivery and/or deployment. In reality they don’t. Practicing true CI/CD means that &lt;em&gt;at any given point in time&lt;/em&gt; there is a version of the code that is ready to be deployed. This means that the candidate release for deployment the candidate release is &lt;em&gt;already&lt;/em&gt; tested. Therefore having a package version of an application “ready” which has not really “passed QA” is not true CI/CD.&lt;/p&gt;
&lt;p&gt;Unfortunately, while most companies have correctly realized that deployments should be automated, because using humans for them is error prone and slow, I still see companies where launching the tests is a semi-manual process. And when I say semi-manual I mean that even though the test suite itself might be automated, there are human tasks for house-keeping such as preparing the test environment or cleaning up the test data after the tests have finished. That is an anti-pattern because it is not true automation. &lt;strong&gt;All&lt;/strong&gt; aspects of testing should be automated.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://blog.codepipes.com/assets/testing-anti-patterns/automated-tests.png&quot; alt=&quot;Automated tests&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Having access to VMs or containers means that it is very easy to create various test environments on demand. Creating a test environment on the fly for an individual pull request should be a standard practice within your organization. This means that each new feature is tested individually on its own. A problematic feature (i.e. that causes tests to fail) should not block the release of the rest of the features that need to be deployed at the same time.&lt;/p&gt;
&lt;p&gt;An easy way to understand the level of test automation within a company is to watch the QA/Test people in their daily job. In the ideal case, testers are just creating new tests that are added to an existing test suite. Testers themselves do not run tests manually. The test suite is run by the build server.&lt;/p&gt;
&lt;p&gt;In summary, testing should be something that happens all the time behind the scenes by the build server. Developers should learn the result of the test for their individual feature after 5-15 minutes of committing code. Testers should create new tests and refactor existing ones, instead of actually running tests.&lt;/p&gt;
&lt;h3 id=&quot;anti-pattern-9---treating-test-code-as-a-second-class-citizen&quot;&gt;Anti-Pattern 9 - Treating test code as a second class citizen&lt;/h3&gt;
&lt;p&gt;If you are a seasoned developer, you will spend always some time to structure new code in your mind before implementing it. There are several philosophies regarding code design and some of them are so significant that have their own Wikipedia entry. Some examples are:&lt;/p&gt;
&lt;p&gt;The first one is arguably the most important one as it forces you to have a single source of truth for code that is reused across multiple features. Depending on your own programming language you may also have access to several other best practices and recommended design patterns. You might even have special guidelines that are specific to your team.&lt;/p&gt;
&lt;p&gt;Yet, for some unknown reason several developers do not apply the same principles to the code that holds the software tests. I have seen projects which have well designed feature code, but suffer from tests with huge code duplication, hardcoded variables, copy-paste segments and several other inefficiencies that would be considered inexcusable if found on the main code.&lt;/p&gt;
&lt;p&gt;Treating test code as a second class citizen makes no sense, because in the long run all code needs maintenance. Tests will need to be updated and refactored in the future. Their variables and structure will need to change. If you write tests without thinking about their design you are creating additional technical debt that will be added to the one already present in the main code.&lt;/p&gt;
&lt;p&gt;Try to design your tests with the same attention that you give to the feature code. All common refactoring techniques should be used on tests as well. As a starting point&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;All test creation code should be centralized. All tests should create test data in the same manner&lt;/li&gt;
&lt;li&gt;Complex verification segments should be extracted in a common domain specific library&lt;/li&gt;
&lt;li&gt;Mocks and stubs that are used too many times should not be copied-pasted.&lt;/li&gt;
&lt;li&gt;Test initialization code should be shared between similar tests.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;If you employ tools for static analysis, source formatting or code quality then configure them to run on test code, too.&lt;/p&gt;
&lt;p&gt;In summary, design your tests with the same detail that you design the main feature code.&lt;/p&gt;
&lt;h3 id=&quot;anti-pattern-10---not-converting-production-bugs-to-tests&quot;&gt;Anti-Pattern 10 - Not converting production bugs to tests&lt;/h3&gt;
&lt;p&gt;One of the goals of testing is to catch regressions. As we have seen in &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-4---testing-the-wrong-functionality&quot;&gt;antipattern 4&lt;/a&gt;, most applications have a “critical” code part where the majority of bugs appear. When you fix a bug you need to make sure that it doesn’t happen again. One of the best ways to enforce this is to write a test for the fix (either unit or integration or both).&lt;/p&gt;
&lt;p&gt;Bugs that slip into production are perfect candidates for writing software tests&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;they show a lack of testing in that area as the bug has already passed into production&lt;/li&gt;
&lt;li&gt;if you write a test for these bugs the test will be very valuable as it guards future releases of the software&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;I am always amazed when I see teams (that otherwise have a sound testing strategy) that don’t write a test for a bug that was found &lt;em&gt;in production&lt;/em&gt;. They correct the code and fix the bug straight away. For some strange reason a lot of developers assume that writing tests is only valuable when you are adding a new feature only.&lt;/p&gt;
&lt;p&gt;This could not be further from the truth. I would even argue that software tests that stem from actual bugs are more valuable than tests which are added as part of new development. After all you never know how often a new feature will break in production (maybe it belongs to non-critical code that will never break). The respective software test is good to have but its value is questionable.&lt;/p&gt;
&lt;p&gt;On the other hand the software test that you write for a real bug is super valuable. Not only it verifies that your fix is correct, but also ensures that your fix will always be active even if refactorings happen in the same area.&lt;/p&gt;
&lt;p&gt;If you join a legacy project that has no tests this is also the most obvious way to start getting value from software testing. Rather than attempting to guess which code needs tests, you should pay attention to the existing bugs and try to cover them with tests. After a while your tests will have covered the critical part of the code, since by definition all tests have verified things that break often. One of &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#the-codepipes-testing-metrics-ctm&quot;&gt;my suggested metrics&lt;/a&gt; embodies the recording of this effort.&lt;/p&gt;
&lt;p&gt;The only case where it is acceptable to &lt;strong&gt;not&lt;/strong&gt; write tests is when bugs that you find in production are unrelated to code and instead stem from the environment itself. A misconfiguration to a load balancer for example is not something that can be solved with a unit test.&lt;/p&gt;
&lt;p&gt;In summary, if you are unsure on what code you need to test next, look at the bugs that slip into production.&lt;/p&gt;
&lt;h3 id=&quot;anti-pattern-11---treating-tdd-as-a-religion&quot;&gt;Anti-Pattern 11 - Treating TDD as a religion&lt;/h3&gt;
&lt;p&gt;TDD stands for &lt;a href=&quot;https://en.wikipedia.org/wiki/Test-driven_development&quot;&gt;Test Driven Development&lt;/a&gt; and like all methodologies before it, it is a good idea on paper until consultants try to convince a company that following TDD blindly is the only way forward. At the time or writing this trend is slowly dying but I decided to mention it here for completeness (as the enterprise world is especially suffering from this anti-pattern).&lt;/p&gt;
&lt;p&gt;Broadly speaking when it comes to software tests:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;you can write tests &lt;em&gt;before&lt;/em&gt; the respective implementation code&lt;/li&gt;
&lt;li&gt;you can write tests &lt;em&gt;at the same time&lt;/em&gt; as the implementation code&lt;/li&gt;
&lt;li&gt;you can write tests &lt;em&gt;after&lt;/em&gt; the implementation code&lt;/li&gt;
&lt;li&gt;you can write 0 tests (i.e. never) for the implementation code&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;One of the core tenets of TDD is always following option 1 (writing tests before the implementation code). Writing tests before the code is a good general practice but is certainly not always the &lt;em&gt;best&lt;/em&gt; practice.&lt;/p&gt;
&lt;p&gt;Writing tests before the implementation code implies that you are certain about your final API, which may or may not be the case. Maybe you have a clear specification document in front of you and thus know the exact signatures of the code methods that need to be implemented. But in other cases you might want to just experiment on something, do a quick spike and work &lt;strong&gt;towards&lt;/strong&gt; a solution instead of a solution itself.&lt;/p&gt;
&lt;p&gt;For a more practical example, it would be immature for a startup to follow blindly TDD. If you work in a startup company you might write code that will change so fast that TDD will not be a big help. You might even throw away code until you get it “right”. Writing tests &lt;em&gt;after&lt;/em&gt; the implementation code, is a perfectly valid strategy in that case.&lt;/p&gt;
&lt;p&gt;Writing no tests at all (option 4) is also a valid strategy. As we have seen in &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-4---testing-the-wrong-functionality&quot;&gt;anti-pattern 4&lt;/a&gt; there is code that never needs testing. Writing software tests for trivial code because this is the correct way to “do TDD” will get you nowhere.&lt;/p&gt;
&lt;p&gt;The obsession of TDD zealots on writing tests first no matter the case, has been a huge detriment to the &lt;a href=&quot;https://softwareengineering.stackexchange.com/questions/98485/tdd-negative-experience&quot;&gt;mental health of sane developers&lt;/a&gt;. This obsession is already documented in various places so hopefully I don’t need to say anything more on the topic (search for “TDD is crap/stupid/dead”).&lt;/p&gt;
&lt;p&gt;At this point I would like to admit that several times I have personally implemented code like this:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Implementing the main feature first&lt;/li&gt;
&lt;li&gt;Writing the test afterwards&lt;/li&gt;
&lt;li&gt;Running the test to see it succeed&lt;/li&gt;
&lt;li&gt;Commenting out critical parts of the feature code&lt;/li&gt;
&lt;li&gt;Running the test to see it fail&lt;/li&gt;
&lt;li&gt;Uncommenting feature code to its original state&lt;/li&gt;
&lt;li&gt;Running the test to see it succeed again&lt;/li&gt;
&lt;li&gt;Commiting the code&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;In summary, TDD is a good idea but you don’t have to follow it all the time. If you work in a fortune 500 company, surrounded by business analysts and getting clear specs on what you need to implement, then TDD &lt;em&gt;might&lt;/em&gt; be helpful.&lt;/p&gt;
&lt;p&gt;On the other hand if you are just playing with a new framework at your house during the weekend and want to understand how it works, then feel free to &lt;strong&gt;not&lt;/strong&gt; follow TDD.&lt;/p&gt;
&lt;h3 id=&quot;anti-pattern-12---writing-tests-without-reading-documentation-first&quot;&gt;Anti-Pattern 12 - Writing tests without reading documentation first&lt;/h3&gt;
&lt;p&gt;A professional developer is one who knows the tools of the trade. You might need to spend extra time at the beginning of a project to learn about the technologies you are going to use. Web frameworks are coming out all the time and it always pays off to know all the capabilities that can be employed in order to write effective and concise code.&lt;/p&gt;
&lt;p&gt;You should treat software tests with the same respect. Because several developers treat tests as something secondary (see also &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-9---treating-test-code-as-a-second-class-citizen&quot;&gt;Anti-pattern 9&lt;/a&gt;) they never sit down to actually learn what their testing framework can do. Copy-pasting testing code from other projects and examples might seem to work at first glance, but this is not the way a professional should behave.&lt;/p&gt;
&lt;p&gt;Unfortunately this pattern happens all too often. People are writing several “helper functions” and “utilities” for tests without realizing that their testing framework already offers this function either in a built-in manner or with the help of external modules.&lt;/p&gt;
&lt;p&gt;These utilities make the tests hard to understand (especially for junior developers) as they are filled with in-house knowledge that is non transferable to other projects/companies. Several times I have replaced “smart in-house testing solutions” with standard off-the-shelf libraries that do the same thing in a standardized manner.&lt;/p&gt;
&lt;p&gt;You should spend some time to learn what your testing framework can do. For example try to find how it can work with:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;parameterized tests&lt;/li&gt;
&lt;li&gt;mocks and stubs&lt;/li&gt;
&lt;li&gt;test setup and teardown&lt;/li&gt;
&lt;li&gt;test categorization&lt;/li&gt;
&lt;li&gt;conditional running of tests&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;If you are also working on the stereotypical web application you should do some minimal research to understand what are the best practices regarding&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;test data creators&lt;/li&gt;
&lt;li&gt;HTTP client libraries&lt;/li&gt;
&lt;li&gt;HTTP mock servers&lt;/li&gt;
&lt;li&gt;mutation/fuzzy testing&lt;/li&gt;
&lt;li&gt;DB cleanup/rollback&lt;/li&gt;
&lt;li&gt;load testing and so on&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;There is no need to re-invent the wheel. The sentence applies to testing code as well. Maybe there are some corner cases where your main application is indeed a snowflake and needs some in-house utility for the core code. But I can bet that your unit and integration tests are not special themselves and thus writing custom testing utilities is a questionable practice.&lt;/p&gt;
&lt;h3 id=&quot;anti-pattern-13---giving-testing-a-bad-reputation-out-of-ignorance&quot;&gt;Anti-Pattern 13 - Giving testing a bad reputation out of ignorance&lt;/h3&gt;
&lt;p&gt;Even though I mention this as the last anti-pattern, this is the one that forced me to write this article. I am always disappointed when I find people at conferences and meetups who “proudly” proclaim that &lt;em&gt;all tests are a waste of time&lt;/em&gt; and that their application works just fine without any testing at all. A more common occurrence is meeting people who are against a specific type of testing (usually either unit or integration) like we have seen in anti-patterns &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-1---having-unit-tests-without-integration-tests&quot;&gt;1&lt;/a&gt; or &lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-2---having-integration-tests-without-unit-tests&quot;&gt;2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When I find people like this, it is my hobby to probe them with questions and understand their reasons behind hating tests. And always, it boils down to anti-patterns. They previously worked in companies where tests were slow (&lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-7---having-flaky-or-slow-tests&quot;&gt;Anti pattern 7&lt;/a&gt;), or needed constant refactoring (&lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-5---testing-internal-implementation&quot;&gt;Antipattern 5&lt;/a&gt;). They have been “burned” by unreasonable requests for 100% code coverage (&lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-6---paying-excessive-attention-to-test-coverage&quot;&gt;Anti-pattern 6&lt;/a&gt;) or TDD zealots (&lt;a href=&quot;http://blog.codepipes.com/testing/software-testing-antipatterns.html#anti-pattern-11---treating-tdd-as-a-religion&quot;&gt;Antipattern 11&lt;/a&gt;) that tried to impose to the whole team their own twisted image of what TDD means.&lt;/p&gt;
&lt;p&gt;If you are one of those people I truly feel for you. I know how hard it is to work in a company that has bad habits.&lt;/p&gt;
&lt;p&gt;Bad experiences of testing in the past should not clutter your judgment when it comes to testing your next greenfield project. Try to look objectively at your team and your project and see if any of the anti-patterns apply to you. If yes, then you are simply testing in the wrong way and no amount of tests will make your application better. Sad but true.&lt;/p&gt;
&lt;p&gt;It is one thing for your team to suffer from bad testing habits, and another to mentor junior developers declaring that “testing is a waste of time”. Please don’t do the latter. There are companies out there that don’t suffer from &lt;strong&gt;any&lt;/strong&gt; of the anti-patterns mentioned in this article. Try to find them!&lt;/p&gt;
&lt;hr /&gt;&lt;p&gt;Go &lt;a href=&quot;http://blog.codepipes.com&quot;&gt;back to contents&lt;/a&gt;, contact me &lt;a href=&quot;http://codepipes.com/contact.html&quot;&gt;via email&lt;/a&gt; or find me at &lt;a href=&quot;http://twitter.com/codepipes&quot;&gt;Twitter&lt;/a&gt; if you have a comment.&lt;/p&gt;
</description>
<pubDate>Sun, 22 Apr 2018 06:04:58 +0000</pubDate>
<dc:creator>kkapelon</dc:creator>
<dc:language>en-us</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://blog.codepipes.com/testing/software-testing-antipatterns.html</dc:identifier>
</item>
</channel>
</rss>