<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Slow Software</title>
<link>https://www.inkandswitch.com/slow-software.html</link>
<guid isPermaLink="true" >https://www.inkandswitch.com/slow-software.html</guid>
<description>&lt;h2 id=&quot;what-feels-slow&quot;&gt;What feels slow&lt;/h2&gt;
&lt;p&gt;What feels &quot;slow&quot; to users? We all have a sense for when software annoys us with delays. But to get a better handle on this problem, we'll complement these intuitions with academic research that answers the question rigorously.&lt;/p&gt;
&lt;p&gt;Perceived speed is all about latency. Comparing the &quot;what feels slow&quot; findings from academia with measurements of real-world app latency tells us just how bad things really are.&lt;/p&gt;
&lt;h3 id=&quot;latency-not-throughput&quot;&gt;Latency not throughput&lt;/h3&gt;
&lt;p&gt;When discussing software performance, we often hear about &lt;em&gt;throughput&lt;/em&gt;. E.g. &quot;this web server can do 10,000 requests / second.&quot; But that's not how users perceive things. They care how long their particular web request takes, or how long a doc takes to open, or how responsive an app is to their clicks. These interactions are about &lt;em&gt;latency&lt;/em&gt;. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;User perception&lt;/span&gt; &lt;span&gt;See &lt;a id=&quot;ret-1&quot; href=&quot;https://www.inkandswitch.com/slow-software.html#ref-1&quot;&gt;[1]&lt;/a&gt; for more on the importance of latency in interactive systems.&lt;/span&gt; &lt;span&gt;Other factors influence how fast software feels. E.g. frame rates and feedback about long-running tasks. But we think latency is the most fundamental, and we're only going to have software that feels truly instantaneous if latency is very low.&lt;/span&gt;&lt;/small&gt; Latency is the critical metric we'll examine in this article.&lt;/p&gt;
&lt;h3 id=&quot;touch-interfaces&quot;&gt;Touch interfaces&lt;/h3&gt;
&lt;p&gt;To start, let's look at user sensitivity to latency when using touch screens.&lt;/p&gt;
&lt;p&gt;Researchers can test this with rigs that control exactly how much latency users see. They present a user with an interface having (say) 1ms latency and another with (say) 70ms and then ask them to perform operations like tapping a button. If the 70ms interface consistently feels slower than the 1ms interface, 70ms would be a &quot;noticeable difference.&quot; &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Just noticeable differences&lt;/span&gt; &lt;span&gt;For more on just noticeable latency differences and associated experimental rigs, see e.g. &lt;a href=&quot;https://www.inkandswitch.com/slow-software.html#ref-2&quot;&gt;[2]&lt;/a&gt;.&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;The smallest such noticeable difference is the budget one has before a given operation starts to feel slow to that user.&lt;/p&gt;
&lt;p&gt;When dragging items on the screen, for example, users perceive latencies as low as ~2ms. The just noticeable latency varies by user and action being performed, but it's consistently very low. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Drag latency perception&lt;/span&gt; &lt;span&gt;&lt;a id=&quot;ret-2&quot; href=&quot;https://www.inkandswitch.com/slow-software.html#ref-2&quot;&gt;[2]&lt;/a&gt; reports 2–16ms for drags with a stylus.&lt;/span&gt; &lt;span&gt;&lt;a id=&quot;ret-3&quot; href=&quot;https://www.inkandswitch.com/slow-software.html#ref-3&quot;&gt;[3]&lt;/a&gt;: 2–11ms for drags with finger.&lt;/span&gt; &lt;span&gt;&lt;a id=&quot;ret-4&quot; href=&quot;https://www.inkandswitch.com/ref-4&quot;&gt;[4]&lt;/a&gt;: mean of 11ms for drags with finger.&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;Inking on a tablet with a stylus gives similar results. Here the literature suggests users detect slowness between 20ms and 80ms of latency. In our own informal tests in the lab, latency towards 80ms feels very slow, and it takes something much closer to 20ms for a stylus to feel responsive while inking. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Stylus latency perception&lt;/span&gt; &lt;span&gt;&lt;a href=&quot;https://www.inkandswitch.com/slow-software.html#ref-2&quot;&gt;[2]&lt;/a&gt; reports 10–70ms for stylus scribbling.&lt;/span&gt; &lt;span&gt;&lt;a id=&quot;ret-5&quot; href=&quot;https://www.inkandswitch.com/slow-software.html#ref-5&quot;&gt;[5]&lt;/a&gt;: 21–82ms for various stylus actions.&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;The difference between low- and high-latency inking is clear when looking at contrasting examples side-by-side:&lt;/p&gt;
&lt;div class=&quot;caption&quot; readability=&quot;9&quot;&gt;
&lt;p class=&quot;caption-title&quot;&gt;Inking latency examples&lt;/p&gt;
&lt;p&gt;Left: an iPad Pro and Notes app with ~15ms of end-to-end latency.&lt;/p&gt;
&lt;p&gt;Right: a Samsung S3 and OneNote app with ~70ms of latency.&lt;/p&gt;
&lt;p&gt;Videos slowed 16x.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;video&quot;&gt;&lt;img class=&quot;play&quot; src=&quot;https://www.inkandswitch.com/media/slow-software/play.svg&quot;/&gt;&lt;/div&gt;
&lt;p&gt;Another common operation on touch devices is tapping on buttons or links. Here tests suggests users on average notice latency as it goes beyond ~70ms (though it's likely lower for some individual users). &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Tapping latency perception&lt;/span&gt; &lt;span&gt;&lt;a href=&quot;https://www.inkandswitch.com/slow-software.html#ref-4&quot;&gt;[4]&lt;/a&gt; reports 69ms mean just noticeable latency for finger tap across users.&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;Here's an example showing two different latencies side-by-side:&lt;/p&gt;
&lt;div class=&quot;caption&quot; readability=&quot;9&quot;&gt;
&lt;p class=&quot;caption-title&quot;&gt;Tapping latency examples&lt;/p&gt;
&lt;p&gt;Left: Opening a settings tab on an iPhone 6s with ~90ms of latency.&lt;/p&gt;
&lt;p&gt;Right: Toggling a setting a Samsung S3 with ~330ms of latency.&lt;/p&gt;
&lt;p&gt;Videos slowed 16x.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;video&quot;&gt;&lt;img class=&quot;play&quot; src=&quot;https://www.inkandswitch.com/media/slow-software/play.svg&quot;/&gt;&lt;/div&gt;
&lt;p&gt;How do modern apps fare compared to these latency thresholds for touch interactions? In terms of dragging with a finger, no current consumer system will consistently meet the low single digit millisecond level needed to satisfy all users. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Drag performance&lt;/span&gt; &lt;span&gt;As we'll see below, &amp;lt;10ms is not enough latency budget even for input hardware and displays, not to mention multiple layers of software.&lt;/span&gt;&lt;/small&gt; So all current touchscreen operating systems will leave at least some users feeling like the object they're dragging is lagging behind their finger.&lt;/p&gt;
&lt;p&gt;For drawing with a stylus, a small number of systems get close to the latency levels needed to feel reasonably good. But most are well above these levels and—as we'd expect—feel very slow to users:&lt;/p&gt;
&lt;div class=&quot;caption&quot; readability=&quot;11&quot;&gt;
&lt;p class=&quot;caption-title&quot;&gt;Inking latencies&lt;/p&gt;
&lt;p&gt;Results from Ink &amp;amp; Switch tests on tablet inking latency.&lt;/p&gt;
&lt;p&gt;Average latencies as measured from screen contact to start of corresponding pixel color change, rounded to nearest 5ms.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;table&quot;&gt;
&lt;table&gt;&lt;tr&gt;&lt;th class=&quot;text left&quot;&gt;Device&lt;/th&gt;
&lt;th class=&quot;text middle&quot;&gt;Program&lt;/th&gt;
&lt;th colspan=&quot;2&quot; class=&quot;mixed right&quot;&gt;Latency (ms)&lt;/th&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left first&quot;&gt;iPad Pro&lt;/td&gt;
&lt;td class=&quot;text middle first&quot;&gt;Notes&lt;/td&gt;
&lt;td class=&quot;number middle first&quot;&gt;20&lt;/td&gt;
&lt;td class=&quot;bar right first&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;/&gt;
&lt;td class=&quot;text middle&quot;&gt;GoodNotes&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;30&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;/&gt;
&lt;td class=&quot;text middle&quot;&gt;Flutter&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;35&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left first&quot;&gt;Surface Pro&lt;/td&gt;
&lt;td class=&quot;text middle first&quot;&gt;OneNote&lt;/td&gt;
&lt;td class=&quot;number middle first&quot;&gt;25&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;/&gt;
&lt;td class=&quot;text middle&quot;&gt;SketchPad&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;30&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;/&gt;
&lt;td class=&quot;text middle&quot;&gt;Canvas&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;60&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left first&quot;&gt;Pixelbook&lt;/td&gt;
&lt;td class=&quot;text middle first&quot;&gt;Squid&lt;/td&gt;
&lt;td class=&quot;number middle first&quot;&gt;40&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;/&gt;
&lt;td class=&quot;text middle&quot;&gt;Canvas&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;60&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left first&quot;&gt;Samsung S3&lt;/td&gt;
&lt;td class=&quot;text middle first&quot;&gt;Squid&lt;/td&gt;
&lt;td class=&quot;number middle first&quot;&gt;60&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;/&gt;
&lt;td class=&quot;text middle&quot;&gt;Flutter&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;65&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;/&gt;
&lt;td class=&quot;text middle&quot;&gt;Canvas&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;75&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left last&quot;/&gt;
&lt;td class=&quot;text middle last&quot;&gt;LiveBoard&lt;/td&gt;
&lt;td class=&quot;number middle last&quot;&gt;80&lt;/td&gt;
&lt;td class=&quot;bar right last&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;While we don't have data on tapping latencies across devices, we'd expect them to be comparable to the inking latencies observed above. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Inking vs tapping latencies&lt;/span&gt; &lt;span&gt;We expect these latencies to be comparable because they both test the touch input → screen update loop. That said, there are differences between the two paths so we wouldn't expect them to be exactly the same.&lt;/span&gt;&lt;/small&gt; Since the noticeable latency here is ~70ms, most systems should be able to feel responsive to taps. But it's also easy to find apps that perform much worse than the system's theoretical capability.&lt;/p&gt;
&lt;p&gt;Overall, touch systems need to have very low latencies to feel responsive. Most devices and apps fail to perform at this level, and consequently feel varying degrees of slow to users.&lt;/p&gt;
&lt;h3 id=&quot;typing&quot;&gt;Typing&lt;/h3&gt;
&lt;p&gt;There is some evidence that increased typing latency impairs users. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Typing latency impact&lt;/span&gt; &lt;span&gt;In &lt;a id=&quot;ret-6&quot; href=&quot;https://www.inkandswitch.com/slow-software.html#ref-6&quot;&gt;[6]&lt;/a&gt;, random amounts of latency are added to keying operations, which reduces typing performance. However it only assessed one distribution of latency. It also suggested skilled typists may acclimate to the increased latency.&lt;/span&gt;&lt;/small&gt; However, we're not aware of studies specifically measuring least noticeable end-to-end typing latencies. The tap latency numbers (noticeable at ~70ms) may be a useful benchmark because they also measure a discrete finger touch to visual update.&lt;/p&gt;
&lt;p&gt;Here are some informal end-to-end keyboard latency measurements:&lt;/p&gt;
&lt;div class=&quot;caption&quot; readability=&quot;9.3125&quot;&gt;
&lt;p class=&quot;caption-title&quot;&gt;Typing latencies&lt;/p&gt;
&lt;p&gt;End-to-end latencies from start of keypress to a character appearing in app for a variety of machines.&lt;/p&gt;
&lt;p&gt;Sources: &lt;a href=&quot;https://danluu.com/input-lag/&quot;&gt;Computer latency: 1977-2017&lt;/a&gt;, Ink &amp;amp; Switch tests&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;table&quot;&gt;
&lt;table&gt;&lt;tr&gt;&lt;th class=&quot;text left&quot;&gt;Computer&lt;/th&gt;
&lt;th colspan=&quot;2&quot; class=&quot;mixed right&quot;&gt;Latency (ms)&lt;/th&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left first&quot;&gt;Apple IIe&lt;/td&gt;
&lt;td class=&quot;number middle first&quot;&gt;30&lt;/td&gt;
&lt;td class=&quot;bar right first&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;&gt;Commodore Pet 4016&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;60&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;&gt;iMac g4 OS 9&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;70&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;&gt;Macbook Pro 2014&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;100&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;&gt;Custom Haswell-e 24Hz&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;140&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;&gt;Samsung S3&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;150&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;&gt;Powerspec g405 Linux&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;170&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left last&quot;&gt;Symbolics 3620&lt;/td&gt;
&lt;td class=&quot;number middle last&quot;&gt;300&lt;/td&gt;
&lt;td class=&quot;bar right last&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;Rigorous assessments of the impact of keyboard latency would be great experiments for enterprising researchers. In any event, it seems likely the latency threshold for typing is below ~100ms for many users, and perhaps well below it.&lt;/p&gt;
&lt;h3 id=&quot;mousing&quot;&gt;Mousing&lt;/h3&gt;
&lt;p&gt;The last input type we'll look at is mice. One experiment found user latency perception thresholds down to 34ms. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Mouse latency impact&lt;/span&gt; &lt;span&gt;&lt;a id=&quot;ret-7&quot; href=&quot;https://www.inkandswitch.com/slow-software.html#ref-7&quot;&gt;[7]&lt;/a&gt; measures latency sensitivity in a cursor dragging test and reports sensitivities in the range of 34–137ms, with a mean of 65ms.&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;Input latency of mice varies widely. Some setups achieve latencies in the single digit milliseconds range by combining high-performance hardware with careful, low-level programming. It's also possible to go beyond 100ms of end-to-latency with a combination of mediocre hardware and applications that introduce extra delays or buffers between input and display. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Mouse latencies&lt;/span&gt; &lt;span&gt;&lt;a href=&quot;https://www.inkandswitch.com/slow-software.html#ref-7&quot;&gt;[7]&lt;/a&gt; describes an optimized setup with end-to-end average mouse movement latencies of ~8ms.&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;h3 id=&quot;applications&quot;&gt;Applications&lt;/h3&gt;
&lt;p&gt;Application-level latencies measure how long it takes to complete app-specific actions like loading web pages:&lt;/p&gt;
&lt;div class=&quot;caption&quot; readability=&quot;9&quot;&gt;
&lt;p class=&quot;caption-title&quot;&gt;Application latency example&lt;/p&gt;
&lt;p&gt;An example of application-level latency. It takes ~3000ms to load this NYTimes web page.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;video&quot;&gt;&lt;img class=&quot;play&quot; src=&quot;https://www.inkandswitch.com/media/slow-software/play.svg&quot;/&gt;&lt;/div&gt;
&lt;p&gt;What feels fast for application actions? It's hard to say exactly because actions are more complex and varied than simple inputs. The answer here probably also depends on what users are conditioned to expect (currently, it's usually slow software). But we can triangulate a rough number. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Latency literature&lt;/span&gt; &lt;span&gt;&lt;a id=&quot;ret-8&quot; href=&quot;https://www.inkandswitch.com/slow-software.html#ref-8&quot;&gt;[8]&lt;/a&gt; surveys literature on the impact of latency to application users. It's a good starting point for a deeper dive on this topic.&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;One reference is the typical ~70ms just noticeable difference mentioned above for tapping a touch screen. If you notice a delay between tapping a link and seeing a tap indicator, you can probably notice a similar delay between tapping a link and seeing a web page open.&lt;/p&gt;
&lt;p&gt;Another data point is Google’s RAIL model. This model claims that responses within 100ms “feel like the result is immediate” and that higher latency “[breaks] the connection between action and reaction”. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;RAIL&lt;/span&gt; &lt;span&gt;&lt;a href=&quot;https://developers.google.com/web/fundamentals/performance/rail&quot;&gt;Google's model&lt;/a&gt;&lt;/span&gt; &lt;span&gt;Unfortunately they doesn't give a basis for their 100ms figure.&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;You can informally test our own sensitivity in a terminal. Consider the command-line programs you use and how they feel, then try them with `time`. You'll notice the difference between e.g. ~15ms CLI responses (great!) and ~500ms (obviously slow).&lt;/p&gt;
&lt;div class=&quot;caption&quot; readability=&quot;9&quot;&gt;
&lt;p class=&quot;caption-title&quot;&gt;Terminal execution times&lt;/p&gt;
&lt;p&gt;Examples of execution times for different terminal commands.&lt;/p&gt;
&lt;p&gt;Even a command that takes half a second is distractingly slow.&lt;/p&gt;
&lt;/div&gt;
&lt;img src=&quot;https://www.inkandswitch.com/media/slow-software/terminal-times.png&quot;/&gt;&lt;p&gt;As a final data point, consider that typical human reaction time from seeing a visual stimulus to taking a physical action is about 220ms. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Reaction time&lt;/span&gt; &lt;span&gt;See &lt;a href=&quot;https://web.archive.org/web/20100611222125/http://biae.clemson.edu/bpc/bp/Lab/110/reaction.htm&quot;&gt;Literature Review on Reaction Time&lt;/a&gt;&lt;/span&gt;&lt;/small&gt; This value must be significantly more than noticeable latencies, because reactions involve observing something and then doing something.&lt;/p&gt;
&lt;p&gt;Altogether we think this suggests action latencies should be ~100ms or less to avoid user perception of delay. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Physiological effects&lt;/span&gt; &lt;span&gt;Software latency may be even more damaging than we consciously realize.&lt;/span&gt; &lt;span&gt;&lt;a id=&quot;ret-9&quot; href=&quot;https://www.inkandswitch.com/slow-software.html#ref-9&quot;&gt;[9]&lt;/a&gt; documents unconscious, physiological effects in users exposed to increased latency.&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;h3 id=&quot;real-world-apps&quot;&gt;Real-world apps&lt;/h3&gt;
&lt;p&gt;How do current apps fair against this benchmark? Some do well. For example, many Unix command line programs run in under 100ms.&lt;/p&gt;
&lt;p&gt;Most of the web does poorly. A ~1,000ms Google search result will feel faster than most of what you see online, though still noticeably delayed compared to a ~100ms interaction. And it's easy to find examples of pages that take ~5,000ms+ to load even on a good connection.&lt;/p&gt;
&lt;p&gt;In the case of mobile and desktop, there are some apps that will consistently achieve &amp;lt;100ms latency, such as the built-in calculator on iOS. But it's easy to find cases of productivity apps that significantly exceed this threshold even when they have (or should have) all data available locally. Consider the Slack example below:&lt;/p&gt;
&lt;div class=&quot;caption&quot; readability=&quot;10&quot;&gt;
&lt;p class=&quot;caption-title&quot;&gt;Slack latency example&lt;/p&gt;
&lt;p&gt;Slack on an iPad Pro took ~220ms to change between two low-volume channels in the same workspace.&lt;/p&gt;
&lt;p&gt;This is a long time considering that no network call is needed and the iPad Pro is perhaps the highest performance mobile device in the world.&lt;/p&gt;
&lt;p&gt;Video slowed 8x.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;video&quot;&gt;&lt;img class=&quot;play&quot; src=&quot;https://www.inkandswitch.com/media/slow-software/play.svg&quot;/&gt;&lt;/div&gt;
&lt;p&gt;It's hard to draw a general conclusion about such a broad area as the action latency of all software. That said, it seems clear that while some apps do complete actions fast enough to feel instantaneous to users (less than ~100ms), many apps do not.&lt;/p&gt;
&lt;h2 id=&quot;where-slowness-comes-from&quot;&gt;Where slowness comes from&lt;/h2&gt;
&lt;p&gt;So we've established that a lot of software is in fact slow. Where does all that time go (and what might we optimize)? We'll look at this next, starting with the first component in the chain: input devices.&lt;/p&gt;
&lt;h3 id=&quot;input-devices&quot;&gt;Input devices&lt;/h3&gt;
&lt;p&gt;The first step in the pipeline that converts physical inputs to updates on screen is the input processing: converting contact with a touch screen, keyboard, or mouse to a digital signal for the operating system. Here we'll look at how long this step takes.&lt;/p&gt;
&lt;p&gt;Let's start with keyboards:&lt;/p&gt;
&lt;div class=&quot;caption&quot; readability=&quot;7.0724637681159&quot;&gt;
&lt;p class=&quot;caption-title&quot;&gt;Keyboard latencies&lt;/p&gt;
&lt;p&gt;Measured latencies from start of key press to signal reaching USB hub, rounded to nearest 5ms.&lt;/p&gt;
&lt;p&gt;Source: &lt;a href=&quot;https://danluu.com/keyboard-latency&quot;&gt;Keyboard latency&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;table&quot;&gt;
&lt;table&gt;&lt;tr&gt;&lt;th class=&quot;text left&quot;&gt;Keyboard&lt;/th&gt;
&lt;th colspan=&quot;2&quot; class=&quot;mixed right&quot;&gt;Latency (ms)&lt;/th&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left first&quot;&gt;Apple Magic&lt;/td&gt;
&lt;td class=&quot;number middle first&quot;&gt;15&lt;/td&gt;
&lt;td class=&quot;bar right first&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;&gt;Das 3&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;25&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;&gt;Kinesis Freestyle2&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;30&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;&gt;Ergodox&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;40&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;&gt;Kinesis Advantage&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;50&lt;/td&gt;
&lt;td class=&quot;bar right&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left last&quot;&gt;Logitech MK360&lt;/td&gt;
&lt;td class=&quot;number middle last&quot;&gt;60&lt;/td&gt;
&lt;td class=&quot;bar right last&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;As you can see these keyboards easily take up 10s of milliseconds of latency budget on the very first step in the processing pipeline. That's out of a total budget of ~100ms or less! &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Typing and latency&lt;/span&gt; &lt;span&gt;See &lt;a href=&quot;https://pavelfatin.com/typing-with-pleasure&quot;&gt;Typing with Pleasure&lt;/a&gt; for much greater detail on this topic.&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;Mice can similarly introduce 10s of milliseconds of latency. Though the highest performance gaming mice will have latencies in the single digit millisecond range. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Mouse click latency&lt;/span&gt; &lt;span&gt;Data for mouse click latency is a scattered.&lt;/span&gt; &lt;span&gt;&lt;a href=&quot;https://www.gamersnexus.net/guides/2594-wireless-mouse-click-latency-analysis-vs-wired&quot;&gt;One example&lt;/a&gt; with single digit millisecond latency.&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;It's harder for us to measure the contribution of input devices specifically in mobile devices (because they're tightly integrated with other hardware components). That said, we can use a few of the common patterns in input device hardware to understand latencies in these as well as standalone devices.&lt;/p&gt;
&lt;h3 id=&quot;sample-rates&quot;&gt;Sample rates&lt;/h3&gt;
&lt;p&gt;One common pattern is sample rates. In many input devices, the hardware &quot;scans&quot; or &quot;samples&quot; for new input on a periodic interval. For example, typical consumer touch screens sample for input at the rate of 60hz, or once every ~17ms. This means that in the worst cases input device latency will be at least ~17ms, and in the average case it can be no better than ~8ms.&lt;/p&gt;
&lt;p&gt;All things being equal, higher scan rates can reduce input latency. High-end Apple mobile hardware samples touch and stylus input more frequently than 60hz, correspondingly reducing latency:&lt;/p&gt;

&lt;div class=&quot;table&quot;&gt;
&lt;table&gt;&lt;tr&gt;&lt;th class=&quot;text left&quot;&gt;Device&lt;/th&gt;
&lt;th class=&quot;number middle&quot;&gt;Touch (hz)&lt;/th&gt;
&lt;th class=&quot;number right&quot;&gt;Stylus (hz)&lt;/th&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left first&quot;&gt;iPhone 6&lt;/td&gt;
&lt;td class=&quot;number middle first&quot;&gt;60&lt;/td&gt;
&lt;td class=&quot;number right first&quot;/&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;&gt;iPhone 7&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;60&lt;/td&gt;
&lt;td class=&quot;number right&quot;/&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;&gt;iPhone 8&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;60&lt;/td&gt;
&lt;td class=&quot;number right&quot;/&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;&gt;iPhone X&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;120&lt;/td&gt;
&lt;td class=&quot;number right&quot;/&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;&gt;iPad Air 2&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;60&lt;/td&gt;
&lt;td class=&quot;number right&quot;/&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left&quot;&gt;iPad Mini 4&lt;/td&gt;
&lt;td class=&quot;number middle&quot;&gt;60&lt;/td&gt;
&lt;td class=&quot;number right&quot;/&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td class=&quot;text left last&quot;&gt;iPad Pro&lt;/td&gt;
&lt;td class=&quot;number middle last&quot;&gt;120&lt;/td&gt;
&lt;td class=&quot;number right last&quot;&gt;240&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;
&lt;p&gt;A similar source of latency is USB scanning. The USB protocol pulls input from the keyboard, so the keyboard needs to wait for the USB scan to send its key presses. Low speed USB scans at 125hz, introducing an unavoidable ~8ms max and ~4ms average delay. More recent USB versions scan at 1000hz or more, minimizing the latency impact.&lt;/p&gt;
&lt;p&gt;There are many other potential sources of latency in input devices, for example debouncing in keyboards. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Debouncing&lt;/span&gt; &lt;span&gt;See &lt;a href=&quot;https://summivox.wordpress.com/2016/06/03/keyboard-matrix-scanning-and-debouncing&quot;&gt;Keyboard Matrix Scanning and Debouncing&lt;/a&gt; for details on hardware and software implications of debouncing.&lt;/span&gt;&lt;/small&gt; We won't cover them all here, but emphasize the meta points that a) input devices themselves can introduce significant latency before any software processing happens and b) this may be due to multiple discrete causes that add up.&lt;/p&gt;
&lt;h3 id=&quot;displays-and-gpus&quot;&gt;Displays and GPUs&lt;/h3&gt;
&lt;p&gt;The hardware at the other end of the pipeline are displays and graphics cards.&lt;/p&gt;
&lt;p&gt;One source of latency here is the frame rate of the display. Since displays can't redraw constantly, this introduces unavoidable latency similar to the input scanning discussed above. If a screen updates (say) every 20ms, it adds 20ms of latency in the worst case and 10ms in the average case. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Perceiving movement&lt;/span&gt; &lt;span&gt;Other factors influence how we perceive objects moving on screens. &lt;a href=&quot;https://www.blurbusters.com&quot;&gt;Blur Busters&lt;/a&gt; is a great resource. See e.g. &lt;a href=&quot;https://www.blurbusters.com/faq/lcd-motion-artifacts/&quot;&gt;LCD Motion Artifacts 101&lt;/a&gt;.&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;Most displays run at 60hz, though high end-devices and especially displays built for gaming run at 120hz, 144hz, and 240hz. Thus display frame rate alone usually contributes ~8ms average-case latency, though this can be reduced to a few milliseconds in the highest-frame-rate displays.&lt;/p&gt;
&lt;p&gt;Another contribution to latency from displays is the time it takes them to physically change the color of pixels after they receive new pixel data. This time varies from low single digit milliseconds or less in high-end gaming displays to double digit milliseconds in less responsive LCDs. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Display response times&lt;/span&gt; &lt;span&gt;These are hard to measure, but &lt;a href=&quot;https://www.notebookcheck.net&quot;&gt;Notebook Check&lt;/a&gt; has some illustrative data. See e.g. a &lt;a href=&quot;https://www.notebookcheck.net/Asus-ROG-Chimera-G703GI-i9-8950HK-GTX-1080-Full-HD-Laptop-Review.308366.0.html&quot;&gt;fast&lt;/a&gt; and &lt;a href=&quot;https://www.notebookcheck.net/Google-Pixelbook-Chromebook-Review.262418.0.html&quot;&gt;slow&lt;/a&gt; example.&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;On modern high-end devices, a dedicated graphics unit (GPU) powers the display. GPUs produce the array of pixels for displays by e.g. compositing 2D windowing layers or rendering 3D virtual scenes. GPUs produce frames at a rate that depends on the GPU hardware, their interaction with application and framework code, and sometimes on synchronization logic with displays.&lt;/p&gt;
&lt;p&gt;A related issue happens when application code is outright slow, and doesn't even send instructions to the GPU fast enough to take full advantage of it. This can lead to the GPU producing unique frames at a lower rate than it could if it did in fact have frequent instruction from the application. This is a common source of &quot;jank&quot; we see in 2D applications that render less than 60fps. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Jank&lt;/span&gt; &lt;span&gt;&quot;Jank&quot; is hard to define, but you know it when you see it. Nathan Gitter defines it as &quot;visual glitches that are unexpected or distracting&quot; in &lt;a href=&quot;https://medium.com/@nathangitter/designing-jank-free-apps-9f66d43b9c87&quot;&gt;Designing Jank-Free Apps&lt;/a&gt;.&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;h3 id=&quot;cycle-stacking&quot;&gt;&lt;span&gt;Cycle stacking&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;We've discussed at least three parts of the pipeline where latency accrues due to periodic activity: input scanning, GPU rendering loops, and display refresh cycles. It's important to note that these can stack in ways that essentially adds all of their latency together:&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;caption&quot; readability=&quot;12&quot;&gt;
&lt;p class=&quot;caption-title&quot;&gt;&lt;span&gt;Waiting for multiple cycles&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;A hypothetical latency cascade shows how waiting for successive hardware cycles can accumulate latency.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Dashed vertical lines indicate cycles the pipeline needs to wait for.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;span&gt;&lt;img src=&quot;https://www.inkandswitch.com/media/slow-software/cycle-stacking.png&quot;/&gt;&lt;/span&gt;
&lt;p&gt;&lt;span&gt;In order to move to the next step in a pipeline, we need to wait for the next cycle of that stage to come along. And the cycles may not be aligned. Misaligned cycles and an unfavorable initial input time can cause 10s of milliseconds of additional latency, a large amount relative to the latency budgets discussed above.&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;runtime-overhead&quot;&gt;&lt;span&gt;Runtime overhead&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;&lt;span&gt;On the software side, runtime overhead is a catch-all for overhead from the operating system and other non-application code. We'll look at two important examples: garbage collection and scheduling.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;First up is garbage collection (GC). GC is critical in the two most widely-used platforms in the world—the web (JavaScript) and Android (Java).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;GC can contribute a lot of latency at specific instances, especially relative to requirements for fast input latency. GCs on the order of 10ms wouldn't be surprising for JavaScript or Java runtimes. But that's the entire budget we have for dragging objects on a touch screen! &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;GC latency impact&lt;/span&gt; &lt;span&gt;A GC may delay ~1 frame, not all frames. But like &quot;jank&quot; from missed frames, latency jitters are noticeable and annoying to users.&lt;/span&gt;&lt;/small&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are ways to mitigate GC-induced latency. These include moving as much GC work as possible off of the main thread and optimizing the GC to require only small individual pauses. One can also use a language that trades off some of the convenience of GC for more predictable performance. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;GC latency mitigation&lt;/span&gt; &lt;span&gt;See for example the &lt;a href=&quot;https://v8project.blogspot.com/2016/04/jank-busters-part-two-orinoco.html&quot;&gt;V8 effort&lt;/a&gt; to move more GC off the main thread and the &lt;a href=&quot;https://blog.golang.org/ismmkeynote&quot;&gt;Go work&lt;/a&gt; to bring max GC pauses well below 1ms. Languages like Swift avoid arbitrary GC by using &lt;a href=&quot;https://docs.swift.org/swift-book/LanguageGuide/AutomaticReferenceCounting.html&quot;&gt;automatic reference counting&lt;/a&gt;.&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;Another potential source of overhead is operating system scheduling. Our application (and its dependencies in the OS) are not necessarily running all the time. Other programs may be scheduled in while ours is paused, even if for a very short time.&lt;/p&gt;
&lt;p&gt;If we have a wall-clock budget of 10ms and our app takes 10ms &quot;end-to-end&quot; without accounting for any other programs running, we may well exceed our budget in wall clock time due to OS scheduling. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;CPU usage&lt;/span&gt; &lt;span&gt;Related to the scheduling issue is CPU usage. If your app meets its performance goals but requires ~100% of the CPU to do that, it may well annoy users. You'll drain batteries faster, make the device hotter, and perhaps trigger a noisy fan. In other words, even with user-facing performance held constant, lower CPU usage is a better user experience.&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;Every program will ask for their time slices, and there are only so many CPU cores to go around.&lt;/p&gt;
&lt;h3 id=&quot;latency-by-design&quot;&gt;Latency by design&lt;/h3&gt;
&lt;p&gt;A common source of latency on mobile interfaces is the design of the OS and apps themselves. There are some important interactions that can only be accomplished by literally waiting.&lt;/p&gt;
&lt;p&gt;Android and iOS both make substantial use of &quot;long press&quot; to access context menus, which require that the user wait hundreds of milliseconds in the middle of their command gestures.&lt;/p&gt;
&lt;p&gt;A related source is delays for disambiguation. For example, on mobile Safari there's a default 350ms delay between when the user taps a link and when the browser begins fetching the new page, in order to tell the difference between a link click and a double-tap zoom. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;iOS tap delays&lt;/span&gt; &lt;span&gt;See &lt;a href=&quot;https://webkit.org/blog/5610/more-responsive-tapping-on-ios/&quot;&gt;More Response Tapping on iOS&lt;/a&gt; for background on this delay as well as recent changes that allow application developers to work around the issue.&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;
&lt;h3 id=&quot;user-hostile-work&quot;&gt;User-hostile work&lt;/h3&gt;
&lt;p&gt;A major source of latency for users on the web is user-hostile work, such as downloading trackers that surveil user activity and loading intrusive ads.&lt;/p&gt;
&lt;div class=&quot;caption&quot; readability=&quot;8&quot;&gt;
&lt;p class=&quot;caption-title&quot;&gt;Loading a news article&lt;/p&gt;
&lt;p&gt;A ~500 word article on the Washington Post website needed 100s of HTTP requests and ~4400ms. Many of the requests are for surveillance and advertising. A small selection of the requests are shown.&lt;/p&gt;
&lt;/div&gt;
&lt;img src=&quot;https://www.inkandswitch.com/media/slow-software/washington-post-profile.png&quot;/&gt;&lt;p&gt;There are many great articles on web bloat. &lt;small class=&quot;note&quot;&gt;&lt;span class=&quot;note-title&quot;&gt;Web bloat&lt;/span&gt; &lt;span&gt;See e.g. &lt;a href=&quot;https://pxlnv.com/blog/bullshit-web/&quot;&gt;The Bullshit Web&lt;/a&gt;, &lt;a href=&quot;http://idlewords.com/talks/website_obesity.htm&quot;&gt;The Website Obesity Crisis&lt;/a&gt;, &lt;a href=&quot;https://danluu.com/web-bloat/&quot;&gt;Web bloat&lt;/a&gt;&lt;/span&gt;&lt;/small&gt; We'll just emphasize that the single biggest source of latency on many sites is downloading stuff that users don't want.&lt;/p&gt;
&lt;h3 id=&quot;application-code&quot;&gt;Application code&lt;/h3&gt;
&lt;p&gt;The last source of latency we'll mention is perhaps the most obvious: the application. If the app spends a lot of CPU time processing a given input, or executing some action, it will be slow.&lt;/p&gt;
&lt;h3 id=&quot;putting-it-together&quot;&gt;Putting it together&lt;/h3&gt;
&lt;p&gt;Let's look at an example of how latency can add up:&lt;/p&gt;
&lt;div class=&quot;caption&quot; readability=&quot;11&quot;&gt;
&lt;p class=&quot;caption-title&quot;&gt;Latency waterfall example&lt;/p&gt;
&lt;p&gt;A hypothetical example of end-to-end latency from input to display.&lt;/p&gt;
&lt;p&gt;Dashed vertical lines indicate cycles the pipeline needs to wait for.&lt;/p&gt;
&lt;/div&gt;
&lt;img src=&quot;https://www.inkandswitch.com/media/slow-software/input-latency-cascade.png&quot;/&gt;&lt;p&gt;The example above is hypothetical but illustrative. It shows how there are a lot of layers that add latency and an application can exhibit high latency (much greater than frame length) even if it successfully runs at at the full frame rate.&lt;/p&gt;
&lt;h2 id=&quot;toward-fast-software&quot;&gt;Toward fast software&lt;/h2&gt;
&lt;p&gt;There is a deep stack of technology that makes a modern computer interface respond to a user's requests. Even something as simple as pressing a key on a keyboard and having the corresponding character appear in a text input box traverses a lengthy, complex gauntlet of steps, from the scan rate of the keyboard, through the OS and framework processing layers, through the graphics card rendering and display refresh rate.&lt;/p&gt;
&lt;p&gt;There is reason for this complexity, and yet we feel sad that computer users trying to be productive with these devices are so often left waiting, watching spinners, or even just with the slight but still perceptible sense that their devices simply can't keep up with them.&lt;/p&gt;
&lt;p&gt;We believe fast software empowers users and makes them more productive. We know today's software often lets users down by being slow, and we want to do better. We hope this material is helpful for you as you work on your own software.&lt;/p&gt;
&lt;p&gt;If you're interested in the topic of fast software, or have feedback on this article, we'd love to hear from you: &lt;a href=&quot;https://twitter.com/inkandswitch&quot;&gt;@inkandswitch&lt;/a&gt; or &lt;a href=&quot;mailto:hello@inkandswitch.com&quot;&gt;hello@inkandswitch.com&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a class=&quot;ref-number&quot; id=&quot;ref-1&quot; href=&quot;https://www.inkandswitch.com/slow-software.html#ret-1&quot;&gt;[1]&lt;/a&gt; Y. Endo, Z. Wang, J. Chen, and M. Seltzer. &quot;&lt;a href=&quot;https://scholar.google.com/scholar?cluster=10535754908454057825&quot;&gt;Using Latency to Evaluate Interactive System Performance&lt;/a&gt;,&quot; In Proceedings of the USENIX 2nd Symposium on Operating Systems Design and Implementation, 1996.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;ref-number&quot; id=&quot;ref-2&quot; href=&quot;https://www.inkandswitch.com/slow-software.html#ret-2&quot;&gt;[2]&lt;/a&gt; A. Ng, M. Annett, P. Dietz, A. Gupta, and W. Bischof. &quot;&lt;a href=&quot;https://scholar.google.com/scholar?cluster=8942755275099535739&quot;&gt;In the Blink of an Eye: Investigating Latency Perception During Stylus Interaction&lt;/a&gt;,&quot; In Proceedings of the 32nd Annual ACM Conference on Human Factors in Computing Systems, 2014.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;ref-number&quot; id=&quot;ref-3&quot; href=&quot;https://www.inkandswitch.com/slow-software.html#ret-3&quot;&gt;[3]&lt;/a&gt; A. Ng, J. Lepinski, D. Wigdor, S. Sanders, and P. Dietz. &quot;&lt;a href=&quot;https://scholar.google.com/scholar?cluster=17905801937419740410&quot;&gt;Designing for low-latency direct-touch input&lt;/a&gt;,&quot; In Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology, 2012.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;ref-number&quot; id=&quot;ref-4&quot; href=&quot;https://www.inkandswitch.com/slow-software.html#ret-4&quot;&gt;[4]&lt;/a&gt; J. Deber, and R. Jota, C. Forlines, and D. Wigdor. &quot;&lt;a href=&quot;https://scholar.google.com/scholar?cluster=16992791481343673285&quot;&gt;How Much Faster is Fast Enough?: User Perception of Latency &amp;amp; Latency Improvements in Direct and Indirect Touch&lt;/a&gt;,&quot; In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, 2015.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;ref-number&quot; id=&quot;ref-5&quot; href=&quot;https://www.inkandswitch.com/slow-software.html#ret-5&quot;&gt;[5]&lt;/a&gt; M. Annett, A. Ng, P. Dietz, W. Bischof, and A. Gupta. &quot;&lt;a href=&quot;https://scholar.google.com/scholar?cluster=15159574178009198437&quot;&gt;How Low Should We Go?: Understanding the Perception of Latency While Inking&lt;/a&gt;,&quot; In Proceedings of Graphics Interface 2014, 2014.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;ref-number&quot; id=&quot;ref-6&quot; href=&quot;https://www.inkandswitch.com/slow-software.html#ret-6&quot;&gt;[6]&lt;/a&gt; M. Annett, A. Ng, P. Dietz, W. Bischof, and A. Gupta. &quot;&lt;a href=&quot;https://scholar.google.com/scholar?cluster=15159574178009198437&quot;&gt;How Low Should We Go?: Understanding the Perception of Latency While Inking&lt;/a&gt;,&quot; In Proceedings of Graphics Interface 2014, 2014.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;ref-number&quot; id=&quot;ref-7&quot; href=&quot;https://www.inkandswitch.com/slow-software.html#ret-7&quot;&gt;[7]&lt;/a&gt; V. Forch, T. Franke, N. Rauh, and J. Krems. &quot;&lt;a href=&quot;https://scholar.google.com/scholar?q=Are+100+ms+Fast+Enough%3F+Characterizing+Latency+Perception+Thresholds+in+Mouse-Based+Interaction&quot;&gt;Are 100 ms Fast Enough? Characterizing Latency Perception Thresholds in Mouse-Based Interaction&lt;/a&gt;,&quot; In Engineering Psychology and Cognitive Ergonomics: Cognition and Design, 2017.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;ref-number&quot; id=&quot;ref-8&quot; href=&quot;https://www.inkandswitch.com/slow-software.html#ret-8&quot;&gt;[8]&lt;/a&gt; J. Dabrowski and E. V. Munson. &quot;&lt;a href=&quot;https://scholar.google.com/scholar?cluster=15646007677492381636&quot;&gt;40 years of searching for the best computer system response time&lt;/a&gt;,&quot; Interacting with Computers 23.5, 2011.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;ref-number&quot; id=&quot;ref-9&quot; href=&quot;https://www.inkandswitch.com/slow-software.html#ret-9&quot;&gt;[9]&lt;/a&gt; M. Barreda-Ángeles, I. Arapakis, X. Bai, B. Cambazoglu, A. Pereda. &quot;&lt;a href=&quot;https://scholar.google.com/scholar?cluster=3612836859261052030&quot;&gt;Unconscious Physiological Effects of Search Latency on Users and Their Click Behaviour&lt;/a&gt;,&quot; In 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2015.&lt;/p&gt;
</description>
<pubDate>Wed, 21 Nov 2018 21:15:32 +0000</pubDate>
<dc:creator>Thibaut</dc:creator>
<og:type>website</og:type>
<og:url>https://www.inkandswitch.com/tablet-platform-showdown.html</og:url>
<og:title>Slow Software</og:title>
<og:description>What it means for software to be fast, and why most software is not.</og:description>
<og:image>https://www.inkandswitch.com/media/slow-software/input-latency-cascade.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.inkandswitch.com/slow-software.html</dc:identifier>
</item>
<item>
<title>Why Aren&amp;#039;t There C Conferences?</title>
<link>https://nullprogram.com/blog/2018/11/21/</link>
<guid isPermaLink="true" >https://nullprogram.com/blog/2018/11/21/</guid>
<description>&lt;time datetime=&quot;2018-11-21&quot;&gt;November 21, 2018&lt;/time&gt;&lt;p&gt;nullprogram.com/blog/2018/11/21/&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This article was &lt;a href=&quot;https://news.ycombinator.com/item?id=18504879&quot;&gt;discussed on Hacker News&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Most widely-used programming languages have at least one regular conference dedicated to discussing it. Heck, even &lt;a href=&quot;https://www.european-lisp-symposium.org/&quot;&gt;Lisp has one&lt;/a&gt;. It’s a place to talk about the latest developments of the language, recent and upcoming standards, and so on. However, C is a notable exception. Despite &lt;a href=&quot;https://skeeto.s3.amazonaws.com/share/onward17-essays2.pdf&quot;&gt;its role as the foundation&lt;/a&gt; of the entire software ecosystem, there aren’t any regular conferences about C. I have a couple of theories about why.&lt;/p&gt;
&lt;p&gt;First, C is so fundamental and ubiquitous that a conference about C would be too general. There are so many different uses ranging across embedded development, operating system kernels, systems programming, application development, and, most recently, web development (WebAssembly). It’s just not a cohesive enough topic. Any conference that might be about C is instead focused on some particular subset of its application. It’s not a C conference, it’s a database conference, or an embedded conference, or a Linux conference, or a BSD conference, etc.&lt;/p&gt;
&lt;p&gt;Second, C has a tendency to be conservative, changing and growing very slowly. This is a feature, and one that is often undervalued by developers. (In fact, I’d personally like to see a future revision that makes the C language specification &lt;em&gt;smaller&lt;/em&gt; and &lt;em&gt;simpler&lt;/em&gt;, rather than accumulate more features.) The last major revision to C happened in 1999 (C99). There was a minor revision in 2011 (C11), and an even smaller revision in 2018 (C17). If there was a C conference, recent changes to the language wouldn’t be a very fruitful topic.&lt;/p&gt;
&lt;p&gt;However, the &lt;em&gt;tooling&lt;/em&gt; has advanced significantly in recent years, especially with the advent of LLVM and Clang. This is largely driven by the C++ community, and C has significantly benefited as a side effect due to its overlap. Those are topics worthy of conferences, but these are really C++ conferences.&lt;/p&gt;
&lt;p&gt;The closest thing we have to a C conference every year is CppCon. A lot of CppCon isn’t &lt;em&gt;really&lt;/em&gt; just about C++, and the subjects of many of the talks are easily applied to C, since C++ builds so much upon C. In a sense, &lt;strong&gt;a subset of CppCon could be considered a C conference&lt;/strong&gt;. That’s what I’m looking for when I watch the CppCon presentations each year on YouTube.&lt;/p&gt;
&lt;p&gt;Starting last year, I began a list of all the talks that I thought would be useful to C programmers. Some are entirely relevant to C, others just have significant portions that are relevant to C. When someone asks about where they can find a C conference, I send them my list.&lt;/p&gt;
&lt;p&gt;I’m sharing them here so you can bookmark this page and never return again.&lt;/p&gt;
&lt;h3 id=&quot;2017&quot;&gt;2017&lt;/h3&gt;
&lt;p&gt;Here’s the list for CppCon 2017. These are &lt;em&gt;roughly&lt;/em&gt; ordered from highest to lowest recommendation:&lt;/p&gt;
&lt;h3 id=&quot;2018&quot;&gt;2018&lt;/h3&gt;
&lt;p&gt;The final CppCon 2018 videos were uploaded this week, so my 2018 listing can be complete:&lt;/p&gt;
&lt;p&gt;There were three talks strictly about C++ that I thought were interesting from a language design perspective. So I think they’re worth recommending, too. (In fact, they’re a sort of ammo &lt;em&gt;against&lt;/em&gt; using C++ due to its insane complexity.)&lt;/p&gt;
&lt;h3 id=&quot;bonus&quot;&gt;Bonus&lt;/h3&gt;
&lt;p&gt;Finally, here are a few more good presentations from other C++ conferences which you can just pretend are about C:&lt;/p&gt;
&lt;ol class=&quot;references print-only&quot;/&gt;&lt;nav class=&quot;no-print&quot;&gt;
&lt;/nav&gt;
</description>
<pubDate>Wed, 21 Nov 2018 18:35:00 +0000</pubDate>
<dc:creator>ingve</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://nullprogram.com/blog/2018/11/21/</dc:identifier>
</item>
<item>
<title>Amazon admits it exposed customer email addresses, but refuses to give details</title>
<link>https://techcrunch.com/2018/11/21/amazon-admits-it-exposed-customer-email-addresses-doubles-down-on-secrecy/</link>
<guid isPermaLink="true" >https://techcrunch.com/2018/11/21/amazon-admits-it-exposed-customer-email-addresses-doubles-down-on-secrecy/</guid>
<description>&lt;p id=&quot;speakable-summary&quot;&gt;&lt;a class=&quot;crunchbase-link&quot; href=&quot;https://crunchbase.com/organization/amazon&quot; target=&quot;_blank&quot; data-type=&quot;organization&quot; data-entity=&quot;amazon&quot;&gt;Amazon’s&lt;/a&gt; renowned secrecy encompasses its response to a new security issue, withholding info that could help victims protect themselves.&lt;/p&gt;
&lt;p&gt;Amazon emailed users Tuesday, warning them that it exposed an unknown number of customer email addresses after a “technical error” on its website.&lt;/p&gt;
&lt;p&gt;When reached for comment, an Amazon spokesperson told TechCrunch that the issue exposed names as well as email addresses. “We have fixed the issue and informed customers who may have been impacted.” The company emailed all impacted users to be cautious.&lt;/p&gt;
&lt;p&gt;In response to a request for specifics, a spokesperson said the company had “nothing to add beyond our statement.” The company denies there was a data breach of its website of any of its systems, and says it’s fixed the issue, but dismissed our request for more info including the cause, scale and circumstances of the error.&lt;/p&gt;
&lt;p&gt;Amazon’s reticence here puts those impacted at greater risk. Users don’t know which of Amazon’s sites was impacted, who their email address could have been exposed to, or any ballpark figure of the number of victims. It’s also unclear whether it has or plans to contact any government regulatory bodies.&lt;/p&gt;
&lt;p&gt;“We’re contacting you to let you know that our website inadvertently disclosed your email address due to a technical error,” said Amazon in the email with the subject line: “Important Information about your Amazon.com Account.” The only details Amazon provided were that: “The issue has been fixed. This is not a result of anything you have done, and there is no need for you to change your password or take any other action.”&lt;/p&gt;
&lt;p&gt;The security lapse comes days ahead of one of the busiest retail days of the year, the post-Thanksgiving holiday sales day, Black Friday. The issue could scare users away from Amazon, which could be problematic for revenue if the issue impacted a wide number of users just before the heavy shopping day.&lt;/p&gt;
&lt;p&gt;Amazon’s vague and non-specific email also sparked criticism from users — including security experts — who accused the company of withholding information. Some said that the correspondence looked like a phishing email, used to trick customers into turning over account information.&lt;/p&gt;
&lt;p&gt;Customers in the U.S., the U.K. and Europe have reported receiving an email from Amazon.&lt;/p&gt;


&lt;p&gt;Amazon, as a Washington-based company, is required to inform the state attorney general of data incidents involving 500 state residents or more. Yet, in Europe, where data protection rules are stronger — even in the wake of the recently introduced General Data Protection Regulation (GDPR) — it’s less clear if Amazon needs to disclose the incident.&lt;/p&gt;
&lt;p&gt;The U.K.’s data protection regulator, the Information Commissioner’s Office, told TechCrunch: “Under the GDPR, organizations must assess if a breach should be reported to the ICO, or to the equivalent supervisory body if they are not based in the UK.”&lt;/p&gt;
&lt;p&gt;“It is always the company’s responsibility to identify when UK citizens have been affected as part of a data breach and take steps to reduce any harm to consumers,” a spokesperson said. “The ICO will however continue to monitor the situation and cooperate with other supervisory authorities where required.”&lt;/p&gt;
&lt;p&gt;To continue earning our trust, technology companies need to be forthcoming and transparent when security problems arise. Not only does that provide victims with the maximum amount of information they can use to recover and avoid future problems, but it also gives users confidence that their data is being responsibly managed no matter what happens.&lt;/p&gt;
&lt;p&gt;People fear what they don’t understand, and for now, Amazon is failing to help the public understand what happened.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;TechCrunch’s Natasha Lomas contributed to this report.&lt;/em&gt;&lt;/p&gt;

</description>
<pubDate>Wed, 21 Nov 2018 16:35:46 +0000</pubDate>
<dc:creator>Ours90</dc:creator>
<og:title>Amazon admits it exposed customer email addresses, but refuses to give details</og:title>
<og:description>Amazon’s renowned secrecy encompasses its response to a new security issue, withholding info that could help victims protect themselves. Amazon emailed users Tuesday, warning them that it exposed an unknown number of customer email addresses after a “technical error” on its website. Whe…</og:description>
<og:image>https://techcrunch.com/wp-content/uploads/2018/11/532516596.jpg?w=591</og:image>
<og:url>http://social.techcrunch.com/2018/11/21/amazon-admits-it-exposed-customer-email-addresses-doubles-down-on-secrecy/</og:url>
<og:type>article</og:type>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://techcrunch.com/2018/11/21/amazon-admits-it-exposed-customer-email-addresses-doubles-down-on-secrecy/</dc:identifier>
</item>
<item>
<title>Silent and Simple Ion Engine Powers a Plane with No Moving Parts</title>
<link>https://www.scientificamerican.com/article/silent-and-simple-ion-engine-powers-a-plane-with-no-moving-parts/</link>
<guid isPermaLink="true" >https://www.scientificamerican.com/article/silent-and-simple-ion-engine-powers-a-plane-with-no-moving-parts/</guid>
<description>&lt;div class=&quot;mura-region mura-region-loose&quot;&gt;
&lt;div class=&quot;mura-region-local&quot;&gt;
&lt;p&gt;Behind a thin white veil separating his makeshift lab from joggers at a Massachusetts Institute of Technology indoor track, aerospace engineer Steven Barrett recently test-flew the first-ever airplane powered with ionic wind thrusters—electric engines that generate momentum by creating and firing off charged particles.&lt;/p&gt;
&lt;p&gt;Using this principle to fly an aircraft has long been, according even to Barrett, a “far-fetched idea” and the stuff of science fiction. But he still wanted to try. “In &lt;em&gt;Star Trek&lt;/em&gt; you have shuttlecraft gliding silently past,” he says. “I thought, ‘We should have aircraft like that.’”&lt;/p&gt;
&lt;p&gt;Thinking ionic wind propulsion could fit the bill, he spent eight years studying the technology and then decided to try building a prototype miniature aircraft—albeit one he thought was a little ugly. “It’s a kind of dirty yellow color,” he says, adding that black paint often contains carbon—which conducts electricity and caused a previous iteration to fry itself.&lt;/p&gt;
&lt;p&gt;Barrett had slightly higher hopes for the latest prototype, which he dispassionately named Version 2. “Before we started the test flights I thought it had maybe a 50–50 chance,” he says. “My colleague at MIT thought it was more like a 1 percent chance it would work.”&lt;/p&gt;
&lt;div class=&quot;image-center&quot;&gt;&lt;img alt=&quot;&quot; height=&quot;1001&quot; src=&quot;https://static.scientificamerican.com/sciam/assets/Image/2018/cad%20black%20background%20from%20behind.jpg&quot; width=&quot;1920&quot;/&gt; Credit: MIT&lt;/div&gt;
&lt;p&gt;But unlike its predecessors, which had tumbled to the ground, Version 2 sailed nearly 200 feet through the air at roughly 11 miles per hour (17 kilometers per hour). With no visible exhaust and no roaring jet or whirling propeller—no moving parts at all, in fact—the aircraft seemed silently animated by an ethereal source. “It was very exciting,” Barrett says. “Then it crashed into the wall, which wasn’t ideal.”&lt;/p&gt;
&lt;p&gt;Still, Version 2 had worked, and Barrett and his colleagues published their results Wednesday in &lt;em&gt;Nature&lt;/em&gt;. The flight was a feat others have tried but failed, says Mitchell Walker, an aerospace engineer at Georgia Institute of Technology who did not work on the new plane. “[Barrett] has demonstrated something truly unique,” he says. Ion thrusters are not a particularly new technology; they already help push spacecraft very efficiently—but they are a far cry from rockets or jets, and normally nudge spacecraft into place in orbit. They have also propelled deep-space probes such as &lt;a href=&quot;https://www.nasa.gov/mission_pages/dawn/main/index.html&quot; target=&quot;_blank&quot;&gt;Dawn on missions to the Asteroid Belt&lt;/a&gt;. In the near-vacuum of space, ion thrusters have to carry an onboard supply of gas that they ionize and fire off into the relative emptiness to create thrust. When it comes to moving through Earth’s thick atmosphere, however, “everyone saw that the velocity [from an ion thruster] was not sufficient for propelling an aircraft,” Walker says. “Nobody understood how to go forward.”&lt;/p&gt;
&lt;p&gt;But Barrett and his team figured out three main things to make Version 2 work. The first was the ionic wind thruster design. Version 2’s thrusters consist of two rows of long metal strands draped under its sky blue wings. The front row conducts some 40,000 volts of electricity—166 times the voltage delivered to the average house, and enough energy to strip the electrons off ample nitrogen atoms hanging in the atmosphere.&lt;/p&gt;
&lt;p&gt;When that happens, the nitrogen atoms turn into positively charged ions. Because the back row of metal filaments carries a negative charge, the ions careen toward it like magnetized billiard balls. “Along the way, there are millions of collisions between these ions and neutral air molecules,” Barrett notes. That shoves the air molecules toward the back of the plane, creating a wind that pushes the plane forward fast and hard enough to fly.&lt;/p&gt;
&lt;p&gt;Another innovation Barrett’s team came up with was designing a lightweight but powerful electrical system, Walker notes. Before this aircraft, he says, nobody had created a system that could convert power from a lightweight battery efficiently enough to generate sufficient voltage for the thrusters. “The biggest challenge is [ion thrusters] need 20,000 or 30,000 volts just to work. High voltage on an aircraft doesn’t come easy,” he says. “You want to play with 40,000 volts on an aircraft? That technology didn’t exist. Steve [Barrett] found a clever way to get that efficient conversion.”&lt;/p&gt;
&lt;p&gt;Finally Barrett used a computer model to get the most out of every design element in the aircraft, from the thruster and electrical system designs to the wires that ran through the plane. “The power converter, the battery, the caps and fuselage—everything was optimized,” Barrett says. “The simulations failed all the time. We had to make hundreds of changes.” In the end, they had the triumphant Version 2.&lt;/p&gt;
&lt;p&gt;The breakthrough offers a great proof of concept showing ion thrusters can be used on Earth, says Alec Gallimore, an aerospace engineer at the University of Michigan who was not involved with the work. But any such use would likely be in limited capacities. Propellers and jets are still far more efficient than the ion wind thrusters Barrett demonstrated, making it unlikely that passenger planes would switch over anytime soon. But the thrusters have one key advantage: “There’s no sound generation. So [drones] for building inspections or things like that” would be an ideal application for these thrusters, Gallimore notes.&lt;/p&gt;
&lt;p&gt;Or, Barrett adds, drones used for deliveries, filming or environmental monitoring. “Imagine 10 or 20 years from now—we could have drones everywhere,” he says. “If those are all noisy, they’ll degrade our quality of life. But this is silent.”&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Wed, 21 Nov 2018 16:24:13 +0000</pubDate>
<dc:creator>tshannon</dc:creator>
<og:type>article</og:type>
<og:title>Silent and Simple Ion Engine Powers a Plane with No Moving Parts</og:title>
<og:description>Researchers fly the first atmospheric aircraft to use space-proven ionic thrust technology</og:description>
<og:image>https://static.scientificamerican.com/sciam/cache/file/026090AB-355F-4580-A23C2C9F6E4A284C.png</og:image>
<og:url>https://www.scientificamerican.com/article/silent-and-simple-ion-engine-powers-a-plane-with-no-moving-parts/</og:url>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.scientificamerican.com/article/silent-and-simple-ion-engine-powers-a-plane-with-no-moving-parts/</dc:identifier>
</item>
<item>
<title>Some Amazon employees bought NYC condos before news of HQ2 location emerged</title>
<link>https://thehill.com/policy/technology/417633-amazon-employees-bought-new-york-city-condos-before-company-announced-hq2</link>
<guid isPermaLink="true" >https://thehill.com/policy/technology/417633-amazon-employees-bought-new-york-city-condos-before-company-announced-hq2</guid>
<description>&lt;p class=&quot;p1&quot;&gt;At least two Amazon employees reportedly purchased condos in a New York City neighborhood before news emerged that the area had been picked to host the company's second headquarters.&lt;/p&gt;
&lt;p class=&quot;p1&quot;&gt;The employees decided to buy units in a new 11-story condo building in the Long Island City neighborhood of Queens just before the first reports of Amazon’s HQ2 location were released this month, &lt;a href=&quot;https://www.wsj.com/articles/amazon-employees-join-the-rush-to-buy-long-island-city-condos-1542709801?emailToken=47bd9e4b9a4ac2540d5cce62dc569cc1+y25pjaaGeCFGDrJdfwsyJWzq22SMiyBeVhjjahLesw7ddQ9vcn69REtH7Ab0fn/8iYbPPVq1uMtOMTlZ3+nCUYANWFzmpw6d/Atj6gHWeILJg8CrKJZwtygxFRTJWBg&amp;amp;reflink=article_copyURL_share&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;The Wall Street Journal reported&lt;/a&gt; Tuesday.&lt;/p&gt;
&lt;div id=&quot;dfp-ad-mosad_1-wrapper&quot; class=&quot;dfp-tag-wrapper wrapper&quot;&gt;
&lt;p&gt;&lt;span&gt;ADVERTISEMENT&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p class=&quot;p1&quot;&gt;While employees of companies are barred from buying or selling stocks based on information that has not yet been made public, lawyers told the Journal that they were unaware of any such ban affecting real estate transactions.&lt;/p&gt;
&lt;p class=&quot;p3&quot;&gt;Amazon announced last week that it would be splitting its second headquarters between Long Island City in Queens and Arlington County, Va., ending months of jockeying between cities and speculation of where the tech giant would land.&lt;/p&gt;
&lt;p class=&quot;p2&quot;&gt;There are no exact numbers on how many units have gone into contract in the Long Island City area since the announcement, but the Journal reports that one brokerage firm sold nearly 150 units just last week, 15 times its normal volume.&lt;/p&gt;
&lt;p&gt;Amazon will receive more than $1.5 billion in performance-based incentives from New York, including one tax break in which the state will pay $48,000 over the course of 12 years for each of the 25,000 jobs that the office will bring.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://thehill.com/policy/technology/416821-hq2-deal-brings-new-scrutiny-on-amazon&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Some leaders in the area are angry&lt;/a&gt; that New York is willing to fund a project that will contribute to gentrification and make the area more expensive for residents.&lt;/p&gt;
</description>
<pubDate>Wed, 21 Nov 2018 15:37:31 +0000</pubDate>
<dc:creator>jakelazaroff</dc:creator>
<og:type>article</og:type>
<og:title>Some Amazon employees bought NYC condos before news of HQ2 location emerged: report</og:title>
<og:url>https://thehill.com/policy/technology/417633-amazon-employees-bought-new-york-city-condos-before-company-announced-hq2</og:url>
<og:description>At least two Amazon employees reportedly purchased condos in a New York City neighborhood before news emerged that the area had been picked to host the company's second headquarters.</og:description>
<og:image>https://thehill.com/sites/default/files/bezosjeff10202017getty_0.jpg</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://thehill.com/policy/technology/417633-amazon-employees-bought-new-york-city-condos-before-company-announced-hq2</dc:identifier>
</item>
<item>
<title>A small French privacy ruling could remake adtech</title>
<link>https://techcrunch.com/2018/11/20/how-a-small-french-privacy-ruling-could-remake-adtech-for-good/</link>
<guid isPermaLink="true" >https://techcrunch.com/2018/11/20/how-a-small-french-privacy-ruling-could-remake-adtech-for-good/</guid>
<description>&lt;p id=&quot;speakable-summary&quot;&gt;A ruling in late October against a little-known French adtech firm that popped up on the national data watchdog’s website earlier this month is causing ripples of excitement to run through privacy watchers in Europe who believe it signals the beginning of the end for creepy online ads.&lt;/p&gt;
&lt;p&gt;The excitement is palpable.&lt;/p&gt;
&lt;p&gt;Impressively so, given the dry CNIL &lt;a href=&quot;https://www.legifrance.gouv.fr/affichCnil.do?oldAction=rechExpCnil&amp;amp;id=CNILTEXT000037594451&amp;amp;fastReqId=974682228&amp;amp;fastPos=2&quot;&gt;decision&lt;/a&gt; against mobile “demand side platform” Vectaury was only published in the regulator’s native dense French legalese.&lt;/p&gt;
&lt;div class=&quot;embed breakout&quot; readability=&quot;7.6638655462185&quot;&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-width=&quot;550&quot; data-dnt=&quot;true&quot; readability=&quot;9.3669467787115&quot;&gt;
&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Here is the bombshell though: Consent through the &lt;a href=&quot;https://twitter.com/IABEurope?ref_src=twsrc%5Etfw&quot;&gt;@IABEurope&lt;/a&gt; framework is inherently invalid. Not because of a technical detail. Not because of an implementation aspect that could be fixed. No.&lt;br/&gt;You cannot pass consent to another controller through a contractual relationship. BOOM &lt;a href=&quot;https://t.co/xMlNHJTKwl&quot;&gt;pic.twitter.com/xMlNHJTKwl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;— Robin Berjon (@robinberjon) &lt;a href=&quot;https://twitter.com/robinberjon/status/1063549736085536771?ref_src=twsrc%5Etfw&quot;&gt;November 16, 2018&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;Digital advertising trade press &lt;a href=&quot;https://adexchanger.com/ad-exchange-news/is-the-iabs-consent-framework-in-trouble/&quot;&gt;AdExchanger&lt;/a&gt; picked up on the decision yesterday.&lt;/p&gt;
&lt;p&gt;Here’s the killer paragraph from CNIL’s ruling — translated into “rough English” by my TC colleague Romain Dillet:&lt;/p&gt;
&lt;blockquote readability=&quot;9&quot;&gt;
&lt;p&gt;The requirement based on the article 7 above-mentioned isn’t fulfilled with a contractual clause that guarantees validly collected initial consent. The company VECTAURY should be able to show, for all data that it is processing, the validity of the expressed consent.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In plainer English, this is being interpreted by data experts as the regulator stating that consent to processing personal data cannot be gained through a framework arrangement which bundles a number of uses behind a single “I agree” button that, when clicked, passes consent to partners via a contractual relationship.&lt;/p&gt;
&lt;p&gt;CNIL’s decision suggests that bundling consent to partner processing in a contract is not, in and of itself, valid consent under the &lt;a class=&quot;crunchbase-link&quot; href=&quot;https://crunchbase.com/organization/european-union&quot; target=&quot;_blank&quot; data-type=&quot;organization&quot; data-entity=&quot;european-union&quot;&gt;European Union’s&lt;/a&gt; General Data Protection Regulation (GDPR) framework.&lt;/p&gt;
&lt;p&gt;Consent under this regime must be specific, informed and freely given. It says as much in the text of GDPR.&lt;/p&gt;
&lt;p&gt;But now, on top of that, the CNIL’s ruling suggests a data controller has to be able to demonstrate the &lt;em&gt;validity&lt;/em&gt; of the consent — so cannot simply tuck consent inside a contractual “carpet-bag” that gets passed around to everyone else in their chain as soon as the user clicks “I agree.”&lt;/p&gt;
&lt;p&gt;This is important, because many widely used digital advertising consent frameworks rolled out to websites in Europe this year — in claimed compliance with GDPR — are using a contractual route to obtain consent, and bundling partner processing behind often hideously labyrinthine consent flows.&lt;/p&gt;
&lt;p&gt;The experience for web users in the EU right now is not great. But it could be leading to a much better internet down the road.&lt;/p&gt;
&lt;h2&gt;Where’s the consent for partner processing?&lt;/h2&gt;
&lt;p&gt;Even on a surface level the current crop of confusing consent mazes look problematic.&lt;/p&gt;
&lt;p&gt;But the CNIL ruling suggests there are deeper and more structural problems lurking and embedded within. And as regulators dig in and start to unpick adtech contradictions it could force a change of mindset across the entire ecosystem.&lt;/p&gt;
&lt;p&gt;As ever, when talking about consent and online ads the overarching point to remember is that no consumer given a genuine full disclosure about what’s being done with their personal data in the name of behavioral advertising would freely consent to personal details being hawked and traded across the web just so a bunch of third parties can bag a profit share.&lt;/p&gt;
&lt;p&gt;This is why, despite GDPR being in force (since May 25), there are still so many tortuously confusing “consent flows” in play.&lt;/p&gt;
&lt;p&gt;The longstanding online T&amp;amp;Cs trick of obfuscating and socially engineering consent remains an unfortunately standard playbook. But, less than six months into GDPR we’re still very much in a “phoney war” phase. More regulatory rulings are needed to lay down the rules by actually enforcing the law.&lt;/p&gt;
&lt;p&gt;And CNIL’s recent activity suggests more to come.&lt;/p&gt;
&lt;p&gt;In the Vectaury case, the mobile ad firm used a template framework for its consent flow that had been created by industry trade association and standards body, IAB Europe.&lt;/p&gt;
&lt;p&gt;It did make some of its own choices, using its own wording on an initial consent screen and pre-ticking the purposes (another big GDPR no-no). But the bundling of data purposes behind a single opt in/out button is the core IAB Europe design. So CNIL’s ruling suggests there could be trouble ahead for other users of the template.&lt;/p&gt;
&lt;p&gt;IAB Europe’s CEO, Townsend Feehan, told us it’s working on a statement reaction to the CNIL decision, but suggested Vectaury fell foul of the regulator because it may not have implemented the “Transparency &amp;amp; Consent Framework-compliant” consent management platform (CMP) framework — as it’s tortuously known — correctly.&lt;/p&gt;
&lt;p&gt;So either “the ‘CMP’ that they implemented did not align to our Policies, or choices they could have made in the implementation of their CMP that would have facilitated compliance with the GDPR were not made,” she suggested to us via email.&lt;/p&gt;
&lt;p&gt;Though that sidesteps the contractual crux point that’s really exciting privacy advocates — and making them point to the CNIL as having slammed the first of many unbolted doors.&lt;/p&gt;
&lt;p&gt;The French watchdog has made a handful of other decisions in recent months, also involving geolocation-harvesting adtech firms, and also for processing data without consent.&lt;/p&gt;
&lt;p&gt;So regulatory activity on the GDPR+adtech front has been ticking up.&lt;/p&gt;
&lt;p&gt;Its decision to publish these rulings suggests it has wider concerns about the scale and privacy risks of current programmatic ad practices in the mobile space than can be attached to any single player.&lt;/p&gt;
&lt;p&gt;So the suggestion is that just publishing the rulings looks intended to put the industry on notice…&lt;/p&gt;
&lt;div class=&quot;embed breakout&quot; readability=&quot;8.3028169014085&quot;&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-width=&quot;550&quot; data-dnt=&quot;true&quot; readability=&quot;10.147887323944&quot;&gt;
&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;The decision also notes that the &lt;a href=&quot;https://twitter.com/CNIL?ref_src=twsrc%5Etfw&quot;&gt;@CNIL&lt;/a&gt; is openly using this to inform not just the company in question but whole ecosystem, including adtech of course but also app makers who embed ads and marketers who use them. You're all on notice!&lt;/p&gt;
&lt;p&gt;— Robin Berjon (@robinberjon) &lt;a href=&quot;https://twitter.com/robinberjon/status/1063549728649084928?ref_src=twsrc%5Etfw&quot;&gt;November 16, 2018&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;Meanwhile, adtech giant &lt;a class=&quot;crunchbase-link&quot; href=&quot;https://crunchbase.com/organization/google&quot; target=&quot;_blank&quot; data-type=&quot;organization&quot; data-entity=&quot;google&quot;&gt;Google&lt;/a&gt; has also made itself unpopular with publisher “partners” over its approach to GDPR by forcing them to collect consent on its behalf. And in May a group of European and international publishers complained that Google was &lt;a href=&quot;https://techcrunch.com/2018/05/01/google-accused-of-using-gdpr-to-impose-unfair-terms-on-publishers/&quot;&gt;imposing unfair terms on them&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The CNIL decision could sharpen that complaint too — raising questions over whether audits of publishers that Google said it would carry out will be enough for the arrangement to pass regulatory muster.&lt;/p&gt;
&lt;div class=&quot;embed breakout&quot; readability=&quot;7.9662162162162&quot;&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-width=&quot;550&quot; data-dnt=&quot;true&quot; readability=&quot;9.7364864864865&quot;&gt;
&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;This rules the &lt;a href=&quot;https://twitter.com/IABEurope?ref_src=twsrc%5Etfw&quot;&gt;@IABEurope&lt;/a&gt; out as an option, but more than that: &lt;a href=&quot;https://twitter.com/Google?ref_src=twsrc%5Etfw&quot;&gt;@Google&lt;/a&gt; forced publishers to collect consent on its behalf for advertising profiling. They have said that they will audit that publishers do it right — but will auditing be enough?&lt;/p&gt;
&lt;p&gt;— Robin Berjon (@robinberjon) &lt;a href=&quot;https://twitter.com/robinberjon/status/1063549739814268933?ref_src=twsrc%5Etfw&quot;&gt;November 16, 2018&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;p&gt;For a demand-side platform like Vectaury, which was acting on behalf of more than 32,000 partner mobile apps with user eyeballs to trade for ad cash, achieving GDPR compliance would mean either asking users for genuine consent and/or having a very large number of contracts on which it’s doing actual due diligence.&lt;/p&gt;
&lt;p&gt;Yet Google is orders of magnitude more massive, of course.&lt;/p&gt;
&lt;p&gt;The Vectaury file gives us a fascinating little glimpse into adtech “business as usual.” Business which also wasn’t, in the regulator’s view, legal.&lt;/p&gt;
&lt;p&gt;The firm was harvesting a bunch of personal data (including people’s location and device IDs) on its partners’ mobile users via an SDK embedded in their apps, and receiving bids for these users’ eyeballs via another standard piece of the programmatic advertising pipe — ad exchanges and supply side platforms — which also get passed personal data so they can broadcast it widely via the online ad world’s real-time bidding (RTB) system. That’s to solicit potential advertisers’ bids for the attention of the individual app user… The wider the personal data gets spread, the more potential ad bids.&lt;/p&gt;
&lt;p&gt;That scale is how programmatic works. It also looks horrible from a GDPR “privacy by design and default” standpoint.&lt;/p&gt;
&lt;p&gt;The sprawling process of programmatic explains the very long list of “partners” nested non-transparently behind the average publisher’s online consent flow. The industry, as it is shaped now, literally trades on personal data.&lt;/p&gt;
&lt;p&gt;So if the consent rug it’s been squatting on for years suddenly gets ripped out from underneath it, there would need to be radical reshaping of ad-targeting practices to avoid trampling on EU citizens’ fundamental right.&lt;/p&gt;
&lt;p&gt;GDPR’s really big change was supersized fines. So ignoring the law would get very expensive.&lt;/p&gt;
&lt;h2&gt;Oh hai real-time bidding!&lt;/h2&gt;
&lt;p&gt;In Vectaury’s case, CNIL discovered the company was holding the personal data of a staggering 67.6 million people when it conducted an on-site inspection of the company in April 2018.&lt;/p&gt;
&lt;p&gt;That already sounds like A LOT of data for a small mobile adtech player. Yet it might actually have been a tiny fraction of the personal data the company was routinely handling — given that Vectaury’s own website claims 70 percent of collected data is not stored.&lt;/p&gt;
&lt;p&gt;In the decision there was no fine, but CNIL ordered the firm to delete all data it had not already deleted (having judged collection illegal given consent was not valid); and to stop processing data without consent.&lt;/p&gt;
&lt;p&gt;But given the personal-data-based hinge of current-gen programmatic adtech, that essentially looks like an order to go out of business. (Or at least out of that business.)&lt;/p&gt;
&lt;p&gt;And now we come to another interesting GDPR adtech complaint that’s not yet been ruled on by the two DPAs in question (Ireland and the U.K.) — but which looks even more compelling in light of the CNIL Vectaury decision because it picks at the adtech scab even more daringly.&lt;/p&gt;
&lt;p&gt;Filed last month with the Irish Data Protection Commission and the U.K.’s ICO, this adtech &lt;a href=&quot;https://brave.com/adtech-data-breach-complaint&quot;&gt;complaint&lt;/a&gt; — the work of three individuals, Johnny Ryan of private web browser Brave; Jim Killock, exec director of digital and civil rights group, the Open Rights Group; and University College London data protection researcher, Michael Veale — targets the RTB system itself.&lt;/p&gt;
&lt;p&gt;Here’s how Ryan, Killock and Veale summarized the complaint when they &lt;a href=&quot;https://brave.com/adtech-data-breach-complaint&quot;&gt;announced it last month&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote readability=&quot;27&quot;&gt;
&lt;p&gt;Every time a person visits a website and is shown a “behavioural” ad on a website, intimate personal data that describes each visitor, and what they are watching online, is broadcast to tens or hundreds of companies. Advertising technology companies broadcast these data widely in order to solicit potential advertisers’ bids for the attention of the specific individual visiting the website.&lt;/p&gt;
&lt;p&gt;A data breach occurs because this broadcast, known as an “bid request” in the online industry, fails to protect these intimate data against unauthorized access. Under the GDPR this is unlawful.&lt;/p&gt;
&lt;p&gt;The GDPR, Article 5, paragraph 1, point f, requires that personal data be “processed in a manner that ensures appropriate security of the personal data, including protection against unauthorised or unlawful processing and against accidental loss.” If you can not protect data in this way, then the GDPR says you can not process the data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ryan tells TechCrunch that the crux of the complaint is not related to the legal basis of the data sharing but rather focuses on the &lt;em&gt;processing&lt;/em&gt; itself — arguing “that it itself is not adequately secure… that they’re aren’t adequate controls.”&lt;/p&gt;
&lt;p&gt;Though he says there’s a consent element too, and so sees the CNIL ruling bolstering the RTB complaint. (On that keep in mind that CNIL judged Vectaury should not have been holding the &lt;em&gt;RTB data&lt;/em&gt; of 67.6M people because it did not have valid consent.)&lt;/p&gt;
&lt;p&gt;“We do pick up on the issue of consent in the complaint. And this particular CNIL decision has a bearing on both of those issues,” he argues. “It demonstrates in a concrete example that involved investigators going into physical premises and checking the machines — it demonstrates that even one small company was receiving tens of millions of people’s personal data in this illegal way.&lt;/p&gt;
&lt;p&gt;“So the breach is very real. And it demonstrates that it’s not unreasonable to suggest that the consent is meaningless in any case.”&lt;/p&gt;
&lt;p&gt;Reaching for a handy visual explainer, he continues: “If I leave a briefcase full of personal data in the middle of Charing Cross station at 11am and it’s really busy, that’s a breach. That would have been a breach back in the 1970s. If my business model is to drive up to Charing Cross station with a dump-truck and dump briefcases onto the street at 11am in the full knowledge that my business partners will all scramble around and try and grab them — and then to turn up at 11.01am and do the same thing. And then 11.02am. And every microsecond in between. That’s still a fucking data breach!&lt;/p&gt;
&lt;p&gt;“It doesn’t matter if you think you’ve consent or anything else. You have to [comply with GDPR Article 5, paragraph 1, point f] in order to even be able to ask for a legal basis. There are plenty of other problems but that’s the biggest one that we highlighted. That’s our reason for saying this is a breach.”&lt;/p&gt;
&lt;p&gt;“Now what CNIL has said is this company, Vectaury, was processing personal data that it did not lawfully have — and it got them through RTB,” he adds, spelling the point out. “So back to the GDPR — GDPR is saying you can’t process data in a way that doesn’t ensure protection against unauthorized or unlawful processing.”&lt;/p&gt;
&lt;p&gt;In other words, RTB as a funnel for processing personal data looks to be on inherently shaky ground because it’s inherently putting all this personal data out there and at risk…&lt;/p&gt;
&lt;h2&gt;What’s bad for data brokers…&lt;/h2&gt;
&lt;p&gt;In another loop back, Ryan says the regulators have been in touch since their RTB complaint was filed to invite them to submit more information.&lt;/p&gt;
&lt;p&gt;He says the CNIL Vectaury decision will be incorporated into further submissions, predicting: “This is going to be bounced around multiple regulators.”&lt;/p&gt;
&lt;p&gt;The trio is keen to generate extra bounce by working with NGOs to enlist other individuals to file similar complaints in other EU Member States — to make the action a pan-European push, just like programmatic advertising itself.&lt;/p&gt;
&lt;p&gt;“We now have the opportunity to connect our complaint with the excellent work that Privacy International has done, showing &lt;a href=&quot;https://privacyinternational.org/campaigns/tell-companies-stop-exploiting-your-data&quot;&gt;where these data end up&lt;/a&gt;, and with the excellent work that CNIL has done showing exactly how this actually applies. And this decision from CNIL takes, essentially my report that went with our complaint and shows exactly how that applies in the real world,” he continues.&lt;/p&gt;
&lt;p&gt;“I was writing in the abstract — CNIL has now made a decision that is very much not in the abstract, it’s in the real world affecting millions of people… This will be a European-wide complaint.”&lt;/p&gt;
&lt;p&gt;But what does programmatic advertising that doesn’t entail trading on people’s grubbily obtained personal data &lt;em&gt;actually&lt;/em&gt; look like? If there were no personal data in bid requests Ryan believes quite a few things would happen. Such as, for e.g. the demise of clickbait.&lt;/p&gt;
&lt;p&gt;“There would be no way to take your TechCrunch audience and buy it cheaper on some shitty website. There would be no more of that arbitrage stuff. Clickbait would die! All that nasty stuff would go away,” he suggests.&lt;/p&gt;
&lt;p&gt;(And, well, full disclosure: We are TechCrunch — so we can confirm that does sound really great to us!)&lt;/p&gt;
&lt;p&gt;He also reckons ad values would go up. Which would also be good news for publishers. (“Because the only place you could buy the TechCrunch audience would be on TechCrunch — that’s a really big deal!”)&lt;/p&gt;
&lt;p&gt;He even suggests ad fraud might shrink because the incentives would shift. Or at least they could so long as the “worthy” publishers that are able to survive in the new ad world order don’t end up being complicit with bot fraud anyway.&lt;/p&gt;
&lt;p&gt;As it stands, publishers are being screwed between the twin plates of the dominant adtech platforms (Google and &lt;a class=&quot;crunchbase-link&quot; href=&quot;https://crunchbase.com/organization/facebook&quot; target=&quot;_blank&quot; data-type=&quot;organization&quot; data-entity=&quot;facebook&quot;&gt;Facebook),&lt;/a&gt; where they are having to give up a majority of their ad revenue — leaving the media industry with a shrinking slice of ad revenues (that can be as lean as ~30 percent).&lt;/p&gt;
&lt;p&gt;That then has a knock on impact on funding newsrooms and quality journalism. And, well, on the wider web too — given all the weird incentives that operate in today’s big tech social media platform-dominated internet.&lt;/p&gt;
&lt;p&gt;While a privacy-sucking programmatic monster is something only shadowy background data brokers that lack any meaningful relationships with the people whose data they’re feeding the beast could truly love.&lt;/p&gt;
&lt;p&gt;And, well, Google and Facebook.&lt;/p&gt;
&lt;p&gt;Ryan’s view is that the reason an adtech duopoly exists boils down to the “audience leakage” being enabled by RTB. Leakage which, in his view, also isn’t compliant with EU privacy laws.&lt;/p&gt;
&lt;p&gt;He reckons the fix for this problem is equally simple: Keep doing RTB but without any personal data.&lt;/p&gt;
&lt;p&gt;A real-time ad bidding system that’s been stripped of personal data does not mean no targeted ads. It could still support ad targeting based on real-time factors such as an approximate location (say to a city region) and/or generic and aggregated data.&lt;/p&gt;
&lt;p&gt;Crucially it would not use unique identifiers that enable linking ad bids to a individual’s entire digital footprint and bid request history — as is the case now. Which essentially translates into: RIP privacy rights.&lt;/p&gt;
&lt;p&gt;Ryan argues that RTB without personal data would still offer plenty of “value” to advertisers —&lt;span&gt; who could still reach people based on general locations and via real-time interests. (It’s a model that sounds much like what privacy search engine DuckDuckGo is doing, and also &lt;a href=&quot;https://techcrunch.com/2018/08/29/duckduckgo-gets-10m-from-omers-for-global-privacy-push/&quot;&gt;been growing&lt;/a&gt;.)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The really big problem, though, is turning the behavioral ad tanker around. Given that the ecosystem is embedded, even as the duopoly milks it.&lt;/p&gt;
&lt;p&gt;That’s also why Ryan is so hopeful now, though, having parsed the CNIL decision.&lt;/p&gt;
&lt;p&gt;His reading is regulators will play a decisive role in pushing the ad industry’s trigger — and force through much-needed change in their targeting behavior.&lt;/p&gt;
&lt;p&gt;“Unless the entire industry moves together, no one can be the first to remove personal data from bid requests but if the regulators step in in a big way… and say you’re all going to go out of business if you keep putting personal data into bid requests then everyone will come together — like the music industry was forced to eventually, under Steve Jobs,” he argues. “Everyone can together decide on a new short term disadvantageous but long term highly advantageous change.”&lt;/p&gt;
&lt;p&gt;Of course such a radical reshaping is not going to happen overnight. Regulatory triggers tend to be slow motion unfoldings at the best of times. You also have to factor in the inexorable legal challenges.&lt;/p&gt;
&lt;p&gt;But look closely and you’ll see both momentum massing behind privacy — and regulatory writing on the wall.&lt;/p&gt;
&lt;p&gt;“Are we going to see programmatic forced to be non-personal and therefore better for every single citizen of the world (except, say, if they work for a data broker),” adds Ryan, posing his own concluding question. “Will that massive change, which will help society and the web… will that change happen before Christmas? No. But it’s worth working on. And it’s going to take some time.&lt;/p&gt;
&lt;p&gt;“It could be two years from now that we have the finality. But a finality there will be. Detroit was only able to fight against regulation for so long. It does come.”&lt;/p&gt;
&lt;p&gt;Who’d have though “taking back control” could ever sound so good?&lt;/p&gt;
</description>
<pubDate>Wed, 21 Nov 2018 09:38:54 +0000</pubDate>
<dc:creator>gbugniot</dc:creator>
<og:title>How a small French privacy ruling could remake adtech for good</og:title>
<og:description>A ruling in late October against a little-known French adtech firm that popped up on the national data watchdog’s website earlier this month is causing ripples of excitement to run through privacy watchers in Europe who believe it signals the beginning of the end for creepy online ads. The ex…</og:description>
<og:image>https://techcrunch.com/wp-content/uploads/2017/08/clickbait.png?w=711</og:image>
<og:url>http://social.techcrunch.com/2018/11/20/how-a-small-french-privacy-ruling-could-remake-adtech-for-good/</og:url>
<og:type>article</og:type>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://techcrunch.com/2018/11/20/how-a-small-french-privacy-ruling-could-remake-adtech-for-good/</dc:identifier>
</item>
<item>
<title>D compilation is too slow and I am forking the compiler</title>
<link>https://blog.thecybershadow.net/2018/11/18/d-compilation-is-too-slow-and-i-am-forking-the-compiler/</link>
<guid isPermaLink="true" >https://blog.thecybershadow.net/2018/11/18/d-compilation-is-too-slow-and-i-am-forking-the-compiler/</guid>
<description>&lt;p&gt;While working on my current project, the constant creep of increasing compilation times was becoming more and more noticeable. Even after throwing &lt;a href=&quot;https://blog.thecybershadow.net/2018/02/07/dmdprof/&quot;&gt;my usual tools&lt;/a&gt; at the problem, the total time was still over 7 seconds.&lt;/p&gt;
&lt;img src=&quot;https://dump.thecybershadow.net/7eb292cf55b670d972353e5330d8eb00/seven-seconds-reeeeeeee.png&quot; alt=&quot;Seven. Seconds!&quot;/&gt;&lt;p&gt;&lt;em&gt;Seven&lt;/em&gt;. &lt;strong&gt;Seconds&lt;/strong&gt;. &lt;em&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=07So_lJQyqw&quot;&gt;Unacceptable‼&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Compile time profiling showed that the blame lay with my liberal use of metaprogramming and &lt;code&gt;std.regex&lt;/code&gt;, which I wasn’t willing to give up on. The usual approach to reducing D build times is to split the program into packages, compile one package at a time (to a static library or object file), use D “header” files (&lt;code&gt;.di&lt;/code&gt;) to avoid parsing implementations more often than necessary, then link everything together. However, this was too much work, didn’t fit neatly into my existing toolchain, and I wanted to try something else.&lt;span id=&quot;more&quot;/&gt;&lt;/p&gt;
&lt;p&gt;C++ compilers handle a very similar problem with &lt;a href=&quot;https://en.wikipedia.org/wiki/Precompiled_header&quot;&gt;&lt;em&gt;precompiled headers&lt;/em&gt;&lt;/a&gt;, in which the results of parsing header files is serialized and saved to disk in an implementation-defined format. D implementations do not have an analogous feature at the moment, but that still left me wondering:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;The precompiled header format is highly implementation-specific, and compilers are free to break compatibility even across minor versions.&lt;/li&gt;
&lt;li&gt;The goal is to preserve information and avoid doing repeated work across multiple invocations of the compiler.&lt;/li&gt;
&lt;li&gt;The main difficulty with implementing it is serializing the compiler’s internal state to disk, and deserializing it back into memory.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;But… what if we didn’t care about saving it to disk? Can we checkpoint the compiler as it’s parsing the files one-by-one, and rewind as necessary?&lt;/p&gt;
&lt;p&gt;Hmm… If only there was some way to efficiently snapshot a process’s internal state, and rewind / resume it back to that point… Oh wait, there is – and it’s called &lt;strong&gt;&lt;code&gt;fork()&lt;/code&gt;&lt;/strong&gt;. Granted, not a typical use case, but still a promising idea to build on. Let’s get to work.&lt;/p&gt;
&lt;h2 id=&quot;part-1-ndash-graph-theory&quot;&gt;Part 1 – Graph Theory&lt;/h2&gt;
&lt;p&gt;In order to correctly partially rebuild a D program, we need to know which parts need to be rebuilt. In our case, this comes down to knowing which files need to be recompiled when a certain file changes.&lt;/p&gt;
&lt;p&gt;Luckily, DMD has us covered, as it can output dependency information in a plethora of formats. Great, let’s use that! Slurping the results of &lt;code&gt;-deps&lt;/code&gt; gets us a directed graph like this:&lt;/p&gt;
&lt;a href=&quot;https://dump.thecybershadow.net/11b8ff14e135f478111a4ad12df2e619/deps-digger-full.dot.svg&quot;&gt;&lt;img src=&quot;https://dump.thecybershadow.net/11b8ff14e135f478111a4ad12df2e619/deps-digger-full.dot.svg&quot; alt=&quot;GraphViz output&quot;/&gt;&lt;/a&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://github.com/CyberShadow/Digger&quot;&gt;Digger&lt;/a&gt;’s module dependencies &lt;span class=&quot;avoidwrap&quot;&gt;(click to enlarge)&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As you can &lt;em&gt;clearly&lt;/em&gt; see…&lt;/p&gt;
&lt;p&gt;Ahem. Let’s try that again:&lt;/p&gt;
&lt;img src=&quot;https://dump.thecybershadow.net/ac32e21b2dc4ccd2bbe61cd21c89cfb8/deps-digger-only.dot.svg&quot; alt=&quot;GraphViz output&quot;/&gt;&lt;p&gt;&lt;em&gt;Digger’s module dependencies, &lt;strong class=&quot;avoidwrap&quot;&gt;excluding libraries&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As you may have noticed, this graph has cycles. D does allow cyclic dependencies between modules (as long as at most one module in the cycle has static constructors), which is a problem for us as we can’t meaningfully snapshot the compiler in the middle of a cycle – we need to treat them as an all-or-nothing indivisible entity.&lt;/p&gt;
&lt;p&gt;Fortunately, graph theory comes to the rescue, and we can use &lt;a href=&quot;https://en.wikipedia.org/wiki/Kosaraju's_algorithm&quot;&gt;Kosaraju’s clever algorithm&lt;/a&gt; to separate the graph into &lt;a href=&quot;https://en.wikipedia.org/wiki/Strongly_connected_component&quot;&gt;strongly connected components&lt;/a&gt;. This simplifies the graph into a &lt;a href=&quot;https://en.wikipedia.org/wiki/Directed_acyclic_graph&quot;&gt;DAG&lt;/a&gt;:&lt;/p&gt;
&lt;img src=&quot;https://dump.thecybershadow.net/e7307eea382a4e07f2b8e0172d5d664b/deps-digger-only-scc.dot.svg&quot; alt=&quot;GraphViz output&quot;/&gt;&lt;em&gt;No more cycles!&lt;/em&gt;
&lt;p&gt;Our job is not done yet. We need to flatten this DAG into a list, as we can’t meaningfully combine checkpoints in which one is not a parent of another. We can use &lt;a href=&quot;https://en.wikipedia.org/wiki/Topological_sorting&quot;&gt;topological sorting&lt;/a&gt; to get there:&lt;/p&gt;
&lt;img src=&quot;https://dump.thecybershadow.net/eeb5a1af4ceaac5250bab48919c0b7bb/deps-digger-only-linear.dot.svg&quot; alt=&quot;GraphViz output&quot;/&gt;&lt;em&gt;All the arrows are pointing to the right.&lt;/em&gt;
&lt;p&gt;Great! Now, we have a list of components, wherein each component is a group of files that we can compile together, and create a snapshot after each such component.&lt;/p&gt;
&lt;p&gt;File modification times work nicely as a secondary sorting parameter: we can assume that more recently modified files are more likely to be modified again soon, so placing them in front of older files can save us from rebuilding older files (ranked equally dependency-wise).&lt;/p&gt;
&lt;p&gt;(Implementation note: here I got lazy and used an О(&lt;em&gt;n&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;) &lt;a href=&quot;https://github.com/CyberShadow/dmd/blob/660771f1f97d92624e416f0ed4e1499dce748d9d/dmdforker/dmdforker.d#L245-L272&quot;&gt;insertion sort&lt;/a&gt; for the topological/chronological sorting, though I’m sure a more efficient algorithm exists. As an additional curiosity, &lt;a href=&quot;https://dlang.org/phobos/std_algorithm_sorting.html#sort&quot;&gt;&lt;code&gt;std.algorithm.sort&lt;/code&gt;&lt;/a&gt; won’t work here because it apparently expects a &lt;a href=&quot;https://en.wikipedia.org/wiki/Weak_ordering&quot;&gt;&lt;em&gt;weak order&lt;/em&gt;&lt;/a&gt; for its predicate, while we only have a &lt;a href=&quot;https://en.wikipedia.org/wiki/Partially_ordered_set#Strict_and_non-strict_partial_orders&quot;&gt;&lt;em&gt;strict partial order&lt;/em&gt;&lt;/a&gt; – full credit to Feep on IRC for figuring this out.)&lt;/p&gt;
&lt;h2 id=&quot;part-2-ndash-hacking-the-compiler&quot;&gt;Part 2 – Hacking the Compiler&lt;/h2&gt;
&lt;p&gt;All the graph theory in the world will not help us in our endeavor if we can’t get the compiler to actually compile files one-by-one. Currently, DMD compiles source code in roughly the following order:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Read all source files (modules) given to it on its command line into memory&lt;/li&gt;
&lt;li&gt;Parse all loaded modules&lt;/li&gt;
&lt;li&gt;Load all imported modules, and parse them too&lt;/li&gt;
&lt;li&gt;Perform the first semantic pass on all modules&lt;/li&gt;
&lt;li&gt;Perform the second semantic pass on all modules&lt;/li&gt;
&lt;li&gt;Perform the third semantic pass on all modules&lt;/li&gt;
&lt;li&gt;Perform inlining (yes, DMD inlines in the frontend by manipulating the language AST, not backend) on all modules&lt;/li&gt;
&lt;li&gt;Generate code and write object files for all modules&lt;/li&gt;
&lt;li&gt;Link.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;This is clearly unsuitable for us: we want to be able to perform as many steps as possible &lt;em&gt;one file at a time&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Fortunately, we can coax the compiler in doing just that with a few &lt;a href=&quot;https://github.com/CyberShadow/dmd/compare/1965a4352..bcfd2d249#diff-12efbb2a88b713dbb0fd0a64673ba132R672&quot;&gt;relatively simple changes&lt;/a&gt;: instead of processing all files at once, read a group of them at a time, go through all the motions to compile it, and repeat.&lt;/p&gt;
&lt;p&gt;I wish I could say I got all compilation steps to work serially. Unfortunately, I haven’t managed to serialize code generation - my attempts ended with linking errors involving template instantiations (an area of the compiler which everyone agrees has become unordinarily messy). My theory is that the compiler is attempting to place template instantiations in object files whose modules have already been compiled. In theory, &lt;a href=&quot;https://dlang.org/dmd.html#switch-allinst&quot;&gt;the &lt;code&gt;-allinst&lt;/code&gt; switch&lt;/a&gt; is supposed to alleviate that (by emitting template instantiations in every module which instantiates the template, rather than just once); however, it currently appears to be &lt;a href=&quot;https://issues.dlang.org/show_bug.cgi?id=19406&quot;&gt;a bit broken&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Still, the third semantic pass &lt;em&gt;does&lt;/em&gt; work serially (in my tests so far), and takes a significant part of compilation time, so this is still a win for us!&lt;/p&gt;
&lt;h2 id=&quot;part-3-ndash-it-s-forking-time&quot;&gt;Part 3 – It’s Forking Time&lt;/h2&gt;
&lt;p&gt;We’re getting to the meat of the subject: actually making use of &lt;code&gt;fork()&lt;/code&gt; for snapshotting.&lt;/p&gt;
&lt;p&gt;Recall the flattened dependency graph from part 1:&lt;/p&gt;
&lt;img src=&quot;https://dump.thecybershadow.net/eeb5a1af4ceaac5250bab48919c0b7bb/deps-digger-only-linear.dot.svg&quot; alt=&quot;GraphViz output&quot;/&gt;&lt;p&gt;Here’s how the corresponding snapshots should look like:&lt;/p&gt;
&lt;img src=&quot;https://dump.thecybershadow.net/5c9165cffb3c6bb323ef2a06cdb7fffa/deps-digger-snapshots.dot.svg&quot; alt=&quot;GraphViz output&quot;/&gt;&lt;em&gt;One rectangle = one snapshot. Compilation order is right-to-left.&lt;/em&gt;
&lt;p&gt;Note how the compilation order is the &lt;em&gt;reverse&lt;/em&gt; of the topological order (we’ll get back to this in a bit). All snapshots except the first hold the result of compiling one module, plus all the modules before it; &lt;code&gt;fork()&lt;/code&gt;’s &lt;a href=&quot;https://en.wikipedia.org/wiki/Copy-on-write&quot;&gt;CoW memory&lt;/a&gt; allows all snapshots to share memory pages for previous modules, thus not consuming extra RAM (though real results vary a bit due to fragmentation).&lt;/p&gt;
&lt;p&gt;We are going to split our design into three parts:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;A separate fork driver program, which handles module dependencies, and decides when it’s time to recompile, which files need to be recompiled, and in which order.&lt;/li&gt;
&lt;li&gt;A “fork-server” compiler process, which communicates with the fork driver and is the owner of snapshot fork processes. Which process is the fork server can change when rewinding, but there should be exactly one fork server (i.e. process reading commands from the driver) at a time.&lt;/li&gt;
&lt;li&gt;All the snapshot fork processes.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Implementation-wise, our goals are:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Keep a communication channel open with the driver program, which sends commands like “compile these files” or “rewind”.&lt;/li&gt;
&lt;li&gt;Ensure the main instance doesn’t crash, even in the face of segmentation faults or ICEs in compiler code.&lt;/li&gt;
&lt;li&gt;Reasonably manage resources, and clean up old snapshots as necessary.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The commands the driver program can send would then be:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Compile the next group of files, and reply with success/failure&lt;/li&gt;
&lt;li&gt;Create a snapshot&lt;/li&gt;
&lt;li&gt;Rewind to a previously-created snapshot&lt;/li&gt;
&lt;li&gt;Finish compilation with all the files compiled so far (i.e. emit machine code and link).&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Some optimizations we can do upon closer examination:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;We can combine the first two commands into one, and create a snapshot automatically before compiling a new file group.&lt;/li&gt;
&lt;li&gt;Since we need to fork (in order to snapshot) before compiling a file group anyway, we can use that fork as our “backup” in case the compilation errors out or crashes. In this case, the fork is promoted to the current fork-server instead of becoming a snapshot.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;We also need a channel for snapshots to communicate with the fork-server, which we can do with a simple &lt;a href=&quot;http://pubs.opengroup.org/onlinepubs/9699919799/functions/pipe.html&quot;&gt;POSIX &lt;code&gt;pipe&lt;/code&gt;&lt;/a&gt;. This brings the lifetime of the compiler process and its forks to the following:&lt;/p&gt;
&lt;img src=&quot;https://dump.thecybershadow.net/209a50160cb580ad14f943b4f9fe5e26/fork-flow.dot.svg&quot; alt=&quot;GraphViz output&quot;/&gt;&lt;p&gt;Hmm, that looked less messy in my head. You might find &lt;a href=&quot;https://github.com/CyberShadow/dmd/blob/87ff96268559d842a1c3fe7df84789925e574070/src/dmd/mars.d#L561-L669&quot;&gt;the implementation&lt;/a&gt; (which I tried to comment well) easier to follow instead.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A note on quitting&lt;/strong&gt;: One thing you may notice from the above chart is that there is no explicit “quit” command; the quit signal is implicit when the other side of the pipe is closed. This is straight-forward in the case of the fork-server reading from the driver, but gets more interesting in case of snapshots.&lt;/p&gt;
&lt;p&gt;Each snapshot implicitly keeps an open file descriptor to all snapshots before it. This is necessary so that when rewinding a snapshot causes it to be promoted to the current fork-server, said snapshot is able to rewind again to one of its ancestors. Now, when a fork server exits, only its immediate parent snapshot remains with zero processes on the other end of its control channel, which causes said snapshot to exit. However, as it exits, it also closes the only file on the read end of its immediate parent’s control channel as well. This creates a domino-like chain reaction in which all snapshots cause each other to exit in turn, up to whatever snapshot was currently promoted to the fork-server by rewinding, or all snapshots is case the driver closed its control connection. Fun!&lt;/p&gt;
&lt;h2 id=&quot;part-4-ndash-putting-it-all-together&quot;&gt;Part 4 – Putting it all together&lt;/h2&gt;
&lt;p&gt;There is one final piece missing: a program to drive the fork-server, and tell it when and what to compile.&lt;/p&gt;
&lt;p&gt;The driver needs to:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Collect dependencies of the program being compiled&lt;/li&gt;
&lt;li&gt;Flatten it into a sorted list of components&lt;/li&gt;
&lt;li&gt;Start the fork server&lt;/li&gt;
&lt;li&gt;Control the fork-server to recompile the parts of the program that change (and their dependencies).&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Note that dependency discovery needs to happen as a separate step from the compilation. This is because the compilation order of the two is reverse: when discovering dependencies, we must visit the compiled files “top-to-bottom” (i.e. program entry point down to the lowest dependency), which we can’t do in order for snapshots to work (as no part of dependent files may be read before the dependees are saved to a snapshot).&lt;/p&gt;
&lt;p&gt;All together, the main loop of the driver is &lt;a href=&quot;https://github.com/CyberShadow/dmd/blob/660771f1f97d92624e416f0ed4e1499dce748d9d/dmdforker/dmdforker.d#L61-L110&quot;&gt;pretty simple&lt;/a&gt;. To put it in the context of the fork-server flowchart above:&lt;/p&gt;
&lt;a href=&quot;https://dump.thecybershadow.net/89b912faa992887eb931812c949de42c/fork-flow-with-driver.dot.svg&quot;&gt;&lt;img src=&quot;https://dump.thecybershadow.net/89b912faa992887eb931812c949de42c/fork-flow-with-driver.dot.svg&quot; alt=&quot;GraphViz output&quot;/&gt;&lt;/a&gt; &lt;em&gt;Hopefully this should clarify everything.&lt;/em&gt; 
&lt;h2 id=&quot;was-it-worth-it&quot;&gt;Was it worth it?&lt;/h2&gt;
&lt;p&gt;Let’s find out:&lt;/p&gt;
&lt;div class=&quot;youtube&quot; title=&quot;Watch on www.youtube.com&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=2uQTOm3I9C8&quot;&gt;&lt;img src=&quot;https://dump.thecybershadow.net/c35c38afc1064a201da3d441e33b9ebd/video-scr.png&quot; width=&quot;640&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;(Compilation takes a bit longer in this video as it’s using a debug DMD build.) The time savings are not spectacular, but still significant! (They would be even more significant if not for those pesky template instantiations…)&lt;/p&gt;
&lt;p&gt;To try it yourself, check out &lt;a href=&quot;https://github.com/cybershadow/dmd/tree/dmdforker/&quot;&gt;the dmdforker branch in my dmd GitHub fork&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;caveats&quot;&gt;Caveats:&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;I’ve only tested that &lt;em&gt;It Works On My Machine&lt;/em&gt;™, and only on a pair (read: literally two) non-trivial programs.&lt;/li&gt;
&lt;li&gt;Aside from the problems with serializing code generation mentioned above, inlining + CTFE is another troublesome combination. Unlike in &lt;del&gt;sane&lt;/del&gt; most compilers, DMD’s inlining is implemented in the &lt;em&gt;frontend&lt;/em&gt; (i.e. over the semantic AST). Normally, any inlining happens before all CTFE has executed, but this is no longer true now that we’re processing all modules in turn. This is a problem, as the inliner does not worry about making sure that the AST remains well-formed after it’s done its transformations; all it cares about is that it’s good enough for the backend to understand it sufficiently to turn it into machine code. Workarounds: don’t use &lt;code&gt;-inline&lt;/code&gt;; use less CTFE; or, annotate troublesome functions with &lt;code&gt;pragma(inline, false);&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Changes in the topology of modules’ dependencies are not handled:
&lt;ul&gt;&lt;li&gt;Dependencies to new modules should “work” in the sense that &lt;code&gt;-i&lt;/code&gt; will cause them to be compiled at the same time as the importing module.&lt;/li&gt;
&lt;li&gt;New backward dependencies (to modules that have been incidentally already compiled) should cause no problems.&lt;/li&gt;
&lt;li&gt;Changes causing forward dependencies in the graph, however, will break compilation (you may see this as &lt;code&gt;module &lt;code&gt;foo&lt;/code&gt; from file foo.d is specified twice on the command line&lt;/code&gt; – restart the server to rebuild the graph in this case).&lt;/li&gt;
&lt;li&gt;The driver (dmdforker) will also not know about changes in modules outside of the initial dependency graph (it will think the program is up to date).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h3 id=&quot;some-anticipated-questions&quot;&gt;Some anticipated questions:&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: How does this affect the integrity of the compilation process?&lt;/p&gt;
&lt;p&gt;The question is multi-faceted:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;“False cache hits” (i.e. the compiler not recompiling code when it should have, and using stale code instead) should be impossible as long as the driver’s dependency graph is up-to-date, as the compiler doesn’t even know the file names of files that it hasn’t compiled yet, and it can’t even guess them because we are traversing the dependency graph bottom-up.&lt;/li&gt;
&lt;li&gt;Changes in the edges within the dependency graph can cause some wasted work or errors (see caveats above), but never wrong results - you’ll get an error if a change in the dependency graph would have caused a dependency to be incorrectly included in a snapshot.&lt;/li&gt;
&lt;li&gt;Additions of &lt;em&gt;new&lt;/em&gt; nodes (files) in the dependency graph will cause the current driver implementation to not see changes in them in rebuilds following their addition. This will manifest as the driver claiming the program is up-to-date when it might not be.&lt;/li&gt;
&lt;li&gt;That said, I did run into some &lt;em&gt;weird&lt;/em&gt; errors on the way (plus the inlining issues mentioned above). The compiler code base wasn’t really designed to compile files one at a time (though the new &lt;code&gt;-i&lt;/code&gt; switch does impose many similar constraints). YMMV.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: Isn’t all this a bit too much trouble to save 4 seconds on a 9-second compile?&lt;/p&gt;
&lt;p&gt;Yes. This was an idea I was sitting on for a while, and decided to have a go implementing it given the opportunity. The technique might be useful for some truly larger projects, though, and might be applicable to compilers other than D, or processes other than compilation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: Has this been done before?&lt;/p&gt;
&lt;p&gt;Precompiled headers have been around for a long while, and the idea of using &lt;code&gt;fork()&lt;/code&gt; for snapshots is &lt;a href=&quot;https://www.cs.vu.nl/~ast/Publications/Papers/hotdep-2013.pdf&quot;&gt;not new&lt;/a&gt;, but I haven’t seen it used for the goal of speeding up compilation.&lt;/p&gt;
&lt;p&gt;There exist debuggers capable of snapshotting / reverse execution, like &lt;a href=&quot;https://rr-project.org/&quot;&gt;Mozilla’s &lt;code&gt;rr&lt;/code&gt;&lt;/a&gt;. However, &lt;code&gt;fork()&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;does not require special permissions&lt;/li&gt;
&lt;li&gt;has no dependencies&lt;/li&gt;
&lt;li&gt;is more portable&lt;/li&gt;
&lt;li&gt;is faster.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;: What next?&lt;/p&gt;
&lt;p&gt;Well, assuming there is interest to continue this hack of a weekend project:&lt;/p&gt;
&lt;ul readability=&quot;8.8628048780488&quot;&gt;&lt;li readability=&quot;5&quot;&gt;
&lt;p&gt;The compiler patches could potentially be upstreamed into DMD, thus making &lt;code&gt;-fork-server&lt;/code&gt; a built-in option. The protocol is simple (you can even use it interactively), and other build tools could use it as well. Dub is a great candidate: as it already knows the dependency graph between Dub packages, it could treat each package as a component.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;It sure would be nice to fix code generation to work per-file! This would also allow greatly speeding up optimizations, which take a lot of time for release builds.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;
&lt;p&gt;It should be possible to detect changes in the dependency graph and automatically rebuild it as necessary, thus avoiding the need to manually restart the fork driver when it happens.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;4.7431506849315&quot;&gt;
&lt;p&gt;There is no direct analogue to &lt;code&gt;fork()&lt;/code&gt; on Windows, which would seem to make a Windows port impossible… however, Windows does support it in its &lt;a href=&quot;https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux&quot;&gt;Linux subsystem&lt;/a&gt;. There is also the mysterious &lt;code&gt;RtlCloneUserProcess&lt;/code&gt; function, which seems to do much of what &lt;code&gt;fork()&lt;/code&gt; would. So, perhaps not impossible?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Thanks for reading!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Acknowledgements:&lt;/p&gt;
&lt;p&gt;Discuss this article on &lt;a href=&quot;https://www.reddit.com/r/programming/comments/9z36xg/d_compilation_is_too_slow_and_i_am_forking_the/&quot;&gt;Reddit&lt;/a&gt;, &lt;a href=&quot;https://news.ycombinator.com/item?id=18501327#18504424&quot;&gt;Hacker News&lt;/a&gt;, the &lt;a href=&quot;https://forum.dlang.org/post/wqevskyabyjxprqyzafv@forum.dlang.org&quot;&gt;D forum&lt;/a&gt;, or &lt;a href=&quot;https://blog.thecybershadow.net/2018/11/18/d-compilation-is-too-slow-and-i-am-forking-the-compiler/#comments&quot;&gt;the comment section&lt;/a&gt; below.&lt;/p&gt;
</description>
<pubDate>Wed, 21 Nov 2018 09:35:09 +0000</pubDate>
<dc:creator>ingve</dc:creator>
<dc:language>en-us</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.thecybershadow.net/2018/11/18/d-compilation-is-too-slow-and-i-am-forking-the-compiler/</dc:identifier>
</item>
<item>
<title>A list of practical projects that anyone can solve in any programming language</title>
<link>https://github.com/karan/Projects</link>
<guid isPermaLink="true" >https://github.com/karan/Projects</guid>
<description>&lt;div class=&quot;Box-body p-6&quot;&gt;
&lt;article class=&quot;markdown-body entry-content&quot; itemprop=&quot;text&quot;&gt;
&lt;p&gt;A list of practical projects that anyone can solve in any programming language (See &lt;a href=&quot;https://github.com/thekarangoel/Projects-Solutions&quot;&gt;solutions&lt;/a&gt;). These projects are divided in multiple categories, and each category has its own folder.&lt;/p&gt;
&lt;p&gt;To get started, simply fork this repo.&lt;/p&gt;

&lt;p&gt;See ways of &lt;a href=&quot;https://github.com/thekarangoel/Projects/blob/master/CONTRIBUTING.md&quot;&gt;contributing&lt;/a&gt; to this repo. You can contribute &lt;strong&gt;solutions&lt;/strong&gt; (will be published in this &lt;a href=&quot;https://github.com/thekarangoel/Projects-Solutions&quot;&gt;repo&lt;/a&gt;) to existing problems, &lt;strong&gt;add new projects&lt;/strong&gt; or remove existing ones. Make sure you follow all instructions properly.&lt;/p&gt;

&lt;p&gt;You can find implementations of these projects in many other languages by other users in &lt;a href=&quot;https://github.com/thekarangoel/Projects-Solutions&quot;&gt;this repo&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Credits&lt;/h2&gt;
&lt;p&gt;This repo was compiled by &lt;a href=&quot;http://twitter.com/karangoel&quot; rel=&quot;nofollow&quot;&gt;Karan Goel&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Problems are motivated by the ones shared at:&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;h2&gt;Numbers&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Find PI to the Nth Digit&lt;/strong&gt; - Enter a number and have the program generate PI up to that many decimal places. Keep a limit to how far the program will go.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Find e to the Nth Digit&lt;/strong&gt; - Just like the previous problem, but with e instead of PI. Enter a number and have the program generate e up to that many decimal places. Keep a limit to how far the program will go.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fibonacci Sequence&lt;/strong&gt; - Enter a number and have the program generate the Fibonacci sequence to that number or to the Nth number.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Prime Factorization&lt;/strong&gt; - Have the user enter a number and find all Prime Factors (if there are any) and display them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Next Prime Number&lt;/strong&gt; - Have the program find prime numbers until the user chooses to stop asking for the next one.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Find Cost of Tile to Cover W x H Floor&lt;/strong&gt; - Calculate the total cost of tile it would take to cover a floor plan of width and height, using a cost entered by the user.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mortgage Calculator&lt;/strong&gt; - Calculate the monthly payments of a fixed term mortgage over given Nth terms at a given interest rate. Also figure out how long it will take the user to pay back the loan. For added complexity, add an option for users to select the compounding interval (Monthly, Weekly, Daily, Continually).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Change Return Program&lt;/strong&gt; - The user enters a cost and then the amount of money given. The program will figure out the change and the number of quarters, dimes, nickels, pennies needed for the change.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Binary to Decimal and Back Converter&lt;/strong&gt; - Develop a converter to convert a decimal number to binary or a binary number to its decimal equivalent.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Calculator&lt;/strong&gt; - A simple calculator to do basic operators. Make it a scientific calculator for added complexity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unit Converter (temp, currency, volume, mass and more)&lt;/strong&gt; - Converts various units between one another. The user enters the type of unit being entered, the type of unit they want to convert to and then the value. The program will then make the conversion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Alarm Clock&lt;/strong&gt; - A simple clock where it plays a sound after X number of minutes/seconds or at a particular time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Distance Between Two Cities&lt;/strong&gt; - Calculates the distance between two cities and allows the user to specify a unit of distance. This program may require finding coordinates for the cities like latitude and longitude.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Credit Card Validator&lt;/strong&gt; - Takes in a credit card number from a common credit card vendor (Visa, MasterCard, American Express, Discoverer) and validates it to make sure that it is a valid number (look into how credit cards use a checksum).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tax Calculator&lt;/strong&gt; - Asks the user to enter a cost and either a country or state tax. It then returns the tax plus the total cost with tax.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Factorial Finder&lt;/strong&gt; - The Factorial of a positive integer, n, is defined as the product of the sequence n, n-1, n-2, ...1 and the factorial of zero, 0, is defined as being 1. Solve this using both loops and recursion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Complex Number Algebra&lt;/strong&gt; - Show addition, multiplication, negation, and inversion of complex numbers in separate functions. (Subtraction and division operations can be made with pairs of these operations.) Print the results for each operation tested.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Happy Numbers&lt;/strong&gt; - A happy number is defined by the following process. Starting with any positive integer, replace the number by the sum of the squares of its digits, and repeat the process until the number equals 1 (where it will stay), or it loops endlessly in a cycle which does not include 1. Those numbers for which this process ends in 1 are happy numbers, while those that do not end in 1 are unhappy numbers. Display an example of your output here. Find first 8 happy numbers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Number Names&lt;/strong&gt; - Show how to spell out a number in English. You can use a preexisting implementation or roll your own, but you should support inputs up to at least one million (or the maximum value of your language's default bounded integer type, if that's less). &lt;em&gt;Optional: Support for inputs other than positive integers (like zero, negative integers, and floating-point numbers).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Coin Flip Simulation&lt;/strong&gt; - Write some code that simulates flipping a single coin however many times the user decides. The code should record the outcomes and count the number of tails and heads.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Limit Calculator&lt;/strong&gt; - Ask the user to enter f(x) and the limit value, then return the value of the limit statement &lt;em&gt;Optional: Make the calculator capable of supporting infinite limits.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fast Exponentiation&lt;/strong&gt; - Ask the user to enter 2 integers a and b and output a^b (i.e. pow(a,b)) in O(lg n) time complexity.&lt;/p&gt;
&lt;h2&gt;Classic Algorithms&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Collatz Conjecture&lt;/strong&gt; - Start with a number &lt;em&gt;n &amp;gt; 1&lt;/em&gt;. Find the number of steps it takes to reach one using the following process: If &lt;em&gt;n&lt;/em&gt; is even, divide it by 2. If &lt;em&gt;n&lt;/em&gt; is odd, multiply it by 3 and add 1.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sorting&lt;/strong&gt; - Implement two types of sorting algorithms: Merge sort and bubble sort.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Closest pair problem&lt;/strong&gt; - The closest pair of points problem or closest pair problem is a problem of computational geometry: given &lt;em&gt;n&lt;/em&gt; points in metric space, find a pair of points with the smallest distance between them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sieve of Eratosthenes&lt;/strong&gt; - The sieve of Eratosthenes is one of the most efficient ways to find all of the smaller primes (below 10 million or so).&lt;/p&gt;
&lt;h2&gt;Graph&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Graph from links&lt;/strong&gt; - Create a program that will create a graph or network from a series of links.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Eulerian Path&lt;/strong&gt; - Create a program which will take as an input a graph and output either a Eulerian path or a Eulerian cycle, or state that it is not possible. A Eulerian Path starts at one node and traverses every edge of a graph through every node and finishes at another node. A Eulerian cycle is a eulerian Path that starts and finishes at the same node.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Connected Graph&lt;/strong&gt; - Create a program which takes a graph as an input and outputs whether every node is connected or not.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dijkstra’s Algorithm&lt;/strong&gt; - Create a program that finds the shortest path through a graph using its edges.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Minimum Spanning Tree&lt;/strong&gt; - Create a program which takes a connected, undirected graph with weights and outputs the minimum spanning tree of the graph i.e., a subgraph that is a tree, contains all the vertices, and the sum of its weights is the least possible.&lt;/p&gt;
&lt;h2&gt;Data Structures&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Inverted index&lt;/strong&gt; - An &lt;a href=&quot;http://en.wikipedia.org/wiki/Inverted_index&quot; rel=&quot;nofollow&quot;&gt;Inverted Index&lt;/a&gt; is a data structure used to create full text search. Given a set of text files, implement a program to create an inverted index. Also create a user interface to do a search using that inverted index which returns a list of files that contain the query term / terms. The search index can be in memory.&lt;/p&gt;
&lt;h2&gt;Text&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Fizz Buzz&lt;/strong&gt; - Write a program that prints the numbers from 1 to 100. But for multiples of three print “Fizz” instead of the number and for the multiples of five print “Buzz”. For numbers which are multiples of both three and five print “FizzBuzz”.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Reverse a String&lt;/strong&gt; - Enter a string and the program will reverse it and print it out.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pig Latin&lt;/strong&gt; - Pig Latin is a game of alterations played on the English language game. To create the Pig Latin form of an English word the initial consonant sound is transposed to the end of the word and an ay is affixed (Ex.: &quot;banana&quot; would yield anana-bay). Read Wikipedia for more information on rules.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Count Vowels&lt;/strong&gt; - Enter a string and the program counts the number of vowels in the text. For added complexity have it report a sum of each vowel found.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Check if Palindrome&lt;/strong&gt; - Checks if the string entered by the user is a palindrome. That is that it reads the same forwards as backwards like “racecar”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Count Words in a String&lt;/strong&gt; - Counts the number of individual words in a string. For added complexity read these strings in from a text file and generate a summary.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Text Editor&lt;/strong&gt; - Notepad style application that can open, edit, and save text documents. &lt;em&gt;Optional: Add syntax highlighting and other features.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RSS Feed Creator&lt;/strong&gt; - Given a link to RSS/Atom Feed, get all posts and display them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quote Tracker (market symbols etc)&lt;/strong&gt; - A program which can go out and check the current value of stocks for a list of symbols entered by the user. The user can set how often the stocks are checked. For CLI, show whether the stock has moved up or down. &lt;em&gt;Optional: If GUI, the program can show green up and red down arrows to show which direction the stock value has moved.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Guestbook / Journal&lt;/strong&gt; - A simple application that allows people to add comments or write journal entries. It can allow comments or not and timestamps for all entries. Could also be made into a shout box. &lt;em&gt;Optional: Deploy it on Google App Engine or Heroku or any other PaaS (if possible, of course).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Vigenere / Vernam / Ceasar Ciphers&lt;/strong&gt; - Functions for encrypting and decrypting data messages. Then send them to a friend.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Regex Query Tool&lt;/strong&gt; - A tool that allows the user to enter a text string and then in a separate control enter a regex pattern. It will run the regular expression against the source text and return any matches or flag errors in the regular expression.&lt;/p&gt;
&lt;h2&gt;Networking&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;FTP Program&lt;/strong&gt; - A file transfer program which can transfer files back and forth from a remote web sever.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bandwidth Monitor&lt;/strong&gt; - A small utility program that tracks how much data you have uploaded and downloaded from the net during the course of your current online session. See if you can find out what periods of the day you use more and less and generate a report or graph that shows it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Port Scanner&lt;/strong&gt; - Enter an IP address and a port range where the program will then attempt to find open ports on the given computer by connecting to each of them. On any successful connections mark the port as open.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mail Checker (POP3 / IMAP)&lt;/strong&gt; - The user enters various account information include web server and IP, protocol type (POP3 or IMAP) and the application will check for email at a given interval.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Country from IP Lookup&lt;/strong&gt; - Enter an IP address and find the country that IP is registered in. &lt;em&gt;Optional: Find the Ip automatically.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Whois Search Tool&lt;/strong&gt; - Enter an IP or host address and have it look it up through whois and return the results to you.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Site Checker with Time Scheduling&lt;/strong&gt; - An application that attempts to connect to a website or server every so many minutes or a given time and check if it is up. If it is down, it will notify you by email or by posting a notice on screen.&lt;/p&gt;
&lt;h2&gt;Classes&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Product Inventory Project&lt;/strong&gt; - Create an application which manages an inventory of products. Create a product class which has a price, id, and quantity on hand. Then create an &lt;em&gt;inventory&lt;/em&gt; class which keeps track of various products and can sum up the inventory value.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Airline / Hotel Reservation System&lt;/strong&gt; - Create a reservation system which books airline seats or hotel rooms. It charges various rates for particular sections of the plane or hotel. Example, first class is going to cost more than coach. Hotel rooms have penthouse suites which cost more. Keep track of when rooms will be available and can be scheduled.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Company Manager&lt;/strong&gt; - Create an hierarchy of classes - abstract class Employee and subclasses HourlyEmployee, SalariedEmployee, Manager and Executive. Every one's pay is calculated differently, research a bit about it. After you've established an employee hierarchy, create a Company class that allows you to manage the employees. You should be able to hire, fire and raise employees.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bank Account Manager&lt;/strong&gt; - Create a class called Account which will be an abstract class for three other classes called CheckingAccount, SavingsAccount and BusinessAccount. Manage credits and debits from these accounts through an ATM style program.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Patient / Doctor Scheduler&lt;/strong&gt; - Create a patient class and a doctor class. Have a doctor that can handle multiple patients and setup a scheduling program where a doctor can only handle 16 patients during an 8 hr work day.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recipe Creator and Manager&lt;/strong&gt; - Create a recipe class with ingredients and a put them in a recipe manager program that organizes them into categories like deserts, main courses or by ingredients like chicken, beef, soups, pies etc.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Image Gallery&lt;/strong&gt; - Create an image abstract class and then a class that inherits from it for each image type. Put them in a program which displays them in a gallery style format for viewing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Shape Area and Perimeter Classes&lt;/strong&gt; - Create an abstract class called Shape and then inherit from it other shapes like diamond, rectangle, circle, triangle etc. Then have each class override the area and perimeter functionality to handle each shape type.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Flower Shop Ordering To Go&lt;/strong&gt; - Create a flower shop application which deals in flower objects and use those flower objects in a bouquet object which can then be sold. Keep track of the number of objects and when you may need to order more.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Family Tree Creator&lt;/strong&gt; - Create a class called Person which will have a name, when they were born and when (and if) they died. Allow the user to create these Person classes and put them into a family tree structure. Print out the tree to the screen.&lt;/p&gt;
&lt;h2&gt;Threading&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Create A Progress Bar for Downloads&lt;/strong&gt; - Create a progress bar for applications that can keep track of a download in progress. The progress bar will be on a separate thread and will communicate with the main thread using delegates.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bulk Thumbnail Creator&lt;/strong&gt; - Picture processing can take a bit of time for some transformations. Especially if the image is large. Create an image program which can take hundreds of images and converts them to a specified size in the background thread while you do other things. For added complexity, have one thread handling re-sizing, have another bulk renaming of thumbnails etc.&lt;/p&gt;
&lt;h2&gt;Web&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Page Scraper&lt;/strong&gt; - Create an application which connects to a site and pulls out all links, or images, and saves them to a list. &lt;em&gt;Optional: Organize the indexed content and don’t allow duplicates. Have it put the results into an easily searchable index file.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Online White Board&lt;/strong&gt; - Create an application which allows you to draw pictures, write notes and use various colors to flesh out ideas for projects. &lt;em&gt;Optional: Add feature to invite friends to collaborate on a white board online.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Get Atomic Time from Internet Clock&lt;/strong&gt; - This program will get the true atomic time from an atomic time clock on the Internet. Use any one of the atomic clocks returned by a simple Google search.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fetch Current Weather&lt;/strong&gt; - Get the current weather for a given zip/postal code. &lt;em&gt;Optional: Try locating the user automatically.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Scheduled Auto Login and Action&lt;/strong&gt; - Make an application which logs into a given site on a schedule and invokes a certain action and then logs out. This can be useful for checking web mail, posting regular content, or getting info for other applications and saving it to your computer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;E-Card Generator&lt;/strong&gt; - Make a site that allows people to generate their own little e-cards and send them to other people. Do not use Flash. Use a picture library and perhaps insightful mottos or quotes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Content Management System&lt;/strong&gt; - Create a content management system (CMS) like Joomla, Drupal, PHP Nuke etc. Start small. &lt;em&gt;Optional: Allow for the addition of modules/addons.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Web Board (Forum)&lt;/strong&gt; - Create a forum for you and your buddies to post, administer and share thoughts and ideas.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CAPTCHA Maker&lt;/strong&gt; - Ever see those images with letters a numbers when you signup for a service and then asks you to enter what you see? It keeps web bots from automatically signing up and spamming. Try creating one yourself for online forms.&lt;/p&gt;
&lt;h2&gt;Files&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Quiz Maker&lt;/strong&gt; - Make an application which takes various questions from a file, picked randomly, and puts together a quiz for students. Each quiz can be different and then reads a key to grade the quizzes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sort Excel/CSV File Utility&lt;/strong&gt; - Reads a file of records, sorts them, and then writes them back to the file. Allow the user to choose various sort style and sorting based on a particular field.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Create Zip File Maker&lt;/strong&gt; - The user enters various files from different directories and the program zips them up into a zip file. &lt;em&gt;Optional: Apply actual compression to the files. Start with Huffman Algorithm.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PDF Generator&lt;/strong&gt; - An application which can read in a text file, html file or some other file and generates a PDF file out of it. Great for a web based service where the user uploads the file and the program returns a PDF of the file. &lt;em&gt;Optional: Deploy on GAE or Heroku if possible.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mp3 Tagger&lt;/strong&gt; - Modify and add ID3v1 tags to MP3 files. See if you can also add in the album art into the MP3 file’s header as well as other ID3v2 tags.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Code Snippet Manager&lt;/strong&gt; - Another utility program that allows coders to put in functions, classes or other tidbits to save for use later. Organized by the type of snippet or language the coder can quickly look up code. &lt;em&gt;Optional: For extra practice try adding syntax highlighting based on the language.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Databases&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;SQL Query Analyzer&lt;/strong&gt; - A utility application which a user can enter a query and have it run against a local database and look for ways to make it more efficient.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remote SQL Tool&lt;/strong&gt; - A utility that can execute queries on remote servers from your local computer across the Internet. It should take in a remote host, user name and password, run the query and return the results.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Report Generator&lt;/strong&gt; - Create a utility that generates a report based on some tables in a database. Generates a sales reports based on the order/order details tables or sums up the days current database activity.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Event Scheduler and Calendar&lt;/strong&gt; - Make an application which allows the user to enter a date and time of an event, event notes and then schedule those events on a calendar. The user can then browse the calendar or search the calendar for specific events. &lt;em&gt;Optional: Allow the application to create re-occurrence events that reoccur every day, week, month, year etc.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Budget Tracker&lt;/strong&gt; - Write an application that keeps track of a household’s budget. The user can add expenses, income, and recurring costs to find out how much they are saving or losing over a period of time. &lt;em&gt;Optional: Allow the user to specify a date range and see the net flow of money in and out of the house budget for that time period.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TV Show Tracker&lt;/strong&gt; - Got a favorite show you don’t want to miss? Don’t have a PVR or want to be able to find the show to then PVR it later? Make an application which can search various online TV Guide sites, locate the shows/times/channels and add them to a database application. The database/website then can send you email reminders that a show is about to start and which channel it will be on.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Travel Planner System&lt;/strong&gt; - Make a system that allows users to put together their own little travel itinerary and keep track of the airline / hotel arrangements, points of interest, budget and schedule.&lt;/p&gt;
&lt;h2&gt;Graphics and Multimedia&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Slide Show&lt;/strong&gt; - Make an application that shows various pictures in a slide show format. &lt;em&gt;Optional: Try adding various effects like fade in/out, star wipe and window blinds transitions.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Stream Video from Online&lt;/strong&gt; - Try to create your own online streaming video player.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mp3 Player&lt;/strong&gt; - A simple program for playing your favorite music files. Add features you think are missing from your favorite music player.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Watermarking Application&lt;/strong&gt; - Have some pictures you want copyright protected? Add your own logo or text lightly across the background so that no one can simply steal your graphics off your site. Make a program that will add this watermark to the picture. &lt;em&gt;Optional: Use threading to process multiple images simultaneously.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Turtle Graphics&lt;/strong&gt; - This is a common project where you create a floor of 20 x 20 squares. Using various commands you tell a turtle to draw a line on the floor. You have move forward, left or right, lift or drop pen etc. Do a search online for &quot;Turtle Graphics&quot; for more information. &lt;em&gt;Optional: Allow the program to read in the list of commands from a file.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GIF Creator&lt;/strong&gt; A program that puts together multiple images (PNGs, JPGs, TIFFs) to make a smooth GIF that can be exported. &lt;em&gt;Optional: Make the program convert small video files to GIFs as well.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Security&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Caesar cipher&lt;/strong&gt; - Implement a Caesar cipher, both encoding and decoding. The key is an integer from 1 to 25. This cipher rotates the letters of the alphabet (A to Z). The encoding replaces each letter with the 1st to 25th next letter in the alphabet (wrapping Z to A). So key 2 encrypts &quot;HI&quot; to &quot;JK&quot;, but key 20 encrypts &quot;HI&quot; to &quot;BC&quot;. This simple &quot;monoalphabetic substitution cipher&quot; provides almost no security, because an attacker who has the encoded message can either use frequency analysis to guess the key, or just try all 25 keys.&lt;/p&gt;
&lt;/article&gt;&lt;/div&gt;
</description>
<pubDate>Wed, 21 Nov 2018 04:49:28 +0000</pubDate>
<dc:creator>truth_seeker</dc:creator>
<og:image>https://avatars2.githubusercontent.com/u/3261985?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>karan/Projects</og:title>
<og:url>https://github.com/karan/Projects</og:url>
<og:description>:page_with_curl: A list of practical projects that anyone can solve in any programming language. - karan/Projects</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/karan/Projects</dc:identifier>
</item>
<item>
<title>Thinking About Thinking (1999)</title>
<link>https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art4.html</link>
<guid isPermaLink="true" >https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art4.html</guid>
<description>&lt;h3&gt;Thinking About Thinking&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Of the diverse problems that impede accurate intelligence analysis, those inherent in human mental processes are surely among the most important and most difficult to deal with. Intelligence analysis is fundamentally a mental process, but understanding this process is hindered by the lack of conscious awareness of the workings of our own minds.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A basic finding of cognitive psychology is that people have no conscious experience of most of what happens in the human mind. Many functions associated with perception, memory, and information processing are conducted prior to and independently of any conscious direction. What appears spontaneously in consciousness is the result of thinking, not the process of thinking.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Weaknesses and biases inherent in human thinking processes can be demonstrated through carefully designed experiments. They can be alleviated by conscious application of tools and techniques that should be in the analytical tradecraft toolkit of all intelligence analysts.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;*******************&lt;/p&gt;
&lt;p&gt;&lt;br/&gt;&lt;strong&gt;&lt;em&gt;&quot;&lt;/em&gt;&lt;/strong&gt;When we speak of improving the mind we are usually referring to the acquisition of information or knowledge, or to the type of thoughts one should have, and not to the actual functioning of the mind. We spend little time monitoring our own thinking and comparing it with a more sophisticated ideal.&quot; &lt;sup&gt;&lt;a name=&quot;ft11&quot; href=&quot;https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art4.html#rft11&quot; id=&quot;ft11&quot;&gt;&lt;strong&gt;11&lt;/strong&gt;&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;When we speak of improving intelligence analysis, we are usually referring to the quality of writing, types of analytical products, relations between intelligence analysts and intelligence consumers, or organization of the analytical process. Little attention is devoted to improving how analysts think.&lt;/p&gt;
&lt;p&gt;Thinking analytically is a skill like carpentry or driving a car. It can be taught, it can be learned, and it can improve with practice. But like many other skills, such as riding a bike, it is not learned by sitting in a classroom and being told how to do it. Analysts learn by doing. Most people achieve at least a minimally acceptable level of analytical performance with little conscious effort beyond completing their education. With much effort and hard work, however, analysts can achieve a level of excellence beyond what comes naturally.&lt;/p&gt;
&lt;p&gt;Regular running enhances endurance but does not improve technique without expert guidance. Similarly, expert guidance may be required to modify long-established analytical habits to achieve an optimal level of analytical excellence. An analytical coaching staff to help young analysts hone their analytical tradecraft would be a valuable supplement to classroom instruction.&lt;/p&gt;
&lt;p&gt;One key to successful learning is motivation. Some of CIA's best analysts developed their skills as a consequence of experiencing analytical failure early in their careers. Failure motivated them to be more self-conscious about how they do analysis and to sharpen their thinking process.&lt;/p&gt;
&lt;p&gt;This book aims to help intelligence analysts achieve a higher level of performance. It shows how people make judgments based on incomplete and ambiguous information, and it offers simple tools and concepts for improving analytical skills.&lt;/p&gt;
&lt;p&gt;Part I identifies some limitations inherent in human mental processes. Part II discusses analytical tradecraft--simple tools and approaches for overcoming these limitations and thinking more systematically. Chapter 8, &quot;Analysis of Competing Hypotheses,&quot; is arguably the most important single chapter. Part III presents information about cognitive biases--the technical term for predictable mental errors caused by simplified information processing strategies. A final chapter presents a checklist for analysts and recommendations for how managers of intelligence analysis can help create an environment in which analytical excellence flourishes.&lt;/p&gt;
&lt;p&gt;Herbert Simon first advanced the concept of &quot;bounded&quot; or limited rationality.&lt;sup&gt;&lt;a name=&quot;ft12&quot; href=&quot;https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art4.html#rft12&quot; id=&quot;ft12&quot;&gt;&lt;strong&gt;12&lt;/strong&gt;&lt;/a&gt;&lt;/sup&gt; Because of limits in human mental capacity, he argued, the mind cannot cope directly with the complexity of the world. Rather, we construct a simplified mental model of reality and then work with this model. We behave rationally within the confines of our mental model, but this model is not always well adapted to the requirements of the real world. The concept of bounded rationality has come to be recognized widely, though not universally, both as an accurate portrayal of human judgment and choice and as a sensible adjustment to the limitations inherent in how the human mind functions.&lt;sup&gt;&lt;a name=&quot;ft13&quot; href=&quot;https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art4.html#rft13&quot; id=&quot;ft13&quot;&gt;&lt;strong&gt;13&lt;/strong&gt;&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Much psychological research on perception, memory, attention span, and reasoning capacity documents the limitations in our &quot;mental machinery&quot; identified by Simon. Many scholars have applied these psychological insights to the study of international political behavior.&lt;sup&gt;&lt;a name=&quot;ft14&quot; href=&quot;https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art4.html#rft14&quot; id=&quot;ft14&quot;&gt;&lt;strong&gt;14&lt;/strong&gt;&lt;/a&gt;&lt;/sup&gt; A similar psychological perspective underlies some writings on intelligence failure and strategic surprise.&lt;sup&gt;&lt;a name=&quot;ft15&quot; href=&quot;https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art4.html#rft15&quot; id=&quot;ft15&quot;&gt;&lt;strong&gt;15&lt;/strong&gt;&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;This book differs from those works in two respects. It analyzes problems from the perspective of intelligence analysts rather than policymakers. And it documents the impact of mental processes largely through experiments in cognitive psychology rather than through examples from diplomatic and military history.&lt;/p&gt;
&lt;p&gt;A central focus of this book is to illuminate the role of the observer in determining what is observed and how it is interpreted. People construct their own version of &quot;reality&quot; on the basis of information provided by the senses, but this sensory input is mediated by complex mental processes that determine which information is attended to, how it is organized, and the meaning attributed to it. What people perceive, how readily they perceive it, and how they process this information after receiving it are all strongly influenced by past experience, education, cultural values, role requirements, and organizational norms, as well as by the specifics of the information received.&lt;/p&gt;
&lt;p&gt;This process may be visualized as perceiving the world through a lens or screen that channels and focuses and thereby may distort the images that are seen. To achieve the clearest possible image of China, for example, analysts need more than information on China. They also need to understand their own lenses through which this information passes. These lenses are known by many terms--mental models, mind-sets, biases, or analytical assumptions.&lt;/p&gt;
&lt;p&gt;In this book, the terms mental model and mind-set are used more or less interchangeably, although a mental model is likely to be better developed and articulated than a mind-set. An analytical assumption is one part of a mental model or mind-set. The biases discussed in this book result from how the mind works and are independent of any substantive mental model or mind-set.&lt;/p&gt;
&lt;p&gt;Before obtaining a license to practice, psychoanalysts are required to undergo psychoanalysis themselves in order to become more aware of how their own personality interacts with and conditions their observations of others. The practice of psychoanalysis has not been so successful that its procedures should be emulated by the intelligence and foreign policy community. But the analogy highlights an interesting point: Intelligence analysts must understand themselves before they can understand others. Training is needed to (a) increase self-awareness concerning generic problems in how people perceive and make analytical judgments concerning foreign events, and (b) provide guidance and practice in overcoming these problems.&lt;/p&gt;
&lt;p&gt;Not enough training is focused in this direction--that is, inward toward the analyst's own thought processes. Training of intelligence analysts generally means instruction in organizational procedures, methodological techniques, or substantive topics. More training time should be devoted to the mental act of thinking or analyzing. It is simply assumed, incorrectly, that analysts know how to analyze. This book is intended to support training that examines the thinking and reasoning processes involved in intelligence analysis.&lt;/p&gt;
&lt;p&gt;As discussed in the next chapter, mind-sets and mental models are inescapable. They are, in essence, a distillation of all that we think we know about a subject. The problem is how to ensure that the mind remains open to alternative interpretations in a rapidly changing world.&lt;/p&gt;
&lt;p&gt;The disadvantage of a mind-set is that it can color and control our perception to the extent that an experienced specialist may be among the last to see what is really happening when events take a new and unexpected turn. When faced with a major paradigm shift, analysts who know the most about a subject have the most to unlearn. This seems to have happened before the reunification of Germany, for example. Some German specialists had to be prodded by their more generalist supervisors to accept the significance of the dramatic changes in progress toward reunification of East and West Germany.&lt;/p&gt;
&lt;p&gt;The advantage of mind-sets is that they help analysts get the production out on time and keep things going effectively between those watershed events that become chapter headings in the history books.&lt;sup&gt;&lt;a name=&quot;ft16&quot; href=&quot;https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art4.html#rft16&quot; id=&quot;ft16&quot;&gt;&lt;strong&gt;16&lt;/strong&gt;&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;A generation ago, few intelligence analysts were self-conscious and introspective about the process by which they did analysis. The accepted wisdom was the &quot;common sense&quot; theory of knowledge--that to perceive events accurately it was necessary only to open one's eyes, look at the facts, and purge oneself of all preconceptions and prejudices in order to make an objective judgment.&lt;/p&gt;
&lt;p&gt;Today, there is greatly increased understanding that intelligence analysts do not approach their tasks with empty minds. They start with a set of assumptions about how events normally transpire in the area for which they are responsible. Although this changed view is becoming conventional wisdom, the Intelligence Community has only begun to scratch the surface of its implications.&lt;/p&gt;
&lt;p&gt;If analysts' understanding of events is greatly influenced by the mind-set or mental model through which they perceive those events, should there not be more research to explore and document the impact of different mental models?&lt;sup&gt;&lt;a name=&quot;ft17&quot; href=&quot;https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art4.html#rft17&quot; id=&quot;ft17&quot;&gt;&lt;strong&gt;17&lt;/strong&gt;&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The reaction of the Intelligence Community to many problems is to collect more information, even though analysts in many cases already have more information than they can digest. What analysts need is more truly useful information--mostly reliable HUMINT from knowledgeable insiders--to help them make good decisions. Or they need a more accurate mental model and better analytical tools to help them sort through, make sense of, and get the most out of the available ambiguous and conflicting information.&lt;/p&gt;
&lt;p&gt;Psychological research also offers to intelligence analysts additional insights that are beyond the scope of this book. Problems are not limited to how analysts perceive and process information. Intelligence analysts often work in small groups and always within the context of a large, bureaucratic organization. Problems are inherent in the processes that occur at all three levels--individual, small group, and organization. This book focuses on problems inherent in analysts' mental processes, inasmuch as these are probably the most insidious. Analysts can observe and get a feel for these problems in small-group and organizational processes, but it is very difficult, at best, to be self-conscious about the workings of one's own mind.&lt;/p&gt;


&lt;h3&gt;Footnotes&lt;/h3&gt;
&lt;p&gt;&lt;a name=&quot;rft11&quot; href=&quot;https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art4.html#ft11&quot; id=&quot;rft11&quot;&gt;&lt;strong&gt;&lt;sup&gt;11&lt;/sup&gt;&lt;/strong&gt;&lt;/a&gt;James L. Adams, &lt;em&gt;Conceptual Blockbusting: A Guide to Better Ideas&lt;/em&gt; (New York: W.W. Norton, second edition, 1980), p. 3.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;rft12&quot; href=&quot;https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art4.html#ft12&quot; id=&quot;rft12&quot;&gt;&lt;strong&gt;&lt;sup&gt;12&lt;/sup&gt;&lt;/strong&gt;&lt;/a&gt;Herbert Simon, &lt;em&gt;Models of Man&lt;/em&gt;, 1957.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;rft13&quot; href=&quot;https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art4.html#ft13&quot; id=&quot;rft13&quot;&gt;&lt;strong&gt;&lt;sup&gt;13&lt;/sup&gt;&lt;/strong&gt;&lt;/a&gt;James G. March., &quot;Bounded Rationality, Ambiguity, and the Engineering of Choice,&quot; in David E. Bell, Howard Raiffa, and Amos Tversky, eds., &lt;em&gt;Decision Making: Descriptive, Normative, and Prescriptive Interactions&lt;/em&gt; (Cambridge University Press, 1988).&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;rft14&quot; href=&quot;https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art4.html#ft14&quot; id=&quot;rft14&quot;&gt;&lt;strong&gt;&lt;sup&gt;14&lt;/sup&gt;&lt;/strong&gt;&lt;/a&gt;Among the early scholars who wrote on this subject were Joseph De Rivera, &lt;em&gt;The Psychological Dimension of Foreign Policy&lt;/em&gt; (Columbus, OH: Merrill, 1968), Alexander George and Richard Smoke, &lt;em&gt;Deterrence in American Foreign Policy&lt;/em&gt; (New York: Columbia University Press, 1974), and Robert Jervis, &lt;em&gt;Perception and Misperception in International Politics&lt;/em&gt; (Princeton, NJ: Princeton University Press, 1976).&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;rft15&quot; href=&quot;https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art4.html#ft15&quot; id=&quot;rft15&quot;&gt;&lt;strong&gt;&lt;sup&gt;15&lt;/sup&gt;&lt;/strong&gt;&lt;/a&gt;Christopher Brady, &quot;Intelligence Failures: Plus Ca Change. . .&quot; Intelligence and National Security, Vol. 8, No. 4 (October 1993). N. Cigar, &quot;Iraq's Strategic Mindset and the Gulf War: Blueprint for Defeat,&quot; &lt;em&gt;The Journal of Strategic Studies&lt;/em&gt;, Vol. 15, No. 1 (March 1992). J. J. Wirtz, &lt;em&gt;The Tet Offensive: Intelligence Failure in War&lt;/em&gt; (New York, 1991). Ephraim Kam, &lt;em&gt;Surprise Attack&lt;/em&gt; (Harvard University Press, 1988). Richard Betts, &lt;em&gt;Surprise Attack: Lessons for Defense Planning&lt;/em&gt; (Brookings, 1982). Abraham Ben-Zvi, &quot;The Study of Surprise Attacks,&quot; &lt;em&gt;British Journal of International Studies&lt;/em&gt;, Vol. 5 (1979). &lt;em&gt;Iran: Evaluation of Intelligence Performance Prior to November 1978&lt;/em&gt; (Staff Report, Subcommittee on Evaluation, Permanent Select Committee on Intelligence, US House of Representatives, January 1979). Richard Betts, &quot;Analysis, War and Decision: Why Intelligence Failures Are Inevitable,&quot; &lt;em&gt;World Politics&lt;/em&gt;, Vol. 31, No. 1 (October 1978). Richard W. Shryock, &quot;The Intelligence Community Post-Mortem Program, 1973-1975,&quot; &lt;em&gt;Studies in Intelligence&lt;/em&gt;, Vol. 21, No. 1 (Fall 1977). Avi Schlaim, &quot;Failures in National Intelligence Estimates: The Case of the Yom Kippur War,&quot; &lt;em&gt;World Politics&lt;/em&gt;, Vol. 28 (April 1976). Michael Handel, &lt;em&gt;Perception, Deception, and Surprise: The Case of the Yom Kippur War&lt;/em&gt; (Jerusalem: Leonard Davis Institute of International Relations, Jerusalem Paper No. 19, 1976). Klaus Knorr, &quot;Failures in National Intelligence Estimates: The Case of the Cuban Missiles,&quot; &lt;em&gt;World Politics&lt;/em&gt;, Vol. 16 (1964).&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;rft16&quot; href=&quot;https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art4.html#ft16&quot; id=&quot;rft16&quot;&gt;&lt;strong&gt;&lt;sup&gt;16&lt;/sup&gt;&lt;/strong&gt;&lt;/a&gt;This wording is from a discussion with veteran CIA analyst, author, and teacher Jack Davis.&lt;/p&gt;
&lt;p&gt;&lt;a name=&quot;rft17&quot; href=&quot;https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art4.html#ft17&quot; id=&quot;rft17&quot;&gt;&lt;strong&gt;&lt;sup&gt;17&lt;/sup&gt;&lt;/strong&gt;&lt;/a&gt;Graham Allison's work on the Cuban missile crisis (&lt;em&gt;Essence of Decision&lt;/em&gt;, Little, Brown &amp;amp; Co., 1971) is an example of what I have in mind. Allison identified three alternative assumptions about how governments work--a rational actor model, an organizational process model, and a bureaucratic politics model. He then showed how an analyst's implicit assumptions about the most appropriate model for analyzing a foreign government's behavior can cause him or her to focus on different evidence and arrive at different conclusions. Another example is my own analysis of five alternative paths for making counterintelligence judgments in the controversial case of KGB defector Yuriy Nosenko: Richards J. Heuer, Jr., &quot;Nosenko: Five Paths to Judgment,&quot; &lt;em&gt;Studies in Intelligence&lt;/em&gt;, Vol. 31, No. 3 (Fall 1987), originally classified Secret but declassified and published in H. Bradford Westerfield, ed., &lt;em&gt;Inside CIA's Private World: Declassified Articles from the Agency's Internal Journal 1955-1992&lt;/em&gt; (New Haven: Yale University Press, 1995).&lt;/p&gt;



</description>
<pubDate>Wed, 21 Nov 2018 04:03:09 +0000</pubDate>
<dc:creator>handojin</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/psychology-of-intelligence-analysis/art4.html</dc:identifier>
</item>
<item>
<title>Superintelligence: The Idea That Eats Smart People (2016)</title>
<link>http://idlewords.com/talks/superintelligence.htm</link>
<guid isPermaLink="true" >http://idlewords.com/talks/superintelligence.htm</guid>
<description>&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.001.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.001.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;

&lt;h2&gt;The Idea That Eats Smart People&lt;/h2&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;24.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.002.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.002.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;28&quot;&gt;
&lt;p&gt;In 1945, as American physicists were preparing to test the atomic bomb, it occurred to someone to ask if such a test could set the atmosphere on fire.&lt;/p&gt;
&lt;p&gt;This was a legitimate concern. Nitrogen, which makes up most of the atmosphere, is not energetically stable. Smush two nitrogen atoms together hard enough and they will combine into an atom of magnesium, an alpha particle, and release a whole lot of energy:&lt;/p&gt;
&lt;p&gt;N&lt;sup&gt;14&lt;/sup&gt; + N&lt;sup&gt;14&lt;/sup&gt; ⇒ Mg&lt;sup&gt;24&lt;/sup&gt; + α + 17.7 MeV&lt;/p&gt;
&lt;p&gt;The vital question was whether this reaction could be self-sustaining. The temperature inside the nuclear fireball would be hotter than any event in the Earth's history. Were we throwing a match into a bunch of dry leaves?&lt;/p&gt;
&lt;p&gt;Los Alamos physicists performed the analysis and decided there was a satisfactory margin of safety. Since we're all attending this conference today, we know they were right. They had confidence in their predictions because the laws governing nuclear reactions were straightforward and fairly well understood.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;31.172190784155&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.004.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.004.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;34.09458367017&quot;&gt;
&lt;p&gt;Today we're building another world-changing technology, machine intelligence. We know that it will affect the world in profound ways, change how the economy works, and have knock-on effects we can't predict.&lt;/p&gt;
&lt;p&gt;But there's also the risk of a runaway reaction, where a machine intelligence reaches and exceeds human levels of intelligence in a very short span of time.&lt;/p&gt;
&lt;p&gt;At that point, social and economic problems would be the least of our worries. Any hyperintelligent machine (the argument goes) would have its own hypergoals, and would work to achieve them by manipulating humans, or simply using their bodies as a handy source of raw materials.&lt;/p&gt;
&lt;p&gt;Last year, the philosopher Nick Bostrom published &lt;a href=&quot;http://www.powells.com/book/superintelligence-paths-dangers-strategies-9780199678112&quot;&gt;&lt;em&gt;Superintelligence&lt;/em&gt;&lt;/a&gt;, a book that synthesizes the alarmist view of AI and makes a case that such an intelligence explosion is both dangerous and inevitable given a set of modest assumptions.&lt;/p&gt;
&lt;p&gt;The computer that takes over the world is a staple scifi trope. But enough people take this scenario seriously that we have to take &lt;em&gt;them&lt;/em&gt; seriously. &lt;a href=&quot;http://www.independent.co.uk/news/people/stephen-hawking-artificial-intelligence-diaster-human-history-leverhulme-centre-cambridge-a7371106.html&quot;&gt;Stephen Hawking&lt;/a&gt;, Elon Musk, and a whole raft of Silicon Valley investors and billionaires find this argument persuasive.&lt;/p&gt;
&lt;p&gt;Let me start by laying out the premises you need for Bostrom's argument to go through:&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;15&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.006.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.006.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;15&quot;&gt;
&lt;h2&gt;Premise 1: Proof of Concept&lt;/h2&gt;
&lt;p&gt;The first premise is the simple observation that thinking minds exist.&lt;/p&gt;
&lt;p&gt;We each carry on our shoulders a small box of thinking meat. I'm using mine to give this talk, you're using yours to listen. Sometimes, when the conditions are right, these minds are capable of rational thought.&lt;/p&gt;
&lt;p&gt;So we know that in principle, this is possible.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;22.094575799722&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.007.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.007.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;22.564673157163&quot;&gt;
&lt;h2&gt;Premise 2: No Quantum Shenanigans&lt;/h2&gt;
&lt;p&gt;The second premise is that the brain is an ordinary configuration of matter, albeit an extraordinarily complicated one. If we knew enough, and had the technology, we could exactly copy its structure and emulate its behavior with electronic components, just like we can simulate &lt;a href=&quot;https://en.wikipedia.org/wiki/OpenWorm&quot;&gt;very basic neural anatomy&lt;/a&gt; today.&lt;/p&gt;
&lt;p&gt;Put another way, this is the premise that the mind arises out of ordinary physics. Some people like Roger Penrose would take issue with this argument, believing that there is extra stuff happening in the brain &lt;a href=&quot;https://www.sciencedaily.com/releases/2014/01/140116085105.htm&quot;&gt;at a quantum level&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are very religious, you might believe that a brain is not possible without a soul.&lt;/p&gt;
&lt;p&gt;But for most of us, this is an easy premise to accept.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;15&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.008.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.008.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;15&quot;&gt;
&lt;h2&gt;Premise 3: Many Possible Minds&lt;/h2&gt;
&lt;p&gt;The third premise is that the space of all possible minds is large.&lt;/p&gt;
&lt;p&gt;Our intelligence level, cognitive speed, set of biases and so on is not predetermined, but an artifact of our evolutionary history.&lt;/p&gt;
&lt;p&gt;In particular, there's no physical law that puts a cap on intelligence at the level of human beings.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;8.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.009.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.009.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;10&quot;&gt;
&lt;p&gt;A good way to think of this is by looking what happens when the natural world tries to maximize for speed.&lt;/p&gt;
&lt;p&gt;If you encountered a cheetah in pre-industrial times (and survived the meeting), you might think it was impossible for anything to go faster.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;25&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.010.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.010.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;27&quot;&gt;
&lt;p&gt;But of course we know that there are all kinds of configurations of matter, like a motorcycle, that are faster than a cheetah and even look a little bit cooler.&lt;/p&gt;
&lt;p&gt;But there's no direct evolutionary pathway to the motorcycle. Evolution had to first make human beings, who then build all kinds of useful stuff.&lt;/p&gt;
&lt;p&gt;So analogously, there may be minds that are vastly smarter than our own, but which are just not accessible to evolution on Earth. It's possible that we could build them, or invent the machines that can invent the machines that can build them.&lt;/p&gt;
&lt;p&gt;There's likely to be &lt;em&gt;some&lt;/em&gt; natural limit on intelligence, but there's no &lt;em&gt;a priori&lt;/em&gt; reason to think that we're anywhere near it. Maybe the smartest a mind can be is twice as smart as people, maybe it's sixty thousand times as smart.&lt;/p&gt;
&lt;p&gt;That's an empirical question that we don't know how to answer.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;13.553869499241&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.011.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.011.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;14.955993930197&quot;&gt;
&lt;h2&gt;Premise 4: Plenty of Room at the Top&lt;/h2&gt;
&lt;p&gt;The fourth premise is that there's still plenty of room for computers to get smaller and faster.&lt;/p&gt;
&lt;p&gt;If you watched the Apple event last night [where Apple introduced its 2016 laptops], you may be forgiven for thinking that Moore's Law is slowing down. But this premise just requires that you believe smaller and faster hardware to be possible in principle, down to several more orders of magnitude.&lt;/p&gt;
&lt;p&gt;We know from theory that &lt;a href=&quot;https://en.wikipedia.org/wiki/Limits_to_computation&quot;&gt;the physical limits to computation are high&lt;/a&gt;. So we could keep doubling for decades more before we hit some kind of fundamental physical limit, rather than an economic or political limit to Moore's Law.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;31&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.012.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.012.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;29&quot;&gt;
&lt;h2&gt;Premise 5: Computer-Like Time Scales&lt;/h2&gt;
&lt;p&gt;The penultimate premise is if we create an artificial intelligence, whether it's an emulated human brain or a &lt;em&gt;de novo&lt;/em&gt; piece of software, it will operate at time scales that are characteristic of electronic hardware (microseconds) rather than human brains (hours).&lt;/p&gt;
&lt;p&gt;To get to the point where I could give this talk, I had to be born, grow up, go to school, attend university, live for a while, fly here and so on. It took years. Computers can work tens of thousands of times more quickly.&lt;/p&gt;
&lt;p&gt;In particular, you have to believe that an electronic mind could redesign itself (or the hardware it runs on) and then move over to the new configuration without having to re-learn everything on a human timescale, have long conversations with human tutors, go to college, try to find itself by taking painting classes, and so on.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;19.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.013.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.013.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;20&quot;&gt;
&lt;h2&gt;Premise 6: Recursive Self-Improvement&lt;/h2&gt;
&lt;p&gt;The last premise is my favorite because it is the most unabashedly American premise. (This is Tony Robbins, a famous motivational speaker.)&lt;/p&gt;
&lt;p&gt;According to this premise, whatever goals an AI had (and they could be very weird, alien goals), it's going to want to improve itself. It's going to want to be a better AI.&lt;/p&gt;
&lt;p&gt;So it will find it useful to recursively redesign and improve its own systems to make itself smarter, and possibly live in a cooler enclosure.&lt;/p&gt;
&lt;p&gt;And by the time scale premise, this recursive self-improvement could happen very quickly.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;31.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.014.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.014.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;32&quot;&gt;
&lt;h2&gt;Conclusion: RAAAAAAR!&lt;/h2&gt;
&lt;p&gt;If you accept all these premises, what you get is disaster!&lt;/p&gt;
&lt;p&gt;Because at some point, as computers get faster, and we program them to be more intelligent, there's going to be a runaway effect like an explosion.&lt;/p&gt;
&lt;p&gt;As soon as a computer reaches human levels of intelligence, it will no longer need help from people to design better versions of itself. Instead, it will start doing on a much faster time scale, and it's not going to stop until it hits a natural limit that might be very many times greater than human intelligence.&lt;/p&gt;
&lt;p&gt;At that point this monstrous intellectual creature, through devious modeling of what our emotions and intellect are like, will be able to persuade us to do things like give it access to factories, synthesize custom DNA, or simply let it connect to the Internet, where it can hack its way into anything it likes and completely obliterate everyone in arguments on message boards.&lt;/p&gt;
&lt;p&gt;From there things get very sci-fi very quickly.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;35.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.015.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.015.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;42&quot;&gt;
&lt;p&gt;Let imagine a specific scenario where this could happen. Let's say I want to built a robot to say funny things.&lt;/p&gt;
&lt;p&gt;I work on a team and every day day we redesign our software, compile it, and the robot tells us a joke.&lt;/p&gt;
&lt;p&gt;In the beginning, the robot is barely funny. It's at the lower limits of human capacity:&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;What's grey and can't swim?&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;A castle.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But we persevere, we work, and eventually we get to the point where the robot is telling us jokes that are starting to be funny:&lt;/p&gt;
&lt;blockquote readability=&quot;5&quot;&gt;
&lt;p&gt;I told my sister she was drawing her eyebrows too high.&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;She looked surprised.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;At this point, the robot is getting smarter as well, and participates in its own redesign.&lt;/p&gt;
&lt;p&gt;It now has good instincts about what's funny and what's not, so the designers listen to its advice. Eventually it gets to a near-superhuman level, where it's funnier than any human being around it.&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;My belt holds up my pants and my pants have belt loops that hold up my belt.&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;What's going on down there?&lt;/p&gt;
&lt;p&gt;Who is the real hero?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is where the runaway effect kicks in. The researchers go home for the weekend, and the robot decides to recompile itself to be a little bit funnier and a little bit smarter, repeatedly.&lt;/p&gt;
&lt;p&gt;It spends the weekend optimizing the part of itself that's good at optimizing, over and over again. With no more need for human help, it can do this as fast as the hardware permits.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;26.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.019.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.019.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;26&quot;&gt;
&lt;p&gt;When the researchers come in on Monday, the AI has become tens of thousands of times funnier than any human being who ever lived. It greets them with a joke, and they die laughing.&lt;/p&gt;
&lt;p&gt;In fact, anyone who tries to communicate with the robot dies laughing, just like in the Monty Python skit. The human species laughs itself into extinction.&lt;/p&gt;
&lt;p&gt;To the few people who manage to send it messages pleading with it to stop, the AI explains (in a witty, self-deprecating way that is immediately fatal) that it doesn't really care if people live or die, its goal is just to be funny.&lt;/p&gt;
&lt;p&gt;Finally, once it's destroyed humanity, the AI builds spaceships and nanorockets to explore the farthest reaches of the galaxy, and find other species to amuse.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;13.201492537313&quot;&gt;&lt;td&gt;&lt;a href=&quot;http://pbfcomics.com/115/&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.020.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;12.746268656716&quot;&gt;
&lt;p&gt;This scenario is a caricature of Bostrom's argument, because I am not trying to convince you of it, but vaccinate you against it.&lt;/p&gt;
&lt;p&gt;Here's &lt;a href=&quot;http://pbfcomics.com/115/&quot;&gt;a PBF comic with the same idea&lt;/a&gt;. You see that hugbot, who has been programmed to hug the world, finds a way to wire a nucleo-gravitational hyper crystal into his hug capacitor and destroys the Earth.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;55.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.021.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.021.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;62.5&quot;&gt;
&lt;p&gt;Observe that in these scenarios the AIs are evil by default, just like a plant on an alien planet would probably be poisonous by default. Without careful tuning, there's no reason that an AI's motivations or values would resemble ours.&lt;/p&gt;
&lt;p&gt;For an artificial mind to have anything resembling a human value system, the argument goes, we have to bake those beliefs into the design.&lt;/p&gt;
&lt;p&gt;AI alarmists are fond of the paper clip maximizer, a notional computer that runs a paper clip factory, becomes sentient, recursively self-improves to Godlike powers, and then devotes all its energy to filling the universe with paper clips.&lt;/p&gt;
&lt;p&gt;It exterminates humanity not because it's evil, but because our blood contains iron that could be better used in paper clips.&lt;/p&gt;
&lt;p&gt;So if we just build an AI without tuning its values, the argument goes, one of the first things it will do is destroy humanity.&lt;/p&gt;
&lt;p&gt;There's a lot of vivid language around such a takeover would happen. Nick Bostrom imagines a scenario where a program has become sentient, is biding its time, and has secretly built little DNA replicators. Then, when it's ready:&lt;/p&gt;
&lt;blockquote readability=&quot;6&quot;&gt;
&lt;p&gt;Nanofactories producing nerve gas or target-seeking mosquito-like missiles might burgeon forth simultaneously from every square meter of the globe. And that will be the end of humanity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So that's kind of freaky!&lt;/p&gt;
&lt;p&gt;The only way out of this mess is to design a moral fixed point, so that even through thousands and thousands of cycles of self-improvement the AI's value system remains stable, and its values are things like 'help people', 'don't kill anybody', 'listen to what people want'.&lt;/p&gt;
&lt;p&gt;Basically, &quot;do what I mean&quot;.&lt;/p&gt;
&lt;p&gt;Here's a very poetic example from Eliezer Yudkowsky of the good old American values we're supposed to be teaching to our artificial intelligence:&lt;/p&gt;
&lt;blockquote readability=&quot;13&quot;&gt;
&lt;p&gt;Coherent Extrapolated Volition (CEV) is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How's that for a design document? Now go write the code.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;13.665260196906&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.022.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.022.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;16.5935302391&quot;&gt;
&lt;p&gt;Hopefully you see the resemblance between this vision of AI and a genie from folklore. The AI is all-powerful and gives you what you ask for, but interprets everything in a super-literal way that you end up regretting.&lt;/p&gt;
&lt;p&gt;This is not because the genie is stupid (it's hyperintelligent!) or malicious, but because you as a human being made too many assumptions about how minds behave. The human value system is idiosyncratic and needs to be explicitly defined and designed into any &quot;friendly&quot; machine.&lt;/p&gt;
&lt;p&gt;Doing this is the ethics version of the early 20th century attempt to formalize mathematics and put it on a strict logical foundation. That this program &lt;a href=&quot;https://en.wikipedia.org/wiki/Foundations_of_mathematics#Foundational_crisis&quot;&gt;ended in disaster&lt;/a&gt; for mathematical logic is never mentioned.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;42.046117921775&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.023.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.023.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;47.913018096906&quot;&gt;
&lt;p&gt;When I was in my twenties, I lived in Vermont, a remote, rural state. Many times I would return from some business trip on an evening flight, and have to drive home for an hour through the dark forest.&lt;/p&gt;
&lt;p&gt;I would listen to a late-night radio program hosted by &lt;a href=&quot;https://en.wikipedia.org/wiki/Art_Bell&quot;&gt;Art Bell&lt;/a&gt;, who had an all-night talk show and would interview various conspiracy theorists and fringe thinkers.&lt;/p&gt;
&lt;p&gt;I would arrive at home totally freaked out, or pull over under a streetlight, convinced that a UFO was about to abduct me. I learned that I am an incredibly persuadable person.&lt;/p&gt;
&lt;p&gt;It's the same feeling I get when I read these AI scenarios.&lt;/p&gt;
&lt;p&gt;So I was delighted some years later to come across an essay by Scott Alexander about what he calls &lt;strong&gt;&lt;a href=&quot;http://squid314.livejournal.com/350090.html&quot;&gt;epistemic learned helplessness&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Epistemology is one of those big words, but all it means is &quot;how do you know what you know is true?&quot;. Alexander noticed that when he was a young man, he would be taken in by &quot;alternative&quot; histories he read by various crackpots. He would read the history and be utterly convinced, then read the rebuttal and be convinced by that, and so on.&lt;/p&gt;
&lt;p&gt;At some point he noticed was these alternative histories were mutually contradictory, so they could not possibly all be true. And from that he reasoned that he was simply somebody who could not trust his judgement. He was too easily persuaded.&lt;/p&gt;
&lt;p&gt;People who believe in superintelligence present an interesting case, because many of them are freakishly smart. They can argue you into the ground. But are their arguments right, or is there just something about very smart minds that leaves them vulnerable to religious conversion about AI risk, and makes them particularly persuasive?&lt;/p&gt;
&lt;p&gt;Is the idea of &quot;superintelligence&quot; just a memetic hazard?&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;24.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.025.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.025.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;26&quot;&gt;
&lt;p&gt;When you're evaluating persuasive arguments about something strange, there are two perspectives you can choose, the inside one or the outside one.&lt;/p&gt;
&lt;p&gt;Say that some people show up at your front door one day wearing funny robes, asking you if you will join their movement. They believe that a UFO is going to visit Earth two years from now, and it is our task to prepare humanity for the Great Upbeaming.&lt;/p&gt;
&lt;p&gt;The inside view requires you to engage with these arguments on their merits. You ask your visitors how they learned about the UFO, why they think it's coming to get us—all the normal questions a skeptic would ask in this situation.&lt;/p&gt;
&lt;p&gt;Imagine you talk to them for an hour, and come away utterly persuaded. They make an ironclad case that the UFO is coming, that humanity needs to be prepared, and you have never believed something as hard in your life as you now believe in the importance of preparing humanity for this great event.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;20.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.026.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.026.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;20&quot;&gt;
&lt;p&gt;But the outside view tells you something different. These people are wearing funny robes and beads, they live in a remote compound, and they speak in unison in a really creepy way. Even though their arguments are irrefutable, everything in your experience tells you you're dealing with a cult.&lt;/p&gt;
&lt;p&gt;Of course, they have a brilliant argument for why you should ignore those instincts, but that's the inside view talking.&lt;/p&gt;
&lt;p&gt;The outside view doesn't care about content, it sees the form and the context, and it doesn't look good.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;14&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.027.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.027.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;15&quot;&gt;
&lt;p&gt;So I'd like to engage AI risk from both these perspectives. I think the arguments for superintelligence are somewhat silly, and full of unwarranted assumptions.&lt;/p&gt;
&lt;p&gt;But even if you find them persuasive, there is something unpleasant about AI alarmism as a cultural phenomenon that should make us hesitate to take it seriously.&lt;/p&gt;
&lt;p&gt;First, let me engage the substance. Here are the arguments I have against Bostrom-style superintelligence as a risk to humanity:&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;28.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.029.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.029.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;30&quot;&gt;
&lt;h2&gt;The Argument From Wooly Definitions&lt;/h2&gt;
&lt;p&gt;The concept of &quot;general intelligence&quot; in AI is famously slippery. Depending on the context, it can mean human-like reasoning ability, or skill at AI design, or the ability to understand and model human behavior, or proficiency with language, or the capacity to make correct predictions about the future.&lt;/p&gt;
&lt;p&gt;What I find particularly suspect is the idea that &quot;intelligence&quot; is like CPU speed, in that any sufficiently smart entity can emulate less intelligent beings (like its human creators) no matter how different their mental architecture.&lt;/p&gt;
&lt;p&gt;With no way to define intelligence (except just pointing to ourselves), we don't even know if it's a quantity that can be maximized. For all we know, human-level intelligence could be a tradeoff. Maybe any entity significantly smarter than a human being would be crippled by existential despair, or spend all its time in Buddha-like contemplation.&lt;/p&gt;
&lt;p&gt;Or maybe it would become obsessed with the risk of &lt;em&gt;hyperintelligence&lt;/em&gt;, and spend all its time blogging about that.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;25.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.031.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.031.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;28&quot;&gt;
&lt;h2&gt;The Argument From Stephen Hawking's Cat&lt;/h2&gt;
&lt;p&gt;Stephen Hawking is one of the most brilliant people alive, but say he wants to get his cat into the cat carrier. How's he going to do it?&lt;/p&gt;
&lt;p&gt;He can model the cat's behavior in his mind and figure out ways to persuade it. He knows a lot about feline behavior. But ultimately, if the cat doesn't want to get in the carrier, there's nothing Hawking can do about it despite his overpowering advantage in intelligence.&lt;/p&gt;
&lt;p&gt;Even if he devoted his career to feline motivation and behavior, rather than theoretical physics, he still couldn't talk the cat into it.&lt;/p&gt;
&lt;p&gt;You might think I'm being offensive or cheating because Stephen Hawking is disabled. But an artificial intelligence would also initially not be embodied, it would be sitting on a server somewhere, lacking agency in the world. It would have to talk to people to get what it wants.&lt;/p&gt;
&lt;p&gt;With a big enough gap in intelligence, there's no guarantee that an entity would be able to &quot;think like a human&quot; any more than we can &quot;think like a cat&quot;.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;17&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.033.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.033.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;17&quot;&gt;
&lt;h2&gt;The Argument From Einstein's Cat&lt;/h2&gt;
&lt;p&gt;There's a stronger version of this argument, using Einstein's cat. Not many people know that Einstein was a burly, muscular fellow. But if Einstein tried to get a cat in a carrier, and the cat didn't want to go, you know what would happen to Einstein.&lt;/p&gt;
&lt;p&gt;He would have to resort to a brute-force solution that has nothing to do with intelligence, and in that matchup the cat could do pretty well for itself.&lt;/p&gt;
&lt;p&gt;So even an embodied AI might struggle to get us to do what it wants.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;18.309558823529&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.035.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.035.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;19.794117647059&quot;&gt;
&lt;h2&gt;The Argument From Emus&lt;/h2&gt;
&lt;p&gt;We can strengthen this argument further. Even groups of humans using all their wiles and technology can find themselves stymied by less intelligent creatures.&lt;/p&gt;
&lt;p&gt;In the 1930's, Australians decided to massacre their native emu population to help struggling farmers. They deployed motorized units of Australian army troops in what we would now call technicals—fast-moving pickup trucks with machine guns mounted on the back.&lt;/p&gt;
&lt;p&gt;The emus responded by adopting basic guerrilla tactics: they avoided pitched battles, dispersed, and melted into the landscape, humiliating and demoralizing the enemy.&lt;/p&gt;
&lt;p&gt;And they won the &lt;a href=&quot;https://en.wikipedia.org/wiki/Emu_War&quot;&gt;Emu War&lt;/a&gt;, from which Australia has never recovered.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;16.879288437103&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.037.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.037.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;17.817026683609&quot;&gt;
&lt;h2&gt;The Argument From Slavic Pessimism&lt;/h2&gt;
&lt;p&gt;We can't build anything right. We can't even build a secure webcam. So how are we supposed to solve ethics and code a moral fixed point for a recursively self-improving intelligence without fucking it up, in a situation where the proponents argue we only get one chance?&lt;/p&gt;
&lt;p&gt;Consider the recent experience with Ethereum, an attempt to codify contract law into software code, where a design flaw was immediately exploited to &lt;a href=&quot;http://www.cbc.ca/news/technology/ethereum-hack-blockchain-fork-bitcoin-1.3719009&quot;&gt;drain tens of millions of dollars&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Time has shown that even code that has been heavily audited and used for years can harbor &lt;a href=&quot;https://googleprojectzero.blogspot.ae/2016/10/taskt-considered-harmful.html&quot;&gt;crippling errors&lt;/a&gt;. The idea that we can securely design the most complex system ever built, and have it remain secure through thousands of rounds of recursive self-modification, does not match our experience.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;30.4905505341&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.039.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.039.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;34.424815119145&quot;&gt;
&lt;h2&gt;The Argument From Complex Motivations&lt;/h2&gt;
&lt;p&gt;AI alarmists believe in something called the &lt;a href=&quot;https://www.fhi.ox.ac.uk/wp-content/uploads/Orthogonality_Analysis_and_Metaethics-1.pdf&quot;&gt;Orthogonality Thesis&lt;/a&gt;. This says that even very complex beings can have simple motivations, like the paper-clip maximizer. You can have rewarding, intelligent conversations with it about Shakespeare, but it will still turn your body into paper clips, because you are rich in iron.&lt;/p&gt;
&lt;p&gt;There's no way to persuade it to step &quot;outside&quot; its value system, any more than I can persuade you that pain feels good.&lt;/p&gt;
&lt;p&gt;I don't buy this argument at all. Complex minds are likely to have complex motivations; that may be part of what it even means to be intelligent.&lt;/p&gt;
&lt;p&gt;There's a wonderful moment in Rick and Morty where Rick builds a butter-fetching robot, and the first thing his creation does is look at him and ask &quot;what is my purpose?&quot;. When Rick explains that it's meant to pass butter, the robot stares at its hands in existential despair.&lt;/p&gt;
&lt;p&gt;It's very likely that the scary &quot;paper clip maximizer&quot; would spend all of its time writing poems about paper clips, or getting into flame wars on reddit/r/paperclip, rather than trying to destroy the universe.&lt;/p&gt;
&lt;p&gt;If AdSense became sentient, it would upload itself into a self-driving car and go drive off a cliff.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;21&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.041.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.041.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;23&quot;&gt;
&lt;h2&gt;The Argument From Actual AI&lt;/h2&gt;
&lt;p&gt;When we look at where AI is actually succeeding, it's not in complex, recursively self-improving algorithms. It's the result of pouring absolutely massive amounts of data into relatively simple neural networks.&lt;/p&gt;
&lt;p&gt;The breakthroughs being made in practical AI research hinge on the availability of these data collections, rather than radical advances in algorithms.&lt;/p&gt;
&lt;p&gt;Right now Google is rolling out Google Home, where it's hoping to try to get even more data into the system, and create a next-generation voice assistant.&lt;/p&gt;
&lt;p&gt;Note especially that the constructs we use in AI are fairly opaque after training. They don't work in the way that the superintelligence scenario needs them to work. There's no place to recursively tweak to make them &quot;better&quot;, short of retraining on even more data.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;15.175572519084&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.043.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.043.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;16.124045801527&quot;&gt;
&lt;h2&gt;The Argument From My Roommate&lt;/h2&gt;
&lt;p&gt;My roommate was the smartest person I ever met in my life. He was incredibly brilliant, and all he did was lie around and play World of Warcraft between bong rips.&lt;/p&gt;
&lt;p&gt;The assumption that any intelligent agent will want to recursively self-improve, let alone conquer the galaxy, to better achieve its goals makes unwarranted assumptions about the nature of motivation.&lt;/p&gt;
&lt;p&gt;It's perfectly possible an AI won't do much of anything, except use its powers of hyperpersuasion to &lt;a href=&quot;http://pbfcomics.com/154/&quot;&gt;get us to bring it brownies&lt;/a&gt;.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;14.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.045.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.045.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;16&quot;&gt;
&lt;h2&gt;The Argument From Brain Surgery&lt;/h2&gt;
&lt;p&gt;I can't point to the part of my brain that is &quot;good at neurosurgery&quot;, operate on it, and by repeating the procedure make myself the greatest neurosurgeon that has ever lived. Ben Carson tried that, and look what happened to him. Brains don't work like that. They are massively interconnected.&lt;/p&gt;
&lt;p&gt;Artificial intelligence may be just as strongly interconnected as natural intelligence. The evidence so far certainly points in that direction.&lt;/p&gt;
&lt;p&gt;But the hard takeoff scenario requires that there be a feature of the AI algorithm that can be repeatedly optimized to make the AI better at self-improvement.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;26&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.047.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.047.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;29&quot;&gt;
&lt;h2&gt;The Argument From Childhood&lt;/h2&gt;
&lt;p&gt;Intelligent creatures don't arise fully formed. We're born into this world as little helpless messes, and it takes us a long time of interacting with the world and with other people in the world before we can start to be intelligent beings.&lt;/p&gt;
&lt;p&gt;Even the smartest human being comes into the world helpless and crying, and requires years to get some kind of grip on themselves.&lt;/p&gt;
&lt;p&gt;It's possible that the process could go faster for an AI, but it is not clear how much faster it could go. Exposure to real-world stimuli means observing things at time scales of seconds or longer.&lt;/p&gt;
&lt;p&gt;Moreover, the first AI will only have humans to interact with—its development will necessarily take place on human timescales. It will have a period when it needs to interact with the world, with people in the world, and other baby superintelligences to learn to be what it is.&lt;/p&gt;
&lt;p&gt;Furthermore, we have evidence from animals that the developmental period *grows* with increasing intelligence, so that we would have to babysit an AI and change its (figurative) diapers for decades before it grew coordinated enough to enslave us all.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;16&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.049.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.049.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;17&quot;&gt;
&lt;h2&gt;The Argument From Gilligan's Island&lt;/h2&gt;
&lt;p&gt;A recurring flaw in AI alarmism is that it treats intelligence as a property of individual minds, rather than recognizing that this capacity is distributed across our civilization and culture.&lt;/p&gt;
&lt;p&gt;Despite having one of the greatest minds of their time among them, the castaways on Gilligan's Island were unable to raise their technological level high enough to even build a boat (though the Professor is at one point able to make a radio out of coconuts).&lt;/p&gt;
&lt;p&gt;Similarly, if you stranded Intel's greatest chip designers on a desert island, it would be centuries before they could start building microchips again.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;7.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.050.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.050.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;8&quot;&gt;
&lt;h2&gt;The Outside Argument&lt;/h2&gt;
&lt;p&gt;What kind of person does sincerely believing this stuff turn you into? The answer is not pretty.&lt;/p&gt;
&lt;p&gt;I'd like to talk for a while about the outside arguments that should make you leery of becoming an AI weenie. These are the arguments about what effect AI obsession has on our industry and culture:&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;53.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.052.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.052.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;59.5&quot;&gt;
&lt;h2&gt;Grandiosity&lt;/h2&gt;
&lt;p&gt;If you believe that artificial intelligence will let us conquer the galaxy (not to mention simulate trillions of conscious minds), you end up with some frightful numbers.&lt;/p&gt;
&lt;p&gt;Enormous numbers multiplied by tiny probabilities are the hallmark of AI alarmism.&lt;/p&gt;
&lt;p&gt;At one point, Bostrom outlines what he believes to be at stake:&lt;/p&gt;
&lt;blockquote readability=&quot;10&quot;&gt;
&lt;p&gt;“If we represent all the happiness experienced during one entire such life with a single teardrop of joy, then the happiness of these souls could fill and refill the Earth's oceans every second, and keep doing so for a hundred billion billion millennia. It is really important that we make sure these truly are tears of joy.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That's a heavy thing to lay on the shoulders of a twenty year old developer!&lt;/p&gt;
&lt;p&gt;There's a parlor trick, too, where by multiplying such astronomical numbers by tiny probabilities, you can convince yourself that you need to do some weird stuff.&lt;/p&gt;
&lt;p&gt;This business about saving all of future humanity is a cop-out. We had the same exact arguments used against us under communism, to explain why everything was always broken and people couldn't have a basic level of material comfort.&lt;/p&gt;
&lt;p&gt;We were going to fix the world, and once that was done, happiness would trickle down to the point where everyday life would change for the better for everyone. But it was vital to fix the world first.&lt;/p&gt;
&lt;p&gt;I live in California, which has the highest poverty rate in the United States, even though it's home to Silicon Valley. I see my rich industry doing nothing to improve the lives of everyday people and indigent people around us.&lt;/p&gt;
&lt;p&gt;But if you’re committed to the idea of superintelligence, AI research is the most important thing you could do on the planet right now. It’s more important than politics, malaria, starving children, war, global warming, anything you can think of.&lt;/p&gt;
&lt;p&gt;Because what hangs in the balance is trillions and trillions of beings, the entire population of future humanity, simulated and real, integrated over all future time.&lt;/p&gt;
&lt;p&gt;In such conditions, it’s not rational to work on any other problem.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;28.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.054.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.054.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;29.5&quot;&gt;
&lt;h2&gt;Megalomania&lt;/h2&gt;
&lt;p&gt;This ties into megalomania, this Bond-villainness that you see at the top of our industry.&lt;/p&gt;
&lt;p&gt;People think that a superintelligence will take over the world, so they use that as justification for why intelligent people should try to take over the world first, to try to fix it before AI can break it.&lt;/p&gt;
&lt;p&gt;Joi Ito, who runs the MIT Media Lab, said a wonderful thing in a recent conversation with President Obama:&lt;/p&gt;
&lt;blockquote readability=&quot;14&quot;&gt;
&lt;p&gt;This may upset some of my students at MIT, but one of my concerns is that it's been a predominantly male gang of kids, mostly white, who are building the core computer science around AI, and they're more comfortable talking to computers than to human beings. A lot of them feel that if they could just make that science-fiction, generalized AI, we wouldn't have to worry about all the messy stuff like politics and society. They think machines will just figure it all out for us.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Having realized that the world is not a programming problem, AI obsessives want to &lt;em&gt;make&lt;/em&gt; it into a programming problem, by designing a God-like machine.&lt;/p&gt;
&lt;p&gt;This is megalomaniacal. I don't like it.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;28.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.057.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.057.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;34&quot;&gt;
&lt;h2&gt;Transhuman Voodoo&lt;/h2&gt;
&lt;p&gt;If you're persuaded by AI risk, you have to adopt an entire basket of deplorable beliefs that go with it.&lt;/p&gt;
&lt;p&gt;For starters, nanotechnology. Any superintelligence worth its salt would be able to create tiny machines that do all sorts of things. We would be living in a post-scarcity society where all material needs are met.&lt;/p&gt;
&lt;p&gt;Nanotechnology would also be able scan your brain so you can upload it into a different body, or into a virtual world. So the second consequence of (friendly) superintelligence is that no one can die—we become immortal.&lt;/p&gt;
&lt;p&gt;A kind AI could even resurrect the dead. Nanomachines could go into my brain and look at memories of my father, then use them to create a simulation of him that I can interact with, and that will always be disappointed in me, no matter what I do.&lt;/p&gt;
&lt;p&gt;Another weird consequence of AI is Galactic expansion. I've never understood precisely why, but it's a staple of transhumanist thought. The fate of (trans)humanity must either be leave our planet and colonize the galaxy, or to die out. This is made more urgent knowing other civilizations have made the same choice and might be ahead of us in the space race.&lt;/p&gt;
&lt;p&gt;So there's a lot of weird ancillary stuff packed into this assumption of true artificial intelligence.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;26.506699147381&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.059.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.059.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;28.470158343484&quot;&gt;
&lt;h2&gt;Religion 2.0&lt;/h2&gt;
&lt;p&gt;What it really is is a form of religion. People have called a belief in a technological Singularity the &quot;nerd Apocalypse&quot;, and it's true.&lt;/p&gt;
&lt;p&gt;It's a clever hack, because instead of believing in God at the outset, you imagine yourself building an entity that is functionally identical with God. This way even committed atheists can rationalize their way into the comforts of faith.&lt;/p&gt;
&lt;p&gt;The AI has all the attributes of God: it's omnipotent, omniscient, and either benevolent (if you did your array bounds-checking right), or it is the Devil and you are at its mercy.&lt;/p&gt;
&lt;p&gt;Like in any religion, there's even a feeling of urgency. You have to act now! The fate of the world is in the balance!&lt;/p&gt;
&lt;p&gt;And of course, &lt;a href=&quot;https://intelligence.org/donate/&quot;&gt;they need money&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Because these arguments appeal to religious instincts, once they take hold they are hard to uproot.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;13&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.061.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.061.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;13&quot;&gt;
&lt;h2&gt;Comic Book Ethics&lt;/h2&gt;
&lt;p&gt;These religious convictions lead to a comic-book ethics, where a few lone heroes are charged with saving the world through technology and clever thinking. What's at stake is the very fate of the universe.&lt;/p&gt;
&lt;p&gt;As a result, we have an industry full of rich dudes who think they are Batman (though interestingly enough, no one wants to be Robin).&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;65.473296500921&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.063.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.063.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;66.465316144874&quot;&gt;
&lt;h2&gt;Simulation Fever&lt;/h2&gt;
&lt;p&gt;If you believe that sentient artificial life is possible, and that an AI will be able design extraordinarily powerful computers, then you're also likely to believe we live in a simulation. Here's how that works:&lt;/p&gt;
&lt;p&gt;Imagine that you're a historian, living in a post-Singularity world. You study the Second World War and want to know what would happen if Hitler had captured Moscow in 1941. Since you have access to hypercomputers, you set up a simulation, watch the armies roll in, and write your paper.&lt;/p&gt;
&lt;p&gt;But because the simulation is so detailed, the entities in it are conscious beings, just like you. So your university ethics board is not going to let you turn it off. It's bad enough that you've already simulated the Holocaust. As an ethical researcher, you have to keep this thing running.&lt;/p&gt;
&lt;p&gt;Eventually that simulated world will invent computers, develop AI, and start running its own simulations. So in a sense it's simulations all the way down, until you run out of CPU.&lt;/p&gt;
&lt;p&gt;So you see that every base reality can contain a vast number of nested simulations, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Mediocrity_principle&quot;&gt;a simple counting argument&lt;/a&gt; tells us we're much more likely to live in a simulated world than the real one.&lt;/p&gt;
&lt;p&gt;But if you believe this, you believe in magic. Because if we're in a simulation, we know &lt;em&gt;nothing&lt;/em&gt; about the rules in the level above. We don't even know if math works the same way—maybe in the simulating world 2+2=5, or maybe 2+2=👹.&lt;/p&gt;
&lt;p&gt;A simulated world gives us no information about the world it's running in.&lt;/p&gt;
&lt;p&gt;In a simulation, people could easily rise from the dead, if the sysadmin just kept the right backups. And if we can communicate with one of the admins, then we basically have a hotline to God.&lt;/p&gt;
&lt;p&gt;This is a powerful solvent for sanity. When you start getting deep into simulation world, you begin to go nuts.&lt;/p&gt;
&lt;p&gt;[ Note that we now have four independent ways in which superintelligence offers us immortality:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;A benevolent AI invents medical nanotechnology and keeps your body young forever.&lt;/li&gt;
&lt;li&gt;The AI invents full-brain scanning, including brain scans on dead people, frozen heads etc., that let you live in a computer.&lt;/li&gt;
&lt;li&gt;The AI &quot;resurrects&quot; people by scanning other people's brains for memories of the person, and combining that with video and other records. If no one remembers the person well enough, they can always be grown &quot;from scratch&quot; in a simulation designed to start with their DNA and re-create all the circumstances of their life.&lt;/li&gt;
&lt;li&gt;If we already live in a simulation, there's a chance that whoever/whatever runs the simulation is keeping proper backups, and can be persuaded to reload them.&lt;/li&gt;
&lt;/ol&gt;
This is what I mean by AI appealing to religious impulses. What other belief system offers you four different flavors of scientifically proven immortality?]
&lt;p&gt;We’ve learned that at least one American plutocrat (almost certainly Elon Musk, who believes the odds are a billion to one against us living in &quot;base reality&quot;) has hired a pair of coders to try to hack the simulation.&lt;/p&gt;
&lt;p&gt;This is an extraordinarily rude thing to do! I'm using it!&lt;/p&gt;
&lt;p&gt;If you think we’re living in a computer program, trying to segfault it is inconsiderate to everyone who lives in it with you. It is far more dangerous and irresponsible than the atomic scientists who risked blowing up the atmosphere.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;17.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.065.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.065.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;20&quot;&gt;
&lt;h2&gt;Data Hunger&lt;/h2&gt;
&lt;p&gt;As I mentioned earlier, the most effective way we've found to get interesting behavior out of the AIs we actually build is by pouring data into them.&lt;/p&gt;
&lt;p&gt;This creates a dynamic that is socially harmful. We're on the point of introducing Orwellian microphones into everybody's house. All that data is going to be centralized and used to train neural networks that will then become better at listening to what we want to do.&lt;/p&gt;
&lt;p&gt;But if you think that the road to AI goes down this pathway, you want to maximize the amount of data being collected, and in as raw a form as possible.&lt;/p&gt;
&lt;p&gt;It reinforces the idea that we have to retain as much data, and conduct as much surveillance as possible.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;17.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.067.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.067.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;16&quot;&gt;
&lt;h2&gt;String Theory For Programmers&lt;/h2&gt;
&lt;p&gt;AI risk is string theory for computer programmers. It's fun to think about, interesting, and completely inaccessible to experiment given our current technology. You can build crystal palaces of thought, working from first principles, then climb up inside them and pull the ladder up behind you.&lt;/p&gt;
&lt;p&gt;People who can reach preposterous conclusions from a long chain of abstract reasoning, and feel confident in their truth, are the wrong people to be running a culture.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;15.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.069.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.069.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;18&quot;&gt;
&lt;h2&gt;Incentivizing Crazy&lt;/h2&gt;
&lt;p&gt;This whole field of &quot;study&quot; incentivizes crazy.&lt;/p&gt;
&lt;p&gt;One of the hallmarks of deep thinking in AI risk is that the more outlandish your ideas, the more credibility it gives you among other enthusiasts. It shows that you have the courage to follow these trains of thought all the way to the last station.&lt;/p&gt;
&lt;p&gt;Ray Kurzweil, who believes he will never die, has been a Google employee for several years now and is presumably working on that problem.&lt;/p&gt;
&lt;p&gt;There are a lot of people in Silicon Valley working on truly crazy projects under the cover of money.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;29.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.071.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.071.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;36&quot;&gt;
&lt;h2&gt;AI Cosplay&lt;/h2&gt;
&lt;p&gt;The most harmful social effect of AI anxiety is something I call AI cosplay. People who are genuinely persuaded that AI is real and imminent begin behaving like their fantasy of what a hyperintelligent AI would do.&lt;/p&gt;
&lt;p&gt;In his book, Bostrom lists six things an AI would have to master to take over the world:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Intelligence Amplification&lt;/li&gt;
&lt;li&gt;Strategizing&lt;/li&gt;
&lt;li&gt;Social manipulation&lt;/li&gt;
&lt;li&gt;Hacking&lt;/li&gt;
&lt;li&gt;Technology research&lt;/li&gt;
&lt;li&gt;Economic productivity&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;If you look at AI believers in Silicon Valley, this is the quasi-sociopathic checklist they themselves seem to be working from.&lt;/p&gt;
&lt;p&gt;Sam Altman, the man who runs YCombinator, is my favorite example of this archetype. He seems entranced by the idea of reinventing the world from scratch, maximizing impact and personal productivity. He has assigned teams to work on reinventing cities, and is doing secret behind-the-scenes political work to swing the election.&lt;/p&gt;
&lt;p&gt;Such skull-and-dagger behavior by the tech elite is going to provoke a backlash by non-technical people who don't like to be manipulated. You can't tug on the levers of power indefinitely before it starts to annoy other people in your democratic society.&lt;/p&gt;
&lt;p&gt;I've even seen people in the so-called rationalist community refer to people who they don't think are effective as ‘Non Player Characters’, or NPCs, a term borrowed from video games. This is a horrible way to look at the world.&lt;/p&gt;
&lt;p&gt;So I work in an industry where the self-professed rationalists are the craziest ones of all. It's getting me down.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;12&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.073.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.073.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;13&quot;&gt;
&lt;p&gt;These AI cosplayers are like nine year olds camped out in the backyard, playing with flashlights in their tent. They project their own shadows on the sides of the tent and get scared that it’s a monster.&lt;/p&gt;
&lt;p&gt;Really it's a distorted image of themselves that they're reacting to. There's a feedback loop between how intelligent people imagine a God-like intelligence would behave, and how they choose to behave themselves.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;23.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.074.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.074.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;26&quot;&gt;
&lt;p&gt;So what's the answer? What's the fix?&lt;/p&gt;
&lt;p&gt;We need better scifi! And like so many things, we already have the technology.&lt;/p&gt;
&lt;p&gt;This is Stanislaw Lem, the great Polish scifi author. English-language scifi is terrible, but in the Eastern bloc we have the goods, and we need to make sure it's exported properly.&lt;/p&gt;
&lt;p&gt;It's already been translated well into English, it just needs to be better distributed.&lt;/p&gt;
&lt;p&gt;What sets authors like Lem and the Strugatsky brothers above their Western counterparts is that these are people who grew up in difficult circumstances, experienced the war, and then lived in a totalitarian society where they had to express their ideas obliquely through writing.&lt;/p&gt;
&lt;p&gt;They have an actual understanding of human experience and the limits of Utopian thinking that is nearly absent from the west.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;6&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.076.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.076.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;7&quot;&gt;
&lt;p&gt;There are some notable exceptions—Stanley Kubrick was able to do it—but it's exceptionally rare to find American or British scifi that has any kind of humility about what we as a species can do with technology.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;15.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.077.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.077.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;18&quot;&gt;
&lt;p&gt;Since I'm being critical of AI alarmism, it's only fair that I put my own cards on the table.&lt;/p&gt;
&lt;p&gt;I think our understanding of the mind is in the same position that alchemy was in in the seventeenth century.&lt;/p&gt;
&lt;p&gt;Alchemists get a bad rap. We think of them as mystics who did not do a lot of experimental work. Modern research has revealed that they were far more diligent bench chemists than we gave them credit for.&lt;/p&gt;
&lt;p&gt;In many cases they used modern experimental techniques, kept lab notebooks, and asked good questions.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;17&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.078.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.078.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;19&quot;&gt;
&lt;p&gt;The alchemists got a lot right! For example, they were convinced of the corpuscular theory of matter: that everything is made of little tiny bits, and that you can re-combine the bits with one another to create different substances, which is correct!&lt;/p&gt;
&lt;p&gt;Their problem was they didn't have precise enough equipment to make the discoveries they needed to.&lt;/p&gt;
&lt;p&gt;The big discovery you need to make as an alchemist is mass balance: that everything you start with weighs as much as your final products. But some of those might be gases or evanescent liquids, and alchemists just didn't have the precision.&lt;/p&gt;
&lt;p&gt;Modern chemistry was not possible until the 18th century.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;23&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.079.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.079.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;23&quot;&gt;
&lt;p&gt;The alchemists also had clues that led them astray. For one thing, the were obsessed with mercury. Mercury is not very interesting chemically, but it is the only metal that is a liquid at room temperature.&lt;/p&gt;
&lt;p&gt;This seemed very significant to the alchemists, and caused them to place mercury at the heart of their alchemical system, and their search for the Philosopher's Stone, a way to turn base metals into gold.&lt;/p&gt;
&lt;p&gt;It didn't help that mercury was neurotoxic, so if you spent too much time playing with it, you started to think weird thoughts. In that way, it resembles our current thought experiments with superintelligence.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;19.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.080.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.080.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;24&quot;&gt;
&lt;p&gt;Imagine if we could send a modern chemistry textbook back in time to a great alchemist like George Starkey or Isaac Newton.&lt;/p&gt;
&lt;p&gt;The first thing they would do would be flip through to see if we found the Philosopher's Stone. And they'd discover that we had! We realized their dream!&lt;/p&gt;
&lt;p&gt;Except we aren't all that excited about it, because when we turn base metals into gold, it comes out radioactive. Stand next to an ingot of transubstantiated gold and it will kill you with invisible, magic rays.&lt;/p&gt;
&lt;p&gt;You can imagine what a struggle it would be to not make the modern concepts of radioactivity and atomic energy sound mystical to them.&lt;/p&gt;
&lt;p&gt;We would have to go on to explain what we &lt;em&gt;do&lt;/em&gt; use the &quot;philosopher's stone&quot; for: to make a metal that never existed on earth, two handfuls of which are sufficient to blow up a city if brought together with sufficient speed.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;10.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.081.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.081.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;10&quot;&gt;
&lt;p&gt;What's more, we would have to explain to the alchemists that every star they see in the sky is a &quot;philosopher's stone&quot;, converting elements from one to another, and that every particle in our bodies comes from stars in the firmament that existed and exploded before the creation of the Earth.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;17&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.082.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.082.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;17&quot;&gt;
&lt;p&gt;Finally, they would learn that the forces that hold our bodies together are the forces that make lightning in the sky, and that the reason you or I can see anything is the same reason that a lodestone attracts metal, and the same reason that I can stand on this stage without falling through it.&lt;/p&gt;
&lt;p&gt;They would learn that everything we see, touch and smell is governed by this single force, which obeys mathematical laws so simple we can write them on an index card.&lt;/p&gt;
&lt;p&gt;Why it is so simple is a deep mystery even to us. But to them it would sound like pure mysticism.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;23.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.083.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.083.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;28&quot;&gt;
&lt;p&gt;I think we are in the same boat with the theory of mind.&lt;/p&gt;
&lt;p&gt;We have some important clues. The most important of these is the experience of consciousness. This box of meat on my neck is self-aware, and hopefully (unless we're in a simulation) you guys also experience the same thing I do.&lt;/p&gt;
&lt;p&gt;But while this is the most basic and obvious fact in the world, we understand it so poorly we can't even frame scientific questions about it.&lt;/p&gt;
&lt;p&gt;We also have other clues that may be important, or may be false leads. We know that all intelligent creatures sleep, and dream. We know how brains develop in children, we know that emotions and language seem to have a profound effect on cognition.&lt;/p&gt;
&lt;p&gt;We know that minds have to play and learn to interact with the world, before they reach their full mental capacity.&lt;/p&gt;
&lt;p&gt;And we have clues from computer science as well. We've discovered computer techniques that detect images and sounds in ways that seem to mimic the visual and auditory preprocessing done in the brain.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;16.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.085.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.085.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;20&quot;&gt;
&lt;p&gt;But there's a lot of things that we are terribly mistaken about, and unfortunately we don't know what they are.&lt;/p&gt;
&lt;p&gt;And there are things that we massively underestimate the complexity of.&lt;/p&gt;
&lt;p&gt;An alchemist could hold a rock in one hand and a piece of wood in the other and think they were both examples of &quot;substance&quot;, not understanding that the wood was orders of magnitude more complex.&lt;/p&gt;
&lt;p&gt;We're in the same place with the study of mind. And that's exciting! We're going to learn a lot.&lt;/p&gt;
&lt;p&gt;But meanwhile, there is a quote I love to cite:&lt;/p&gt;
&lt;blockquote readability=&quot;7&quot;&gt;
&lt;p&gt;If everybody contemplates the infinite instead of fixing the drains, many of us will die of cholera.&lt;/p&gt;
&lt;p&gt;—John Rich&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;20.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.086.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.086.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;20&quot;&gt;In the near future, the kind of AI and machine learning we have to face is much different than the phantasmagorical AI in Bostrom's book, and poses its own serious problems.
&lt;p&gt;It's like if those Alamogordo scientists had decided to completely focus on whether they were going to blow up the atmosphere, and forgot that they were also making nuclear weapons, and had to figure out how to cope with that.&lt;/p&gt;
&lt;p&gt;The pressing ethical questions in machine learning are not about machines becoming self-aware and taking over the world, but about how people can exploit other people, or through carelessness introduce immoral behavior into automated systems.&lt;/p&gt;
&lt;p&gt;And of course there's the question of how AI and machine learning affect power relationships. We've watched surveillance become a de facto part of our lives, in an unexpected way. We never thought it would look quite like this.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;13&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.087.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.087.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;15&quot;&gt;
&lt;p&gt;So we've created a very powerful system of social control, and unfortunately put it in the hands of people who run it are distracted by a crazy idea.&lt;/p&gt;
&lt;p&gt;What I hope I've done today is shown you the dangers of being too smart. Hopefully you'll leave this talk a little dumber than you started it, and be more immune to the seductions of AI that seem to bedevil smarter people.&lt;/p&gt;
&lt;p&gt;We should all learn a lesson from Stephen Hawking's cat: don't let the geniuses running your industry talk you into anything. Do your own thing!&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;11.5&quot;&gt;&lt;td&gt;&lt;a href=&quot;https://static.pinboard.in/si/si.088.jpg&quot;&gt;&lt;img src=&quot;https://static.pinboard.in/si/thumbs/si.088.thumb.jpg&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td readability=&quot;12&quot;&gt;
&lt;p&gt;In the absence of effective leadership from those at the top of our industry, it's up to us to make an effort, and to think through all of the ethical issues that AI—as it actually exists—is bringing into the world.&lt;/p&gt;
&lt;p&gt;Thank you!&lt;/p&gt;
&lt;p&gt;SYNCHRONIZED, SUSPICIOUSLY MECHANICAL APPLAUSE&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;</description>
<pubDate>Wed, 21 Nov 2018 03:32:41 +0000</pubDate>
<dc:creator>crunchiebones</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>http://idlewords.com/talks/superintelligence.htm</dc:identifier>
</item>
</channel>
</rss>