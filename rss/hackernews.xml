<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>YouTube and Facebook Are Losing Creators to Blockchain-Powered Rivals</title>
<link>https://www.bloomberg.com/news/articles/2018-04-10/youtube-and-facebook-are-losing-creators-to-blockchain-powered-rivals</link>
<guid isPermaLink="true" >https://www.bloomberg.com/news/articles/2018-04-10/youtube-and-facebook-are-losing-creators-to-blockchain-powered-rivals</guid>
<description>&lt;p&gt;Peter “Furious Pete” Czerwinski has close to 5 million &lt;a href=&quot;https://www.youtube.com/channel/UCspJ-h5Mw9_zeEhJDzMpkkA&quot; itemprop=&quot;StoryLink&quot; itemscope=&quot;itemscope&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;YouTube followers&lt;/a&gt;, but they can’t see most of his new videos there. To get 46 of the 71 weightlifting and competitive-eating videos he’s posted in the past two months, fans have to use &lt;a href=&quot;https://d.tube/#!/s/furious%20pete&quot; itemprop=&quot;StoryLink&quot; itemscope=&quot;itemscope&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;DTube&lt;/a&gt;. It looks a lot like YouTube, but video creators rely on donations from viewers instead of ad revenue, and the site’s moderators rarely try to censor potentially offensive material. Czerwinski, who made the switch two months ago, has said he felt his material could no longer get the circulation it deserved on YouTube. He didn’t respond to requests for comment.&lt;/p&gt;


&lt;p&gt;While YouTube has had to start taking a tougher line on censoring offensive videos that advertisers don’t want to be associated with, a growing swath of creators have fled to sites such as DTube to avoid the constraints. Like other upstart sites, DTube runs on the blockchain network Steem, and users can pay creators and commenters in digital tokens. That’s another point of distinction with YouTube, as well as with Facebook and Twitter. All three advertising-driven sites are phasing out ads for cryptocurrencies, shielding themselves from potential legal liability if the ads are scams or the digital coins are eventually regulated as securities. Video creators with an interest in cryptocurrency say that’s also a factor driving them away from the big names. In the wake of Facebook’s data scandal, privacy is a third. YouTube didn’t respond to a request for comment for this story; Facebook referred to previous statements that the company is working to repair its reputation but hasn’t seen a significant drop in users.&lt;/p&gt;


&lt;p&gt;The less centralized platforms keep more power—and potentially, privacy—in the hands of creators and users, says &lt;a href=&quot;https://steemit.com/@ned&quot; itemprop=&quot;StoryLink&quot; itemscope=&quot;itemscope&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Ned Scott&lt;/a&gt;, who runs the Steem-based social network &lt;a href=&quot;https://steemit.com/&quot; itemprop=&quot;StoryLink&quot; itemscope=&quot;itemscope&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Steemit&lt;/a&gt;. In lieu of ads or selling user data, sites such as Scott’s rely on user growth to make their digital tokens ever more valuable. “The whole experience is more transparent,” he says. “There won’t be many single authorities dictating how social media operates.” Two-year-old Steemit has about a million accounts and added some 120,000 last month, according to Scott. Rival &lt;a href=&quot;https://lbry.io/&quot; itemprop=&quot;StoryLink&quot; itemscope=&quot;itemscope&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;LBRY&lt;/a&gt; has about 600,000 registered users.&lt;/p&gt;




&lt;p&gt;That’s far from Facebook’s 2 billion monthly users, or even the up to 87 million whose data was given to Cambridge Analytica. But &lt;a href=&quot;https://www.youtube.com/channel/UCSuHzQ3GrHSzoBbwrIq3LLA&quot; itemprop=&quot;StoryLink&quot; itemscope=&quot;itemscope&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Naomi Brockwell&lt;/a&gt;, a video creator in New York who specializes in crypto material, says she’s already making an average of $40 a video in Steem tokens, which could take her months on YouTube. Brockwell says she needs to find alternatives to the name-brand social networks as they phase out ads on her videos. “I don’t think this is about protection,” she says. “This is about control.”&lt;/p&gt;


&lt;p&gt;Brockwell, like many creators on these newish sites, was initially skeptical that the Steem tokens were worth anything. And it’s something of a pain to exchange them for U.S. dollars: On most cryptocurrency exchange sites, you have to trade them for Bitcoin first, then swap the Bitcoin for cash. Easy enough, Brockwell says, given the upside.&lt;/p&gt;
&lt;p&gt;Creators can expect to retain significant control with blockchain sites because there are few barriers if they decide to leave, says LBRY Chief Executive Officer &lt;a href=&quot;https://lbry.io/team&quot; itemprop=&quot;StoryLink&quot; itemscope=&quot;itemscope&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Jeremy Kauffman&lt;/a&gt;. Like email, he says, it’s relatively easy to switch to a similar site if you don’t like his. And because of the decentralized nature of blockchain networks, tech-savvy users can find ways to post controversial material, even if he tries to block or ban it.&lt;/p&gt;
&lt;aside class=&quot;inline-newsletter&quot; data-state=&quot;ready&quot;/&gt;&lt;p&gt;The flip side, of course, is that such lax rules can make these sites havens for precisely the kinds of hate speech, conspiracy theories, and otherwise undesirable material that YouTube and others are slowly taking action to block. Stemming the spread of those kinds of videos without compromising free speech or creators’ wallets has been a difficult balancing act. &lt;a href=&quot;https://www.bloomberg.com/news/videos/2018-04-04/suspect-in-youtube-shooting-identified-video&quot; itemprop=&quot;StoryLink&quot; itemscope=&quot;itemscope&quot; target=&quot;_blank&quot;&gt;Nasim Aghdam&lt;/a&gt;, a YouTube creator from San Diego, complained to relatives repeatedly about the &lt;a href=&quot;https://www.bloomberg.com/news/articles/2018-04-04/youtube-shooting-spotlights-debate-raging-inside-video-giant&quot; itemprop=&quot;StoryLink&quot; itemscope=&quot;itemscope&quot; target=&quot;_blank&quot;&gt;company’s compensation practices&lt;/a&gt; before she opened fire at YouTube’s headquarters in San Bruno, Calif., on April 3, killing herself and injuring three other people.&lt;/p&gt;
&lt;p&gt;If such services as DTube and Steemit are successful enough, their bigger problem may be YouTube’s or Facebook’s cash hoard, says Lance Morginn, CEO of the Blockchain Intelligence Group, which analyzes cryptocurrency transactions. Still, some platform creators say the big names are tarnished enough that there’s an opportunity for new ones. “Until now, I’m not sure people realized just how valuable their personal data is,” says &lt;a href=&quot;https://www.linkedin.com/in/danielnovaes/&quot; itemprop=&quot;StoryLink&quot; itemscope=&quot;itemscope&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Dan Novaes&lt;/a&gt;, co-founder and CEO of &lt;a href=&quot;https://current.us/index.html&quot; itemprop=&quot;StoryLink&quot; itemscope=&quot;itemscope&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Current&lt;/a&gt;, a blockchain-based streaming site. “With blockchain, people will finally have the chance to be rewarded for their time, attention, and data. No longer will their valuable data be controlled by a few giant companies.”&lt;/p&gt;
</description>
<pubDate>Tue, 10 Apr 2018 09:33:19 +0000</pubDate>
<dc:creator>rbanffy</dc:creator>
<og:description>Some creators say privacy concerns, censorship, and a coming ban on cryptocurrency ads are driving them away from the big names.</og:description>
<og:image>https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iKYqAs2BQUdA/v0/1200x857.png</og:image>
<og:title>YouTube and Facebook Are Losing Creators to Blockchain-Powered Rivals</og:title>
<og:type>article</og:type>
<og:url>https://www.bloomberg.com/news/articles/2018-04-10/youtube-and-facebook-are-losing-creators-to-blockchain-powered-rivals</og:url>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bloomberg.com/news/articles/2018-04-10/youtube-and-facebook-are-losing-creators-to-blockchain-powered-rivals</dc:identifier>
</item>
<item>
<title>Apple says it’s now powered by renewable energy worldwide</title>
<link>https://www.apple.com/ca/newsroom/2018/04/apple-now-globally-powered-by-100-percent-renewable-energy/</link>
<guid isPermaLink="true" >https://www.apple.com/ca/newsroom/2018/04/apple-now-globally-powered-by-100-percent-renewable-energy/</guid>
<description>&lt;p&gt;Toronto, Ontario— As part of its commitment to combat climate change and create a healthier environment, Apple today announced its global facilities are powered with 100 percent clean energy. This achievement includes retail stores, offices, data centres and co-located facilities in 43 countries — including the United States, the United Kingdom, China and India. The company also announced nine additional manufacturing partners have committed to power all of their Apple production with 100 percent clean energy, bringing the total number of supplier commitments to 23.&lt;/p&gt;
&lt;p&gt;“We’re committed to leaving the world better than we found it. After years of hard work we’re proud to have reached this significant milestone,” said Tim Cook, Apple’s CEO. “We’re going to keep pushing the boundaries of what is possible with the materials in our products, the way we recycle them, our facilities and our work with suppliers to establish new creative and forward looking sources of renewable energy because we know the future depends on it.”&lt;/p&gt;
</description>
<pubDate>Tue, 10 Apr 2018 03:51:08 +0000</pubDate>
<dc:creator>iamspoilt</dc:creator>
<og:type>article</og:type>
<og:title>Apple now globally powered by 100 percent renewable energy</og:title>
<og:description>As part of Apple’s commitment to fostering a healthier planet through innovation, its global facilities are powered with 100 percent clean energy.</og:description>
<og:url>https://www.apple.com/ca/newsroom/2018/04/apple-now-globally-powered-by-100-percent-renewable-energy/</og:url>
<og:image>https://www.apple.com/newsroom/images/values/environment/renewable_energy_apple_solar_panel_full.jpg.og.jpg?201804092057</og:image>
<dc:language>en-CA</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.apple.com/ca/newsroom/2018/04/apple-now-globally-powered-by-100-percent-renewable-energy/</dc:identifier>
</item>
<item>
<title>Portugal electricity generation temporarily reaches 100% renewable</title>
<link>https://reneweconomy.com.au/portugal-reaches-100-renewables-ends-fossil-fuel-subsidies-32820/</link>
<guid isPermaLink="true" >https://reneweconomy.com.au/portugal-reaches-100-renewables-ends-fossil-fuel-subsidies-32820/</guid>
<description>&lt;div class=&quot;printfriendly pf-alignright&quot;&gt;&lt;a href=&quot;https://reneweconomy.com.au/portugal-reaches-100-renewables-ends-fossil-fuel-subsidies-32820/#&quot; rel=&quot;nofollow&quot; onclick=&quot;window.print(); return false;&quot; class=&quot;noslimstat&quot; title=&quot;Printer Friendly, PDF &amp;amp; Email&quot;&gt;&lt;img src=&quot;https://cdn.printfriendly.com/buttons/print-button-nobg.png&quot; alt=&quot;Print Friendly, PDF &amp;amp; Email&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;&lt;a href=&quot;https://reneweconomy.com.au/wp-content/uploads/2018/04/alto-lindoso-luis-copy.jpg&quot;&gt;&lt;img class=&quot;aligncenter size-full wp-image-109314&quot; src=&quot;https://reneweconomy.com.au/wp-content/uploads/2018/04/alto-lindoso-luis-copy.jpg&quot; alt=&quot;&quot; width=&quot;550&quot; height=&quot;366&quot; srcset=&quot;https://reneweconomy.com.au/wp-content/uploads/2018/04/alto-lindoso-luis-copy.jpg 550w, https://reneweconomy.com.au/wp-content/uploads/2018/04/alto-lindoso-luis-copy-300x200.jpg 300w&quot; sizes=&quot;(max-width: 550px) 100vw, 550px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Portugal’s renewable energy sources generated enough power to exceed total grid demand across the month of March, a new report has found, setting a standard that is expected to become the norm for the European nation.&lt;/p&gt;
&lt;p&gt;According to Portuguese grid operator, REN, renewable energy output over the month reached 4,812GWh, surpassing the nation’s total electricity needs for March, which only topped 4,647GWh.&lt;/p&gt;
&lt;p&gt;In that time, power generated by Portugal’s hydroelectric dams accounted for 55 per cent of monthly consumption – boosted by drought-breaking rainfall of four times the monthly average – and wind power, 42 per cent.&lt;/p&gt;
&lt;p&gt;The achievement comes nearly one year after &lt;a href=&quot;https://reneweconomy.com.au/portugal-runs-on-100-renewables-for-4-days-29784/&quot;&gt;hydro, wind, and solar power helped push the Iberian country to run on 100 per cent renewable electricity for 107 hours straight&lt;/a&gt;. Last March, however, the average renewables supply was 62 per cent.&lt;/p&gt;

&lt;p&gt;The new record coincides with the move by the Portuguese government, last Tuesday, to suspend annual subsidies of around €20 million for guaranteed power supplies paid to producers – most of which goes to fossil fuel plants left in stand-by mode.&lt;/p&gt;
&lt;p&gt;“Last month’s achievement is an example of what will happen more frequently in the near future,” said the Portuguese Renewable Energy Association and the Sustainable Earth System Association in a report published last week.&lt;/p&gt;
&lt;p&gt;“It is expected that by 2040 the production of renewable electricity will be able to guarantee, in a cost-effective way, the total annual electricity consumption of mainland Portugal.”&lt;/p&gt;
&lt;p&gt;The group noted that while fossil fuel plants still worked for short periods to complement the electricity supply, those were fully compensated by other periods of greater renewable production.&lt;/p&gt;
&lt;p&gt;“These data, besides indicating a historical milestone in the Portuguese electricity sector, demonstrate that renewable energy can be relied upon as a secure and viable source with which to completely meet the country’s electricity demands.”&lt;/p&gt;
&lt;p&gt;The effort was also praised by Green MEP Claude Turmes, who cited Portugal’s example as evidence that the EU should support a renewable energy target of more than 27 per cent for 2030.&lt;/p&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot; readability=&quot;8.145896656535&quot;&gt;
&lt;p dir=&quot;ltr&quot; lang=&quot;en&quot;&gt;Impressive news from Portugal: &lt;a href=&quot;https://twitter.com/hashtag/renewables?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#renewables&lt;/a&gt; produced more than 100% of the country’s electricity consumption throughout the month of March! That shows how ridiculous a 27% target for 2030 is. Who will be the next country to follow that path? &lt;a href=&quot;https://twitter.com/hashtag/CleanEnergyEU?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#CleanEnergyEU&lt;/a&gt; &lt;a href=&quot;https://t.co/feUNyBqcPK&quot;&gt;https://t.co/feUNyBqcPK&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;— Claude Turmes (@ClaudeTurmes) &lt;a href=&quot;https://twitter.com/ClaudeTurmes/status/981212868661121026?ref_src=twsrc%5Etfw&quot;&gt;April 3, 2018&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As &lt;a href=&quot;https://www.euractiv.com/section/energy/news/portugal-breaks-100-renewables-mark-but-remains-isolated/&quot;&gt;Euractive reports&lt;/a&gt;, the European Parliament, Commission and member states are currently negotiating an update to the bloc’s renewable rules, with MEPs calling for a 35 per cent renewables target, while the EU executive and national capitals favour the current target.&lt;/p&gt;

&lt;p&gt;Portugal’s renewable energy target, meanwhile, is not all that much higher. According to the IEA, it has a 2020 target of just 31 per cent; 59.6 per cent to come through renewable electricity demand, 35.9 per cent from heating and cooling, and 11.3 per cent from the transport sector.  &lt;/p&gt;
</description>
<pubDate>Tue, 10 Apr 2018 01:17:43 +0000</pubDate>
<dc:creator>mgdo</dc:creator>
<og:type>article</og:type>
<og:title>Portugal reaches 100% renewables, ends fossil fuel subsidies</og:title>
<og:url>https://reneweconomy.com.au/portugal-reaches-100-renewables-ends-fossil-fuel-subsidies-32820/</og:url>
<og:description>Portugal grid averages 103% renewable electricity over month of March; government suspends power supply subsidies in April.</og:description>
<og:image>https://reneweconomy.com.au/wp-content/uploads/2018/04/alto-lindoso-luis-copy.jpg</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://reneweconomy.com.au/portugal-reaches-100-renewables-ends-fossil-fuel-subsidies-32820/</dc:identifier>
</item>
<item>
<title>ZFS on Linux: Unlistable and disappearing files</title>
<link>https://github.com/zfsonlinux/zfs/issues/7401</link>
<guid isPermaLink="true" >https://github.com/zfsonlinux/zfs/issues/7401</guid>
<description>&lt;h3&gt;System information&lt;/h3&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Type&lt;/th&gt;
&lt;th&gt;Version/Name&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody readability=&quot;1&quot;&gt;&lt;tr&gt;&lt;td&gt;Distribution Name&lt;/td&gt;
&lt;td&gt;Scientific Linux&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Distribution Version&lt;/td&gt;
&lt;td&gt;6.8&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;2&quot;&gt;&lt;td&gt;Linux Kernel&lt;/td&gt;
&lt;td&gt;2.6.32-696.23.1.el6.x86_64&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Architecture&lt;/td&gt;
&lt;td&gt;x86_64&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;ZFS Version&lt;/td&gt;
&lt;td&gt;0.7.7&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;SPL Version&lt;/td&gt;
&lt;td&gt;0.7.7&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3&gt;Describe the problem you're observing&lt;/h3&gt;
&lt;p&gt;Data loss when copying a directory with large-ish number of files. For example, &lt;code&gt;cp -r SRC DST&lt;/code&gt; with 10000 files in SRC is likely to result in a couple of &quot;cp: cannot create regular file `DST/XXX': No space left on device&quot; error messages, and a few thousand files missing from the listing of the DST directory. (Needless to say, filesystem being full is not the problem.)&lt;/p&gt;
&lt;p&gt;The missing files are missing in the sense that they don't appear in the directory listing, but can be accessed using their name (except for the couple of files for which &lt;code&gt;cp&lt;/code&gt; generated &quot;No space left on device&quot; error). For example:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;# ls -l DST | grep FOO | wc -l
0
# ls -l DST/FOO
-rw-r--r-- 1 root root 5 Apr  6 14:59 DST/FOO
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The content of DST/FOO are accessible by path (e.g. &lt;code&gt;cat DST/FOO&lt;/code&gt; works) and is the same as SRC/FOO. If caches are dropped (&lt;code&gt;echo 3 &amp;gt; /proc/sys/vm/drop_caches&lt;/code&gt;) or the machine is rebooted, opening FOO directly by path fails.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;ls -ld DST&lt;/code&gt; reports N fewer hard links than SRC, where N is the number of files for which &lt;code&gt;cp&lt;/code&gt; reported &quot;No space left on device&quot; error.&lt;/p&gt;
&lt;p&gt;Names of missing files are mostly predictable if SRC is small.&lt;/p&gt;
&lt;p&gt;Scrub does not find any errors.&lt;/p&gt;
&lt;p&gt;I think the problem appeared in 0.7.7, but I am not sure.&lt;/p&gt;
&lt;h3&gt;Describe how to reproduce the problem&lt;/h3&gt;
&lt;pre&gt;
&lt;code&gt;# mkdir SRC
# for i in $(seq 1 10000); do echo $i &amp;gt; SRC/$i ; done
# cp -r SRC DST
cp: cannot create regular file `DST/8442': No space left on device
cp: cannot create regular file `DST/2629': No space left on device
# ls -l
total 3107
drwxr-xr-x 2 root root 10000 Apr  6 15:28 DST
drwxr-xr-x 2 root root 10002 Apr  6 15:27 SRC
# find DST -type f | wc -l 
8186
# ls -l DST | grep 8445 | wc -l
0
# ls -l DST/8445
-rw-r--r-- 1 root root 5 Apr  6 15:28 DST/8445
# cat DST/8445
8445
# echo 3 &amp;gt; /proc/sys/vm/drop_caches
# cat DST/8445
cat: DST/8445: No such file or directory
&lt;/code&gt;
&lt;/pre&gt;
&lt;h3&gt;Include any warning/errors/backtraces from the system logs&lt;/h3&gt;
&lt;pre&gt;
&lt;code&gt;# zpool status
  pool: tank
 state: ONLINE
  scan: scrub repaired 0B in 87h47m with 0 errors on Sat Mar 31 07:09:27 2018
config:

        NAME                        STATE     READ WRITE CKSUM
        tank                        ONLINE       0     0     0
          raidz1-0                  ONLINE       0     0     0
            wwn-0x5000c50085ac4c0f  ONLINE       0     0     0
            wwn-0x5000c50085acda77  ONLINE       0     0     0
            wwn-0x5000c500858db3d7  ONLINE       0     0     0
            wwn-0x5000c50085ac9887  ONLINE       0     0     0
            wwn-0x5000c50085aca6df  ONLINE       0     0     0
          raidz1-1                  ONLINE       0     0     0
            wwn-0x5000c500858db743  ONLINE       0     0     0
            wwn-0x5000c500858db347  ONLINE       0     0     0
            wwn-0x5000c500858db4a7  ONLINE       0     0     0
            wwn-0x5000c500858dbb0f  ONLINE       0     0     0
            wwn-0x5000c50085acaa97  ONLINE       0     0     0
          raidz1-2                  ONLINE       0     0     0
            wwn-0x5000c50085accb4b  ONLINE       0     0     0
            wwn-0x5000c50085acab9f  ONLINE       0     0     0
            wwn-0x5000c50085ace783  ONLINE       0     0     0
            wwn-0x5000c500858db67b  ONLINE       0     0     0
            wwn-0x5000c50085acb983  ONLINE       0     0     0
          raidz1-3                  ONLINE       0     0     0
            wwn-0x5000c50085ac4fd7  ONLINE       0     0     0
            wwn-0x5000c50085acb24b  ONLINE       0     0     0
            wwn-0x5000c50085ace13b  ONLINE       0     0     0
            wwn-0x5000c500858db43f  ONLINE       0     0     0
            wwn-0x5000c500858db61b  ONLINE       0     0     0
          raidz1-4                  ONLINE       0     0     0
            wwn-0x5000c500858dbbb7  ONLINE       0     0     0
            wwn-0x5000c50085acce7f  ONLINE       0     0     0
            wwn-0x5000c50085acd693  ONLINE       0     0     0
            wwn-0x5000c50085ac3d87  ONLINE       0     0     0
            wwn-0x5000c50085acc89b  ONLINE       0     0     0
          raidz1-5                  ONLINE       0     0     0
            wwn-0x5000c500858db28b  ONLINE       0     0     0
            wwn-0x5000c500858db68f  ONLINE       0     0     0
            wwn-0x5000c500858dbadf  ONLINE       0     0     0
            wwn-0x5000c500858db623  ONLINE       0     0     0
            wwn-0x5000c500858db48b  ONLINE       0     0     0
          raidz1-6                  ONLINE       0     0     0
            wwn-0x5000c500858db6ef  ONLINE       0     0     0
            wwn-0x5000c500858db39b  ONLINE       0     0     0
            wwn-0x5000c500858db47f  ONLINE       0     0     0
            wwn-0x5000c500858dbb23  ONLINE       0     0     0
            wwn-0x5000c500858db803  ONLINE       0     0     0
        logs
          zfs-slog                  ONLINE       0     0     0
        spares
          wwn-0x5000c500858db463    AVAIL   

errors: No known data errors
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;# zpool list
NAME   SIZE  ALLOC   FREE  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
tank   254T   159T  94.3T         -    27%    62%  1.00x  ONLINE  -
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;# zfs list -t all
NAME           USED  AVAIL  REFER  MOUNTPOINT
tank           127T  69.0T  11.5T  /mnt/tank
tank/jade      661G  69.0T   661G  /mnt/tank/jade
tank/simprod   115T  14.8T   115T  /mnt/tank/simprod
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;# zfs get all tank
NAME  PROPERTY              VALUE                  SOURCE
tank  type                  filesystem             -
tank  creation              Sat Jan 20 12:11 2018  -
tank  used                  127T                   -
tank  available             68.9T                  -
tank  referenced            11.6T                  -
tank  compressratio         1.00x                  -
tank  mounted               yes                    -
tank  quota                 none                   default
tank  reservation           none                   default
tank  recordsize            128K                   default
tank  mountpoint            /mnt/tank              local
tank  sharenfs              off                    default
tank  checksum              on                     default
tank  compression           off                    default
tank  atime                 off                    local
tank  devices               on                     default
tank  exec                  on                     default
tank  setuid                on                     default
tank  readonly              off                    default
tank  zoned                 off                    default
tank  snapdir               hidden                 default
tank  aclinherit            restricted             default
tank  createtxg             1                      -
tank  canmount              on                     default
tank  xattr                 sa                     local
tank  copies                1                      default
tank  version               5                      -
tank  utf8only              off                    -
tank  normalization         none                   -
tank  casesensitivity       sensitive              -
tank  vscan                 off                    default
tank  nbmand                off                    default
tank  sharesmb              off                    default
tank  refquota              none                   default
tank  refreservation        none                   default
tank  guid                  2271746520743372128    -
tank  primarycache          all                    default
tank  secondarycache        all                    default
tank  usedbysnapshots       0B                     -
tank  usedbydataset         11.6T                  -
tank  usedbychildren        116T                   -
tank  usedbyrefreservation  0B                     -
tank  logbias               latency                default
tank  dedup                 off                    default
tank  mlslabel              none                   default
tank  sync                  standard               default
tank  dnodesize             legacy                 default
tank  refcompressratio      1.00x                  -
tank  written               11.6T                  -
tank  logicalused           128T                   -
tank  logicalreferenced     11.6T                  -
tank  volmode               default                default
tank  filesystem_limit      none                   default
tank  snapshot_limit        none                   default
tank  filesystem_count      none                   default
tank  snapshot_count        none                   default
tank  snapdev               hidden                 default
tank  acltype               off                    default
tank  context               none                   default
tank  fscontext             none                   default
tank  defcontext            none                   default
tank  rootcontext           none                   default
tank  relatime              off                    default
tank  redundant_metadata    all                    default
tank  overlay               off                    default
&lt;/code&gt;
&lt;/pre&gt;
&lt;pre&gt;
&lt;code&gt;# zpool get all tank   
NAME  PROPERTY                       VALUE                          SOURCE
tank  size                           254T                           -
tank  capacity                       62%                            -
tank  altroot                        -                              default
tank  health                         ONLINE                         -
tank  guid                           7056741522691970971            -
tank  version                        -                              default
tank  bootfs                         -                              default
tank  delegation                     on                             default
tank  autoreplace                    on                             local
tank  cachefile                      -                              default
tank  failmode                       wait                           default
tank  listsnapshots                  off                            default
tank  autoexpand                     off                            default
tank  dedupditto                     0                              default
tank  dedupratio                     1.00x                          -
tank  free                           94.2T                          -
tank  allocated                      160T                           -
tank  readonly                       off                            -
tank  ashift                         0                              default
tank  comment                        -                              default
tank  expandsize                     -                              -
tank  freeing                        0                              -
tank  fragmentation                  27%                            -
tank  leaked                         0                              -
tank  multihost                      off                            default
tank  feature@async_destroy          enabled                        local
tank  feature@empty_bpobj            active                         local
tank  feature@lz4_compress           active                         local
tank  feature@multi_vdev_crash_dump  enabled                        local
tank  feature@spacemap_histogram     active                         local
tank  feature@enabled_txg            active                         local
tank  feature@hole_birth             active                         local
tank  feature@extensible_dataset     active                         local
tank  feature@embedded_data          active                         local
tank  feature@bookmarks              enabled                        local
tank  feature@filesystem_limits      enabled                        local
tank  feature@large_blocks           enabled                        local
tank  feature@large_dnode            enabled                        local
tank  feature@sha512                 enabled                        local
tank  feature@skein                  enabled                        local
tank  feature@edonr                  enabled                        local
tank  feature@userobj_accounting     active                         local
&lt;/code&gt;
&lt;/pre&gt;</description>
<pubDate>Mon, 09 Apr 2018 22:46:47 +0000</pubDate>
<dc:creator>heinrichhartman</dc:creator>
<og:image>https://avatars0.githubusercontent.com/u/22751545?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>Unlistable and disappearing files · Issue #7401 · zfsonlinux/zfs</og:title>
<og:url>https://github.com/zfsonlinux/zfs/issues/7401</og:url>
<og:description>System information Type Version/Name Distribution Name Scientific Linux Distribution Version 6.8 Linux Kernel 2.6.32-696.23.1.el6.x86_64 Architecture x86_64 ZFS Version 0.7.7 SPL ...</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/zfsonlinux/zfs/issues/7401</dc:identifier>
</item>
<item>
<title>Weirdstuff Warehouse is closed</title>
<link>http://weirdstuff.com/</link>
<guid isPermaLink="true" >http://weirdstuff.com/</guid>
<description>&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;/&gt;&lt;title&gt;Weirdstuff&lt;/title&gt;&lt;meta name=&quot;description&quot; content=&quot;Weirdstuff Landing Page&quot;/&gt;&lt;meta name=&quot;author&quot; content=&quot;Sweet-Geek&quot;/&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;css/styles.css&quot; type=&quot;text/css&quot;/&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;41.726174496644&quot;&gt;
&lt;header&gt;&lt;img src=&quot;http://weirdstuff.com/images/newbanner.png&quot; usemap=&quot;#map&quot;/&gt;&lt;/header&gt;
&lt;section readability=&quot;36.036241610738&quot;&gt;&lt;h2&gt;Weirdstuff Warehouse is closed.&lt;/h2&gt;
&lt;p&gt;Customers looking to purchase any goods typically found at Weirdstuff Warehouse, please contact &lt;a href=&quot;http://www.outbackequipment.com/&quot;&gt;Outback Equipment Company&lt;/a&gt; at (408) 886-3751 as we sold all of our inventory to them on April 9, 2018.&lt;/p&gt;
&lt;p&gt;Customers wanting to sell excess inventories or who want to schedule a pickup of surplus materials can also contact &lt;a href=&quot;http://www.outbackequipment.com/&quot;&gt;Outback Equipment Company.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Customers who purchased items from Weirdstuff Warehouse prior to April 9 will have any warranties honored by Weirdstuff, Inc. contact us at (408) 743-5651.&lt;/p&gt;
&lt;h3&gt;Domain and Trademark for Sale&lt;/h3&gt;
&lt;p&gt;Our Corporation, Weirdstuff, Inc. Is looking for a buyer for our registered trademark (serial no. 74037074) “Weird Stuff” and a suite of domain names. We have 11 registered internet domains. The main domain is “weirdstuff.com”. It is supported by 10 redirected domains: weirdstuff.net, weirdstuff.biz, weirdstuff.org, weirdstuff.asia, weirdstuff.co. And because some people like to put the “i” before the “e” we also own and redirect wierdstuff.com, wierdstuff.net, wierdstuff.biz, wierdstuff.asia, and wierdstuff.org.&lt;/p&gt;
&lt;p&gt;We expect that the Weirdstuff trademark and domains, which have served us well for many years, could easily be repurposed by any entity that would like to mass market goods on the internet. Weirdstuff, as a name, is easy to remember, only 10 characters long and easy to pass on via word of mouth.&lt;/p&gt;
&lt;p&gt;If interested in the trademark or domains please contact us via email at &lt;a href=&quot;mailto:weirdstuffdomain@gmail.com&quot;&gt;weirdstuffdomain@gmail.com&lt;/a&gt;&lt;/p&gt;
&lt;/section&gt;&lt;/body&gt;</description>
<pubDate>Mon, 09 Apr 2018 21:47:51 +0000</pubDate>
<dc:creator>kevbin</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://weirdstuff.com/</dc:identifier>
</item>
<item>
<title>Qusim.py – A toy multi-qubit quantum computer simulator written in Python</title>
<link>https://github.com/adamisntdead/QuSimPy</link>
<guid isPermaLink="true" >https://github.com/adamisntdead/QuSimPy</guid>
<description>&lt;h3&gt;README.md&lt;/h3&gt;
&lt;article class=&quot;markdown-body entry-content&quot; itemprop=&quot;text&quot;&gt;
&lt;p&gt;Qusim.py is a toy multi-qubit quantum computer simulator, written in 150 lines of python&lt;/p&gt;
&lt;p&gt;This code makes it easy for you to see how a quantum computer computes by following the linear algebra!&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-python&quot;&gt;
&lt;pre&gt;
&lt;span class=&quot;pl-k&quot;&gt;from&lt;/span&gt; QuSim &lt;span class=&quot;pl-k&quot;&gt;import&lt;/span&gt; QuantumRegister

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;############################################&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;                 Introduction              #&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;############################################&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Here Will Be A Few Example of Different   #&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Quantum States / Algorithms, So You Can   #&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Get A Feel For How The Module Works, and  #&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Some Algorithmic Ideas                    #&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;############################################&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;############################################&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;            Quantum Measurement              #&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;############################################&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; This experiment will prepare 2 states, of a&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Single qubit, and of 5 qubits, and will just&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Measure them&lt;/span&gt;

OneQubit &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; QuantumRegister(&lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;)  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; New Quantum Register of 1 Qubit&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;One Qubit: &lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;+&lt;/span&gt; OneQubit.measure())  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Should Print 'One Qubit: 0'&lt;/span&gt;

FiveQubits &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; QuantumRegister(&lt;span class=&quot;pl-c1&quot;&gt;5&lt;/span&gt;)  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; New Quantum Register of 5 Qubits&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Should Print 'Five Qubits: 00000'&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;Five Qubits: &lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;+&lt;/span&gt; FiveQubits.measure())

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;############################################&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;                 Swap 2 Qubits             #&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;############################################&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Here, We Will Apply a Pauli-X Gate / NOT Gate&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; To the first qubit, and then after the algorithm,&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; it will be swapped to the second qubit.&lt;/span&gt;

Swap &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; QuantumRegister(&lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; New Quantum Register of 2 qubits&lt;/span&gt;
Swap.applyGate(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;X&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;)  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Apply The NOT Gate. If Measured Now, it should be 10&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Start the swap algorithm&lt;/span&gt;
Swap.applyGate(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;CNOT&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)
Swap.applyGate(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;H&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;)
Swap.applyGate(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;H&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)
Swap.applyGate(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;CNOT&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)
Swap.applyGate(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;H&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;)
Swap.applyGate(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;H&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)
Swap.applyGate(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;CNOT&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; End the swap algorithm&lt;/span&gt;

&lt;span class=&quot;pl-c1&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;SWAP: |&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;+&lt;/span&gt; Swap.measure() &lt;span class=&quot;pl-k&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&amp;gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;)  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Measure the State, Should be 01&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;############################################&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;               Fair Coin Flip              #&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;############################################&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Shown in this 'Experiment', is a so called 'Fair Coin Flip',&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Where a state will be prepared, that has an equal chance of&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Flipping to Each Possible State. to do this, the Hadamard&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Gate will be used.&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; New Quantum Register of 1 Qubit (As a coin has only 2 states)&lt;/span&gt;
FairCoinFlip &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; QuantumRegister(&lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;)
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; If measured at this point, it should be |0&amp;gt;&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Apply the hadamard gate, now theres an even chance of measuring 0 or 1&lt;/span&gt;
FairCoinFlip.applyGate(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;H&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;)

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Now, the state will be measured, flipping the state to&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; either 0 or 1. If its 0, we will say &quot;Heads&quot;, or if its&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; 1, we will say &quot;Tails&quot;&lt;/span&gt;
FairCoinFlipAnswer &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; FairCoinFlip.measure()  &lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Now its flipped, so we can test&lt;/span&gt;
&lt;span class=&quot;pl-k&quot;&gt;if&lt;/span&gt; FairCoinFlipAnswer &lt;span class=&quot;pl-k&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;0&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;:
    &lt;span class=&quot;pl-c1&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;FairCoinFlip: Heads&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;span class=&quot;pl-k&quot;&gt;elif&lt;/span&gt; FairCoinFlipAnswer &lt;span class=&quot;pl-k&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;1&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;:
    &lt;span class=&quot;pl-c1&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;FairCoinFlip: Tails&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;)

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;############################################&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;             CNOT Gate                     #&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt;############################################&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; In this experiment, 4 states will be prepared, {00, 01, 10, 11}&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; And then the same CNOT Gate will be run on them,&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; To Show The Effects of the CNOT. The Target Qubit will be 2, and the control 1&lt;/span&gt;

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; New Quantum Register of 2 Qubits, done 4 times.&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; If any are measured at this time, the result will be 00&lt;/span&gt;
ZeroZero &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; QuantumRegister(&lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)
ZeroOne &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; QuantumRegister(&lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)
OneZero &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; QuantumRegister(&lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)
OneOne &lt;span class=&quot;pl-k&quot;&gt;=&lt;/span&gt; QuantumRegister(&lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Now prepare Each Into The State Based On Their Name&lt;/span&gt;
&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; ZeroZero Will be left, as thats the first state anyway&lt;/span&gt;
ZeroOne.applyGate(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;X&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)
OneZero.applyGate(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;X&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;)
OneOne.applyGate(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;X&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;)
OneOne.applyGate(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;X&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Now, a CNOT Will Be Applied To Each.&lt;/span&gt;
ZeroZero.applyGate(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;CNOT&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)
ZeroOne.applyGate(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;CNOT&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)
OneZero.applyGate(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;CNOT&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)
OneOne.applyGate(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;CNOT&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;pl-c1&quot;&gt;2&lt;/span&gt;)

&lt;span class=&quot;pl-c&quot;&gt;&lt;span class=&quot;pl-c&quot;&gt;#&lt;/span&gt; Print the results.&lt;/span&gt;
&lt;span class=&quot;pl-c1&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;CNOT on 00: |&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;+&lt;/span&gt; ZeroZero.measure() &lt;span class=&quot;pl-k&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&amp;gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;span class=&quot;pl-c1&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;CNOT on 01: |&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;+&lt;/span&gt; ZeroOne.measure() &lt;span class=&quot;pl-k&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&amp;gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;span class=&quot;pl-c1&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;CNOT on 10: |&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;+&lt;/span&gt; OneZero.measure() &lt;span class=&quot;pl-k&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&amp;gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;span class=&quot;pl-c1&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;CNOT on 11: |&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;pl-k&quot;&gt;+&lt;/span&gt; OneOne.measure() &lt;span class=&quot;pl-k&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;pl-s&quot;&gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&amp;gt;&lt;span class=&quot;pl-pds&quot;&gt;'&lt;/span&gt;&lt;/span&gt;)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Largely based on the code from &lt;a href=&quot;https://github.com/corbett/QuantumComputing&quot;&gt;corbett/QuantumComputing&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you are interested in a efficient, high performance, hardware accelerated quantum computer simulator written in Rust, please check out &lt;a href=&quot;https://github.com/qcgpu/qcgpu-rust&quot;&gt;QCGPU&lt;/a&gt;&lt;/p&gt;
&lt;/article&gt;</description>
<pubDate>Mon, 09 Apr 2018 21:32:53 +0000</pubDate>
<dc:creator>adamisntdead</dc:creator>
<og:image>https://avatars1.githubusercontent.com/u/17102600?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>adamisntdead/QuSimPy</og:title>
<og:url>https://github.com/adamisntdead/QuSimPy</og:url>
<og:description>QuSimPy - A Multi-Qubit Ideal Quantum Computer Simulator</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/adamisntdead/QuSimPy</dc:identifier>
</item>
<item>
<title>AV1: A new general-purpose video codec</title>
<link>https://people.xiph.org/~xiphmont/demo/av1/demo1.shtml</link>
<guid isPermaLink="true" >https://people.xiph.org/~xiphmont/demo/av1/demo1.shtml</guid>
<description>&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;&lt;link rel=&quot;icon&quot; href=&quot;http://www.xiph.org/images/logos/xiph.ico&quot; type=&quot;image/x-icon&quot; /&gt;&lt;link rel=&quot;stylesheet&quot; title=&quot;default demosheet&quot; media=&quot;screen&quot; href=&quot;demo.css&quot; type=&quot;text/css&quot; /&gt;&lt;title&gt;next generation video: Introducing AV1&lt;/title&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;366.6769533647&quot;&gt;
&lt;div id=&quot;xiphlogo&quot;&gt;&lt;a href=&quot;https://www.xiph.org/&quot;&gt;&lt;img src=&quot;https://www.xiph.org/images/logos/fish_xiph_org.png&quot; alt=&quot;Fish Logo and Xiph.org&quot; /&gt;&lt;/a&gt;


&lt;/div&gt;

&lt;div&gt;&lt;img src=&quot;https://people.xiph.org/~xiphmont/demo/av1/whiteboard-smaller.jpg&quot; /&gt;&lt;div readability=&quot;9&quot;&gt;
&lt;div readability=&quot;13&quot;&gt;&lt;img src=&quot;https://people.xiph.org/~xiphmont/demo/av1/av1-logo.png&quot; alt=&quot;AV1&quot; /&gt;&lt;p&gt;AV1 is a new general-purpose video codec developed by the Alliance for Open Media. The alliance began development of this new codec using Google's VPX codecs, Cisco's Thor codec, and Mozilla's/Xiph.Org's Daala codec as starting point. AV1 leapfrogs the performance of VP9 and HEVC, making it a next-next-generation codec. The AV1 format is and will always be royalty-free with a permissive FOSS license.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Those of you who &lt;a href=&quot;https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml&quot;&gt;followed Daala development&lt;/a&gt; know that when the &lt;a href=&quot;https://aomedia.org/&quot;&gt;Alliance for Open Media&lt;/a&gt; formed, Xiph and Mozilla submitted our Daala codec as one of the inputs toward standardization. Along with Daala, Google submitted its VP9 codec, and Cisco submitted Thor. The idea was to build a new codec out of pieces of all three, along with any other useful research we found. I hadn't written any new demo pages about new tech in Daala or AV1 since then; for a long time we had little idea of what the final codec was going to look like.&lt;/p&gt;
&lt;p&gt;About two years ago, AOM voted to base the fundamental structure of the new codec on VP9, rather than Daala or Thor. AOM member companies wanted the shortest path to shipping a useful codec without royalty or licensing strings, and VP9 was decided to be the lowest-technical-risk choice. I agree with that choice; Daala was a contender, but I also think both the lapping approach and frequency-domain techniques required by Daala weren't (and still aren't) mature enough for deployment. There were still technical unknowns in Daala, and choosing VP9 as a starting point avoided most of them.&lt;/p&gt;
&lt;p&gt;As a result of starting with VP9, AV1 (the AOM Video Codec 1) is a mostly familiar codec built along traditional block-based transform coding lines. Of course, it also includes new exciting things, several of which are taken from Daala! Now that we're rapidly approaching the final spec freeze, it's time to write more of the long-delayed codec technology demos in the context of AV1.&lt;/p&gt;
&lt;h2&gt;An Updated look at Chroma from Luma Prediction (CfL)&lt;/h2&gt;
&lt;p&gt;Chroma from Luma prediction (CfL for short) is one of the new prediction techniques adopted by AV1. As the name implies, it predicts the colors in an image (chroma) based on brightness values (luma). Luma values are coded and decoded first, and then CfL makes an educated prediction of the colors. When the guess is good, this reduces the amount of color information to be coded, saving space.&lt;/p&gt;
&lt;p&gt;CfL is not actually &lt;em&gt;brand new&lt;/em&gt; in AV1. &lt;a href=&quot;http://dx.doi.org/10.1109/ICIP.2009.5413727&quot;&gt;The seminal CfL paper&lt;/a&gt; dates from 2009, and LG and Samsung jointly propsed an &lt;a href=&quot;https://people.xiph.org/~xiphmont/demo/av1/phenix.int-evry.fr/jct/doc_end_user/documents/5_Geneva/wg11/JCTVC-E266-v4.zip&quot;&gt;early implementation of CfL named LM Mode&lt;/a&gt; that was rejected during the design of HEVC. You'll remember I wrote about &lt;a href=&quot;https://people.xiph.org/~xiphmont/demo/daala/demo4.shtml&quot;&gt;the particularly advanced version of CfL used in the the Daala codec&lt;/a&gt;. Cisco's Thor codec also had a CfL technique similar to LM Mode, and HEVC eventually added an improved version called Cross-Channel Prediction (CCP) via the HEVC Range Extension.&lt;/p&gt;
&lt;table class=&quot;caption&quot;&gt;&lt;tr&gt;&lt;th&gt;&lt;/th&gt;
&lt;th&gt;LM Mode&lt;/th&gt;
&lt;th&gt;Thor CfL&lt;/th&gt;
&lt;th&gt;Daala CfL&lt;/th&gt;
&lt;th&gt;HEVC CCP&lt;/th&gt;
&lt;th&gt;AV1 CfL&lt;/th&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;Prediction Domain&lt;/th&gt;
&lt;td&gt;spatial&lt;/td&gt;
&lt;td&gt;spatial&lt;/td&gt;
&lt;td&gt;frequency&lt;/td&gt;
&lt;td&gt;spatial&lt;/td&gt;
&lt;td class=&quot;highlight&quot;&gt;spatial&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;Coding&lt;/th&gt;
&lt;td&gt;none&lt;/td&gt;
&lt;td&gt;none&lt;/td&gt;
&lt;td&gt;sign bit&lt;/td&gt;
&lt;td&gt;index + signs&lt;/td&gt;
&lt;td class=&quot;highlight&quot;&gt;joint sign + index&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;Activation Mechanism&lt;/th&gt;
&lt;td&gt;LM_MODE&lt;/td&gt;
&lt;td&gt;threshold&lt;/td&gt;
&lt;td&gt;signal&lt;/td&gt;
&lt;td&gt;binary flag&lt;/td&gt;
&lt;td class=&quot;highlight&quot;&gt;CFL_PRED&lt;br /&gt;(uv-only mode)&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;Requires PVQ&lt;/th&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td class=&quot;highlight&quot;&gt;no&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;Decoder modeling?&lt;/th&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td class=&quot;highlight&quot;&gt;no&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;div class=&quot;caption&quot; readability=&quot;26&quot;&gt;
&lt;p&gt;Above: A summary of the characteristics of various Chroma from Luma (CfL)implementations.&lt;/p&gt;
&lt;p&gt;LG's LM Mode and Thor were similar in that the encoder and decoder both run an identical prediction model in parallel, and do not need to code any parameters. Unfortunately, this parallel/implicit model reduces fit accuracy and increases decoder complexity.&lt;/p&gt;
&lt;p&gt;Unlike the others, Daala's CfL worked in the frequency domain. It signaled only an activation and sign bit, with the other parameter information already implicitly encoded via PVQ.&lt;/p&gt;
&lt;p&gt;The final AV1 CfL implementation builds on the Daala implementation, borrowing model ideas from Thor and improving on both through additional new research. It avoids any complexity increase in the decoder, implements a model search that also reduces encoder complexity over its predecessors, and notably improves the encoded model fit and accuracy.&lt;/p&gt;
&lt;/div&gt;
&lt;h3&gt;The need for better intra prediction&lt;/h3&gt;
&lt;p&gt;At a fundamental level, compression is the art of prediction. Until the last generation or so, video compression focused primarily on &lt;strong&gt;inter-frame&lt;/strong&gt; (or just &lt;strong&gt;inter&lt;/strong&gt;) prediction, that is, coding a frame as a set of changes from other frames. Frames that use inter-frame prediction are themselves collectively referred to as &lt;strong&gt;inter frames&lt;/strong&gt;. Inter-frame prediction has become fantastically powerful over the past few decades.&lt;/p&gt;
&lt;p&gt;Despite the power of inter-prediction, we still need occasional standalone &lt;strong&gt;keyframes&lt;/strong&gt;. Keyframes, by definition, do not rely on information from any other frames; as a result, they can only use &lt;strong&gt;intra-frame&lt;/strong&gt; (or just &lt;strong&gt;intra&lt;/strong&gt;) prediction that works entirely &lt;em&gt;within&lt;/em&gt; a frame. Because keyframes can only use intra-prediction, they are also often referred to as &lt;strong&gt;intra frames&lt;/strong&gt;. Intra/Keyframes make it possible to seek in a video, otherwise we'd always have to start playing at and only at the very beginning&lt;a href=&quot;https://people.xiph.org/~xiphmont/demo/av1/demo1.shtml#rolling-intra&quot;&gt;*&lt;/a&gt;. &lt;img class=&quot;caption&quot; src=&quot;https://people.xiph.org/~xiphmont/demo/av1/framebits.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Above: Bit usage histogram of the first sixty frames of a test video, beginning with a keyframe. In this clip, the keyframe is 20-30 times the size of the subsequent inter frames. In a low-motion or mostly static video, a keyframe can be hundreds of times as large as an inter frame.&lt;/p&gt;
&lt;p&gt;Compared to inter frames, keyframes are enormously large, so they tend to be used as seldom as possible and spaced widely apart. Despite this, as inter frames have gotten smaller and smaller, keyframes have taken up an increasingly large proportion of a bitstream. As a result, video codec research has looked for newer, more-powerful forms of intra-prediction to shrink keyframe size. And, despite their name, inter frames can also use intra-prediction techniques in those cases where it's beneficial.&lt;/p&gt;
&lt;p&gt;Improved intra-prediction is a double win!&lt;/p&gt;
&lt;p&gt;Chroma from Luma works entirely from luma blocks within a frame, and thus is an intra-prediction technique.&lt;/p&gt;
&lt;h3&gt;Energy is Power&lt;a href=&quot;https://people.xiph.org/~xiphmont/demo/av1/demo1.shtml#xtime&quot;&gt;†&lt;/a&gt;&lt;br /&gt;Correlated Energy is Information&lt;/h3&gt;
&lt;p&gt;What makes us think we can predict color based on brightness?&lt;/p&gt;
&lt;p&gt;Most video representations reduce channel correlation by using a YUV-like color space. Y is the luma channel, the grayscale version of the video signal made by adding together weighted versions of the original red, green and blue signals. The chroma channels, U and V, subtract the luma signal from blue, and the luma signal from red respectively. YUV is simple and substantially reduces coding redundancy across channels.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;caption&quot; src=&quot;https://people.xiph.org/~xiphmont/demo/av1/persimmon-yuv.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Above: decomposition of an image (far left) into a YUV, or more correctly, bt.601 Y'CbCr colorspace. The middle left image displays the luma channel and the two right images show the chroma channels. There is less cross channel redundancy in YUV than in an RGB image, but features from the original image are still plainly visible in all three channels of the YUV decomposition; all three channels still have edges in the same places.&lt;/p&gt;
&lt;p&gt;And yet, it's obvious looking at YUV decompositions of frames that edges in the luma and chroma planes still happen in the same places. There's remaining correlation that we can exploit to reduce bitrate; let's try to reuse some more of that luma data to predict color.&lt;/p&gt;
&lt;h3&gt;Getting Out The Crayons&lt;/h3&gt;
&lt;p&gt;Chroma from Luma prediction is, at its heart, the process of colorizing a monochrome image based on educated guessing. It's not unlike taking an old black-and-white photo, some colored pencils, and getting to work coloring in the photo. Of course, CfL's color predictions must be accurate to be useful; they can't be &lt;em&gt;wild&lt;/em&gt; guesses.&lt;/p&gt;
&lt;p&gt;This work is made easier by the fact that modern video codecs break an image down into a hierarchy of smaller units, doing most of the encoding work on these units independently.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;caption&quot; src=&quot;https://people.xiph.org/~xiphmont/demo/av1/demo1-split.jpg&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Above: The AV1 encoder splits this frame into individual prediction units to maximize encoding fidelity, but just as importantly, to break the frame into smaller chunks that are easier to analyze and allow the codec to adjust prediction as it proceeds through the image.&lt;/p&gt;
&lt;p&gt;A model that predicts color across the entire image at once would be unwieldy, complex, and error-prone, but we don't need to predict the entire image. Because the encoder is working with small chunks at a time, we only need to look at correlations over small areas, and over these small areas, we can predict color from brightness with a high degree of accuracy using a fairly simple model. Consider the small portion of the image below, highlighted in red:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;caption&quot; src=&quot;https://people.xiph.org/~xiphmont/demo/av1/snowbird.jpg&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Above: A single block highlighted from a single frame of video, illustrating that localizing chroma prediction into small blocks can be an effective means of simplifying prediction needs.&lt;/p&gt;
&lt;p&gt;Over this small range for this example, the correct 'rule' for coloring the image is simple: Brighter areas are green, and color desaturates along with brightness down to black. Most blocks will have similarly simple coloration rules. We can get as fancy as we like, but simple also works very well, so let's start with simple and fit the data to a simple αx+β line:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;caption&quot; src=&quot;https://people.xiph.org/~xiphmont/demo/av1/snowbird2.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Above: Cb and Cr (U and V) values plotted versus luma (Y) for pixels in the highlighted block from the previous picture. A quantized and encoded straight-line model fit is superimposed over the scatterplot as a line. Note that a fit consists of two lines; in this example, the lines are superimposed.&lt;/p&gt;
&lt;p&gt;Well, OK, it's two lines-- one for the U channel (Blue difference, Cb) and one for the V channel (Red difference, Cr). In other words, where &lt;tt&gt;L&lt;span class=&quot;super&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;sub&quot;&gt;ij&lt;/span&gt;&lt;/tt&gt; &lt;span class=&quot;sub&quot;&gt;are the reconstructed luma values, we compute the chroma values as follows:&lt;/span&gt;&lt;/p&gt;
&lt;table&gt;&lt;tr&gt;&lt;td&gt;&lt;tt&gt;U&lt;span class=&quot;sub&quot;&gt;ij&lt;/span&gt;&lt;/tt&gt;&lt;/td&gt;
&lt;td&gt;&lt;tt&gt; = &lt;/tt&gt;&lt;/td&gt;
&lt;td&gt;&lt;tt&gt;α&lt;span class=&quot;sub&quot;&gt;U&lt;/span&gt;L&lt;span class=&quot;super&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;sub&quot;&gt;ij&lt;/span&gt; + &lt;/tt&gt;&lt;/td&gt;
&lt;td&gt;&lt;tt&gt;β&lt;span class=&quot;sub&quot;&gt;U&lt;/span&gt;&lt;/tt&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;tt&gt;V&lt;span class=&quot;sub&quot;&gt;ij&lt;/span&gt;&lt;/tt&gt;&lt;/td&gt;
&lt;td&gt;&lt;tt&gt; = &lt;/tt&gt;&lt;/td&gt;
&lt;td&gt;&lt;tt&gt;α&lt;span class=&quot;sub&quot;&gt;V&lt;/span&gt;L&lt;span class=&quot;super&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;sub&quot;&gt;ij&lt;/span&gt; + &lt;/tt&gt;&lt;/td&gt;
&lt;td&gt;&lt;tt&gt;β&lt;span class=&quot;sub&quot;&gt;V&lt;/span&gt;&lt;/tt&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;What do these parameters look like? The &lt;em&gt;α&lt;/em&gt;s select a specific hue (and anti-hue) from a 2D plane of choices that will be scaled/applied according to the luma:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;caption&quot; src=&quot;https://people.xiph.org/~xiphmont/demo/av1/alphacbcr.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Above: CfL's &lt;em&gt;α&lt;/em&gt; parameters select a hue for block colorization from a 2D color plane.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;β&lt;/em&gt;s alter the zero-crossing point of the color scale, that is, they're the knobs that shift the minimum and maximum levels of colorization applied. Note that &lt;em&gt;β&lt;/em&gt; allows us to apply &lt;em&gt;negative&lt;/em&gt; color as well; that gives us the opposite of the hue selected by &lt;em&gt;α&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Our task now boils down to choosing the correct &lt;em&gt;α&lt;/em&gt;s and &lt;em&gt;β&lt;/em&gt;s, and then encoding them. Here's one straightforward implicit approach from &lt;a href=&quot;https://arxiv.org/abs/1711.03951&quot;&gt;Predicting Chroma from Luma in AV1&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;caption&quot; src=&quot;https://people.xiph.org/~xiphmont/demo/av1/alphabeta.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;That looks scarier that it is. In English: Perform a least squares fit of the chroma values versus the reconstructed luma values to find &lt;em&gt;α&lt;/em&gt;, then use &lt;em&gt;α&lt;/em&gt; to solve for the chroma offset &lt;em&gt;β&lt;/em&gt;. At least, this is one possible way to handle the fit, and it's often used in CfL implementations (such as LM Mode and Thor) that do not signal either &lt;em&gt;α&lt;/em&gt; or &lt;em&gt;β&lt;/em&gt;. In this case, the fit is made using already decoded chroma values of neighboring pixels that have already been fully decoded.&lt;/p&gt;
&lt;h3&gt;Chroma from Luma in Daala&lt;/h3&gt;
&lt;p&gt;Daala performs all prediction in the frequency domain, CfL included, providing a prediction vector as one of the inputs to &lt;a href=&quot;https://people.xiph.org/~jm/daala/pvq_demo/&quot;&gt;PVQ encoding&lt;/a&gt;. PVQ is a gain/shape encoding; the luma PVQ vector encodes the location of the shapes and edges in luma, and we simply re-use it as a predictor of the shapes and edges in chroma.&lt;/p&gt;
&lt;p&gt;Daala does not need to encode an &lt;em&gt;α&lt;/em&gt; value, as that's subsumed into the PVQ gain (except for the sign). Nor does Daala need to encode a &lt;em&gt;β&lt;/em&gt; value; because Daala applies CfL only to the AC chroma coefficients, &lt;em&gt;β&lt;/em&gt; is always zero. This reinforces an insight: &lt;em&gt;β&lt;/em&gt; is conceptually just the chroma values' DC-offset.&lt;/p&gt;
&lt;p&gt;In effect, because Daala uses PVQ to encode transform blocks, it gets CfL almost for free, both in terms of bit cost and computational cost in the encoder as well as the decoder.&lt;/p&gt;
&lt;h3&gt;Chroma from Luma in AV1&lt;/h3&gt;
&lt;p&gt;AV1 did not adopt PVQ, so the cost of CfL is approximately equal whether CfL is computed in the pixel or frequency domain; there's no longer a special bonus to working in frequency. In addition, &lt;a href=&quot;https://people.xiph.org/~xiphmont/demo/daala/demo3.shtml&quot;&gt;TF (Time-Frequency resolution switching)&lt;/a&gt;, which Daala uses to glue the smallest luma blocks together to make subsampled chroma blocks large enough, currently only works with the DCT and Walsh-Hadamard transforms. As AV1 also uses the discrete sine transform and a pixel domain identity transform, we can't easily perform AV1 CfL in the frequency domain, at least when we use subsampled chroma.&lt;/p&gt;
&lt;p&gt;But unlike Daala, AV1 doesn't &lt;em&gt;need&lt;/em&gt; to do CfL in the frequency domain. So, we move Daala's frequency-domain CfL back into the pixel domain for AV1. One of the neat things about CfL is that the basic equations work the same way in both domains.&lt;/p&gt;
&lt;p&gt;CfL in AV1 must keep reconstruction complexity to a minimum. For this reason, we explicitly code &lt;em&gt;α&lt;/em&gt; so that there is no expensive least-squares fitting in the decoder. The bit cost of explicitly coding &lt;em&gt;α&lt;/em&gt; is more than outweighed by the additional accuracy gained by computing it using the current block's chroma pixels as opposed to neighboring reconstructed chroma pixels.&lt;/p&gt;
&lt;p&gt;Next, we optimize the fitting complexity on the encoder-side. In Daala, which operates in the frequency domain, we perform CfL using only luma's AC coefficients. AV1 performs CfL fitting in the pixel domain, but we can subtract the average (that is, that already-computed DC value) from each pixel, rendering the pixel values zero-mean and equivalent to the AC coefficient contribution as in Daala. Zero-mean luma values cancel out a substantial portion of the least-squares equation, greatly reducing the computational overhead:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;caption&quot; src=&quot;https://people.xiph.org/~xiphmont/demo/av1/alphabeta2.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;There's more we can do. Remembering that &lt;em&gt;β&lt;/em&gt; is simply chroma's DC-offset, we realize that the encoder and decoder already perform DC-prediction for the chroma planes as it's needed for other prediction modes. Of course, a predicted DC value is not going to be as accurate as an explicitly coded DC/&lt;em&gt;β&lt;/em&gt; value, but testing shows that it's still quite good:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;caption&quot; src=&quot;https://people.xiph.org/~xiphmont/demo/av1/DCbeta.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Above: Error analysis of using the default DC predictor value calculated using neighboring pixels, as opposed to coding an explicit &lt;em&gt;β&lt;/em&gt; value calculated from pixels in the current block.&lt;/p&gt;
&lt;p&gt;As a result, we simply use the pre-existing chroma DC prediction instead of &lt;em&gt;β&lt;/em&gt;. This not only means we don't need to explicitly code &lt;em&gt;β&lt;/em&gt;, it also means we do not need to explicitly compute &lt;em&gt;β&lt;/em&gt; from &lt;em&gt;α&lt;/em&gt; in either the decoder &lt;em&gt;or&lt;/em&gt; encoder. Thus, our final CfL prediction equation becomes:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;caption&quot; src=&quot;https://people.xiph.org/~xiphmont/demo/av1/av1cfl.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;In those cases where prediction alone isn't accurate enough, we encode a transform-domain residual. And, of course, when the prediction isn't good enough to save any bits at all, we simply don't have to use it.&lt;/p&gt;
&lt;h3&gt;Results&lt;/h3&gt;
&lt;p&gt;CfL gains are, like any other prediction technique, dependent on the test. AOM uses a number of &lt;a href=&quot;https://media.xiph.org/video/derf/&quot;&gt;standardized test sets hosted at Xiph.Org&lt;/a&gt;, and made available through the automated &lt;a href=&quot;https://arewecompressedyet.com/&quot;&gt;'Are We Compressed Yet?'&lt;/a&gt; testing tool.&lt;/p&gt;
&lt;p&gt;CfL is an intra-prediction technique, and to best isolate its usefulness in intra-coding, we can look at its performance on keyframes using the 'subset-1' image test set:&lt;/p&gt;
&lt;table class=&quot;textable&quot;&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;
&lt;td colspan=&quot;7&quot; class=&quot;title&quot;&gt;BD-rate&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;
&lt;td&gt;PSNR&lt;/td&gt;
&lt;td&gt;PSNR-HVS&lt;/td&gt;
&lt;td&gt;SSIM&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;CIEDE2000&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;PSNR Cb&lt;/td&gt;
&lt;td&gt;PSNR Cr&lt;/td&gt;
&lt;td&gt;MS SSIM&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;result&quot;&gt;&lt;td&gt;Average&lt;/td&gt;
&lt;td&gt;-0.53&lt;/td&gt;
&lt;td&gt;-0.31&lt;/td&gt;
&lt;td&gt;-0.34&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;-4.87&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;-12.87&lt;/td&gt;
&lt;td&gt;-10.75&lt;/td&gt;
&lt;td&gt;-0.34&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;Most of the metrics here are not color-sensitive, they're simply included because they're always included and it's nice to see CfL doesn't &lt;em&gt;damage&lt;/em&gt; them. Of course, it shouldn't; by making color coding more efficient, it is also freeing up bits that can be used to better represent luma as well.&lt;/p&gt;
&lt;p&gt;That said, CIE delta-E 2000 is the metric to pay attention to; it implements a perceptually-uniform color error metric. We see that CfL saves nearly 5% in bitrate when both luma and chroma are considered! That's a stunning number for a single prediction technique.&lt;/p&gt;
&lt;p&gt;CfL is available for intra-blocks within inter-frames as well. During AV1 development, the objective-1-fast set was the standard test set for metric evaluation of motion sequences:&lt;/p&gt;
&lt;table class=&quot;textable&quot;&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;
&lt;td colspan=&quot;7&quot; class=&quot;title&quot;&gt;BD-rate&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;
&lt;td&gt;PSNR&lt;/td&gt;
&lt;td&gt;PSNR-HVS&lt;/td&gt;
&lt;td&gt;SSIM&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;CIEDE2000&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;PSNR Cb&lt;/td&gt;
&lt;td&gt;PSNR Cr&lt;/td&gt;
&lt;td&gt;MS SSIM&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;result&quot;&gt;&lt;td&gt;Average&lt;/td&gt;
&lt;td&gt;-0.43&lt;/td&gt;
&lt;td&gt;-0.42&lt;/td&gt;
&lt;td&gt;-0.38&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;-2.41&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;-5.85&lt;/td&gt;
&lt;td&gt;-5.51&lt;/td&gt;
&lt;td&gt;-0.40&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;1080p&lt;/td&gt;
&lt;td&gt;-0.32&lt;/td&gt;
&lt;td&gt;-0.37&lt;/td&gt;
&lt;td&gt;-0.28&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;-2.52&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;-6.80&lt;/td&gt;
&lt;td&gt;-5.31&lt;/td&gt;
&lt;td&gt;-0.31&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;1080p-screen&lt;/td&gt;
&lt;td&gt;-1.82&lt;/td&gt;
&lt;td&gt;-1.72&lt;/td&gt;
&lt;td&gt;-1.71&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;-8.22&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;-17.76&lt;/td&gt;
&lt;td&gt;-12.00&lt;/td&gt;
&lt;td&gt;-1.75&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;720p&lt;/td&gt;
&lt;td&gt;-0.12&lt;/td&gt;
&lt;td&gt;-0.11&lt;/td&gt;
&lt;td&gt;-0.07&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;-0.52&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;-1.08&lt;/td&gt;
&lt;td&gt;-1.23&lt;/td&gt;
&lt;td&gt;-0.12&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;360p&lt;/td&gt;
&lt;td&gt;-0.15&lt;/td&gt;
&lt;td&gt;-0.05&lt;/td&gt;
&lt;td&gt;-0.10&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;-0.80&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;-2.17&lt;/td&gt;
&lt;td&gt;-6.45&lt;/td&gt;
&lt;td&gt;-0.04&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;As expected, we still see solid gains, though CfL's contribution is watered down somewhat by the preponderance of inter-prediction in use. Intra blocks are used primarily in keyframes, each of these test sequences coded only a single keyframe, and intra-coding is not used often in inter-frames.&lt;/p&gt;
&lt;p&gt;The big exception is '1080p-screen' content where we see a whopping 8% rate reduction. This makes sense; most screencasting content is fairly static, and where areas change they are almost always wholesale updates suited to intra coding rather than the smooth motion suited to inter. These screencasting clips code more intra blocks and so see more gain from CfL.&lt;/p&gt;
&lt;p&gt;This is true of synthetic and rendered content as well:&lt;/p&gt;
&lt;table class=&quot;textable&quot;&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;
&lt;td colspan=&quot;7&quot; class=&quot;title&quot;&gt;BD-rate&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;
&lt;td&gt;PSNR&lt;/td&gt;
&lt;td&gt;PSNR-HVS&lt;/td&gt;
&lt;td&gt;SSIM&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;CIEDE2000&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;PSNR Cb&lt;/td&gt;
&lt;td&gt;PSNR Cr&lt;/td&gt;
&lt;td&gt;MS SSIM&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;result&quot;&gt;&lt;td&gt;Twitch&lt;/td&gt;
&lt;td&gt;-1.01&lt;/td&gt;
&lt;td&gt;-0.93&lt;/td&gt;
&lt;td&gt;-0.90&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;-5.74&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;-15.58&lt;/td&gt;
&lt;td&gt;-9.96&lt;/td&gt;
&lt;td&gt;-0.81&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;p&gt;The Twitch test set is entirely live-streamed video game content, and we see solid gains here as well.&lt;/p&gt;
&lt;p&gt;Chroma From Luma is not, of course, the only technique being adopted in a production codec for the first time in AV1. Next post we'll look at a technique that really is entirely brand new in AV1: The Constrained Directional Enhancement Filter.&lt;/p&gt;
&lt;address&gt;—Monty (&lt;a href=&quot;mailto:monty@xiph.org&quot;&gt;monty@xiph.org&lt;/a&gt;, &lt;a href=&quot;mailto:cmontgomery@mozilla.com&quot;&gt;cmontgomery@mozilla.com&lt;/a&gt;) April 9, 2018&lt;/address&gt;
&lt;p&gt;&lt;em&gt;&lt;a name=&quot;rolling-intra&quot; id=&quot;rolling-intra&quot;&gt;*&lt;/a&gt; It's also possible to spread a keyframe through other frames using a technique called &lt;strong&gt;rolling intra&lt;/strong&gt;. Rolling intra splits standalone keyframes into standalone blocks that are sprinkled through preceding inter frames. Rather than seeking to a keyframe and simply beginning playback at that point, a rolling intra codec seeks to an earlier point, reads forward collecting the standalone keyframe pieces that are spread out through other frames, then begins playback after it has enough information to construct a complete, up-to-date frame. Rolling intra does not improve compression; it merely spreads out bitrate spikes caused by large keyframes. It can also be used as a form of error resiliency.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a name=&quot;xtime&quot; id=&quot;xtime&quot;&gt;†&lt;/a&gt;Technically, Energy is Power x Time&lt;/em&gt;. When comparing apples and oranges, it is important to express both in watt-hours.&lt;/p&gt;
&lt;h2&gt;Additional Resources&lt;/h2&gt;
&lt;ol&gt;&lt;li&gt;Alliance for Open Media Design Document: &lt;a href=&quot;https://docs.google.com/document/d/1T86spqQqt7o3LWj3FeaOPUJ2vZ_DSgImJ2r0on8wP-g/&quot;&gt;Chroma from Luma in AV1&lt;/a&gt;, Luc Trudeau, David Michael Barr&lt;/li&gt;
&lt;li&gt;The seminal paper on spatial domain Chroma from Luma prediction: &lt;a href=&quot;http://dx.doi.org/10.1109/ICIP.2009.5413727&quot;&gt;Intra Prediction Method Based on the Linear Relationship between the Channels for YUV 4:2:0 Intra Coding&lt;/a&gt;, Sang Heon Lee, 2009 16th IEEE International Conference on Image Processing (ICIP)&lt;/li&gt;
&lt;li&gt;LG proposal to include spatial Chroma from Luma in HEVC: &lt;a href=&quot;http://wftp3.itu.int/av-arch/jctvc-site/2010_07_B_Geneva/JCTVC-B021.doc&quot;&gt;New intra chroma prediction using inter-channel correlation&lt;/a&gt;, Kim et. al. 2010&lt;/li&gt;
&lt;li&gt;Samsung and LG joint LM Mode proposal to HEVC: &lt;a href=&quot;https://people.xiph.org/~xiphmont/demo/av1/phenix.int-evry.fr/jct/doc_end_user/documents/5_Geneva/wg11/JCTVC-E266-v4.zip&quot;&gt;Chroma intra prediction by reconstructed luma samples&lt;/a&gt;, Kim et. al. 2011&lt;/li&gt;
&lt;li&gt;CfL as implemented in Daala: &lt;a href=&quot;https://arxiv.org/abs/1603.03482&quot;&gt;Predicting Chroma from Luma with Frequency Domain Intra Prediction&lt;/a&gt;, Nathan E. Egge, Jean-Marc Valin, 10 March 2016&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.03951&quot;&gt;Predicting Chroma from Luma in AV1&lt;/a&gt;, Luc N. Trudeau, Nathan E. Egge, David Barr, 17 January 2018&lt;/li&gt;
&lt;li&gt;Daala technology &lt;a href=&quot;https://people.xiph.org/~xiphmont/demo/daala/demo3.shtml&quot;&gt;demo page for Time-Frequency resolution switching&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Daala technology &lt;a href=&quot;https://people.xiph.org/~xiphmont/demo/daala/demo4.shtml&quot;&gt;demo page for Chroma-From-Luma&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chroma From Luma VDD presentation: &lt;a href=&quot;https://docs.google.com/presentation/d/1LYQQ4EnsIIbTTdXlLpxbxK90ijkdNhz7Pt5a1CJoxsY/&quot;&gt;Chroma from Luma (CfL) in AV1&lt;/a&gt; , Luc Trudeau, David Michael Barr, September 2017&lt;/li&gt;
&lt;li&gt;Data Compression Conference presentation: &lt;a href=&quot;https://docs.google.com/presentation/d/13yUG1lyNmf_TtWARvOJRGa05EVknKya-VEDZ_8L22P8/&quot;&gt;Chroma from Luma Intra Prediction for AV1&lt;/a&gt;, Luc N. Trudeau, Nathan E. Egge, David Barr, March 2018&lt;/li&gt;
&lt;li&gt;Xiph.Org's &lt;a href=&quot;https://media.xiph.org/video/derf/&quot;&gt;standard 'derf' test sets&lt;/a&gt;, hosted at &lt;a href=&quot;https://media.xiph.org/&quot;&gt;media.xiph.org&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Automated testing harness and metrics used by Daala and AV1 development: &lt;a href=&quot;https://arewecompressedyet.com/&quot;&gt;Are We Compressed Yet?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;&lt;div class=&quot;mr&quot;&gt;
&lt;div&gt;&lt;a href=&quot;http://mozilla.org/&quot;&gt;&lt;img src=&quot;https://people.xiph.org/~xiphmont/demo/moz-logo2.png&quot; /&gt;&lt;/a&gt;&lt;/div&gt;

&lt;div class=&quot;mrcenter&quot; readability=&quot;5.9333333333333&quot;&gt;
&lt;div class=&quot;mrcontent&quot; readability=&quot;28.819047619048&quot;&gt;Monty's codec documentation work is sponsored by&lt;br /&gt;&lt;a href=&quot;http://www.mozilla.org/en-US/research/&quot;&gt;Mozilla Research&lt;/a&gt;.&lt;br /&gt;(C) Copyright 2018 Mozilla and Xiph.Org&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;mrright&quot;&gt;
&lt;div class=&quot;mrcontent&quot;&gt;&lt;a href=&quot;http://www.mozilla.org/en-US/research/&quot;&gt;&lt;img src=&quot;https://people.xiph.org/~xiphmont/demo/hack.jpg&quot; /&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/body&gt;</description>
<pubDate>Mon, 09 Apr 2018 19:26:08 +0000</pubDate>
<dc:creator>TD-Linux</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://people.xiph.org/~xiphmont/demo/av1/demo1.shtml</dc:identifier>
</item>
<item>
<title>Color: From Hex codes to Eyeballs</title>
<link>http://jamie-wong.com/post/color/</link>
<guid isPermaLink="true" >http://jamie-wong.com/post/color/</guid>
<description>&lt;p&gt;&lt;img src=&quot;http://jamie-wong.com/images/color/Hero.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Why do we perceive &lt;code&gt;background-color: #9B51E0&lt;/code&gt; as this particular purple?&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://jamie-wong.com/images/color/Purple.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;This is one of those questions where I thought I’d known the answer for a long time, but as I inspected my understanding, I realized there were pretty significant gaps.&lt;/p&gt;
&lt;p&gt;Through an exploration of electromagnetic radiation, optical biology, colorimetry, and display hardware, I hope to start filling in some of these gaps. If you want to skip ahead, here’s the lay of the land we’ll be covering:&lt;/p&gt;
&lt;nav id=&quot;TableOfContents&quot;/&gt;&lt;p&gt;Otherwise, let’s start with the physics.&lt;/p&gt;

&lt;p&gt;Radio waves, microwaves, infrared, visible light, ultraviolet, x-rays, and gamma rays are all forms of electromagnetic radiation. While these things all go by different names, these names really only label different ranges of wavelengths within the electromagnetic spectrum.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/electromagneticSpectrum.png&quot;/&gt; The electromagnetic spectrum
&lt;p&gt;The smallest unit of electromagnetic radiation is a photon. The energy contained within a photon is proportional to the frequency of its corresponding wave, with high energy photons corresponding with high frequency waves.&lt;/p&gt;
&lt;p&gt;To really understand color, we need to first understand radiation. Let’s take a closer look at the radiation of an incandescent light bulb.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/incandescent.png&quot;/&gt; Photo by &lt;a href=&quot;https://unsplash.com/photos/HfR0W6HW_Cw&quot;&gt;Alex Iby&lt;/a&gt;
&lt;p&gt;We might want to know how much energy the bulb is radiating. The &lt;strong&gt;radiant flux&lt;/strong&gt; ($$\Phi_e$$) of an object is the total energy emitted per second, and is measured in Watts. The radiant flux of a 100W incandescent lightbulb is about 80W, with the remaining 20W being converted directly to non-radiated heat.&lt;/p&gt;
&lt;p&gt;If we want to know how much of that energy comes from each wavelength, we can look at the &lt;strong&gt;spectral flux&lt;/strong&gt;. The spectral flux ($$\Phi_{e,\lambda}$$) of an object is radiant flux per unit wavelength, and is typically measured in Watts/nanometer.&lt;/p&gt;
&lt;p&gt;If we were to graph the spectral flux of our incandescent lightbulb as a function of wavelength, it might look something like this:&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/SpectralFlux1.png&quot;/&gt;&lt;p&gt;The area under this curve will give the radiant flux. As an equation, $$\Phi_e = \int_0^\infty \Phi_{e,\lambda}(\lambda) d\lambda$$. In this case, the area under the curve will be about 80W.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/SpectralFlux2.png&quot;/&gt;&lt;p&gt;$$\Phi_{e}^{\text{bulb}} = \int_0^\infty \Phi_{e,\lambda}^\text{bulb}(\lambda) d\lambda = 80\text{W}$$&lt;/p&gt;
&lt;p&gt;Now you might’ve heard from eco-friendly campaigns that incandescent lightbulbs are brutally inefficient, and might be thinking to yourself, “well, 80% doesn’t seem so bad”.&lt;/p&gt;
&lt;p&gt;And it’s true — an incandescent lightbulb is a pretty efficient way to convert electricity into radiation. Unfortunately, it’s a terrible way to convert electricity into &lt;em&gt;human visible&lt;/em&gt; radiation.&lt;/p&gt;

&lt;p&gt;Visible light is the wavelength range of electromagnetic radiation from $$\lambda = 380\text{nm}$$ to $$\lambda = 750\text{nm}$$. On our graph of an incandescent bulb, that’s the shaded region below.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/SpectralFlux3.png&quot;/&gt;&lt;p&gt;$$\int_{380 \text{nm}}^{750 \text{nm}} \Phi_{e,\lambda}^\text{bulb}(\lambda) d\lambda = 8.7W$$&lt;/p&gt;
&lt;p&gt;Okay, so the energy radiated &lt;em&gt;within the visible spectrum&lt;/em&gt; is $$8.7W$$ for an efficiency of $$8.7\%$$. That seems pretty awful. But it gets worse.&lt;/p&gt;
&lt;p&gt;To understand why, let’s consider why visible light is, well, &lt;em&gt;visible&lt;/em&gt;.&lt;/p&gt;

&lt;img src=&quot;http://jamie-wong.com/images/color/bweye.png&quot;/&gt; Photo by &lt;a href=&quot;https://unsplash.com/photos/QaGNhezu_5Q&quot;&gt;Christopher Burns&lt;/a&gt;
&lt;p&gt;Just as we saw that an incandescent light bulb doesn’t radiate equally at all wavelengths, our eyes aren’t equally sensitive to radiation at all wavelengths. If we measure a human eye’s sensitivity to every wavelength, we get a &lt;a href=&quot;https://en.wikipedia.org/wiki/Luminosity_function&quot;&gt;luminosity function&lt;/a&gt;. The standard luminosity function, $$\bar y(\lambda)$$ looks like this:&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/SpectralFlux4.png&quot;/&gt;&lt;p&gt;The bounds of this luminosity function &lt;em&gt;define&lt;/em&gt; the range of visible light. Anything outside this range isn’t visible because, well, our eyes aren’t sensitive to it!&lt;/p&gt;
&lt;p&gt;This curve also shows that our eyes are &lt;em&gt;much&lt;/em&gt; more sensitive to radiation at 550nm than they are to radiation at either 650nm or 450nm.&lt;/p&gt;
&lt;p&gt;Other animals have eyes that are sensitive to a different range of wavelengths, and therefore different luminosity functions. Birds can see ultraviolet radiation in the range between $$\lambda=300\text{nm}$$ to $$\lambda=400\text{nm}$$, so if scholarly birds had defined the electromagnetic spectrum, that would’ve been part of the “visible light” range for them!&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/owl.png&quot;/&gt; Photo by &lt;a href=&quot;https://unsplash.com/photos/0J6cTw0V2lE&quot;&gt;Timothy Rhyne&lt;/a&gt;
&lt;p&gt;By multiplying the graph of spectral flux with the luminosity function $$\bar y(\lambda)$$, we get a function which describes the contributions to human perceived brightness for each wavelength emitted by a light source.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/SpectralFlux5.png&quot;/&gt;&lt;p&gt;This is the &lt;strong&gt;spectral luminous flux (&lt;/strong&gt;$$\Phi_{v,\lambda}$$&lt;strong&gt;)&lt;/strong&gt;. To acknowledge this is about human perception rather than objective power, the luminous flux is typically measured in lumens rather than Watts using a conversion ratio of 683.002 lm/W.&lt;/p&gt;
&lt;p&gt;$$\Phi_{v,\lambda}(\lambda) = 683.002 \frac{\text{lm}}{\text{W}} \cdot \bar y(\lambda) \cdot \Phi_{e,\lambda}(\lambda)$$&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;luminous flux (&lt;/strong&gt;$$\Phi_v$$&lt;strong&gt;)&lt;/strong&gt; of a light source is the total &lt;em&gt;human perceived&lt;/em&gt; power of the light.&lt;/p&gt;
&lt;p&gt;Just as we calculated the radiant flux by taking the area under the spectral flux curve, we can find the luminous flux by taking the area under the spectral &lt;em&gt;luminous&lt;/em&gt; flux curve, with a constant conversion from perceived watts to lumens:&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/SpectralFlux5.5.png&quot;/&gt;&lt;p&gt;$$\Phi_{v}^\text{bulb} = \int_0^\infty \bar y(\lambda) \cdot \Phi_{e,\lambda}^\text{bulb}(\lambda) d\lambda = 683.002 \frac{\text{lm}}{\text{W}} \cdot 2.4\text{W} \approx 1600 \text{lm}$$&lt;/p&gt;
&lt;p&gt;So the luminous flux of our 100W incandescent lightbulb is a measly 2.4W or 1600lm! The bulb has a luminous efficiency of 2.4%, a far cry from the 80% efficiency converting electricity into radiation.&lt;/p&gt;
&lt;p&gt;Perhaps if we had a light source that concentrated its emission into the visible range, we’d be able to get more efficient lighting. Let’s compare the spectra of incandescent, fluorescent, and LED bulbs:&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/SpectralFlux6.png&quot;/&gt;&lt;p&gt;And indeed, we can see that far less of the radiation in fluorescent or led bulbs is wasted on wavelengths that humans can’t see. Where incandescent bulbs might have an efficiency of 1-3%, fluorescent bulbs can be around 10% efficient, and LED bulbs can achieve up to 20% efficiency!&lt;/p&gt;
&lt;p&gt;Enough about brightness, let’s return to the focus of this post: color!&lt;/p&gt;

&lt;img src=&quot;http://jamie-wong.com/images/color/lemon.png&quot;/&gt; Photo by &lt;a href=&quot;https://unsplash.com/photos/sil2Hx4iupI&quot;&gt;Lauren Mancke&lt;/a&gt;
&lt;p&gt;How might we identify a given color? If I have a lemon in front of me, how can I tell you over the phone what color it is? I might tell you “the lemon is yellow”, but which yellow? How would you precisely identify each of these yellows?&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_382135F512C449943D36A9C35B9E8A4F38EB382A3D72DF63DC63D011CB7A8322_1521414272066_Yellows.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Armed with the knowledge that color is humans’ interpretation of electromagnetic radiation, we might be tempted to define color mathematically via spectral flux. Any human visible color will be some weighted combination of the monochromatic (single wavelength) colors. Monochromatic colors are also known as spectral colors.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/Rainbow.png&quot;/&gt; The monochromatic colors by wavelength
&lt;p&gt;For any given object, we can measure its emission (or reflectance) spectrum, and use that to precisely identify a color. If we can reproduce the spectrum, we can certainly reproduce the color!&lt;/p&gt;
&lt;p&gt;The sunlight reflected from a point on a lemon might have a reflectance spectrum that looks like this:&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/ReflectanceSpectrum.png&quot;/&gt;&lt;p&gt;&lt;em&gt;Note: the power and spectral distribution of radiation that reaches your eye is going to depend upon&lt;/em&gt; &lt;em&gt;the&lt;/em&gt; &lt;em&gt;power &amp;amp; emission spectrum of the light source, the distance of the light source from the illuminated object, the size and shape of the object, the absorption spectrum of the object, and your distance from the object. That’s a lot to think about, so let’s focus just on what happens when that light hits your eye. Let’s also disregard units for now to focus on concepts.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When energy with this spectral distribution hits our eyes, we perceive it as “yellow”. Now let’s say I take a photo of the lemon and upload it to my computer. Next, I carefully adjust the colors on my screen until a particular point of the on-screen lemon is imperceptibly different from the color of the actual lemon in my actual hand.&lt;/p&gt;
&lt;p&gt;If you were to measure the spectral power distribution coming off of the screen, what would you expect the distribution to look like? You might reasonably expect it to look similar to the reflectance spectrum of the lemon above. But it would actually look something like this:&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/EmissionSpectrum.png&quot;/&gt;&lt;p&gt;Two different spectral power distributions that look the same to human observers are called &lt;a href=&quot;https://en.wikipedia.org/wiki/Metamerism_(color)&quot;&gt;&lt;strong&gt;metamers&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/Metamers1.png&quot;/&gt;&lt;p&gt;To understand how this is possible, let’s take a look at the biology of the eye.&lt;/p&gt;

&lt;img src=&quot;http://jamie-wong.com/images/color/coloreye.png&quot;/&gt; Photo by &lt;a href=&quot;https://unsplash.com/photos/UbJMy92p8wk&quot;&gt;Amanda Dalbjörn&lt;/a&gt;
&lt;p&gt;Our perception of light is the responsibility of specialized cells in our eyes called “rods” and “cones”. Rods are predominately important in low-light settings, and don’t play much role in color vision, so we’ll focus on the cones.&lt;/p&gt;
&lt;p&gt;Humans typically have 3 kinds of cones. Having three different kinds of cones makes humans “trichromats”. There is, however, at least one confirmed case of a &lt;a href=&quot;http://nymag.com/scienceofus/2015/02/what-like-see-a-hundred-million-colors.html&quot;&gt;tetrochromat human&lt;/a&gt;! Other animals have even more cone categories. &lt;a href=&quot;http://theoatmeal.com/comics/mantis_shrimp&quot;&gt;Mantis shrimp&lt;/a&gt; have &lt;em&gt;sixteen&lt;/em&gt; different kinds of cones.&lt;/p&gt;
&lt;p&gt;Each kind of cone is labelled by the range of wavelengths of light they are excited by. The standard labelling is “S”, “M”, and “L” (short, medium, long).&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/Cones.png&quot;/&gt;&lt;p&gt;These three curves indicate how sensitive the corresponding cone is to each wavelength. The highest point on each curve is called the “peak wavelength”, indicating the wavelength of radiation that the cone is most sensitive to.&lt;/p&gt;
&lt;p&gt;Let’s see how our cones process the light bouncing off the lemon in my hand.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/ConeExcitation1.png&quot;/&gt;&lt;p&gt;By looking at the normalized areas under the curves, we can see how much the radiation reflected from the real lemon excites each of cones. In this case, the normalized excitations of the S, M, and L cones are 0.02, 0.12, and 0.16 respectively. Now let’s repeat the process for the on-screen lemon.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/ConeExcitation2.png&quot;/&gt;&lt;p&gt;Despite having totally different radiation spectra reaching the eye, the cone &lt;em&gt;excitations&lt;/em&gt; are the same (S=0.02, M=0.12, L=0.16). That’s why the point on the real lemon and the point on the digital lemon look the same to us!&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/Metamers2.png&quot;/&gt; The normalized cone area under the stimulation curves will always be equal for all 3 cone types in the case of metamers.
&lt;p&gt;Our 3 sets of cones reduce any spectral flux curve $$\Phi_e(\lambda)$$ down to a triplet of three numbers $$(S, M, L)$$, and every distinct $$(S, M, L)$$ triplet will be a distinct color! This is pretty convenient, because (0.02, 0.12, 0.16) is much easier to communicate than a complicated continuous function. For the mathematically inclined, our eyes are doing a dimensional reduction from an infinite dimensional space into 3 dimensions, which is pretty damn cool thing to be able to do subconsciously.&lt;/p&gt;
&lt;p&gt;This $$(S, M, L)$$ triplet is, in fact, our first example of a &lt;strong&gt;color space&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Color spaces allow us to define with numeric precision what color we’re talking about. In the previous section, we saw that a specific yellow could be represented as (0.02, 0.12, 0.16) in the SML color space, which is more commonly known as the &lt;a href=&quot;https://en.wikipedia.org/wiki/LMS_color_space&quot;&gt;LMS color space&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Since this color space is describing the stimulation of cones, by definition any human visible color can be represented by positive LMS coordinates (excluding the extremely rare tetrachromat humans, who would need four coordinates instead of three).&lt;/p&gt;
&lt;p&gt;But, alas, this color space has some unhelpful properties.&lt;/p&gt;
&lt;p&gt;For one, not all triplet values (also called &lt;strong&gt;tristimulus values)&lt;/strong&gt; are &lt;em&gt;physically possible&lt;/em&gt;. Consider the LMS coordinates (0, 1, 0). To physically achieve this coordinate, we would need to find some way of stimulating the M cones without stimulating the L or S cones &lt;em&gt;at all&lt;/em&gt;. Because the M cone’s sensitivity curve significantly overlaps at least one of L or S at all wavelengths, this is impossible!&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/Cones.png&quot;/&gt; Any wavelength which stimulates the M cone will also stimulate either the L or S cone (or both!)
&lt;p&gt;A problematic side effect of this fact is that it’s really difficult to increase stimulation of only one of the cones. This, in particular, would make it not a great candidate for building display hardware.&lt;/p&gt;
&lt;p&gt;Another historical, pragmatic problem was that the cone sensitivities weren’t accurately known until the 1990s, and a need to develop a mathematically precise model of color significantly predates that. The first significant progress on that front came about in the late 1920’s.&lt;/p&gt;

&lt;p&gt;In the late 1920’s, William David Wright and John Guild conducted experiments to precisely define color in terms of contributions from 3 specific wavelengths of light.&lt;/p&gt;
&lt;p&gt;Even though they may not have known about the three classes of cones in the eye, the idea that all visible colors could be created as the combination of three colors had been proposed at least a hundred years earlier.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/tricolor.png&quot;/&gt; An example of a tricolor theory by Charles Hayter, 1826
&lt;p&gt;Wright &amp;amp; Guild had the idea to construct an apparatus that would allow test subjects to reconstruct a test color as the combination of three fixed wavelength light sources. The setup would’ve looked something like this:&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/ColorMatching1.png&quot;/&gt;&lt;p&gt;The experimenter would set the lamp on the bottom to a target wavelength, (for instance, 600nm) then ask the test subject to adjust the three lamp power controls until the colors they were seeing matched.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/ColorMatching2.png&quot;/&gt;&lt;p&gt;The power settings of the three dials give us a (red, green, blue) triplet identifying the pure spectral color associated with 600nm. Repeating this process every 5nm with about 10 test subjects, a graph emerges showing the amounts of red (700nm), green (546nm), and blue (435nm) light needed to reconstruct the appearance of a given wavelength. These functions are known as &lt;strong&gt;color matching function (CMFs)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;These particular color matching functions are known as $$\bar r(\lambda)$$, $$\bar g(\lambda)$$, and $$\bar b (\lambda)$$.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/cmfs1.png&quot;/&gt;&lt;p&gt;This gives the pure spectral color associated with 600nm an $$(R, G, B)$$ coordinate of (0.34, 0.062, 0.00). This is a value in the &lt;a href=&quot;https://en.wikipedia.org/wiki/CIE_1931_color_space#CIE_RGB_color_space&quot;&gt;CIE 1931 RGB color space&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hold on though — what does it mean when the functions go negative, like here?&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/cmfs2.png&quot;/&gt;&lt;p&gt;The pure spectral color associated with 500nm has an $$(R, G, B)$$ coordinate of (-0.72, 0.85, 0.48). So what exactly does that -0.72 mean?&lt;/p&gt;
&lt;p&gt;It turns out that no matter what you set the red (700nm) dial to, it will be impossible to match a light outputting at 500nm, regardless of the values of blue and green dials. You can, however, make the two sides match by adding red light to the &lt;em&gt;bottom&lt;/em&gt; side.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/ColorMatching3.png&quot;/&gt;&lt;p&gt;The actual setup probably would’ve had a full set of 3 variable power, fixed wavelength lights on either side of the divider to allow any of them to be adjusted to go negative.&lt;/p&gt;
&lt;p&gt;Using our color matching functions, we can match any monochromatic light using a combination of (possibly negative) amounts of red (700nm), green (546nm), and blue (435nm) light.&lt;/p&gt;
&lt;p&gt;Just as we were able to use our L, M, and S cone sensitivity functions to determine cone excitation for any spectral distribution, we can do the same thing with our color matching functions. Let’s apply that to the lemon color from before:&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/ColorMatchingLemon.png&quot;/&gt;&lt;p&gt;By taking the area under the curve of product of the spectral curve and the color matching functions, we’re left with an $$(R, G, B)$$ triplet (1.0, 0.8, 0.2) uniquely identifying this color.&lt;/p&gt;
&lt;p&gt;While the $$(L, M, S)$$ color space gave us a precise way to &lt;em&gt;identify&lt;/em&gt; colors, this $$(R, G, B)$$ color space gives us a precise way to &lt;em&gt;reproduce&lt;/em&gt; colors. But, as we saw in the color matching functions, any colors with a negative $$(R, G, B)$$ coordinate can’t actually be reproduced.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/cmfs3.png&quot;/&gt;&lt;p&gt;But this graph only shows which spectral colors can’t be reproduced. What about non-spectral colors? Can I produce pink with an R, G, B combination? What about teal?&lt;/p&gt;
&lt;p&gt;To answer these questions, we’ll need a better way of visualizing color space.&lt;/p&gt;

&lt;p&gt;So far most of our graphs have put wavelength on the horizontal axis, and we’ve plotted multiple series to represent the other values of interest.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/WavelengthPlots.png&quot;/&gt;&lt;p&gt;Instead, we could plot color as a function of $$(R, G, B)$$ or $$(L, M, S)$$. Let’s see what color plotted in 3D $$(R, G, B)$$ space looks like.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/LinearRGBCube.png&quot;/&gt;&lt;p&gt;Cool! This gives us a visualization of a broader set of colors, not just the spectral colors of the rainbow.&lt;/p&gt;
&lt;p&gt;A simple way to reduce this down to two dimensions would be to have a separate plot for each pair of values, like so:&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/RGBPairPlots.png&quot;/&gt; Component pairs plotted, holding the third coordinate constant
&lt;p&gt;In each of these plots, we discard one dimension by holding one thing constant. Rather than holding one of red, green, and blue constant, it would be really nice to have a plot showing all the colors of the rainbow &amp;amp; their combinations, while holding &lt;em&gt;lightness&lt;/em&gt; constant.&lt;/p&gt;
&lt;p&gt;Looking at the cube pictures again, we can see that (0, 0, 0) is black, and (1, 1, 1) is white.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/LinearRGBCube.png&quot;/&gt;&lt;p&gt;What happens if we slice the cube diagonally across the plane containing $$(1, 0, 0)$$, $$(0, 1, 0)$$, and $$(0, 0, 1)$$?&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/TriangleSliceRGB.png&quot;/&gt;&lt;p&gt;This triangle slice of the cube has the property that $$R + G + B = 1$$, and we can use $$R + G + B$$ as a crude approximation of lightness. If we take a top-down view of this triangular slice, then we get this:&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/rgChromaticity1.png&quot;/&gt;&lt;p&gt;This two dimensional representation of color is called &lt;strong&gt;chromaticity&lt;/strong&gt;. This particular kind is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Rg_chromaticity&quot;&gt;&lt;strong&gt;rg chromaticity&lt;/strong&gt;&lt;/a&gt;. Chromaticity gives us information about the &lt;em&gt;ratio&lt;/em&gt; of the primary colors independent of the lightness.&lt;/p&gt;
&lt;p&gt;This means we can have the same chromaticity at many different intensities.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/rgChromaticity2.png&quot;/&gt;&lt;p&gt;We can even make a chromaticity graph where the intensity varies with r &amp;amp; g in order to maximize intensity while preserving the ratio between $$R$$, $$G$$, and $$B$$.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/rgChromaticity3.png&quot;/&gt;&lt;p&gt;Chromaticity is a useful property of a color to consider because it stays constant as the intensity of a light source changes, so long as the light source retains the same spectral distribution. As you change the brightness of your screen, chromaticity is the thing that stays constant!&lt;/p&gt;
&lt;p&gt;There are many different ways of dividing chromaticity into two dimensions. One of the common methods is used in both the HSL and HSV color spaces. Both color spaces split chromaticity into “hue” and “saturation”, like so:&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/HSL.png&quot;/&gt;&lt;p&gt;It might appear at a glance that the rg chromaticity triangle and these hue vs. saturation squares contains every color of the rainbow. It’s time to revisit those pesky negative values in our color matching functions.&lt;/p&gt;

&lt;p&gt;If we take our color matching functions $$\bar r(\lambda)$$, $$\bar g(\lambda)$$, and $$\bar b(\lambda)$$ and use them to plot the rg chromaticities of the spectral colors, we end up with a plot like this:&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/rgChromaticityPlot1.png&quot;/&gt;&lt;p&gt;The black curve with the colorful dots on it shows the chromaticities of all the pure spectral colors. The curve is called the &lt;strong&gt;spectral locus&lt;/strong&gt;. The stars mark the wavelengths of the variable power test lamps used in the color matching experiments.&lt;/p&gt;
&lt;p&gt;If we overlay our previous chromaticity triangles onto this chart, we’re left with this:&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/rgChromaticityPlot2.png&quot;/&gt;&lt;p&gt;The area inside the spectral locus represents all of the chromaticities that are visible to humans. The checkerboard area represents chromaticities that humans can recognize, but that &lt;em&gt;cannot&lt;/em&gt; be reproduced by any positive combination of 435nm, 546nm, and 700nm lights. From this diagram, we can see that we’re unable to reproduce any of the spectral colors between 435nm and 546 nm, which includes pure cyan.&lt;/p&gt;
&lt;p&gt;The triangle on the right without the checkerboard is all of the chromaticities that &lt;em&gt;can&lt;/em&gt; be reproduced by a positive combination. We call the area that can be reproduced the &lt;strong&gt;gamut&lt;/strong&gt; of the color space.&lt;/p&gt;
&lt;p&gt;Before we can &lt;em&gt;finally&lt;/em&gt; return to hexcodes, we have one more color space we need to cover.&lt;/p&gt;

&lt;p&gt;In 1931, the International Comission on Illumination convened and created two color spaces. The first was the RGB color space we’ve already discussed, which was created based on the results of Wright &amp;amp; Guild’s color matching experiments. The second was the XYZ color space.&lt;/p&gt;
&lt;p&gt;One of the goals of the XYZ color space was to have positive values for all human visible colors, and therefore have all chromaticities fit in the range [0, 1] on both axes. To achieve this, a linear transformation of RGB space was carefully selected.&lt;/p&gt;
&lt;p&gt;$$ \begin{bmatrix} X \\ Y \\ Z \end{bmatrix} = \frac{1}{b_{21}} \begin{bmatrix} b_{11} &amp;amp; b_{12} &amp;amp; b_{13} \\ b_{21} &amp;amp; b_{22} &amp;amp; b_{23} \\ b_{31} &amp;amp; b_{32} &amp;amp; b_{33} \end{bmatrix} \begin{bmatrix} R \\ G \\ B \end{bmatrix} = \frac{1}{0.17697} \begin{bmatrix} 0.49000 &amp;amp; 0.31000 &amp;amp; 0.20000 \\ 0.17697 &amp;amp; 0.81240 &amp;amp; 0.010630 \\ 0.0000 &amp;amp; 0.010000 &amp;amp; 0.99000 \end{bmatrix} \begin{bmatrix} R \\ G \\ B \end{bmatrix} $$&lt;/p&gt;
&lt;p&gt;The analog of rg chromaticity for XYZ space is xy chromaticity and is the more standard coordinate system used for chromaticities diagrams.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/xyChromaticityPlot.png&quot;/&gt;&lt;p&gt;Gamuts are typically represented by a triangle placed into an xy chromaticity diagram. For instance, here’s the gamut of CIE RGB again, this time in xy space.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/gamut1.png&quot;/&gt;&lt;p&gt;With an understanding of gamuts &amp;amp; chromaticity, we can finally start to discuss how digital displays are able to display an intended color.&lt;/p&gt;

&lt;p&gt;Regardless of the manufacturer of your display, if you took a powerful magnifying glass to your display, you would find a grid of pixels, where each pixel is composed of 3 types of subpixels: one type emitting red, one green, and one blue. It might look something like this:&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/Subpixels.png&quot;/&gt;&lt;p&gt;Unlike the test lamps used in the color matching experiments, the subpixels do not emit monochromatic light. Each type of subpixel has its own spectral distribution, and these will vary from device to device.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/subpixelSpectra.png&quot;/&gt; MacBook Air subpixel spectral data from &lt;a href=&quot;https://fluxometer.com/rainbow/&quot;&gt;f.luxometer&lt;/a&gt;
&lt;p&gt;Using &lt;a href=&quot;https://support.apple.com/guide/colorsync-utility/welcome/mac&quot;&gt;ColorSync Utility&lt;/a&gt; on my Macbook Pro, I was able to determine the xy space gamut of my screen.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/gamut3.png&quot;/&gt;&lt;p&gt;Notice that the corners of the gamut no longer lie along the spectral locus. This makes sense, since the subpixels do not emit pure monochromatic light. This gamut represents the full range of chromaticities that this monitor can faithfully reproduce.&lt;/p&gt;
&lt;p&gt;While gamuts of monitors will vary, modern monitors should try to enclose a specific other gamut: sRGB.&lt;/p&gt;

&lt;p&gt;sRGB (“standard Red Green Blue”) is a color space created by HP and Microsoft in 1996 to help ensure that color data was being transferred faithfully between mediums.&lt;/p&gt;
&lt;p&gt;The standard specifies the chromaticities of the red, green, and blue primaries.&lt;/p&gt;
&lt;table class=&quot;datatable&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Chromaticity&lt;/th&gt;
&lt;th&gt;Red&lt;/th&gt;
&lt;th&gt;Green&lt;/th&gt;
&lt;th&gt;Blue&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;x&lt;/td&gt;
&lt;td&gt;0.6400&lt;/td&gt;
&lt;td&gt;0.3000&lt;/td&gt;
&lt;td&gt;0.1500&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;y&lt;/td&gt;
&lt;td&gt;0.3300&lt;/td&gt;
&lt;td&gt;0.6000&lt;/td&gt;
&lt;td&gt;0.0600&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;0.2126&lt;/td&gt;
&lt;td&gt;0.751&lt;/td&gt;
&lt;td&gt;0.0722&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;If we plot these, we wind up with a gamut similar to, but slightly smaller than, the MacBook LCD screen.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/gamut2.png&quot;/&gt;&lt;p&gt;There are parts of the official sRGB gamut that aren’t within the MacBook Pro LCD gamut, meaning that the LCD can’t faithfully reproduce them. To accommodate for that, my MacBook seems to use a modified sRGB gamut.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/gamut4.png&quot;/&gt;&lt;p&gt;sRGB is the default color space used almost everywhere, and is the standard color space used by browsers (&lt;a href=&quot;https://www.w3.org/TR/css-color-4/#color-type&quot;&gt;specified in the CSS standard&lt;/a&gt;). All of the diagrams in this blog post are in sRGB color space. That means that all colors outside of the sRGB gamut aren’t accurately reproduced in the diagrams in this post!&lt;/p&gt;
&lt;p&gt;Which brings us, finally, to how colors are specified on the web.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;#9B51E0&lt;/code&gt; specifies a color in sRGB space. To convert it to its associated (R, G, B) coordinate, we divide each of the three components by &lt;code&gt;0xFF&lt;/code&gt; aka 255. In this case:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;0x9B/0xFF = 0.61
0x51/0xFF = 0.32
0xE0/0xFF = 0.88
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;So the coordinate associated with &lt;code&gt;#9BE1E0&lt;/code&gt; is $$(0.61, 0.32, 0.88)$$.&lt;/p&gt;
&lt;p&gt;Before we send these values to the display hardware to set subpixel intensities, there’s &lt;em&gt;one&lt;/em&gt; more step: gamma correction.&lt;/p&gt;

&lt;p&gt;With each coordinate in RGB space being given 256 possible values, we want to ensure that each adjacent pair is as different as possible. For example, we want &lt;code&gt;#030000&lt;/code&gt; to be as different from &lt;code&gt;#040000&lt;/code&gt; as &lt;code&gt;#F40000&lt;/code&gt; is from &lt;code&gt;#F50000&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Human vision is much more sensitive to small changes in low energy lights than small changes to high energy lights, so we want to allocate more of the 256 values to representing low energy values.&lt;/p&gt;
&lt;p&gt;To see how, let’s imagine we wanted to encode greyscale values, and only had 3 bits to do it, giving us 8 possible values.&lt;/p&gt;
&lt;p&gt;If we plot grey values as a linear function of energy, it would look something like this:&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/linearEnergy.png&quot;/&gt;&lt;p&gt;We’ll call our 3 bit encoded value $$Y$$. If our encoding scheme spaces out each value we encode evenly ($$Y = \frac{\left\lfloor8E\right\rfloor}{8}$$), then it would look like this:&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/gamma1.png&quot;/&gt;&lt;p&gt;You can see that the perceptual difference between $$Y=0$$ and $$Y=1$$ is significantly greater than the difference between $$Y=6$$ and $$Y=7$$.&lt;/p&gt;
&lt;p&gt;Now let’s see what happens if we use a power function instead. Let’s try $$Y = \left(\frac{\left\lfloor8E\right\rfloor}{8}\right)^2$$.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/gamma2.png&quot;/&gt;&lt;p&gt;We’re getting much closer to perceptual uniformity here, where each adjacent pair of values is as different as any other adjacent pair.&lt;/p&gt;
&lt;p&gt;This process of taking energy values and mapping them to discrete values is called &lt;strong&gt;gamma encoding&lt;/strong&gt;. The inverse operations (converting discrete values to energy values) is called &lt;strong&gt;gamma decoding&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In general form, gamma correction has the equation $$V_{out} = A V_{in}^\gamma$$. The exponent is the greek letter “gamma”, hence the name.&lt;/p&gt;
&lt;p&gt;The encoding &amp;amp; decoding rules for sRGB use a similar idea, but slightly more complex.&lt;/p&gt;
&lt;p&gt;$$C_\mathrm{linear}= \begin{cases}\frac{C_\mathrm{sRGB}}{12.92}, &amp;amp; C_\mathrm{sRGB}\le0.04045\\ \left(\frac{C_\mathrm{sRGB}+0.055}{1.055}\right)^{2.4}, &amp;amp; C_\mathrm{sRGB}&amp;gt;0.04045 \end{cases}$$&lt;/p&gt;
&lt;p&gt;If we plot sRGB values against linear values, it would look like this:&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/gamma3.png&quot;/&gt;&lt;p&gt;Okay! That was the last piece we needed to understand to see how we get from hex codes to eyeballs! Let’s do the walkthrough 😀&lt;/p&gt;

&lt;p&gt;First, we take &lt;code&gt;#9B51E0&lt;/code&gt;, split it up into its R, G, B components, and normalize those components to be the range $$[0, 1]$$.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/summary1.png&quot;/&gt;&lt;p&gt;This gives us a coordinate of $$(0.61, 0.32, 0.88)$$ in sRGB space. Next, we take our sRGB components and convert them to linear values.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/summary2.png&quot;/&gt;&lt;p&gt;This gives us a coordinate $$(0.33, 0.08, 0.10)$$ in linear RGB space. These values are used to set the intensity of the subpixels on the screen.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/summary3.png&quot;/&gt;&lt;p&gt;The spectral distributions of the subpixels combine to a single spectral distribution for the whole pixel.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/summary4.png&quot;/&gt;&lt;p&gt;The electromagnetic radiation travels from the pixel through your cornea and hits your retina, exciting your 3 kinds of cones.&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/summary5.png&quot;/&gt;&lt;p&gt;Putting it all together for a different color, we’re left with the image that opens this post!&lt;/p&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/Hero.png&quot;/&gt;
&lt;img src=&quot;http://jamie-wong.com/images/color/brightness.png&quot;/&gt;&lt;p&gt;Before sRGB values are converted into subpixel brightness, they’ll be attenuated by the device’s brightness setting. So the &lt;code&gt;0xFF0000&lt;/code&gt; on a display at 50% brightness might match the &lt;code&gt;0x7F0000&lt;/code&gt; on the same display at 100% brightness.&lt;/p&gt;
&lt;p&gt;In an ideal screen, this would mean that regardless of the brightness setting, black pixels $$(0, 0, 0)$$ would emit no light. Most phone &amp;amp; laptop screens are LCD screens, however, where each subpixel is a filter acting upon white light. This video is a great teardown of how LCDs work:&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/jiejNAUwcQ8&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;[embedded content]&lt;/iframe&gt;
&lt;p&gt;The filter is imperfect, so as brightness is increased, black pixels will emit light as the backlight bleeds through. OLED screens (like on the iPhone X and Pixel 2) don’t use a backlight, allowing them to have a consistent black independent of screen brightness.&lt;/p&gt;

&lt;p&gt;This post intentionally glosses over many facets of color reproduction and recognition. For instance, we didn’t talk about what your brain does with the cone excitation information in the &lt;a href=&quot;https://psych.ucalgary.ca/PACE/VA-Lab/colourperceptionweb/theories.htm&quot;&gt;opponent-process theory&lt;/a&gt; or the effects of &lt;a href=&quot;https://en.wikipedia.org/wiki/Color_constancy&quot;&gt;color constancy&lt;/a&gt;. We didn’t talk about &lt;a href=&quot;https://en.wikipedia.org/wiki/Additive_color&quot;&gt;additive color&lt;/a&gt; vs. &lt;a href=&quot;https://en.wikipedia.org/wiki/Subtractive_color&quot;&gt;subtractive color&lt;/a&gt;. We didn’t talk about &lt;a href=&quot;http://www.colour-blindness.com/general/how-it-works-science/&quot;&gt;color blindness&lt;/a&gt;. We didn’t talk about the difference between &lt;a href=&quot;https://en.wikipedia.org/wiki/Photometry_(optics)#Photometric_quantities&quot;&gt;luminous flux, luminous intensity, luminance, illuminance, and luminous emittance&lt;/a&gt;. We didn’t talk about &lt;a href=&quot;https://en.wikipedia.org/wiki/ICC_profile&quot;&gt;ICC device color profiles&lt;/a&gt; or what programs like &lt;a href=&quot;https://justgetflux.com/&quot;&gt;f.lux&lt;/a&gt; do to color perception.&lt;/p&gt;
&lt;p&gt;I left them out because this post is already way too long! As a friend of mine said: even if you’re a person who understands that most things are deeper than they look, color is way deeper than you would reasonably expect.&lt;/p&gt;

&lt;p&gt;I spent an unusually large portion of the time writing this post just reading because I kept discovering that I was missing something I needed to explain as completely as I’d like.&lt;/p&gt;
&lt;p&gt;Here’s a short list of the more helpful ones:&lt;/p&gt;
&lt;p&gt;I also needed to draw upon many data tables to produce the charts in this post:&lt;/p&gt;
&lt;p&gt;Special thanks to &lt;a href=&quot;http://www.coopernetics.com/blog/&quot;&gt;Chris Cooper&lt;/a&gt; and &lt;a href=&quot;http://rykap.com/&quot;&gt;Ryan Kaplan&lt;/a&gt; for providing feedback on the draft of this post.&lt;/p&gt;
</description>
<pubDate>Mon, 09 Apr 2018 18:31:41 +0000</pubDate>
<dc:creator>markdog12</dc:creator>
<og:image>http://jamie-wong.com/images/logo.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://jamie-wong.com/post/color/</dc:identifier>
</item>
<item>
<title>FreedomBox</title>
<link>https://freedombox.org/</link>
<guid isPermaLink="true" >https://freedombox.org/</guid>
<description>[unable to retrieve full-text content]&lt;p&gt;Article URL: &lt;a href=&quot;https://freedombox.org/&quot;&gt;https://freedombox.org/&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Comments URL: &lt;a href=&quot;https://news.ycombinator.com/item?id=16794830&quot;&gt;https://news.ycombinator.com/item?id=16794830&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Points: 237&lt;/p&gt;&lt;p&gt;# Comments: 57&lt;/p&gt;</description>
<pubDate>Mon, 09 Apr 2018 17:06:54 +0000</pubDate>
<dc:creator>phantom_oracle</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://freedombox.org/</dc:identifier>
</item>
<item>
<title>Instagram ranked worst social network for young people&amp;#039;s mental health</title>
<link>https://www.telegraph.co.uk/news/2017/05/19/instagram-ranked-worst-social-network-young-peoples-mental-health/</link>
<guid isPermaLink="true" >https://www.telegraph.co.uk/news/2017/05/19/instagram-ranked-worst-social-network-young-peoples-mental-health/</guid>
<description>&lt;div class=&quot;articleBodyText version-2 section&quot;&gt;
&lt;div class=&quot;article-body-text component version-2&quot;&gt;
&lt;div class=&quot;component-content&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://www.telegraph.co.uk/technology/0/permanently-delete-instagram-account/&quot;&gt;&lt;span class=&quot;m_first-letter&quot;&gt;I&lt;/span&gt;nstagram&lt;/a&gt; is the worst social media site in terms of its impact on the mental health of young people, a report has suggested.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;https://twitter.com/search?q=%23StatusOfMind&quot;&gt;#StatusofMind&lt;/a&gt; survey found the photo-sharing app negatively impacted on people's body image, sleep and fear of missing out.&lt;/p&gt;
&lt;p&gt;However, the survey of 1,479 youngsters aged 14 to 24, found &lt;a href=&quot;https://www.telegraph.co.uk/instagram/&quot;&gt;Instagram&lt;/a&gt; was positive in terms of self-expression and self-identity.&lt;/p&gt;
&lt;p&gt;Respondents were asked to score how each of the social media platforms they use impact upon issues such as anxiety, loneliness and community building.&lt;/p&gt;
&lt;p&gt;The site with the most positive rating was YouTube, followed by Twitter. Facebook and Snapchat came third and fourth respectively.&lt;/p&gt;
&lt;p&gt;The Royal Society for Public Health (RSPH) report said: &quot;The platforms that are supposed to help young people connect with each other may actually be fuelling a mental health crisis.&quot;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;div class=&quot;articleBodyText section&quot;&gt;
&lt;div class=&quot;article-body-text component&quot;&gt;
&lt;div class=&quot;component-content&quot;&gt;
&lt;p&gt;&lt;span class=&quot;m_first-letter m_first-letter--flagged&quot;&gt;R&lt;/span&gt;ecommendations included introducing pop-ups on sites such as Twitter and Facebook warning users about heavy usage - which the RSPH said was supported by seven in 10 people surveyed - and social media platforms discreetly signposting help to those potentially suffering from mental health issues.&lt;/p&gt;
&lt;p&gt;Shirley Cramer CBE, chief executive of the RSPH, said: &quot;Social media has been described as more addictive than cigarettes and alcohol, and is now so entrenched in the lives of young people that it is no longer possible to ignore it when talking about young people's mental health issues.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;articleBodyText section&quot;&gt;
&lt;div class=&quot;article-body-text component&quot;&gt;
&lt;div class=&quot;component-content&quot;&gt;
&lt;p&gt;&quot;It's interesting to see Instagram and Snapchat ranking as the worst for mental health and wellbeing - both platforms are very image-focused and it appears they may be driving feelings of inadequacy and anxiety in young people.&lt;/p&gt;
&lt;p&gt;&quot;As the evidence grows that there may be potential harms from heavy use of social media, and as we upgrade the status of mental health within society, it is important that we have checks and balances in place to make social media less of a wild west when it comes to young people's mental health and wellbeing.&quot;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;articleBodyText section&quot;&gt;
&lt;div class=&quot;article-body-text component&quot;&gt;
&lt;div class=&quot;component-content&quot;&gt;
&lt;p&gt;&lt;span class=&quot;m_first-letter m_first-letter--flagged&quot;&gt;D&lt;/span&gt;r Becky Inkster, honorary research fellow at the University of Cambridge, said: &quot;Young people sometimes feel more comfortable talking about personal issues online.&lt;/p&gt;
&lt;p&gt;&quot;As health professionals, we must make every attempt to understand modern youth culture expressions, lexicons, and terms to better connect with their thoughts and feelings.&quot;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Mon, 09 Apr 2018 16:37:06 +0000</pubDate>
<dc:creator>startupflix</dc:creator>
<og:title>Instagram ranked worst social network for young people's mental health</og:title>
<og:description>Instagram is the worst social media site in terms of its impact on the mental health of young people, a report has suggested.</og:description>
<og:type>article</og:type>
<og:url>https://www.telegraph.co.uk/news/2017/05/19/instagram-ranked-worst-social-network-young-peoples-mental-health/</og:url>
<og:image>https://www.telegraph.co.uk/content/dam/news/2017/05/19/TELEMMGLPICT000129163807-xlarge_trans_NvBQzQNjv4Bq1e80u7Jh6k2vK8fLT-Us3aHb20uzI0lT6a4n8MXSBJ8.jpeg</og:image>
<dc:language>en-GB</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.telegraph.co.uk/news/2017/05/19/instagram-ranked-worst-social-network-young-peoples-mental-health/</dc:identifier>
</item>
</channel>
</rss>