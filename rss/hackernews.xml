<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Tech Start-Up Fires Engineers Amid Union Organizing Effort</title>
<link>https://www.bna.com/tech-startup-fires-n73014474793/</link>
<guid isPermaLink="true" >https://www.bna.com/tech-startup-fires-n73014474793/</guid>
<description>&lt;p&gt;By &lt;a href=&quot;mailto:hassankanu@bloomberglaw.com&quot;&gt;Hassan A. Kanu&lt;/a&gt; (Bloomberg Law) and Josh Eidelson (Bloomberg)&lt;/p&gt;
&lt;p&gt;A group of Lanetix Inc. software engineers in San Francisco and Washington, D.C., were laid off for trying to join a union, according to organizers working with the group and a complaint obtained by Bloomberg Law.&lt;/p&gt;
&lt;p&gt;One of the engineers was fired earlier in the year for “participating in group discussions on an internal” Slack instant messaging group, the Communication Workers of America said in the National Labor Relations Board complaint. The remaining 14 were laid off Jan. 26.&lt;/p&gt;
&lt;p&gt;Engineers in the San Francisco office were informed of the decision after being called into an impromptu meeting. “By the time they left that meeting their computers were gone,” Melinda Fiedler, a CWA organizer working with the group, told Bloomberg Law Jan. 30.&lt;/p&gt;
&lt;p&gt;The move came less than two weeks after the workers filed a petition to join a CWA unit and days before a union election hearing scheduled for Jan 31. The workers said the company told them the layoffs were due to lackluster fourth quarter performance last year, Fiedler said.&lt;/p&gt;
&lt;p&gt;Lanetix didn’t respond to Bloomberg Law’s multiple requests for comment.&lt;/p&gt;
&lt;p&gt;“They said they were looking at moving their engineering operations overseas,” Cet Parks, an executive director for a D.C.-based unit of the CWA, told Bloomberg Law. Parks said the company’s timing and explanation for the terminations “seems kind of crazy with the tax cut and everything—a lot of companies are trying to share and they’re doing the exact opposite of what the cuts were supposed to be for.”&lt;/p&gt;
&lt;p&gt;Employees who inquired were told the company was moving engineering operations to contract workers in Eastern Europe, according to the CWA representatives. Lanetix had engineers and a supervisor working out of an an office in Arlington, Va., in the metropolitan Washington, D.C., area, but the company recently took that location off its website and added “Eastern Europe (coming soon),” Fiedler told Bloomberg Law.&lt;br/&gt;&lt;/p&gt;
&lt;h2&gt;Alleged Threats After Organizing Conversations&lt;/h2&gt;
&lt;p&gt;Outsourcing jobs abroad for the purpose of punishing or discouraging union organizing, as opposed to a valid business reason, would violate the National Labor Relations Act.&lt;/p&gt;
&lt;p&gt;“The issue that typically would come up is the employer would claim it had some legitimate business reason, or maybe it would claim we were planning to do it all along,” Wilma Liebman, who chaired the NLRB under President Obama, told Bloomberg Law.&lt;/p&gt;
&lt;p&gt;Even if the company had intended to eventually move the work to Europe, it may have violated federal labor law by speeding up the process in response to the union drive, Liebman said.&lt;/p&gt;
&lt;p&gt;The union alleged in the complaint, which was filed with the NLRB Jan. 29, that the company threatened the software engineers after they began discussing unionizing in an internal instant messaging group. Lanetix later terminated “all engineers and senior engineers in retaliation for demanding recognition,” CWA said.&lt;/p&gt;
&lt;p&gt;Businesses often fight organizing campaigns in an effort to retain more control of a workplace and the terms of employment when workers seek to organize. Union officials alleged that the recent decision to shut down DNAInfo, Gothamist, and a number of affiliated news publications late last year was in reaction to a union organizing drive. DNAInfo’s owners said financial struggles caused them to shut the doors.&lt;/p&gt;
&lt;p&gt;“We believe one of the most anti-unions things you can do is to let all employees go,” Parks said. “We’re going to fight as hard as we can.”&lt;/p&gt;
&lt;p&gt;The Lanetix engineers were seeking to organize under CWA’s Washington-Baltimore News Guild banner. The Guild represents Bloomberg Law reporters and other staff members.&lt;br/&gt;&lt;/p&gt;
&lt;h2&gt;Case Could Test New Board Prosecutor&lt;/h2&gt;
&lt;p&gt;A regional NLRB office is likely to get the first look at the Lanetix complaint, but it may ultimately be up to NLRB General Counsel Peter Robb’s (R) office to decide what to do with it.&lt;/p&gt;
&lt;p&gt;The remedy for violations like those CWA is alleging often includes ordering that the outsourcing be canceled and that the workers be reinstated. The Lanetix dispute is the kind of case in which a general counsel would often be asked to seek a temporary court injunction, Liebman said.&lt;/p&gt;
&lt;p&gt;“If jobs are about to be moved away, particularly if they’re about to be moved outside the country, you want to act fast to prevent that.”&lt;/p&gt;
&lt;p&gt;Liebman said the case “will be a test” for Robb, a former management attorney who was appointed by President Donald Trump and confirmed by the Senate last fall. Worker advocates will likely be watching whether the labor board’s top prosecutor is willing to intervene in order to keep U.S. jobs from being outsourced.&lt;/p&gt;
&lt;p&gt;“That typically has been one of the measures of how committed a board or a general counsel is to rigorous enforcement of the law,” said Liebman. “Certainly the case is one overall which will be a reflection of the views of this general counsel about the statute.”&lt;/p&gt;
</description>
<pubDate>Wed, 31 Jan 2018 03:10:17 +0000</pubDate>
<dc:creator>thinkpad20</dc:creator>
<og:title>Tech Start-Up Fires Engineers Amid Union Organizing Effort</og:title>
<og:description>A group of Lanetix Inc. software engineers in San Francisco and Washington, D.C., were laid off for trying to join a union, according to organizers working with the group and a complaint obtained by Bloomberg Law.</og:description>
<og:url>https://www.bna.com/tech-startup-fires-n73014474793/?_escaped_fragment_=</og:url>
<og:image>https://www.bna.com?_escaped_fragment_=/uploadedImages/BNA_V2/Company/BnaSocialShare.jpg</og:image>
<og:type>website</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bna.com/tech-startup-fires-n73014474793/?_escaped_fragment_=</dc:identifier>
</item>
<item>
<title>Content-aware image resize library</title>
<link>https://github.com/esimov/caire</link>
<guid isPermaLink="true" >https://github.com/esimov/caire</guid>
<description>&lt;h3&gt;README.md&lt;/h3&gt;
&lt;article class=&quot;markdown-body entry-content&quot; itemprop=&quot;text&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://travis-ci.org/esimov/caire&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://camo.githubusercontent.com/7a7da78f87faa3539a1e40a8ac9c7e7b1ba47e67/68747470733a2f2f7472617669732d63692e6f72672f6573696d6f762f63616972652e7376673f6272616e63683d6d6173746572&quot; alt=&quot;Build Status&quot; data-canonical-src=&quot;https://travis-ci.org/esimov/caire.svg?branch=master&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Caire&lt;/strong&gt; is a content aware image resize library based on &lt;em&gt;&lt;a href=&quot;https://inst.eecs.berkeley.edu/%7Ecs194-26/fa16/hw/proj4-seamcarving/imret.pdf&quot; rel=&quot;nofollow&quot;&gt;Seam Carving for Content-Aware Image Resizing&lt;/a&gt;&lt;/em&gt; paper.&lt;/p&gt;
&lt;h3&gt;How does it works&lt;/h3&gt;
&lt;ul&gt;&lt;li&gt;An energy map (edge detection) is generated from the provided image.&lt;/li&gt;
&lt;li&gt;The algorithm tries to find the least important parts of the image taking into account the lowest energy values.&lt;/li&gt;
&lt;li&gt;Using a dynamic programming approach the algorithm will generate individual seams accrossing the image from top to down, or from left to right (depending on the horizontal or vertical resizing) and will allocate for each seam a custom value, the least important pixels having the lowest energy cost and the most important ones having the highest cost.&lt;/li&gt;
&lt;li&gt;Traverse the image from the second row to the last row and compute the cumulative minimum energy for all possible connected seams for each entry.&lt;/li&gt;
&lt;li&gt;The minimum energy level is calculated by summing up the current pixel with the lowest value of the neighboring pixels from the previous row.&lt;/li&gt;
&lt;li&gt;Traverse the image from top to bottom and compute the minimum energy level. For each pixel in a row we compute the energy of the current pixel plus the energy of one of the three possible pixels above it.&lt;/li&gt;
&lt;li&gt;Find the lowest cost seam from the energy matrix starting from the last row and remove it.&lt;/li&gt;
&lt;li&gt;Repeat the process.&lt;/li&gt;
&lt;/ul&gt;&lt;h4&gt;The process illustrated:&lt;/h4&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;center&quot;&gt;Original image&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;Energy map&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;Seams applied&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://user-images.githubusercontent.com/883386/35481925-de130752-0435-11e8-9246-3950679b4fd6.jpg&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/883386/35481925-de130752-0435-11e8-9246-3950679b4fd6.jpg&quot; alt=&quot;original&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://user-images.githubusercontent.com/883386/35481899-5d5096ca-0435-11e8-9f9b-a84fefc06470.jpg&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/883386/35481899-5d5096ca-0435-11e8-9f9b-a84fefc06470.jpg&quot; alt=&quot;sobel&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://user-images.githubusercontent.com/883386/35481949-5c74dcb0-0436-11e8-97db-a6169cb150ca.jpg&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/883386/35481949-5c74dcb0-0436-11e8-97db-a6169cb150ca.jpg&quot; alt=&quot;debug&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;Features&lt;/h2&gt;
&lt;p&gt;Key features which differentiates from the other existing open source solutions:&lt;/p&gt;
&lt;h3&gt;To Do&lt;/h3&gt;
&lt;h2&gt;Install&lt;/h2&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
$ go get github.com/esimov/caire/cmd/caire
$ go install
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Usage&lt;/h2&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
$ caire -in input.jpg -out output.jpg
&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Supported commands:&lt;/h3&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
$ caire --help
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The following flags are supported:&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Flag&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;in&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;n/a&lt;/td&gt;
&lt;td&gt;Input file&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;out&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;n/a&lt;/td&gt;
&lt;td&gt;Output file&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;width&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;n/a&lt;/td&gt;
&lt;td&gt;New width&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;height&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;n/a&lt;/td&gt;
&lt;td&gt;New height&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;perc&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;Reduce image by percentage&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;blur&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Blur radius&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;sobel&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;Sobel filter threshold&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;debug&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;Use debugger&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;In case you wish to reduce the image size by a specific percentage, it can be used the &lt;code&gt;-perc&lt;/code&gt; boolean flag, which means you need only to specify that you want to deal with percentages instead of concrete values. In this case the values provided for width and height will be expressed as percentage. Here is a sample command using &lt;code&gt;-perc&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
caire -in input/source.jpg -out ./out.jpg -perc=1 -width 20 -height 20 -debug &lt;span class=&quot;pl-c1&quot;&gt;false&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;which reduces the image width &amp;amp; height by 20%.&lt;/p&gt;
&lt;p&gt;The CLI command can process all the images from a specific directory too.&lt;/p&gt;
&lt;div class=&quot;highlight highlight-source-shell&quot;&gt;
&lt;pre&gt;
$ caire -in ./input-directory -out ./output-directory
&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Sample images&lt;/h2&gt;
&lt;h4&gt;Shrinked images&lt;/h4&gt;
&lt;h4&gt;Enlarged images&lt;/h4&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Original&lt;/th&gt;
&lt;th&gt;Extended&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&quot;https://user-images.githubusercontent.com/883386/35498662-e11853c4-04d7-11e8-98d7-fcdb27207362.jpg&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/883386/35498662-e11853c4-04d7-11e8-98d7-fcdb27207362.jpg&quot; alt=&quot;scotland&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://user-images.githubusercontent.com/883386/35498559-87eb6426-04d7-11e8-825c-2dd2abdfc112.jpg&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/883386/35498559-87eb6426-04d7-11e8-825c-2dd2abdfc112.jpg&quot; alt=&quot;scotland&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;a href=&quot;https://user-images.githubusercontent.com/883386/35498466-1375b88a-04d7-11e8-8f8e-9d202da6a6b3.jpg&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/883386/35498466-1375b88a-04d7-11e8-8f8e-9d202da6a6b3.jpg&quot; alt=&quot;dubai&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://user-images.githubusercontent.com/883386/35498827-8cee502c-04d8-11e8-8449-05805f196d60.jpg&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/883386/35498827-8cee502c-04d8-11e8-8449-05805f196d60.jpg&quot; alt=&quot;dubai&quot;/&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3&gt;Useful resources&lt;/h3&gt;
&lt;h2&gt;License&lt;/h2&gt;
&lt;p&gt;This project is under the MIT License. See the LICENSE file for the full license text.&lt;/p&gt;
&lt;/article&gt;</description>
<pubDate>Tue, 30 Jan 2018 23:17:00 +0000</pubDate>
<dc:creator>boyter</dc:creator>
<og:image>https://avatars0.githubusercontent.com/u/883386?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>esimov/caire</og:title>
<og:url>https://github.com/esimov/caire</og:url>
<og:description>caire - Content aware image resize library</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/esimov/caire</dc:identifier>
</item>
<item>
<title>Red Hat to Acquire CoreOS</title>
<link>https://www.redhat.com/en/about/press-releases/red-hat-acquire-coreos-expanding-its-kubernetes-and-containers-leadership</link>
<guid isPermaLink="true" >https://www.redhat.com/en/about/press-releases/red-hat-acquire-coreos-expanding-its-kubernetes-and-containers-leadership</guid>
<description>&lt;p&gt;&lt;span class=&quot;location&quot;&gt;RALEIGH, N.C.&lt;/span&gt; — &lt;span class=&quot;dtstart&quot;&gt;&lt;time datetime=&quot;2018-01-30T00:00:00-05:00&quot;&gt;&lt;span class=&quot;date-display-single&quot;&gt;January 30, 2018&lt;/span&gt;&lt;/time&gt;&lt;/span&gt; —&lt;/p&gt;&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;Red Hat, Inc. (NYSE: RHT), the world's leading provider of open source solutions, today announced that it has signed a definitive agreement to acquire CoreOS, Inc., an innovator and leader in Kubernetes and container-native solutions, for a purchase price of $250 million, subject to certain adjustments at closing that are not expected to be material. Red Hat’s acquisition of CoreOS will further its vision of enabling customers to build any application and deploy them in any environment with the flexibility afforded by open source. By combining CoreOS’s complementary capabilities with Red Hat’s already broad Kubernetes and container-based portfolio, including Red Hat OpenShift, Red Hat aims to further accelerate adoption and development of the industry’s leading hybrid cloud platform for modern application workloads.&lt;/span&gt;&lt;/p&gt;
&lt;blockquote class=&quot;pr-pullquote&quot; readability=&quot;6&quot;&gt;
&lt;p&gt;We believe this acquisition cements Red Hat as a cornerstone of hybrid cloud and modern app deployments.&lt;/p&gt;
&lt;footer&gt;&lt;span class=&quot;pr-pullquote-name&quot;&gt;Paul Cormier&lt;/span&gt;&lt;span class=&quot;pr-pullquote-title&quot;&gt;president, Products and Technologies, Red Hat&lt;/span&gt;&lt;/footer&gt;&lt;/blockquote&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;As applications move to hybrid and multicloud environments, a growing number of organizations are using containers to more easily build, deploy and move applications to, from, and across clouds. IDC noted&lt;/span&gt;, “Substantial advances in cloud adoption, simplification, and portability are underway. The demand for cloud continues to grow, and enterprises now anticipate that cloud architecture will dominate their spending for the next several years. With the growing sophistication of containers, customers are looking to their application platform providers to help them use containers to transition and extend existing production applications to be useful in public or private cloud.”&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;Founded in 2013, CoreOS was created with a goal of building and delivering infrastructure for organizations of all sizes that mirrored that of large-scale software companies, automatically updating and patching servers and helping to solve pain points like downtime, security and resilience. Since its early work to popularize lightweight Linux operating systems optimized for containers, CoreOS has become well-regarded as a leader behind award-winning technologies that are enabling the broad adoption of scalable and resilient containerized applications.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;CoreOS is the creator of CoreOS Tectonic, an enterprise-ready Kubernetes platform that provides automated operations, enables portability across private and public cloud providers, and is based on open source software. It also offers CoreOS Quay, an enterprise-ready container registry. CoreOS is also well-known for helping to drive many of the open source innovations that are at the heart of containerized applications, including Kubernetes, where it is a leading contributor; Container Linux, a lightweight Linux distribution created and maintained by CoreOS that automates software updates and is streamlined for running containers; etcd, the distributed data store for Kubernetes; and rkt, an application container engine, donated to the Cloud Native Computing Foundation (CNCF), that helped drive the current Open Container Initiative (OCI) standard.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;Red Hat was early to embrace containers and container orchestration and has contributed deeply to related open source communities, including Kubernetes, where it is the second-leading contributor behind only Google. Red Hat is also a leader in enabling organizations around the world to embrace container-based applications, including its work on Red Hat OpenShift, the industry’s most comprehensive enterprise Kubernetes platform. Now with the combination of Red Hat and CoreOS, Red Hat amplifies its leadership in both upstream community and enterprise container-based solutions.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;The transaction is expected to have no material impact to Red Hat's guidance for its fourth fiscal quarter or fiscal year ending Feb. 28, 2018.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;The transaction is expected to close in January 2018, subject to customary closing conditions.&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;Read more about the acquisition on Red Hat’s website via the Red Hat blog.&lt;/span&gt;&lt;/p&gt;
&lt;h2 dir=&quot;ltr&quot;&gt;&lt;span&gt;Supporting Quotes&lt;/span&gt;&lt;/h2&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;em&gt;&lt;span&gt;Paul Cormier, president, Products&lt;/span&gt;&lt;/em&gt; &lt;span&gt;&lt;em&gt;and&lt;/em&gt;&lt;/span&gt; &lt;em&gt;&lt;span&gt;Technologies, Red Hat&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;“The next era of technology is being driven by container-based applications that span multi- and hybrid cloud environments, including physical, virtual, private cloud and public cloud platforms. Kubernetes, containers and Linux are at the heart of this transformation, and, like Red Hat, CoreOS has been a leader in both the upstream open source communities that are fueling these innovations and its work to bring enterprise-grade Kubernetes to customers. We believe this acquisition cements Red Hat as a cornerstone of hybrid cloud and modern app deployments.”&lt;/span&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;em&gt;&lt;span&gt;Alex Polvi, CEO, CoreOS&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p dir=&quot;ltr&quot;&gt;&lt;span&gt;“Red Hat and CoreOS’s relationship began many years ago as open source collaborators developing some of the key innovations in containers and distributed systems, helping to make automated operations a reality. This announcement marks a new stage in our shared aim to make these important technologies ubiquitous in business and the world. Thank you to the CoreOS family, our customers, partners, and most of all, the free software community for supporting us in our mission to make the internet more secure through automated operations.”&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Tue, 30 Jan 2018 21:23:01 +0000</pubDate>
<dc:creator>noahl</dc:creator>
<og:image>https://www.redhat.com/profiles/rh/themes/redhatdotcom/img/Red_Hat_RGB.jpg</og:image>
<og:url>https://www.redhat.com/en/about/press-releases/red-hat-acquire-coreos-expanding-its-kubernetes-and-containers-leadership</og:url>
<og:title>Red Hat to Acquire CoreOS, Expanding its Kubernetes and Containers Leadership</og:title>
<og:description>With the combination of Red Hat and CoreOS, Red Hat amplifies its leadership in both upstream community and enterprise container-based solutions.</og:description>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.redhat.com/en/about/press-releases/red-hat-acquire-coreos-expanding-its-kubernetes-and-containers-leadership</dc:identifier>
</item>
<item>
<title>Facebook is banning all ads promoting cryptocurrencies</title>
<link>https://www.recode.net/2018/1/30/16950926/facebook-mark-zuckerberg-bans-crypto-advertising-bitcoin-james-altucher</link>
<guid isPermaLink="true" >https://www.recode.net/2018/1/30/16950926/facebook-mark-zuckerberg-bans-crypto-advertising-bitcoin-james-altucher</guid>
<description>&lt;p id=&quot;Ki2Pb3&quot;&gt;Facebook is banning all ads that promote cryptocurrencies, including bitcoin, in an effort to prevent people from advertising what the company is calling “financial products and services frequently associated with misleading or deceptive promotional practices.”&lt;/p&gt;
&lt;p id=&quot;QUdmND&quot;&gt;That means no advertiser — even those that operate legal, legitimate businesses — will be able to promote things like bitcoin and other cryptocurrencies, initial coin offerings — &lt;a href=&quot;https://www.recode.net/2017/9/19/16243110/initial-coin-offering-ico-explained-what-is-money-bitcoin-digital-currency&quot;&gt;ICOs&lt;/a&gt; for short — or binary options, according to a Facebook &lt;a href=&quot;https://www.facebook.com/business/news/new-ads-policy-improving-integrity-and-security-of-financial-product-and-services-ads&quot;&gt;blog&lt;/a&gt; post.&lt;/p&gt;
&lt;div class=&quot;c-float-right&quot;&gt;&lt;span class=&quot;e-image__inner&quot;&gt;&lt;span class=&quot;e-image__image&quot; data-original=&quot;https://cdn.vox-cdn.com/uploads/chorus_asset/file/10123885/IMG_1052.jpeg&quot;&gt;&lt;img class=&quot;c-dynamic-image&quot; src=&quot;data:image/gif;base64,R0lGODlhAQABAIAAAAUEBAAAACwAAAAAAQABAAACAkQBADs&quot; alt=&quot;A James Altucher crypto ad about bitcoin, delivered by Facebook’s Audience Network&quot; data-chorus-optimize-field=&quot;main_image&quot; data-cid=&quot;site/dynamic_size_image-1517364309_7434_71262&quot; data-cdata=&quot;{&amp;quot;asset_id&amp;quot;:10123885,&amp;quot;ratio&amp;quot;:&amp;quot;*&amp;quot;}&quot;/&gt;&lt;noscript&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.vox-cdn.com/uploads/chorus_asset/file/10123885/IMG_1052.jpeg&quot; alt=&quot;A James Altucher crypto ad about bitcoin, delivered by Facebook’s Audience Network&quot;/&gt;&lt;/p&gt;
&lt;/noscript&gt;&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;e-image__meta&quot;&gt;A James Altucher crypto ad, delivered by Facebook’s Audience Network &lt;cite&gt;Peter Kafka&lt;/cite&gt;&lt;/span&gt;&lt;/div&gt;
&lt;p id=&quot;9tK7gR&quot;&gt;That also means that &lt;a href=&quot;https://www.wired.com/story/a-debate-about-bitcoin-that-was-a-debate-about-nothing/&quot;&gt;“crypto-genius” James Altucher&lt;/a&gt;, whose ads have appeared all over the internet and have become a meme of sorts for the entire crypto industry, won’t be able to advertise on Facebook.&lt;/p&gt;
&lt;p id=&quot;thdlqU&quot;&gt;Ads that violate the company’s new policy will be banned on Facebook’s core app, but also in other places where Facebook sells ads, including Instagram and its ad network, Audience Network, which places ads on third-party apps.&lt;/p&gt;
&lt;p id=&quot;pUQW2h&quot;&gt;“This policy is intentionally broad while we work to better detect deceptive and misleading advertising practices,” wrote Rob Leathern, one of Facebook’s ad tech directors. “We will revisit this policy and how we enforce it as our signals improve.”&lt;/p&gt;
&lt;p id=&quot;FKnwMw&quot;&gt;The cryptocurrency boom/bubble has &lt;a href=&quot;https://www.buzzfeed.com/ryanmac/cryptocurrency-scammers-are-running-wild-on-telegram?utm_term=.wc6qMVdVNn#.asPX29Q9v4&quot;&gt;led to scams&lt;/a&gt; and wild price fluctuations that have cost a lot of people — including unsophisticated investors — a lot of money. Scams are illegal, but gambling on investments you don’t understand is not.&lt;/p&gt;
&lt;p id=&quot;TjA7aK&quot;&gt;Look for blowback from entrepreneurs and investors who argue that the move unfairly punishes legitimate cryptocurrency companies and related crypto products. Facebook’s board of directors includes two investors — &lt;a href=&quot;https://dealbook.nytimes.com/2014/01/21/why-bitcoin-matters/&quot;&gt;Marc Andreessen&lt;/a&gt; and &lt;a href=&quot;https://www.wsj.com/articles/peter-thiels-founders-fund-makes-big-bet-on-bitcoin-1514917433&quot;&gt;Peter Thiel&lt;/a&gt; — whose firms have been prominent crypto backers. Facebook Messenger boss, David Marcus, is also on the board at the popular crypto exchange Coinbase.&lt;/p&gt;
&lt;hr class=&quot;p-entry-hr&quot; id=&quot;EWOTSY&quot;/&gt;&lt;aside id=&quot;HhGzUV&quot; readability=&quot;1.4192307692308&quot;&gt;
&lt;/aside&gt;</description>
<pubDate>Tue, 30 Jan 2018 19:25:59 +0000</pubDate>
<dc:creator>imartin2k</dc:creator>
<og:description>It’s an &quot;intentionally broad&quot; policy aimed at stopping scammers.</og:description>
<og:image>https://cdn.vox-cdn.com/thumbor/Er3Q8tdEv0KVaDA40pxeLTLy4ho=/0x18:4726x2492/fit-in/1200x630/cdn.vox-cdn.com/uploads/chorus_asset/file/10123961/909560166.jpg.jpg</og:image>
<og:title>Facebook is banning all ads promoting cryptocurrencies — including bitcoin and ICOs</og:title>
<og:type>article</og:type>
<og:url>https://www.recode.net/2018/1/30/16950926/facebook-mark-zuckerberg-bans-crypto-advertising-bitcoin-james-altucher</og:url>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.recode.net/2018/1/30/16950926/facebook-mark-zuckerberg-bans-crypto-advertising-bitcoin-james-altucher</dc:identifier>
</item>
<item>
<title>U.S. Regulators to Subpoena Crypto Exchange Bitfinex, Tether</title>
<link>https://www.bloomberg.com/news/articles/2018-01-30/crypto-exchange-bitfinex-tether-said-to-get-subpoenaed-by-cftc</link>
<guid isPermaLink="true" >https://www.bloomberg.com/news/articles/2018-01-30/crypto-exchange-bitfinex-tether-said-to-get-subpoenaed-by-cftc</guid>
<description>&lt;p&gt;U.S. regulators are scrutinizing one of the world’s largest cryptocurrency exchanges as questions mount over a digital token linked to its backers.&lt;/p&gt;


&lt;p&gt;The U.S. Commodity Futures Trading Commission sent subpoenas on Dec. 6 to virtual-currency venue Bitfinex and Tether, a company that issues a widely traded coin and claims it’s pegged to the dollar, according to a person familiar with the matter, who asked not to be identified discussing private information. The firms share the same chief executive officer.&lt;/p&gt;


&lt;p&gt;Tether’s coins have become a popular substitute for dollars on cryptocurrency exchanges worldwide, with about $2.3 billion of the tokens outstanding as of Tuesday. While Tether has said all of its coins are backed by U.S. dollars held in reserve, the company has yet to provide conclusive evidence of its holdings to the public or have its accounts audited. Skeptics have questioned whether the money is really there.&lt;/p&gt;



&lt;p&gt;“We routinely receive legal process from law enforcement agents and regulators conducting investigations,” Bitfinex and Tether said Tuesday in an emailed statement. “It is our policy not to comment on any such requests.”&lt;/p&gt;


&lt;p&gt;Erica Richardson, a CFTC spokeswoman, declined to comment.&lt;/p&gt;
&lt;p&gt;Bitcoin, the biggest cryptocurrency by market value, tumbled 10 percent on Tuesday. It fell another 3.2 percent to $9,766.41 as of 9:19 a.m. in Hong Kong, according to composite pricing on Bloomberg. The virtual currency hasn’t closed below $10,000 since November.&lt;/p&gt;
&lt;p data-tout-type=&quot;story&quot;&gt;&lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.bloomberg.com/news/articles/2017-12-05/mystery-shrouds-tether-and-its-links-to-biggest-bitcoin-exchange&quot; title=&quot;Click for full story&quot; target=&quot;_blank&quot;&gt;See also: Mystery shrouds Tether and its links to Bitcoin exchange&lt;/a&gt;&lt;/p&gt;
&lt;aside class=&quot;inline-newsletter&quot; data-state=&quot;ready&quot;/&gt;&lt;p&gt;While Tether and Bitfinex don’t disclose on their websites or in public documents where they’re located or who’s in charge, Ronn Torossian, a spokesman for the firms, said in a Dec. 3 email that Jan Ludovicus van der Velde is the CEO of both. Phil Potter is a Tether director, according to documents -- dubbed the Paradise Papers -- recently leaked by the International Consortium of Investigative Journalists. He’s also the chief strategy officer at Bitfinex.&lt;/p&gt;

&lt;p&gt;Last year, &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.bloomberg.com/quote/WFC:US&quot; title=&quot;Company Primer&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Wells Fargo &amp;amp; Co.&lt;/a&gt; ended its role as a correspondent bank through which customers in the U.S. could send money to bank accounts held by Bitfinex and Tether in Taiwan. The firms sued the lender, but later withdrew the complaint. Torossian previously declined to identify the banks used by Bitfinex unless a non-disclosure agreement was signed, which Bloomberg News refused.&lt;/p&gt;
&lt;h3&gt;No Audit&lt;/h3&gt;
&lt;p&gt;While little public information exists about how tethers are created, market pricing suggests traders believe that each coin is worth $1. Trading the token for Bitcoin at Bitfinex has helped drive up Bitcoin prices, Barry Leybovich, a product manager at IPC Systems Inc. who creates risk and compliance products for financial institutions interested in blockchain applications, said last month.&lt;/p&gt;
&lt;p&gt;A &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://tether.to/wp-content/uploads/2017/09/Final-Tether-Consulting-Report-9-15-17_Redacted.pdf&quot; title=&quot;Tether Audit&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;document&lt;/a&gt; on Tether’s website, compiled by accounting firm Friedman LLP, shows it had $443 million and 1,590 euros ($1,970) in bank accounts as of Sept. 15. Tether tokens were valued at &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://coinmarketcap.com/currencies/tether/historical-data/?start=20170915&amp;amp;end=20170916&quot; title=&quot;CoinMarketCap Data on Tether&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;$420 million&lt;/a&gt; that day, according to Coinmarketcap.com. Tether hasn’t identified the banks where that money was held, and their names were blacked out in the document.&lt;/p&gt;
&lt;p&gt;Friedman said in its report that it didn’t investigate the reliability of Tether’s records. The accounting firm and Tether have recently cut ties, Tether said in a separate statement Monday.&lt;/p&gt;
&lt;p&gt;“Given the excruciatingly detailed procedures Friedman was undertaking for the relatively simple balance sheet of Tether, it became clear that an audit would be unattainable in a reasonable timeframe,” Tether said.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For more on cryptocurrencies, check out the &lt;em&gt;Decrypted&lt;/em&gt; podcast: &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Friedman didn’t reply to messages seeking comment.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;— With assistance by Todd White, and Peter Eichenbaum&lt;/em&gt;&lt;/p&gt;
</description>
<pubDate>Tue, 30 Jan 2018 18:09:15 +0000</pubDate>
<dc:creator>chollida1</dc:creator>
<og:description>U.S. regulators are scrutinizing one of the world’s largest cryptocurrency exchanges as questions mount over a digital token linked to its backers.</og:description>
<og:image>https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iiGziWyJfEic/v0/1200x961.png</og:image>
<og:title>U.S. Regulators Subpoena Crypto Exchange Bitfinex, Tether</og:title>
<og:type>article</og:type>
<og:url>https://www.bloomberg.com/news/articles/2018-01-30/crypto-exchange-bitfinex-tether-said-to-get-subpoenaed-by-cftc</og:url>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bloomberg.com/news/articles/2018-01-30/crypto-exchange-bitfinex-tether-said-to-get-subpoenaed-by-cftc</dc:identifier>
</item>
<item>
<title>Xi: an editor for the next 20 years [video]</title>
<link>https://www.recurse.com/events/localhost-raph-levien</link>
<guid isPermaLink="true" >https://www.recurse.com/events/localhost-raph-levien</guid>
<description>&lt;div readability=&quot;29&quot;&gt;
&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Xi is a project to build a modern text editor with uncompromising performance. Its thoroughly async, loosely coupled design promises performance and rich extensibility, but creates interesting engineering challenges, requiring advanced algorithms and data structures. In addition to pushing the state of computer science for text handling, the project also seeks to build an open-source community for teaching and learning, and working together to create a joyful editing experience.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;About Raph:&lt;/strong&gt; Raph is a software engineer at Google, currently on the Fuchsia team working on text and Rust infrastructure, and holds a PhD in Computer Science from UC Berkeley, where his thesis topic was tools for interactive font design. He has been active in the open source community for over 25 years, with contributions in text, 2D graphics, fonts, and other areas. Raph is also a Recurse Center alum, from the Fall 1, 2017 batch.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;public-article__aside public-article__aside--further-reading&quot;&gt;&lt;section class=&quot;other-links&quot; readability=&quot;2.1396396396396&quot;&gt;&lt;div class=&quot;other-links__inner font-weight-bold&quot; readability=&quot;8.5585585585586&quot;&gt;
&lt;p class=&quot;m-t-0&quot;&gt;This talk was part of our monthly &lt;a href=&quot;https://www.recurse.com/localhost&quot;&gt;Localhost series&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It was given on January 23rd, 2018 and hosted by Two Sigma.&lt;/p&gt;
&lt;/div&gt;
&lt;/section&gt;&lt;/aside&gt;</description>
<pubDate>Tue, 30 Jan 2018 17:43:40 +0000</pubDate>
<dc:creator>davidbalbert</dc:creator>
<og:type>website</og:type>
<og:title>Xi: an editor for the next 20 years - Recurse Center</og:title>
<og:description>The Recurse Center is a self-directed, community-driven educational retreat for programmers in New York City.</og:description>
<og:url>https://www.recurse.com/</og:url>
<og:image>https://d29xw0ra2h4o4u.cloudfront.net/assets/logo_square-051508b5ecf8868635aea567bb86f423f4d1786776e5dfce4adf2bc7edf05804.png</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.recurse.com/events/localhost-raph-levien</dc:identifier>
</item>
<item>
<title>Matrix Calculus for Deep Learning</title>
<link>http://parrt.cs.usfca.edu/doc/matrix-calculus/index.html</link>
<guid isPermaLink="true" >http://parrt.cs.usfca.edu/doc/matrix-calculus/index.html</guid>
<description>&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot; /&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400&quot; /&gt;&lt;title&gt;&lt;/title&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;1027.149501269&quot;&gt;


&lt;p&gt;&lt;a href=&quot;http://parrt.cs.usfca.edu&quot;&gt;Terence Parr&lt;/a&gt; and &lt;a href=&quot;http://www.fast.ai/about/#jeremy&quot;&gt;Jeremy Howard&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;(We teach in University of San Francisco's &lt;a href=&quot;https://www.usfca.edu/arts-sciences/graduate-programs/data-science&quot;&gt;MS in Data Science program&lt;/a&gt; and have other nefarious projects underway. You might know Terence as the creator of the &lt;a href=&quot;http://www.antlr.org&quot;&gt;ANTLR parser generator&lt;/a&gt;. For more material, see Jeremy's &lt;a href=&quot;http://course.fast.ai&quot;&gt;fast.ai courses&lt;/a&gt; and University of San Francisco's Data Institute &lt;a href=&quot;https://www.usfca.edu/data-institute/certificates/deep-learning-part-one&quot;&gt;in-person version of the deep learning course&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus.pdf&quot;&gt;Printable version&lt;/a&gt; (This HTML was generated from markup using &lt;a href=&quot;https://github.com/parrt/bookish&quot;&gt;bookish&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do &lt;strong&gt;not&lt;/strong&gt; need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don't worry if you get stuck at some point along the way---just go back and reread the previous section, and try writing down and working through some examples. And if you're still stuck, we're happy to answer your questions in the &lt;a href=&quot;http://forums.fast.ai/c/theory&quot;&gt;Theory category at forums.fast.ai&lt;/a&gt;. &lt;strong&gt;Note&lt;/strong&gt;: There is a &lt;a href=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/index.html#reference&quot;&gt;reference section&lt;/a&gt; at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here.&lt;/p&gt;

&lt;h2 id=&quot;intro&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Most of us last saw calculus in school, but derivatives are a critical part of machine learning, particularly deep neural networks, which are trained by optimizing a loss function. Pick up a machine learning paper or the documentation of a library such as &lt;a href=&quot;http://pytorch.org&quot;&gt;PyTorch&lt;/a&gt; and calculus comes screeching back into your life like distant relatives around the holidays. And it's not just any old scalar calculus that pops up---you need differential &lt;em&gt;matrix calculus&lt;/em&gt;, the shotgun wedding of &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_algebra&quot;&gt;linear algebra&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariable_calculus&quot;&gt;multivariate calculus&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Well... maybe &lt;em&gt;need&lt;/em&gt; isn't the right word; Jeremy's courses show how to become a world-class deep learning practitioner with only a minimal level of scalar calculus, thanks to leveraging the automatic differentiation built in to modern deep learning libraries. But if you really want to really understand what's going on under the hood of these libraries, and grok academic papers discussing the latest advances in model training techniques, you'll need to understand certain bits of the field of matrix calculus.&lt;/p&gt;
&lt;p&gt;For example, the activation of a single computation unit in a neural network is typically calculated using the dot product (from linear algebra) of an edge weight vector &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; with an input vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; plus a scalar bias (threshold): &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-EEDCFA4252D0992243A283CE0EB777A6-depth003.31.svg&quot; /&gt;. Function &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-C599D931407509E0FA08F8686B205B6D-depth003.25.svg&quot; /&gt; is called the unit's &lt;em&gt;affine function&lt;/em&gt; and is followed by a &lt;a href=&quot;https://goo.gl/7BXceK&quot;&gt;rectified linear unit&lt;/a&gt;, which clips negative values to zero: &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-66258AA93A4746DA10D306190271DE4B-depth003.25.svg&quot; /&gt;. Such a computational unit is sometimes referred to as an “artificial neuron” and looks like:&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/neuron.png&quot; alt=&quot;neuron.png&quot; width=&quot;250&quot; /&gt;&lt;/center&gt;
&lt;p&gt;Neural networks consist of many of these units, organized into multiple collections of neurons called &lt;em&gt;layers&lt;/em&gt;. The activation of one layer's units become the input to the next layer's units. The activation of the unit or units in the final layer is called the network output.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Training&lt;/em&gt; this neuron means choosing weights &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; and bias &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt; so that we get the desired output for all &lt;span class=&quot;eqn&quot;&gt;N&lt;/span&gt; inputs &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;. To do that, we minimize a &lt;em&gt;loss function&lt;/em&gt; that compares the network's final &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-7971D42A6C6C6A28D6443F0645E4A036-depth003.25.svg&quot; /&gt; with the &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-E146B831A1E53B95E4C63775285D62CF-depth003.25.svg&quot; /&gt; (desired output of &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;) for all input &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; vectors. To minimize the loss, we use some variation on gradient descent, such as plain &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;stochastic gradient descent&lt;/a&gt; (SGD), SGD with momentum, or &lt;a href=&quot;https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam&quot;&gt;Adam&lt;/a&gt;. All of those require the partial derivative (the gradient) of &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-7971D42A6C6C6A28D6443F0645E4A036-depth003.25.svg&quot; /&gt; with respect to the model parameters &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt;. Our goal is to gradually tweak &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt; so that the overall loss function keeps getting smaller across all &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; inputs.&lt;/p&gt;
&lt;p&gt;If we're careful, we can derive the gradient by differentiating the scalar version of a common loss function (mean squared error):&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-A129949CD1EF7BE2CA8BD424D34F9930.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;But this is just one neuron, and neural networks must train the weights and biases of all neurons in all layers simultaneously. Because there are multiple inputs and (potentially) multiple network outputs, we really need general rules for the derivative of a function with respect to a vector and even rules for the derivative of a vector-valued function with respect to a vector.&lt;/p&gt;
&lt;p&gt;This article walks through the derivation of some important rules for computing partial derivatives with respect to vectors, particularly those useful for training neural networks. This field is known as &lt;em&gt;matrix calculus&lt;/em&gt;, and the good news is, we only need a small subset of that field, which we introduce here. While there is a lot of online material on multivariate calculus and linear algebra, they are typically taught as two separate undergraduate courses so most material treats them in isolation. The pages that do discuss matrix calculus often are really just lists of rules with minimal explanation or are just pieces of the story. They also tend to be quite obscure to all but a narrow audience of mathematicians, thanks to their use of dense notation and minimal discussion of foundational concepts. (See the annotated list of resources at the end.)&lt;/p&gt;
&lt;p&gt;In contrast, we're going to rederive and rediscover some key matrix calculus rules in an effort to explain them. It turns out that matrix calculus is really not that hard! There aren't dozens of new rules to learn; just a couple of key concepts. Our hope is that this short paper will get you started quickly in the world of matrix calculus as it relates to training neural networks. We're assuming you're already familiar with the basics of neural network architecture and training. If you're not, head over to &lt;a href=&quot;http://course.fast.ai&quot;&gt;Jeremy's course&lt;/a&gt; and complete part 1 of that, then we'll see you back here when you're done. (Note that, unlike many more academic approaches, we strongly suggest &lt;em&gt;first&lt;/em&gt; learning to train and use neural networks in practice and &lt;em&gt;then&lt;/em&gt; study the underlying math. The math will be much more understandable with the context in place; besides, it's not necessary to grok all this calculus to become an effective practitioner.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A note on notation&lt;/em&gt;: Jeremy's course exclusively uses code, instead of math notation, to explain concepts since unfamiliar functions in code are easy to search for and experiment with. In this paper, we do the opposite: there is a lot of math notation because one of the goals of this paper is to help you understand the notation that you'll see in deep learning papers and books. At the &lt;a href=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/index.html#notation&quot;&gt;end of the paper&lt;/a&gt;, you'll find a brief table of the notation used, including a word or phrase you can use to search for more details.&lt;/p&gt;
&lt;h2 id=&quot;sec2&quot;&gt;Review: Scalar derivative rules&lt;/h2&gt;
&lt;p&gt;Hopefully you remember some of these main scalar derivative rules. If your memory is a bit fuzzy on this, have a look at &lt;a href=&quot;https://www.khanacademy.org/math/ap-calculus-ab/ab-derivative-rules&quot;&gt;Khan academy vid on scalar derivative rules&lt;/a&gt;.&lt;/p&gt;
&lt;center&gt;
&lt;/center&gt;
&lt;p&gt;There are other rules for trigonometry, exponentials, etc., which you can find at &lt;a href=&quot;https://www.khanacademy.org/math/differential-calculus&quot;&gt;Khan Academy differential calculus course&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When a function has a single parameter, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-50BBD36E1FD2333108437A2CA378BE62-depth003.25.svg&quot; /&gt;, you'll often see &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-6BDA8AF54C40BC23ED858E9E9F5C11D2-depth002.72.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-74CAF4D1EC90D3A36EA7C7BBFE65B516-depth003.25.svg&quot; /&gt; used as shorthands for &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-5BA9D16419154B1BDBECA39D99E8E809-depth004.58.svg&quot; /&gt;. We recommend against this notation as it does not make clear the variable we're taking the derivative with respect to.&lt;/p&gt;
&lt;p&gt;You can think of &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-624FEBB9A49A3FC96353C861D175C806-depth004.58.svg&quot; /&gt; as an operator that maps a function of one parameter to another function. That means that &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-EC0CEC5F9488EC510F8D688E7003222D-depth004.58.svg&quot; /&gt; maps &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-50BBD36E1FD2333108437A2CA378BE62-depth003.25.svg&quot; /&gt; to its derivative with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;, which is the same thing as &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-07A5EA519C4CEA1A3539E3A7FC289163-depth004.58.svg&quot; /&gt;. Also, if &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-FD91C508F91C2C84498680BD337C1D7A-depth003.25.svg&quot; /&gt;, then &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-B1ED3CF9BA4D6F25A5A4F481C45EC658-depth004.58.svg&quot; /&gt;. Thinking of the derivative as an operator helps to simplify complicated derivatives because the operator is distributive and lets us pull out constants. For example, in the following equation, we can pull out the constant 9 and distribute the derivative operator across the elements within the parentheses.&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-A1EC7F214318E08949CC8BFCED138D94.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;That procedure reduced the derivative of &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-FDFD125C741CD062B2CA779DDE0524BE-depth003.25.svg&quot; /&gt; to a bit of arithmetic and the derivatives of &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-32F5240D0DBF2CCBE75EF7F8EF2015E0-depth000.14.svg&quot; /&gt;, which are much easier to solve than the original derivative.&lt;/p&gt;
&lt;h2 id=&quot;sec3&quot;&gt;Introduction to vector calculus and partial derivatives&lt;/h2&gt;
&lt;p&gt;Neural network layers are not single functions of a single parameter, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-50BBD36E1FD2333108437A2CA378BE62-depth003.25.svg&quot; /&gt;. So, let's move on to functions of multiple parameters such as &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-3BAF1600AE50930A155F58AE172B51BD-depth003.25.svg&quot; /&gt;. For example, what is the derivative of &lt;span class=&quot;eqn&quot;&gt;xy&lt;/span&gt; (i.e., the multiplication of &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;)? In other words, how does the product &lt;span class=&quot;eqn&quot;&gt;xy&lt;/span&gt; change when we wiggle the variables? Well, it depends on whether we are changing &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; or &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;. We compute derivatives with respect to one variable (parameter) at a time, giving us two different &lt;em&gt;partial derivatives&lt;/em&gt; for this two-parameter function (one for &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; and one for &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;). Instead of using operator &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-624FEBB9A49A3FC96353C861D175C806-depth004.58.svg&quot; /&gt;, the partial derivative operator is &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-4D6C379F66675645B3FFE28A15306857-depth004.67.svg&quot; /&gt; (a stylized &lt;span class=&quot;eqn&quot;&gt;d&lt;/span&gt; and not the Greek letter &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-77A3B715842B45E440A5BEE15357AD29-depth000.22.svg&quot; /&gt;). So, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-08DCCFBE629A14FCCD9FB9A20F2E367C-depth004.67.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-1FDCF7A9F137AE48FA25EE34A69F8201-depth006.34.svg&quot; /&gt; are the partial derivatives of &lt;span class=&quot;eqn&quot;&gt;xy&lt;/span&gt;; often, these are just called the &lt;em&gt;partials&lt;/em&gt;. For functions of a single parameter, operator &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-4D6C379F66675645B3FFE28A15306857-depth004.67.svg&quot; /&gt; is equivalent to &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-624FEBB9A49A3FC96353C861D175C806-depth004.58.svg&quot; /&gt; (for sufficiently smooth functions). However, it's better to use &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-624FEBB9A49A3FC96353C861D175C806-depth004.58.svg&quot; /&gt; to make it clear you're referring to a scalar derivative.&lt;/p&gt;
&lt;p&gt;The partial derivative with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; is just the usual scalar derivative, simply treating any other variable in the equation as a constant. Consider function &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-D6DEAE7E403381C2C425D4B40CCA936E-depth003.25.svg&quot; /&gt;. The partial derivative with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; is written &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-F063B8EC812DF3D204F9327F5D094073-depth004.67.svg&quot; /&gt;. There are three constants from the perspective of &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-4D6C379F66675645B3FFE28A15306857-depth004.67.svg&quot; /&gt;: 3, 2, and &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;. Therefore, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-D981BA4BD14AC44C43A4E4E0EC750B4A-depth004.67.svg&quot; /&gt;. The partial derivative with respect to &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; treats &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; like a constant: &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-55A3A400FAD3326FEF1BB9DDD2658383-depth006.34.svg&quot; /&gt;. It's a good idea to derive these yourself before continuing otherwise the rest of the article won't make sense. Here's the &lt;a href=&quot;https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives&quot;&gt;Khan Academy video on partials&lt;/a&gt; if you need help.&lt;/p&gt;
&lt;p&gt;To make it clear we are doing vector calculus and not just multivariate calculus, let's consider what we do with the partial derivatives &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-84D799755A7F73945BD58B2E057121AB-depth004.67.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A2DE2EC029172B84A0A0E8A8D00F5A6F-depth006.34.svg&quot; /&gt; (another way to say &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-75645E70B6C95F7466C353E9C2306FE0-depth004.67.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-B20FF70C037320C2D0B710F4B592927E-depth006.34.svg&quot; /&gt;) that we computed for &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-D6DEAE7E403381C2C425D4B40CCA936E-depth003.25.svg&quot; /&gt;. Instead of having them just floating around and not organized in any way, let's organize them into a horizontal vector. We call this vector the &lt;em&gt;gradient&lt;/em&gt; of &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-3BAF1600AE50930A155F58AE172B51BD-depth003.25.svg&quot; /&gt; and write it as:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-0C95BB61B2BFFB0C2A95A9DC5D8AF44E.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;So the gradient of &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-3BAF1600AE50930A155F58AE172B51BD-depth003.25.svg&quot; /&gt; is simply a vector of its partials. Gradients are part of the vector calculus world, which deals with functions that map &lt;span class=&quot;eqn&quot;&gt;n&lt;/span&gt; scalar parameters to a single scalar. Now, let's get crazy and consider derivatives of multiple functions simultaneously.&lt;/p&gt;
&lt;h2 id=&quot;sec4&quot;&gt;Matrix calculus&lt;/h2&gt;
&lt;p&gt;When we move from derivatives of one function to derivatives of many functions, we move from the world of vector calculus to matrix calculus. Let's compute partial derivatives for two functions, both of which take two parameters. We can keep the same &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-D6DEAE7E403381C2C425D4B40CCA936E-depth003.25.svg&quot; /&gt; from the last section, but let's also bring in &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-D182AD2135D1E887AFFCA045F432B2CA-depth003.25.svg&quot; /&gt;. The gradient for &lt;span class=&quot;eqn&quot;&gt;g&lt;/span&gt; has two entries, a partial derivative for each parameter:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-FDB56AA8804E0D13E1555DB8E0E1AAEE.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-13AF8214DD5A2040D650C7B460C88129.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;giving us gradient &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-5E1F4483B06DEA88CD339E02F7980A74-depth003.25.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;Gradient vectors organize all of the partial derivatives for a specific scalar function. If we have two functions, we can also organize their gradients into a matrix by stacking the gradients. When we do so, we get the &lt;em&gt;Jacobian matrix&lt;/em&gt; (or just the &lt;em&gt;Jacobian&lt;/em&gt;) where the gradients are rows:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-5F27807D5093D7C953E3C0446EAAACD0.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Welcome to matrix calculus!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note that there are multiple ways to represent the Jacobian.&lt;/strong&gt; We are using the so-called &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions&quot;&gt;numerator layout&lt;/a&gt; but many papers and software will use the &lt;em&gt;denominator layout&lt;/em&gt;. This is just transpose of the numerator layout Jacobian (flip it around its diagonal):&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-2138AD5F37EB975B1AFC453BF23396E8.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;h3 id=&quot;sec4.1&quot;&gt;Generalization of the Jacobian&lt;/h3&gt;
&lt;p&gt;So far, we've looked at a specific example of a Jacobian matrix. To define the Jacobian matrix more generally, let's combine multiple parameters into a single vector argument: &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-8C56090E55CDB76D1CD0E738EBA7F164-depth003.25.svg&quot; /&gt;. (You will sometimes see notation &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-B07B5ABD67EE0B72F4136C82C68A0C48-depth000.14.svg&quot; /&gt; for vectors in the literature as well.) Lowercase letters in bold font such as &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; are vectors and those in italics font like &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; are scalars. &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is the &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-97361F12A3555FC4FC4E2FFCE1799AC3-depth000.14.svg&quot; /&gt; element of vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; and is in italics because a single vector element is a scalar. We also have to define an orientation for vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;. We'll assume that all vectors are vertical by default of size &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-88512AB12706879FEC83C0C3AA79931F-depth001.08.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-D76C868C669197F65B05E96473454834.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;With multiple scalar-valued functions, we can combine them all into a vector just like we did with the parameters. Let &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A16203A31AD7C6CC63FD297D522170F1-depth003.25.svg&quot; /&gt; be a vector of &lt;span class=&quot;eqn&quot;&gt;m&lt;/span&gt; scalar-valued functions that each take a vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; of length &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-E3114C625CDDDC18ED29BA629242BD65-depth003.25.svg&quot; /&gt; where &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-DEA8E196A572D082201CD5ABF2FA82DE-depth003.25.svg&quot; /&gt; is the cardinality (count) of elements in &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;. Each &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; function within &lt;span class=&quot;eqnvec&quot;&gt;f&lt;/span&gt; returns a scalar just as in the previous section:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-CD6121D27CD89157BF272E5E50AE32FE.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;For instance, we'd represent &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-D6DEAE7E403381C2C425D4B40CCA936E-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-D182AD2135D1E887AFFCA045F432B2CA-depth003.25.svg&quot; /&gt; from the last section as&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-AB738ABA2B35F37C4A171037A396E5F5.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;It's very often the case that &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A193EBC083B4745370F6F1343383D9CC-depth000.14.svg&quot; /&gt; because we will have a scalar function result for each element of the &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; vector. For example, consider the identity function &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-23225C9E5521B6A9777579BE4B92245C-depth003.25.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-4BAF672444FD71616154DE2BE79A5DD6.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;So we have &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A193EBC083B4745370F6F1343383D9CC-depth000.14.svg&quot; /&gt; functions and parameters, in this case. Generally speaking, though, the Jacobian matrix is the collection of all &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-FBFEB9C8459FEE5A2BD529C07B881153-depth001.08.svg&quot; /&gt; possible partial derivatives (&lt;span class=&quot;eqn&quot;&gt;m&lt;/span&gt; rows and &lt;span class=&quot;eqn&quot;&gt;n&lt;/span&gt; columns), which is the stack of &lt;span class=&quot;eqn&quot;&gt;m&lt;/span&gt; gradients with respect to &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-C6F45926C0FEAD3BD359AA24A7FB23A2.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Each &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-98F52E62B85A836D750F1CEDF32E1D68-depth004.67.svg&quot; /&gt; is a horizontal &lt;span class=&quot;eqn&quot;&gt;n&lt;/span&gt;-vector because the partial derivative is with respect to a vector, &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;, whose length is &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A921357AFE67E47CC9D1DB575BCE1B77-depth003.25.svg&quot; /&gt;. The width of the Jacobian is &lt;span class=&quot;eqn&quot;&gt;n&lt;/span&gt; if we're taking the partial derivative with respect to &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; because there are &lt;span class=&quot;eqn&quot;&gt;n&lt;/span&gt; parameters we can wiggle, each potentially changing the function's value. Therefore, the Jacobian is always &lt;span class=&quot;eqn&quot;&gt;m&lt;/span&gt; rows for &lt;span class=&quot;eqn&quot;&gt;m&lt;/span&gt; equations. It helps to think about the possible Jacobian shapes visually:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/latex-6793E76E433509E38529D4B70EB4D956.svg&quot; alt=&quot; \begin{tabular}{c|ccl} &amp;amp; \begin{tabular}[t]{c} scalar\ \framebox(18,18){$x$}\ \end{tabular} &amp;amp; \begin{tabular}{c} vector\ \framebox(18,40){$\mathbf{x}$} \end{tabular}\ \hline \\[\dimexpr-\normalbaselineskip+5pt] \begin{tabular}[b]{c} scalar\ \framebox(18,18){$f$}\ \end{tabular} &amp;amp;\framebox(18,18){$\frac{\partial f}{\partial {x}}$} &amp;amp; \framebox(40,18){$\frac{\partial f}{\partial {\mathbf{x}}}$}&amp;amp;\ \begin{tabular}[b]{c} vector\ \framebox(18,40){$\mathbf{f}$}\ \end{tabular} &amp;amp; \framebox(18,40){$\frac{\partial \mathbf{f}}{\partial {x}}$} &amp;amp; \framebox(40,40){$\frac{\partial \mathbf{f}}{\partial \mathbf{x}}$}\ \end{tabular} &quot; /&gt;&lt;/div&gt;
&lt;p&gt;The Jacobian of the identity function &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-3ABE4A0471143ABFC180C9FA485E5F0A-depth003.25.svg&quot; /&gt;, with &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-E314355F5E2135483279531C62D7E8EC-depth003.25.svg&quot; /&gt;, has &lt;span class=&quot;eqn&quot;&gt;n&lt;/span&gt; functions and each function has &lt;span class=&quot;eqn&quot;&gt;n&lt;/span&gt; parameters held in a single vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;. The Jacobian is, therefore, a square matrix since &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A193EBC083B4745370F6F1343383D9CC-depth000.14.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/latex-EAAB93715A5C885A1F867EE3C1A2931A.svg&quot; alt=&quot; \begin{eqnarray*} \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{bmatrix} \frac{\partial}{\partial {x}} f_1(\mathbf{x}) \ \frac{\partial}{\partial {x}} f_2(\mathbf{x})\ \ldots\ \frac{\partial}{\partial {x}} f_m(\mathbf{x}) \end{bmatrix} &amp;amp;=&amp;amp; \begin{bmatrix} \frac{\partial}{\partial {x_1}} f_1(\mathbf{x})~ \frac{\partial}{\partial {x_2}} f_1(\mathbf{x}) ~\ldots~ \frac{\partial}{\partial {x_n}} f_1(\mathbf{x}) \ \frac{\partial}{\partial {x_1}} f_2(\mathbf{x})~ \frac{\partial}{\partial {x_2}} f_2(\mathbf{x}) ~\ldots~ \frac{\partial}{\partial {x_n}} f_2(\mathbf{x}) \ \ldots\ ~\frac{\partial}{\partial {x_1}} f_m(\mathbf{x})~ \frac{\partial}{\partial {x_2}} f_m(\mathbf{x}) ~\ldots~ \frac{\partial}{\partial {x_n}} f_m(\mathbf{x}) \ \end{bmatrix}\\\ &amp;amp; = &amp;amp; \begin{bmatrix} \frac{\partial}{\partial {x_1}} x_1~ \frac{\partial}{\partial {x_2}} x_1 ~\ldots~ \frac{\partial}{\partial {x_n}} x_1 \ \frac{\partial}{\partial {x_1}} x_2~ \frac{\partial}{\partial {x_2}} x_2 ~\ldots~ \frac{\partial}{\partial {x_n}} x_2 \ \ldots\ ~\frac{\partial}{\partial {x_1}} x_n~ \frac{\partial}{\partial {x_2}} x_n ~\ldots~ \frac{\partial}{\partial {x_n}} x_n \ \end{bmatrix}\\\ &amp;amp; &amp;amp; (\text{and since } \frac{\partial}{\partial {x_j}} x_i = 0 \text{ for } j \neq i)\ &amp;amp; = &amp;amp; \begin{bmatrix} \frac{\partial}{\partial {x_1}} x_1 &amp;amp; 0 &amp;amp; \ldots&amp;amp; 0 \ 0 &amp;amp; \frac{\partial}{\partial {x_2}} x_2 &amp;amp;\ldots &amp;amp; 0 \ &amp;amp; &amp;amp; \ddots\ 0 &amp;amp; 0 &amp;amp;\ldots&amp;amp; \frac{\partial}{\partial {x_n}} x_n \ \end{bmatrix}\\\ &amp;amp; = &amp;amp; \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; \ldots&amp;amp; 0 \ 0 &amp;amp;1 &amp;amp;\ldots &amp;amp; 0 \ &amp;amp; &amp;amp; \ddots\ 0 &amp;amp; 0 &amp;amp; \ldots &amp;amp;1 \ \end{bmatrix}\\\ &amp;amp; = &amp;amp; I ~~~(I \text{ is the identity matrix with ones down the diagonal})\ \end{eqnarray*} &quot; /&gt;&lt;/div&gt;
&lt;p&gt;Make sure that you can derive each step above before moving on. If you get stuck, just consider each element of the matrix in isolation and apply the usual scalar derivative rules. That is a generally useful trick: Reduce vector expressions down to a set of scalar expressions and then take all of the partials, combining the results appropriately into vectors and matrices at the end.&lt;/p&gt;
&lt;p&gt;Also be careful to track whether a matrix is vertical, &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;, or horizontal, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-FAAEE783424BC0E27E9AA2F56A7B50B8-depth000.00.svg&quot; /&gt; where &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-FAAEE783424BC0E27E9AA2F56A7B50B8-depth000.00.svg&quot; /&gt; means &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; transpose. Also make sure you pay attention to whether something is a scalar-valued function, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-D3172B9A65679CB6EF09F17BE0918890-depth002.65.svg&quot; /&gt;, or a vector of functions (or a vector-valued function), &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-BCAEE673D10908E8197A79E9D4FB6249-depth002.33.svg&quot; /&gt;.&lt;/p&gt;
&lt;h3 id=&quot;sec4.2&quot;&gt;Derivatives of vector element-wise binary operators&lt;/h3&gt;
&lt;p&gt;Element-wise binary operations on vectors, such as vector addition &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-F26AC9FE4C8C9B953164428DCE00BE9C-depth001.06.svg&quot; /&gt;, are important because we can express many common vector operations, such as the multiplication of a vector by a scalar, as element-wise binary operations. By “element-wise binary operations” we simply mean applying an operator to the first item of each vector to get the first item of the output, then to the second items of the inputs for the second item of the output, and so forth. This is how all the basic math operators are applied by default in numpy or tensorflow, for example. Examples that often crop up in deep learning are &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-E50C0CB908AF7566CCC6D4585634EDC2-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-C65762AB635D8E08F759F1203D223C29-depth000.51.svg&quot; /&gt; (returns a vector of ones and zeros).&lt;/p&gt;
&lt;p&gt;We can generalize the element-wise binary operations with notation &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-E2E59AE84EE7A5B1C905E50FA7753A31-depth003.25.svg&quot; /&gt; where &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-5D19E7D8CDFE53DEB40F29D8936E6C89-depth003.25.svg&quot; /&gt;. (Reminder: &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-CF513DECF6E4ACE0E25CB1C932AAA049-depth003.25.svg&quot; /&gt; is the number of items in &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;.) The &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-1A74909B6CBAA4532A76D83B72C12DE0-depth002.52.svg&quot; /&gt; symbol represents any element-wise operator (such as &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-26B17225B626FB9238849FD60EABDF60-depth001.06.svg&quot; /&gt;) and not the &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-1B3C1A40F9CB094D47E8C6F9B0DF773F-depth000.00.svg&quot; /&gt; function composition operator. Here's what equation &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-E2E59AE84EE7A5B1C905E50FA7753A31-depth003.25.svg&quot; /&gt; looks like when we zoom in to examine the scalar equations:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-ADD1230AE3E64A1B7FA77851BB1F07A1.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;where we write &lt;span class=&quot;eqn&quot;&gt;n&lt;/span&gt; (not &lt;span class=&quot;eqn&quot;&gt;m&lt;/span&gt;) equations vertically to emphasize the fact that the result of element-wise operators give &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A193EBC083B4745370F6F1343383D9CC-depth000.14.svg&quot; /&gt; sized vector results.&lt;/p&gt;
&lt;p&gt;Using the ideas from the last section, we can see that the general case for the Jacobian with respect to &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; is the square matrix:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-54F95B3CFFD404740FAD218B308DEF70.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;and the Jacobian with respect to &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; is:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-2693C112F589CD0E26853EAD5ED36CFD.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;That's quite a furball, but fortunately the Jacobian is very often a diagonal matrix, a matrix that is zero everywhere but the diagonal. Because this greatly simplifies the Jacobian, let's examine in detail when the Jacobian reduces to a diagonal matrix for element-wise operations.&lt;/p&gt;
&lt;p&gt;In a diagonal Jacobian, all elements off the diagonal are zero, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-8B14469E98630C19F16578F90C45F62E-depth007.21.svg&quot; /&gt; where &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-B064F8555EC660F2F8BDC927D9636A06-depth002.72.svg&quot; /&gt;. (Notice that we are taking the partial derivative with respect to &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;j&lt;/sub&gt;&lt;/span&gt; not &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;.) Under what conditions are those off-diagonal elements zero? Precisely when &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;g&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; are contants with respect to &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;j&lt;/sub&gt;&lt;/span&gt;, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-44C0281EC00A2C0E93E4E3863EE9083D-depth007.21.svg&quot; /&gt;. Regardless of the operator, if those partial derivatives go to zero, the operation goes to zero, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A180A4C31CAC188626034423680B71E0-depth002.52.svg&quot; /&gt; no matter what, and the partial derivative of a constant is zero.&lt;/p&gt;
&lt;p&gt;Those partials go to zero when &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;g&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; are not functions of &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;j&lt;/sub&gt;&lt;/span&gt;. We know that element-wise operations imply that &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is purely a function of &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;g&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is purely a function of &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;. For example, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-2361C23CB78AC04537D7D642DF065EF5-depth001.06.svg&quot; /&gt; sums &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-4AF62C4A6B74D712D3FFD3FA4A0062BD-depth002.05.svg&quot; /&gt;. Consequently, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-45553270D20A27EBD4AAE84292606CDD-depth003.25.svg&quot; /&gt; reduces to &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-5F2A3B3A730ABED47918785C5EBF5039-depth003.25.svg&quot; /&gt; and the goal becomes &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-767EBC7C8A1785557E38FD32A10FB123-depth007.21.svg&quot; /&gt;. &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-DB590A5BF8B0ACC05B1FEEDC07929CD7-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-1BF8B4ED9F2C1D993DC0A1E547BDF5CB-depth003.25.svg&quot; /&gt; look like constants to the partial differentiation operator with respect to &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;j&lt;/sub&gt;&lt;/span&gt; when &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-B064F8555EC660F2F8BDC927D9636A06-depth002.72.svg&quot; /&gt; so the partials are zero off the diagonal. (Notation &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-DB590A5BF8B0ACC05B1FEEDC07929CD7-depth003.25.svg&quot; /&gt; is technically an abuse of our notation because &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;g&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; are functions of vectors not individual elements. We should really write something like &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-616C179024B43E6C340B1EE24D414DE8-depth003.25.svg&quot; /&gt;, but that would muddy the equations further, and programmers are comfortable overloading functions, so we'll proceed with the notation anyway.)&lt;/p&gt;
&lt;p&gt;We'll take advantage of this simplification later and refer to the constraint that &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-BD5A63074F44F11CB2ED06325816582A-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-5E597DAA4E5D9263DCBFB6AB02BDB67F-depth003.25.svg&quot; /&gt; access at most &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;, respectively, as the &lt;em&gt;element-wise diagonal condition&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Under this condition, the elements along the diagonal of the Jacobian are &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-C196BF268027E86D3D2420C2A205AF28-depth005.92.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-4CFC6C644E4A95B5760435C5094BE095.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;(The large “0”s are a shorthand indicating all of the off-diagonal are 0.)&lt;/p&gt;
&lt;p&gt;More succinctly, we can write:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-3D114C6873F46EE41AF91BF8B1BB37CD.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-78E1B2628221D9FD588A011D54670619.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;where &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-4935E4C6B875FD6C7C181871B566AB1A-depth003.25.svg&quot; /&gt; constructs a matrix whose diagonal elements are taken from vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;: &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-358C0E8F99CB66017CCD32DBC6C23C50-depth003.25.svg&quot; /&gt;. &lt;span class=&quot;eqn&quot;&gt;I&lt;/span&gt; represents the square identity matrix of appropriate dimensions that is zero everywhere but the diagonal, which contains all ones. The &lt;span class=&quot;eqn&quot;&gt;T&lt;/span&gt; exponent of &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-FAAEE783424BC0E27E9AA2F56A7B50B8-depth000.00.svg&quot; /&gt; represents the transpose of the indicated vector. In this case, it flips a vertical vector to a horizontal vector.&lt;/p&gt;
&lt;p&gt;Because we do lots of simple vector arithmetic, the general function &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-5B39FE68B4FDD0AF04290BA579A993CB-depth003.25.svg&quot; /&gt; in the binary element-wise operation is often just the vector &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt;. Any time the general function is a vector, we know that &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-BD5A63074F44F11CB2ED06325816582A-depth003.25.svg&quot; /&gt; reduces to &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-F5BDA09DDE6E00806B01094F5BED3026-depth003.25.svg&quot; /&gt;. For example, vector addition &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-DF3F5FDED9142A243031D03CF82121AE-depth001.06.svg&quot; /&gt; fits our element-wise diagonal condition because &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-1ED79D2E2BBE9EE796433D13773157A7-depth003.25.svg&quot; /&gt; has scalar equations &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-F7C644AAA7A70D588A0E003C7C9E439E-depth003.25.svg&quot; /&gt; that reduce to just &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-85550E18F87E4AF75645A38273B97A80-depth003.25.svg&quot; /&gt; with partial derivatives:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-A5DC35FE0CEC748B35BB5991933C4698.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-C2B0BF832F19994D832F90C18B1F04AF.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;That gives us &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-2935E504F134B53B2C03072175BCCD1F-depth004.67.svg&quot; /&gt;, the identity matrix, because every element along the diagonal is 1.&lt;/p&gt;
&lt;p&gt;Given the simplicity of this special case, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-BD5A63074F44F11CB2ED06325816582A-depth003.25.svg&quot; /&gt; reducing to &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-DB590A5BF8B0ACC05B1FEEDC07929CD7-depth003.25.svg&quot; /&gt;, you should be able to derive the Jacobians for the common element-wise binary operations on vectors:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-0C9CE28C888576E0D4873BDD69BC74EA.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-10B3C04502114250E4A74A1EB5F27F05.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-790C76CEB13E928D08EDC53D7AC4BB5C-depth001.08.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-98FF0549EBE322C195C2B36FD5EEAD33-depth001.08.svg&quot; /&gt; operators are element-wise multiplication and division; &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-790C76CEB13E928D08EDC53D7AC4BB5C-depth001.08.svg&quot; /&gt; is sometimes called the &lt;em&gt;Hadamard product&lt;/em&gt;. There isn't a standard notation for element-wise multiplication and division so we're using an approach consistent with our general binary operation notation.&lt;/p&gt;
&lt;h3 id=&quot;sec4.3&quot;&gt;Derivatives involving scalar expansion&lt;/h3&gt;
&lt;p&gt;When we multiply or add scalars to vectors, we're implicitly expanding the scalar to a vector and then performing an element-wise binary operation. For example, adding scalar &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; to vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-6307CEA088D2D4E98E5B163B9CE8F510-depth002.33.svg&quot; /&gt;, is really &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-8D9C0B9B15490F45C353D9DE64565A4F-depth003.25.svg&quot; /&gt; where &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-3ABE4A0471143ABFC180C9FA485E5F0A-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-BD2335FC4BBF16BE9590D2501CE8C030-depth003.25.svg&quot; /&gt;. (The notation &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-C2C0146718E407005D0C74774C5C5FFC-depth000.00.svg&quot; /&gt; represents a vector of ones of appropriate length.) &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; is any scalar that doesn't depend on &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;, which is useful because then &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-EBCC35EEAE3B420D59689973D8B6BD2E-depth005.92.svg&quot; /&gt; for any &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and that will simplify our partial derivative computations. (It's okay to think of variable &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; as a constant for our discussion here.) Similarly, multiplying by a scalar, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-C9A7B878F49F3D964AFEC9C1F78061CF-depth002.33.svg&quot; /&gt;, is really &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-93A461AB49FD151E602D9344358732CD-depth003.25.svg&quot; /&gt; where &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-790C76CEB13E928D08EDC53D7AC4BB5C-depth001.08.svg&quot; /&gt; is the element-wise multiplication (Hadamard product) of the two vectors.&lt;/p&gt;
&lt;p&gt;The partial derivatives of vector-scalar addition and multiplication with respect to vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; use our element-wise rule:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-F8F0A8F213DE2D87CB4F0C88B2CE8F4C.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;This follows because functions &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-3ABE4A0471143ABFC180C9FA485E5F0A-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-BD2335FC4BBF16BE9590D2501CE8C030-depth003.25.svg&quot; /&gt; clearly satisfy our element-wise diagonal condition for the Jacobian (that &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-DAE17AB5EE9C0A7FFA3E9B1774E80201-depth003.25.svg&quot; /&gt; refer at most to &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-9F8AE8514327A98189F8F05E2ECD6496-depth003.25.svg&quot; /&gt; refers to the &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-97361F12A3555FC4FC4E2FFCE1799AC3-depth000.14.svg&quot; /&gt; value of the &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-49508A178E36D0863E041575526BEA1A-depth001.05.svg&quot; /&gt; vector).&lt;/p&gt;
&lt;p&gt;Using the usual rules for scalar partial derivatives, we arrive at the following diagonal elements of the Jacobian for vector-scalar addition:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-6B4EFB58ED5F9EBD623321FE1975FA4E.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;So, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-5107D6819CDC50A8988D3EA0FB9B94CE-depth004.67.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;Computing the partial derivative with respect to the scalar parameter &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt;, however, results in a vertical vector, not a diagonal matrix. The elements of the vector are:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-5B07C016CBBA27F3E3650DA92BF06A24.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Therefore, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-F05F04525E4A1B8959DE54DC7C692060-depth005.22.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;The diagonal elements of the Jacobian for vector-scalar multiplication involve the product rule for scalar derivatives:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-3E6008F6437DA818B481A79FD47D38E4.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;So, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-3F231096152DFB321FAC62F57A808C35-depth004.67.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;The partial derivative with respect to scalar parameter &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; is a vertical vector whose elements are:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-C1C1AD1A9E7A6ECCAAACE952B8355BFB.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;This gives us &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-82B897A0A23D9C6BB66EDF17D1D3CB02-depth005.22.svg&quot; /&gt;.&lt;/p&gt;
&lt;h3 id=&quot;sec4.4&quot;&gt;Vector sum reduction&lt;/h3&gt;
&lt;p&gt;Summing up the elements of a vector is an important operation in deep learning, such as the network loss function, but we can also use it as a way to simplify computing the derivative of vector dot product and other operations that reduce vectors to scalars.&lt;/p&gt;
&lt;p&gt;Let &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-DC5FB5DC7AEB54D8C206744EED4AD748-depth003.31.svg&quot; /&gt;. Notice we were careful here to leave the parameter as a vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; because each function &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; could use all values in the vector, not just &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;. The sum is over the &lt;strong&gt;results&lt;/strong&gt; of the function and not the parameter. The gradient (&lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-4C8971A9939B0BB2D8AF44195C5BD833-depth001.08.svg&quot; /&gt; Jacobian) of vector summation is:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-40C0C67E5948039B40D9718ECC2858AE.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;(The summation inside the gradient elements can be tricky so make sure to keep your notation consistent.)&lt;/p&gt;
&lt;p&gt;Let's look at the gradient of the simple &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-5FAFFB6A723E437AC6433DCA0B269846-depth003.25.svg&quot; /&gt;. The function inside the summation is just &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-E314355F5E2135483279531C62D7E8EC-depth003.25.svg&quot; /&gt; and the gradient is then:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-B6DF766D85C48FF7434B8FFF7BEC9410.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Because &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-9B32CEB09BA5F0B84935A24BF81D3C9C-depth007.21.svg&quot; /&gt; for &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-B064F8555EC660F2F8BDC927D9636A06-depth002.72.svg&quot; /&gt;, we can simplify to:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-9D07CB9DFE389978D329A8CFE7568825.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Notice that the result is a horizontal vector full of 1s, not a vertical vector, and so the gradient is &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-DFB70C45B6CCB149DFDA0E3690715F92-depth000.00.svg&quot; /&gt;. It's very important to keep the shape of all of your vectors and matrices in order otherwise it's impossible to compute the derivatives of complex functions.&lt;/p&gt;
&lt;p&gt;As another example, let's sum the result of multiplying a vector by a constant scalar. If &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-729968724651D87C8269B6FFEAD6EA90-depth003.25.svg&quot; /&gt; then &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A5D03DE59DCDBA948F463FAABD04791D-depth003.25.svg&quot; /&gt;. The gradient is:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-4C28E3734FC6110AF58C567604ED3462.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The derivative with respect to scalar variable &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; is &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-5ED2D4C114D036610B8E20271C5026EF-depth001.08.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-C196ACAC45CCCF31A1541B15402C94E7.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;h3 id=&quot;sec4.5&quot;&gt;The Chain Rules&lt;/h3&gt;
&lt;p&gt;We can't compute partial derivatives of very complicated functions using just the basic matrix calculus rules we've seen so far. For example, we can't take the derivative of nested expressions like &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-DCA4F9F0CE7F7CA365E8B26987ED972A-depth003.25.svg&quot; /&gt; directly without reducing it to its scalar equivalent. We need to be able to combine our basic vector rules using what we can call the &lt;em&gt;vector chain rule&lt;/em&gt;. Unfortunately, there are a number of rules for differentiation that fall under the name “chain rule” so we have to be careful which chain rule we're talking about. Part of our goal here is to clearly define and name three different chain rules and indicate in which situation they are appropriate. To get warmed up, we'll start with what we'll call the &lt;em&gt;single-variable chain rule&lt;/em&gt;, where we want the derivative of a scalar function with respect to a scalar. Then we'll move on to an important concept called the &lt;em&gt;total derivative&lt;/em&gt; and use it to define what we'll pedantically call the &lt;em&gt;single-variable total-derivative chain rule&lt;/em&gt;. Then, we'll be ready for the vector chain rule in its full glory as needed for neural networks.&lt;/p&gt;
&lt;p&gt;The chain rule is conceptually a divide and conquer strategy (like Quicksort) that breaks complicated expressions into subexpressions whose derivatives are easier to compute. Its power derives from the fact that we can process each simple subexpression in isolation yet still combine the intermediate results to get the correct overall result.&lt;/p&gt;
&lt;p&gt;The chain rule comes into play when we need the derivative of an expression composed of nested subexpressions. For example, we need the chain rule when confronted with expressions like &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-06D67DC7FE74C1895AEF564F8295E918-depth004.58.svg&quot; /&gt;. The outermost expression takes the &lt;span class=&quot;eqn&quot;&gt;sin&lt;/span&gt; of an intermediate result, a nested subexpression that squares &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;. Specifically, we need the single-variable chain rule, so let's start by digging into that in more detail.&lt;/p&gt;
&lt;h4 id=&quot;sec4.5.1&quot;&gt;Single-variable chain rule&lt;/h4&gt;
&lt;p&gt;Let's start with the solution to the derivative of our nested expression: &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-AB5ECA885C5685990CD778580665B3A4-depth004.58.svg&quot; /&gt;. It doesn't take a mathematical genius to recognize components of the solution that smack of scalar differentiation rules, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-1A32AA532898DEBB80C0C7A818C5C70B-depth004.58.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-DC7C2E5FB11394451EA6A2010904F0B2-depth004.58.svg&quot; /&gt;. It looks like the solution is to multiply the derivative of the outer expression by the derivative of the inner expression or “chain the pieces together,” which is exactly right. In this section, we'll explore the general principle at work and provide a process that works for highly-nested expressions of a single variable.&lt;/p&gt;
&lt;p&gt;Chain rules are typically defined in terms of nested functions, such as &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-281717E562B6C04AA861AC9F2801D016-depth003.25.svg&quot; /&gt; for single-variable chain rules. (You will also see the chain rule defined using function composition &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-88806005B64072FE5A06E4E609A9E251-depth003.25.svg&quot; /&gt;, which is the same thing.) Some sources write the derivative using shorthand notation &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-3D59985B51738C2BF54BA3D955AB8588-depth003.25.svg&quot; /&gt;, but that hides the fact that we are introducing an intermediate variable: &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-8C36385E70550C1C0EA86E14320174DF-depth003.25.svg&quot; /&gt;, which we'll see shortly. It's better to define the &lt;a href=&quot;http://m.wolframalpha.com/input/?i=chain+rule&quot;&gt;single-variable chain rule&lt;/a&gt; of &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-8BC3A7E80988236E8F017205F413461C-depth003.25.svg&quot; /&gt; explicitly so we never take the derivative with respect to the wrong variable. Here is the formulation of the single-variable chain rule we recommend:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-9D3919C42833D1FF1456DEA11D8CC927.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;To deploy the single-variable chain rule, follow these steps:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Introduce intermediate variables for nested subexpressions and subexpressions for both binary and unary operators; e.g., &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-60C13E05D3EC8C10B8564EAE7023D9DB-depth001.08.svg&quot; /&gt; is binary, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-6FFE53F1F614CC1D470F85B6C56A3AFB-depth003.25.svg&quot; /&gt; and other trigonometric functions are usually unary because there is a single operand. This step normalizes all equations to single operators or function applications.&lt;/li&gt;
&lt;li&gt;Compute derivatives of the intermediate variables with respect to their parameters.&lt;/li&gt;
&lt;li&gt;Combine all derivatives of intermediate variables by multiplying them together to get the overall result.&lt;/li&gt;
&lt;li&gt;Substitute intermediate variables back in if any are referenced in the derivative equation.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;The third step puts the “chain” in “chain rule” because it chains together intermediate results. Multiplying the intermediate derivatives together is the common theme among all variations of the chain rule.&lt;/p&gt;
&lt;p&gt;Let's try this process on &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-872F24FC57CA661E3704C0A10869C6B5-depth003.25.svg&quot; /&gt;:&lt;/p&gt;
&lt;ol readability=&quot;0&quot;&gt;&lt;li readability=&quot;3&quot;&gt;Introduce intermediate variables. Let &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-FA510039FEDBE5A935A70EF6E3B46394-depth000.14.svg&quot; /&gt; represent subexpression &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-32F5240D0DBF2CCBE75EF7F8EF2015E0-depth000.14.svg&quot; /&gt; (shorthand for &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-879B57F3F479F58707D2477B46B060CF-depth003.25.svg&quot; /&gt;). This gives us:
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-165CA85C4E868C4589FDC97854EF5AFE.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The order of these subexpressions does not affect the answer, but we recommend working in the reverse order of operations dictated by the nesting (innermost to outermost). That way, expressions and derivatives are always functions of previously-computed elements.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Compute derivatives.
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-33DB49B9B2BFE622EF83565332547027.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Combine.
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-9D1B1984635759F4C2D23464EBBAA995.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Substitute.
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-6E1052BD462233E4BEA04D695437A984.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Notice how easy it is to compute the derivatives of the intermediate variables in isolation! The chain rule says it's legal to do that and tells us how to combine the intermediate results to get &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-E4D3AF86AC3E4315148DA23A886A72EA-depth003.25.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;You can think of the combining step of the chain rule in terms of units canceling. If we let &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; be gallons of gas, &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; be the gallons in a gas tank, and &lt;span class=&quot;eqn&quot;&gt;u&lt;/span&gt; as miles we can interpret &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-70047C98E4163674E78BB42D0CF4AEA8-depth004.58.svg&quot; /&gt; as &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-F98E0EBA888E860C7E51565C7225EBFA-depth006.34.svg&quot; /&gt;. The &lt;span class=&quot;eqn&quot;&gt;gallon&lt;/span&gt; denominator and numerator cancel.&lt;/p&gt;
&lt;p&gt;Another way to to think about the single-variable chain rule is to visualize the overall expression as a dataflow diagram or chain of operations (or &lt;a href=&quot;https://en.wikipedia.org/wiki/Abstract_syntax_tree&quot;&gt;abstract syntax tree&lt;/a&gt; for compiler people):&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/sin-square.png&quot; alt=&quot;sin-square.png&quot; width=&quot;130&quot; /&gt;&lt;/center&gt;
&lt;p&gt;Changes to function parameter &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; bubble up through a squaring operation then through a &lt;span class=&quot;eqn&quot;&gt;sin&lt;/span&gt; operation to change result &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;. You can think of &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-B13DB657CBF4B8318DBF2799E687D1A1-depth004.58.svg&quot; /&gt; as “getting changes from &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;u&lt;/span&gt;” and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-6E939E305B65D08F88FF95E7E028796B-depth004.58.svg&quot; /&gt; as “getting changes from &lt;span class=&quot;eqn&quot;&gt;u&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;.” Getting from &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; requires an intermediate hop. The chain rule is, by convention, usually written from the output variable down to the parameter(s), &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-70047C98E4163674E78BB42D0CF4AEA8-depth004.58.svg&quot; /&gt;. But, the &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;-to-&lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; perspective would be more clear if we reversed the flow and used the equivalent &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-256F5475DE15D99D8D55FE6F3A15CEA4-depth004.58.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Conditions under which the single-variable chain rule applies&lt;/strong&gt;. Notice that there is a single dataflow path from &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to the root &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;. Changes in &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; can influence output &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; in only one way. That is the condition under which we can apply the single-variable chain rule. An easier condition to remember, though one that's a bit looser, is that none of the intermediate subexpression functions, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-3BCB9E96DA63C9CDC1E56647C2071688-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-C6E9D6FA0C33AA632E25E953C6E5C35D-depth003.25.svg&quot; /&gt;, have more than one parameter. Consider &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-026E2EF5E8E6906B6CE75FF6CDB0F14E-depth003.25.svg&quot; /&gt;, which would become &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A1829C9E1CDCFF4FE0BA9C0E7A70E635-depth003.25.svg&quot; /&gt; after introducing intermediate variable &lt;span class=&quot;eqn&quot;&gt;u&lt;/span&gt;. As we'll see in the next section, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-2C284C56D6E912E6D71990A11005902E-depth003.25.svg&quot; /&gt; has multiple paths from &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;. To handle that situation, we'll deploy the single-variable total-derivative chain rule.&lt;/p&gt;
&lt;div readability=&quot;34&quot;&gt;As an aside for those interested in automatic differentiation, papers and library documentation use terminology &lt;em&gt;forward differentiation&lt;/em&gt; and &lt;em&gt;backward differentiation&lt;/em&gt; (for use in the back-propagation algorithm). From a dataflow perspective, we are computing a forward differentiation because it follows the normal data flow direction. Backward differentiation, naturally, goes the other direction and we're asking how a change in the output would affect function parameter &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;. Because backward differentiation can determine changes in all function parameters at once, it turns out to be much more efficient for computing the derivative of functions with lots of parameters. Forward differentiation, on the other hand, must consider how a change in each parameter, in turn, affects the function output &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;. The following table emphasizes the order in which partial derivatives are computed for the two techniques.
&lt;center&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Forward differentiation from &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;&lt;/th&gt;
&lt;th align=&quot;left&quot;&gt;Backward differentiation from &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-256F5475DE15D99D8D55FE6F3A15CEA4-depth004.58.svg&quot; /&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-70047C98E4163674E78BB42D0CF4AEA8-depth004.58.svg&quot; /&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;p&gt;Automatic differentiation is beyond the scope of this article, but we're setting the stage for a future article.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Many readers can solve &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-59B8E919C92D6DBE1DC50C5BE2CD8C1C-depth004.58.svg&quot; /&gt; in their heads, but our goal is a process that will work even for very complicated expressions. This process is also how &lt;a href=&quot;https://en.wikipedia.org/wiki/Automatic_differentiation&quot;&gt;automatic differentiation&lt;/a&gt; works in libraries like PyTorch. So, by solving derivatives manually in this way, you're also learning how to define functions for custom neural networks in PyTorch.&lt;/p&gt;
&lt;p&gt;With deeply nested expressions, it helps to think about deploying the chain rule the way a compiler unravels nested function calls like &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-7559A5FC5EC5CA0B3E50011742D0A87B-depth003.25.svg&quot; /&gt; into a sequence (chain) of calls. The result of calling function &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is saved to a temporary variable called a register, which is then passed as a parameter to &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-36930D135F4763C21D1191803AC41B85-depth002.72.svg&quot; /&gt;. Let's see how that looks in practice by using our process on a highly-nested equation like &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-642FD3E590A2B2D21AFE5254BE8E832F-depth003.25.svg&quot; /&gt;:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Introduce intermediate variables.
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-A153933499426CFC383D252C30A87953.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Compute derivatives.
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-4561212E91367D4B2DCC40262E36921D.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Combine four intermediate values.
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-2CF824877C0FB75B7648CE66E56FB509.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Substitute.
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-9BC49A78C13740AC58294EAA333AF3CC.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;Here is a visualization of the data flow through the chain of operations from &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;:&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/chain-tree.png&quot; alt=&quot;chain-tree.png&quot; width=&quot;150&quot; /&gt;&lt;/center&gt;
&lt;p&gt;At this point, we can handle derivatives of nested expressions of a single variable, &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;, using the chain rule but only if &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; can affect &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; through a single data flow path. To handle more complicated expressions, we need to extend our technique, which we'll do next.&lt;/p&gt;
&lt;h4 id=&quot;sec4.5.2&quot;&gt;Single-variable total-derivative chain rule&lt;/h4&gt;
&lt;p&gt;Our single-variable chain rule has limited applicability because all intermediate variables must be functions of single variables. But, it demonstrates the core mechanism of the chain rule, that of multiplying out all derivatives of intermediate subexpressions. To handle more general expressions such as &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-4E2EFF28C6823738FA61BFC9A3DD6D0F-depth003.25.svg&quot; /&gt;, however, we need to augment that basic chain rule.&lt;/p&gt;
&lt;p&gt;Of course, we immediately see &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-18EF15EC4DDA421BE5BC86F0295D36CD-depth004.58.svg&quot; /&gt;, but that is using the scalar addition derivative rule, not the chain rule. If we tried to apply the single-variable chain rule, we'd get the wrong answer. In fact, the previous chain rule is meaningless in this case because derivative operator &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-624FEBB9A49A3FC96353C861D175C806-depth004.58.svg&quot; /&gt; does not apply to multivariate functions, such as &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-89AE78BE880A004AA5404AC874A01BFF-depth001.95.svg&quot; /&gt; among our intermediate variables:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-5CB23F92FE51ABF1B1885A985EA61BC6.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Let's try it anyway to see what happens. If we pretend that &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-400C939294CFCAC13949F5A92DD9537A-depth005.85.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-8B1AAFE58A962E6F06775EBD2808D5FE-depth004.58.svg&quot; /&gt;, then &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-69A0BF0F0F217A5C8CDB490B4C60ABEE-depth005.85.svg&quot; /&gt; instead of the right answer &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-632334BF7A82AE1CEB6BF98756648B4E-depth001.06.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;Because &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-3DD76E5D04C44A4E7170558B8BFE3219-depth003.25.svg&quot; /&gt; has multiple parameters, partial derivatives come into play. Let's blindly apply the partial derivative operator to all of our equations and see what we get:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-7A7B19296641D8B6B96136527F381589.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Ooops! The partial &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-9A001AB7445AD6C36176920C0E5D253F-depth004.67.svg&quot; /&gt; is wrong because it violates a key assumption for partial derivatives. When taking the partial derivative with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;, the other variables must not vary as &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; varies. Otherwise, we could not act as if the other variables were constants. Clearly, though, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-0F0BF7F8711E6437357749F43EF529D8-depth003.25.svg&quot; /&gt; is a function of &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; and therefore varies with &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;. &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-78D6E87F2E36D9F4C482E9236F997C4C-depth004.67.svg&quot; /&gt; because &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-C9E3A40CD8239D58296F30B000154F1A-depth004.67.svg&quot; /&gt;. A quick look at the data flow diagram for &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-AC2672C7A79B0E068FB3AC3D7FBF94C9-depth003.25.svg&quot; /&gt; shows multiple paths from &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;, thus, making it clear we need to consider direct and indirect (through &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-D8F3471EF0522DC14012F0DC5D01D570-depth003.25.svg&quot; /&gt;) dependencies on &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;:&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/plus-square.png&quot; alt=&quot;plus-square.png&quot; width=&quot;150&quot; /&gt;&lt;/center&gt;
&lt;p&gt;A change in &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; affects &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; both as an operand of the addition and as the operand of the square operator. Here's an equation that describes how tweaks to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; affect the output:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-69B14AB41D9A7514E8C105FDDF9649C5.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Then, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-461DF6ABE0C0386D728B786B3116A5B1-depth002.65.svg&quot; /&gt;, which we can read as “the change in &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; is the difference between the original &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; at a tweaked &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;.”&lt;/p&gt;
&lt;p&gt;If we let &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A255512F9D61A6777BD5A304235BD26D-depth000.14.svg&quot; /&gt;, then &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-433AD860B59D47738D7AECAB6367A8AD-depth002.65.svg&quot; /&gt;. If we bump &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; by 1, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-60BD3A416346416B27D420F8EFEE9C9E-depth000.14.svg&quot; /&gt;, then &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-4DCF3FC9093036B469E832AEDCFDA608-depth003.25.svg&quot; /&gt;. The change in &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; is not &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-C4CA4238A0B923820DCC509A6F75849B-depth000.00.svg&quot; /&gt;, as &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-BE27DFA2B74F7608759BD413AF458EB2-depth003.25.svg&quot; /&gt; would lead us to believe, but &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-F0B6923B35563AE91BDFC8B06222E495-depth001.08.svg&quot; /&gt;!&lt;/p&gt;
&lt;p&gt;Enter the “law” of &lt;a href=&quot;https://en.wikipedia.org/wiki/Total_derivative&quot;&gt;total derivatives&lt;/a&gt;, which basically says that to compute &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-3BAFFD623D24688B6229E8808F4DD24A-depth004.58.svg&quot; /&gt;, we need to sum up all possible contributions from changes in &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to the change in &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;. The total derivative with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; assumes all variables, such as &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-D33DEF0EB4933F91B88EB4E784ADAF05-depth001.95.svg&quot; /&gt; in this case, are functions of &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; and potentially vary as &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; varies. The total derivative of &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-20596CA2E264644A086ABF3ABCA89367-depth003.25.svg&quot; /&gt; that depends on &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; directly and indirectly via intermediate variable &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-D8F3471EF0522DC14012F0DC5D01D570-depth003.25.svg&quot; /&gt; is given by:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-09EB861D79D7E60D9B37567CE097631B.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Using this formula, we get the proper answer:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-217390CDB48372744AC16E8277C9D0CC.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;That is an application of what we can call the &lt;em&gt;single-variable total-derivative chain rule&lt;/em&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-4586E8ADC3AA440DD41501217E7B6E67.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The total derivative assumes all variables are potentially codependent whereas the partial derivative assumes all variables but &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; are constants.&lt;/p&gt;
&lt;p&gt;There is something subtle going on here with the notation. All of the derivatives are shown as partial derivatives because &lt;span class=&quot;eqn&quot;&gt;f&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;u&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; are functions of multiple variables. This notation mirrors that of &lt;a href=&quot;http://mathworld.wolfram.com/TotalDerivative.html&quot;&gt;MathWorld's notation&lt;/a&gt; but differs from &lt;a href=&quot;https://en.wikipedia.org/wiki/Total_derivative&quot;&gt;Wikipedia&lt;/a&gt;, which uses &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-8A4956B34F845398E8CB25E9789E1477-depth003.25.svg&quot; /&gt; instead (possibly to emphasize the total derivative nature of the equation). We'll stick with the partial derivative notation so that it's consistent with our discussion of the vector chain rule in the next section.&lt;/p&gt;
&lt;p&gt;In practice, just keep in mind that when you take the total derivative with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;, other variables might also be functions of &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; so add in their contributions as well. The left side of the equation looks like a typical partial derivative but the right-hand side is actually the total derivative. It's common, however, that many temporary variables are functions of a single parameter, which means that the single-variable total-derivative chain rule degenerates to the single-variable chain rule.&lt;/p&gt;
&lt;p&gt;Let's look at a nested subexpression, such as &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-FD3AC7E953C3B294864C747188F6F370-depth003.25.svg&quot; /&gt;. We introduce three intermediate variables:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-B5215A178D5EDFCCA280ED63D64A8025.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;and partials:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-C3A02B93F5C0F2C8979E524BB28D35AC.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;where both &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A5D73F881FB427D3CD136DA4815CACA4-depth004.67.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-110F22244D49C5AD607D2BEFE91944A5-depth004.67.svg&quot; /&gt; have &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-4E8A57E5001AA58AEAA927CF98746B9C-depth004.67.svg&quot; /&gt; terms that take into account the total derivative.&lt;/p&gt;
&lt;p&gt;Also notice that the total derivative formula always &lt;strong&gt;sums&lt;/strong&gt; versus, say, multiplies terms &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-935DA2BFCBE67A179A8DAAB35E19A1DA-depth005.92.svg&quot; /&gt;. It's tempting to think that summing up terms in the derivative makes sense because, for example, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-FCCD52402617C355E7BB85B3336D0142-depth002.65.svg&quot; /&gt; adds two terms. Nope. The total derivative is adding terms because it represents a weighted sum of all &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; contributions to the change in &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;. For example, given &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-5C5A09876AA823C381141FDAC1D28BA6-depth002.65.svg&quot; /&gt; instead of &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-C26DA96086B50304FCE872846D4BE19F-depth002.65.svg&quot; /&gt;, the total-derivative chain rule formula still adds partial derivative terms. (&lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-94E561C972A5FE1DDE82761012FB6DB1-depth001.08.svg&quot; /&gt; simplifies to &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-8C0FB3B076D9AEA142467B34F0F794EB-depth000.14.svg&quot; /&gt; but for this demonstration, let's not combine the terms.) Here are the intermediate variables and partial derivatives:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-A1BE672D8676E7C9DE634935C1CDEBBA.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The form of the total derivative remains the same, however:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-50D1EABA22B46536559D83F8C21F749D.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;It's the partials (weights) that change, not the formula, when the intermediate variable operators change.&lt;/p&gt;
&lt;p&gt;Those readers with a strong calculus background might wonder why we aggressively introduce intermediate variables even for the non-nested subexpressions such as &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-32F5240D0DBF2CCBE75EF7F8EF2015E0-depth000.14.svg&quot; /&gt; in &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-06671AD4FF442E623460886DA749C797-depth001.06.svg&quot; /&gt;. We use this process for three reasons: (i) computing the derivatives for the simplified subexpressions is usually trivial, (ii) we can simplify the chain rule, and (iii) the process mirrors how automatic differentiation works in neural network libraries.&lt;/p&gt;
&lt;p&gt;Using the intermediate variables even more aggressively, let's see how we can simplify our single-variable total-derivative chain rule to its final form. The goal is to get rid of the &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-F6E0346D1D3410B0FBE32B41B85999AA-depth004.67.svg&quot; /&gt; sticking out on the front like a sore thumb:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-D8F0438A2B5867000190B7AB280DDD9A.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;We can achieve that by simply introducing a new temporary variable as an alias for &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;: &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-80A59ADB26E30571A3056E2EB3E8DDCB-depth002.69.svg&quot; /&gt;. Then, the formula reduces to our final form:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-E67E8C776D0E1C3174B27A33B9850506.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;This chain rule that takes into consideration the total derivative degenerates to the single-variable chain rule when all intermediate variables are functions of a single variable. Consequently, you can remember this more general formula to cover both cases. As a bit of dramatic foreshadowing, notice that the summation sure looks like a vector dot product, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-3F95AED50A8E6A5C514952D767B771CE-depth004.67.svg&quot; /&gt;, or a vector multiply &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-51D7B0B0455FB229AB4920E9CA4AB032-depth004.67.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;Before we move on, a word of caution about terminology on the web. Unfortunately, the chain rule given in this section, based upon the total derivative, is universally called “multivariable chain rule” in calculus discussions, which is highly misleading! Only the intermediate variables are multivariate functions. The overall function, say, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A5A7963264669BCFA0CCFA897853A1E0-depth003.25.svg&quot; /&gt;, is a scalar function that accepts a single parameter &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;. The derivative and parameter are scalars, not vectors, as one would expect with a so-called multivariate chain rule. (Within the context of a non-matrix calculus class, “multivariate chain rule” is likely unambiguous.) To reduce confusion, we use “single-variable total-derivative chain rule” to spell out the distinguishing feature between the simple single-variable chain rule, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-9D3919C42833D1FF1456DEA11D8CC927-depth004.58.svg&quot; /&gt;, and this one.&lt;/p&gt;
&lt;h4 id=&quot;sec4.5.3&quot;&gt;Vector chain rule&lt;/h4&gt;
&lt;p&gt;Now that we've got a good handle on the total-derivative chain rule, we're ready to tackle the chain rule for vectors of functions and vector variables. Surprisingly, this more general chain rule is just as simple looking as the single-variable chain rule for scalars. Rather than just presenting the vector chain rule, let's rediscover it ourselves so we get a firm grip on it. We can start by computing the derivative of a sample vector function with respect to a scalar, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-82C28F66A012D28717CA0CFC8ED7F09B-depth003.25.svg&quot; /&gt;, to see if we can abstract a general formula.&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-428F0EFA4C7B2EEC64829258E8DAFF86.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Let's introduce two intermediate variables, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-E11BA37D5D784AF689E175BEC8A2F284-depth002.65.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-D7F116997176D81A3BBD4E6DFC6FE6B0-depth002.65.svg&quot; /&gt;, one for each &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; so that &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; looks more like &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-C5AF10997DA85D7540CDF87F1F10016C-depth003.25.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-5CE597B34A604EDC0DD0B1AD97CFD690.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-E04563108617169E3793740388785DAB.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The derivative of vector &lt;span class=&quot;eqnvec&quot;&gt;y&lt;/span&gt; with respect to scalar &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; is a vertical vector with elements computed using the single-variable total-derivative chain rule:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-63BFCE8E2B4E8F603E209ECA0C1DADCE.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Ok, so now we have the answer using just the scalar rules, albeit with the derivatives grouped into a vector. Let's try to abstract from that result what it looks like in vector form. The goal is to convert the following vector of scalar operations to a vector operation.&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-0EA3C72DBB9F820121EE6A27D76EC7CC.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;If we split the &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-97D1ACA90CB316ED8AE1EDFFAED02C77-depth007.21.svg&quot; /&gt; terms, isolating the &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-D513134B704605A9687A47F8841D7D29-depth004.67.svg&quot; /&gt; terms into a vector, we get a matrix by vector multiplication:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-692581F3416029FED8E1CE09890F4A5E.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;That means that the Jacobian is the multiplication of two other Jacobians, which is kinda cool. Let's check our results:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-4A8689EA58BF9FA2AF675AAE0C093010.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Whew! We get the same answer as the scalar approach. This vector chain rule for vectors of functions and a single parameter appears to be correct and, indeed, mirrors the single-variable chain rule. Compare the vector rule:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-B0D7932C93DA81FD62418DA5DF3CBE14.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;with the single-variable chain rule:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-8617320E088DCA9EC6795865C614324A.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;To make this formula work for multiple parameters or vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;, we just have to change &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; to vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; in the equation. The effect is that &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A24633DDE0B5346B4AE6B49395AC8B6D-depth004.67.svg&quot; /&gt; and the resulting Jacobian, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A9A13E2AE753365278B4F2CD198BBF92-depth004.67.svg&quot; /&gt;, are now matrices instead of vertical vectors. Our complete &lt;em&gt;vector chain rule&lt;/em&gt; is:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-63A95E8A883CCB871C2C68B2D8B6EAA4.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The beauty of the vector formula over the single-variable chain rule is that it automatically takes into consideration the total derivative while maintaining the same notational simplicity. The Jacobian contains all possible combinations of &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; with respect to &lt;span class=&quot;eqn&quot;&gt;g&lt;sub&gt;j&lt;/sub&gt;&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;g&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;j&lt;/sub&gt;&lt;/span&gt;. For completeness, here are the two Jacobian components in their full glory:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-875D6B48E0F3610A491D91FA12067AED.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;where &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-99ADCB62C02EE5D10CC7B5F211BFA15B-depth003.25.svg&quot; /&gt;, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-6E37471488C004C0ABACDF4148B8F3D6-depth003.25.svg&quot; /&gt;, and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-26312045D51B3E69C8357FF7FAF3BB3F-depth003.25.svg&quot; /&gt;. The resulting Jacobian is &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-FBFEB9C8459FEE5A2BD529C07B881153-depth001.08.svg&quot; /&gt; (an &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-2DC13003CC069B1027F12896B1A00631-depth001.08.svg&quot; /&gt; matrix multiplied by a &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-6D999BBC8A6ECBB820C04121088529A1-depth001.08.svg&quot; /&gt; matrix).&lt;/p&gt;
&lt;p&gt;Even within this &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A7896E2D919F33CF607DC9E972C70458-depth006.23.svg&quot; /&gt; formula, we can simplify further because, for many applications, the Jacobians are square (&lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A193EBC083B4745370F6F1343383D9CC-depth000.14.svg&quot; /&gt;) and the off-diagonal entries are zero. It is the nature of neural networks that the associated mathematics deals with functions of vectors not vectors of functions. For example, the neuron affine function has term &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-DE94554BD6F158BD8A829624C65169F4-depth003.25.svg&quot; /&gt; and the activation function is &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-EA2DFCC1C759A6D2EE675EBD05B2C593-depth003.25.svg&quot; /&gt;; we'll consider derivatives of these functions in the next section.&lt;/p&gt;
&lt;p&gt;As we saw in a previous section, element-wise operations on vectors &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; and &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; yield diagonal matrices with elements &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-D840B92230E9F1C5F2545BCA90B34038-depth005.92.svg&quot; /&gt; because &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is a function purely of &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; but not &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;j&lt;/sub&gt;&lt;/span&gt; for &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-B064F8555EC660F2F8BDC927D9636A06-depth002.72.svg&quot; /&gt;. The same thing happens here when &lt;span class=&quot;eqn&quot;&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is purely a function of &lt;span class=&quot;eqn&quot;&gt;g&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;g&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is purely a function of &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-4869BA6DCF2C9A404EECD993808A74B7.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-8866C26258F279CD69740D3A26C0CD90.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;In this situation, the vector chain rule simplifies to:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-2D82A2CFEA0A49E9B7D3C8F986DAF14A.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Therefore, the Jacobian reduces to a diagonal matrix whose elements are the single-variable chain rule values.&lt;/p&gt;
&lt;p&gt;After slogging through all of that mathematics, here's the payoff. All you need is the vector chain rule because the single-variable formulas are special cases of the vector chain rule. The following table summarizes the appropriate components to multiply in order to get the Jacobian.&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/latex-17BE59DB8766A07658ADAA8522995C53.svg&quot; alt=&quot; \begin{tabular}[t]{c|cccc} &amp;amp; \multicolumn{2}{c}{ \begin{tabular}[t]{c} scalar\ \framebox(18,18){$x$}\ \end{tabular}} &amp;amp; &amp;amp;\begin{tabular}{c} vector\ \framebox(18,40){$\mathbf{x}$}\ \end{tabular} \ \begin{tabular}{c}$\frac{\partial}{\partial \mathbf{x}} \mathbf{f}(\mathbf{g}(\mathbf{x}))$ = $\frac{\partial \mathbf{f}}{\partial \mathbf{g}}\frac{\partial\mathbf{g}}{\partial \mathbf{x}}$ \ \end{tabular} &amp;amp; \begin{tabular}[t]{c} scalar\ \framebox(18,18){$u$}\ \end{tabular} &amp;amp; \begin{tabular}{c} vector\ \framebox(18,40){$\mathbf{u}$} \end{tabular}&amp;amp; &amp;amp; \begin{tabular}{c} vector\ \framebox(18,40){$\mathbf{u}$}\ \end{tabular} \ \hline \\[\dimexpr-\normalbaselineskip+5pt] \begin{tabular}[b]{c} scalar\ \framebox(18,18){$f$}\ \end{tabular} &amp;amp;\framebox(18,18){$\frac{\partial f}{\partial {u}}$} \framebox(18,18){$\frac{\partial u}{\partial {x}}$} ~~~&amp;amp; \raisebox{22pt}{\framebox(40,18){$\frac{\partial f}{\partial {\mathbf{u}}}$}} \framebox(18,40){$\frac{\partial \mathbf{u}}{\partial x}$} &amp;amp; ~~~&amp;amp; \raisebox{22pt}{\framebox(40,18){$\frac{\partial f}{\partial {\mathbf{u}}}$}} \framebox(40,40){$\frac{\partial \mathbf{u}}{\partial \mathbf{x}}$} \ \begin{tabular}[b]{c} vector\ \framebox(18,40){$\mathbf{f}$}\ \end{tabular} &amp;amp; \framebox(18,40){$\frac{\partial \mathbf{f}}{\partial {u}}$} \raisebox{22pt}{\framebox(18,18){$\frac{\partial u}{\partial {x}}$}} &amp;amp; \framebox(40,40){$\frac{\partial \mathbf{f}}{\partial \mathbf{u}}$} \framebox(18,40){$\frac{\partial \mathbf{u}}{\partial x}$} &amp;amp; &amp;amp; \framebox(40,40){$\frac{\partial \mathbf{f}}{\partial \mathbf{u}}$} \framebox(40,40){$\frac{\partial \mathbf{u}}{\partial \mathbf{x}}$}\ \end{tabular} &quot; /&gt;&lt;/div&gt;
&lt;h2 id=&quot;sec5&quot;&gt;The gradient of neuron activation&lt;/h2&gt;
&lt;p&gt;We now have all of the pieces needed to compute the derivative of a typical neuron activation for a single neural network computation unit with respect to the model parameters, &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-F3041D1B0AB2DA26CFE6581CCE10BF0F.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;(This represents a neuron with fully connected weights and rectified linear unit activation. There are, however, other affine functions such as convolution and other activation functions, such as exponential linear units, that follow similar logic.)&lt;/p&gt;
&lt;p&gt;Let's worry about &lt;span class=&quot;eqn&quot;&gt;max&lt;/span&gt; later and focus on computing &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-37826EBEE16CD487A60FA876F5038265-depth004.67.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-45DA2FE78D565E2361F35FF898D806A8-depth004.67.svg&quot; /&gt;. (Recall that neural networks learn through optimization of their weights and biases.) We haven't discussed the derivative of the dot product yet, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-31E193E45068EDA5F2E229B246720968-depth003.25.svg&quot; /&gt;, but we can use the chain rule to avoid having to memorize yet another rule. (Note notation &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; not &lt;span class=&quot;eqnvec&quot;&gt;y&lt;/span&gt; as the result is a scalar not a vector.)&lt;/p&gt;
&lt;p&gt;The dot product &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-9CA8C3CD16894AF7620468A20C53D6FA-depth000.00.svg&quot; /&gt; is just the summation of the element-wise multiplication of the elements: &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-4B722CF0EFB8F8ABCBB968086BB587E8-depth003.31.svg&quot; /&gt;. (You might also find it useful to remember the linear algebra notation &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-DE393E92B808CB595831DB7AF0D46F39-depth000.00.svg&quot; /&gt;.) We know how to compute the partial derivatives of &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-1B8AC0FD13AAD81B7EFBE58CDD162D02-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-68EE6C0B6C39602AC2A620854B5785B2-depth001.08.svg&quot; /&gt; but haven't looked at partial derivatives for &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-307751654E2CF1B689CE1D06776B3934-depth003.25.svg&quot; /&gt;. We need the chain rule for that and so we can introduce an intermediate vector variable &lt;span class=&quot;eqnvec&quot;&gt;u&lt;/span&gt; just as we did using the single-variable chain rule:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-AA40E45F705402308665F4778260405C.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Once we've rephrased &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt;, we recognize two subexpressions for which we already know the partial derivatives:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-CB86F7761DC5757FCE7D9B440DEB6630.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The vector chain rule says to multiply the partials:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-735EE304812513469D0BAE8D1D32E578.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;To check our results, we can grind the dot product down into a pure scalar function:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-8E233C707CFA165FFECCE145E88AEB24.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Then:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-44FAB0E50B6FA0C7012FF79FB03EBD14.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Hooray! Our scalar results match the vector chain rule results.&lt;/p&gt;
&lt;p&gt;Now, let &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-EABE2A2F7035C793F48B3885A2EA0009-depth002.65.svg&quot; /&gt;, the full expression within the &lt;span class=&quot;eqn&quot;&gt;max&lt;/span&gt; activation function call. We have two different partials to compute, but we don't need the chain rule:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-0F1D53EBF96D7DC463E2226B77812776.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Let's tackle the partials of the neuron activation, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-602D78206C69DAFE86EDC0775CF04CDB-depth003.25.svg&quot; /&gt;. The use of the &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-2E8B663420C79458FF788F7C8F198AA2-depth003.25.svg&quot; /&gt; function call on scalar &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; just says to treat all negative &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; values as 0. The derivative of the max function is a piecewise function. When &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-B960DEAE3DC80FA85FEA3304A9474DB5-depth001.72.svg&quot; /&gt;, the derivative is 0 because &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; is a constant. When &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-E929E61EB1C05B5B7AC234027C595BE2-depth001.05.svg&quot; /&gt;, the derivative of the max function is just the derivative of &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt;, which is &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-C4CA4238A0B923820DCC509A6F75849B-depth000.00.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-61BE19B395EB3114577B2100997DFB6D.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div readability=&quot;18&quot;&gt;An aside on broadcasting functions across scalars. When one or both of the &lt;span class=&quot;eqn&quot;&gt;max&lt;/span&gt; arguments are vectors, such as &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-EA2DFCC1C759A6D2EE675EBD05B2C593-depth003.25.svg&quot; /&gt;, we broadcast the single-variable function &lt;span class=&quot;eqn&quot;&gt;max&lt;/span&gt; across the elements. This is an example of an element-wise unary operator. Just to be clear:
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-94240FE4B77DEE9DA38F596CD4149F9D.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;For the derivative of the broadcast version then, we get a vector of zeros and ones where:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-5B3555A4691C5DA19688E4F76BA1C3AD.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-4EA88847E78682CFDBCA0019C1623945.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;To get the derivative of the &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-FD043EAC3999F8DEAF1FF1E131B3346C-depth003.25.svg&quot; /&gt; function, we need the chain rule because of the nested subexpression, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-6C156670CEC09096976A6722592523F3-depth001.06.svg&quot; /&gt;. Following our process, let's introduce intermediate scalar variable &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; to represent the affine function giving:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-261BC49758F84DF99117345CD8D22CFE.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-9E429AF61D15BF6942A3132FABAC77A2.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The vector chain rule tells us:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-B0C84C12426A7A698FBBCB890502411F.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;which we can rewrite as follows:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-AE892FF5E074073E025BB3BBE586B9F5.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;and then substitute &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-F394C21EEE331911707D5EDBD9BCAE20-depth001.06.svg&quot; /&gt; back in:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-A123AAABD8432822C27BEE74393F78AD.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;That equation matches our intuition. When the activation function clips affine function output &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; to 0, the derivative is zero with respect to any weight &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;. When &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-E929E61EB1C05B5B7AC234027C595BE2-depth001.05.svg&quot; /&gt;, it's as if the &lt;span class=&quot;eqn&quot;&gt;max&lt;/span&gt; function disappears and we get just the derivative of &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; with respect to the weights.&lt;/p&gt;
&lt;p&gt;Turning now to the derivative of the neuron activation with respect to &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt;, we get:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-9CE42F7BD715F354A87DF9043310E3BB.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Let's use these partial derivatives now to handle the entire loss function.&lt;/p&gt;
&lt;h2 id=&quot;sec6&quot;&gt;The gradient of the neural network loss function&lt;/h2&gt;
&lt;p&gt;Training a neuron requires that we take the derivative of our loss or “cost” function with respect to the parameters of our model, &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt;. Because we train with multiple vector inputs (e.g., multiple images) and scalar targets (e.g., one classification per image), we need some more notation. Let&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-D9D9E4DA80EA78BCCCDDC0BE89A198CD.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;where &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-BEDD04B41054D234623B0BB3759ABF73-depth003.25.svg&quot; /&gt;, and then let&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-F924EBF36B5E655648826C8AE83DE16D.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;where &lt;span class=&quot;eqn&quot;&gt;y&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is a scalar. Then the cost equation becomes:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-3D04A32BFDCD990F21451D8230C46FB1.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Following our chain rule process introduces these intermediate variables:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-ED01A1463E7C7657E0DD1546F6C48BFB.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Let's compute the gradient with respect to &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; first.&lt;/p&gt;
&lt;h3 id=&quot;sec6.1&quot;&gt;The gradient with respect to the weights&lt;/h3&gt;
&lt;p&gt;From before, we know:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-577C100C01A97DEA7FB361169DA383B5.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-8E94E788919EA6D0BE11C2615A11C009.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Then, for the overall gradient, we get:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/latex-9F8112A77C51E95057A9E56D29FFB669.svg&quot; alt=&quot; \begin{eqnarray*} \frac{\partial C(v)}{\partial \mathbf{w}} &amp;amp; = &amp;amp; \frac{\partial }{\partial \mathbf{w}}\frac{1}{N} \sum_{i=1}^N v^2\\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \frac{\partial}{\partial \mathbf{w}} v^2\\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \frac{\partial v^2}{\partial v} \frac{\partial v}{\partial \mathbf{w}} \\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N 2v \frac{\partial v}{\partial \mathbf{w}} \\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \begin{cases} 2v\vec{0}^T = \vec{0}^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b \leq 0\ -2v\mathbf{x}^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b &amp;gt; 0\ \end{cases}\\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \begin{cases} \vec{0}^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b \leq 0\ -2(y_i-u)\mathbf{x}_i^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b &amp;gt; 0\ \end{cases}\\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \begin{cases} \vec{0}^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b \leq 0\ -2(y_i-max(0, \mathbf{w}\cdot\mathbf{x}_i+b))\mathbf{x}_i^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b &amp;gt; 0\ \end{cases}\ \phantom{\frac{\partial C(v)}{\partial \mathbf{w}}} &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \begin{cases} \vec{0}^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b \leq 0\ -2(y_i-(\mathbf{w}\cdot\mathbf{x}_i+b))\mathbf{x}_i^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b &amp;gt; 0\ \end{cases}\\\ &amp;amp; = &amp;amp; \begin{cases} \vec{0}^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b \leq 0\ \frac{-2}{N} \sum_{i=1}^N (y_i-(\mathbf{w}\cdot\mathbf{x}_i+b))\mathbf{x}_i^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b &amp;gt; 0\ \end{cases}\\\ &amp;amp; = &amp;amp; \begin{cases} \vec{0}^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b \leq 0\ \frac{2}{N} \sum_{i=1}^N (\mathbf{w}\cdot\mathbf{x}_i+b-y_i)\mathbf{x}_i^T &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b &amp;gt; 0\ \end{cases} \end{eqnarray*} &quot; /&gt;&lt;/div&gt;
&lt;p&gt;To interpret that equation, we can substitute an error term &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-A90A948C72E3B6E10EF49E9CA3323248-depth002.65.svg&quot; /&gt; yielding:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-6F2CDC50A69419550C3127B318DB71CD.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;From there, notice that this computation is a weighted average across all &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;&lt;sub class=&quot;eqn&quot;&gt;i&lt;/sub&gt; in &lt;span class=&quot;eqn&quot;&gt;X&lt;/span&gt;. The weights are the error terms, the difference between the target output and the actual neuron output for each &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;&lt;sub class=&quot;eqn&quot;&gt;i&lt;/sub&gt; input. The resulting gradient will, on average, point in the direction of higher cost or loss because large &lt;span class=&quot;eqn&quot;&gt;e&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; emphasize their associated &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;&lt;sub class=&quot;eqn&quot;&gt;i&lt;/sub&gt;. Imagine we only had one input vector, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-75147DC2E59AB7AF04E48C0E3C2D71EA-depth003.25.svg&quot; /&gt;, then the gradient is just &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-26DDD7C22F3D8F5B07FC7EE421EEE6ED-depth003.45.svg&quot; /&gt;. If the error is 0, then the gradient is zero and we have arrived at the minimum loss. If &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-3104BCC503B3B8C16FFA2940B56AAF1C-depth001.95.svg&quot; /&gt; is some small positive difference, the gradient is a small step in the direction of &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-262A3BD0318D5034272A8F904D6FAD24-depth001.95.svg&quot; /&gt;. If &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-3104BCC503B3B8C16FFA2940B56AAF1C-depth001.95.svg&quot; /&gt; is large, the gradient is a large step in that direction. If &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-3104BCC503B3B8C16FFA2940B56AAF1C-depth001.95.svg&quot; /&gt; is negative, the gradient is reversed, meaning the highest cost is in the negative direction.&lt;/p&gt;
&lt;p&gt;Of course, we want to reduce, not increase, the loss, which is why the &lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot;&gt;gradient descent&lt;/a&gt; recurrence relation takes the negative of the gradient to update the current position (for scalar learning rate &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-FFE9F913124F345732E9F00FA258552E-depth002.65.svg&quot; /&gt;):&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-30F3667BFB088A174DD6CC1F0908780A.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Because the gradient indicates the direction of higher cost, we want to update &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; in the opposite direction.&lt;/p&gt;
&lt;h3 id=&quot;sec6.2&quot;&gt;The derivative with respect to the bias&lt;/h3&gt;
&lt;p&gt;To optimize the bias, &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt;, we also need the partial with respect to &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt;. Here are the intermediate variables again:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-18067E5F73988B179A304788A7BC5786.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;We computed the partial with respect to the bias for equation &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-FBD5708C00E92A391A69A42591580BE0-depth003.25.svg&quot; /&gt; previously:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-97F6B2C0D2E31579875BAE3E458BF333.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;For &lt;span class=&quot;eqn&quot;&gt;v&lt;/span&gt;, the partial is:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-8C418D6F20C9CD5F0C56184F94005AF3.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;And for the partial of the cost function itself we get:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/latex-D2EB5E709D4D4474EDB3DE6699F91F9A.svg&quot; alt=&quot; \begin{eqnarray*} \frac{\partial C(v)}{\partial b} &amp;amp; = &amp;amp; \frac{\partial }{\partial b}\frac{1}{N} \sum_{i=1}^N v^2\\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \frac{\partial}{\partial b} v^2\\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \frac{\partial v^2}{\partial v} \frac{\partial v}{\partial b} \\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N 2v \frac{\partial v}{\partial b} \\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \begin{cases} 0 &amp;amp; \mathbf{w} \cdot \mathbf{x} + b \leq 0\ -2v &amp;amp; \mathbf{w} \cdot \mathbf{x} + b &amp;gt; 0\ \end{cases}\\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \begin{cases} 0 &amp;amp; \mathbf{w} \cdot \mathbf{x} + b \leq 0\ -2(y_i-max(0, \mathbf{w}\cdot\mathbf{x}_i+b)) &amp;amp; \mathbf{w} \cdot \mathbf{x} + b &amp;gt; 0\ \end{cases}\\\ &amp;amp; = &amp;amp; \frac{1}{N} \sum_{i=1}^N \begin{cases} 0 &amp;amp; \mathbf{w} \cdot \mathbf{x} + b \leq 0\ 2(\mathbf{w}\cdot\mathbf{x}_i+b-y_i) &amp;amp; \mathbf{w} \cdot \mathbf{x} + b &amp;gt; 0\ \end{cases}\\\ &amp;amp; = &amp;amp; \begin{cases} 0 &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b \leq 0\ \frac{2}{N} \sum_{i=1}^N (\mathbf{w}\cdot\mathbf{x}_i+b-y_i) &amp;amp; \mathbf{w} \cdot \mathbf{x}_i + b &amp;gt; 0\ \end{cases} \end{eqnarray*} &quot; /&gt;&lt;/div&gt;
&lt;p&gt;As before, we can substitute an error term:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-2D57432B77DCFDC3D65FC04C9F6621A7.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The partial derivative is then just the average error or zero, according to the activation level. To update the neuron bias, we nudge it in the opposite direction of increased cost:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-301542C82A1BF05D145392056ADC0AC2.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;In practice, it is convenient to combine &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt; into a single vector parameter rather than having to deal with two different partials: &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-1C5C3BA710F818B84DF992E699DB50C3-depth003.25.svg&quot; /&gt;. This requires a tweak to the input vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; as well but simplifies the activation function. By tacking a 1 onto the end of &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-CDA92F9769DA156F5D82B4BF0D40A8B4-depth003.25.svg&quot; /&gt;, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-6C156670CEC09096976A6722592523F3-depth001.06.svg&quot; /&gt; becomes &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-03741D422AA7DF7FF34288D8E4395143-depth000.00.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;This finishes off the optimization of the neural network loss function because we have the two partials necessary to perform a gradient descent.&lt;/p&gt;
&lt;h2 id=&quot;sec7&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Hopefully you've made it all the way through to this point. You're well on your way to understanding matrix calculus! We've included a reference that summarizes all of the rules from this article in the next section. Also check out the annotated resource link below.&lt;/p&gt;
&lt;p&gt;Your next step would be to learn about the partial derivatives of matrices not just vectors. For example, you can take a look at the matrix differentiation section of &lt;a href=&quot;https://atmos.washington.edu/~dennis/MatrixCalculus.pdf&quot;&gt;Matrix calculus&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Acknowledgements&lt;/strong&gt;. We thank &lt;a href=&quot;https://www.usfca.edu/faculty/yannet-interian&quot;&gt;Yannet Interian&lt;/a&gt; (Faculty in MS data science program at University of San Francisco) and &lt;a href=&quot;http://www.cs.usfca.edu/~duminsky/&quot;&gt;David Uminsky&lt;/a&gt; (Faculty/director of MS data science) for their help with the notation presented here.&lt;/p&gt;
&lt;h2 id=&quot;reference&quot;&gt;Matrix Calculus Reference&lt;/h2&gt;
&lt;h3 id=&quot;sec8.1&quot;&gt;Gradients and Jacobians&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;gradient&lt;/em&gt; of a function of two variables is a horizontal 2-vector:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-D72132A48C466D3BFB703D0F1E183152.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The &lt;em&gt;Jacobian&lt;/em&gt; of a vector-valued function that is a function of a vector is an &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-FBFEB9C8459FEE5A2BD529C07B881153-depth001.08.svg&quot; /&gt; (&lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-4B7BE5D4BAEFA7643CD9638A527AC10F-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-E3114C625CDDDC18ED29BA629242BD65-depth003.25.svg&quot; /&gt;) matrix containing all possible scalar partial derivatives:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-137DC03E772BD8D2A21C78E3A744132A.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The Jacobian of the identity function &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-3ABE4A0471143ABFC180C9FA485E5F0A-depth003.25.svg&quot; /&gt; is &lt;span class=&quot;eqn&quot;&gt;I&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&quot;sec8.2&quot;&gt;Element-wise operations on vectors&lt;/h3&gt;
&lt;p&gt;Define generic &lt;em&gt;element-wise operations&lt;/em&gt; on vectors &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; and &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; using operator &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-1A74909B6CBAA4532A76D83B72C12DE0-depth002.52.svg&quot; /&gt; such as &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-26B17225B626FB9238849FD60EABDF60-depth001.06.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-70BD11FE09064F041D0EBEC6D8E84FBA.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The Jacobian with respect to &lt;span class=&quot;eqnvec&quot;&gt;w&lt;/span&gt; (similar for &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;) is:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-A61DCB134D4B8F779EA6856022B98B45.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Given the constraint (&lt;em&gt;element-wise diagonal condition&lt;/em&gt;) that &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-BD5A63074F44F11CB2ED06325816582A-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-5E597DAA4E5D9263DCBFB6AB02BDB67F-depth003.25.svg&quot; /&gt; access at most &lt;span class=&quot;eqn&quot;&gt;w&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; and &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;, respectively, the Jacobian simplifies to a diagonal matrix:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-80D7EB6F16ABCCCAA6F1CFB0D7CA05D2.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Here are some sample element-wise operators:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-0D9C6372E2681B466B6E1AF1373C07F4.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;h3 id=&quot;sec8.3&quot;&gt;Scalar expansion&lt;/h3&gt;
&lt;p&gt;Adding scalar &lt;span class=&quot;eqn&quot;&gt;z&lt;/span&gt; to vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-6307CEA088D2D4E98E5B163B9CE8F510-depth002.33.svg&quot; /&gt;, is really &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-8D9C0B9B15490F45C353D9DE64565A4F-depth003.25.svg&quot; /&gt; where &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-3ABE4A0471143ABFC180C9FA485E5F0A-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-BD2335FC4BBF16BE9590D2501CE8C030-depth003.25.svg&quot; /&gt;.&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-32264352DB9E0540766087FB1B70A249.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-A328432124854CC510FE59FEC916AC6C.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Scalar multiplication yields:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-2A0AFB09BE7BE042E565C9C7FCC8B136.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-82B897A0A23D9C6BB66EDF17D1D3CB02.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;h3 id=&quot;sec8.4&quot;&gt;Vector reductions&lt;/h3&gt;
&lt;p&gt;The partial derivative of a vector sum with respect to one of the vectors is:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-77BD17F51D7E67D76D508948DB571A81.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;For &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-5FAFFB6A723E437AC6433DCA0B269846-depth003.25.svg&quot; /&gt;:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-7D69817EEF004855C22D1BB441F8C8BF.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;For &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-9D785998BBDB763E1D5EE5546D47E47E-depth003.25.svg&quot; /&gt; and &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-B00911788A7536593130B4C89B6653A2-depth003.25.svg&quot; /&gt;, we get:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-C49A553A6A2A5CFFDF4A4EE2C82A7C06.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-81FC9F39A0B5FBAD8CE9B15CF7870207.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Vector dot product &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-CF556A1D1CDF0863FDD547594536501A-depth003.31.svg&quot; /&gt;. Substituting &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-02A67275B737296ACC7D3FACA124192C-depth001.08.svg&quot; /&gt; and using the vector chain rule, we get:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-8FF1F0A13C3FF271E22ADEC15D7F10DC.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;Similarly, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-D0E108FD2A63FFC84018DF9BCBE4C91B-depth004.58.svg&quot; /&gt;.&lt;/p&gt;
&lt;h3 id=&quot;sec8.5&quot;&gt;Chain rules&lt;/h3&gt;
&lt;p&gt;The &lt;em&gt;vector chain rule&lt;/em&gt; is the general form as it degenerates to the others. When &lt;span class=&quot;eqn&quot;&gt;f&lt;/span&gt; is a function of a single variable &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; and all intermediate variables &lt;span class=&quot;eqn&quot;&gt;u&lt;/span&gt; are functions of a single variable, the single-variable chain rule applies. When some or all of the intermediate variables are functions of multiple variables, the single-variable total-derivative chain rule applies. In all other cases, the vector chain rule applies.&lt;/p&gt;
&lt;center&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th align=&quot;left&quot;&gt;Single-variable rule&lt;/th&gt;
&lt;th align=&quot;left&quot;&gt;Single-variable total-derivative rule&lt;/th&gt;
&lt;th align=&quot;left&quot;&gt;Vector rule&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-32235B531450ABE9E39C9C91D083A8E2-depth004.58.svg&quot; /&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-ED4CD5FBA6B6EC8A51FA203E2AAFF531-depth004.67.svg&quot; /&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;&lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-6396586BD585A1CFB33959EB6FA8BFA0-depth006.23.svg&quot; /&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/center&gt;
&lt;h2 id=&quot;notation&quot;&gt;Notation&lt;/h2&gt;
&lt;p&gt;Lowercase letters in bold font such as &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; are vectors and those in italics font like &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; are scalars. &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt; is the &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-97361F12A3555FC4FC4E2FFCE1799AC3-depth000.14.svg&quot; /&gt; element of vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt; and is in italics because a single vector element is a scalar. &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-DEA8E196A572D082201CD5ABF2FA82DE-depth003.25.svg&quot; /&gt; means “length of vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;.”&lt;/p&gt;
&lt;p&gt;The &lt;span class=&quot;eqn&quot;&gt;T&lt;/span&gt; exponent of &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-FAAEE783424BC0E27E9AA2F56A7B50B8-depth000.00.svg&quot; /&gt; represents the transpose of the indicated vector.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-60D8CDDDBF54CF43BC22AF322D2BB8E3-depth003.31.svg&quot; /&gt; is just a for-loop that iterates &lt;span class=&quot;eqn&quot;&gt;i&lt;/span&gt; from &lt;span class=&quot;eqn&quot;&gt;a&lt;/span&gt; to &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt;, summing all the &lt;span class=&quot;eqn&quot;&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Notation &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-50BBD36E1FD2333108437A2CA378BE62-depth003.25.svg&quot; /&gt; refers to a function called &lt;span class=&quot;eqn&quot;&gt;f&lt;/span&gt; with an argument of &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;eqn&quot;&gt;I&lt;/span&gt; represents the square “identity matrix” of appropriate dimensions that is zero everywhere but the diagonal, which contains all ones.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-4935E4C6B875FD6C7C181871B566AB1A-depth003.25.svg&quot; /&gt; constructs a matrix whose diagonal elements are taken from vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;: &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-358C0E8F99CB66017CCD32DBC6C23C50-depth003.25.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;The dot product &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-9CA8C3CD16894AF7620468A20C53D6FA-depth000.00.svg&quot; /&gt; is the summation of the element-wise multiplication of the elements: &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-4B722CF0EFB8F8ABCBB968086BB587E8-depth003.31.svg&quot; /&gt;. Or, you can look at it as &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-CF0A6D64FC3321DB0EC98B7683024367-depth000.22.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;Differentiation &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-624FEBB9A49A3FC96353C861D175C806-depth004.58.svg&quot; /&gt; is an operator that maps a function of one parameter to another function. That means that &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-EC0CEC5F9488EC510F8D688E7003222D-depth004.58.svg&quot; /&gt; maps &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-50BBD36E1FD2333108437A2CA378BE62-depth003.25.svg&quot; /&gt; to its derivative with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;, which is the same thing as &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-07A5EA519C4CEA1A3539E3A7FC289163-depth004.58.svg&quot; /&gt;. Also, if &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-FD91C508F91C2C84498680BD337C1D7A-depth003.25.svg&quot; /&gt;, then &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-B1ED3CF9BA4D6F25A5A4F481C45EC658-depth004.58.svg&quot; /&gt;.&lt;/p&gt;
&lt;p&gt;The partial derivative of the function with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-2E335C72CE43928A54BED52C5A6FCC87-depth004.67.svg&quot; /&gt;, performs the usual scalar derivative holding all other variables constant.&lt;/p&gt;
&lt;p&gt;The gradient of &lt;span class=&quot;eqn&quot;&gt;f&lt;/span&gt; with respect to vector &lt;span class=&quot;eqnvec&quot;&gt;x&lt;/span&gt;, &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-69AA2CF3DFE9D41CB1DB567D1B0AD275-depth003.25.svg&quot; /&gt;, organizes all of the partial derivatives for a specific scalar function.&lt;/p&gt;
&lt;p&gt;The Jacobian organizes the gradients of multiple functions into a matrix by stacking them:&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-620A2D5A110082A77BCB7A2BA1E00590.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;The following notation means that &lt;span class=&quot;eqn&quot;&gt;y&lt;/span&gt; has the value &lt;span class=&quot;eqn&quot;&gt;a&lt;/span&gt; upon &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-585B40029A25F6E19FF42DBC26AE5702-depth001.95.svg&quot; /&gt; and value &lt;span class=&quot;eqn&quot;&gt;b&lt;/span&gt; upon &lt;img src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/eqn-CBECF4275AFD44DAD4B312042088DA7E-depth001.95.svg&quot; /&gt;.&lt;/p&gt;
&lt;div&gt;&lt;img class=&quot;blkeqn&quot; src=&quot;http://parrt.cs.usfca.edu/doc/matrix-calculus/images/blkeqn-AE0AEA302ADBC6C30F0A32446C7912AA.svg&quot; alt=&quot;&quot; /&gt;&lt;/div&gt;
&lt;h2 id=&quot;sec10&quot;&gt;Resources&lt;/h2&gt;
&lt;p&gt;When looking for resources on the web, search for “matrix calculus” not “vector calculus.” Here are some comments on the top links that come up from a &lt;a href=&quot;https://www.google.com/search?q=matrix+calculus&amp;amp;oq=matrix+calculus&quot;&gt;Google search&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;To learn more about neural networks and the mathematics behind optimization and back propagation, we highly recommend &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap1.html&quot;&gt;Michael Nielsen's book&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We reference the law of &lt;a href=&quot;https://en.wikipedia.org/wiki/Total_derivative&quot;&gt;total derivative&lt;/a&gt;, which is an important concept that just means derivatives with respect to &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; must take into consideration the derivative with respect &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt; of all variables that are a function of &lt;span class=&quot;eqn&quot;&gt;x&lt;/span&gt;.&lt;/p&gt;
&lt;/body&gt;</description>
<pubDate>Tue, 30 Jan 2018 17:40:18 +0000</pubDate>
<dc:creator>jph00</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>http://parrt.cs.usfca.edu/doc/matrix-calculus/index.html</dc:identifier>
</item>
<item>
<title>Regexper – Regular expressions visualizer</title>
<link>https://regexper.com/</link>
<guid isPermaLink="true" >https://regexper.com/</guid>
<description>&lt;header&gt;&lt;div class=&quot;logo&quot;&gt;
        
        &lt;!-- n. One who regexpes --&gt;
        &lt;span&gt;You thought you only had two problems…&lt;/span&gt;
      &lt;/div&gt;

      &lt;nav&gt;&lt;ul&gt;&lt;li&gt;
            &lt;a class=&quot;inline-icon&quot; href=&quot;https://regexper.com/changelog.html&quot;&gt;&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot; viewbox=&quot;0 0 8 8&quot;&gt;&lt;use xlink:href=&quot;#list-rich&quot;/&gt;&lt;/svg&gt;Changelog&lt;/a&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;a class=&quot;inline-icon&quot; href=&quot;https://regexper.com/documentation.html&quot;&gt;&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot; viewbox=&quot;0 0 8 8&quot;&gt;&lt;use xlink:href=&quot;#document&quot;/&gt;&lt;/svg&gt;Documentation&lt;/a&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;a class=&quot;inline-icon&quot; href=&quot;https://github.com/javallone/regexper-static&quot;&gt;&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot; viewbox=&quot;0 0 8 8&quot;&gt;&lt;use xlink:href=&quot;#code&quot;/&gt;&lt;/svg&gt;Source on GitHub&lt;/a&gt;
          &lt;/li&gt;
        &lt;/ul&gt;&lt;/nav&gt;&lt;/header&gt;&lt;main id=&quot;content&quot;&gt;&lt;div class=&quot;application&quot;&gt;
      &lt;form id=&quot;regexp-form&quot;&gt;
        &lt;textarea id=&quot;regexp-input&quot; autofocus=&quot;autofocus&quot; placeholder=&quot;Enter JavaScript-style regular expression to display&quot;/&gt;&lt;button type=&quot;submit&quot;&gt;Display&lt;/button&gt;

        &lt;ul class=&quot;inline-list&quot;&gt;&lt;li class=&quot;download&quot;&gt;
            &lt;a href=&quot;https://regexper.com/#&quot; class=&quot;inline-icon&quot; data-action=&quot;download&quot; download=&quot;image.svg&quot; type=&quot;image/svg+xml&quot;&gt;&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot; viewbox=&quot;0 0 8 8&quot;&gt;&lt;use xlink:href=&quot;#data-transfer-download&quot;/&gt;&lt;/svg&gt;Download&lt;/a&gt;
          &lt;/li&gt;
          &lt;li class=&quot;permalink&quot;&gt;
            &lt;a href=&quot;https://regexper.com/#&quot; class=&quot;inline-icon&quot; data-action=&quot;permalink&quot;&gt;&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; xmlns:xlink=&quot;http://www.w3.org/1999/xlink&quot; viewbox=&quot;0 0 8 8&quot;&gt;&lt;use xlink:href=&quot;#link-intact&quot;/&gt;&lt;/svg&gt;Permalink&lt;/a&gt;
          &lt;/li&gt;
        &lt;/ul&gt;&lt;/form&gt;
    &lt;/div&gt;

    &lt;div class=&quot;results&quot;&gt;
      

      &lt;ul id=&quot;warnings&quot;/&gt;
    &lt;/div&gt;


    &lt;/main&gt;&lt;footer&gt;&lt;ul class=&quot;inline-list&quot;&gt;&lt;li&gt;Created by &lt;a href=&quot;mailto:jeff.avallone@gmail.com&quot;&gt;Jeff Avallone&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;
            Generated images licensed:
            &lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by/3.0/&quot;&gt;&lt;img alt=&quot;Creative Commons License&quot; src=&quot;https://licensebuttons.net/l/by/3.0/80x15.png&quot;/&gt;&lt;/a&gt;
          &lt;/li&gt;
        &lt;/ul&gt;&lt;/footer&gt;&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; id=&quot;open-iconic&quot;&gt;&lt;!-- These icon are from the Open Iconic project https://useiconic.com/open/ --&gt;&lt;defs&gt;&lt;g id=&quot;code&quot;&gt;&lt;path d=&quot;M5 0l-3 6h1l3-6h-1zm-4 1l-1 2 1 2h1l-1-2 1-2h-1zm5 0l1 2-1 2h1l1-2-1-2h-1z&quot; transform=&quot;translate(0 1)&quot;/&gt;&lt;/g&gt;&lt;g id=&quot;data-transfer-download&quot;&gt;&lt;path d=&quot;M3 0v3h-2l3 3 3-3h-2v-3h-2zm-3 7v1h8v-1h-8z&quot;/&gt;&lt;/g&gt;&lt;g id=&quot;document&quot;&gt;&lt;path d=&quot;M0 0v8h7v-4h-4v-4h-3zm4 0v3h3l-3-3zm-3 2h1v1h-1v-1zm0 2h1v1h-1v-1zm0 2h4v1h-4v-1z&quot;/&gt;&lt;/g&gt;&lt;g id=&quot;link-intact&quot;&gt;&lt;path d=&quot;M5.88.03c-.18.01-.36.03-.53.09-.27.1-.53.25-.75.47a.5.5 0 1 0 .69.69c.11-.11.24-.17.38-.22.35-.12.78-.07 1.06.22.39.39.39 1.04 0 1.44l-1.5 1.5c-.44.44-.8.48-1.06.47-.26-.01-.41-.13-.41-.13a.5.5 0 1 0-.5.88s.34.22.84.25c.5.03 1.2-.16 1.81-.78l1.5-1.5c.78-.78.78-2.04 0-2.81-.28-.28-.61-.45-.97-.53-.18-.04-.38-.04-.56-.03zm-2 2.31c-.5-.02-1.19.15-1.78.75l-1.5 1.5c-.78.78-.78 2.04 0 2.81.56.56 1.36.72 2.06.47.27-.1.53-.25.75-.47a.5.5 0 1 0-.69-.69c-.11.11-.24.17-.38.22-.35.12-.78.07-1.06-.22-.39-.39-.39-1.04 0-1.44l1.5-1.5c.4-.4.75-.45 1.03-.44.28.01.47.09.47.09a.5.5 0 1 0 .44-.88s-.34-.2-.84-.22z&quot;/&gt;&lt;/g&gt;&lt;g id=&quot;list-rich&quot;&gt;&lt;path d=&quot;M0 0v3h3v-3h-3zm4 0v1h4v-1h-4zm0 2v1h3v-1h-3zm-4 2v3h3v-3h-3zm4 0v1h4v-1h-4zm0 2v1h3v-1h-3z&quot;/&gt;&lt;/g&gt;&lt;g id=&quot;warning&quot;&gt;&lt;path d=&quot;M3.09 0c-.06 0-.1.04-.13.09l-2.94 6.81c-.02.05-.03.13-.03.19v.81c0 .05.04.09.09.09h6.81c.05 0 .09-.04.09-.09v-.81c0-.05-.01-.14-.03-.19l-2.94-6.81c-.02-.05-.07-.09-.13-.09h-.81zm-.09 3h1v2h-1v-2zm0 3h1v1h-1v-1z&quot;/&gt;&lt;/g&gt;&lt;/defs&gt;&lt;/svg&gt;</description>
<pubDate>Tue, 30 Jan 2018 15:13:00 +0000</pubDate>
<dc:creator>xchip</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://regexper.com/</dc:identifier>
</item>
<item>
<title>Amazon, Berkshire, JPMorgan to Create Healthcare Company</title>
<link>https://www.bloomberg.com/news/articles/2018-01-30/amazon-berkshire-jpmorgan-to-create-healthcare-company-jd1lraa9</link>
<guid isPermaLink="true" >https://www.bloomberg.com/news/articles/2018-01-30/amazon-berkshire-jpmorgan-to-create-healthcare-company-jd1lraa9</guid>
<description>&lt;p&gt;It’s no secret Jeff Bezos has been looking to crack health care. But no one expected him to pull in Warren Buffett and Jamie Dimon, too.&lt;/p&gt;


&lt;p&gt;News Tuesday that Bezos’s &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.bloomberg.com/quote/AMZN:US&quot; title=&quot;Bloomberg Intelligence Profile&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Amazon.com Inc.&lt;/a&gt;, Buffett’s Berkshire Hathaway Inc. and &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.bloomberg.com/quote/JPM:US&quot; title=&quot;Bloomberg Intelligence Profile&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;JPMorgan Chase &amp;amp; Co.&lt;/a&gt;, led by Dimon, plan to join forces to change how health care is provided to their combined 1 million U.S. employees sent shock waves through the health-care industry.&lt;/p&gt;


&lt;p&gt;The plan, while in early stages and focused solely on the three giants’ staff for now, seems almost certain to set its sights on disrupting the broader industry. It’s the first big move by Amazon in the sector after months of speculation that the internet behemoth might make an entry. The Amazon-Berkshire-JPMorgan collaboration will likely pressure profits for middlemen in the health-care supply chain.&lt;/p&gt;



&lt;p&gt;Details were scant in &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.bloomberg.com/news/terminal/P3DAO0MEQTXQ&quot; title=&quot;Amazon, Berkshire Hathaway and JPMorgan Chase &amp;amp; Co. to partner on U.S. employee healthcare&quot; class=&quot;terminal-news-story&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;a short joint statement&lt;/a&gt; on Tuesday. The three companies said they plan to set up a new independent company “that is free from profit-making incentives and constraints.”&lt;/p&gt;


&lt;p&gt;It was enough to sink health-care stocks. &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.bloomberg.com/quote/ESRX:US&quot; title=&quot;Pre-Trading&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Express Scripts Holding Co.&lt;/a&gt; and CVS Health Corp., which manage pharmacy benefits, slumped 6.9 percent and 4.9 percent, respectively. Health insurers such as Cigna Corp. and Anthem Inc. and &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.bloomberg.com/news/articles/2018-01-30/biotech-investors-fear-bezos-more-than-trump-as-costs-targeted&quot; title=&quot;Biotech Investors Fear Bezos More Than Trump as Costs Targeted&quot; target=&quot;_blank&quot;&gt;biotechnology companies&lt;/a&gt; also dropped.&lt;/p&gt;
&lt;div class=&quot;image&quot;&gt;
&lt;div id=&quot;lazy-img-323212058&quot; class=&quot;lazy-img&quot;&gt;&lt;img src=&quot;https://assets.bwbx.io/images/users/iqjWHBFdfxIU/i_gbRm3cmCF8/v2/60x-1.png&quot; data-native-src=&quot;https://assets.bwbx.io/images/users/iqjWHBFdfxIU/i_gbRm3cmCF8/v2/-1x-1.png&quot; class=&quot;lazy-img__image&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The group announced the news in the very early stages because it plans to hire a CEO and start partnering with other organizations, according to a person familiar with the matter. The effort would be focused internally first, and the companies would bring their data and bargaining power to bear on lowering health-care costs, the person said. Potential ways to bring down costs include providing more transparency over the prices for doctor visits and lab tests, as well as by enabling direct purchasing of some medical items, the person said.&lt;/p&gt;
&lt;p&gt;“I’m in favor of anything that helps move the markets a bit, incentivizes competition and puts pressure on the big insurance carriers,” said Ashraf Shehata, a partner in KPMG LLP’s health care and life sciences advisory practice in the U.S. “An employer coalition can do a lot of things. You can encourage reimbursement models and provide incentives for the use of technology.”&lt;/p&gt;
&lt;p&gt;“Hard as it might be, reducing health care’s burden on the economy while improving outcomes for employees and their families would be worth the effort,” Bezos said in the statement. “Success is going to require talented experts, a beginner’s mind, and a long-term orientation.”&lt;/p&gt;

&lt;p&gt;The initial focus of the new company will be on technology solutions that will provide U.S. employees and their families with simplified, high-quality and transparent health care at a reasonable costs. In the statement, JPMorgan CEO Dimon said the initiative could ultimately expand beyond the three companies.&lt;/p&gt;
&lt;p&gt;“Our goal is to create solutions that benefit our U.S. employees, their families and, potentially, all Americans,” he said.&lt;/p&gt;
&lt;h3&gt;HTA Alliance&lt;/h3&gt;
&lt;p&gt;Amazon, Berkshire and JPMorgan are among the largest private employers in the U.S. And they’re among the most valuable, with a combined market capitalization of $1.6 trillion, according to data compiled by Bloomberg.&lt;/p&gt;
&lt;p&gt;This isn’t the first time big companies have teamed up in an effort to tackle health-care costs. International Business Machines Corp., Berkshire’s BNSF Railway and American Express Co. were among the founding members of the &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;http://www.htahealth.com&quot; title=&quot;Link to website&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Health Transformation Alliance&lt;/a&gt;, which now includes about 40 big companies that want to &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;http://www.htahealth.com/wp-content/uploads/2017/11/TG-Leading_US_Companies_Announce_Plan_to_Transform_the_Corporate_Health_Care_System.pdf&quot; title=&quot;Link to statement&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;transform&lt;/a&gt; health care. The group ultimately &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;http://www.htahealth.com/wp-content/uploads/2017/11/TG-Private_Sector_Health_Care_Reform_Moves_Ahead-1.pdf&quot; title=&quot;Link to statement&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;partnered&lt;/a&gt; with existing industry players including CVS and UnitedHealth Group Inc.’s OptumRx.&lt;/p&gt;
&lt;h3&gt;Top Team&lt;/h3&gt;
&lt;p&gt;The latest effort is being spearheaded by Todd Combs, who helps oversee investments at Berkshire; Marvelle Sullivan Berchtold, a managing director of JPMorgan; and Beth Galetti, a senior vice president for human resources at Amazon.&lt;/p&gt;
&lt;div class=&quot;image&quot;&gt;
&lt;div id=&quot;lazy-img-323197272&quot; class=&quot;lazy-img&quot;&gt;&lt;img src=&quot;https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iE99LwnqvreU/v0/60x-1.jpg&quot; data-native-src=&quot;https://assets.bwbx.io/images/users/iqjWHBFdfxIU/iE99LwnqvreU/v0/-1x-1.jpg&quot; class=&quot;lazy-img__image&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;news-figure-caption-text caption&quot;&gt;
&lt;p&gt;Todd Combs&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Photographer: Daniel Acker/Bloomberg&lt;/p&gt;
&lt;p&gt;Buffett handpicked Combs in 2010 as one of his two key stockpickers. Combs, 47, has been taking on a larger role at Berkshire in recent years, and Buffett has said that Combs and Ted Weschler, who also helps oversee investments, will eventually manage the company’s whole portfolio. Combs also joined JPMorgan’s board in 2016.&lt;/p&gt;
&lt;p&gt;Sullivan Berchtold joined JPMorgan in August after eight years at the Swiss pharmaceutical company Novartis AG, where she was most recently the global head of mergers and acquisitions, according to her &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.linkedin.com/in/marvellesullivan/&quot; title=&quot;Click to view webpage.&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;LinkedIn&lt;/a&gt; profile.&lt;/p&gt;
&lt;aside class=&quot;inline-newsletter&quot; data-state=&quot;ready&quot;/&gt;&lt;p&gt;One of the highest ranking women at Amazon, Galetti has worked in human resources at the e-commerce giant since mid-2013, becoming senior vice president almost two years ago, according to her &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.linkedin.com/in/beth-galetti-60b1106/&quot; title=&quot;Click to view webpage.&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;LinkedIn profile&lt;/a&gt;. As of late 2017 she was the only woman on Amazon’s elite S-team, a group of just over a dozen senior executives who meet regularly with Bezos, according to published &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.nytimes.com/2017/10/20/technology/amazon-sexual-harassment.html&quot; title=&quot;Click to view webpage.&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;reports&lt;/a&gt;. Previously Galetti worked in planning, engineering and operations at FedEx Express, the cargo airline of FedEx Corp. She has a degree in electrical engineering from Lehigh University and an MBA from Colorado Technical University.&lt;/p&gt;
&lt;p&gt;The management team, location of the headquarters and other operational details will be announced later, the companies said.&lt;/p&gt;
&lt;p&gt;Health-care spending was estimated to account for about 18 percent of the U.S. economy last year, far more than in other developed nations. Buffett has long bemoaned the cost of U.S. health care. Last year, he came out in favor of drastic changes in the U.S. health system, &lt;a itemscope=&quot;itemscope&quot; itemprop=&quot;StoryLink&quot; href=&quot;https://www.pbs.org/newshour/show/america-stand-just-wealth-says-warren-buffett&quot; title=&quot;Link to story&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;telling PBS NewsHour&lt;/a&gt; that government-run health care is probably the best approach and would bring down costs.&lt;/p&gt;
&lt;p&gt;“The ballooning costs of health care act as a hungry tapeworm on the American economy,” Buffett said in Tuesday’s statement. “Our group does not come to this problem with answers. But we also do not accept it as inevitable.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;— With assistance by Jennifer Surane, Katherine Chiglinsky, John Lauerman, Chitra Somayaji, Michelle Cortez, and Nancy Moran&lt;/em&gt;&lt;/p&gt;
</description>
<pubDate>Tue, 30 Jan 2018 12:18:26 +0000</pubDate>
<dc:creator>uptown</dc:creator>
<og:description>It’s no secret Jeff Bezos has been looking to crack health care. But no one expected him to pull in Warren Buffett and Jamie Dimon, too.</og:description>
<og:image>https://assets.bwbx.io/s3/javelin/public/javelin/images/social-default-a4f15fa7ee.jpg</og:image>
<og:title>Amazon, Berkshire, JPMorgan Link Up to Form New Health-Care Company</og:title>
<og:type>article</og:type>
<og:url>https://www.bloomberg.com/news/articles/2018-01-30/amazon-berkshire-jpmorgan-to-set-up-a-health-company-for-staff</og:url>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.bloomberg.com/news/articles/2018-01-30/amazon-berkshire-jpmorgan-to-set-up-a-health-company-for-staff</dc:identifier>
</item>
<item>
<title>UK mass digital surveillance regime ruled unlawful</title>
<link>https://www.theguardian.com/uk-news/2018/jan/30/uk-mass-digital-surveillance-regime-ruled-unlawful-appeal-ruling-snoopers-charter</link>
<guid isPermaLink="true" >https://www.theguardian.com/uk-news/2018/jan/30/uk-mass-digital-surveillance-regime-ruled-unlawful-appeal-ruling-snoopers-charter</guid>
<description>&lt;header class=&quot;content__head content__head--article tonal__head tonal__head--tone-news&quot;&gt;
&lt;div class=&quot;content__labels content__labels--not-immersive&quot;&gt;
&lt;div class=&quot;content__section-label&quot;&gt;&lt;a class=&quot;content__section-label__link&quot; data-link-name=&quot;article section&quot; href=&quot;https://www.theguardian.com/uk/uksecurity&quot;&gt;UK security and counter-terrorism&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;content__headline-standfirst-wrapper&quot;&gt;

&lt;div class=&quot;tonal__standfirst u-cf&quot;&gt;
&lt;div class=&quot;content__standfirst&quot; data-link-name=&quot;standfirst&quot; data-component=&quot;standfirst&quot;&gt;
&lt;p&gt;Judges say snooper’s charter lacks adequate safeguards around accessing personal data&lt;br/&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div class=&quot;u-responsive-ratio&quot;&gt;&lt;img class=&quot;maxed responsive-img&quot; itemprop=&quot;contentUrl&quot; alt=&quot;A man on his mobile phone walks past a CCTV sign&quot; src=&quot;https://i.guim.co.uk/img/media/b7b056ff959f9f266e06eabde0df85df0e06a7d2/246_151_3714_2228/master/3714.jpg?w=300&amp;amp;q=55&amp;amp;auto=format&amp;amp;usm=12&amp;amp;fit=max&amp;amp;s=e57aa35b6e4a3db04dbb79e305291d24&quot;/&gt;&lt;/div&gt;
&lt;input type=&quot;checkbox&quot; id=&quot;show-caption&quot; class=&quot;mobile-only u-h reveal-caption__checkbox&quot;/&gt; The court said the law allowed police to authorise their own access to personal phone and web records without adequate oversight. Photograph: Felix Clay for the Guardian&lt;/header&gt;&lt;p&gt;Appeal court judges have ruled the government’s mass digital surveillance regime unlawful in a case brought by the Labour deputy leader, Tom Watson.&lt;/p&gt;
&lt;p&gt;Liberty, the human rights campaign group which represented Watson in the case, said the ruling meant significant parts of theInvestigatory Powers Act 2016 – known as the snooper’s charter – are effectively unlawful and must be urgently changed.&lt;/p&gt;
&lt;p&gt;The government defended its use of communications data to fight serious and organised crime and said that the judgment related to out of date legislation. Minister Ben Wallace said that it would not affect the way law enforcement would tackle crime.&lt;/p&gt;
&lt;p&gt;The court of appeal ruling on Tuesday said the powers in the Data Retention and Investigatory Powers Act 2014, which paved the way for the snooper’s charter legislation, did not restrict the accessing of confidential personal phone and web browsing records to investigations of serious crime, and allowed police and other public bodies to authorise their own access without adequate oversight.&lt;/p&gt;
&lt;p&gt;The three judges said Dripa was “inconsistent with EU law” because of this lack of safeguards, including the absence of “prior review by a court or independent administrative authority”.&lt;/p&gt;
&lt;p&gt;Responding to the ruling, Watson said: “This legislation was flawed from the start. It was rushed through parliament just before recess without proper parliamentary scrutiny.&lt;/p&gt;
&lt;p&gt;“The government must now bring forward changes to the Investigatory Powers Act to ensure that hundreds of thousands of people, many of whom are innocent victims or witnesses to crime, are protected by a system of independent approval for access to communications data. I’m proud to have played my part in safeguarding citizens’ fundamental rights.”&lt;/p&gt;
&lt;p&gt;Martha Spurrier, the director of Liberty, said: “Yet again a UK court has ruled the government’s extreme mass surveillance regime unlawful. This judgement tells ministers in crystal clear terms that they are breaching the public’s human rights.”&lt;/p&gt;
&lt;p&gt;She said no politician was above the law. “When will the government stop bartering with judges and start drawing up a surveillance law that upholds our democratic freedoms?”&lt;/p&gt;
&lt;p&gt;The Home Office &lt;a href=&quot;https://www.theguardian.com/technology/2017/nov/30/police-to-lose-phone-and-web-data-search-authorisation-powers&quot; data-link-name=&quot;in body link&quot; class=&quot;u-underline&quot;&gt;announced a series of safeguards&lt;/a&gt; in November in anticipation of the ruling. They include removing the power of self-authorisation for senior police officers and requiring approval for requests for confidential communications data to be granted by the new investigatory powers commissioner. Watson and other campaigners said the safeguards were “half-baked” and did not go far enough.&lt;br tabindex=&quot;-1&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The judges, headed by Sir Geoffrey Vos, declined to rule on the Home Office claim that the more rigorous “Watson safeguards” were not necessary for the use of bulk communications data for wider national security purposes.&lt;br tabindex=&quot;-1&quot;/&gt;
The judges said the appeal court did not need to rule on this point because it had already been referred to the European court of justice in a case which is due to be heard in February.&lt;/p&gt;
&lt;p&gt;Watson launched his legal challenge in 2014 in partnership with David Davis, who withdrew when he entered the government as Brexit secretary in 2016. The European court of justice &lt;a draggable=&quot;true&quot; href=&quot;https://www.theguardian.com/law/2016/dec/21/eus-highest-court-delivers-blow-to-uk-snoopers-charter&quot; data-link-name=&quot;in body link&quot; class=&quot;u-underline&quot;&gt;ruled in December 2016&lt;/a&gt; that the “general and indiscriminate retention” of confidential personal communications data was unlawful without safeguards, including independent judicial authorisation.&lt;/p&gt;
&lt;p&gt;Security minister Ben Wallace responded to the ruling saying: “Communications data is used in the vast majority of serious and organised crime prosecutions and has been used in every major security service counter-terrorism investigation over the last decade. It is often the only way to identify paedophiles involved in online child abuse as it can be used to find where and when these horrendous crimes have taken place.”&lt;/p&gt;
&lt;p&gt;He said the judgment related to legislation which was no longer in force and did not change the way in which law enforcement agencies could detect and disrupt crimes.&lt;/p&gt;
&lt;p&gt;“We had already announced that we would be amending the Investigatory Powers Act to address the two areas in which the court of appeal has found against the previous data retention regime. We welcome the fact that the court of appeal ruling does not undermine the regime and we will continue to defend these vital powers, which Parliament agreed were necessary in 2016, in ongoing litigation,” he said.&lt;/p&gt;


</description>
<pubDate>Tue, 30 Jan 2018 11:43:01 +0000</pubDate>
<dc:creator>robin_reala</dc:creator>
<og:url>http://www.theguardian.com/uk-news/2018/jan/30/uk-mass-digital-surveillance-regime-ruled-unlawful-appeal-ruling-snoopers-charter</og:url>
<og:description>Judges say snooper’s charter lacks adequate safeguards around accessing personal data</og:description>
<og:image>https://i.guim.co.uk/img/media/b7b056ff959f9f266e06eabde0df85df0e06a7d2/246_151_3714_2228/master/3714.jpg?w=1200&amp;h=630&amp;q=55&amp;auto=format&amp;usm=12&amp;fit=crop&amp;crop=faces%2Centropy&amp;bm=normal&amp;ba=bottom%2Cleft&amp;blend64=aHR0cHM6Ly91cGxvYWRzLmd1aW0uY28udWsvMjAxOC8wMS8xOC9mYWNlYm9va19kZWZhdWx0LnBuZw&amp;s=4c7f93e0a828199f7f15fd05b6a5c946</og:image>
<og:type>article</og:type>
<og:title>UK mass digital surveillance regime ruled unlawful</og:title>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.theguardian.com/uk-news/2018/jan/30/uk-mass-digital-surveillance-regime-ruled-unlawful-appeal-ruling-snoopers-charter</dc:identifier>
</item>
</channel>
</rss>