<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Shell Style Guide</title>
<link>https://google.github.io/styleguide/shell.xml</link>
<guid isPermaLink="true" >https://google.github.io/styleguide/shell.xml</guid>
<description>&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;&lt;title&gt;&lt;/title&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;395.8955348449&quot;&gt;
&lt;p align=&quot;right&quot;&gt;Revision 1.26&lt;/p&gt;
&lt;address&gt;Paul Armstrong&lt;br /&gt;Too many more to mention&lt;br /&gt;&lt;/address&gt;
&lt;p&gt;&lt;code&gt;Bash&lt;/code&gt; is the only shell scripting language permitted for executables.&lt;/p&gt;
&lt;p&gt;Executables must start with &lt;code&gt;#!/bin/bash&lt;/code&gt; and a minimum number of flags. Use &lt;code&gt;set&lt;/code&gt; to set shell options so that calling your script as &lt;code&gt;bash &lt;em&gt;&amp;lt;script_name&amp;gt;&lt;/em&gt;&lt;/code&gt; does not break its functionality.&lt;/p&gt;
&lt;p&gt;Restricting all executable shell scripts to &lt;strong&gt;bash&lt;/strong&gt; gives us a consistent shell language that's installed on all our machines.&lt;/p&gt;
&lt;p&gt;The only exception to this is where you're forced to by whatever you're coding for. One example of this is Solaris SVR4 packages which require plain Bourne shell for any scripts.&lt;/p&gt;
&lt;p&gt;Shell should only be used for small utilities or simple wrapper scripts.&lt;/p&gt;
&lt;p&gt;While shell scripting isn't a development language, it is used for writing various utility scripts throughout Google. This style guide is more a recognition of its use rather than a suggestion that it be used for widespread deployment.&lt;/p&gt;
&lt;p&gt;Some guidelines:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;If you're mostly calling other utilities and are doing relatively little data manipulation, shell is an acceptable choice for the task.&lt;/li&gt;
&lt;li&gt;If performance matters, use something other than shell.&lt;/li&gt;
&lt;li&gt;If you find you need to use arrays for anything more than assignment of &lt;code&gt;${PIPESTATUS}&lt;/code&gt;, you should use Python.&lt;/li&gt;
&lt;li&gt;If you are writing a script that is more than 100 lines long, you should probably be writing it in Python instead. Bear in mind that scripts grow. Rewrite your script in another language early to avoid a time-consuming rewrite at a later date.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Executables should have no extension (strongly preferred) or a &lt;code&gt;.sh&lt;/code&gt; extension. Libraries must have a &lt;code&gt;.sh&lt;/code&gt; extension and should not be executable.&lt;/p&gt;
&lt;p&gt;It is not necessary to know what language a program is written in when executing it and shell doesn't require an extension so we prefer not to use one for executables.&lt;/p&gt;
&lt;p&gt;However, for libraries it's important to know what language it is and sometimes there's a need to have similar libraries in different languages. This allows library files with identical purposes but different languages to be identically named except for the language-specific suffix.&lt;/p&gt;
&lt;p&gt;SUID and SGID are &lt;em&gt;forbidden&lt;/em&gt; on shell scripts.&lt;/p&gt;
&lt;p&gt;There are too many security issues with shell that make it nearly impossible to secure sufficiently to allow SUID/SGID. While bash does make it difficult to run SUID, it's still possible on some platforms which is why we're being explicit about banning it.&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;sudo&lt;/code&gt; to provide elevated access if you need it.&lt;/p&gt;
&lt;p&gt;All error messages should go to &lt;code&gt;STDERR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This makes it easier to separate normal status from actual issues.&lt;/p&gt;
&lt;p&gt;A function to print out error messages along with other status information is recommended. err() { echo &quot;[$(date +'%Y-%m-%dT%H:%M:%S%z')]: $@&quot; &amp;gt;&amp;amp;2 } if ! do_something; then err &quot;Unable to do_something&quot; exit &quot;${E_DID_NOTHING}&quot; fi&lt;/p&gt;
&lt;p&gt;Start each file with a description of its contents.&lt;/p&gt;
&lt;p&gt;Every file must have a top-level comment including a brief overview of its contents. A copyright notice and author information are optional.&lt;/p&gt;
&lt;p&gt;Example: #!/bin/bash # # Perform hot backups of Oracle databases.&lt;/p&gt;
&lt;p&gt;Any function that is not both obvious and short must be commented. Any function in a library must be commented regardless of length or complexity.&lt;/p&gt;
&lt;p&gt;It should be possible for someone else to learn how to use your program or to use a function in your library by reading the comments (and self-help, if provided) without reading the code.&lt;/p&gt;
&lt;p&gt;All function comments should contain:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Description of the function&lt;/li&gt;
&lt;li&gt;Global variables used and modified&lt;/li&gt;
&lt;li&gt;Arguments taken&lt;/li&gt;
&lt;li&gt;Returned values other than the default exit status of the last command run&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Example: #!/bin/bash # # Perform hot backups of Oracle databases. export PATH='/usr/xpg4/bin:/usr/bin:/opt/csw/bin:/opt/goog/bin' ####################################### # Cleanup files from the backup dir # Globals: # BACKUP_DIR # ORACLE_SID # Arguments: # None # Returns: # None ####################################### cleanup() { ... }&lt;/p&gt;
&lt;p&gt;Comment tricky, non-obvious, interesting or important parts of your code.&lt;/p&gt;
&lt;p&gt;This follows general Google coding comment practice. Don't comment everything. If there's a complex algorithm or you're doing something out of the ordinary, put a short comment in.&lt;/p&gt;
&lt;p&gt;Use TODO comments for code that is temporary, a short-term solution, or good-enough but not perfect.&lt;/p&gt;
&lt;p&gt;This matches the convention in the &lt;a href=&quot;https://google.github.io/styleguide/cppguide.html?showone=TODO_Comments#TODO_Comments&quot;&gt;C++ Guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;TODOs should include the string TODO in all caps, followed by your username in parentheses. A colon is optional. It's preferable to put a bug/ticket number next to the TODO item as well.&lt;/p&gt;
&lt;p&gt;Examples: # TODO(mrmonkey): Handle the unlikely edge cases (bug ####)&lt;/p&gt;
&lt;p&gt;While you should follow the style that's already there for files that you're modifying, the following are required for any new code.&lt;/p&gt;
&lt;p&gt;Indent 2 spaces. No tabs.&lt;/p&gt;
&lt;p&gt;Use blank lines between blocks to improve readability. Indentation is two spaces. Whatever you do, don't use tabs. For existing files, stay faithful to the existing indentation.&lt;/p&gt;
&lt;p&gt;Maximum line length is 80 characters.&lt;/p&gt;
&lt;p&gt;If you have to write strings that are longer than 80 characters, this should be done with a here document or an embedded newline if possible. Literal strings that have to be longer than 80 chars and can't sensibly be split are ok, but it's strongly preferred to find a way to make it shorter.&lt;/p&gt;
&lt;p&gt;# DO use 'here document's cat &amp;lt;&amp;lt;END; I am an exceptionally long string. END # Embedded newlines are ok too long_string=&quot;I am an exceptionally long string.&quot;&lt;/p&gt;
&lt;p&gt;Pipelines should be split one per line if they don't all fit on one line.&lt;/p&gt;
&lt;p&gt;If a pipeline all fits on one line, it should be on one line.&lt;/p&gt;
&lt;p&gt;If not, it should be split at one pipe segment per line with the pipe on the newline and a 2 space indent for the next section of the pipe. This applies to a chain of commands combined using '|' as well as to logical compounds using '||' and '&amp;amp;&amp;amp;'. # All fits on one line command1 | command2 # Long commands command1 \ | command2 \ | command3 \ | command4&lt;/p&gt;
&lt;p&gt;Put &lt;code&gt;; do&lt;/code&gt; and &lt;code&gt;; then&lt;/code&gt; on the same line as the &lt;code&gt;while&lt;/code&gt;, &lt;code&gt;for&lt;/code&gt; or &lt;code&gt;if&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Loops in shell are a bit different, but we follow the same principles as with braces when declaring functions. That is: &lt;code&gt;; then&lt;/code&gt; and &lt;code&gt;; do&lt;/code&gt; should be on the same line as the if/for/while. &lt;code&gt;else&lt;/code&gt; should be on its own line and closing statements should be on their own line vertically aligned with the opening statement.&lt;/p&gt;
&lt;p&gt;Example: for dir in ${dirs_to_cleanup}; do if [[ -d &quot;${dir}/${ORACLE_SID}&quot; ]]; then log_date &quot;Cleaning up old files in ${dir}/${ORACLE_SID}&quot; rm &quot;${dir}/${ORACLE_SID}/&quot;* if [[ &quot;$?&quot; -ne 0 ]]; then error_message fi else mkdir -p &quot;${dir}/${ORACLE_SID}&quot; if [[ &quot;$?&quot; -ne 0 ]]; then error_message fi fi done&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Indent alternatives by 2 spaces.&lt;/li&gt;
&lt;li&gt;A one-line alternative needs a space after the close parenthesis of the pattern and before the &lt;code&gt;;;&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Long or multi-command alternatives should be split over multiple lines with the pattern, actions, and &lt;code&gt;;;&lt;/code&gt; on separate lines.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The matching expressions are indented one level from the 'case' and 'esac'. Multiline actions are indented another level. In general, there is no need to quote match expressions. Pattern expressions should not be preceded by an open parenthesis. Avoid the &lt;code&gt;;&amp;amp;&lt;/code&gt; and &lt;code&gt;;;&amp;amp;&lt;/code&gt; notations.&lt;/p&gt;
&lt;p&gt;case &quot;${expression}&quot; in a) variable=&quot;...&quot; some_command &quot;${variable}&quot; &quot;${other_expr}&quot; ... ;; absolute) actions=&quot;relative&quot; another_command &quot;${actions}&quot; &quot;${other_expr}&quot; ... ;; *) error &quot;Unexpected expression '${expression}'&quot; ;; esac&lt;/p&gt;
&lt;p&gt;Simple commands may be put on the same line as the pattern &lt;em&gt;and&lt;/em&gt; &lt;code&gt;;;&lt;/code&gt; as long as the expression remains readable. This is often appropriate for single-letter option processing. When the actions don't fit on a single line, put the pattern on a line on its own, then the actions, then &lt;code&gt;;;&lt;/code&gt; also on a line of its own. When on the same line as the actions, use a space after the close parenthesis of the pattern and another before the &lt;code&gt;;;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;verbose='false' aflag='' bflag='' files='' while getopts 'abf:v' flag; do case &quot;${flag}&quot; in a) aflag='true' ;; b) bflag='true' ;; f) files=&quot;${OPTARG}&quot; ;; v) verbose='true' ;; *) error &quot;Unexpected option ${flag}&quot; ;; esac done In order of precedence: Stay consistent with what you find; quote your variables; prefer &quot;${var}&quot; over &quot;$var&quot;, but see details.&lt;/p&gt;
&lt;p&gt;These are meant to be guidelines, as the topic seems too controversial for a mandatory regulation.&lt;br /&gt;They are listed in order of precedence.&lt;/p&gt;
&lt;ol readability=&quot;3.9141981613892&quot;&gt;&lt;li&gt;Stay consistent with what you find for existing code.&lt;/li&gt;
&lt;li&gt;Quote variables, see &lt;a href=&quot;https://google.github.io/styleguide/shell.xml#Quoting&quot;&gt;Quoting section below&lt;/a&gt;.&lt;/li&gt;
&lt;li readability=&quot;11&quot;&gt;
&lt;p&gt;Don't brace-quote single character shell specials / positional parameters, unless strictly necessary or avoiding deep confusion.&lt;br /&gt;Prefer brace-quoting all other variables. # Section of &lt;em&gt;recommended&lt;/em&gt; cases. # Preferred style for 'special' variables: echo &quot;Positional: $1&quot; &quot;$5&quot; &quot;$3&quot; echo &quot;Specials: !=$!, -=$-, _=$_. ?=$?, #=$# *=$* @=$@ \$=$$ ...&quot; # Braces necessary: echo &quot;many parameters: ${10}&quot; # Braces avoiding confusion: # Output is &quot;a0b0c0&quot; set -- a b c echo &quot;${1}0${2}0${3}0&quot; # Preferred style for other variables: echo &quot;PATH=${PATH}, PWD=${PWD}, mine=${some_var}&quot; while read f; do echo &quot;file=${f}&quot; done &amp;lt; &amp;lt;(ls -l /tmp) # Section of &lt;em&gt;discouraged&lt;/em&gt; cases # Unquoted vars, unbraced vars, brace-quoted single letter # shell specials. echo a=$avar &quot;b=$bvar&quot; &quot;PID=${$}&quot; &quot;${1}&quot; # Confusing use: this is expanded as &quot;${1}0${2}0${3}0&quot;, # not &quot;${10}${20}${30} set -- a b c echo &quot;$10$20$30&quot;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;Always quote strings containing variables, command substitutions, spaces or shell meta characters, unless careful unquoted expansion is required.&lt;/li&gt;
&lt;li&gt;Prefer quoting strings that are &quot;words&quot; (as opposed to command options or path names).&lt;/li&gt;
&lt;li&gt;Never quote &lt;em&gt;literal&lt;/em&gt; integers.&lt;/li&gt;
&lt;li&gt;Be aware of the quoting rules for &lt;a href=&quot;https://google.github.io/styleguide/shell.xml#Test,_%5B_and_%5B%5B&quot;&gt;pattern matches in [[&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Use &quot;$@&quot; unless you have a specific reason to use $*.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;# 'Single' quotes indicate that no substitution is desired. # &quot;Double&quot; quotes indicate that substitution is required/tolerated. # Simple examples # &quot;quote command substitutions&quot; flag=&quot;$(some_command and its args &quot;$@&quot; 'quoted separately')&quot; # &quot;quote variables&quot; echo &quot;${flag}&quot; # &quot;never quote literal integers&quot; value=32 # &quot;quote command substitutions&quot;, even when you expect integers number=&quot;$(generate_number)&quot; # &quot;prefer quoting words&quot;, not compulsory readonly USE_INTEGER='true' # &quot;quote shell meta characters&quot; echo 'Hello stranger, and well met. Earn lots of $$$' echo &quot;Process $$: Done making \$\$\$.&quot; # &quot;command options or path names&quot; # ($1 is assumed to contain a value here) grep -li Hugo /dev/null &quot;$1&quot; # Less simple examples # &quot;quote variables, unless proven false&quot;: ccs might be empty git send-email --to &quot;${reviewers}&quot; ${ccs:+&quot;--cc&quot; &quot;${ccs}&quot;} # Positional parameter precautions: $1 might be unset # Single quotes leave regex as-is. grep -cP '([Ss]pecial|\|?characters*)$' ${1:+&quot;$1&quot;} # For passing on arguments, # &quot;$@&quot; is right almost everytime, and # $* is wrong almost everytime: # # * $* and $@ will split on spaces, clobbering up arguments # that contain spaces and dropping empty strings; # * &quot;$@&quot; will retain arguments as-is, so no args # provided will result in no args being passed on; # This is in most cases what you want to use for passing # on arguments. # * &quot;$*&quot; expands to one argument, with all args joined # by (usually) spaces, # so no args provided will result in one empty string # being passed on. # (Consult 'man bash' for the nit-grits ;-) set -- 1 &quot;2 two&quot; &quot;3 three tres&quot;; echo $# ; set -- &quot;$*&quot;; echo &quot;$#, $@&quot;) set -- 1 &quot;2 two&quot; &quot;3 three tres&quot;; echo $# ; set -- &quot;$@&quot;; echo &quot;$#, $@&quot;)&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;$(command)&lt;/code&gt; instead of backticks.&lt;/p&gt;
&lt;p&gt;Nested backticks require escaping the inner ones with &lt;code&gt;\&lt;/code&gt;. The &lt;code&gt;$(command)&lt;/code&gt; format doesn't change when nested and is easier to read.&lt;/p&gt;
&lt;p&gt;Example: # This is preferred: var=&quot;$(command &quot;$(command1)&quot;)&quot; # This is not: var=&quot;`command \`command1\``&quot;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;[[ ... ]]&lt;/code&gt; is preferred over &lt;code&gt;[&lt;/code&gt;, &lt;code&gt;test&lt;/code&gt; and &lt;code&gt;/usr/bin/[&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;[[ ... ]]&lt;/code&gt; reduces errors as no pathname expansion or word splitting takes place between &lt;code&gt;[[&lt;/code&gt; and &lt;code&gt;]]&lt;/code&gt; and &lt;code&gt;[[ ... ]]&lt;/code&gt; allows for regular expression matching where &lt;code&gt;[ ... ]&lt;/code&gt; does not. # This ensures the string on the left is made up of characters in the # alnum character class followed by the string name. # Note that the RHS should not be quoted here. # For the gory details, see # E14 at https://tiswww.case.edu/php/chet/bash/FAQ if [[ &quot;filename&quot; =~ ^[[:alnum:]]+name ]]; then echo &quot;Match&quot; fi # This matches the exact pattern &quot;f*&quot; (Does not match in this case) if [[ &quot;filename&quot; == &quot;f*&quot; ]]; then echo &quot;Match&quot; fi # This gives a &quot;too many arguments&quot; error as f* is expanded to the # contents of the current directory if [ &quot;filename&quot; == f* ]; then echo &quot;Match&quot; fi&lt;/p&gt;
&lt;p&gt;Use quotes rather than filler characters where possible.&lt;/p&gt;
&lt;p&gt;Bash is smart enough to deal with an empty string in a test. So, given that the code is much easier to read, use tests for empty/non-empty strings or empty strings rather than filler characters. # Do this: if [[ &quot;${my_var}&quot; = &quot;some_string&quot; ]]; then do_something fi # -z (string length is zero) and -n (string length is not zero) are # preferred over testing for an empty string if [[ -z &quot;${my_var}&quot; ]]; then do_something fi # This is OK (ensure quotes on the empty side), but not preferred: if [[ &quot;${my_var}&quot; = &quot;&quot; ]]; then do_something fi # Not this: if [[ &quot;${my_var}X&quot; = &quot;some_stringX&quot; ]]; then do_something fi&lt;/p&gt;
&lt;p&gt;To avoid confusion about what you're testing for, explicitly use &lt;code&gt;-z&lt;/code&gt; or &lt;code&gt;-n&lt;/code&gt;. # Use this if [[ -n &quot;${my_var}&quot; ]]; then do_something fi # Instead of this as errors can occur if ${my_var} expands to a test # flag if [[ &quot;${my_var}&quot; ]]; then do_something fi&lt;/p&gt;
&lt;p&gt;Use an explicit path when doing wildcard expansion of filenames.&lt;/p&gt;
&lt;p&gt;As filenames can begin with a &lt;code&gt;-&lt;/code&gt;, it's a lot safer to expand wildcards with &lt;code&gt;./*&lt;/code&gt; instead of &lt;code&gt;*&lt;/code&gt;. # Here's the contents of the directory: # -f -r somedir somefile # This deletes almost everything in the directory by force psa@bilby$ rm -v * removed directory: `somedir' removed `somefile' # As opposed to: psa@bilby$ rm -v ./* removed `./-f' removed `./-r' rm: cannot remove `./somedir': Is a directory removed `./somefile'&lt;/p&gt;
&lt;p&gt;&lt;code&gt;eval&lt;/code&gt; should be avoided.&lt;/p&gt;
&lt;p&gt;Eval munges the input when used for assignment to variables and can set variables without making it possible to check what those variables were. # What does this set? # Did it succeed? In part or whole? eval $(set_my_variables) # What happens if one of the returned values has a space in it? variable=&quot;$(eval some_function)&quot;&lt;/p&gt;
&lt;p&gt;Use process substitution or for loops in preference to piping to while. Variables modified in a while loop do not propagate to the parent because the loop's commands run in a subshell.&lt;/p&gt;
&lt;p&gt;The implicit subshell in a pipe to while can make it difficult to track down bugs. last_line='NULL' your_command | while read line; do last_line=&quot;${line}&quot; done # This will output 'NULL' echo &quot;${last_line}&quot;&lt;/p&gt;
&lt;p&gt;Use a for loop if you are confident that the input will not contain spaces or special characters (usually, this means not user input). total=0 # Only do this if there are no spaces in return values. for value in $(command); do total+=&quot;${value}&quot; done&lt;/p&gt;
&lt;p&gt;Using process substitution allows redirecting output but puts the commands in an explicit subshell rather than the implicit subshell that bash creates for the while loop. total=0 last_file= while read count filename; do total+=&quot;${count}&quot; last_file=&quot;${filename}&quot; done &amp;lt; &amp;lt;(your_command | uniq -c) # This will output the second field of the last line of output from # the command. echo &quot;Total = ${total}&quot; echo &quot;Last one = ${last_file}&quot;&lt;/p&gt;
&lt;p&gt;Use while loops where it is not necessary to pass complex results to the parent shell - this is typically where some more complex &quot;parsing&quot; is required. Beware that simple examples are probably more easily done with a tool such as awk. This may also be useful where you specifically don't want to change the parent scope variables. # Trivial implementation of awk expression: # awk '$3 == &quot;nfs&quot; { print $2 &quot; maps to &quot; $1 }' /proc/mounts cat /proc/mounts | while read src dest type opts rest; do if [[ ${type} == &quot;nfs&quot; ]]; then echo &quot;NFS ${dest} maps to ${src}&quot; fi done&lt;/p&gt;
&lt;p&gt;Lower-case, with underscores to separate words. Separate libraries with &lt;code&gt;::&lt;/code&gt;. Parentheses are required after the function name. The keyword &lt;code&gt;function&lt;/code&gt; is optional, but must be used consistently throughout a project.&lt;/p&gt;
&lt;p&gt;If you're writing single functions, use lowercase and separate words with underscore. If you're writing a package, separate package names with &lt;code&gt;::&lt;/code&gt;. Braces must be on the same line as the function name (as with other languages at Google) and no space between the function name and the parenthesis. # Single function my_func() { ... } # Part of a package mypackage::my_func() { ... }&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;function&lt;/code&gt; keyword is extraneous when &quot;()&quot; is present after the function name, but enhances quick identification of functions.&lt;/p&gt;
&lt;p&gt;As for function names.&lt;/p&gt;
&lt;p&gt;Variables names for loops should be similarly named for any variable you're looping through. for zone in ${zones}; do something_with &quot;${zone}&quot; done&lt;/p&gt;
&lt;p&gt;All caps, separated with underscores, declared at the top of the file.&lt;/p&gt;
&lt;p&gt;Constants and anything exported to the environment should be capitalized. # Constant readonly PATH_TO_FILES='/some/path' # Both constant and environment declare -xr ORACLE_SID='PROD'&lt;/p&gt;
&lt;p&gt;Some things become constant at their first setting (for example, via getopts). Thus, it's OK to set a constant in getopts or based on a condition, but it should be made readonly immediately afterwards. Note that &lt;code&gt;declare&lt;/code&gt; doesn't operate on global variables within functions, so &lt;code&gt;readonly&lt;/code&gt; or &lt;code&gt;export&lt;/code&gt; is recommended instead.&lt;/p&gt;
&lt;p&gt;VERBOSE='false' while getopts 'v' flag; do case &quot;${flag}&quot; in v) VERBOSE='true' ;; esac done readonly VERBOSE Lowercase, with underscores to separate words if desired.&lt;/p&gt;
&lt;p&gt;This is for consistency with other code styles in Google: &lt;code&gt;maketemplate&lt;/code&gt; or &lt;code&gt;make_template&lt;/code&gt; but not &lt;code&gt;make-template&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;readonly&lt;/code&gt; or &lt;code&gt;declare -r&lt;/code&gt; to ensure they're read only.&lt;/p&gt;
&lt;p&gt;As globals are widely used in shell, it's important to catch errors when working with them. When you declare a variable that is meant to be read-only, make this explicit. zip_version=&quot;$(dpkg --status zip | grep Version: | cut -d ' ' -f 2)&quot; if [[ -z &quot;${zip_version}&quot; ]]; then error_message else readonly zip_version fi&lt;/p&gt;
&lt;p&gt;Declare function-specific variables with &lt;code&gt;local&lt;/code&gt;. Declaration and assignment should be on different lines.&lt;/p&gt;
&lt;p&gt;Ensure that local variables are only seen inside a function and its children by using &lt;code&gt;local&lt;/code&gt; when declaring them. This avoids polluting the global name space and inadvertently setting variables that may have significance outside the function.&lt;/p&gt;
&lt;p&gt;Declaration and assignment must be separate statements when the assignment value is provided by a command substitution; as the 'local' builtin does not propagate the exit code from the command substitution. my_func2() { local name=&quot;$1&quot; # Separate lines for declaration and assignment: local my_var my_var=&quot;$(my_func)&quot; || return # DO NOT do this: $? contains the exit code of 'local', not my_func local my_var=&quot;$(my_func)&quot; [[ $? -eq 0 ]] || return ... }&lt;/p&gt;
&lt;p&gt;Put all functions together in the file just below constants. Don't hide executable code between functions.&lt;/p&gt;
&lt;p&gt;If you've got functions, put them all together near the top of the file. Only includes, &lt;code&gt;set&lt;/code&gt; statements and setting constants may be done before declaring functions.&lt;/p&gt;
&lt;p&gt;Don't hide executable code between functions. Doing so makes the code difficult to follow and results in nasty surprises when debugging.&lt;/p&gt;
&lt;p&gt;A function called &lt;code&gt;main&lt;/code&gt; is required for scripts long enough to contain at least one other function.&lt;/p&gt;
&lt;p&gt;In order to easily find the start of the program, put the main program in a function called &lt;code&gt;main&lt;/code&gt; as the bottom most function. This provides consistency with the rest of the code base as well as allowing you to define more variables as &lt;code&gt;local&lt;/code&gt; (which can't be done if the main code is not a function). The last non-comment line in the file should be a call to &lt;code&gt;main&lt;/code&gt;: main &quot;$@&quot;&lt;/p&gt;
&lt;p&gt;Obviously, for short scripts where it's just a linear flow, &lt;code&gt;main&lt;/code&gt; is overkill and so is not required.&lt;/p&gt;
&lt;p&gt;Always check return values and give informative return values.&lt;/p&gt;
&lt;p&gt;For unpiped commands, use &lt;code&gt;$?&lt;/code&gt; or check directly via an &lt;code&gt;if&lt;/code&gt; statement to keep it simple.&lt;/p&gt;
&lt;p&gt;Example: if ! mv &quot;${file_list}&quot; &quot;${dest_dir}/&quot; ; then echo &quot;Unable to move ${file_list} to ${dest_dir}&quot; &amp;gt;&amp;amp;2 exit &quot;${E_BAD_MOVE}&quot; fi # Or mv &quot;${file_list}&quot; &quot;${dest_dir}/&quot; if [[ &quot;$?&quot; -ne 0 ]]; then echo &quot;Unable to move ${file_list} to ${dest_dir}&quot; &amp;gt;&amp;amp;2 exit &quot;${E_BAD_MOVE}&quot; fi&lt;/p&gt;
&lt;p&gt;Bash also has the &lt;code&gt;PIPESTATUS&lt;/code&gt; variable that allows checking of the return code from all parts of a pipe. If it's only necessary to check success or failure of the whole pipe, then the following is acceptable: tar -cf - ./* | ( cd &quot;${dir}&quot; &amp;amp;&amp;amp; tar -xf - ) if [[ &quot;${PIPESTATUS[0]}&quot; -ne 0 || &quot;${PIPESTATUS[1]}&quot; -ne 0 ]]; then echo &quot;Unable to tar files to ${dir}&quot; &amp;gt;&amp;amp;2 fi&lt;/p&gt;
&lt;p&gt;However, as &lt;code&gt;PIPESTATUS&lt;/code&gt; will be overwritten as soon as you do any other command, if you need to act differently on errors based on where it happened in the pipe, you'll need to assign &lt;code&gt;PIPESTATUS&lt;/code&gt; to another variable immediately after running the command (don't forget that &lt;code&gt;[&lt;/code&gt; is a command and will wipe out &lt;code&gt;PIPESTATUS&lt;/code&gt;). tar -cf - ./* | ( cd &quot;${DIR}&quot; &amp;amp;&amp;amp; tar -xf - ) return_codes=(${PIPESTATUS[*]}) if [[ &quot;${return_codes[0]}&quot; -ne 0 ]]; then do_something fi if [[ &quot;${return_codes[1]}&quot; -ne 0 ]]; then do_something_else fi&lt;/p&gt;
&lt;p&gt;Given the choice between invoking a shell builtin and invoking a separate process, choose the builtin.&lt;/p&gt;
&lt;p&gt;We prefer the use of builtins such as the &lt;em&gt;Parameter Expansion&lt;/em&gt; functions in &lt;code&gt;bash(1)&lt;/code&gt; as it's more robust and portable (especially when compared to things like sed).&lt;/p&gt;
&lt;p&gt;Example: # Prefer this: addition=$((${X} + ${Y})) substitution=&quot;${string/#foo/bar}&quot; # Instead of this: addition=&quot;$(expr ${X} + ${Y})&quot; substitution=&quot;$(echo &quot;${string}&quot; | sed -e 's/^foo/bar/')&quot;&lt;/p&gt;
&lt;p&gt;Use common sense and &lt;em&gt;BE CONSISTENT&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Please take a few minutes to read the Parting Words section at the bottom of the &lt;a href=&quot;https://google.github.io/styleguide/cppguide.html&quot;&gt;C++ Guide&lt;/a&gt;.&lt;/p&gt;
&lt;p align=&quot;right&quot;&gt;Revision 1.26&lt;/p&gt;
&lt;/body&gt;</description>
<pubDate>Tue, 15 May 2018 12:57:18 +0000</pubDate>
<dc:creator>gkfasdfasdf</dc:creator>
<dc:format>application/xml</dc:format>
<dc:identifier>https://google.github.io/styleguide/shell.xml</dc:identifier>
</item>
<item>
<title>Professor Frisby&amp;#039;s Mostly Adequate Guide to Functional Programming (2015)</title>
<link>https://mostly-adequate.gitbooks.io/mostly-adequate-guide/</link>
<guid isPermaLink="true" >https://mostly-adequate.gitbooks.io/mostly-adequate-guide/</guid>
<description>&lt;p&gt;&lt;a href=&quot;https://mostly-adequate.gitbooks.io/mostly-adequate-guide/SUMMARY.md&quot;&gt;&lt;img src=&quot;https://mostly-adequate.gitbooks.io/mostly-adequate-guide/images/cover.png&quot; alt=&quot;cover&quot;/&gt;&lt;/a&gt;&lt;/p&gt;&lt;h2 id=&quot;about-this-book&quot;&gt;About this book&lt;/h2&gt;
&lt;p&gt;This is a book on the functional paradigm in general. We'll use the world's most popular functional programming language: JavaScript. Some may feel this is a poor choice as it's against the grain of the current culture which, at the moment, feels predominately imperative. However, I believe it is the best way to learn FP for several reasons:&lt;/p&gt;
&lt;ul readability=&quot;-0.5&quot;&gt;&lt;li readability=&quot;2&quot;&gt;
&lt;p&gt;&lt;strong&gt;You likely use it every day at work.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This makes it possible to practice and apply your acquired knowledge each day on real world programs rather than pet projects on nights and weekends in an esoteric FP language.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;ul readability=&quot;1&quot;&gt;&lt;li readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;strong&gt;We don't have to learn everything up front to start writing programs.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In a pure functional language, you cannot log a variable or read a DOM node without using monads. Here we can cheat a little as we learn to purify our codebase. It's also easier to get started in this language since it's mixed paradigm and you can fall back on your current practices while there are gaps in your knowledge.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;ul readability=&quot;3&quot;&gt;&lt;li readability=&quot;9&quot;&gt;
&lt;p&gt;&lt;strong&gt;The language is fully capable of writing top notch functional code.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We have all the features we need to mimic a language like Scala or Haskell with the help of a tiny library or two. Object-oriented programming currently dominates the industry, but it's clearly awkward in JavaScript. It's akin to camping off of a highway or tap dancing in galoshes. We have to &lt;code&gt;bind&lt;/code&gt; all over the place lest &lt;code&gt;this&lt;/code&gt; change out from under us, we don't have classes (yet), we have various work arounds for the quirky behavior when the &lt;code&gt;new&lt;/code&gt; keyword is forgotten, private members are only available via closures. To a lot of us, FP feels more natural anyways.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;That said, typed functional languages will, without a doubt, be the best place to code in the style presented by this book. JavaScript will be our means of learning a paradigm, where you apply it is up to you. Luckily, the interfaces are mathematical and, as such, ubiquitous. You'll find yourself at home with Swiftz, Scalaz, Haskell, PureScript, and other mathematically inclined environments.&lt;/p&gt;
&lt;h2 id=&quot;read-it-online&quot;&gt;Read it Online&lt;/h2&gt;
&lt;p&gt;For a best reading experience, &lt;a href=&quot;https://mostly-adequate.gitbooks.io/mostly-adequate-guide/&quot; target=&quot;_blank&quot;&gt;read it online via Gitbook&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Quick-access side-bar&lt;/li&gt;
&lt;li&gt;In-browser exercises&lt;/li&gt;
&lt;li&gt;In-depth examples&lt;/li&gt;
&lt;/ul&gt;&lt;h2 id=&quot;download-it&quot;&gt;Download it&lt;/h2&gt;
&lt;h2 id=&quot;do-it-yourself&quot;&gt;Do it yourself&lt;/h2&gt;
&lt;pre&gt;
&lt;code&gt;git clone https://github.com/MostlyAdequate/mostly-adequate-guide.git
cd mostly-adequate-guide/
npm install
npm run setup
gitbook pdf
&lt;/code&gt;
&lt;/pre&gt;

&lt;p&gt;See &lt;a href=&quot;https://mostly-adequate.gitbooks.io/mostly-adequate-guide/SUMMARY.md&quot;&gt;SUMMARY.md&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;contributing&quot;&gt;Contributing&lt;/h3&gt;
&lt;p&gt;See &lt;a href=&quot;https://mostly-adequate.gitbooks.io/mostly-adequate-guide/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;translations&quot;&gt;Translations&lt;/h3&gt;
&lt;p&gt;See &lt;a href=&quot;https://mostly-adequate.gitbooks.io/mostly-adequate-guide/TRANSLATIONS.md&quot;&gt;TRANSLATIONS.md&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;faq&quot;&gt;FAQ&lt;/h3&gt;
&lt;p&gt;See &lt;a href=&quot;https://mostly-adequate.gitbooks.io/mostly-adequate-guide/FAQ.md&quot;&gt;FAQ.md&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Part 1&lt;/strong&gt; (chapters 1-7) is a guide to the basics. I'm updating as I find errors since this is the initial draft. Feel free to help!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Part 2&lt;/strong&gt; (chapters 8-10) will address type classes like functors and monads all the way through to traversable. I hope to squeeze in transformers and a pure application.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Part 3&lt;/strong&gt; (chapters 11+) will start to dance the fine line between practical programming and academic absurdity. We'll look at comonads, f-algebras, free monads, yoneda, and other categorical constructs.&lt;/li&gt;
&lt;/ul&gt;&lt;hr/&gt;&lt;p align=&quot;center&quot;&gt;&lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-sa/4.0/&quot; target=&quot;_blank&quot;&gt;&lt;img alt=&quot;Creative Commons License&quot; src=&quot;https://i.creativecommons.org/l/by-sa/4.0/88x31.png&quot;/&gt;&lt;/a&gt;&lt;br/&gt;This work is licensed under a &lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-sa/4.0/&quot; target=&quot;_blank&quot;&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt;
</description>
<pubDate>Tue, 15 May 2018 09:15:08 +0000</pubDate>
<dc:creator>AdrianRossouw</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://mostly-adequate.gitbooks.io/mostly-adequate-guide/</dc:identifier>
</item>
<item>
<title>Memory transfer between snails challenges view of how brain remembers</title>
<link>https://www.statnews.com/2018/05/14/memory-transfer-between-snails-challenges-standard-theory/</link>
<guid isPermaLink="true" >https://www.statnews.com/2018/05/14/memory-transfer-between-snails-challenges-standard-theory/</guid>
<description>&lt;p class=&quot;big-cap-wrap&quot;&gt;&lt;span class=&quot;big-cap&quot;&gt;L&lt;/span&gt;&lt;/p&gt;&lt;p&gt;OS ANGELES — UCLA neuroscientists reported Monday that they have transferred a memory from one animal to another via injections of RNA, a startling result that challenges the widely held view of where and how memories are stored in the brain.&lt;/p&gt;
&lt;p&gt;The finding from the lab of David Glanzman hints at the potential for new RNA-based treatments to one day restore lost memories and, if correct, could shake up the field of memory and learning.&lt;/p&gt;
&lt;p&gt;“It’s pretty shocking,” said Dr. Todd Sacktor, a neurologist and memory researcher at SUNY Downstate Medical Center in Brooklyn, N.Y. “The big picture is we’re working out the basic alphabet of how memories are stored for the first time.” He was not involved in the research, which was &lt;a href=&quot;http://www.eneuro.org/content/early/2018/05/14/ENEURO.0038-18.2018&quot; target=&quot;_blank&quot;&gt;published in eNeuro&lt;/a&gt;, the &lt;a href=&quot;http://www.eneuro.org&quot; target=&quot;_blank&quot;&gt;online journal&lt;/a&gt; of the Society for Neuroscience.&lt;/p&gt;
&lt;p class=&quot;ad-label&quot;&gt;advertisement&lt;/p&gt;

&lt;p&gt;Many scientists are expected to view the research more cautiously. The work is in snails, animals that have proven a powerful model organism for neuroscience but whose simple brains work far differently than those of humans. The experiments will need to be replicated, including in animals with more complex brains. And the results fly in the face of a massive amount of evidence supporting the deeply entrenched idea that memories are stored through changes in the strength of connections, or synapses, between neurons.&lt;/p&gt;
&lt;aside class=&quot;read-more statplus&quot;&gt;&lt;div class=&quot;read-more-thumbnail-wrapper&quot;&gt;
&lt;div class=&quot;read-more-thumbnail&quot;&gt;&lt;a href=&quot;https://www.statnews.com/stat-plus/&quot;&gt;&lt;img src=&quot;https://www.statnews.com/wp-content/themes/stat/images/icons/stat-plus-white.svg&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;/aside&gt;&lt;p&gt;“If he’s right, this would be absolutely earth-shattering,” said Tomás Ryan, an assistant professor at Trinity College Dublin, whose lab hunts for engrams, or the physical traces of memory. “But I don’t think it’s right.”&lt;/p&gt;
&lt;p&gt;Glanzman knows his unceremonial demotion of the synapse is not going to go over well in the field. “I expect a lot of astonishment and skepticism,” he said. “I don’t expect people are going to have a parade for me at the next Society for Neuroscience meeting.”&lt;/p&gt;
&lt;p&gt;Even his own colleagues were dubious. “It took me a long time to convince the people in my lab to do the experiment,” he said. “They thought it was nuts.”&lt;/p&gt;
&lt;p&gt;Glanzman’s experiments — funded by the National Institutes of Health and the National Science Foundation — involved giving mild electrical shocks to the marine snail Aplysia californica. Shocked snails learn to withdraw their delicate siphons and gills for nearly a minute as a defense when they subsequently receive a weak touch; snails that have not been shocked withdraw only briefly.&lt;/p&gt;
&lt;p&gt;The researchers extracted RNA from the nervous systems of snails that had been shocked and injected the material into unshocked snails. RNA’s primary role is to serve as a messenger inside cells, carrying protein-making instructions from its cousin DNA. But when this RNA was injected, these naive snails withdrew their siphons for extended periods of time after a soft touch. Control snails that received injections of RNA from snails that had not received shocks did not withdraw their siphons for as long.&lt;/p&gt;
&lt;p&gt;“It’s as if we transferred a memory,” Glanzman said.&lt;/p&gt;
&lt;p&gt;Glanzman’s group went further, showing that Aplysia sensory neurons in Petri dishes were more excitable, as they tend to be after being shocked, if they were exposed to RNA from shocked snails. Exposure to RNA from snails that had never been shocked did not cause the cells to become more excitable.&lt;/p&gt;
&lt;div class=&quot;quote&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;quote-inner&quot; readability=&quot;8&quot;&gt;
&lt;p class=&quot;quote-left&quot;&gt;“I expect a lot of astonishment and skepticism. I don’t expect people are going to have a parade for me at the next Society for Neuroscience meeting.”&lt;/p&gt;
&lt;div class=&quot;quote-right&quot;&gt;
&lt;p class=&quot;name&quot;&gt;David Glanzman, UCLA&lt;/p&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;The results, said Glanzman, suggest that memories may be stored within the nucleus of neurons, where RNA is synthesized and can act on DNA to turn genes on and off. He said he thought memory storage involved these epigenetic changes — changes in the activity of genes and not in the DNA sequences that make up those genes — that are mediated by RNA.&lt;/p&gt;
&lt;p&gt;This view challenges the widely held notion that memories are stored by enhancing synaptic connections between neurons. Rather, Glanzman sees synaptic changes that occur during memory formation as flowing from the information that the RNA is carrying.&lt;/p&gt;
&lt;p&gt;“This idea is radical and definitely challenges the field,” said Li-Huei Tsai, a neuroscientist who directs the Picower Institute for Learning and Memory at the Massachusetts Institute of Technology. Tsai, who recently co-authored a &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4874022/&quot; target=&quot;_blank&quot;&gt;major review on memory formation&lt;/a&gt;, called Glanzman’s study “impressive and interesting” and said a number of studies support the notion that epigenetic mechanisms play some role in memory formation, which is likely a complex and multifaceted process. But she said she strongly disagreed with Glanzman’s notion that synaptic connections do not play a key role in memory storage.&lt;/p&gt;
&lt;p&gt;Trinity College’s Ryan, like Glanzman, stands with a minority of neuroscientists — some call them rebels — who &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/28548457&quot; target=&quot;_blank&quot;&gt;question the idea&lt;/a&gt; that memory is stored through synaptic strength. In 2015, Ryan was lead author of a Science paper with MIT Nobelist Susumu Tonegawa that showed &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/26023136&quot; target=&quot;_blank&quot;&gt;memories could be retrieved&lt;/a&gt; even after synapse strengthening was blocked. Ryan said he is pursuing the idea that memories are stored through ensembles of neurons bound together by new synaptic connections, not by strengthening of existing connections.&lt;/p&gt;
&lt;p&gt;Ryan knows Glanzman and trusts his work. He said he believes the data in the new paper. But he doesn’t think the behavior of the snails, or the cells, proves that RNA is transferring memories. He said he doesn’t understand how RNA, which works on a time scale of minutes to hours, could be causing memory recall that is almost instantaneous, or how RNA could connect numerous parts of the brain, like the auditory and visual systems, that are involved in more complex memories.&lt;/p&gt;

&lt;p&gt;But Glanzman said he is convinced RNA is playing a role that eclipses the synapse. In 2014, his lab showed that &lt;a href=&quot;https://elifesciences.org/articles/03896&quot; target=&quot;_blank&quot;&gt;memories of shocks that had been lost&lt;/a&gt; in snails due to a series of experimental procedures could be recovered — but the synapse patterns that were lost with the memory reformed in random ways when the memories were recovered, suggesting memories were not stored there. Glanzman’s lab and others have also shown that long-term memory formation can be blocked by preventing epigenetic changes, even when synapse formation or strengthening is not altered.&lt;/p&gt;
&lt;p&gt;“Synapses can come and go, but the memory can still be there,” he said, saying he sees synapses as merely the “reflection of knowledge held in the nucleus.”&lt;/p&gt;
&lt;p&gt;Glanzman has studied memory for more than three decades. He did postdoctoral work with none other than Eric Kandel — the neuroscientist who shared the 2000 Nobel prize for research on Aplysia, probing the role of the synapse in memory — and he said he has spent most of his career believing that synaptic change was the key to memory storage.&lt;/p&gt;
&lt;p&gt;But he said a series of findings from other labs and his own in recent years have led him to start questioning the synaptic dogma. He calls himself “a recovering synaptologist.”&lt;/p&gt;
&lt;p&gt;The skepticism over Glanzman’s research may be in part because the work harkens back to an unnerving episode in science involving an unconventional psychologist, James V. McConnell, who spent years at the University of Michigan attempting to prove that something outside the brain — a factor he called “memory RNA” — could transfer memories. In the ’50s and ’60s, McConnell trained flatworms and then fed the bodies of trained worms to untrained worms. The untrained worms then appeared to exhibit the behavior of the trained worms they’d cannibalized, suggesting that memories were somehow transferred. He also showed that trained worms that were beheaded could remember their training after they grew new heads.&lt;/p&gt;
&lt;p&gt;Though the work was replicated by some other labs, McConnell’s work was largely ridiculed and is often described as a cautionary tale because so much time and money was spent by other labs trying, often unsuccessfully, to replicate the work. (McConnell died in 1990, five years after he’d been a target of the Unabomber Theodore Kaczynski.)&lt;/p&gt;
&lt;aside class=&quot;read-more standard&quot;&gt;
&lt;/aside&gt;&lt;p&gt;Recently, developmental biologist Michael Levin at Tufts has &lt;a href=&quot;https://www.theverge.com/2015/3/18/8225321/memory-research-flatworm-cannibalism-james-mcconnell-michael-levin&quot; target=&quot;_blank&quot;&gt;replicated McConnell’s experiments&lt;/a&gt; on headless worms under more controlled settings and thinks McConnell may have indeed been correct.&lt;/p&gt;
&lt;p&gt;Glanzman said one of McConnell’s students, Al Jacobson, demonstrated the transfer of memories between flatworms via RNA injections, coincidentally while an assistant professor at UCLA. The work was &lt;a href=&quot;http://www.nature.com/articles/209599a0&quot; target=&quot;_blank&quot;&gt;published in Nature in 1966&lt;/a&gt; but Jacobsen never received tenure, perhaps because of doubts about his findings. The experiment was, however, &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC224185/&quot; target=&quot;_blank&quot;&gt;replicated in rats&lt;/a&gt; shortly afterward.&lt;/p&gt;
&lt;p&gt;Glanzman learned about McConnell’s work — and his satirical journal “Worm Runner’s Digest” — while he was a psychology undergraduate at Indiana University but never took the results seriously. Now, while he’s still not convinced McConnell was exactly right about being able to transfer memories, he does think both McConnell and Jacobson were onto something.&lt;/p&gt;
&lt;p&gt;Working in the memory field can be tough for those who challenge the status quo. SUNY’s Sacktor, for example, has &lt;a href=&quot;https://www.statnews.com/2016/06/23/memory-research-neuroscience/&quot;&gt;spent more than 25 years&lt;/a&gt; — despite the skepticism, rejection, and outright derision of fellow scientists — chasing down a single molecule, PKMzeta, that he believes is critical to the formation of long-term memories and may be connected to the RNA mechanisms that Glanzman has uncovered.&lt;/p&gt;
&lt;p&gt;The stakes in the field are high because memory is so key to our sense of self and many scientists feel understanding the workings of memory is something that should have been figured out by now. “It’s the last of the great 20th-century questions in biology,” Sacktor said. “Some aspect has made it difficult for neuroscientists to figure out.”&lt;/p&gt;
&lt;p&gt;The difficulty may be due in part to the overwhelming focus on synaptic strength. Some 12,000 papers have been published on synaptic strength without providing a good explanation for how memories are stored, Ryan noted, adding that he applauds Glanzman for opening up a new path, radical as it is, to explore.&lt;/p&gt;
&lt;p&gt;“The reality is we know so little about memory,” Ryan said. “I’m excited about any new vistas and avenues.”&lt;/p&gt;

&lt;div class=&quot;content-meta&quot;&gt;

&lt;div class=&quot;author-info&quot; readability=&quot;3&quot;&gt;
&lt;div class=&quot;author-avatar&quot;&gt;&lt;img src=&quot;https://www.statnews.com/wp-content/uploads/2018/01/Circular_Usha.png&quot; srcset=&quot;https://www.statnews.com/wp-content/uploads/2018/01/Circular_Usha.png 160w, https://www.statnews.com/wp-content/uploads/2018/01/Circular_Usha-80x80.png 80w&quot; width=&quot;80&quot; height=&quot;80&quot; class=&quot;avatar&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;author-data&quot; readability=&quot;3.5&quot;&gt;

&lt;p&gt;Contributing Writer&lt;/p&gt;
&lt;p&gt;Usha is a Contributing Writer for STAT.&lt;/p&gt;


&lt;/div&gt;
&lt;/div&gt;


&lt;/div&gt;
</description>
<pubDate>Tue, 15 May 2018 01:56:30 +0000</pubDate>
<dc:creator>laurex</dc:creator>
<og:type>article</og:type>
<og:title>Memory transfer between snails challenges view of how brain remembers</og:title>
<og:description>Many scientists greeted the finding with skepticism, because the work needs to replicated, and the results fly in the face of a massive amount of evidence supporting the deeply entrenched idea that memories are stored through changes in the strength of connections, or synapses, between neurons.</og:description>
<og:url>https://www.statnews.com/2018/05/14/memory-transfer-between-snails-challenges-standard-theory/</og:url>
<og:image>https://www.statnews.com/wp-content/uploads/2018/05/Snail-memory-transfer-1024x576.jpg</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.statnews.com/2018/05/14/memory-transfer-between-snails-challenges-standard-theory/</dc:identifier>
</item>
<item>
<title>I Don’t Know How to Waste Time on the Internet Anymore</title>
<link>http://nymag.com/selectall/2018/05/i-dont-know-how-to-waste-time-on-the-internet-anymore.html</link>
<guid isPermaLink="true" >http://nymag.com/selectall/2018/05/i-dont-know-how-to-waste-time-on-the-internet-anymore.html</guid>
<description>&lt;p class=&quot;clay-paragraph&quot; data-editable=&quot;text&quot; data-uri=&quot;nymag.com/selectall/_components/clay-paragraph/instances/cjgsdoma0002sory69l0p255u@published&quot; data-word-count=&quot;84&quot;&gt;The other day, I found myself looking at a blinking cursor in a blank address bar in a new tab of my web browser. I was bored. I didn’t really feel like doing work, but I felt some distant compulsion to sit at my computer in a kind of work-simulacrum, so that at least at the end of the day I would feel gross and tired in the manner of someone who &lt;em&gt;had&lt;/em&gt; worked. What I really wanted to do was waste some time.&lt;/p&gt;
&lt;p class=&quot;clay-paragraph&quot; data-editable=&quot;text&quot; data-uri=&quot;nymag.com/selectall/_components/clay-paragraph/instances/cjgsdpflu000n3b5zxrnujinr@published&quot; data-word-count=&quot;66&quot;&gt;But … I didn’t know &lt;em&gt;how&lt;/em&gt;. I did not know what to type into the address bar of my browser. I stared at the cursor. Eventually, I typed “nytimes.com” and hit enter. Like a freaking dad. The entire world of the internet, one that used to boast so many ways to waste time, and here I was, &lt;em&gt;reading the news&lt;/em&gt;. It was even worse than working.&lt;/p&gt;

&lt;p class=&quot;clay-paragraph&quot; data-editable=&quot;text&quot; data-uri=&quot;nymag.com/selectall/_components/clay-paragraph/instances/cjgsdpfnm000s3b5zzrmlmf28@published&quot; data-word-count=&quot;161&quot;&gt;In high school, I took a computer class. I have no idea what I was supposed to be learning. Instead I browsed Fark (user-submitted links from around the web, sort of a proto-Reddit) and eBaum’s World (a mix of early memes, stolen content, and ads for hard-core porn), and printed guitar tabs that would turn out to be wildly incorrect. In college, I hung out on forums like Something Awful, a gigantic repository of jokes (some good), advice (mostly bad), and aimless chatter among thousands of also bored teens, experimenting and working within the staccato confines of the Bulletin Board System. There were writers, too; I read Seanbaby and Old Man Murray and other anarchic internet writers, posting irregularly and with zero professionalism on garish websites. Red text on black backgrounds, broken navigations. I wrote a LiveJournal, badly, and read the LiveJournals of my friends and friends of friends. Everyone said too much and said it poorly. It was incredibly entertaining.&lt;/p&gt;



&lt;p class=&quot;clay-paragraph&quot; data-editable=&quot;text&quot; data-uri=&quot;nymag.com/selectall/_components/clay-paragraph/instances/cjgsdpfn8000p3b5z34whtqhu@published&quot; data-word-count=&quot;100&quot;&gt;Facebook came in my first year of college. Just as eBaum’s World’s videos gave me a welcome excuse to ignore my computer class, albums of strangely similar photos taken on digicams in dimly lit house parties became my preferred time waster. &lt;em&gt;There’s Steve, from high school, in a spectacularly unflattering shot in someone’s dirty living room in a college town in Virginia, lit by a nuclear flash from someone’s Nikon Coolpix. Sick. Hey, what happened to that girl Steve dated? (She’s also in someone’s dirty living room, her eyes neon red, drinking right out of an $8 bottle of wine.)&lt;/em&gt;&lt;/p&gt;
&lt;p class=&quot;clay-paragraph&quot; data-editable=&quot;text&quot; data-uri=&quot;nymag.com/selectall/_components/clay-paragraph/instances/cjgsdpfn6000o3b5zh1vdlwoo@published&quot; data-word-count=&quot;119&quot;&gt;This world — of blogs and forums and weird personal sites and early, college-era Facebook — was made for dicking around. After college, when I had a real job, with health insurance and a Keurig machine, I would read blogs, funny people talking about nothing in particular with no goal besides being entertaining for a three- to eight-minute block. These were evolutions of the Seanbaby type of writers. Their websites were comparatively elegant, set up for ease of reading. Gawker, Videogum, the Awl, the A.V. Club, Wonkette, various blogs even less commercial than those. There was one that just made fun of &lt;em&gt;Saved by the Bell&lt;/em&gt; episodes. I never even watched &lt;em&gt;Saved by the Bell&lt;/em&gt;, but I loved that one.&lt;/p&gt;

&lt;p class=&quot;clay-paragraph&quot; data-editable=&quot;text&quot; data-uri=&quot;nymag.com/selectall/_components/clay-paragraph/instances/cjgsdpfnr000t3b5z5iyins5x@published&quot; data-word-count=&quot;78&quot;&gt;I started a Twitter account, and fell into a world of good, dumb, weird jokes, links to new sites and interesting ideas. It was such an excellent place to waste time that I almost didn’t notice that the blogs and link-sharing sites I’d once spent hours on had become less and less viable. Where once we’d had a rich ecosystem of extremely stupid and funny sites on which we might procrastinate, we now had only Twitter and Facebook.&lt;/p&gt;
&lt;p class=&quot;clay-paragraph&quot; data-editable=&quot;text&quot; data-uri=&quot;nymag.com/selectall/_components/clay-paragraph/instances/cjgsdpfnc000q3b5zswdozpbg@published&quot; data-word-count=&quot;66&quot;&gt;And then, one day, I think in 2013, Twitter and Facebook were not really very fun anymore. And worse, the fun things they had supplanted were never coming back. Forums were depopulated; blogs were shut down. Twitter, one agent of their death, became completely worthless: a water-drop-torture feed of performative outrage, self-promotion, and discussion of Twitter itself. Facebook had become, well … you’ve been on Facebook.&lt;/p&gt;
&lt;p class=&quot;clay-paragraph&quot; data-editable=&quot;text&quot; data-uri=&quot;nymag.com/selectall/_components/clay-paragraph/instances/cjgsdpfnf000r3b5zk55gpq5w@published&quot; data-word-count=&quot;87&quot;&gt;In the decade since I took that computer class, the web browser has taken over the entire computing experience. There is nothing to “learn” about computers, really, except how to use a browser; everything you might want to do is done from that stupid empty address bar. Today, through that web browser, there are movies and TV shows and every song ever recorded; it’s where I do my writing and chatting and messaging; it’s where my notes and calendars and social networks live. It’s everything except fun.&lt;/p&gt;
&lt;p class=&quot;clay-paragraph&quot; data-editable=&quot;text&quot; data-uri=&quot;nymag.com/selectall/_components/clay-paragraph/instances/cjgsdpfo9000u3b5zvkvv8cxf@published&quot; data-word-count=&quot;116&quot;&gt;There is an argument that this my fault. I followed the wrong people; I am too nostalgic about bad blogs; I am in my 30s and what I used to think was fun time-killing is now deadly. But I don’t think so. What happened is that the internet stopped being something you went to in order to separate from the real world — from your job and your work and your obligations and responsibilities. It’s not the place you seek to waste time, but the place you go to so that you’ll someday have time to waste. The internet is a utility world for me now. It is efficient and all-encompassing. It is not very much fun.&lt;/p&gt;
</description>
<pubDate>Mon, 14 May 2018 18:32:29 +0000</pubDate>
<dc:creator>minimaxir</dc:creator>
<og:title>I Don’t Know How to Waste Time on the Internet Anymore</og:title>
<og:url>http://nymag.com/selectall/2018/05/i-dont-know-how-to-waste-time-on-the-internet-anymore.html</og:url>
<og:description>The other day, I found myself looking at a blank address bar in a new tab of my web browser. I wanted to waste time, but … I didn’t know how.</og:description>
<og:image>https://pixel.nymag.com/imgs/daily/selectall/2018/05/07/07-search.w1200.h630.gif</og:image>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://nymag.com/selectall/2018/05/i-dont-know-how-to-waste-time-on-the-internet-anymore.html</dc:identifier>
</item>
<item>
<title>Launch HN: Necto (YC W18) – ISP Starter Kit</title>
<link>https://news.ycombinator.com/item?id=17067144</link>
<guid isPermaLink="true" >https://news.ycombinator.com/item?id=17067144</guid>
<description>&lt;td colspan=&quot;2&quot;/&gt;&lt;td readability=&quot;44.178781317662&quot;&gt;Hey, we're Ben and Adam, the founders of Necto (&lt;a href=&quot;https://nectolab.io&quot; rel=&quot;nofollow&quot;&gt;https://nectolab.io&lt;/a&gt;). We're enabling local entrepreneurs to start their own Internet Service Providers by providing network engineering, monitoring, and business support as a service. We've seen huge improvements in last mile distribution technology in the last few years (cost, reliability, thoroughput, ease of deployment), but it hasn't translated into an explosion of ISP operators. We want to change that by allowing non-network-engineers to deploy their own networks and compete with the incumbents. Necto handles the networking setup, deals with the backbone providers, helps with distribution planning, and provides ongoing monitoring and support. The operators pick the markets, set the prices, and provide a great overall experience to their customers.
&lt;p&gt;We started our own ISP here in the underserved San Francisco markets of Bayview and Portola, with more neighborhoods to come. If you live in SF, we'd love to be your ISP (&lt;a href=&quot;https://joinnecto.com&quot; rel=&quot;nofollow&quot;&gt;https://joinnecto.com&lt;/a&gt;). If you're interested in starting an ISP, we're looking for an initial batch of 5 operators. You can learn more about that here: &lt;a href=&quot;https://nectolab.io&quot; rel=&quot;nofollow&quot;&gt;https://nectolab.io&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;Our product is a combination of a few important requirements for running an ISP effectively: a centralized Network Operations Center (NOC), a Operational Support System (OSS) to manage the subscribers and get visibility into issues, and an Operator's Handbook that covers the how-to's of running an ISP (both technically and our advice on the business side). Our NOC will handle things like BGP, routing, reachability, hardware issues, upstream connectivity, and distribution provisioning. The OSS supports managing subscribers, diagnosing common issues, and performing installations. Our handbook provides a list Standard Operating Procedures for day-to-day management of the ISP and, in combination with our community of ISP operators, strategies on how to effectively launch and grow an ISP.&lt;/p&gt;
&lt;p&gt;We charge an initial setup fee and an ongoing percentage of revenue. The initial setup fee covers us designing your initial network, sourcing your backbone connection, and the cost of the core routing stack. The ongoing percentage of revenue aligns our incentives with our operators and covers monitoring, the NOC, and ongoing enhancements for the software and community. The exact numbers depend on the scale of the network the operators are building.&lt;/p&gt;
&lt;p&gt;We're staunch supporters of Net Neutrality and increasing broadband penetration without sacrificing privacy. We don't sell personal information or throttle traffic (and our operators won't either). We believe that the future is in highly localized ISPs competing on service quality. We're excited to tackle this problem because we've had to deal with poor internet service before, and we now know that you can make a great business out of providing better quality access. Our backgrounds are in enterprise automation technology and the home services industry (air conditioning, plumbing, electric). We're happy to answer as many questions about any of this as we can! If you're at all considering starting an ISP in your neighborhood after reading this, let us know at nectolab.io and include your HN username!&lt;/p&gt;
&lt;p&gt;Thanks, Ben &amp;amp; Adam&lt;/p&gt;
&lt;/td&gt;
</description>
<pubDate>Mon, 14 May 2018 17:00:00 +0000</pubDate>
<dc:creator>montasaurus</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://news.ycombinator.com/item?id=17067144</dc:identifier>
</item>
<item>
<title>Prefrontal cortex as a meta-reinforcement learning system</title>
<link>https://deepmind.com/blog/prefrontal-cortex-meta-reinforcement-learning-system/</link>
<guid isPermaLink="true" >https://deepmind.com/blog/prefrontal-cortex-meta-reinforcement-learning-system/</guid>
<description>&lt;p&gt;In fact, we found that the meta-RL agent could learn to quickly adapt in a wide domain of tasks with different rules and structures. And because the network learned how to adapt to a variety of  tasks, it also learned general principles about how to learn efficiently.&lt;/p&gt;
&lt;p&gt;Importantly, we saw that the majority of learning took place in the recurrent network, which supports our proposal that dopamine plays a more integral role in the meta-learning process than previously thought. Dopamine is traditionally understood to strengthen synaptic links in the prefrontal system, reinforcing particular behaviours. In AI, this means the dopamine-like reward signal adjusts the artificial synaptic weights in a neural network as it learns the right way to solve a task. However, in our experiments the weights of the neural network were frozen, meaning they couldn’t be adjusted during the learning process, yet, the meta-RL agent was still able to solve and adapt to new tasks. This shows us that dopamine-like reward isn't only used to adjust weights, but it also conveys and encodes important information about abstract task and rule structure, allowing faster adaptation to new tasks.&lt;/p&gt;
&lt;p&gt;Neuroscientists have long observed similar patterns of neural activations in the prefrontal cortex, which is quick to adapt and flexible, but have struggled to find an adequate explanation for why that’s the case. The idea that the prefrontal cortex isn’t relying on slow synaptic weight changes to learn rule structures, but is using abstract model-based information directly encoded in dopamine, offers a more satisfactory reason for its versatility.&lt;/p&gt;
&lt;p&gt;In demonstrating that the key ingredients thought to give rise to meta-reinforcement learning in AI also exist in the brain, we’ve posed a theory that not only fits with what is known about both dopamine and prefrontal cortex but that also explains a range of mysterious findings from neuroscience and psychology. In particular, the theory sheds new light on how structured, model-based learning emerges in the brain, why dopamine itself contains model-based information, and how neurons in the prefrontal cortex become tuned to learning-related signals. Leveraging insights from AI which can be applied to explain findings in neuroscience and psychology highlights the value each field can offer the other. Going forward, we anticipate that much benefit can be gained in the reverse direction, by taking guidance from specific organisation of brain circuits in designing new models for learning in reinforcement learning agents.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;This work was completed by Jane X. Wang, Zeb Kurth-Nelson, Dharshan Kumaran, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, Demis Hassabis and Matthew Botvinick.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 14 May 2018 16:49:16 +0000</pubDate>
<dc:creator>godelmachine</dc:creator>
<og:title>Prefrontal cortex as a meta-reinforcement learning system | DeepMind</og:title>
<og:url>https://deepmind.com/blog/prefrontal-cortex-meta-reinforcement-learning-system/</og:url>
<og:description>In our new paper in Nature Neuroscience, we use the meta-reinforcement learning framework developed in AI research to investigate the role of dopamine in the brain in helping us to learn. We propose that dopamine’s role goes beyond just using reward to learn the value of past actions and that it plays an integral role, specifically within the prefrontal cortex area, in allowing us to learn efficiently, rapidly and flexibly on new tasks.</og:description>
<og:image>https://storage.googleapis.com/deepmind-live-cms/images/Blog-NN-Thumb-Img-180509-r01.width-600.png</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://deepmind.com/blog/prefrontal-cortex-meta-reinforcement-learning-system/</dc:identifier>
</item>
<item>
<title>John Carmack: My Steve Jobs Stories</title>
<link>https://m.facebook.com/permalink.php?story_fbid=2146412825593223&amp;id=100006735798590</link>
<guid isPermaLink="true" >https://m.facebook.com/permalink.php?story_fbid=2146412825593223&amp;id=100006735798590</guid>
<description>&lt;p&gt;Steve Jobs&lt;/p&gt;
&lt;p&gt;My wife once asked me “Why do you drop what you are doing when Steve Jobs asks you to do something? You don’t do that for anyone else.”&lt;/p&gt;
&lt;p&gt;It is worth thinking about.&lt;/p&gt;
&lt;p&gt;As a teenage Apple computer fan, Jobs and Wozniak were revered figures for me, and wanting an Apple 2 was a defining characteristic of several years of my childhood. Later on, seeing NeXT at a computer show just as I was selling my first commercial software felt like a vision into the future. (But $10k+, yikes!)&lt;/p&gt;
&lt;p&gt;As Id Software grew successful through Commander Keen and Wolfenstein 3D, the first major personal purchase I made wasn’t a car, but rather a NeXT computer. It turned out to be genuinely valuable for our software development, and we moved the entire company onto NeXT hardware.&lt;/p&gt;
&lt;p&gt;We loved our NeXTs, and we wanted to launch Doom with an explicit “Developed on NeXT computers” logo during the startup process, but when we asked, the request was denied.&lt;/p&gt;
&lt;p&gt;Some time after launch, when Doom had begun to make its cultural mark, we heard that Steve had changed his mind and would be happy to have NeXT branding on it, but that ship had sailed. I did think it was cool to trade a few emails with Steve Jobs.&lt;/p&gt;
&lt;p&gt;Several things over the years made me conclude that, at his core, Steve didn’t think very highly of games, and always wished they weren’t as important to his platforms as they turned out to be. I never took it personally.&lt;/p&gt;
&lt;p&gt;When NeXT managed to sort of reverse-acquire Apple and Steve was back in charge, I was excited by the possibilities of a resurgent Apple with the virtues of NeXT in a mainstream platform.&lt;/p&gt;
&lt;p&gt;I was brought in to talk about the needs of games in general, but I made it my mission to get Apple to adopt OpenGL as their 3D graphics API. I had a lot of arguments with Steve.&lt;/p&gt;
&lt;p&gt;Part of his method, at least with me, was to deride contemporary options and dare me to tell him differently. They might be pragmatic, but couldn’t actually be good. “I have Pixar. We will make something [an API] that is actually good.”&lt;/p&gt;
&lt;p&gt;It was often frustrating, because he could talk, with complete confidence, about things he was just plain wrong about, like the price of memory for video cards and the amount of system bandwidth exploitable by the AltiVec extensions.&lt;/p&gt;
&lt;p&gt;But when I knew what I was talking about, I would stand my ground against anyone.&lt;/p&gt;
&lt;p&gt;When Steve did make up his mind, he was decisive about it. Dictates were made, companies were acquired, keynotes were scheduled, and the reality distortion field kicked in, making everything else that was previously considered into obviously terrible ideas.&lt;/p&gt;
&lt;p&gt;I consider this one of the biggest indirect impacts on the industry that I have had. OpenGL never seriously threatened D3D on PC, but it was critical at Apple, and that meant that it remained enough of a going concern to be the clear choice when mobile devices started getting GPUs. While long in the tooth now, it was so much better than what we would have gotten if half a dozen SoC vendors rolled their own API back at the dawn of the mobile age.&lt;/p&gt;
&lt;p&gt;I wound up doing several keynotes with Steve, and it was always a crazy fire drill with not enough time to do things right, and generally requiring heroic effort from many people to make it happen at all. I tend to think this was also a calculated part of his method.&lt;/p&gt;
&lt;p&gt;My first impression of “Keynote Steve” was him berating the poor stage hands over “This Home Depot shit” that was rolling out the display stand with the new Mac, very much not to his satisfaction. His complaints had a valid point, and he improved the quality of the presentation by caring about details, but I wouldn’t have wanted to work for him in that capacity.&lt;/p&gt;
&lt;p&gt;One time, my wife, then fiancée, and I were meeting with Steve at Apple, and he wanted me to do a keynote that happened to be scheduled on the same day as our wedding. With a big smile and full of charm, he suggested that we postpone it. We declined, but he kept pressing. Eventually my wife countered with a suggestion that if he really wanted “her” John so much, he should loan John Lassiter to her media company for a day of consulting. Steve went from full charm to ice cold really damn quick. I didn’t do that keynote.&lt;/p&gt;
&lt;p&gt;When I was preparing an early technology demo of Doom 3 for a keynote in Japan, I was having a hard time dealing with some of the managers involved that were insisting that I change the demo because “Steve doesn’t like blood.” I knew that Doom 3 wasn’t to his taste, but that wasn’t the point of doing the demo.&lt;/p&gt;
&lt;p&gt;I brought it to Steve, with all the relevant people on the thread. He replied to everyone with:&lt;/p&gt;
&lt;p&gt;“I trust you John, do whatever you think is great.”&lt;/p&gt;
&lt;p&gt;That goes a long way, and nobody said a thing after that.&lt;/p&gt;
&lt;p&gt;When my wife and I later started building games for feature phones (DoomRPG! Orcs&amp;amp;Elves!), I advocated repeatedly to Steve that an Apple phone could be really great. Every time there was a rumor that Apple might be working on a phone, I would refine the pitch to him. Once he called me at home on a Sunday (How did he even get my number?) to ask a question, and I enthused at length about the possibilities.&lt;/p&gt;
&lt;p&gt;I never got brought into the fold, but I was excited when the iPhone actually did see the light of day. A giant (for the time) true color display with a GPU! We could do some amazing things with this!&lt;/p&gt;
&lt;p&gt;Steve first talked about application development for iPhone at the same keynote I was demonstrating the new ID Tech 5 rendering engine on Mac, so I was in the front row. When he started going on about “Web Apps”, I was (reasonably quietly) going “Booo!!!”.&lt;/p&gt;
&lt;p&gt;After the public cleared out and the rest of us were gathered in front of the stage, I started urgently going on about how web apps are terrible, and wouldn’t show the true potential of the device. We could do so much more with real native access!&lt;/p&gt;
&lt;p&gt;Steve responded with a line he had used before: “Bad apps could bring down cell phone towers.” I hated that line. He could have just said “We aren’t ready”, and that would have been fine.&lt;/p&gt;
&lt;p&gt;I was making some guesses, but I argued that the iPhone hardware and OS provided sufficient protection for native apps. I pointed at a nearby engineer and said “Don’t you have an MMU and process isolation on the iPhone now?” He had a wide eyed look of don’t-bring-me-into-this, but I eventually got a “yes” out of him.&lt;/p&gt;
&lt;p&gt;I said that OS-X was surely being used for things that were more security critical than a phone, and if Apple couldn’t provide enough security there, they had bigger problems. He came back with a snide “You’re a smart guy John, why don’t you write a new OS?” At the time, my thought was, “Fuck you, Steve.”.&lt;/p&gt;
&lt;p&gt;People were backing away from us. If Steve was mad, Apple employees didn’t want him to associate the sight of them with the experience. Afterwards, one of the execs assured me that “Steve appreciates vigorous conversation”.&lt;/p&gt;
&lt;p&gt;Still deeply disappointed about it, I made some comments that got picked up by the press. Steve didn’t appreciate that.&lt;/p&gt;
&lt;p&gt;The Steve Jobs “hero / shithead” rollercoaster was real, and after riding high for a long time, I was now on the down side. Someone told me that Steve explicitly instructed them to not give me access to the early iPhone SDK when it finally was ready.&lt;/p&gt;
&lt;p&gt;I wound up writing several successful iPhone apps on the side (all of which are now gone due to dropping 32 bit support, which saddens me), and I had many strong allies inside Apple, but I was on the outs with Steve.&lt;/p&gt;
&lt;p&gt;The last iOS product I worked on was Rage for iOS, which I thought set a new bar for visual richness on mobile, and also supported some brand new features like TV out. I heard that it was well received inside Apple.&lt;/p&gt;
&lt;p&gt;I was debriefing the team after the launch when I got a call. I was busy, so I declined it. A few minutes later someone came in and said that Steve was going to call me. Oops.&lt;/p&gt;
&lt;p&gt;Everyone had a chuckle about me “hanging up on Steve Jobs”, but that turned out to be my last interaction with him.&lt;/p&gt;
&lt;p&gt;As the public story of his failing health progressed, I started several emails to try to say something meaningful and positive to part on, but I never got through them, and I regret it.&lt;/p&gt;
&lt;p&gt;I corroborate many of the negative character traits that he was infamous for, but elements of the path that led to where I am today were contingent on the dents he left in the universe.&lt;/p&gt;
&lt;p&gt;I showed up for him.&lt;/p&gt;
</description>
<pubDate>Mon, 14 May 2018 16:31:10 +0000</pubDate>
<dc:creator>AJRF</dc:creator>
<og:title>John Carmack</og:title>
<og:description>Steve Jobs My wife once asked me “Why do you drop what you are doing when Steve Jobs asks you to do something? You don’t do that for anyone else.” It is worth thinking about. As a teenage Apple...</og:description>
<og:image>https://scontent.fphx1-2.fna.fbcdn.net/v/t1.0-1/cp0/e15/q65/p200x200/10525819_1515727755328403_3791257847922948008_n.jpg?_nc_cat=0&amp;oh=dc82d206cde6af8719c14561e335f586&amp;oe=5B902B9F</og:image>
<og:url>https://www.facebook.com/story.php?story_fbid=2146412825593223&amp;id=100006735798590</og:url>
<dc:format>text/html</dc:format>
<dc:identifier>https://m.facebook.com/permalink.php?story_fbid=2146412825593223&amp;id=100006735798590</dc:identifier>
</item>
<item>
<title>Employers are monitoring computers, toilet breaks, even emotions</title>
<link>https://www.theguardian.com/world/2018/may/14/is-your-boss-secretly-or-not-so-secretly-watching-you</link>
<guid isPermaLink="true" >https://www.theguardian.com/world/2018/may/14/is-your-boss-secretly-or-not-so-secretly-watching-you</guid>
<description>&lt;p&gt;&lt;span class=&quot;drop-cap&quot;&gt;&lt;span class=&quot;drop-cap__inner&quot;&gt;L&lt;/span&gt;&lt;/span&gt;ast year an &lt;a href=&quot;https://www.theguardian.com/commentisfree/2017/aug/04/surveillance-employers-spy-implanted-chipped&quot; data-link-name=&quot;in body link&quot; class=&quot;u-underline&quot;&gt;American company microchipped dozens of its workers&lt;/a&gt;. In a “chip party” that made headlines around the world, employees lined up to have a device the size of a grain of rice implanted under the skin between their thumb and forefinger. At first, Todd Westby, the CEO of Three Square Market, thought only about five or six people – him and a couple of directors, some of the people who worked in the IT department – would volunteer. But of the 90 people who work at the headquarters, 72 are now chipped; Westby has a chip in each hand. They can be used to open security doors, log on to computers and make payments at the company’s vending machines.&lt;/p&gt;
&lt;p&gt;Can he see it taking off at lots of other companies? “Not necessarily,” he says. Or at least not yet. It’s partly a generational thing, he believes. “You may never want to be chipped but if you’re a millennial, you have no problems. They think it’s cool.” There are other uses for it – two months ago, the company (whose core business is selling vending machines and kiosks) started chipping people with dementia in Puerto Rico. If someone wanders off and gets lost, police can scan the chip “and they will know all their medical information, what drugs they can and can’t have, they’ll know their identity.” So far, Three Square Market has chipped 100 people, but plans to do 10,000.&lt;/p&gt;
&lt;p&gt;The company has just launched a mobile phone app that pairs the chip with the phone’s GPS, enabling the implantee’s location to be tracked. Last week, it started using it with people released from prison on probation, as a replacement for ankle tags, which Westby describes as “intimidating and degrading”. Could he ever see the company using GPS to track its chipped employees? “No,” he says. “There’s no reason to.”&lt;/p&gt;

&lt;div class=&quot;u-responsive-ratio&quot;&gt;&lt;img class=&quot;gu-image&quot; itemprop=&quot;contentUrl&quot; alt=&quot;Tony Danna, vice president of at Three Square Market, gets his microchip implant.&quot; src=&quot;https://i.guim.co.uk/img/media/da44c631715ac49e4b0da9aeedc3cb45e175fc95/0_487_4753_2851/master/4753.jpg?w=300&amp;amp;q=55&amp;amp;auto=format&amp;amp;usm=12&amp;amp;fit=max&amp;amp;s=1b8cef1c1a44117f876f4b2a6827a44c&quot;/&gt;&lt;/div&gt;

Tony Danna, vice-president at Three Square Market, gets his microchip implant. Photograph: Jeff Baenen/AP
&lt;p&gt;Not all firms would agree. Tech companies are coming up with ever more bizarre and intrusive ways to monitor workforces. Last week the Times reported that some Chinese companies are using sensors in helmets and hats to scan workers’ brainwaves and detect fatigue, stress and even emotions such as anger. It added that one electrical company uses brainwave scans to decide how many breaks workers get, and for how long. The technology is used on high-speed train drivers to “detect fatigue and attention loss”. While this sort of technology may have legitimate safety applications – &lt;a href=&quot;https://learninglegacy.crossrail.co.uk/documents/workforce-fatigue-risk-management-using-wearable-technolgy/&quot; data-link-name=&quot;in body link&quot; class=&quot;u-underline&quot;&gt;a similar project was carried out with Crossrail workers&lt;/a&gt; using wristbands that sensed fatigue – it’s not hard to see how it could creep into other areas.&lt;/p&gt;
&lt;p&gt;In February, it was reported that Amazon had been granted patents for a wristband that not only tracked workers’ locations in the warehouse as they “picked” items to be dispatched, but could “read” their hand movements, buzzing or emitting a pulse to alert them when they were reaching for the wrong item. In the filing, Amazon describes it as being able to “monitor performance of the placing of the incoming inventory item into the identified storage location by the inventory system worker”.&lt;/p&gt;
&lt;p&gt;There are tech companies selling products that can take regular screenshots of employees’ work, monitor keystrokes and web usage, and even photograph them at their desks using their computers’ webcams. Working from home offers no protection, as all this can be done remotely. Software can monitor social media usage, analyse language or be installed on employees’ phones to monitor encrypted apps such as WhatsApp. Employees can be fitted with badges that not only track their location, but also monitor their tone of voice, how often they speak in meetings and who they speak to and for how long.&lt;/p&gt;
&lt;p&gt;Employees have always been watched at work, and technology has always been used to do it. But where it was once a factory foreman with a stopwatch, or workers having to physically clock in and out, now “all of that physical stuff has gone into digital technology”, says &lt;a href=&quot;https://www.theguardian.com/profile/andre-spicer&quot; data-link-name=&quot;in body link&quot; class=&quot;u-underline&quot;&gt;André Spicer&lt;/a&gt;, professor of organisational behaviour at Cass Business School. “It captures things that you weren’t able to capture in the past, like how many keystrokes are people taking, what are they looking at on their screen while they’re at work, what kind of language are they using. And surveillance follows you outside the workplace now.”&lt;/p&gt;

&lt;div class=&quot;u-responsive-ratio&quot;&gt;&lt;img class=&quot;gu-image&quot; itemprop=&quot;contentUrl&quot; alt=&quot;Some Chinese companies are using sensors to detect employees’ levels of fatigue and their emotional state.&quot; src=&quot;https://i.guim.co.uk/img/media/0bd2ab0ed07808c4187f10b04309f2de518946bb/0_87_4200_2519/master/4200.jpg?w=300&amp;amp;q=55&amp;amp;auto=format&amp;amp;usm=12&amp;amp;fit=max&amp;amp;s=046380adb5331b83f7caa1557486224c&quot;/&gt;&lt;/div&gt;

Some Chinese companies are using sensors to detect employees’ levels of fatigue and their emotional state. Photograph: AFP/Getty Images
&lt;p&gt;How much of this is legal? In the UK, employers are allowed to monitor which websites you look at while at work, says Philip Landau, a partner at Landau Law Solicitors who specialises in employment law. “However, the device they monitor must be partly or wholly provided by work. Employers must also give prior warning if they are going to monitor your online activity, and should make you aware of the relevant social media policy.” It is also legal to monitor keystrokes, though again employees must be told they will be watched. “In companies where this system is in place, it is not uncommon for employers to speak to employees if they feel that their number of keystrokes is low,” says Landau. “It is worth noting that a high number of keystrokes does not necessarily mean high levels of productivity and vice versa.”&lt;/p&gt;
&lt;p&gt;Employers could theoretically use your computer’s webcam to see when you’re at your desk but “there should be a justification for such monitoring, and you should be informed of it beforehand. You should also be informed what the pictures will be used for, and how they will be stored.” As for GPS tracking, “a company may track any vehicles that they supply to their staff. However, the data they collect must only be used for the management purposes of the company. Any GPS device is not allowed to be turned on if the employee is using the vehicle for personal reasons outside of work.”&lt;/p&gt;
&lt;p&gt;James Bloodworth spent a month working as a “picker” – the person who locates the products ordered – for Amazon in March 2016 for his book &lt;a href=&quot;https://www.theguardian.com/books/2018/mar/11/hired-six-months-undercover-in-low-wage-britain-zero-hours-review-james-bloodworth&quot; data-link-name=&quot;in body link&quot; class=&quot;u-underline&quot;&gt;Hired: Six Months Undercover in Low-Wage Britain&lt;/a&gt;. “We carried this handheld device at all times and it tracks your productivity,” he says. It would direct workers to the items they need to find on the shelves in one of Amazon’s vast warehouses. “Each time you picked up an item, there would be this countdown timer [to get to the next item] which would measure your productivity.” Bloodworth says supervisors would tell people how productive they were being; he was warned he was in the bottom 10%. “You were also sent admonishments through the device saying you need to get your productivity up. You’re constantly tracked and rated. I found you couldn’t keep up with the productivity targets without running – yet you were also told you weren’t allowed to run, and if you did, you’d get a disciplinary. But if you fell behind in productivity, you’d get a disciplinary for that as well.” It didn’t feel, he says, “that you were really treated as a human being”. Workers had to go through airport-style security scanners at the beginning and end of their shifts, or to get to the break areas. He says going to the loo was described as “idle time” and once found a bottle of urine on one of the shelves.&lt;/p&gt;

&lt;div class=&quot;u-responsive-ratio&quot;&gt;&lt;img class=&quot;gu-image&quot; itemprop=&quot;contentUrl&quot; alt=&quot;A worker in an Amazon distribution centre.&quot; src=&quot;https://i.guim.co.uk/img/media/7633fa096b42db6468e07a740b5e2c2a939b6fa1/0_0_3500_2100/master/3500.jpg?w=300&amp;amp;q=55&amp;amp;auto=format&amp;amp;usm=12&amp;amp;fit=max&amp;amp;s=2844eb5c78ca448d54c7627c832ffa4b&quot;/&gt;&lt;/div&gt;

A worker in an Amazon distribution centre. Photograph: Ralph Freso/Reuters
&lt;p&gt;Amazon says its scanning devices “are common across the warehouse and logistics sector as well as in supermarkets, department stores and other businesses, and are designed to assist our people in performing their roles”, while the company “ensures all of its associates have easy access to toilet facilities, which are just a short walk from where they are working”. It adds: “Associates are allowed to use the toilet whenever needed. We do not monitor toilet breaks.”&lt;/p&gt;
&lt;p&gt;Some of Bloodworth’s colleagues, he says, were angry about the level of monitoring – “but it was more cynicism and resignation. Most of the people I met hadn’t been in the job very long or were looking for other jobs. Every job was temporary and it was a workforce completely in flux.” Has Bloodworth seen the future? Will we all be monitored like this by our bosses in years to come? Possibly, he says. “One of the things that has arisen in response to the book is that people say work is going to be automated anyway, or workers need to be more flexible, as if this is the way of the future and it’s inevitable, which I think is quite dangerous. Amazon can get away with this because of political choices and because the trade union movement is quite weak. I think other businesses will look at Amazon, see they have had success with this business model – and seek to replicate it.”&lt;/p&gt;
&lt;p&gt;For his book &lt;a href=&quot;https://www.theguardian.com/careers/2017/feb/16/as-a-call-centre-worker-i-saw-how-employees-are-stripped-of-their-rights&quot; data-link-name=&quot;in body link&quot; class=&quot;u-underline&quot;&gt;Working the Phones, Jamie Woodcock&lt;/a&gt;, a sociologist of work at the Oxford Internet Institute, spent six months working in a call centre. You get a sense of the monitoring, he says, “from the moment you walk in. You have TV screens that have everyone’s relative performance to each other displayed. Managers collect data on almost every single part of what you do. Every single phone call I ever made was digitally recorded and stored. In terms of monitoring, it’s like being able to call back every single thing somebody has made on an assembly line and retrospectively judge it for quality. We all make mistakes and we all have bad days, but this kind of monitoring can be made retrospectively to sack people and is used to give people a sense that they could lose their jobs at any moment.”&lt;/p&gt;
&lt;p&gt;Monitoring is built into many of the jobs that form the so-called “gig economy”. It’s not easy to object to the constant surveillance when you’re desperate for work. What has surprised Spicer is how willingly people in better-paid jobs have taken to it. “Prisoners in the past were forced to wear tracking bands but now we willingly put on step trackers or other kinds of tracking devices given to us by our employers, and in some cases we pay for the privilege.” Companies such as IBM, BP, Bank of America, Target and Barclays &lt;a href=&quot;https://www.fastcompany.com/3058462/how-fitbit-became-the-next-big-thing-in-corporate-wellness&quot; data-link-name=&quot;in body link&quot; class=&quot;u-underline&quot;&gt;have offered their employees Fitbit activity trackers&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It is part, Spicer says, of “this whole idea of wanting to improve or optimise yourself. A lot of technology is designed to not just feed back data about your performance to your boss, but also give it to you. I guess they’re also seen as cool or fashionable, so it’s not surprising they’re taken up so readily.”&lt;/p&gt;
&lt;p&gt;Spicer has watched the shift away from “monitoring something like emails to monitoring people’s bodies – the rise of bio-tracking basically. The monitoring of your vital signs, emotions, moods.” Of Three Square Market’s practice of chipping employees, he says: “You can imagine that slowly extending. You could imagine things like employers asking to have your DNA in the future, and other kinds of data.”&lt;/p&gt;

&lt;div class=&quot;u-responsive-ratio&quot;&gt;&lt;img class=&quot;gu-image&quot; itemprop=&quot;contentUrl&quot; alt=&quot;Deliveroo already monitors its riders and drivers’ performance.&quot; src=&quot;https://i.guim.co.uk/img/media/71ed23395ec3817a93be7cde981935aca0ca03b8/0_46_3500_2100/master/3500.jpg?w=300&amp;amp;q=55&amp;amp;auto=format&amp;amp;usm=12&amp;amp;fit=max&amp;amp;s=6e72bbf197c3dee949f1b7863d0c29cf&quot;/&gt;&lt;/div&gt;

Deliveroo already monitors its riders and drivers’ performance. Photograph: Charles Platiau/Reuters
&lt;p&gt;Surveillance can have positive applications. It’s necessary (and legally required) in the financial industry to prevent insider trading. It could be used to prevent harassment and bullying, and to root out bias and discrimination. &lt;a href=&quot;https://hbr.org/2017/10/a-study-used-sensors-to-show-that-men-and-women-are-treated-differently-at-work&quot; data-link-name=&quot;in body link&quot; class=&quot;u-underline&quot;&gt;One interesting study last year&lt;/a&gt; monitored emails and productivity, and used sensors to track behaviour and interaction with management, and found that men and women behaved almost identically at work. The findings challenged the belief that the reason women are not promoted to senior levels is that they are less proactive or have fewer interactions with leaders, and simply need to “lean in”.&lt;/p&gt;
&lt;p&gt;Still, says, Woodcock, “we need to have a conversation in society about whether work should be somewhere that you’re surveilled”. That need is perhaps most urgent where low-paid, insecure jobs are concerned. “If you work in the gig economy, you have a smartphone,” Woodcock points out, and that smartphone can be used to track you. “I think because many of these workplaces don’t have traditional forms of organisation or trade unions, management are able to introduce these things with relatively little collective resistance.”&lt;/p&gt;
&lt;p&gt;The Independent Workers Union of Great Britain is well aware of the issues of monitoring and data collection. James Farrar is the chair of its United Private Hire Drivers branch, and the Uber driver who won a legal battle against the company last year for drivers’ rights. “They do collect an awful lot of information,” he says. “One of the things they will report to you on a daily basis is how good your acceleration and braking has been. You get a rating. The question is: why are they collecting that information?” Uber also monitors “unusual movements” of the phone when someone is driving (implying it knows if someone is using their phone while at the wheel) and, of course, tracks cars and drivers by GPS.&lt;/p&gt;
&lt;p&gt;“My concern with it is this information is being fed into a dispatch algorithm,” he says. “We should have access to the data and understand how it’s being used. If some kind of quality score on my driving capability [is put into an algorithm], I may be offered less valuable work, kept away from the most valuable clients – who knows?” It’s not an unreasonable fear – the food delivery company Deliveroo already does something similar, monitoring its riders’ and drivers’ performance, and has started offering &lt;a href=&quot;https://www.buzzfeed.com/saraspary/deliveroo-drivers-say-the-companys-new-terms-are-forcing?utm_term=.ceeRLjazA#.iyx273Ow5&quot; data-link-name=&quot;in body link&quot; class=&quot;u-underline&quot;&gt;“priority access” when booking shifts to those who “provide the most consistent, quality service”&lt;/a&gt;. Uber, however, says its monitoring is intended only to deliver “a smoother, safer ride … This data is used to inform drivers of their driving habits and is not used to affect future trip requests.”&lt;/p&gt;
&lt;p&gt;Not all surveillance is bad, says Farrar. In some ways, he would like more. He was assaulted by a passenger and is calling for CCTV in all vehicles, partly for the safety of drivers. “There is a role for surveillance technology,” he says. Ironically, when Farrar went for a meeting with Uber to discuss the assault, the company made him turn his phone off to prove he wasn’t recording it.&lt;/p&gt;


</description>
<pubDate>Mon, 14 May 2018 15:36:42 +0000</pubDate>
<dc:creator>pmoriarty</dc:creator>
<og:url>http://www.theguardian.com/world/2018/may/14/is-your-boss-secretly-or-not-so-secretly-watching-you</og:url>
<og:description>From microchip implants to wristband trackers and sensors that can detect fatigue and depression, new technology is enabling employers to watch staff in more and more intrusive ways. How worried should we be?</og:description>
<og:image>https://i.guim.co.uk/img/media/4fe1c0e4e173f2ac7e3428b93d99e5ce2dcded5a/43_0_1280_768/master/1280.jpg?w=1200&amp;h=630&amp;q=55&amp;auto=format&amp;usm=12&amp;fit=crop&amp;crop=faces%2Centropy&amp;bm=normal&amp;ba=bottom%2Cleft&amp;blend64=aHR0cHM6Ly91cGxvYWRzLmd1aW0uY28udWsvMjAxOC8wMS8zMS9mYWNlYm9va19kZWZhdWx0LnBuZw&amp;s=9e1b57c80d4ae497fae2babab0953ea2</og:image>
<og:type>article</og:type>
<og:title>Employers are monitoring computers, toilet breaks – even emotions. Is your boss watching you?</og:title>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.theguardian.com/world/2018/may/14/is-your-boss-secretly-or-not-so-secretly-watching-you</dc:identifier>
</item>
<item>
<title>Wi-Fi alliance announces mesh networking standard</title>
<link>https://www.wi-fi.org/discover-wi-fi/wi-fi-easymesh</link>
<guid isPermaLink="true" >https://www.wi-fi.org/discover-wi-fi/wi-fi-easymesh</guid>
<description>&lt;p&gt;Wi-Fi EasyMesh is a certification program that defines multiple access point home and small office Wi-Fi networks that are easy to install and use, self-adapting, and add multi-vendor interoperability. This technology brings both consumers and service providers additional flexibility in choosing Wi-Fi EasyMesh devices for home deployment.&lt;/p&gt;&lt;p&gt;Wi-Fi EasyMesh uses a controller to manage the network, which consists of the controller plus additional APs, called agents. Establishing controllers to manage and coordinate activity among the agents ensures that each AP does not interfere with the other, bringing both expanded, uniform coverage and more efficient service.&lt;/p&gt;
</description>
<pubDate>Mon, 14 May 2018 14:26:31 +0000</pubDate>
<dc:creator>jwilliams</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.wi-fi.org/discover-wi-fi/wi-fi-easymesh</dc:identifier>
</item>
<item>
<title>How Rust 1.26 more than tripled the speed of my code</title>
<link>http://troubles.md/posts/the-power-of-compilers/</link>
<guid isPermaLink="true" >http://troubles.md/posts/the-power-of-compilers/</guid>
<description>&lt;aside/&gt;&lt;p&gt;I’d like to share a quick story about the sheer power of LLVM and the benefits of using higher-level languages over assembly.&lt;/p&gt;
&lt;p&gt;I work at Parity Technologies, who maintains the &lt;a href=&quot;https://github.com/paritytech/parity&quot;&gt;Parity Ethereum client&lt;/a&gt;. In this client we have a need for performant 256-bit arithmetic, which we have to emulate in software since no modern hardware supports it natively.&lt;/p&gt;
&lt;p&gt;For a long time we’ve maintained parallel implementations of arithmetic, one in Rust for stable builds and one in inline assembly (which is automatically used when you compile with the nightly compiler). We do this because we store these 256-bit numbers as arrays of 64-bit numbers and there is no way to multiply two 64-bit numbers to get a more-than-64-bit result in Rust (since Rust’s integer types only go up to &lt;code&gt;u64&lt;/code&gt;). This is despite the fact that x86_64 (our main target platform) natively supports 128-bit results of calculations with 64-bit numbers. So, we resort to splitting the 64-bit numbers into two 32-bit numbers (because we &lt;em&gt;can&lt;/em&gt; multiply two 32-bit numbers to get a 64-bit result).&lt;/p&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;24&quot;&gt;
&lt;pre&gt;
&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span&gt;impl&lt;/span&gt; U256 {
  &lt;span&gt;fn&lt;/span&gt; &lt;span&gt;full_mul&lt;/span&gt;(self, other: &lt;span&gt;Self&lt;/span&gt;) -&amp;gt; &lt;span&gt;U512&lt;/span&gt; {
    &lt;span&gt;let&lt;/span&gt; U256(&lt;span&gt;ref&lt;/span&gt; me) &lt;span&gt;=&lt;/span&gt; self;
    &lt;span&gt;let&lt;/span&gt; U256(&lt;span&gt;ref&lt;/span&gt; you) &lt;span&gt;=&lt;/span&gt; other;
    &lt;span&gt;let&lt;/span&gt; &lt;span&gt;mut&lt;/span&gt; ret &lt;span&gt;=&lt;/span&gt; [&lt;span&gt;0&lt;/span&gt;&lt;span&gt;u64&lt;/span&gt;; U512_SIZE];


    &lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt; &lt;span&gt;0&lt;/span&gt;..U256_SIZE {
      &lt;span&gt;let&lt;/span&gt; &lt;span&gt;mut&lt;/span&gt; carry &lt;span&gt;=&lt;/span&gt; &lt;span&gt;0&lt;/span&gt;&lt;span&gt;u64&lt;/span&gt;;
      &lt;span&gt;// `split` splits a 64-bit number into upper and lower halves
&lt;/span&gt;      &lt;span&gt;let&lt;/span&gt; (b_u, b_l) &lt;span&gt;=&lt;/span&gt; split(you[i]);

      &lt;span&gt;for&lt;/span&gt; j &lt;span&gt;in&lt;/span&gt; &lt;span&gt;0&lt;/span&gt;..U256_SIZE {
        &lt;span&gt;// This process is so slow that it's faster to check for 0 and skip
&lt;/span&gt;        &lt;span&gt;// it if possible.
&lt;/span&gt;        &lt;span&gt;if&lt;/span&gt; me[j] &lt;span&gt;!=&lt;/span&gt; &lt;span&gt;0&lt;/span&gt; &lt;span&gt;||&lt;/span&gt; carry &lt;span&gt;!=&lt;/span&gt; &lt;span&gt;0&lt;/span&gt; {
          &lt;span&gt;let&lt;/span&gt; a &lt;span&gt;=&lt;/span&gt; split(me[j]);

          &lt;span&gt;// `mul_u32` multiplies a 64-bit number that's been split into
&lt;/span&gt;          &lt;span&gt;// an `(upper, lower)` pair by a 32-bit number to get a 96-bit
&lt;/span&gt;          &lt;span&gt;// result. Yes, 96-bit (it returns a `(u32, u64)` pair).
&lt;/span&gt;          &lt;span&gt;let&lt;/span&gt; (c_l, overflow_l) &lt;span&gt;=&lt;/span&gt; mul_u32(a, b_l, ret[i &lt;span&gt;+&lt;/span&gt; j]);

          &lt;span&gt;// Since we have to multiply by a 64-bit number, we have to do
&lt;/span&gt;          &lt;span&gt;// this twice.
&lt;/span&gt;          &lt;span&gt;let&lt;/span&gt; (c_u, overflow_u) &lt;span&gt;=&lt;/span&gt; mul_u32(a, b_u, c_l &lt;span&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span&gt;32&lt;/span&gt;);
          ret[i &lt;span&gt;+&lt;/span&gt; j] &lt;span&gt;=&lt;/span&gt; (c_l &lt;span&gt;&amp;amp;&lt;/span&gt; &lt;span&gt;0xffffffff&lt;/span&gt;) &lt;span&gt;+&lt;/span&gt; (c_u &lt;span&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span&gt;32&lt;/span&gt;);

          &lt;span&gt;// Then we have to do this complex logic to set the result. Gross.
&lt;/span&gt;          &lt;span&gt;let&lt;/span&gt; res &lt;span&gt;=&lt;/span&gt; (c_u &lt;span&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span&gt;32&lt;/span&gt;) &lt;span&gt;+&lt;/span&gt; (overflow_u &lt;span&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span&gt;32&lt;/span&gt;);
          &lt;span&gt;let&lt;/span&gt; (res, o1) &lt;span&gt;=&lt;/span&gt; res.overflowing_add(overflow_l &lt;span&gt;+&lt;/span&gt; carry);
          &lt;span&gt;let&lt;/span&gt; (res, o2) &lt;span&gt;=&lt;/span&gt; res.overflowing_add(ret[i &lt;span&gt;+&lt;/span&gt; j &lt;span&gt;+&lt;/span&gt; &lt;span&gt;1&lt;/span&gt;]);
          ret[i &lt;span&gt;+&lt;/span&gt; j &lt;span&gt;+&lt;/span&gt; &lt;span&gt;1&lt;/span&gt;] &lt;span&gt;=&lt;/span&gt; res;

          carry &lt;span&gt;=&lt;/span&gt; (o1 &lt;span&gt;|&lt;/span&gt; o2) &lt;span&gt;as&lt;/span&gt; &lt;span&gt;u64&lt;/span&gt;;
        }
      }
    }

    U512(ret)
  }
}&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You don’t even have to understand all of the code to see how non-optimal this is. Inspecting the output of the compiler shows that the generated assembly is extremely suboptimal. It does much more work than necessary essentially just to work around limitations in the Rust language. So we wrote an inline assembly version. The important thing about using inline assembly here is that x86_64 natively supports multiplying two 64-bit values into a 128-bit result. When Rust does &lt;code&gt;a * b&lt;/code&gt; when &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are both &lt;code&gt;u64&lt;/code&gt; the CPU actually multiplies them to create a 128-bit result and then Rust just throws away the upper 64 bits. We want the upper 64 in this case though, and the only way to access it efficiently is by using inline assembly.&lt;/p&gt;
&lt;p&gt;As you can imagine, our assembly implementation was much faster:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt; name            u64.bench ns/iter  inline_asm.bench ns/iter  diff ns/iter   diff %  speedup 
 u256_full_mul   243,159            197,396                        -45,763  -18.82%   x 1.23 
 u256_mul        268,750            95,843                        -172,907  -64.34%   x 2.80 
 u256_mul_small  1,608              789                               -819  -50.93%   x 2.04 
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;u256_full_mul&lt;/code&gt; tests the function above, &lt;code&gt;u256_mul&lt;/code&gt; multiplies two 256-bit numbers to get a 256-bit result (in Rust, we just create a 512-bit result and then throw away the top half but in assembly we have a seperate implementation), and &lt;code&gt;u256_mul_small&lt;/code&gt; multiplies two small 256-bit numbers. As you can see, the assembly implementation is up to 65% faster. This is way, way better. Unfortunately, it only works on nightly, and even then only on x86_64. The truth is that it was a lot of effort and a number of thrown-away implementations to even get the Rust code to “only” half the speed of the assembly, too. There was simply no good way to give the compiler the information necessary.&lt;/p&gt;
&lt;p&gt;All that changed with &lt;a href=&quot;https://blog.rust-lang.org/2018/05/10/Rust-1.26.html&quot;&gt;Rust 1.26&lt;/a&gt;. Now we can do &lt;code&gt;a as u128 * b as u128&lt;/code&gt; and the compiler will use x86_64’s native u64-to-u128 multiplication (even though you cast both numbers to &lt;code&gt;u128&lt;/code&gt; it knows that they’re “really” just &lt;code&gt;u64&lt;/code&gt;, you just want a &lt;code&gt;u128&lt;/code&gt; result). That means our code now looks like this:&lt;/p&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;15&quot;&gt;
&lt;pre&gt;
&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span&gt;impl&lt;/span&gt; U256 {
  &lt;span&gt;fn&lt;/span&gt; &lt;span&gt;full_mul&lt;/span&gt;(self, other: &lt;span&gt;Self&lt;/span&gt;) -&amp;gt; &lt;span&gt;U512&lt;/span&gt; {
    &lt;span&gt;let&lt;/span&gt; U256(&lt;span&gt;ref&lt;/span&gt; me) &lt;span&gt;=&lt;/span&gt; self;
    &lt;span&gt;let&lt;/span&gt; U256(&lt;span&gt;ref&lt;/span&gt; you) &lt;span&gt;=&lt;/span&gt; other;
    &lt;span&gt;let&lt;/span&gt; &lt;span&gt;mut&lt;/span&gt; ret &lt;span&gt;=&lt;/span&gt; [&lt;span&gt;0&lt;/span&gt;&lt;span&gt;u64&lt;/span&gt;; U512_SIZE];

    &lt;span&gt;for&lt;/span&gt; i &lt;span&gt;in&lt;/span&gt; &lt;span&gt;0&lt;/span&gt;..U256_SIZE {
      &lt;span&gt;let&lt;/span&gt; &lt;span&gt;mut&lt;/span&gt; carry &lt;span&gt;=&lt;/span&gt; &lt;span&gt;0&lt;/span&gt;&lt;span&gt;u64&lt;/span&gt;;
      &lt;span&gt;let&lt;/span&gt; b &lt;span&gt;=&lt;/span&gt; you[i];

      &lt;span&gt;for&lt;/span&gt; j &lt;span&gt;in&lt;/span&gt; &lt;span&gt;0&lt;/span&gt;..U256_SIZE {
        &lt;span&gt;let&lt;/span&gt; a &lt;span&gt;=&lt;/span&gt; me[j];

        &lt;span&gt;// This compiles down to just use x86's native 128-bit arithmetic
&lt;/span&gt;        &lt;span&gt;let&lt;/span&gt; (hi, low) &lt;span&gt;=&lt;/span&gt; split_u128(a &lt;span&gt;as&lt;/span&gt; u128 &lt;span&gt;*&lt;/span&gt; b &lt;span&gt;as&lt;/span&gt; u128);

        &lt;span&gt;let&lt;/span&gt; overflow &lt;span&gt;=&lt;/span&gt; {
          &lt;span&gt;let&lt;/span&gt; existing_low &lt;span&gt;=&lt;/span&gt; &lt;span&gt;&amp;amp;&lt;/span&gt;&lt;span&gt;mut&lt;/span&gt; ret[i &lt;span&gt;+&lt;/span&gt; j];
          &lt;span&gt;let&lt;/span&gt; (low, o) &lt;span&gt;=&lt;/span&gt; low.overflowing_add(&lt;span&gt;*&lt;/span&gt;existing_low);
          &lt;span&gt;*&lt;/span&gt;existing_low &lt;span&gt;=&lt;/span&gt; low;
          o
        };

        carry &lt;span&gt;=&lt;/span&gt; {
          &lt;span&gt;let&lt;/span&gt; existing_hi &lt;span&gt;=&lt;/span&gt; &lt;span&gt;&amp;amp;&lt;/span&gt;&lt;span&gt;mut&lt;/span&gt; ret[i &lt;span&gt;+&lt;/span&gt; j &lt;span&gt;+&lt;/span&gt; &lt;span&gt;1&lt;/span&gt;];
          &lt;span&gt;let&lt;/span&gt; hi &lt;span&gt;=&lt;/span&gt; hi &lt;span&gt;+&lt;/span&gt; overflow &lt;span&gt;as&lt;/span&gt; &lt;span&gt;u64&lt;/span&gt;;
          &lt;span&gt;let&lt;/span&gt; (hi, o0) &lt;span&gt;=&lt;/span&gt; hi.overflowing_add(carry);
          &lt;span&gt;let&lt;/span&gt; (hi, o1) &lt;span&gt;=&lt;/span&gt; hi.overflowing_add(&lt;span&gt;*&lt;/span&gt;existing_hi);
          &lt;span&gt;*&lt;/span&gt;existing_hi &lt;span&gt;=&lt;/span&gt; hi;

          (o0 &lt;span&gt;|&lt;/span&gt; o1) &lt;span&gt;as&lt;/span&gt; &lt;span&gt;u64&lt;/span&gt;
        }
      }
    }

    U512(ret)
  }
}&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Although it’s almost certainly not as fast as using the LLVM-native &lt;code&gt;i256&lt;/code&gt; type, the speed is much, much better. Here it is compared to the original Rust implementation:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt; name            u64.bench ns/iter  u128.bench ns/iter  diff ns/iter   diff %  speedup 
 u256_full_mul   243,159            73,416                  -169,743  -69.81%   x 3.31 
 u256_mul        268,750            85,797                  -182,953  -68.08%   x 3.13 
 u256_mul_small  1,608              558                       -1,050  -65.30%   x 2.88 
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Which is great, we now get a speed boost on stable. Since we only compile the binaries for the Parity client on stable the only people who could use the assembly before were those who compiled from source, so this is an improvement for a lot of users. But wait, there’s more! The new compiled code actually manages to beat the assembly implementation by a significant margin, even beating the assembly on the benchmark that multiplies two 256-bit numbers to get a 256-bit result. This is despite the fact that the Rust code still produces a 512-bit result first and then discards the upper half, where the assembly implementation does not:&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt; name            inline_asm.bench ns/iter  u128.bench ns/iter  diff ns/iter   diff %  speedup 
 u256_full_mul   197,396                   73,416                  -123,980  -62.81%   x 2.69 
 u256_mul        95,843                    85,797                   -10,046  -10.48%   x 1.12 
 u256_mul_small  789                       558                         -231  -29.28%   x 1.41 
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;For the full multiplication that’s an absolutely massive improvement, especially since the original code used highly-optimised assembly incantations from our resident cycle wizard. Here’s where the faint of heart might want to step out for a moment, because I’m about to dive into the generated assembly.&lt;/p&gt;
&lt;p&gt;Here’s the hand-written assembly. I’ve presented it without comment because I want to comment the assembly that is actually emitted by the compiler (since, as you’ll see, the &lt;code&gt;asm!&lt;/code&gt; macro hides more than you’d expect):&lt;/p&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;110&quot;&gt;
&lt;pre&gt;
&lt;code class=&quot;language-rust&quot; data-lang=&quot;rust&quot;&gt;&lt;span&gt;impl&lt;/span&gt; U256 {
  &lt;span&gt;/// Multiplies two 256-bit integers to produce full 512-bit integer
&lt;/span&gt;  &lt;span&gt;/// No overflow possible
&lt;/span&gt;  &lt;span&gt;pub&lt;/span&gt; &lt;span&gt;fn&lt;/span&gt; &lt;span&gt;full_mul&lt;/span&gt;(self, other: &lt;span&gt;U256&lt;/span&gt;) -&amp;gt; &lt;span&gt;U512&lt;/span&gt; {
    &lt;span&gt;let&lt;/span&gt; self_t: &lt;span&gt;&amp;amp;&lt;/span&gt;[&lt;span&gt;u64&lt;/span&gt;; &lt;span&gt;4&lt;/span&gt;] &lt;span&gt;=&lt;/span&gt; &lt;span&gt;&amp;amp;&lt;/span&gt;self.&lt;span&gt;0&lt;/span&gt;;
    &lt;span&gt;let&lt;/span&gt; other_t: &lt;span&gt;&amp;amp;&lt;/span&gt;[&lt;span&gt;u64&lt;/span&gt;; &lt;span&gt;4&lt;/span&gt;] &lt;span&gt;=&lt;/span&gt; &lt;span&gt;&amp;amp;&lt;/span&gt;other.&lt;span&gt;0&lt;/span&gt;;
    &lt;span&gt;let&lt;/span&gt; &lt;span&gt;mut&lt;/span&gt; result: [&lt;span&gt;u64&lt;/span&gt;; &lt;span&gt;8&lt;/span&gt;] &lt;span&gt;=&lt;/span&gt; &lt;span&gt;unsafe&lt;/span&gt; { ::core::mem::uninitialized() };
    &lt;span&gt;unsafe&lt;/span&gt; {
      asm&lt;span&gt;!&lt;/span&gt;(&lt;span&gt;&quot;
&lt;/span&gt;&lt;span&gt;        mov $8, %rax
&lt;/span&gt;&lt;span&gt;        mulq $12
&lt;/span&gt;&lt;span&gt;        mov %rax, $0
&lt;/span&gt;&lt;span&gt;        mov %rdx, $1
&lt;/span&gt;&lt;span&gt;        mov $8, %rax
&lt;/span&gt;&lt;span&gt;        mulq $13
&lt;/span&gt;&lt;span&gt;        add %rax, $1
&lt;/span&gt;&lt;span&gt;        adc $$0, %rdx
&lt;/span&gt;&lt;span&gt;        mov %rdx, $2
&lt;/span&gt;&lt;span&gt;        mov $8, %rax
&lt;/span&gt;&lt;span&gt;        mulq $14
&lt;/span&gt;&lt;span&gt;        add %rax, $2
&lt;/span&gt;&lt;span&gt;        adc $$0, %rdx
&lt;/span&gt;&lt;span&gt;        mov %rdx, $3
&lt;/span&gt;&lt;span&gt;        mov $8, %rax
&lt;/span&gt;&lt;span&gt;        mulq $15
&lt;/span&gt;&lt;span&gt;        add %rax, $3
&lt;/span&gt;&lt;span&gt;        adc $$0, %rdx
&lt;/span&gt;&lt;span&gt;        mov %rdx, $4
&lt;/span&gt;&lt;span&gt;        mov $9, %rax
&lt;/span&gt;&lt;span&gt;        mulq $12
&lt;/span&gt;&lt;span&gt;        add %rax, $1
&lt;/span&gt;&lt;span&gt;        adc %rdx, $2
&lt;/span&gt;&lt;span&gt;        adc $$0, $3
&lt;/span&gt;&lt;span&gt;        adc $$0, $4
&lt;/span&gt;&lt;span&gt;        xor $5, $5
&lt;/span&gt;&lt;span&gt;        adc $$0, $5
&lt;/span&gt;&lt;span&gt;        xor $6, $6
&lt;/span&gt;&lt;span&gt;        adc $$0, $6
&lt;/span&gt;&lt;span&gt;        xor $7, $7
&lt;/span&gt;&lt;span&gt;        adc $$0, $7
&lt;/span&gt;&lt;span&gt;        mov $9, %rax
&lt;/span&gt;&lt;span&gt;        mulq $13
&lt;/span&gt;&lt;span&gt;        add %rax, $2
&lt;/span&gt;&lt;span&gt;        adc %rdx, $3
&lt;/span&gt;&lt;span&gt;        adc $$0, $4
&lt;/span&gt;&lt;span&gt;        adc $$0, $5
&lt;/span&gt;&lt;span&gt;        adc $$0, $6
&lt;/span&gt;&lt;span&gt;        adc $$0, $7
&lt;/span&gt;&lt;span&gt;        mov $9, %rax
&lt;/span&gt;&lt;span&gt;        mulq $14
&lt;/span&gt;&lt;span&gt;        add %rax, $3
&lt;/span&gt;&lt;span&gt;        adc %rdx, $4
&lt;/span&gt;&lt;span&gt;        adc $$0, $5
&lt;/span&gt;&lt;span&gt;        adc $$0, $6
&lt;/span&gt;&lt;span&gt;        adc $$0, $7
&lt;/span&gt;&lt;span&gt;        mov $9, %rax
&lt;/span&gt;&lt;span&gt;        mulq $15
&lt;/span&gt;&lt;span&gt;        add %rax, $4
&lt;/span&gt;&lt;span&gt;        adc %rdx, $5
&lt;/span&gt;&lt;span&gt;        adc $$0, $6
&lt;/span&gt;&lt;span&gt;        adc $$0, $7
&lt;/span&gt;&lt;span&gt;        mov $10, %rax
&lt;/span&gt;&lt;span&gt;        mulq $12
&lt;/span&gt;&lt;span&gt;        add %rax, $2
&lt;/span&gt;&lt;span&gt;        adc %rdx, $3
&lt;/span&gt;&lt;span&gt;        adc $$0, $4
&lt;/span&gt;&lt;span&gt;        adc $$0, $5
&lt;/span&gt;&lt;span&gt;        adc $$0, $6
&lt;/span&gt;&lt;span&gt;        adc $$0, $7
&lt;/span&gt;&lt;span&gt;        mov $10, %rax
&lt;/span&gt;&lt;span&gt;        mulq $13
&lt;/span&gt;&lt;span&gt;        add %rax, $3
&lt;/span&gt;&lt;span&gt;        adc %rdx, $4
&lt;/span&gt;&lt;span&gt;        adc $$0, $5
&lt;/span&gt;&lt;span&gt;        adc $$0, $6
&lt;/span&gt;&lt;span&gt;        adc $$0, $7
&lt;/span&gt;&lt;span&gt;        mov $10, %rax
&lt;/span&gt;&lt;span&gt;        mulq $14
&lt;/span&gt;&lt;span&gt;        add %rax, $4
&lt;/span&gt;&lt;span&gt;        adc %rdx, $5
&lt;/span&gt;&lt;span&gt;        adc $$0, $6
&lt;/span&gt;&lt;span&gt;        adc $$0, $7
&lt;/span&gt;&lt;span&gt;        mov $10, %rax
&lt;/span&gt;&lt;span&gt;        mulq $15
&lt;/span&gt;&lt;span&gt;        add %rax, $5
&lt;/span&gt;&lt;span&gt;        adc %rdx, $6
&lt;/span&gt;&lt;span&gt;        adc $$0, $7
&lt;/span&gt;&lt;span&gt;        mov $11, %rax
&lt;/span&gt;&lt;span&gt;        mulq $12
&lt;/span&gt;&lt;span&gt;        add %rax, $3
&lt;/span&gt;&lt;span&gt;        adc %rdx, $4
&lt;/span&gt;&lt;span&gt;        adc $$0, $5
&lt;/span&gt;&lt;span&gt;        adc $$0, $6
&lt;/span&gt;&lt;span&gt;        adc $$0, $7
&lt;/span&gt;&lt;span&gt;        mov $11, %rax
&lt;/span&gt;&lt;span&gt;        mulq $13
&lt;/span&gt;&lt;span&gt;        add %rax, $4
&lt;/span&gt;&lt;span&gt;        adc %rdx, $5
&lt;/span&gt;&lt;span&gt;        adc $$0, $6
&lt;/span&gt;&lt;span&gt;        adc $$0, $7
&lt;/span&gt;&lt;span&gt;        mov $11, %rax
&lt;/span&gt;&lt;span&gt;        mulq $14
&lt;/span&gt;&lt;span&gt;        add %rax, $5
&lt;/span&gt;&lt;span&gt;        adc %rdx, $6
&lt;/span&gt;&lt;span&gt;        adc $$0, $7
&lt;/span&gt;&lt;span&gt;        mov $11, %rax
&lt;/span&gt;&lt;span&gt;        mulq $15
&lt;/span&gt;&lt;span&gt;        add %rax, $6
&lt;/span&gt;&lt;span&gt;        adc %rdx, $7
&lt;/span&gt;&lt;span&gt;        &quot;&lt;/span&gt;
      : &lt;span&gt;/* $0 */&lt;/span&gt; &lt;span&gt;&quot;={r8}&quot;&lt;/span&gt;(result[&lt;span&gt;0&lt;/span&gt;]),  &lt;span&gt;/* $1 */&lt;/span&gt; &lt;span&gt;&quot;={r9}&quot;&lt;/span&gt;(result[&lt;span&gt;1&lt;/span&gt;]),  &lt;span&gt;/* $2 */&lt;/span&gt; &lt;span&gt;&quot;={r10}&quot;&lt;/span&gt;(result[&lt;span&gt;2&lt;/span&gt;]),
        &lt;span&gt;/* $3 */&lt;/span&gt; &lt;span&gt;&quot;={r11}&quot;&lt;/span&gt;(result[&lt;span&gt;3&lt;/span&gt;]), &lt;span&gt;/* $4 */&lt;/span&gt; &lt;span&gt;&quot;={r12}&quot;&lt;/span&gt;(result[&lt;span&gt;4&lt;/span&gt;]), &lt;span&gt;/* $5 */&lt;/span&gt; &lt;span&gt;&quot;={r13}&quot;&lt;/span&gt;(result[&lt;span&gt;5&lt;/span&gt;]),
        &lt;span&gt;/* $6 */&lt;/span&gt; &lt;span&gt;&quot;={r14}&quot;&lt;/span&gt;(result[&lt;span&gt;6&lt;/span&gt;]), &lt;span&gt;/* $7 */&lt;/span&gt; &lt;span&gt;&quot;={r15}&quot;&lt;/span&gt;(result[&lt;span&gt;7&lt;/span&gt;])

      : &lt;span&gt;/* $8 */&lt;/span&gt; &lt;span&gt;&quot;m&quot;&lt;/span&gt;(self_t[&lt;span&gt;0&lt;/span&gt;]),   &lt;span&gt;/* $9 */&lt;/span&gt; &lt;span&gt;&quot;m&quot;&lt;/span&gt;(self_t[&lt;span&gt;1&lt;/span&gt;]),   &lt;span&gt;/* $10 */&lt;/span&gt;  &lt;span&gt;&quot;m&quot;&lt;/span&gt;(self_t[&lt;span&gt;2&lt;/span&gt;]),
        &lt;span&gt;/* $11 */&lt;/span&gt; &lt;span&gt;&quot;m&quot;&lt;/span&gt;(self_t[&lt;span&gt;3&lt;/span&gt;]),  &lt;span&gt;/* $12 */&lt;/span&gt; &lt;span&gt;&quot;m&quot;&lt;/span&gt;(other_t[&lt;span&gt;0&lt;/span&gt;]), &lt;span&gt;/* $13 */&lt;/span&gt; &lt;span&gt;&quot;m&quot;&lt;/span&gt;(other_t[&lt;span&gt;1&lt;/span&gt;]),
        &lt;span&gt;/* $14 */&lt;/span&gt; &lt;span&gt;&quot;m&quot;&lt;/span&gt;(other_t[&lt;span&gt;2&lt;/span&gt;]), &lt;span&gt;/* $15 */&lt;/span&gt; &lt;span&gt;&quot;m&quot;&lt;/span&gt;(other_t[&lt;span&gt;3&lt;/span&gt;])
      : &lt;span&gt;&quot;rax&quot;&lt;/span&gt;, &lt;span&gt;&quot;rdx&quot;&lt;/span&gt;
      :
      );
    }

    U512(result)
  }
}&lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And here’s what that generates. I’ve heavily commented it so you can understand what’s going on even if you’ve never touched assembly in your life, but you will need to know basic low-level details like the difference between memory and registers. If you want to get a primer on the structure of a CPU, the &lt;a href=&quot;https://en.wikipedia.org/wiki/Central_processing_unit#Structure_and_implementation&quot;&gt;Wikipedia article on structure and implementation of CPUs&lt;/a&gt; is a good place to start:&lt;/p&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;132&quot;&gt;
&lt;pre&gt;
&lt;code class=&quot;language-assembly&quot; data-lang=&quot;assembly&quot;&gt;libbigint.so`disassemble:
    ;; Function prelude - this is generated by Rust
    pushq  %r15
    pushq  %r14
    pushq  %r13
    pushq  %r12
    subq   $0x40, %rsp

    ;; Load the input arrays into registers...
    movq   0x68(%rsp), %rax
    movq   0x70(%rsp), %rcx
    movq   0x78(%rsp), %rdx
    movq   0x80(%rsp), %rsi
    movq   0x88(%rsp), %r8
    movq   0x90(%rsp), %r9
    movq   0x98(%rsp), %r10
    movq   0xa0(%rsp), %r11

    ;; ...and then immediately back into memory
    ;; This is done by the Rust compiler. There is a way to avoid
    ;; this happening but I'll get to that later
    ;; These four are the first input array
    movq   %rax, 0x38(%rsp)
    movq   %rcx, 0x30(%rsp)
    movq   %rdx, 0x28(%rsp)
    movq   %rsi, 0x20(%rsp)
    ;; These four are the output array, which is initialised to be
    ;; the same as the second input array.
    movq   %r8,  0x18(%rsp)
    movq   %r9,  0x10(%rsp)
    movq   %r10, 0x8(%rsp)
    movq   %r11, (%rsp)

    ;; This is the main loop, you'll see the same code repeated many
    ;; times since it's been unrolled so I won't go over it every time.
    ;; This takes the form of a loop that looks like:
    ;;
    ;; for i in 0..U256_SIZE {
    ;;     for j in 0..U256_SIZE {
    ;;         /* Loop body */
    ;;     }
    ;; }

    ;; Load the `0`th element of the input array into the &quot;%rax&quot;
    ;; register so we can operate on it. The first element is actually
    ;; already in `%rax` at this point but it gets loaded again anyway.
    ;; This is because the `asm!` macro is hiding a lot of details, which
    ;; I'll get to later.
    movq   0x38(%rsp), %rax
    ;; Multiply it with the `0`th element of the output array This operates
    ;; on memory rather than a register, and so is significantly slower than
    ;; if the same operation had been done on a register. Again, I'll get to
    ;; that soon.
    mulq   0x18(%rsp)
    ;; `mulq` multiplies two 64-bit numbers and stores the low and high
    ;; 64 bits of the result in `%rax` and `%rdx`, respectively. We move
    ;; the low bits into `%r8` (the lowest 64 bits of the 512-bit result)
    ;; and the high bits into `%r9` (the second-lowest 64 bits of the
    ;; result).
    movq   %rax, %r8
    movq   %rdx, %r9

    ;; We do the same for `i = 0, j = 1`
    movq   0x38(%rsp), %rax
    mulq   0x10(%rsp)

    ;; Whereas above we moved the values into the output registers, this time
    ;; we have to add the results to the output.
    addq   %rax, %r9

    ;; Here we add 0 because the CPU will use the &quot;carry bit&quot; (whether or not
    ;; the previous addition overflowed) as an additional input. This is
    ;; essentially the same as adding 1 to `rdx` if the previous addition
    ;; overflowed.
    adcq   $0x0, %rdx

    ;; Then we move the upper 64 bits of the multiplication (plus the carry bit
    ;; from the addition) into the third-lowest 64 bits of the output.
    movq   %rdx, %r10

    ;; Then we continue for `j = 2` and `j = 3`
    movq   0x38(%rsp), %rax
    mulq   0x8(%rsp)
    addq   %rax,       %r10
    adcq   $0x0,       %rdx
    movq   %rdx,       %r11
    movq   0x38(%rsp), %rax
    mulq   (%rsp)
    addq   %rax,       %r11
    adcq   $0x0,       %rdx
    movq   %rdx,       %r12

    ;; Then we do the same for `i = 1`, `i = 2` and `i = 3`
    movq   0x30(%rsp), %rax
    mulq   0x18(%rsp)  
    addq   %rax,       %r9
    adcq   %rdx,       %r10
    adcq   $0x0,       %r11
    adcq   $0x0,       %r12

    ;; This `xor` just ensures that `%r13` is zeroed. Again, this is
    ;; non-optimal (we don't need to zero these registers at all) but
    ;; I'll get to that.
    xorq   %r13,       %r13
    adcq   $0x0,       %r13
    xorq   %r14,       %r14
    adcq   $0x0,       %r14
    xorq   %r15,       %r15
    adcq   $0x0,       %r15
    movq   0x30(%rsp), %rax
    mulq   0x10(%rsp)  
    addq   %rax,       %r10
    adcq   %rdx,       %r11
    adcq   $0x0,       %r12
    adcq   $0x0,       %r13
    adcq   $0x0,       %r14
    adcq   $0x0,       %r15
    movq   0x30(%rsp), %rax
    mulq   0x8(%rsp)   
    addq   %rax,       %r11
    adcq   %rdx,       %r12
    adcq   $0x0,       %r13
    adcq   $0x0,       %r14
    adcq   $0x0,       %r15
    movq   0x30(%rsp), %rax
    mulq   (%rsp)      
    addq   %rax,       %r12
    adcq   %rdx,       %r13
    adcq   $0x0,       %r14
    adcq   $0x0,       %r15
    movq   0x28(%rsp), %rax
    mulq   0x18(%rsp)  
    addq   %rax,       %r10
    adcq   %rdx,       %r11
    adcq   $0x0,       %r12
    adcq   $0x0,       %r13
    adcq   $0x0,       %r14
    adcq   $0x0,       %r15
    movq   0x28(%rsp), %rax
    mulq   0x10(%rsp)  
    addq   %rax,       %r11
    adcq   %rdx,       %r12
    adcq   $0x0,       %r13
    adcq   $0x0,       %r14
    adcq   $0x0,       %r15
    movq   0x28(%rsp), %rax
    mulq   0x8(%rsp)   
    addq   %rax,       %r12
    adcq   %rdx,       %r13
    adcq   $0x0,       %r14
    adcq   $0x0,       %r15
    movq   0x28(%rsp), %rax
    mulq   (%rsp)      
    addq   %rax,       %r13
    adcq   %rdx,       %r14
    adcq   $0x0,       %r15
    movq   0x20(%rsp), %rax
    mulq   0x18(%rsp)  
    addq   %rax,       %r11
    adcq   %rdx,       %r12
    adcq   $0x0,       %r13
    adcq   $0x0,       %r14
    adcq   $0x0,       %r15
    movq   0x20(%rsp), %rax
    mulq   0x10(%rsp)  
    addq   %rax,       %r12
    adcq   %rdx,       %r13
    adcq   $0x0,       %r14
    adcq   $0x0,       %r15
    movq   0x20(%rsp), %rax
    mulq   0x8(%rsp)   
    addq   %rax,       %r13
    adcq   %rdx,       %r14
    adcq   $0x0,       %r15
    movq   0x20(%rsp), %rax
    mulq   (%rsp)      
    addq   %rax,       %r14
    adcq   %rdx,       %r15

    ;; Finally, we move everything out of registers so we can
    ;; return it on the stack
    movq   %r8,   (%rdi)
    movq   %r9,   0x8(%rdi)
    movq   %r10,  0x10(%rdi)
    movq   %r11,  0x18(%rdi)
    movq   %r12,  0x20(%rdi)
    movq   %r13,  0x28(%rdi)
    movq   %r14,  0x30(%rdi)
    movq   %r15,  0x38(%rdi)
    movq   %rdi,  %rax
    addq   $0x40, %rsp
    popq   %r12
    popq   %r13
    popq   %r14
    popq   %r15
    retq   &lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So as you can see from my comments, there are a lot of inefficiencies in this code. We multiply on variables from memory instead of from registers, we do superfluous stores and loads, also the CPU has to do many stores and loads before even getting to the “real” code (the multiply-add loop), which is important because although the CPU can do loads and stores in parallel with calculations, the way that this code is written requires it to wait for everything to be loaded before it starts doing calculations. This is because the &lt;code&gt;asm&lt;/code&gt; macro hides a lot of details. Essentially you’re telling the compiler to put the input data wherever it likes, and then to substitute wherever it put the data into your assembly code with string manipulation. The compiler stores everything into registers, but then we instruct it to put the input arrays in memory (with the &lt;code&gt;&quot;m&quot;&lt;/code&gt; before the input parameters) so it loads it back into memory again. There are ways that you could write this code to remove the inefficiencies in it, but it is clearly very difficult for even a seasoned professional to write the correct code here. This code is bug-prone - if you hadn’t zeroed the output registers with the series of &lt;code&gt;xor&lt;/code&gt; instructions then the code would fail sometimes but not always, with seemingly-random values that depended on the calling function’s internal state. It could probably be sped up by replacing &lt;code&gt;&quot;m&quot;&lt;/code&gt; with &lt;code&gt;&quot;r&quot;&lt;/code&gt; here (I hadn’t tested that because I only realised that this is a problem while investigating why the old assembly was so much slower in the course of writing this article), but that’s not clear from reading the source code of the program and only someone with quite in-depth knowledge of LLVM’s assembly syntax would realise that when looking at the code.&lt;/p&gt;
&lt;p&gt;By comparison, the Rust code that uses &lt;code&gt;u128&lt;/code&gt; is about as say-what-you-mean as you can get. Even if your goal was not optimisation you would probably write something similar to it as the simplest solution to the problem, but the code that LLVM produces is very high-quality. You can see already that it’s not too different to our hand-written code, but it addresses some of the issues (commented below) while also including a couple more optimisations that I wouldn’t have even thought of. I couldn’t find any significant optimisations that it missed.&lt;/p&gt;
&lt;p&gt;Here’s the generated assembly:&lt;/p&gt;
&lt;div class=&quot;highlight&quot; readability=&quot;147&quot;&gt;
&lt;pre&gt;
&lt;code class=&quot;language-assembly&quot; data-lang=&quot;assembly&quot;&gt;libbigint.so`disassemble:
    ;; Function prelude
    pushq  %rbp
    movq   %rsp, %rbp
    pushq  %r15
    pushq  %r14
    pushq  %r13
    pushq  %r12
    pushq  %rbx
    subq   $0x48, %rsp

    movq   0x10(%rbp), %r11
    movq   0x18(%rbp), %rsi

    movq   %rsi, -0x38(%rbp)

    ;; I originally thought that this was a missed optimisation,
    ;; but it actually has to do this (instead of doing
    ;; `movq 0x30(%rbp), %rax`) because the `%rax` register gets
    ;; clobbered by the `mulq` below. This means it can multiply
    ;; the first element of the first array by each of the
    ;; elements of th without having to reload it from memory
    ;; like the hand-written assembly does.
    movq   0x30(%rbp), %rcx
    movq   %rcx,       %rax

    ;; LLVM multiplies from a register instead of from memory
    mulq   %r11

    ;; LLVM moves `%rdx` (the upper bits) into a register, since
    ;; we need to operate on it further. It moves `%rax` (the
    ;; lower bits) directly into memory because we don't need
    ;; to do any further work on it. This is better than moving
    ;; in and out of memory like we do in the previous code.
    movq   %rdx, %r9
    movq   %rax, -0x70(%rbp)
    movq   %rcx, %rax
    mulq   %rsi
    movq   %rax, %rbx
    movq   %rdx, %r8

    movq   0x20(%rbp), %rsi
    movq   %rcx,       %rax
    mulq   %rsi

    ;; LLVM uses `%r13` as an intermediate because it needs this
    ;; value in `%r13` later to operate on it anyway.
    movq   %rsi,       %r13
    movq   %r13,       -0x40(%rbp)

    ;; Again, we have to operate on both the low and high bits
    ;; so LLVM moves them both into registers.
    movq   %rax,       %r10
    movq   %rdx,       %r14
    movq   0x28(%rbp), %rdx
    movq   %rdx,       -0x48(%rbp)
    movq   %rcx,       %rax
    mulq   %rdx
    movq   %rax,       %r12
    movq   %rdx,       -0x58(%rbp)
    movq   0x38(%rbp), %r15
    movq   %r15,       %rax
    mulq   %r11
    addq   %r9,        %rbx
    adcq   %r8,        %r10

    ;; These two instructions store the flags into the `%rcx`
    ;; register.
    pushfq 
    popq   %rcx
    addq   %rax, %rbx
    movq   %rbx, -0x68(%rbp)
    adcq   %rdx, %r10

    ;; This stores the flags from the previous calculation into
    ;; `%r8`.
    pushfq 
    popq   %r8

    ;; LLVM takes the flags back out of `%rcx` and then does an
    ;; add including the carry flag. This is smart. It means we
    ;; don't need to do the weird-looking addition of zero since
    ;; we combine the addition of the carry flag and the addition
    ;; of the number's components together into one instruction.
    ;;
    ;; It's possible that the way LLVM does it is faster on modern
    ;; processors, but storing this in `%rcx` is unnecessary,
    ;; because the flags would be at the top of the stack anyway
    ;; (i.e. you could remove the `popq %rcx` above and this
    ;; `pushq %rcx` and it would act the same). If it is slower
    ;; then the difference will be negligible.
    pushq  %rcx
    popfq  
    adcq   %r14, %r12

    pushfq 
    popq   %rax
    movq   %rax,        -0x50(%rbp)
    movq   %r15,        %rax
    movq   -0x38(%rbp), %rsi
    mulq   %rsi
    movq   %rdx,        %rbx
    movq   %rax,        %r9
    addq   %r10,        %r9
    adcq   $0x0,        %rbx
    pushq  %r8
    popfq  
    adcq   $0x0,        %rbx

    ;; `setb` is used instead of explicitly zeroing registers and
    ;; then adding the carry bit. `setb` just sets the byte at the
    ;; given address to 1 if the carry flag is set (since this is
    ;; basically a `mov` it's faster than zeroing and then adding)
    setb   -0x29(%rbp)
    addq   %r12, %rbx

    setb   %r10b
    movq   %r15, %rax
    mulq   %r13
    movq   %rax, %r12
    movq   %rdx, %r8
    movq   0x40(%rbp), %r14
    movq   %r14, %rax
    mulq   %r11
    movq   %rdx, %r13
    movq   %rax, %rcx
    movq   %r14, %rax
    mulq   %rsi
    movq   %rdx, %rsi
    addq   %r9, %rcx
    movq   %rcx, -0x60(%rbp)

    ;; This is essentially a hack to add `%r12` and `%rbx` and store
    ;; the output in `%rcx`. It's one instruction instead of the two
    ;; that would be otherwise required. `leaq` is the take-address-of
    ;; instruction, so this line is essentially the same as if you did
    ;; `&amp;amp;((void*)first)[second]` instead of `first + second` in C. In
    ;; assembly, though, there are no hacks. Every dirty trick is fair
    ;; game.
    leaq   (%r12,%rbx), %rcx

    ;; The rest of the code doesn't have any new tricks, just the same
    ;; ones repeated.
    adcq   %rcx,        %r13
    pushfq 
    popq   %rcx
    addq   %rax,        %r13
    adcq   $0x0,        %rsi
    pushq  %rcx
    popfq  
    adcq   $0x0,        %rsi
    setb   -0x2a(%rbp)
    orb    -0x29(%rbp), %r10b
    addq   %r12,        %rbx
    movzbl %r10b,       %ebx
    adcq   %r8,         %rbx
    setb   %al
    movq   -0x50(%rbp), %rcx
    pushq  %rcx
    popfq  
    adcq   -0x58(%rbp), %rbx
    setb   %r8b
    orb    %al,         %r8b
    movq   %r15,        %rax
    mulq   -0x48(%rbp)
    movq   %rdx,        %r12
    movq   %rax,        %rcx
    addq   %rbx,        %rcx
    movzbl %r8b,        %eax
    adcq   %rax,        %r12
    addq   %rsi,        %rcx
    setb   %r10b
    movq   %r14,        %rax
    mulq   -0x40(%rbp)
    movq   %rax,        %r8
    movq   %rdx,        %rsi
    movq   0x48(%rbp),  %r15
    movq   %r15,        %rax
    mulq   %r11
    movq   %rdx,        %r9
    movq   %rax,        %r11
    movq   %r15,        %rax
    mulq   -0x38(%rbp)
    movq   %rdx,        %rbx
    addq   %r13,        %r11
    leaq   (%r8,%rcx),  %rdx
    adcq   %rdx,        %r9
    pushfq 
    popq   %rdx
    addq   %rax,        %r9
    adcq   $0x0,        %rbx
    pushq  %rdx
    popfq  
    adcq   $0x0,        %rbx
    setb   %r13b
    orb    -0x2a(%rbp), %r10b
    addq   %r8,         %rcx
    movzbl %r10b,       %ecx
    adcq   %rsi,        %rcx
    setb   %al
    addq   %r12,        %rcx
    setb   %r8b
    orb    %al,         %r8b
    movq   %r14,        %rax
    movq   -0x48(%rbp), %r14
    mulq   %r14
    movq   %rdx,        %r10
    movq   %rax,        %rsi
    addq   %rcx,        %rsi
    movzbl %r8b,        %eax
    adcq   %rax,        %r10
    addq   %rbx,        %rsi
    setb   %cl
    orb    %r13b,       %cl
    movq   %r15,        %rax
    mulq   -0x40(%rbp)
    movq   %rdx,        %rbx
    movq   %rax,        %r8
    addq   %rsi,        %r8
    movzbl %cl,         %eax
    adcq   %rax,        %rbx
    setb   %al
    addq   %r10,        %rbx
    setb   %cl
    orb    %al,         %cl
    movq   %r15,        %rax
    mulq   %r14
    addq   %rbx,        %rax
    movzbl %cl,         %ecx
    adcq   %rcx,        %rdx
    movq   -0x70(%rbp), %rcx
    movq   %rcx,        (%rdi)
    movq   -0x68(%rbp), %rcx
    movq   %rcx,        0x8(%rdi)
    movq   -0x60(%rbp), %rcx
    movq   %rcx,        0x10(%rdi)
    movq   %r11,        0x18(%rdi)
    movq   %r9,         0x20(%rdi)
    movq   %r8,         0x28(%rdi)
    movq   %rax,        0x30(%rdi)
    movq   %rdx,        0x38(%rdi)
    movq   %rdi,        %rax
    addq   $0x48,       %rsp
    popq   %rbx
    popq   %r12
    popq   %r13
    popq   %r14
    popq   %r15
    popq   %rbp
    retq   &lt;/code&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Although there are a few more instructions in the LLVM-generated version, the slowest type of instruction (loads and stores) are minimised, it (for the most part) avoids redundant work and it applies many cheeky optimisations on top. The end result is that the code runs significantly faster.&lt;/p&gt;
&lt;p&gt;This is not the first time that a carefully-written Rust implementation has outperformed our assembly code - some months ago I rewrote the Rust implementations of addition and subtraction, making them outperform the assembly implementation by 20% and 15%, respectively. Those didn’t require 128-bit arithmetic to beat the assembly (to get the full power of the hardware in Rust you only need &lt;code&gt;u64::checked_add&lt;/code&gt;/&lt;code&gt;checked_sub&lt;/code&gt;), although who knows - maybe in a future PR we’ll use 128-bit arithmetic and see the speed improve further still.&lt;/p&gt;
&lt;p&gt;You can see the code from this PR &lt;a href=&quot;https://github.com/paritytech/bigint/pull/38&quot;&gt;here&lt;/a&gt; and the code from the addition/subtraction PR &lt;a href=&quot;https://github.com/paritytech/bigint/pull/26&quot;&gt;here&lt;/a&gt;. I should note that although the latter PR shows multiplication already outperforming the assembly implementation, this was actually due to a benchmark that mostly multiplied numbers with 0. Whoops. If there’s something we can learn from that, it’s that there can be no informed optimisation without representative benchmarks.&lt;/p&gt;
&lt;p&gt;My point is not that we should take what we’ve learnt from the LLVM-generated code and write a new version of our hand-rolled assembly. The point is that optimising compilers are &lt;em&gt;really good&lt;/em&gt;. There are very smart people working on them and computers are really good at this kind of optimisation problem (in the mathematic sense) in a way that humans find quite difficult. It’s the job of language designers to give us the tools we need to inform the optimiser as best we can as to what our true intent is, and larger integer sizes are another step towards that. Rust has done a great job of allowing programmers to write programs that are easily understandable by humans and compilers alike, and it’s just that power that has largely driven its success.&lt;/p&gt;
</description>
<pubDate>Mon, 14 May 2018 13:03:12 +0000</pubDate>
<dc:creator>nonsince</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://troubles.md/posts/the-power-of-compilers/</dc:identifier>
</item>
</channel>
</rss>