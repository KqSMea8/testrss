<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Secret Service agent stole 1,600 Bitcoins from Silk Road wallet</title>
<link>https://www.justice.gov/usao-ndca/pr/former-secret-service-agent-sentenced-scheme-related-silk-road-investigation</link>
<guid isPermaLink="true" >https://www.justice.gov/usao-ndca/pr/former-secret-service-agent-sentenced-scheme-related-silk-road-investigation</guid>
<description>&lt;p&gt;SAN FRANCISCO – A former U.S. Secret Service Special Agent, who had been a member of the Baltimore Silk Road Task Force, was sentenced to prison today on charges of money laundering, announced U.S. Attorney Brian J. Stretch, Acting Assistant Attorney General Kenneth A. Blanco of the Justice Department’s Criminal Division, Chief Don Fort of the Internal Revenue Service Criminal Investigation (IRS-CI), Special Agent in Charge John F. Bennett of the FBI’s San Francisco Division, and Special Agent in Charge of the Department of Homeland Security Office of the Inspector General Houston Field Office David Green.&lt;/p&gt;
&lt;p&gt;Shaun W. Bridges, 35, of Laurel, Md., was sentenced to 24 months in prison by U.S. District Judge Richard Seeborg in San Francisco following his earlier guilty plea to one count of money laundering.  Judge Seeborg ordered that the sentence be served consecutively to a previous sentence that Bridges is currently serving.  Bridges was also ordered to forfeit approximately 1,500 bitcoin and other fiat currency which has a current value of approximately $10.4 million.   &lt;/p&gt;
&lt;p&gt;Bridges had been a Special Agent with the U.S. Secret Service for approximately six years in the Baltimore Field Office.  Between 2012 and 2014, he was assigned to the Baltimore Silk Road Task Force, a multi-agency group investigating illegal activity on the Silk Road, a covert online marketplace for illicit goods, including drugs.  Bridges’ responsibilities included, among other things, conducting forensic computer investigations in an effort to locate, identify and prosecute targets of the Baltimore Task Force, including Ross Ulbricht, aka “Dread Pirate Roberts,” who ran the Silk Road from the Northern District of California.  In 2015, Bridges was arrested and taken into custody on charges related to the theft of approximately 1,600 bitcoin from a digital wallet belonging to the U.S. government.  According to admissions made in connection with his guilty plea, Bridges admitted to using a private key to access a digital wallet belonging to the U.S. government, and subsequently transferring the bitcoin to other digital wallets at other bitcoin exchanges to which only he had access.  As part of his plea, Bridges agreed to turn over the stolen bitcoin to U.S. agents.                 &lt;br/&gt;  &lt;br/&gt;The case is being investigated by the FBI’s San Francisco Division, IRS-CI’s Washington, D.C. Field Office Cyber Crimes Unit, and the Department of Homeland Security Office of the Inspector General in Washington D.C.  The case is being prosecuted by Assistant U.S. Attorney William Frentzen and Trial Attorney Richard B. Evans of the Criminal Division’s Public Integrity Section.  Assistant U.S. Attorney David Countryman handled the asset forfeiture aspects of the case.    &lt;br/&gt; &lt;/p&gt;
</description>
<pubDate>Fri, 10 Nov 2017 09:42:42 +0000</pubDate>
<dc:creator>FriedPickles</dc:creator>
<og:title>Former Secret Service Agent Sentenced In Scheme Related To Silk Road Investigation</og:title>
<og:image>https://www.justice.gov/sites/all/modules/features/doj_sharing/images/doj-seal-fb.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.justice.gov/usao-ndca/pr/former-secret-service-agent-sentenced-scheme-related-silk-road-investigation</dc:identifier>
</item>
<item>
<title>Simple React Patterns</title>
<link>http://lucasmreis.github.io/blog/simple-react-patterns/</link>
<guid isPermaLink="true" >http://lucasmreis.github.io/blog/simple-react-patterns/</guid>
<description>&lt;p&gt;Dealing With Side-Effects In React&lt;/p&gt;
&lt;p&gt;I've been writing React applications for a few years now, and I've noticed that some patterns tend to repeat themselves. In this post, I'll review these patterns which will summarize about 99% of the React code I write every day.&lt;/p&gt;
&lt;p&gt;As a sample spec, let's build an app that fetches information about the Dagobah planet from Star Wars API and shows it to the user.&lt;/p&gt;
&lt;h2 id=&quot;simple-everyday-patterns&quot;&gt;Simple everyday patterns&lt;/h2&gt;
&lt;p&gt;About 95% of the code written every day will be either simple view components or components with some logic. This is the first pattern, and it's the easiest one:&lt;/p&gt;
&lt;h3 id=&quot;the-vanilla-or-mixed-pattern&quot;&gt;The Vanilla or Mixed Pattern&lt;/h3&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs js&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;default&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;Dagobah&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;React&lt;/span&gt;.&lt;span class=&quot;hljs-title&quot;&gt;Component&lt;/span&gt; &lt;/span&gt;{
  
  
  
  
  state = { loading: &lt;span class=&quot;hljs-literal&quot;&gt;true&lt;/span&gt; };

  componentDidMount() {
    fetch(&lt;span class=&quot;hljs-string&quot;&gt;&quot;https://swapi.co/api/planets/5&quot;&lt;/span&gt;)
      .then(res =&amp;gt; res.json())
      .then(
        planet =&amp;gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.setState({ loading: &lt;span class=&quot;hljs-literal&quot;&gt;false&lt;/span&gt;, planet }),
        error =&amp;gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.setState({ loading: &lt;span class=&quot;hljs-literal&quot;&gt;false&lt;/span&gt;, error })
      );
  }

  renderLoading() {
    &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;Loading...&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;;&lt;/span&gt;
  }

  renderError() {
    &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;I'm sorry! Please try again.&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;;&lt;/span&gt;
  }

  renderPlanet() {
    &lt;span class=&quot;hljs-keyword&quot;&gt;const&lt;/span&gt; { name, climate, terrain } = &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.state.planet;
    &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; (
      &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;h2&lt;/span&gt;&amp;gt;&lt;/span&gt;{name}&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;h2&lt;/span&gt;&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;Climate: {climate}&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;Terrain: {terrain}&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;
    )&lt;/span&gt;;
  }

  render() {
    &lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.state.loading) {
      &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.renderLoading();
    } &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.state.planet) {
      &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.renderPlanet();
    } &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; {
      &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.renderError();
    }
  }
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is a &lt;em&gt;vanilla&lt;/em&gt; React component, something you will write after reading the docs. Writing a component like this one has its benefits: the main one is that it's easy, and it's self-contained. Plug a &lt;code&gt;&amp;lt;Dagobah /&amp;gt;&lt;/code&gt; anywhere in your application, and it will fetch and render the data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Side note&lt;/strong&gt;: whenever we deal with fetching data from somewhere in a way that may take time or fail, &lt;em&gt;we need to define views for those states&lt;/em&gt;. We always need to define a view for the loading state and a view for the error state. No network is perfect, and we need to prepare our app for problems! You can even define more intricate logic, such as waiting milliseconds before showing the loading view to avoid blinking screens, and so on. This is a great subject, but I won't go further in this blog post. I'll stick to the simple Loading / Error / Success pattern in all the examples.&lt;/p&gt;
&lt;p&gt;So, what problems could this component have? Let's say we want to use a style guide tool like &lt;a href=&quot;https://storybook.js.org/&quot;&gt;Storybook&lt;/a&gt; to render the component in all three states to be able to polish each version well or even showcase it to other teams. Is it possible? What if I want to unit test the view without fetching the data every time, or without mocking the requests? It's not going to happen.&lt;/p&gt;
&lt;p&gt;Both the logic and the view are intertwined in one indivisible component, and that's why I also call this pattern the &lt;em&gt;Mixed Component&lt;/em&gt; pattern. For a better workflow and simpler, more testable and more maintainable code, we need to separate the logic and the view. And that's why this second pattern is probably the most useful, and the one I try to use as much as possible:&lt;/p&gt;
&lt;h3 id=&quot;the-container-view-pattern&quot;&gt;The Container / View Pattern&lt;/h3&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs js&quot;&gt;&lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;PlanetView&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;React&lt;/span&gt;.&lt;span class=&quot;hljs-title&quot;&gt;Component&lt;/span&gt; &lt;/span&gt;{
  renderLoading() {
    &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;Loading...&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;;&lt;/span&gt;
  }

  renderError() {
    &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;I'm sorry! Please try again.&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;;&lt;/span&gt;
  }

  renderPlanet() {
    &lt;span class=&quot;hljs-keyword&quot;&gt;const&lt;/span&gt; { name, climate, terrain } = &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.props.planet;
    &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; (
      &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;h2&lt;/span&gt;&amp;gt;&lt;/span&gt;{name}&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;h2&lt;/span&gt;&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;Climate: {climate}&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;Terrain: {terrain}&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;
    )&lt;/span&gt;;
  }

  render() {
    &lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.props.loading) {
      &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.renderLoading();
    } &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.props.planet) {
      &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.renderPlanet();
    } &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; {
      &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.renderError();
    }
  }
}






&lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;DagobahContainer&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;React&lt;/span&gt;.&lt;span class=&quot;hljs-title&quot;&gt;Component&lt;/span&gt; &lt;/span&gt;{
  state = { loading: &lt;span class=&quot;hljs-literal&quot;&gt;true&lt;/span&gt; };

  componentDidMount() {
    fetch(&lt;span class=&quot;hljs-string&quot;&gt;&quot;https://swapi.co/api/planets/5&quot;&lt;/span&gt;)
      .then(res =&amp;gt; res.json())
      .then(
        planet =&amp;gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.setState({ loading: &lt;span class=&quot;hljs-literal&quot;&gt;false&lt;/span&gt;, planet }),
        error =&amp;gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.setState({ loading: &lt;span class=&quot;hljs-literal&quot;&gt;false&lt;/span&gt;, error })
      );
  }

  render() {
    &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;PlanetView&lt;/span&gt; {&lt;span class=&quot;hljs-attribute&quot;&gt;...this.state&lt;/span&gt;} /&amp;gt;&lt;/span&gt;;&lt;/span&gt;
  }
}

&lt;span class=&quot;hljs-keyword&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;default&lt;/span&gt; DagobahContainer;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;That's it. You've just seen the majority of the code I write today. A simple view-only generic planet component, and a logic-only component, that simply calls the view in its render function.&lt;/p&gt;
&lt;p&gt;With the separated view component, we can very easily use it in a style guide, and fine tune each of the variants just by providing different props. Also, we can easily test the view using &lt;a href=&quot;https://github.com/airbnb/enzyme&quot;&gt;Enzyme&lt;/a&gt; for instance.&lt;/p&gt;
&lt;p&gt;Also, my experience shows that this pattern scales better: maybe one of the view states gets big, and it's straightforward to extract it through a new component. On the logic side, it's also much easier to understand and change code not polluted with view related stuff.&lt;/p&gt;
&lt;p&gt;Notice that the view component has some &quot;if&quot; logic to define what to render. We can extract it into its own component, in what could be considered a variant of the Container / View pattern:&lt;/p&gt;
&lt;h3 id=&quot;the-container-branch-view-pattern&quot;&gt;The Container / Branch / View Pattern&lt;/h3&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs js&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;const&lt;/span&gt; LoadingView = () =&amp;gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;Loading...&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;;&lt;/span&gt;

&lt;span class=&quot;hljs-keyword&quot;&gt;const&lt;/span&gt; ErrorView = () =&amp;gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;I'm sorry! Please try again.&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;;&lt;/span&gt;

&lt;span class=&quot;hljs-keyword&quot;&gt;const&lt;/span&gt; PlanetView = ({ name, climate, terrain }) =&amp;gt; (
  &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;h2&lt;/span&gt;&amp;gt;&lt;/span&gt;{name}&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;h2&lt;/span&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;Climate: {climate}&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;Terrain: {terrain}&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;
)&lt;/span&gt;;

&lt;span class=&quot;hljs-keyword&quot;&gt;const&lt;/span&gt; PlanetBranch = ({ loading, planet }) =&amp;gt; {
  &lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (loading) {
    &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;LoadingView&lt;/span&gt; /&amp;gt;&lt;/span&gt;;&lt;/span&gt;
  } &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (planet) {
    &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;PlanetView&lt;/span&gt; {&lt;span class=&quot;hljs-attribute&quot;&gt;...planet&lt;/span&gt;} /&amp;gt;&lt;/span&gt;;&lt;/span&gt;
  } &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; {
    &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;ErrorView&lt;/span&gt; /&amp;gt;&lt;/span&gt;;&lt;/span&gt;
  }
};






&lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;DagobahContainer&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;React&lt;/span&gt;.&lt;span class=&quot;hljs-title&quot;&gt;Component&lt;/span&gt; &lt;/span&gt;{
  state = { loading: &lt;span class=&quot;hljs-literal&quot;&gt;true&lt;/span&gt; };

  componentDidMount() {
    fetch(&lt;span class=&quot;hljs-string&quot;&gt;&quot;https://swapi.co/api/planets/5&quot;&lt;/span&gt;)
      .then(res =&amp;gt; res.json())
      .then(
        planet =&amp;gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.setState({ loading: &lt;span class=&quot;hljs-literal&quot;&gt;false&lt;/span&gt;, planet }),
        error =&amp;gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.setState({ loading: &lt;span class=&quot;hljs-literal&quot;&gt;false&lt;/span&gt;, error })
      );
  }

  render() {
    &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;PlanetBranch&lt;/span&gt; {&lt;span class=&quot;hljs-attribute&quot;&gt;...this.state&lt;/span&gt;} /&amp;gt;&lt;/span&gt;;&lt;/span&gt;
  }
}

&lt;span class=&quot;hljs-keyword&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;default&lt;/span&gt; DagobahContainer;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Now the individual views are even more isolated, which can help the testing, showcasing and development workflow. Deciding how much to break the view is best done with a case by case analysis, and the rule of thumb is to keep it simple to understand. This can vary a lot, so use your best judgment!&lt;/p&gt;
&lt;p&gt;The only situation that these initial patterns are not useful for is when we need to &lt;em&gt;reuse the logic with different views&lt;/em&gt;. These can be interesting cases, and there are two main ways of dealing with them. Let start with the &quot;oldest&quot; one:&lt;/p&gt;
&lt;h2 id=&quot;higher-order-components&quot;&gt;Higher Order Components&lt;/h2&gt;
&lt;p&gt;Higher-Order Components (HOCs) are simply functions that take at least one component as a parameter and return another component. Usually it adds props to the passed component after doing some work. For instance, we could have a &lt;code&gt;withDagobah&lt;/code&gt; HOC that fetches info about Dagobah and passes the result as a prop:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs js&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;const&lt;/span&gt; withDagobah = PlanetViewComponent =&amp;gt;
  &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;React&lt;/span&gt;.&lt;span class=&quot;hljs-title&quot;&gt;Component&lt;/span&gt; &lt;/span&gt;{
    state = { loading: &lt;span class=&quot;hljs-literal&quot;&gt;true&lt;/span&gt; };

    componentDidMount() {
      fetch(&lt;span class=&quot;hljs-string&quot;&gt;&quot;https://swapi.co/api/planets/5&quot;&lt;/span&gt;)
        .then(res =&amp;gt; res.json())
        .then(
          planet =&amp;gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.setState({ loading: &lt;span class=&quot;hljs-literal&quot;&gt;false&lt;/span&gt;, planet }),
          error =&amp;gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.setState({ loading: &lt;span class=&quot;hljs-literal&quot;&gt;false&lt;/span&gt;, error })
        );
    }

    render() {
      &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;PlanetViewComponent&lt;/span&gt; {&lt;span class=&quot;hljs-attribute&quot;&gt;...this.state&lt;/span&gt;} /&amp;gt;&lt;/span&gt;;&lt;/span&gt;
    }
  };

&lt;span class=&quot;hljs-keyword&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;default&lt;/span&gt; withDagobah(PlanetBranch);&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Now, all the planet fetching logic is inside this HOC, and is &lt;em&gt;not dependent on any view logic&lt;/em&gt;. It does not have any dependency on any particular React views, and it only adds some props to a passed component. That way, you can, for instance, use it in all your routes, with different components rendering planets differently.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; if you use this HOC in two components being rendered on the same screen, it will fetch the data twice. Fetching data is an expensive side effect, and usually, we try to do it as little as possible. I'll talk about how to deal with it in the last pattern of this post, so keep on reading! :)&lt;/p&gt;
&lt;p&gt;A HOC can also accept different parameters to define its behavior. We could have for instance a &lt;code&gt;withPlanet&lt;/code&gt; HOC that fetches different planets:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs js&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;const&lt;/span&gt; hoc = withPlanet(&lt;span class=&quot;hljs-string&quot;&gt;'tatooine'&lt;/span&gt;);
&lt;span class=&quot;hljs-keyword&quot;&gt;const&lt;/span&gt; Tatooine = hoc(PlanetView);


render() {
  &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; (
    &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;Tatooine&lt;/span&gt; /&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;div&lt;/span&gt;&amp;gt;&lt;/span&gt;
  )&lt;/span&gt;;
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;An example of a HOC with this ability is &lt;a href=&quot;https://github.com/ctrlplusb/react-sizeme&quot;&gt;react-sizeme&lt;/a&gt;. It receives an options object and a component and returns another component with a &lt;code&gt;size&lt;/code&gt; prop containing height, width, and position information.&lt;/p&gt;
&lt;p&gt;So, what are the cons of HOCSs? The first painful one is that every view that will be used with the HOC has to understand the shape of the props passed. In our example, we add &lt;code&gt;loading&lt;/code&gt;, &lt;code&gt;error&lt;/code&gt; and &lt;code&gt;planet&lt;/code&gt;, and our views need to be prepared for it. Sometimes we have to have a component whose only purpose is transforming props into the intended ones, and that feels inefficient (interestingly, one of the most used HOCs does not have this problem: &lt;a href=&quot;https://github.com/reactjs/react-redux&quot;&gt;react-redux&lt;/a&gt;'s &lt;code&gt;connect&lt;/code&gt;, because the user decides the shape of the props passed to the view component).&lt;/p&gt;
&lt;p&gt;Some HOCs will always lead to branched views, like our &lt;code&gt;withDagobah&lt;/code&gt; that almost always will be viewed with Loading, Error and Success views. That can give rise to a HOC variant:&lt;/p&gt;
&lt;h3 id=&quot;variation-branching-higher-order-components&quot;&gt;Variation: Branching Higher Order Components&lt;/h3&gt;
&lt;p&gt;We can put the branching logic inside the HOC:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs js&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;const&lt;/span&gt; withDagobah = ({
  LoadingViewComponent,
  ErrorViewComponent,
  PlanetViewComponent
}) =&amp;gt;
  &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;React&lt;/span&gt;.&lt;span class=&quot;hljs-title&quot;&gt;Component&lt;/span&gt; &lt;/span&gt;{
    state = { loading: &lt;span class=&quot;hljs-literal&quot;&gt;true&lt;/span&gt; };

    componentDidMount() {
      fetch(&lt;span class=&quot;hljs-string&quot;&gt;&quot;https://swapi.co/api/planets/5&quot;&lt;/span&gt;)
        .then(res =&amp;gt; res.json())
        .then(
          planet =&amp;gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.setState({ loading: &lt;span class=&quot;hljs-literal&quot;&gt;false&lt;/span&gt;, planet }),
          error =&amp;gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.setState({ loading: &lt;span class=&quot;hljs-literal&quot;&gt;false&lt;/span&gt;, error })
        );
    }

    render() {
      &lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.state.loading) {
        &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;LoadingViewComponent&lt;/span&gt; /&amp;gt;&lt;/span&gt;;&lt;/span&gt;
      } &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.state.planet) {
        &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;PlanetViewComponent&lt;/span&gt; {&lt;span class=&quot;hljs-attribute&quot;&gt;...this.state.planet&lt;/span&gt;} /&amp;gt;&lt;/span&gt;;&lt;/span&gt;
      } &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; {
        &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;ErrorViewComponent&lt;/span&gt; /&amp;gt;&lt;/span&gt;;&lt;/span&gt;
      }
    }
  };


&lt;span class=&quot;hljs-keyword&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;default&lt;/span&gt; withDagobah({
  LoadingViewComponent: LoadingView,
  ErrorViewComponent: ErrorView,
  PlanetViewComponent: PlanetView
});&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;There's a trade-off here: even though the views are simpler, there's more logic inside the HOC. It's only worth it if you know that more than one view is going to be used and that the branching logic will be the same every time. An example of a branching HOC is &lt;a href=&quot;https://github.com/thejameskyle/react-loadable&quot;&gt;react-loadable&lt;/a&gt;, which accepts both a dynamically loaded component and a Loading component that handles both the loading and the error state.&lt;/p&gt;
&lt;h2 id=&quot;render-props&quot;&gt;Render Props&lt;/h2&gt;
&lt;p&gt;There is another widely used pattern that separates the logic from the view, the Render Props (also known as Children as Function). &lt;a href=&quot;https://cdb.reacttraining.com/use-a-render-prop-50de598f11ce&quot;&gt;Some people swear by it&lt;/a&gt;,and some people &lt;a href=&quot;http://americanexpress.io/faccs-are-an-antipattern/&quot;&gt;consider it an anti-pattern&lt;/a&gt;. Opinions aside, this is how our Dagobah logic would be implemented as a Render Prop:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs js&quot;&gt;&lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;DagobahRP&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;React&lt;/span&gt;.&lt;span class=&quot;hljs-title&quot;&gt;Component&lt;/span&gt; &lt;/span&gt;{
  state = { loading: &lt;span class=&quot;hljs-literal&quot;&gt;true&lt;/span&gt; };

  componentDidMount() {
    fetch(&lt;span class=&quot;hljs-string&quot;&gt;&quot;https://swapi.co/api/planets/5&quot;&lt;/span&gt;)
      .then(res =&amp;gt; res.json())
      .then(
        planet =&amp;gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.setState({ loading: &lt;span class=&quot;hljs-literal&quot;&gt;false&lt;/span&gt;, planet }),
        error =&amp;gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.setState({ loading: &lt;span class=&quot;hljs-literal&quot;&gt;false&lt;/span&gt;, error })
      );
  }

  render() {
    &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.props.render(&lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.state);
  }
}


&lt;span class=&quot;hljs-keyword&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;default&lt;/span&gt; () =&amp;gt; (
  &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;DagobahRP&lt;/span&gt;
    &lt;span class=&quot;hljs-attribute&quot;&gt;render&lt;/span&gt;=&lt;span class=&quot;hljs-value&quot;&gt;{({&lt;/span&gt; &lt;span class=&quot;hljs-attribute&quot;&gt;loading&lt;/span&gt;, &lt;span class=&quot;hljs-attribute&quot;&gt;error&lt;/span&gt;, &lt;span class=&quot;hljs-attribute&quot;&gt;planet&lt;/span&gt; }) =&amp;gt;&lt;/span&gt; {
      if (loading) {
        return &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;LoadingView&lt;/span&gt; /&amp;gt;&lt;/span&gt;;&lt;/span&gt;
      } &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (planet) {
        &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;PlanetView&lt;/span&gt; {&lt;span class=&quot;hljs-attribute&quot;&gt;...planet&lt;/span&gt;} /&amp;gt;&lt;/span&gt;;&lt;/span&gt;
      } &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; {
        &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;ErrorView&lt;/span&gt; /&amp;gt;&lt;/span&gt;;&lt;/span&gt;
      }
    }}
  /&amp;gt;
);&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; in the Render Props debate, the performance issue has been raised many times. This post shows how &lt;a href=&quot;https://cdb.reacttraining.com/react-inline-functions-and-performance-bdff784f5578&quot;&gt;it's not a straightforward issue&lt;/a&gt;. Anytime we talk about performance, we should also be talking about measurements. If you have any doubts about performance on a specific issue, load your profilers and measure it! :)&lt;/p&gt;
&lt;p&gt;I tend to feel that the benefits of HOCs versus Render Props vary from situation to situation. At my previous job, we tended to write more Render Props, and at my current job we tend to write more HOCs, and I don't feel those choices made either of the teams more productive, or the code more readable in general. I feel one pattern is better than the other whenever I see it in the code, but as I said, it's on a case by case basis. As always, use your better judgment.&lt;/p&gt;
&lt;p&gt;The first time I saw the Render Props pattern was in the &lt;a href=&quot;https://github.com/chenglou/react-motion&quot;&gt;React Motion library&lt;/a&gt;. &lt;a href=&quot;https://reacttraining.com/react-router/web/api/Route/render-func&quot;&gt;React Router v4&lt;/a&gt; is another large library implementing it. The two authors are probably the most influential render props enthusiasts, and they have a couple of other &lt;a href=&quot;https://reacttraining.com/react-idle/&quot;&gt;small libraries&lt;/a&gt; &lt;a href=&quot;https://reacttraining.com/react-network/&quot;&gt;using it&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Render props can also lead to a lot of branching views code, so I feel I also should register here the Branching Render Props variant:&lt;/p&gt;
&lt;h3 id=&quot;variation-branching-render-props&quot;&gt;Variation: Branching Render Props&lt;/h3&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs js&quot;&gt;&lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;DagobahRP&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;React&lt;/span&gt;.&lt;span class=&quot;hljs-title&quot;&gt;Component&lt;/span&gt; &lt;/span&gt;{
  state = { loading: &lt;span class=&quot;hljs-literal&quot;&gt;true&lt;/span&gt; };

  componentDidMount() {
    fetch(&lt;span class=&quot;hljs-string&quot;&gt;&quot;https://swapi.co/api/planets/5&quot;&lt;/span&gt;)
      .then(res =&amp;gt; res.json())
      .then(
        planet =&amp;gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.setState({ loading: &lt;span class=&quot;hljs-literal&quot;&gt;false&lt;/span&gt;, planet }),
        error =&amp;gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.setState({ loading: &lt;span class=&quot;hljs-literal&quot;&gt;false&lt;/span&gt;, error })
      );
  }

  render() {
    &lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.state.loading) {
      &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.props.renderLoading();
    } &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (&lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.state.planet) {
      &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.props.renderPlanet(&lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.state.planet);
    } &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; {
      &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.props.renderError(&lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.state.error);
    }
  }
}


&lt;span class=&quot;hljs-keyword&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;default&lt;/span&gt; () =&amp;gt; (
  &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;DagobahRP&lt;/span&gt;
    &lt;span class=&quot;hljs-attribute&quot;&gt;renderLoading&lt;/span&gt;=&lt;span class=&quot;hljs-value&quot;&gt;{()&lt;/span&gt; =&amp;gt;&lt;/span&gt; &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;LoadingView&lt;/span&gt; /&amp;gt;&lt;/span&gt;}
    renderError={error =&amp;gt; &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;ErrorView&lt;/span&gt; /&amp;gt;&lt;/span&gt;}
    renderPlanet={planet =&amp;gt; &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;PlanetView&lt;/span&gt; {&lt;span class=&quot;hljs-attribute&quot;&gt;...planet&lt;/span&gt;} /&amp;gt;&lt;/span&gt;}
  /&amp;gt;
)&lt;/span&gt;;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;And that's it.&lt;/p&gt;
&lt;h2 id=&quot;what-if-the-side-effects-are-costly-&quot;&gt;What if the side effects are costly?&lt;/h2&gt;
&lt;p&gt;In a lot of situations, the logic provided by the HOC or Render Prop leads to costly code that we want to avoid if possible. The most common case is fetching data remotely. In our Dagobah case, we would like for instance to fetch the planet data only once, and make it available to view components through HOCs or render props. How would we achieve it?&lt;/p&gt;
&lt;h2 id=&quot;the-provider-pattern&quot;&gt;The Provider Pattern&lt;/h2&gt;
&lt;p&gt;This is one of the most powerful patterns in React. It's relatively simple: you gather your data, put it in the React context object, and then in a HOC (or Render Prop) you access the context object and pass it as a prop to the intended components. If you don't know what the context object is in React, please &lt;a href=&quot;https://reactjs.org/docs/context.html&quot;&gt;head to the official docs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let's implement it for our Dagobah example. First, we need to implement the &lt;code&gt;DagobahProvider&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs js&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; React &lt;span class=&quot;hljs-keyword&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;hljs-string&quot;&gt;&quot;react&quot;&lt;/span&gt;;
&lt;span class=&quot;hljs-keyword&quot;&gt;import&lt;/span&gt; PropTypes &lt;span class=&quot;hljs-keyword&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;hljs-string&quot;&gt;&quot;prop-types&quot;&lt;/span&gt;;



&lt;span class=&quot;hljs-keyword&quot;&gt;const&lt;/span&gt; contextTypes = {
  dagobah: PropTypes.shape({
    loading: PropTypes.bool,
    error: PropTypes.object,
    planet: PropTypes.shape({
      name: PropTypes.string,
      climate: PropTypes.string,
      terrain: PropTypes.string
    })
  })
};

&lt;span class=&quot;hljs-keyword&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;DagobahProvider&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;React&lt;/span&gt;.&lt;span class=&quot;hljs-title&quot;&gt;Component&lt;/span&gt; &lt;/span&gt;{
  state = { loading: &lt;span class=&quot;hljs-literal&quot;&gt;true&lt;/span&gt; };

  componentDidMount() {
    fetch(&lt;span class=&quot;hljs-string&quot;&gt;&quot;https://swapi.co/api/planets/5&quot;&lt;/span&gt;)
      .then(res =&amp;gt; res.json())
      .then(
        planet =&amp;gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.setState({ loading: &lt;span class=&quot;hljs-literal&quot;&gt;false&lt;/span&gt;, planet }),
        error =&amp;gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.setState({ loading: &lt;span class=&quot;hljs-literal&quot;&gt;false&lt;/span&gt;, error })
      );
  }

  static childContextTypes = contextTypes;

  getChildContext() {
    &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; { dagobah: &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.state };
  }

  render() {
    &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;.props.children;
  }
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The provider uses the same logic we had before, and it's handled in the &lt;code&gt;componentDidMount&lt;/code&gt; method. The only difference to the previous implementations is that it adds a &lt;code&gt;dagobah&lt;/code&gt; property to the context object, via the &lt;code&gt;getChildContext&lt;/code&gt; method. Then, it simply renders its children by returning the children props in the render method.&lt;/p&gt;
&lt;p&gt;Now, any component under the provider will have access to the &lt;code&gt;dagobah&lt;/code&gt; object inside the context. But accessing the context object in a component is usually considered bad practice, since the context is kind of an &quot;invisible&quot; input, and it makes testing and reasoning about the code a little bit tougher. Let's implement a HOC to access the context and inject the &lt;code&gt;dagobah&lt;/code&gt; object in a component props:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs js&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;const&lt;/span&gt; withDagobah = PlanetViewComponent =&amp;gt;
  &lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;React&lt;/span&gt;.&lt;span class=&quot;hljs-title&quot;&gt;Component&lt;/span&gt; &lt;/span&gt;{
    static contextTypes = contextTypes;

    render() {
      &lt;span class=&quot;hljs-keyword&quot;&gt;const&lt;/span&gt; { props, context } = &lt;span class=&quot;hljs-keyword&quot;&gt;this&lt;/span&gt;;
      &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;PlanetViewComponent&lt;/span&gt; {&lt;span class=&quot;hljs-attribute&quot;&gt;...props&lt;/span&gt;} {&lt;span class=&quot;hljs-attribute&quot;&gt;...context.dagobah&lt;/span&gt;} /&amp;gt;&lt;/span&gt;;&lt;/span&gt;
    }
  };&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Easy, right? Notice the &lt;code&gt;contextTypes&lt;/code&gt; property: we need it to be defined with the same schema of the provider to be able to access the context. Then, we spread it to the passed component. That way, we can use as many &lt;code&gt;withDagobah&lt;/code&gt; in the same screen, and data will only be fetched once!&lt;/p&gt;
&lt;p&gt;And of course, we could also access the context through a Render Props:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs js&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;const&lt;/span&gt; DagobahRp = ({ render }, { dagobah }) =&amp;gt; render(dagobah);

DagobahRp.contextTypes = contextTypes;&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Very easy too! And this is how we could use it in an application:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;hljs js&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;const&lt;/span&gt; DagobahPlanet = withDagobah(View);

&lt;span class=&quot;hljs-class&quot;&gt;&lt;span class=&quot;hljs-keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;App&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;extends&lt;/span&gt; &lt;span class=&quot;hljs-title&quot;&gt;Component&lt;/span&gt; &lt;/span&gt;{
  render() {
    &lt;span class=&quot;hljs-keyword&quot;&gt;return&lt;/span&gt; (
      &lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;DagobahProvider&lt;/span&gt;&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;DagobahPlanet&lt;/span&gt; /&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;DagobahPlanet&lt;/span&gt; /&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;DagobahPlanet&lt;/span&gt; /&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;DagobahRp&lt;/span&gt; &lt;span class=&quot;hljs-attribute&quot;&gt;render&lt;/span&gt;=&lt;span class=&quot;hljs-value&quot;&gt;{props&lt;/span&gt; =&amp;gt;&lt;/span&gt; &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;View&lt;/span&gt; {&lt;span class=&quot;hljs-attribute&quot;&gt;...props&lt;/span&gt;} /&amp;gt;&lt;/span&gt;} /&amp;gt;
        &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;DagobahRp&lt;/span&gt; &lt;span class=&quot;hljs-attribute&quot;&gt;render&lt;/span&gt;=&lt;span class=&quot;hljs-value&quot;&gt;{props&lt;/span&gt; =&amp;gt;&lt;/span&gt; &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;View&lt;/span&gt; {&lt;span class=&quot;hljs-attribute&quot;&gt;...props&lt;/span&gt;} /&amp;gt;&lt;/span&gt;} /&amp;gt;
        &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;DagobahRp&lt;/span&gt; &lt;span class=&quot;hljs-attribute&quot;&gt;render&lt;/span&gt;=&lt;span class=&quot;hljs-value&quot;&gt;{props&lt;/span&gt; =&amp;gt;&lt;/span&gt; &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;&lt;span class=&quot;hljs-title&quot;&gt;View&lt;/span&gt; {&lt;span class=&quot;hljs-attribute&quot;&gt;...props&lt;/span&gt;} /&amp;gt;&lt;/span&gt;} /&amp;gt;
      &lt;span class=&quot;hljs-tag&quot;&gt;&amp;lt;/&lt;span class=&quot;hljs-title&quot;&gt;DagobahProvider&lt;/span&gt;&amp;gt;&lt;/span&gt;
    )&lt;/span&gt;;
  }
}&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Dagobah is going to be rendered six times, and data will only be fetched once.&lt;/p&gt;
&lt;p&gt;A lot of libraries use the Provider pattern, including the aforementioned &lt;a href=&quot;https://github.com/reactjs/react-redux&quot;&gt;react-redux&lt;/a&gt; and &lt;a href=&quot;https://reacttraining.com/react-router/web/api/Route/render-func&quot;&gt;React Router v4&lt;/a&gt;. &lt;a href=&quot;https://github.com/yahoo/react-intl&quot;&gt;React-intl&lt;/a&gt; is also a good example of the pattern.&lt;/p&gt;
&lt;p&gt;Going back to the percentages, I would say my React code (and most I've come across) is about 99% written using those patterns. The other 1% would be weird integration code with some non-React libraries. Also, most of the main React libraries will fall in one of the categories above! Learn how they work, why they exist and you'll understand most of the React world :)&lt;/p&gt;
&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Regular React components work well most of the time, but it's better to try separating logic from view. If you need to reuse logic for different view components, use HOCs or Render Props. If the logic involves expensive side effects that should only run once, use a provider.&lt;/p&gt;
&lt;p&gt;The code used in this post can &lt;a href=&quot;https://github.com/lucasmreis/react-patterns/tree/master/src/planet&quot;&gt;be found here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;October 8, 2017.&lt;/p&gt;
</description>
<pubDate>Fri, 10 Nov 2017 03:43:52 +0000</pubDate>
<dc:creator>kawera</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>http://lucasmreis.github.io/blog/simple-react-patterns/</dc:identifier>
</item>
<item>
<title>DOJ: Strong encryption that we don’t have access to is “unreasonable”</title>
<link>https://arstechnica.com/tech-policy/2017/11/doj-strong-encryption-that-we-dont-have-access-to-is-unreasonable/</link>
<guid isPermaLink="true" >https://arstechnica.com/tech-policy/2017/11/doj-strong-encryption-that-we-dont-have-access-to-is-unreasonable/</guid>
<description>&lt;img src=&quot;https://cdn.arstechnica.net/wp-content/uploads/2017/11/GettyImages-866330386-800x533.jpg&quot;/&gt;&lt;div class=&quot;caption-text&quot;&gt;&lt;a href=&quot;https://cdn.arstechnica.net/wp-content/uploads/2017/11/GettyImages-866330386.jpg&quot; class=&quot;enlarge-link&quot; data-height=&quot;2000&quot; data-width=&quot;3000&quot;&gt;Enlarge&lt;/a&gt; &lt;span class=&quot;sep&quot;&gt;/&lt;/span&gt; US Deputy Attorney General Rod Rosenstein delivers remarks at the 65th Annual Attorney General's Awards Ceremony at the Daughters of the American Revolution Constitution Hall October 25, 2017 in Washington, DC.&lt;/div&gt;&lt;aside id=&quot;social-left&quot;&gt;&lt;a title=&quot;179 posters participating, including story author.&quot; class=&quot;comment-count icon-comment-bubble-down&quot; href=&quot;https://arstechnica.com/tech-policy/2017/11/doj-strong-encryption-that-we-dont-have-access-to-is-unreasonable/?comments=1&quot;&gt;&lt;span class=&quot;comment-count-before&quot;&gt;reader comments&lt;/span&gt; &lt;span class=&quot;comment-count-number&quot;&gt;273&lt;/span&gt;&lt;/a&gt;
&lt;div class=&quot;share-links&quot;&gt;&lt;span&gt;Share this story&lt;/span&gt;    &lt;/div&gt;
&lt;/aside&gt;&lt;aside class=&quot;pullbox sidebar story-sidebar right&quot;&gt;
&lt;/aside&gt;
Just two days after &lt;a href=&quot;https://arstechnica.com/information-technology/2017/11/fbi-cant-break-the-encryption-on-texas-shooters-smartphone/&quot;&gt;the FBI said&lt;/a&gt; it could not get into the Sutherland Springs shooter's seized iPhone, &lt;a href=&quot;http://politicopro.com/&quot;&gt;Politico Pro&lt;/a&gt; published a lengthy interview with a top Department of Justice official who has become the &quot;government’s unexpected encryption warrior.&quot;
&lt;p&gt;According to the interview, which was summarized and published in transcript form on Thursday for subscribers of the website, Deputy Attorney General Rod Rosenstein indicated that the showdown between the DOJ and Silicon Valley is quietly intensifying.&lt;/p&gt;
&lt;p&gt;&quot;We have an ongoing dialogue with a lot of tech companies in a variety of different areas,&quot; he told Politico Pro. &quot;There's some areas where they are cooperative with us. But on this particular issue of encryption, the tech companies are moving in the opposite direction. They're moving in favor of more and more warrant-proof encryption.&quot;&lt;/p&gt;
&lt;p&gt;While the &lt;a href=&quot;https://arstechnica.com/information-technology/2015/12/what-the-government-shouldve-learned-about-backdoors-from-the-clipper-chip/&quot;&gt;battle against encryption has been going on within federal law enforcement circles&lt;/a&gt; since at least the early 1990s, Rosenstein has been the most outspoken DOJ official on this issue in recent months.&lt;/p&gt;
&lt;aside class=&quot;pullbox sidebar story-sidebar right&quot;&gt;
&lt;/aside&gt;
The DOJ's number two has given &lt;a href=&quot;https://arstechnica.com/tech-policy/2017/10/trumps-doj-tries-to-rebrand-weakened-encryption-as-responsible-encryption/&quot;&gt;multiple public speeches in which he has called for &quot;responsible encryption.&quot;&lt;/a&gt; The interview with Politico Pro represents the clearest articulation of the DOJ’s position on this issue, and it suggests that a redux of the &lt;a href=&quot;https://arstechnica.com/series/apples-encryption-battle/&quot;&gt;2016 FBI v. Apple showdown&lt;/a&gt; is inevitable in the near future.
&lt;p&gt;&quot;I want our prosecutors to know that, if there's a case where they believe they have an appropriate need for information and there is a legal avenue to get it, they should not be reluctant to pursue it,&quot; Rosenstein said. &quot;I wouldn't say we're searching for a case. I'’d say we’re receptive, if a case arises, that we would litigate.&quot;&lt;/p&gt;
&lt;p&gt;What Rosenstein didn't note, however, is that the DOJ and its related agencies, including the FBI, are not taking encryption lying down.&lt;/p&gt;
&lt;p&gt;The FBI maintains an office, known as the &lt;a href=&quot;https://ndcac.fbi.gov/&quot;&gt;National Domestic Communications Assistance Center &lt;/a&gt;(NDCAC), which actively provides technical assistance to local law enforcement in high profile cases.&lt;/p&gt;
&lt;p&gt;In its most recently published minutes from May 2017, the NDCAC said that one of its goals is to make such commercial tools, like Cellebrite's services, &quot;&lt;a href=&quot;https://www.documentcloud.org/documents/4178493-may2017eabmeetingminutesappendices.html#document/p12/a387162&quot;&gt;more widely available&lt;/a&gt;&quot; to state and local law enforcement. Earlier this year, the NDCAC &lt;a href=&quot;https://arstechnica.com/tech-policy/2017/07/with-fbi-and-cellebrites-help-miami-police-bust-into-seized-iphone-6/&quot;&gt;provided&lt;/a&gt; money to Miami authorities to pay Cellebrite to successfully get into a seized iPhone in a local sextortion case.&lt;/p&gt;
&lt;h2&gt;“Unreasonable”&lt;/h2&gt;
&lt;aside class=&quot;pullbox sidebar story-sidebar right&quot;&gt;
&lt;/aside&gt;
In the interview, Rosenstein also said he &quot;favors strong encryption.&quot;
&lt;p&gt;&quot;I favor strong encryption, because the stronger the encryption, the more secure data is against criminals who are trying to commit fraud,&quot; he explained. &quot;And I'm in favor of that, because that means less business for us prosecuting cases of people who have stolen data and hacked into computer networks and done all sorts of damage. So I'm in favor of strong encryption.&quot;&lt;/p&gt;
&lt;p&gt;&quot;This is, obviously, a related issue, but it's distinct, which is, what about cases where people are using electronic media to commit crimes? Having access to those devices is going to be critical to have evidence that we can present in court to prove the crime. I understand why some people merge the issues. I understand that they're related. But I think logically, we have to look at these differently. People want to secure their houses, but they still need to get in and out. Same issue here.&quot;&lt;/p&gt;
&lt;p&gt;He later added that the claim that the &quot;absolutist position&quot; that strong encryption should be by definition, unbreakable, is &quot;unreasonable.&quot;&lt;/p&gt;
&lt;p&gt;&quot;And I think it's necessary to weigh law enforcement equities in appropriate cases against the interest in security,&quot; he said.&lt;/p&gt;
&lt;h2&gt;Poison the well&lt;/h2&gt;
&lt;aside class=&quot;pullbox sidebar story-sidebar right&quot;&gt;
&lt;/aside&gt;
The DOJ's position runs counter to the consensus of information security experts, who say that it is impossible to build the strongest encryption system possible that would also allow the government access under certain conditions.
&lt;p&gt;&quot;Of course, criminals and terrorists have used, are using, and will use encryption to hide their planning from the authorities, just as they will use many aspects of society's capabilities and infrastructure: cars, restaurants, telecommunications,&quot; Bruce Schneier, a well-known cryptographer, &lt;a href=&quot;https://www.documentcloud.org/documents/3866752-Dont-Panic-Making-Progress-on-Going-Dark-Debate.html#document/p25/a387160&quot;&gt;wrote&lt;/a&gt; last year.&lt;/p&gt;
&lt;p&gt;&quot;In general, we recognize that such things can be used by both honest and dishonest people. Society thrives nonetheless because the honest so outnumber the dishonest. Compare this with the tactic of secretly poisoning all the food at a restaurant. Yes, we might get lucky and poison a terrorist before he strikes, but we'll harm all the innocent customers in the process. Weakening encryption for everyone is harmful in exactly the same way.&quot;&lt;/p&gt;
&lt;p&gt;Rosenstein closed his interview by noting that he understands re-engineering encryption to accommodate government may make it weaker.&lt;/p&gt;
&lt;p&gt;&quot;And I think that's a legitimate issue that we can debate—how much risk are we willing to take in return for the reward?&quot; he said.&lt;/p&gt;
&lt;p&gt;&quot;My point is simply that I think somebody needs to consider what's on the other side of the balance. There is a cost to having impregnable security, and we've talked about some of the aspects of that. The cost is that criminals are going to be able to get away with stuff, and that's going to prevent us in law enforcement from holding them accountable.&quot;&lt;/p&gt;
</description>
<pubDate>Fri, 10 Nov 2017 02:02:56 +0000</pubDate>
<dc:creator>nimbius</dc:creator>
<og:url>https://arstechnica.com/tech-policy/2017/11/doj-strong-encryption-that-we-dont-have-access-to-is-unreasonable/</og:url>
<og:title>DOJ: Strong encryption that we don’t have access to is “unreasonable”</og:title>
<og:image>https://cdn.arstechnica.net/wp-content/uploads/2017/11/GettyImages-866330386-760x380.jpg</og:image>
<og:description> Rod Rosenstein: We should weigh “law enforcement equities” against security.</og:description>
<og:type>article</og:type>
<dc:language>en-us</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://arstechnica.com/tech-policy/2017/11/doj-strong-encryption-that-we-dont-have-access-to-is-unreasonable/</dc:identifier>
</item>
<item>
<title>Business questions engineers should ask when interviewing at ML/AI companies</title>
<link>https://medium.com/@danielgross/seven-questions-to-ask-when-interviewing-for-an-ml-job-1963ccee3a19</link>
<guid isPermaLink="true" >https://medium.com/@danielgross/seven-questions-to-ask-when-interviewing-for-an-ml-job-1963ccee3a19</guid>
<description>&lt;div class=&quot;section-inner sectionLayout--insetColumn&quot; readability=&quot;18.813836104513&quot;&gt;

&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*Knk0DDdi6GKU4QUehhn6iA.png&quot; data-width=&quot;1960&quot; data-height=&quot;852&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*Knk0DDdi6GKU4QUehhn6iA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*Knk0DDdi6GKU4QUehhn6iA.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;dafc&quot; id=&quot;dafc&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;A few folks have been asking me if such-and-such would be good AI/ML company to work at. If you’re a data scientist or engineer and are considering a job, here are some interesting questions to ask during the interview. Note: these are focused on the business, not the technology.&lt;/p&gt;
&lt;ol class=&quot;postList&quot;&gt;&lt;li name=&quot;20a2&quot; id=&quot;20a2&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Why does anyone need this?&lt;/strong&gt; Like all advice, this sounds deceptively simple. But make sure you get a &lt;em class=&quot;markup--em markup--li-em&quot;&gt;very&lt;/em&gt; compelling answer here. Many AI companies are a solution-in-search-of-a-problem. Reverse engineering from the technology to the market almost never works.&lt;/li&gt;
&lt;li name=&quot;312d&quot; id=&quot;312d&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;How was this problem being solved before the AI came around?&lt;/strong&gt; Was the pre-AI “manual” solution good enough? Common answer: “we’re replacing humans.” That isn’t enough. Often having a human is &lt;em class=&quot;markup--em markup--li-em&quot;&gt;desirable&lt;/em&gt; (bedside manner, dexterity, perfection a requirement). Often a human is affordable due to margin structure. You’re looking to get a sense that the product provided is something that was never possible before, 10X better, or just-as-good but 10X cheaper. Not 20% cheaper. 10X.&lt;/li&gt;
&lt;li name=&quot;07fc&quot; id=&quot;07fc&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;How many users have you spoken to? What have you learned from them?&lt;/strong&gt; All founders talk to &lt;em class=&quot;markup--em markup--li-em&quot;&gt;some&lt;/em&gt; users, but few talk to &lt;em class=&quot;markup--em markup--li-em&quot;&gt;enough&lt;/em&gt; users. Too often I meet founders who are convinced people will want their solution based on limited data-points. The best founders are endlessly talking to their customers. Importantly, they have intimate knowlege of the &lt;em class=&quot;markup--em markup--li-em&quot;&gt;underlying&lt;/em&gt; &lt;em class=&quot;markup--em markup--li-em&quot;&gt;problems&lt;/em&gt; users are experiencing today (as opposed to their excitement about your product). This expertise is important when building stochastic product (“how much recall/precision do we need to launch?”).&lt;/li&gt;
&lt;li name=&quot;5c3c&quot; id=&quot;5c3c&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;How do you make money?&lt;/strong&gt; Be on the lookout for what I call “multistage rockets”: “Today, we’re doing X. But our grand plan is to do Y, which will be really profitable”. These usually fail.&lt;/li&gt;
&lt;li name=&quot;042a&quot; id=&quot;042a&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;How will you grow? How will anyone find out about you?&lt;/strong&gt; Bad answer: word of mouth. Everyone wants to have a positive &lt;a href=&quot;https://en.wikipedia.org/wiki/K-factor_%28marketing%29&quot; data-href=&quot;https://en.wikipedia.org/wiki/K-factor_(marketing)&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;k-factor&lt;/a&gt;. Sometimes it works out (I’m sure you’d love to be early at Facebook). Making a viral product demands striking gold or possessing incredible artistic finesse about what makes humans tick. Unless you’re seeing either one of those, I’d suggest looking for the time-tested alternative: paid marketing. A great answer includes the cost of acquiring a customer, life-time value of a customer, marketing channels used, etc.&lt;/li&gt;
&lt;li name=&quot;5109&quot; id=&quot;5109&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;How big is this market?&lt;/strong&gt; I suggest this only as a founder-mentality canary test. Are they focused on making a massive company, or doing research? A bad answer is just saying a really big number. “$400B”. A better approach will have a back-of-the-envelope calculation which once multiplied out paints a picture: “We make $10 per customer per month. We think there are about 150,000,000 people in this market, so that’s $18B of annual revenue.”&lt;/li&gt;
&lt;li name=&quot;5477&quot; id=&quot;5477&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;What is defensible about the business?&lt;/strong&gt; Bad answer: an algorithm. In software algorithms are rarely sustainable moats. Google &lt;em class=&quot;markup--em markup--li-em&quot;&gt;got&lt;/em&gt; great because of PageRank, but it &lt;em class=&quot;markup--em markup--li-em&quot;&gt;stayed&lt;/em&gt; great due to network effects.&lt;/li&gt;
&lt;/ol&gt;&lt;p name=&quot;09c8&quot; id=&quot;09c8&quot; class=&quot;graf graf--p graf-after--li graf--trailing&quot;&gt;There many other factors to optimize for, like the &lt;a href=&quot;https://medium.com/@danielgross/the-frequently-overlooked-path-to-happiness-2af7b17e1d34&quot; data-href=&quot;https://medium.com/@danielgross/the-frequently-overlooked-path-to-happiness-2af7b17e1d34&quot; class=&quot;markup--anchor markup--p-anchor&quot; target=&quot;_blank&quot;&gt;people you’ll work with&lt;/a&gt;, the technologies you’ll work on, commute, etc. I hope this is a helpful guide at sizing up the market elements of the decision. I’d be happy to help with any personalized advice. My email is daniel@dcgross.com.&lt;/p&gt;
&lt;/div&gt;
</description>
<pubDate>Fri, 10 Nov 2017 01:37:50 +0000</pubDate>
<dc:creator>danicgross</dc:creator>
<og:title>Business questions engineers should ask when interviewing at ML/AI companies</og:title>
<og:url>https://medium.com/@danielgross/seven-questions-to-ask-when-interviewing-for-an-ml-job-1963ccee3a19</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*Knk0DDdi6GKU4QUehhn6iA.png</og:image>
<og:description>A few folks have been asking me if such-and-such would be good AI/ML company to work at. If you’re a data scientist or engineer and are…</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://medium.com/@danielgross/seven-questions-to-ask-when-interviewing-for-an-ml-job-1963ccee3a19</dc:identifier>
</item>
<item>
<title>Writing a basic x86-64 JIT compiler from scratch in stock Python</title>
<link>https://csl.name/post/python-jit/</link>
<guid isPermaLink="true" >https://csl.name/post/python-jit/</guid>
<description>&lt;p class=&quot;sans-serif author&quot;&gt;&lt;strong&gt;By &lt;a href=&quot;https://csl.name/&quot;&gt;Christian Stigen Larsen&lt;/a&gt;&lt;/strong&gt;&lt;br/&gt;08 Nov 2017&lt;/p&gt;
&lt;p&gt;In this post I'll show how to write a rudimentary, native x86-64 &lt;a href=&quot;https://en.wikipedia.org/wiki/Just-in-time_compilation&quot;&gt;just-in-time compiler (JIT)&lt;/a&gt; in CPython, using only the built-in modules.&lt;/p&gt;
&lt;p&gt;The code here specifically targets the UNIX systems macOS and Linux, but should be easily translated to other systems such as Windows. The complete code is available at &lt;a href=&quot;https://github.com/cslarsen/minijit&quot;&gt;https://github.com/cslarsen/minijit&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The goal is to generate new versions of the below assembly code at runtime and execute it.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;48 b8 ed ef be ad de  movabs $0xdeadbeefed, %rax
00 00 00
48 0f af c7           imul   %rdi,%rax
c3                    retq
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;We will mainly deal with the left hand side — the byte sequence &lt;code&gt;48 b8 ed ...&lt;/code&gt; and so on. Those fifteen &lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_code&quot;&gt;machine code bytes&lt;/a&gt; comprise an x86-64 function that multiplies its argument with the constant &lt;a href=&quot;https://en.wikipedia.org/wiki/Magic_number_(programming)&quot;&gt;&lt;code&gt;0xdeadbeefed&lt;/code&gt;&lt;/a&gt;. The JIT step will create functions with different such constants. While being a contrived form of &lt;a href=&quot;https://en.wikipedia.org/wiki/Run-time_algorithm_specialisation&quot;&gt;specialization&lt;/a&gt;, it illuminates the basic mechanics of just-in-time compilation.&lt;/p&gt;
&lt;p&gt;Our general strategy is to rely on the built-in &lt;a href=&quot;https://docs.python.org/3/library/ctypes.html#module-ctypes&quot;&gt;&lt;code&gt;ctypes&lt;/code&gt;&lt;/a&gt; Python module to load the C standard library. From there, we can access system functions to interface with the virtual memory manager. We'll use &lt;a href=&quot;http://man7.org/linux/man-pages/man2/mmap.2.html&quot;&gt;&lt;code&gt;mmap&lt;/code&gt;&lt;/a&gt; to fetch a page-aligned block of memory. It needs to be aligned for it to become executable. That's the reason why we can't simply use the usual C function &lt;code&gt;malloc&lt;/code&gt;, because it may return memory that spans page boundaries.&lt;/p&gt;
&lt;p&gt;The function &lt;a href=&quot;http://man7.org/linux/man-pages/man2/mprotect.2.html&quot;&gt;&lt;code&gt;mprotect&lt;/code&gt;&lt;/a&gt; will be used to mark the memory block as read-only and executable. After that, we should be able to call into our freshly compiled block of code through ctypes.&lt;/p&gt;
&lt;h2&gt;The boiler-plate part&lt;/h2&gt;
&lt;p&gt;Before we can do anything, we need to load the standard C library.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;import ctypes

if sys.platform.startswith(&quot;darwin&quot;):
    libc = ctypes.cdll.LoadLibrary(&quot;libc.dylib&quot;)
    # ...
elif sys.platform.startswith(&quot;linux&quot;):
    libc = ctypes.cdll.LoadLibrary(&quot;libc.so.6&quot;)
    # ...
else:
    raise RuntimeError(&quot;Unsupported platform&quot;)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;There are other ways to achieve this, for example&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&amp;gt;&amp;gt;&amp;gt; import ctypes
&amp;gt;&amp;gt;&amp;gt; import ctypes.util
&amp;gt;&amp;gt;&amp;gt; libc = ctypes.CDLL(ctypes.util.find_library(&quot;c&quot;))
&amp;gt;&amp;gt;&amp;gt; libc
&amp;lt;CDLL '/usr/lib/libc.dylib', handle 110d466f0 at 103725ad0&amp;gt;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;To find the page size, we'll call &lt;a href=&quot;http://man7.org/linux/man-pages/man3/sysconf.3.html&quot;&gt;&lt;code&gt;sysconf(_SC_PAGESIZE)&lt;/code&gt;&lt;/a&gt;. The &lt;code&gt;_SC_PAGESIZE&lt;/code&gt; constant is 29 on macOS but 30 on Linux. We'll just hard-code those in our program. You can find them by digging into system header files or writing a simple C program that print them out. A more robust and elegant solution would be to use the &lt;a href=&quot;https://github.com/cffi/cffi&quot;&gt;&lt;code&gt;cffi&lt;/code&gt; module&lt;/a&gt; instead of ctypes, because it can automatically parse header files. However, since I wanted to stick to the default CPython distribution, we'll continue using ctypes.&lt;/p&gt;
&lt;p&gt;We need a few additional constants for &lt;code&gt;mmap&lt;/code&gt; and friends. They're just written out below. You may have to look them up for other UNIX variants.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;import ctypes

if sys.platform.startswith(&quot;darwin&quot;):
    libc = ctypes.cdll.LoadLibrary(&quot;libc.dylib&quot;)
    _SC_PAGESIZE = 29
    MAP_ANONYMOUS = 0x1000
    MAP_PRIVATE = 0x0002
    PROT_EXEC = 0x04
    PROT_NONE = 0x00
    PROT_READ = 0x01
    PROT_WRITE = 0x02
    MAP_FAILED = -1 # voidptr actually
elif sys.platform.startswith(&quot;linux&quot;):
    libc = ctypes.cdll.LoadLibrary(&quot;libc.so.6&quot;)
    _SC_PAGESIZE = 30
    MAP_ANONYMOUS = 0x20
    MAP_PRIVATE = 0x0002
    PROT_EXEC = 0x04
    PROT_NONE = 0x00
    PROT_READ = 0x01
    PROT_WRITE = 0x02
    MAP_FAILED = -1 # voidptr actually
else:
    raise RuntimeError(&quot;Unsupported platform&quot;)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Although not strictly required, it is very useful to tell ctypes the signature of the functions we'll use. That way, we'll get exceptions if we mix invalid types. For example&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;# Set up sysconf
sysconf = libc.sysconf
sysconf.argtypes = [ctypes.c_int]
sysconf.restype = ctypes.c_long
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;tells ctypes that &lt;code&gt;sysconf&lt;/code&gt; is a function that takes a single integer and produces a long integer. After this, we can get the current page size with&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;pagesize = sysconf(_SC_PAGESIZE)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The machine code we are going to generate will be interpreted as unsigned 8-bit bytes, so we need to declare a new pointer type:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;# 8-bit unsigned pointer type
c_uint8_p = ctypes.POINTER(ctypes.c_uint8)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Below we just dish out the remaining signatures for the functions that we'll use. For error reporting, it's good to have the &lt;a href=&quot;http://man7.org/linux/man-pages/man3/strerror.3.html&quot;&gt;&lt;code&gt;strerror&lt;/code&gt;&lt;/a&gt; function available. We'll use &lt;a href=&quot;http://man7.org/linux/man-pages/man3/munmap.3p.html&quot;&gt;&lt;code&gt;munmap&lt;/code&gt;&lt;/a&gt; to destroy the machine code block after we're done with it. It lets the operating system reclaim that memory.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;strerror = libc.strerror
strerror.argtypes = [ctypes.c_int]
strerror.restype = ctypes.c_char_p

mmap = libc.mmap
mmap.argtypes = [ctypes.c_void_p,
                 ctypes.c_size_t,
                 ctypes.c_int,
                 ctypes.c_int,
                 ctypes.c_int,
                 # Below is actually off_t, which is 64-bit on macOS
                 ctypes.c_int64]
mmap.restype = c_uint8_p

munmap = libc.munmap
munmap.argtypes = [ctypes.c_void_p, ctypes.c_size_t]
munmap.restype = ctypes.c_int

mprotect = libc.mprotect
mprotect.argtypes = [ctypes.c_void_p, ctypes.c_size_t, ctypes.c_int]
mprotect.restype = ctypes.c_int
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;At this point, it's hard to justify using Python rather than C. With C, we don't need any of the above boiler-plate code. But down the line, Python will allow us to experiment much more easily.&lt;/p&gt;
&lt;p&gt;Now we're ready to write the &lt;code&gt;mmap&lt;/code&gt; wrapper.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;def create_block(size):
    ptr = mmap(0, size, PROT_WRITE | PROT_READ,
            MAP_PRIVATE | MAP_ANONYMOUS, 0, 0)

    if ptr == MAP_FAILED:
        raise RuntimeError(strerror(ctypes.get_errno()))

    return ptr
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This function uses &lt;code&gt;mmap&lt;/code&gt; to allocate page-aligned memory for us. We mark the memory region as readable and writable with the &lt;code&gt;PROT&lt;/code&gt; flags, and we also mark it as private and anonymous. The latter means the memory will not be visible from other processes and that it will not be file-backed. The &lt;a href=&quot;http://man7.org/linux/man-pages/man2/mmap.2.html&quot;&gt;Linux &lt;code&gt;mmap&lt;/code&gt; manual page&lt;/a&gt; covers the details (but be sure to view the man page for your system). If the &lt;code&gt;mmap&lt;/code&gt; call fails, we raise it as a Python error.&lt;/p&gt;
&lt;p&gt;To mark memory as executable,&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;def make_executable(block, size):
    if mprotect(block, size, PROT_READ | PROT_EXEC) != 0:
        raise RuntimeError(strerror(ctypes.get_errno()))
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;With this &lt;code&gt;mprotect&lt;/code&gt; call, we mark the region as readable and executable. If we wanted to, we could have made it writable as well, but some systems will refuse to execute writable memory. This is sometimes called &lt;a href=&quot;https://en.wikipedia.org/wiki/W%5EX&quot;&gt;the W^X security feature&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To destroy the memory block, we'll use&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;def destroy_block(block, size):
    if munmap(block, size) == -1:
        raise RuntimeError(strerror(ctypes.get_errno()))
    del block
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The last &lt;code&gt;del&lt;/code&gt; may be superfluous, or may not work as intended. To be honest, I haven't checked if it will work inside a function. The idea is to let Python decrement the reference count, rendering it unusable.&lt;/p&gt;
&lt;h2&gt;The fun part&lt;/h2&gt;
&lt;p&gt;Now we're finally ready to create an insanely simple piece of JIT code!&lt;/p&gt;
&lt;p&gt;Recall the assembly listing at the top: It's a small function — without a local stack frame — that multiplies an input number with a constant. In Python, we'd write that as&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;def create_multiplication_function(constant):
    return lambda n: n * constant
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This is indeed a contrived example, but qualifies as JIT. After all, we do create native code at runtime and execute it. It's easy to imagine more advanced examples such as JIT-compiling &lt;a href=&quot;https://en.wikipedia.org/wiki/Brainfuck&quot;&gt;Brainfuck&lt;/a&gt; to x86-64 machine code. Or using &lt;a href=&quot;https://en.wikipedia.org/wiki/Advanced_Vector_Extensions&quot;&gt;AVX&lt;/a&gt; instructions for blazing fast, vectorized math ops.&lt;/p&gt;
&lt;p&gt;The disassembly at the top of this post was actually generated by compiling and disassembling the following C code:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;#include &amp;lt;stdint.h&amp;gt;

uint64_t multiply(uint64_t n)
{
  return n*0xdeadbeefedULL;
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;If you want to compile it yourself, use something like&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ gcc -Os -fPIC -shared -fomit-frame-pointer \
    -march=native multiply.c -olibmultiply.so
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Here I optimized for space (&lt;code&gt;-Os&lt;/code&gt;) to generate as little machine code as possible, with position-independent code (&lt;code&gt;-fPIC&lt;/code&gt;) to prevent using absolute jumps, without any frame pointers (&lt;code&gt;-fomit-frame-pointer&lt;/code&gt;) to remove superfluous stack setup code (but it may be required for more advanced functions) and using the current CPU's native instruction set (&lt;code&gt;-march=native&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;We could have passed &lt;code&gt;-S&lt;/code&gt; to produce a disassembly listing, but we're actually interested in the &lt;em&gt;machine code&lt;/em&gt;, so we'll rather use a tool like &lt;code&gt;objdump&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ objdump -d libmultiply.so
...
0000000000000f71 &amp;lt;_multiply&amp;gt;:
 f71:   48 b8 ed ef be ad de    movabs $0xdeadbeefed,%rax
 f78:   00 00 00 
 f7b:   48 0f af c7             imul   %rdi,%rax
 f7f:   c3                      retq
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;In case you are not familiar with assembly, I'll let you know how this function works. First, the &lt;code&gt;movabs&lt;/code&gt; function just puts an &lt;em&gt;immediate&lt;/em&gt; number in the RAX register. &lt;em&gt;Immediate&lt;/em&gt; is assembly-jargon for encoding something right in the machine code. In other words, it's an embedded argument for the &lt;code&gt;movabs&lt;/code&gt; instruction. So RAX now holds the constant &lt;code&gt;0xdeadbeefed&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Also — by &lt;a href=&quot;https://software.intel.com/sites/default/files/article/402129/mpx-linux64-abi.pdf&quot;&gt;AMD64&lt;/a&gt; convention — the first integer argument will be in RDI, and the return value in RAX. So RDI will hold the number to multiply with. That's what &lt;code&gt;imul&lt;/code&gt; does. It multiplies RAX and RDI and puts the result in RAX. Finally, we pop a 64-bit return address off the stack and jump to it with RETQ. At this level, it's easy to imagine how one could implement &lt;a href=&quot;https://en.wikipedia.org/wiki/Continuation-passing_style&quot;&gt;continuation-passing style&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that the the constant &lt;code&gt;0xdeadbeefed&lt;/code&gt; is in little-endian format. We need to remember to do the same when we patch the code. (By the way, a good mnemonic for remembering the word order is that little endian means &quot;little-end first&quot;).&lt;/p&gt;
&lt;p&gt;We are now ready to put everything in a Python function.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;def make_multiplier(block, multiplier):
    # Encoding of: movabs &amp;lt;multiplier&amp;gt;, rax
    block[0] = 0x48
    block[1] = 0xb8

    # Little-endian encoding of multiplication constant
    block[2] = (multiplier &amp;amp; 0x00000000000000ff) &amp;gt;&amp;gt;  0
    block[3] = (multiplier &amp;amp; 0x000000000000ff00) &amp;gt;&amp;gt;  8
    block[4] = (multiplier &amp;amp; 0x0000000000ff0000) &amp;gt;&amp;gt; 16
    block[5] = (multiplier &amp;amp; 0x00000000ff000000) &amp;gt;&amp;gt; 24
    block[6] = (multiplier &amp;amp; 0x000000ff00000000) &amp;gt;&amp;gt; 32
    block[7] = (multiplier &amp;amp; 0x0000ff0000000000) &amp;gt;&amp;gt; 40
    block[8] = (multiplier &amp;amp; 0x00ff000000000000) &amp;gt;&amp;gt; 48
    block[9] = (multiplier &amp;amp; 0xff00000000000000) &amp;gt;&amp;gt; 56

    # Encoding of: imul rdi, rax
    block[10] = 0x48
    block[11] = 0x0f
    block[12] = 0xaf
    block[13] = 0xc7

    # Encoding of: retq
    block[14] = 0xc3

    # Return a ctypes function with the right prototype
    function = ctypes.CFUNCTYPE(ctypes.c_uint64)
    function.restype = ctypes.c_uint64
    return function
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;At the bottom, we return the ctypes function signature to be used with this code. It's somewhat arbitrarily placed, but I thought it was good to keep the signature close to the machine code.&lt;/p&gt;
&lt;h2&gt;The final part&lt;/h2&gt;
&lt;p&gt;Now that we have the basic parts we can weave everything together. The first part is to allocate one page of memory:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;pagesize = sysconf(_SC_PAGESIZE)
block = create_block(pagesize)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Next, we generate the machine code. Let's pick the number 101 to use as a multiplier.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;mul101_signature = make_multiplier(block, 101)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;We now mark the memory region as executable and read-only:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;make_executable(block, pagesize)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Take the address of the first byte in the memory block and cast it to a callable ctypes function with proper signature:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;address = ctypes.cast(block, ctypes.c_void_p).value
mul101 = mul101_signature(address)
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;To get the memory address of the block, we use ctypes to cast it to a void pointer and extract its value. Finally, we instantiate an actual function from this address using the &lt;code&gt;mul101_signature&lt;/code&gt; constructor.&lt;/p&gt;
&lt;p&gt;Voila! We now have a piece of &lt;em&gt;native&lt;/em&gt; code that we can call from Python. If you're in a REPL, you can try it directly:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;&amp;gt;&amp;gt;&amp;gt; print(mul101(8))
808
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Note that this small multiplication function will run slower than a native Python calculation. That's mainly because ctypes, being a foreign-function library, has a lot of overhead: It needs to inspect what dynamic types you pass the function every time you call it, then unbox them, convert them and then do the same with the return value. So the trick is to either use assembly because you have to access some new Intel instruction, or because you compile something like Brainfuck to native code.&lt;/p&gt;
&lt;p&gt;Finally, if you want to, you can let the system reclaim the memory holding the function. Beware that after this, you will probably crash the process if you try calling the code again. So probably best to delete all references in Python as well:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;destroy_block(block, pagesize)

del block
del mul101
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;If you run the code in its complete form from the &lt;a href=&quot;https://github.com/cslarsen/minijit&quot;&gt;GitHub&lt;/a&gt; repository, you can run the &lt;code&gt;mj.py&lt;/code&gt; program and put the multiplication constant on the command line:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;    $ python mj.py 11
    Pagesize: 4096
    Allocating one page of memory
    JIT-compiling a native mul-function w/arg 11
    Making function block executable
    Testing function
    mul(0) = 0
    mul(1) = 11
    mul(2) = 22
    mul(3) = 33
    mul(4) = 44
    mul(5) = 55
    mul(6) = 66
    mul(7) = 77
    mul(8) = 88
    mul(9) = 99
    Deallocating function
&lt;/code&gt;
&lt;/pre&gt;
&lt;h2&gt;Debugging JIT-code&lt;/h2&gt;
&lt;p&gt;If you want to continue learning with this simple program, you'll quickly want to disassemble the machine code you generate. One option is to simply use gdb or lldb, but you need to know where to break. One trick is to just print the hex value of the &lt;code&gt;block&lt;/code&gt; address and then wait for a keystroke:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;print(&quot;address: 0x%x&quot; % address)
print(&quot;Press enter to continue&quot;)
raw_input()
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Then you just run the program in the debugger, break into the debugger while the program is pausing, and disassemble the memory location. Of course you can also step-debug through the assembly code if you want to see what's going on. Here's an example lldb session:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;$ lldb python
...
(lldb) run mj.py 101
...
(lldb) c
Process 19329 resuming
...
address 0x1002fd000
Press ENTER to continue
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;At this point, hit CTRL+C to break back into the debugger, then disassemble from the memory location:&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;(lldb) x/3i 0x1002fd000
    0x1002fd000: 48 b8 65 00 00 00 00 00 00 00  movabsq $0x65, %rax
    0x1002fd00a: 48 0f af c7                    imulq  %rdi, %rax
    0x1002fd00e: c3                             retq
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Notice that 65 hex is 101 in decimal, which was the command line argument we passed above.&lt;/p&gt;
&lt;p&gt;If you're not interested in debugging, but just want a disassembler, I recommend the &lt;a href=&quot;http://www.capstone-engine.org/lang_python.html&quot;&gt;Capstone&lt;/a&gt; module.&lt;/p&gt;
&lt;h2&gt;What's next?&lt;/h2&gt;
&lt;p&gt;A good exercise would be to JIT-compile &lt;a href=&quot;https://en.wikipedia.org/wiki/Brainfuck&quot;&gt;Brainfuck programs&lt;/a&gt; to native code. If you want to jump right in, I've made a GitHub repository at &lt;a href=&quot;https://github.com/cslarsen/brainfuck-jit&quot;&gt;https://github.com/cslarsen/brainfuck-jit&lt;/a&gt;. I even have a &lt;a href=&quot;https://speakerdeck.com/csl/how-to-make-a-simple-virtual-machine&quot;&gt;Speaker Deck presentation&lt;/a&gt; to go with it. It performs JIT-ing and optimizations, but uses GNU Lightning to compile native code instead of this approach. It should be extremely simple to boot out GNU Lightning in favor or some code generation of your own. An interesting note on the Brainfuck project is that if you just JIT-compile each Brainfuck instruction one-by-one, you won't get much of a speed boost, even if you run native code. The entire speed boost is done in the &lt;em&gt;code optimization&lt;/em&gt; stage, where you can bulk up integer operations into one or a few x86 instructions. Another candidate for such compilation would be the &lt;a href=&quot;https://github.com/nornagon/jonesforth/blob/master/jonesforth.S&quot;&gt;Forth language&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Also, before you get serious about expanding this JIT-compiler, take a look at the &lt;a href=&quot;https://github.com/Maratyszcza/PeachPy&quot;&gt;Peachpy project&lt;/a&gt;. It goes way beyond this and includes a disassembler and supports seemingly the entire x86-64 instruction set right up to &lt;a href=&quot;https://en.wikipedia.org/wiki/Advanced_Vector_Extensions&quot;&gt;AVX&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As mentioned, there is a good deal of overhad when using ctypes to call into functions. You can use the &lt;code&gt;cffi&lt;/code&gt; module to overcome some of this, but the fact remains that if you want to call very small JIT-ed functions a large number of times, it's usually faster to just use pure Python.&lt;/p&gt;
&lt;p&gt;What other cool uses are there? I've seen some math libraries in Python that switch to vector operations for higher performance. But I can imagine other fun things as well. For example, tools to compress and decompress native code, access virtualization primitives, sign code and so on. I do know that some &lt;a href=&quot;https://en.wikipedia.org/wiki/Berkeley_Packet_Filter&quot;&gt;BPF&lt;/a&gt; tools and regex modules JIT-compile queries for faster processing.&lt;/p&gt;
&lt;p&gt;What I think is fun about this exercise is to get into deeper territory than pure assembly. One thing that comes to mind is how different instructions are disassembled to the same mnemonic. For example, the RETQ instruction has a different opcode than an ordinary RET, because it operates on 64-bit values. This is something that may not be important when doing assembly programming, because it's a detail that may not always matter, but it's worth being aware of the difference. I saw that gcc, lldb and objdump gave slightly different disassembly listings of the same code for RETQ and MOVABSQ.&lt;/p&gt;
&lt;p&gt;There's another takeaway. I've mentioned that the native Brainfuck compiler I made didn't initially produce very fast code. I had to optimize to get it fast. So things won't go fast just because you use AVX, Cuda or whatever. The cold truth is that gcc contains a vast database of optimizations that you cannot possibly replicate by hand. Felix von Letiner has a &lt;a href=&quot;http://www.fefe.de/source-code-optimization.pdf&quot;&gt;classic talk about source code optimization&lt;/a&gt; that I recommend for more on this.&lt;/p&gt;
</description>
<pubDate>Thu, 09 Nov 2017 20:22:57 +0000</pubDate>
<dc:creator>csl</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://csl.name/post/python-jit/</dc:identifier>
</item>
<item>
<title>Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning</title>
<link>https://blogs.dropbox.com/tech/2017/04/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning/</link>
<guid isPermaLink="true" >https://blogs.dropbox.com/tech/2017/04/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning/</guid>
<description>&lt;p&gt;In this post we will take you behind the scenes on how we built a state-of-the-art Optical Character Recognition (OCR) pipeline for our &lt;a href=&quot;https://blogs.dropbox.com/dropbox/2016/06/new-dropbox-productivity-tools/&quot;&gt;mobile document scanner&lt;/a&gt;. We used computer vision and deep learning advances such as bi-directional Long Short Term Memory (LSTMs), Connectionist Temporal Classification (CTC), convolutional neural nets (CNNs), and more. In addition, we will also dive deep into what it took to actually make our OCR pipeline production-ready at Dropbox scale.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-large&quot; src=&quot;https://dropboxmainblog.files.wordpress.com/2016/08/document-scanning.gif?w=650&amp;amp;h=325&quot; width=&quot;650&quot; height=&quot;325&quot;/&gt;&lt;/p&gt;&lt;noscript&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-large&quot; src=&quot;https://dropboxmainblog.files.wordpress.com/2016/08/document-scanning.gif?w=650&amp;amp;h=325&quot; width=&quot;650&quot; height=&quot;325&quot;/&gt;&lt;/p&gt;
&lt;/noscript&gt;
&lt;p&gt;In &lt;a href=&quot;https://blogs.dropbox.com/tech/tag/doc-scanner/&quot;&gt;previous posts&lt;/a&gt; we have described how Dropbox’s mobile document scanner works. The document scanner makes it possible to use your mobile phone to take photos and &lt;a href=&quot;https://blogs.dropbox.com/dropbox/2016/06/new-dropbox-productivity-tools/&quot;&gt;“&lt;/a&gt;scan” items like receipts and invoices. Our mobile document scanner only outputs an image — any text in the image is just a set of pixels as far as the computer is concerned, and can’t be copy-pasted, searched for, or any of the other things you can do with text.&lt;/p&gt;
&lt;p&gt;Hence the need to apply Optical Character Recognition, or OCR. This process extracts actual text from our doc-scanned image. Once OCR is run, we can then enable the following features for our &lt;a href=&quot;https://www.dropbox.com/business&quot;&gt;Dropbox Business&lt;/a&gt; users:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Extract all the text in scanned documents and index it, so that it can be searched for later&lt;/li&gt;
&lt;li&gt;Create a hidden overlay so text can be copied and pasted from the scans saved as PDFs&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;When we built the first version of the mobile document scanner, we used a commercial off-the-shelf OCR library, in order to do product validation before diving too deep into creating our own machine learning-based OCR system. This meant integrating the commercial system into our scanning pipeline, offering both features above to our business users to see if they found sufficient use from the OCR. Once we confirmed that there was indeed strong user demand for the mobile document scanner and OCR, we decided to build our own in-house OCR system for several reasons.&lt;/p&gt;
&lt;p&gt;First, there was a cost consideration: having our own OCR system would save us significant money as the licensed commercial OCR SDK charged us based on the number of scans. Second, the commercial system was tuned for the traditional OCR world of images from flat bed scanners, whereas our operating scenario was much tougher, because mobile phone photos are far more unconstrained, with crinkled or curved documents, shadows and uneven lighting, blurriness and reflective highlights, etc. Thus, there might be an opportunity for us to improve recognition accuracy.&lt;/p&gt;
&lt;p&gt;In fact, a sea change has happened in the world of computer vision that gave us a unique opportunity. Traditionally, OCR systems were heavily pipelined, with hand-built and highly-tuned modules taking advantage of all kinds of conditions they could assume to be true for images captured using a flatbed scanner. For example, one module might find lines of text, then the next module would find words and segment letters, then another module might apply different techniques to each piece of a character to figure out what the character is, etc. Most methods rely on binarization of the input image as an early stage, and this can be brittle and discards important cues. The process to build these OCR systems was very specialized and labor intensive, and the systems could generally only work with fairly constrained imagery from flat bed scanners.&lt;/p&gt;
&lt;p&gt;The last few years has seen the successful application of deep learning to numerous problems in computer vision that have given us powerful new tools for tackling OCR without having to replicate the complex processing pipelines of the past, relying instead on large quantities of data to have the system automatically learn how to do many of the previously manually-designed steps.&lt;/p&gt;
&lt;p&gt;Perhaps the most important reason for building our own system is that it would give us more control over own destiny, and allow us to work on more innovative features in the future.&lt;/p&gt;
&lt;p&gt;In the rest of this blog post we will take you behind the scenes of how we built this pipeline at Dropbox scale. Most commercial machine learning projects follow three major steps:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Research and prototyping to see if something is possible&lt;/li&gt;
&lt;li&gt;Productionization of the model for actual end users&lt;/li&gt;
&lt;li&gt;Refinement of the system in the real world&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;We will take you through each of these steps in turn.&lt;/p&gt;

&lt;p&gt;Our initial task was to see if we could even build a state of the art OCR system at all.&lt;/p&gt;
&lt;p&gt;We began by collecting a representative set of donated document images that match what users might upload, such as receipts, invoices, letters, etc. To gather this set, we asked a small percentage of users whether they would donate some of their image files for us to improve our algorithms. At Dropbox, we take user privacy very seriously and thus made it clear that this was completely optional, and if donated, the files would be kept private and secure. We use a wide variety of safety precautions with such user-donated data, including never keeping donated data on local machines in permanent storage, maintaining extensive auditing, requiring strong authentication to access any of it, and more.&lt;/p&gt;
&lt;p&gt;Another important, machine learning-specific component for user-donated data is how to label it. Most current machine learning techniques are &lt;a href=&quot;https://en.wikipedia.org/wiki/Supervised_learning&quot;&gt;strongly-supervised&lt;/a&gt;, meaning that they require explicit manual labeling of input data so that the algorithms can learn to make predictions themselves. Traditionally, this labeling is done by outside workers, often using a micro-work platform such as &lt;a href=&quot;https://www.mturk.com/mturk/welcome&quot;&gt;Amazon’s Mechanical Turk&lt;/a&gt; (MTurk). However, a downside to using MTurk is that each item might be seen and labeled by a different worker, and we certainly don’t want to expose user-donated data in the wild like this!&lt;/p&gt;
&lt;p&gt;Thus, our team at Dropbox created our own platform for data annotation, named DropTurk. DropTurk can submit labeling jobs either to MTurk (if we are dealing with public non-user data) or a small pool of hired contractors for user-donated data. These contractors are under a strict non-disclosure agreement (NDA) to ensure that they cannot keep or share any of the data they label. DropTurk contains a standard list of annotation task UI templates that we can rapidly assemble and customize for new datasets and labeling tasks, which enables us to annotate our datasets quite fast.&lt;/p&gt;
&lt;p&gt;For example, here is a DropTurk UI meant to provide ground truth data for individual word images, including one of the following options for the workers to complete:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Transcribing the actual text in an image&lt;/li&gt;
&lt;li&gt;Marking whether the word is oriented incorrectly&lt;/li&gt;
&lt;li&gt;Marking whether it’s a non-English script&lt;/li&gt;
&lt;li&gt;Marking whether it’s unreadable or contains no text&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;img src=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1490394894991_file.png&quot;/&gt;&lt;/p&gt;&lt;noscript&gt;
&lt;p&gt;&lt;img src=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1490394894991_file.png&quot;/&gt;&lt;/p&gt;
&lt;/noscript&gt;
&lt;p&gt;DropTurk UI for adding ground truth data for word images&lt;/p&gt;
&lt;p&gt;Our DropTurk platform includes dashboards to get an overview of past jobs, watch the progress of current jobs, and access the results securely. In addition, we can get analytics to assess workers’ performance, even getting worker-level graphical monitoring of annotations of ongoing jobs to catch potential issues early on:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_210A512EACB734A568AA42939567DF61F69F9A67776880720B996DF23C4CF340_1491511809318_DropTurk-censored.png&quot;/&gt;&lt;/p&gt;&lt;noscript&gt;
&lt;p&gt;&lt;img src=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_210A512EACB734A568AA42939567DF61F69F9A67776880720B996DF23C4CF340_1491511809318_DropTurk-censored.png&quot;/&gt;&lt;/p&gt;
&lt;/noscript&gt;
&lt;p&gt;DropTurk Dashboard&lt;/p&gt;
&lt;p&gt;Using DropTurk, we collected both a word-level dataset, which has images of individual words and their annotated text, as well as a full document-level dataset, which has images of full documents (like receipts) and fully transcribed text. We used the latter to measure the accuracy of existing state-of-the-art OCR systems; this would then inform our efforts by telling us the score we would have to meet or beat for our own system. On this particular dataset, the accuracy percentage we had to achieve was in the mid-90s.&lt;/p&gt;
&lt;p&gt;Our first task was to determine if the OCR problem was even going to be solvable in a reasonable amount of time. So we broke the OCR problem into two pieces. First, we would use computer vision to take an image of a document and segment it into lines and words; we call that the Word Detector. Then, we would take each word and feed it into a deep net to turn the word image into actual text; we call that the Word Deep Net.&lt;/p&gt;
&lt;p&gt;We felt that the Word Detector would be relatively straightforward, and so focused our efforts first on the Word Deep Net, which we were less sure about.&lt;/p&gt;
&lt;h2 id=&quot;word-deep-net&quot;&gt;Word Deep Net&lt;/h2&gt;
&lt;p&gt;The Word Deep Net combines neural network architectures used in computer vision and automatic speech recognition systems. Images of cropped words are fed into a &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;&gt;Convolutional Neural Net (CNN)&lt;/a&gt; with several &lt;a href=&quot;https://en.wikipedia.org/wiki/Convolutional_neural_network&quot;&gt;convolutional layers&lt;/a&gt;. The visual features that are output by the CNN are then fed as a sequence to a &lt;a href=&quot;https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks&quot;&gt;Bidirectional LSTM&lt;/a&gt; (&lt;a href=&quot;https://en.wikipedia.org/wiki/Long_short-term_memory&quot;&gt;Long Short Term Memory&lt;/a&gt;) — common in speech recognition systems — which make sense of our word “pieces,” and finally arrives at a text prediction using a &lt;a href=&quot;http://www.cs.toronto.edu/~graves/icml_2006.pdf&quot;&gt;Connectionist Temporal Classification (CTC)&lt;/a&gt; layer. &lt;a href=&quot;https://arxiv.org/abs/1502.03167&quot;&gt;Batch Normalization&lt;/a&gt; is used where appropriate.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1490396191936_file.png&quot;/&gt;&lt;/p&gt;&lt;noscript&gt;
&lt;p&gt;&lt;img src=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1490396191936_file.png&quot;/&gt;&lt;/p&gt;
&lt;/noscript&gt;
&lt;p&gt;OCR Word Deep Net&lt;/p&gt;
&lt;p&gt;Once we had decided on this network architecture for turning an image of a single word into text, we then needed to figure out how to collect enough data to train it. Deep learning systems typically need huge amounts of training data to achieve good recognition performance; in fact, the amount of training data is often the most significant bottleneck in current systems. Normally, all this data has to be collected and then labeled manually, a time-consuming and expensive process.&lt;/p&gt;
&lt;p&gt;An alternative is to programmatically generate training data. However, in most computer vision problems it’s currently too difficult to generate realistic-enough images for training algorithms: the variety of imaging environments and transformations is too varied to effectively simulate. (One promising area of current research is &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_adversarial_networks&quot;&gt;Generative Adversarial Networks&lt;/a&gt; (GANs), which seem to be well-suited to generating realistic data.) Fortunately, our problem in this case is a perfect match for using synthetic data, since the types of images we need to generate are quite constrained and can thus be rendered automatically. Unlike images of natural or most manmade objects, documents and their text are synthetic and the variability of individual characters is relatively limited.&lt;/p&gt;
&lt;p&gt;Our synthetic data pipeline consists of three pieces:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;A corpus with words to use&lt;/li&gt;
&lt;li&gt;A collection of fonts for drawing the words&lt;/li&gt;
&lt;li&gt;A set of geometric and photometric transformations meant to simulate real world distortions&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;The generation algorithm simply samples from each of these to create a unique training example.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1489522702588_file.png&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1489522702588_file.png&quot;/&gt;&lt;/p&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Synthetically generated word images&lt;/p&gt;
&lt;p&gt;We started simply with all three, with words coming from a collection of &lt;a href=&quot;https://www.gutenberg.org/&quot;&gt;Project Gutenberg&lt;/a&gt; books from the 19th century, about a thousand fonts we collected, and some simple distortions like rotations, underlines, and blurs. We generated about a million synthetic words, trained our deep net, and then tested our accuracy, which was around 79%. That was okay, but not good enough.&lt;/p&gt;
&lt;p&gt;Through many iterations, we evolved each piece of our synthetic data pipeline in many ways to improve the recognition accuracy. Some highlights:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;We noticed that we weren’t doing well on receipts, so we expanded our word corpus to include the &lt;a href=&quot;https://en.wikipedia.org/wiki/Universal_Product_Code&quot;&gt;Uniform Product Code&lt;/a&gt; (UPC) database, which has entries like “24QT TISSUE PAPER” which commonly occur on receipts.&lt;/li&gt;
&lt;li&gt;We noticed the network was struggling with letters with disconnected segments. This revealed a deeper problem: receipts are often printed with thermal fonts that have stippled, disconnected, or ink smudged letters, but our network had only been given training data with smooth continuous fonts (like from a laser printer) or lightly bit-mapped characters (like in a screenshot). To address this shortcoming, we eventually tracked down a font vendor in China who could provide us with representative ancient thermal printer fonts.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;img class=&quot;aligncenter size-large&quot; src=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1489522804692_file.png&quot; width=&quot;686&quot; height=&quot;54&quot;/&gt;&lt;/p&gt;&lt;noscript&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-large&quot; src=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1489522804692_file.png&quot; width=&quot;686&quot; height=&quot;54&quot;/&gt;&lt;/p&gt;
&lt;/noscript&gt;
&lt;p&gt;Synthetically generated words using different thermal printer fonts, common in receipts&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Our font selection procedure was too naive initially. We ended up hand-selecting about 2,000 fonts. Not all fonts are used equally. So, we did research on the top 50 fonts in the world and created a font frequency system that allowed us to sample from common fonts (such as Helvetica or Times New Roman) more frequently, while still retaining a long tail of rare fonts (such as some ornate logo fonts). In addition, we discovered that some fonts have incorrect symbols or limited support, resulting in just squares, or their lower or upper case letters are mismatched and thus incorrect. We had to go through all two thousand fonts by hand and mark those that had invalid symbols, numbers, or casing, so that we didn’t inadvertently train the network with incorrect data.&lt;/li&gt;
&lt;li&gt;The upstream Word Detector (described later) was tuned to provide high recall and low precision. It was overzealous in finding text in images so that it wouldn’t miss any of the actual text (high recall) at the expense of often “finding” words that weren’t actually there (low precision). This meant that the Word Deep Net had to deal with a very large number of essentially empty images with noise. So we had our synthetic data pipeline generate representative negative training examples with empty ground truth strings, including common textured backgrounds, like wood, marble countertops, etc.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;a href=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1489523002348_file.png&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1489523002348_file.png&quot;/&gt;&lt;/p&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Synthetically generated negative training examples&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;From a histogram of the synthetically generated words, we discovered that many symbols were underrepresented, such as / or &amp;amp;. We artificially boosted the frequency of these in our synthetic corpus, by synthetically generating representative dates, prices, URLs, etc.&lt;/li&gt;
&lt;li&gt;We added a large number of visual transformations, such as warping, fake shadows, and fake creases, and much more.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;img class=&quot;aligncenter size-large&quot; src=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1489523118327_file.png&quot; width=&quot;262&quot; height=&quot;52&quot;/&gt;&lt;/p&gt;&lt;noscript&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-large&quot; src=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1489523118327_file.png&quot; width=&quot;262&quot; height=&quot;52&quot;/&gt;&lt;/p&gt;
&lt;/noscript&gt;
&lt;p&gt;Fake shadow effect&lt;/p&gt;
&lt;p&gt;Data is as important as the machine learning model used, so we spent a great deal of time refining this data generation pipeline. At some point, we will open source and release this synthetically generated data for others to train and validate their own systems and research on.&lt;/p&gt;
&lt;p&gt;We trained our network on Amazon EC2 G2 GPU instances, spinning up many experiments in parallel. All of our experiments went into a lab notebook that included everything necessary to replicate experiments so we could track unexpected accuracy bumps or losses.&lt;/p&gt;
&lt;p&gt;Our lab notebook contained numbered experiments, with the most recent experiment first. It tracked everything needed for machine learning &lt;a href=&quot;https://stripe.com/blog/reproducible-research&quot;&gt;reproducibility&lt;/a&gt;, such as a unique git hash for the code that was used, pointers to &lt;a href=&quot;https://aws.amazon.com/s3/&quot;&gt;S3&lt;/a&gt; with generated data sets and results, evaluation results, graphs, a high-level description of the goal of that experiment, and more. As we built our synthetic data pipeline and trained our network, we also built many special purpose tools to visualize fonts, debug network guesses, etc.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1490402580761_file.png&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1490402580761_file.png&quot;/&gt;&lt;/p&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Example early experiment tracking error rate vs. how long our Word Deep Net had trained, against an evaluation dataset that consisted of just single words (Single Word Accuracy)&lt;/p&gt;
&lt;p&gt;Our early experiments tracked how well Word Deep Net did on OCR-ing images of single words, which we called Single Word Accuracy (SWA). Accuracy in this context meant how many of the ground truth words the deep net got right. In addition, we tracked precision and recall for the network. Precision refers to the fraction of words returned by the deep net that were actually correct, while recall refers to the fraction of evaluation data that is correctly predicted by the deep net. There tends to be a tradeoff between precision and recall.&lt;/p&gt;
&lt;p&gt;For example, imagine we have a machine learning model that is designed to classify an email as spam or not. Precision would be whether all the things that were labeled as spam by the classifier, how many were actually spam? Recall, in contrast, would be whether of all the things that truly are spam, how many did we label? It is possible to correctly label spam emails (high precision) while not actually labeling all the true spam emails (low recall).&lt;/p&gt;
&lt;p&gt;Week over week, we tracked how well we were doing. We divided our dataset into different categories, such as &lt;code&gt;register_tapes&lt;/code&gt; (receipts), &lt;code&gt;screenshots&lt;/code&gt;, &lt;code&gt;scanned_docs&lt;/code&gt;, etc., and computed accuracies both individually for each category and overall across all data. For example, the entry below shows early work in our lab notebook for our first full end-to-end test, with a real Word Detector coupled to our real Word Deep Net. You can see that we did pretty terribly at the start:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1490403314138_file.png&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1490403314138_file.png&quot;/&gt;&lt;/p&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Screenshot from early end-to-end experiments in our lab notebook&lt;/p&gt;
&lt;p&gt;At a certain point our synthetic data pipeline was resulting in a Single Word Accuracy (SWA) percentage in the high-80s on our OCR benchmark set, and we decided we were done with that portion. We then collected about 20,000 real images of words (compared to our 1 million synthetically generated words) and used these to fine tune the Word Deep Net. This took us to an SWA in the mid-90s.&lt;/p&gt;
&lt;p&gt;We now had a system that could do very well on individual word images, but of course a real OCR system operates on images of entire documents. Our next step was to focus on the document-level Word Detector.&lt;/p&gt;
&lt;h2 id=&quot;word-detector&quot;&gt;Word Detector&lt;/h2&gt;
&lt;p&gt;For our Word Detector we decided to not use a deep net-based approach. The primary candidates for such approaches were object detection systems, like &lt;a href=&quot;https://github.com/rbgirshick/rcnn&quot;&gt;RCNN&lt;/a&gt;, that try to detect the locations (bounding boxes) of objects like dogs, cats, or plants from images. Most images only have perhaps one to five instances of a given object.&lt;/p&gt;
&lt;p&gt;However, most documents don’t just have a handful of words — they have hundreds or even thousands of them, i.e., a few orders of magnitude more objects than most neural network-based object detection systems were capable of finding at the time. We were thus not sure that such algorithms would scale up to the level our OCR system needed.&lt;/p&gt;
&lt;p&gt;Another important consideration was that traditional computer vision approaches using feature detectors might be easier to debug, as neural networks as notoriously opaque and have internal representations that are hard to understand and interpret.&lt;/p&gt;
&lt;p&gt;We ended up using a classic computer vision approach named &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximally_stable_extremal_regions&quot;&gt;Maximally Stable Extremal Regions&lt;/a&gt; (MSERs), using &lt;a href=&quot;http://opencv.org/&quot;&gt;OpenCV&lt;/a&gt;’s implementation. The MSER algorithm finds connected regions at different thresholds, or levels, of the image. Essentially, they detect blobs in images, and are thus particularly good for text.&lt;/p&gt;
&lt;p&gt;Our Word Detector first detects MSER features in an image, then strings these together into word and line detections. One tricky aspect is that our word deep net accepts fixed size word image inputs. This requires the word detector to thus sometimes include more than one word in a single detection box, or chop a single word in half if it is too long to fit the deep net’s input size. Information on this chopping then has to be propagated through the entire pipeline, so that we can re-assemble it after the deep net has run. Another bit of trickiness is dealing with images with white text on dark backgrounds, as opposed to dark text on white backgrounds, forcing our MSER detector to be able to handle both scenarios.&lt;/p&gt;
&lt;h2 id=&quot;combined-end-to-end-system&quot;&gt;Combined End-to-End System&lt;/h2&gt;
&lt;p&gt;Once we had refined our Word Detector to an acceptable point, we chained it together with our Word Deep Net so that we could benchmark the entire combined system end-to-end against document-level images rather than our older Single Word Accuracy benchmarking suite. However, when we first measured the end-to-end accuracy, we found that we were performing around 44% — quite a bit worse than the competition.&lt;/p&gt;
&lt;p&gt;The primary issues were spacing and spurious garbage text from noise in the image. Sometimes we would incorrectly combine two words, such as “&lt;em&gt;helloworld”&lt;/em&gt;, or incorrectly fragment a single word, such as “&lt;em&gt;wo rld”&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Our solution was to modify the Connectionist Temporal Classification (CTC) layer of the network to also give us a confidence score in addition to the predicted text. We then used this confidence score to bucket predictions in three ways:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;If the confidence was high, we kept the prediction as is.&lt;/li&gt;
&lt;li&gt;If the confidence was low, we simply filtered them out, making a bet that these were noise predictions.&lt;/li&gt;
&lt;li&gt;If the confidence was somewhere in the middle, we then ran it through a lexicon generated from the Oxford English Dictionary, applying different transformations between and within word prediction boxes, attempting to combine words or split them in various ways to see if they were in the lexicon.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;We also had to deal with issues caused by the previously mentioned fixed receptive image size of the Word Deep Net: namely, that a single “word” window might actually contain multiple words or only part of a very long word. We thus run these outputs along with the original outputs from the Word Detector through a module we call the Wordinator, which gives discrete bounding boxes for each individual OCRed word. This results in individual word coordinates along with their OCRed text.&lt;/p&gt;
&lt;p&gt;For example, in the following debug visualization from our system you can see boxes around detected words before the Wordinator:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1490404408762_file.png&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1490404408762_file.png&quot;/&gt;&lt;/p&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The Wordinator will break some of these boxes into individual word coordinate boxes, such as “of” and “Engineering”, which are currently part of the same box.&lt;/p&gt;
&lt;p&gt;Finally, now that we had a fully working end-to-end system, we generated more than ten million synthetic words and trained our neural net for a very large number of iterations to squeeze out as much accuracy as we could. All of this finally gave us the accuracy, precision, and recall numbers that all met or exceeded the OCR state-of-the-art.&lt;/p&gt;
&lt;p&gt;We briefly patted ourselves on the back, then began to prepare for the next tough stage: productionization.&lt;/p&gt;

&lt;p&gt;At this point, we had a collection of prototype Python and Lua scripts wrapping Torch — and a trained model, of course! — that showed that we could achieve state of the art OCR accuracy. However, this is a long way from a system an actual user can use in a distributed setting with reliability, performance, and solid engineering. We needed to create a distributed pipeline suitable for use by millions of users and a system replacing our prototype scripts. In addition, we had to do this without disrupting the existing OCR system using the commercial off the shelf SDK.&lt;/p&gt;
&lt;p&gt;Here’s a diagram of the productionized OCR pipeline:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1491598157110_ocr_system_diagram.png&quot;&gt;
&lt;p&gt;&lt;img src=&quot;https://d2mxuefqeaa7sj.cloudfront.net/s_A5D57EE03480AD99ADF37089497274A382BEAFCAAC54E1A98F6D51E323E2FC30_1491598157110_ocr_system_diagram.png&quot;/&gt;&lt;/p&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Overall Productionized OCR Pipeline&lt;/p&gt;
&lt;p&gt;We started by creating an abstraction for different OCR engines, including our own engine and the commercial one, and gated this using our in-house experiments framework, &lt;a href=&quot;https://blogs.dropbox.com/tech/2017/03/introducing-stormcrow/&quot;&gt;Stormcrow&lt;/a&gt;. This allowed us to introduce the skeleton of our new pipeline without disrupting the existing OCR system, which was already running in production for millions of our Business customers.&lt;/p&gt;
&lt;p&gt;We also ported our Torch based model, including the CTC layer, to &lt;a href=&quot;https://www.tensorflow.org/&quot;&gt;TensorFlow&lt;/a&gt; for a few reasons. First, we’d already standardized on TensorFlow in production to make it easier to manage models and deployments. Second, we prefer to work with Python rather than Lua, and TensorFlow has excellent Python bindings.&lt;/p&gt;
&lt;p&gt;In the new pipeline, mobile clients upload scanned document images to our in-house asynchronous work queue. When the upload is finished, we then send the image via a Remote Procedure Call (RPC) to a cluster of servers running the OCR service.&lt;/p&gt;
&lt;p&gt;The actual OCR service uses OpenCV and TensorFlow, both written in C++ and with complicated library dependencies; so security exploits are a real concern. We’ve isolated the actual OCR portion into jails using technologies like &lt;a href=&quot;https://en.wikipedia.org/wiki/LXC&quot;&gt;LXC&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Cgroups&quot;&gt;CGroups&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Linux_namespaces&quot;&gt;Linux Namespaces&lt;/a&gt;, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Seccomp&quot;&gt;Seccomp&lt;/a&gt; to provide isolation and syscall whitelisting, using IPCs to talk into and out of the isolated container. If someone compromises the jail they will still be completely separated from the rest of our system.&lt;/p&gt;
&lt;p&gt;Our jail infrastructure allows us to efficiently set up expensive resources a single time at startup, such as loading our trained models, then have these resources be cloned into a jail to satisfy a single OCR request. The resources are cloned &lt;a href=&quot;https://en.wikipedia.org/wiki/Copy-on-write&quot;&gt;Copy-on-Write&lt;/a&gt; into the forked jail and are read-only for how we use our models so it’s quite efficient and fast. We had to patch TensorFlow to make it easier to do this kind of forking. (We submitted the patch upstream.)&lt;/p&gt;
&lt;p&gt;Once we get word bounding boxes and their OCRed text, we merge them back into the original PDF produced by the mobile document scanner as an OCR hidden layer. The user thus gets a PDF that has both the scanned image and the detected text. The OCRed text is also added to Dropbox’s search index. The user can now highlight and copy-paste text from the PDF, with the highlights going in the correct place due to our hidden word box coordinates. They can also search for the scanned PDF via its OCRed text on Dropbox.&lt;/p&gt;
&lt;h2 id=&quot;performance-tuning&quot;&gt;Performance tuning&lt;/h2&gt;
&lt;p&gt;At this point, we now had an actual engineering pipeline (with unit tests and continual integration!), but still had performance issues.&lt;/p&gt;
&lt;p&gt;The first question was whether we would use CPUs or GPUs in production at inference time. Training a deep net takes much longer than using it at inference time. It is common to use GPUs during training (as we did), as they vastly decrease the amount of time it takes to train a deep net. However, using GPUs at inference time is a harder call to make currently.&lt;/p&gt;
&lt;p&gt;First, having high-end GPUs in a production data center such as Dropbox’s is still a bit exotic and different than the rest of the fleet. In addition, GPU-based machines are more expensive and configurations are churning faster based on rapid development. We did an extensive analysis of how our Word Detector and Word Deep Net performed on CPUs vs GPUs, assuming full use of all cores on each CPU and the characteristics of the CPU. After much analysis, we decided that we could hit our performance targets on just CPUs at similar or lower costs than with GPU machines.&lt;/p&gt;
&lt;p&gt;Once we decided on CPUs, we then needed to optimize our system &lt;a href=&quot;https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms&quot;&gt;BLAS&lt;/a&gt; libraries for the Word Deep Net, to tune our network a bit, and to configure TensorFlow to use available cores. Our Word Detector was also a significant bottleneck. We ended up essentially rewriting OpenCV’s C++ MSER implementation in a more modular way to avoid duplicating slow work when doing two passes (to be able to handle both black on white text as well as white on black text); to expose more to our Python layer (the underlying MSER tree hierarchy) for more efficient processing; and to make the code actually readable. We also had to optimize the post-MSER Word Detection pipeline to tune and vectorize certain slow portions of it.&lt;/p&gt;
&lt;p&gt;After all this work, we now had a productionized and highly-performant system that we could “shadow turn on” for a small number of users, leading us to the third phase: refinement.&lt;/p&gt;

&lt;p&gt;With our proposed system running silently in production side-by-side with the commercial OCR system, we needed to confirm that our system was truly better, as measured on real user data. We take user data privacy very seriously at Dropbox, so we couldn’t just view and test random mobile document scanned images. Instead, we used the user-image donation flow detailed earlier to get evaluation images. We then used these donated images, being very careful about their privacy, to do a qualitative blackbox test of both OCR systems end-to-end, and were elated to find that we indeed performed the same or better than the older commercial OCR SDK, allowing us to ramp up our system to 100% of &lt;a href=&quot;https://www.dropbox.com/business/landing-t61fl&quot;&gt;Dropbox Business&lt;/a&gt; users.&lt;/p&gt;
&lt;p&gt;Next, we tested whether fine-tuning our trained deep net on these donated documents versus our hand chosen fine tuning image suite helped accuracy. Unfortunately, it didn’t move the needle.&lt;/p&gt;
&lt;p&gt;Another important refinement was doing orientation detection, which we had not done in the original pipeline. Images from the mobile document scanner can be rotated by 90° or even upside down. We built an orientation predictor using another deep net based on the &lt;a href=&quot;https://research.googleblog.com/2016/08/improving-inception-and-image.html&quot;&gt;Inception Resnet v2&lt;/a&gt; architecture, changed the final layer to predict orientation, collected an orientation training and validation data set, and fine-tuned from an &lt;a href=&quot;https://en.wikipedia.org/wiki/ImageNet&quot;&gt;ImageNet&lt;/a&gt;-trained model biased towards our own needs. We put this orientation predictor into our pipeline, using its detected orientation to rotate the image to upright before doing word detection and OCRing.&lt;/p&gt;
&lt;p&gt;One tricky aspect of the orientation predictor was that only a small percentage of images are actually rotated; we needed to make sure our system didn’t inadvertently rotate upright images (the most common case) while trying to fix the orientation for the smaller number of non-upright images. In addition, we had to solve various tricky issues in combining our upright rotated images with the different ways the PDF file format can apply its own transformation matrices for rotation.&lt;/p&gt;
&lt;p&gt;Finally, we were surprised to find some tough issues with the PDF file format containing our scanned OCRed hidden layer in Apple’s native Preview application. Most PDF renderers respect spaces embedded in the text for copy and paste, but Apple’s Preview application performs its own heuristics to determine word boundaries based on text position. This resulted in unacceptable quality for copy and paste from this PDF renderer, causing most spaces to be dropped and all the words to be “glommed together”. We had to do extensive testing across a range of PDF renderers to find the right PDF tricks and workarounds that would solve this problem.&lt;/p&gt;
&lt;p&gt;In all, this entire round of researching, productionization, and refinement took about 8 months, at the end of which we had built and deployed a state-of-the-art OCR pipeline to millions of users using modern computer vision and deep neural network techniques. Our work also provides a solid foundation for future OCR-based products at Dropbox.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;em&gt;Interested in applying the latest machine learning research to hard, real&lt;strong&gt;–&lt;/strong&gt;world problems and shipping to millions of Dropbox users?&lt;/em&gt; &lt;a href=&quot;https://www.dropbox.com/jobs/listing/533100&quot;&gt;&lt;em&gt;Our team is hiring&lt;/em&gt;&lt;/a&gt;&lt;em&gt;!&lt;/em&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 09 Nov 2017 17:16:41 +0000</pubDate>
<dc:creator>wwarner</dc:creator>
<og:title>Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning</og:title>
<og:url>https://blogs.dropbox.com/tech/2017/04/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning/</og:url>
<og:image>https://dropboxtechblog.files.wordpress.com/2017/04/ocr-teaser.png?w=150</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://blogs.dropbox.com/tech/2017/04/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning/</dc:identifier>
</item>
<item>
<title>.IO domain name reliability issues and how we’re working around them</title>
<link>https://getstream.io/blog/stop-using-io-domain-names-for-production-traffic/</link>
<guid isPermaLink="true" >https://getstream.io/blog/stop-using-io-domain-names-for-production-traffic/</guid>
<description>&lt;p&gt;&lt;em&gt;&lt;span&gt;Note: If you’re a user of&lt;/span&gt;&lt;/em&gt; &lt;a href=&quot;https://getstream.io/&quot;&gt;&lt;em&gt;&lt;span&gt;Stream&lt;/span&gt;&lt;/em&gt;&lt;/a&gt;&lt;em&gt;&lt;span&gt;, be sure to update your API client to the latest version for a large improvement in reliability. For those of you on a custom API client, have a look at our updated&lt;/span&gt;&lt;/em&gt; &lt;a href=&quot;https://getstream.io/docs_rest/&quot;&gt;&lt;em&gt;&lt;span&gt;REST documentation&lt;/span&gt;&lt;/em&gt;&lt;/a&gt;&lt;em&gt;&lt;span&gt;.&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;Domain resolution is one of the backbone services of the Internet. It’s something we typically spend very little time thinking about. Of course, that changes when it breaks. Over the past year, IO domain outages have been the number one reason our customers couldn’t use &lt;a href=&quot;https://getstream.io/&quot;&gt;Stream&lt;/a&gt;. Specifically, the outage on September 20th, 2017 turned out to be a major headache. This article will go into the details behind the .IO domain name reliability issues and how we’re working around them.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;The infrastructure of internet Domain Name System (DNS) is large and complex. Due to its centralized and distributed nature, if the problem is with your DNS provider or the broader DNS infrastructure, there is little you can do other than sit and wait for the problem to be resolved. The only practical solution for dealing with DNS outages is to fallback to a backup domain.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;This makes DNS outages quite nasty. Many risks are complex and costly to mitigate and in some scenarios, it is virtually impossible to do so.&lt;/span&gt;&lt;/p&gt;
&lt;h2 class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;What Went Wrong: A Global Outage at the ‘.io’ Top Level Domain&lt;/span&gt;&lt;/h2&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;On September 20th, 2017, our system monitors and health checks started to show intermittent failures. Pings to our website and API servers were failing to resolve “getstream.io” records to a valid hostname.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;Domain name resolution is necessary to access our core API service and dashboard. Without it, clients are not able to find the address to our servers. It goes without saying that this was immediately triaged as critical and received the full attention of our team.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;After an initial investigation, we discovered that resolving any getstream.io record would randomly fail with an incorrect NXDOMAIN error returned. Subsequently, one of our engineers identified that the resolution of .io domains would consistently fail on 2 of the 6 authoritative .io nameservers. The remaining four were operating correctly which explained the apparent random nature of the errors.&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;oembed-gist&quot;&gt;&lt;noscript&gt;
&lt;p&gt;View the code on &lt;a href=&quot;https://gist.github.com/jessienewell/099666e5832fcf4b0224012a87b7411c&quot;&gt;Gist&lt;/a&gt;.&lt;/p&gt;
&lt;/noscript&gt;&lt;/div&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;A bad one looks like the following:&lt;/span&gt;&lt;/p&gt;
&lt;div class=&quot;oembed-gist&quot;&gt;&lt;noscript&gt;
&lt;p&gt;View the code on &lt;a href=&quot;https://gist.github.com/jessienewell/633c842968a7b030bdd229140af4738d&quot;&gt;Gist&lt;/a&gt;.&lt;/p&gt;
&lt;/noscript&gt;&lt;/div&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;Since this happened on the authoritative nameservers, we reached out to our DNS provider and then tried to get in touch with NIC.io as well. To our surprise, we found out that &lt;strong&gt;NIC.io could only be reached via phone between 7 AM to 12 AM UTC Monday through Friday and did not expose any status about the health of the service&lt;/strong&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;In the meantime, we started looking at who else was affected by this outage and posted about it on Twitter and &lt;a href=&quot;https://news.ycombinator.com/item?id=15293578&quot;&gt;&lt;span class=&quot;s2&quot;&gt;Hacker News&lt;/span&gt;&lt;/a&gt;. While waiting for the outage to end, we also increased DNS TTL so that the amount of DNS queries would be as low as possible. Shortly after that, we received a reply from Gandi.net informing us that NIC.io was fixing the problem.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;p3&quot;&gt;&lt;span class=&quot;s1&quot;&gt;The &lt;a href=&quot;http://status.getstream.io/incidents/kgqqpz89lpr0&quot;&gt;&lt;span class=&quot;s2&quot;&gt;outage&lt;/span&gt;&lt;/a&gt; lasted for almost 2 hours, during which 1/5th of DNS queries for any .getstream.io record would fail. For something that sits in front of our service, this is a huge problem and raised a more than a few questions on our end.&lt;/span&gt;&lt;/p&gt;
&lt;h2 class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;Couldn’t This Happen with Any TLD?&lt;/span&gt;&lt;/h2&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;We get it. Sometimes things break. Realistically a similar outage could have occurred to any top level domain.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;Back when we started in 2014 we decided that .io was great from a branding perspective. &lt;a href=&quot;https://getstream.io/&quot;&gt;Stream&lt;/a&gt; is a technical product and our audience is mainly technical, so .io seemed like a great match. Using the same domain for the APIs was more of a consequence than a thoughtful decision.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;p3&quot;&gt;&lt;span class=&quot;s1&quot;&gt;It is impossible to estimate the likelihood of .com nameservers having the same kind of outages as the .io nameservers. One thing that surprised us was that while &lt;strong&gt;about 20% of DNS resolutions for all .io domains were totally broken&lt;/strong&gt;, it was hard to find people complaining about that on Twitter. In fact I believe we were one of the first to tweet this. Had this happened on all .com domains, all news sources would have been on fire.&lt;/span&gt;&lt;/p&gt;
&lt;h2 class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;What Really Went Wrong&lt;/span&gt;&lt;/h2&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;Unfortunately, we found out the hard way that NIC.IO isn’t equipped with the technical support and systems necessary to manage a top-level domain. Being unable to reach them while a major outage was happening is unacceptable.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;Looking further, it does not take a lot of research to find out that the .io TLD team made several mistakes over the past few years. Just to name a few:&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;Searching for &lt;a href=&quot;https://hn.algolia.com/?query=io%20domain&amp;amp;sort=byPopularity&amp;amp;prefix&amp;amp;page=0&amp;amp;dateRange=all&amp;amp;type=story&quot;&gt;&lt;span class=&quot;s2&quot;&gt;.io on HN&lt;/span&gt;&lt;/a&gt; returns a long list of similar outages.&lt;/span&gt;&lt;/p&gt;
&lt;h2 class=&quot;p3&quot;&gt;&lt;span class=&quot;s1&quot;&gt;What Is the Best Immediate Solution?&lt;/span&gt;&lt;/h2&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;Adding a .com domain and using it as the default on all our API clients is clearly the low hanging fruit. Of course we could have the same problem if .com had an outage, however, we are vastly more confident in the management behind .com. It is clear that not only would the issue have been identified earlier, but it also wouldn’t have taken hours for people to acknowledge and remedy the situation.&lt;/span&gt;&lt;/p&gt;
&lt;h2 class=&quot;p3&quot;&gt;&lt;span class=&quot;s1&quot;&gt;Our Roadmap to a DNS Outage-Free Service&lt;/span&gt;&lt;/h2&gt;
&lt;p class=&quot;p3&quot;&gt;&lt;span class=&quot;s1&quot;&gt;These DNS issues caused us to pause and think about all the ways in which a DNS can break.&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;&lt;li class=&quot;p3&quot;&gt;&lt;span class=&quot;s1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;&lt;strong&gt;We lose control of our own domain.&lt;/strong&gt; This can happen in an alarmingly large amount of ways:&lt;/span&gt;&lt;/span&gt;
&lt;/li&gt;
&lt;li class=&quot;p3&quot;&gt;&lt;strong&gt;Route53 outage.&lt;/strong&gt; &lt;span&gt;Since we are delegating getstream.io to Route53 nameservers, an outage on their nameservers would disrupt our service. The&lt;/span&gt; &lt;a href=&quot;https://dyn.com/blog/dyn-statement-on-10212016-ddos-attack/&quot;&gt;&lt;span class=&quot;s2&quot;&gt;DynDNS&lt;/span&gt;&lt;/a&gt; &lt;span&gt;DDoS outage from 2016 is an example.&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;p3&quot;&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;span class=&quot;s1&quot;&gt;&lt;strong&gt;com TLD outage.&lt;/strong&gt;&lt;br/&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;Since we control the API clients, implementing a failover mechanism is easy. Setting up and maintaining a backup domain and/or a backup DNS provider can be very challenging. In the first case, we would need to keep hundreds of DNS records in sync and double our SSL certificates; secondly we would need to only change our infrastructure to not use any Route53 specific feature. For that, we need to keep all DNS records in sync across two different providers and ensure we don’t use any vendor specific feature. As an AWS customer, this is a major challenge as DNS is deeply integrated in many ways.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;Looking forward, our plan is to add a .org domain and find a DNS provider to manage the nameservers.&lt;/span&gt;&lt;/p&gt;
&lt;h2 class=&quot;p1&quot;&gt;&lt;span class=&quot;s1&quot;&gt;Conclusion&lt;/span&gt;&lt;/h2&gt;
&lt;p class=&quot;p2&quot;&gt;&lt;span class=&quot;s1&quot;&gt;In hindsight, using a .IO domain for our core APIs was not a great choice. The outage on September 20th showed how severe the problems and support infrastructure are. Based on our experience we would advise against using a .IO domain name if availability is important.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;p2&quot;&gt;&lt;span class=&quot;s1&quot;&gt;To work around the DNS issue, Stream’s API traffic now runs on a .com domain name. The site still runs on .io since this is harder to change and not as critical in terms of uptime. The &lt;a href=&quot;https://getstream.io/get_started/&quot;&gt;API&lt;/a&gt; powers the feeds for over 200 million end users and we don’t want to take any risk in terms of uptime. To further improve reliability we’re considering:&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;ul1&quot;&gt;&lt;li class=&quot;li2&quot;&gt;&lt;span class=&quot;s1&quot;&gt;Adding a backup .ORG domain name.&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;li2&quot;&gt;&lt;span class=&quot;s1&quot;&gt;Using a backup DNS provider for either the .COM or .ORG domain name.&lt;/span&gt;&lt;/li&gt;
&lt;li class=&quot;li2&quot;&gt;&lt;span class=&quot;s1&quot;&gt;Implementing client-side DNS failover in our SDKs.&lt;br/&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;p class=&quot;p2&quot;&gt;&lt;span class=&quot;s1&quot;&gt;DNS as a whole is one of those things that most take for granted but can easily cause serious downtime and trouble. Using a widely used TLD like .com/.net/.org is the best and easiest way to ensure reliability.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thanks for reading! If you’ve never used Stream before, you’ll enjoy this quick &lt;a href=&quot;https://getstream.io/get_started/&quot;&gt;5 minute tutorial about the API&lt;/a&gt;.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;em&gt;Also published on &lt;a href=&quot;https://medium.com/@tommaso_82444/stop-using-io-domain-names-for-production-traffic-46f89b1f1012&quot;&gt;Medium&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 09 Nov 2017 17:04:06 +0000</pubDate>
<dc:creator>sbierwagen</dc:creator>
<og:type>article</og:type>
<og:title>Why Stream Stopped Using .IO Domain Names for Production Traffic</og:title>
<og:description>Switching from a .io domain name to a .com domain name helped with downtime and improve reliability on our API clients.</og:description>
<og:url>https://getstream.io/blog/stop-using-io-domain-names-for-production-traffic/</og:url>
<og:image>https://getstream-blog.imgix.net/blog/wp-content/uploads/2017/11/stop-io.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://getstream.io/blog/stop-using-io-domain-names-for-production-traffic/</dc:identifier>
</item>
<item>
<title>Low-cost, non-invasive melanoma detector wins award</title>
<link>https://jamesdysonaward.org/news/skan-james-dyson-award-2017-international-winner/</link>
<guid isPermaLink="true" >https://jamesdysonaward.org/news/skan-james-dyson-award-2017-international-winner/</guid>
<description>&lt;p&gt;A team of medical and bioengineering undergraduates from McMaster University, Canada have been chosen as the international James Dyson Award 2017 winners. Their design solution, the sKan, is a low cost and non-invasive melanoma detection device.&lt;/p&gt;
&lt;p&gt;The sKan was chosen as the international winner by James Dyson who says “by using widely available and inexpensive components, the sKan allows for melanoma skin cancer detection to be readily accessible to the many. It’s a very clever device with the potential to save lives around the world. This is why I have selected it at this year’s international winner.”&lt;/p&gt;
&lt;p&gt;Annually, skin cancer accounts for 1 in every 3 cancer diagnoses&lt;sup&gt;1&lt;/sup&gt;. The estimated 5-year survival rate for patients whose melanoma is detected early is approximately 98 percent&lt;sup&gt;2&lt;/sup&gt;. Current melanoma detection methods either rely on a visual inspection, or need a specialist’s opinion which is time consuming and costly. With high numbers of patients needing a rapid diagnosis to begin treatment, the health services are at maximum capacity. The sKan poses a viable solution.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://1dtbef22buhyxzall2snlwn1-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Screen-Shot-2017-11-08-at-9.48.04-AM.png&quot;&gt;&lt;img class=&quot;alignnone size-medium wp-image-342317&quot; src=&quot;https://1dtbef22buhyxzall2snlwn1-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Screen-Shot-2017-11-08-at-9.48.04-AM-300x224.png&quot; alt=&quot;&quot; width=&quot;300&quot; height=&quot;224&quot; srcset=&quot;https://1dtbef22buhyxzall2snlwn1-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Screen-Shot-2017-11-08-at-9.48.04-AM-300x224.png 300w, https://1dtbef22buhyxzall2snlwn1-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Screen-Shot-2017-11-08-at-9.48.04-AM-768x575.png 768w, https://1dtbef22buhyxzall2snlwn1-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Screen-Shot-2017-11-08-at-9.48.04-AM-1024x766.png 1024w, https://1dtbef22buhyxzall2snlwn1-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Screen-Shot-2017-11-08-at-9.48.04-AM-476x356.png 476w, https://1dtbef22buhyxzall2snlwn1-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Screen-Shot-2017-11-08-at-9.48.04-AM-980x733.png 980w, https://1dtbef22buhyxzall2snlwn1-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Screen-Shot-2017-11-08-at-9.48.04-AM-873x653.png 873w, https://1dtbef22buhyxzall2snlwn1-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Screen-Shot-2017-11-08-at-9.48.04-AM.png 1188w&quot; sizes=&quot;(max-width: 300px) 100vw, 300px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Research shows that cancerous cells have a higher metabolic rate than normal tissue cells. When an area of interest on the skin is rapidly cooled, cancerous tissue will regain heat at a faster rate than non-cancerous tissue. The sKan uses accurate and inexpensive temperature sensors to pinpoint areas of tissue that gain heat quicker than the surrounding area of skin. The results of this are displayed as a heat map and temperature difference time plot on using a regular computer. A medical professional can use the quantitative findings produced by the sKan to indicate whether the patient needs to be referred for further investigation or not.&lt;/p&gt;
&lt;p&gt;“We are truly humbled and excited to be given this remarkable opportunity,” says the sKan team. The team plans to use the $40,000 prize money to reiterate and refine their design to ensure it passes the US Food and Drug Administration’s standards.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=BY3DbQxuM5U&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Watch the winners’ video.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The international James Dyson Award runners up are Atropos and Twistlight. Atropos is a 6-axis 3D printing robotic arm that uses continuous fiber composites material, to produce high-performance objects. The designers, Gabriele Natale and Michele Tonizzo, hope to tackle the amount of waste produced by current high performance 3D printing tools. Twistlight, designed by Tina Zimmer, uses LED lights to make veins appear highly contrasted within their surrounding dermal tissue. The light can be used to easily insert needles and catheters into a patient’s skin.  Despite being the most common medical procedure, 33% of first vein puncture attempts fail. Multiple discarded attempts cause patient pain and waste medical materials.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;a href=&quot;http://www.who.int/uv/faq/skincancer/en/index1.html&quot;&gt;http://www.who.int/uv/faq/skincancer/en/index1.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;a href=&quot;http://www.skincancer.org/skin-cancer-information/skin-cancer-facts&quot;&gt;http://www.skincancer.org/skin-cancer-information/skin-cancer-facts&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;sup&gt; &lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;sup&gt; &lt;/sup&gt;&lt;/p&gt;
</description>
<pubDate>Thu, 09 Nov 2017 14:07:20 +0000</pubDate>
<dc:creator>ductionist</dc:creator>
<og:type>article</og:type>
<og:title>The sKan is James Dyson Award 2017 international winner - James Dyson Award</og:title>
<og:description>A team of medical and bioengineering undergraduates from McMaster University, Canada have been chosen as the international James Dyson Award 2017 winners.</og:description>
<og:url>https://jamesdysonaward.org/news/skan-james-dyson-award-2017-international-winner/</og:url>
<og:image>https://jamesdysonaward.org/wp-content/uploads/2017/11/20171030_dyson_jda_1273.jpg</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://jamesdysonaward.org/news/skan-james-dyson-award-2017-international-winner/</dc:identifier>
</item>
<item>
<title>Sean Parker unloads on Facebook “exploiting” human psychology</title>
<link>https://www.axios.com/sean-parker-unloads-on-facebook-2508036343.html</link>
<guid isPermaLink="true" >https://www.axios.com/sean-parker-unloads-on-facebook-2508036343.html</guid>
<description>&lt;div class=&quot;js--toggle-brief animate-keep-reading&quot; readability=&quot;33.602550478215&quot;&gt;
&lt;p&gt;&lt;strong&gt;The leaders made a big show&lt;/strong&gt; of announcing $250 billion in deals, but some, &lt;a href=&quot;https://www.bloomberg.com/news/articles/2017-11-09/boeing-wins-china-orders-for-300-planes-worth-over-37-billion&quot; target=&quot;_blank&quot;&gt;including&lt;/a&gt; Boeing's $37 billion plane order, were repackaged from deals already announced, while others are MOUs that may never come to fruition. The Chinese are always happy to announce big deals during presidential visits as they are flashy, often non-binding, and do nothing to address the structural barriers.&lt;/p&gt;
&lt;p&gt;Sources told me before the trip that the administration's approach to the structural issues was going to be &quot;you know what you need to do&quot; rather than a set of specific asks. Trump delivered that message in his private meetings, and in his public comments on the trade deficit he credited the Chinese and blamed his predecessors:&lt;br/&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&quot;Both the United States and China will have a more prosperous future if we can achieve a level economic playing field. Right now, unfortunately, it is a very one-sided and unfair one. But — but I don't blame China. (Applause.) After all, who can blame a country for being able to take advantage of another country for the benefit of its citizens? I give China great credit&quot;&lt;/li&gt;
&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;&quot;But, in actuality, I do blame past administrations for allowing this out-of-control trade deficit to take place and to grow. We have to fix this because it just doesn't work for our great American companies, and it doesn't work for our great American workers. It is just not sustainable. I look forward to working toward that goal and to pursuing fair and lasting engagement.&quot;&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;There was no announced progress on North Korea&lt;/strong&gt;, just a reiteration that both countries want &quot;a complete, verifiable, and permanent denuclearization of the Korean Peninsula&quot;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bottom line:&lt;/strong&gt; Xi is also not stupid and while he prefers a constructive relationship with the U.S., the Chinese are prepared for and in fact are expecting more friction with the U.S.&lt;br/&gt;&lt;/p&gt;
&lt;p&gt;Sign up for the Axios China newsletter &lt;a href=&quot;http://link.axios.com/join/china-signup?utm_source=nl_launch&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
</description>
<pubDate>Thu, 09 Nov 2017 13:03:32 +0000</pubDate>
<dc:creator>jbuild</dc:creator>
<og:type>article</og:type>
<og:url>https://www.axios.com/sean-parker-unloads-on-facebook-2508036343.html</og:url>
<og:image>https://axios-img.rbl.ms/simage/https%3A%2F%2Fassets.rbl.ms%2F13705169%2F1200x600.jpg/2000%2C2000/BLlkY95AmgsXKLGy/img.jpg</og:image>
<og:title>Sean Parker unloads on Facebook: “God only knows what it's doing to our children's brains”</og:title>
<og:description>Sean Parker, the founding president of Facebook, gave me a candid insider's look at how social networks purposely hook and potentially hurt our brains.</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.axios.com/sean-parker-unloads-on-facebook-2508036343.html</dc:identifier>
</item>
<item>
<title>Skyrim rendered in text</title>
<link>https://medium.com/@filiph/skyrim-rendered-in-text-1899548ab2c4</link>
<guid isPermaLink="true" >https://medium.com/@filiph/skyrim-rendered-in-text-1899548ab2c4</guid>
<description>&lt;p&gt;

&lt;h2 name=&quot;4fc6&quot; id=&quot;4fc6&quot; class=&quot;graf graf--h4 graf-after--h3 graf--subtitle&quot;&gt;Fractal stories, or how to build an open-world text adventure&lt;/h2&gt;
&lt;/p&gt;&lt;div readability=&quot;423.90225118483&quot;&gt;
&lt;p name=&quot;9308&quot; id=&quot;9308&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;I decided some time ago to create a text-based Skyrim. That sounds overly ambitious at first, but as I developed the story and the game’s mechanics, I discovered its basic elements: a sword &amp;amp; sorcery game in a living, simulated world that is presented as a Choose Your Own Adventure (CYOA) book.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*4ua91tHXnmxS9kf0qwW8IQ.png&quot; data-width=&quot;936&quot; data-height=&quot;833&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*4ua91tHXnmxS9kf0qwW8IQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1200/1*4ua91tHXnmxS9kf0qwW8IQ.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;1085&quot; id=&quot;1085&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The idea has already brought a short game into the world, &lt;a href=&quot;https://egamebook.com/vermin&quot; data-href=&quot;https://egamebook.com/vermin&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Insignificant Little Vermin&lt;/a&gt;, which I submitted to this year’s IFCOMP. In this article, I’ll walk you through the process of building that game and talk about what I learned from watching people play it (on Twitch).&lt;/p&gt;
&lt;p name=&quot;a300&quot; id=&quot;a300&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Why Skyrim?&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;bb00&quot; id=&quot;bb00&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Don’t get too hung up on the fact that I’m referring to Skyrim here. Skyrim, for me, is just a sufficiently well-known fantasy open-world videogame that we can use as an example.&lt;/p&gt;
&lt;ol class=&quot;postList&quot;&gt;&lt;li name=&quot;040b&quot; id=&quot;040b&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Sufficiently well-known&lt;/strong&gt; because I want people to immediately have an image in mind.&lt;/li&gt;
&lt;li name=&quot;c488&quot; id=&quot;c488&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Fantasy&lt;/strong&gt; because the combat style in fantasy games is especially well-suited for our needs (more on that later).&lt;/li&gt;
&lt;li name=&quot;cc25&quot; id=&quot;cc25&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Open-world&lt;/strong&gt; because these types of games are a great form of escapism, and therefore a good fit for our needs (more on that later).&lt;/li&gt;
&lt;li name=&quot;5f42&quot; id=&quot;5f42&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Videogame&lt;/strong&gt; because I put emphasis on the play/flow aspect more than on the interactive story (again, more on that later).&lt;/li&gt;
&lt;/ol&gt;&lt;p name=&quot;cf25&quot; id=&quot;cf25&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;The important thing is that we simulate a world that the player can explore. This world is inhabited by actors (monsters, NPCs) and entities (swords, doors, chests, etc.) that the player can interact with. In Skyrim, the world is rendered in 3D, and the player gives low-level commands such as “move forward” or “use weapon in left hand”.&lt;/p&gt;
&lt;p name=&quot;001c&quot; id=&quot;001c&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The goal of the simulation is to entertain the player. That is to say, I’m not using the word “simulation” to suggest that Skyrim tries to “&lt;a href=&quot;https://en.wikipedia.org/wiki/Simulation#Classification_and_terminology&quot; data-href=&quot;https://en.wikipedia.org/wiki/Simulation#Classification_and_terminology&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;imitate real-world processes&lt;/a&gt;” as closely as possible. To me, every videogame that has a steady &lt;a href=&quot;http://gameprogrammingpatterns.com/game-loop.html&quot; data-href=&quot;http://gameprogrammingpatterns.com/game-loop.html&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;update loop&lt;/a&gt; is a simulation of some sort — even if it’s a 2D simulation of an Italian plumber’s journey to save a princess.&lt;/p&gt;
&lt;p name=&quot;bc6e&quot; id=&quot;bc6e&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;First try, a deliberately naive start&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;442d&quot; id=&quot;442d&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The most naive way of porting Skyrim to a text adventure would be to take the world simulation as it exists in the 3D game and play it one frame at a time, describing what’s going on and asking the player for low level input. It would look something like this:&lt;/p&gt;
&lt;blockquote name=&quot;009e&quot; id=&quot;009e&quot; class=&quot;graf graf--blockquote graf-after--p&quot;&gt;
&lt;p&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;Outside of Whiterun&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;92ee&quot; id=&quot;92ee&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot;&gt;
&lt;p&gt;[…]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;465a&quot; id=&quot;465a&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;You stand on coordinates {351.0, 211.9}, facing NNE. Your sword is 12% midswing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;fd65&quot; id=&quot;fd65&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;You see a bandit on coordinates {351.1, 210.8}, facing SSW. His axe is 78% midswing and it missed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;b4d0&quot; id=&quot;b4d0&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;push up arrow&lt;br/&gt; &lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;&amp;gt; push down arrow&lt;br/&gt;&lt;/strong&gt;push left arrow&lt;br/&gt;push right arrow&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;97b3&quot; id=&quot;97b3&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;6&quot;&gt;
&lt;p&gt;You take a step back to coordinates {354.9, 212.5}. […]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;065f&quot; id=&quot;065f&quot; class=&quot;graf graf--p graf-after--blockquote&quot;&gt;As you can imagine, the gameplay and story experience here would absolutely suck. And no matter how you tweak the output, the input, or the length of each frame represented in text, it would still suck.&lt;/p&gt;
&lt;p name=&quot;21ce&quot; id=&quot;21ce&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Second try&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;2c54&quot; id=&quot;2c54&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;One way to solve this is by increasing the level of abstraction. We still use Skyrim’s world simulation, but only on the highest level — the map, distances between objects and places, the location of actors, NPCs, where to spawn monsters, etc. That gives us something like this:&lt;/p&gt;
&lt;blockquote name=&quot;44b7&quot; id=&quot;44b7&quot; class=&quot;graf graf--blockquote graf-after--p&quot;&gt;
&lt;p&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;Outside of Whiterun&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;cbca&quot; id=&quot;cbca&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;You arrive at Whiterun from the south. There’s a solitary bandit waiting for you, axe in hand and ready to fight.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;64da&quot; id=&quot;64da&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot;&gt;
&lt;p&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;&amp;gt; kill bandit&lt;br/&gt;&lt;/strong&gt;run&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;e9ae&quot; id=&quot;e9ae&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;You raise your sword and charge the bandit. &amp;lt;Insert interesting description of a fight.&amp;gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;9e03&quot; id=&quot;9e03&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;That last swing connects with the bandit’s throat and he falls to the ground. As you kneel next to him and try taking his gold, an arrow slams into the ground less than an inch from your knee. You jump up and catch sight of an archer on a knoll nearby.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;bc65&quot; id=&quot;bc65&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;&amp;gt; kill archer&lt;br/&gt;&lt;/strong&gt;search bandit&lt;br/&gt;run&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;cd6f&quot; id=&quot;cd6f&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;&amp;lt;Insert another interesting fight description.&amp;gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;12e2&quot; id=&quot;12e2&quot; class=&quot;graf graf--p graf-after--blockquote&quot;&gt;Now, the gameplay here is a bit better, and this kind of experience could even be fun. The player could go to the different places in the simulated world, talk to people, quest for them, and experience the Skyrim storyline.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*9gHq7Zf_YPfHe8teQM0LGw.jpeg&quot; data-width=&quot;3732&quot; data-height=&quot;1572&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*9gHq7Zf_YPfHe8teQM0LGw.jpeg&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*9gHq7Zf_YPfHe8teQM0LGw.jpeg&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;6e8d&quot; id=&quot;6e8d&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;But let’s face it, combat is a big part of what makes Skyrim fun. So “kill bandit, kill archer” doesn’t quite cut it. At best, with really interesting descriptions of the fights, you’ll have a longer version of something like &lt;a href=&quot;http://ifdb.tads.org/viewgame?id=lb2hf5pqx68wbqhh&quot; data-href=&quot;http://ifdb.tads.org/viewgame?id=lb2hf5pqx68wbqhh&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Conan Kill Everything&lt;/a&gt;. More likely, though, the player will get bored after the first few fights and will quit.&lt;/p&gt;
&lt;p name=&quot;4aca&quot; id=&quot;4aca&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;That brings us to an important question: What makes combat in Skyrim fun, anyway? We do the same things over and over again and each battle is largely similar. How do we not get bored with them after the first dungeon?&lt;/p&gt;
&lt;p name=&quot;fadf&quot; id=&quot;fadf&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The answer is twofold:&lt;/p&gt;
&lt;ol class=&quot;postList&quot;&gt;&lt;li name=&quot;3596&quot; id=&quot;3596&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;You have a high level of agency during the fights. At any time, you can advance, retreat, attack, defend, change weapons, jump, crouch, flank, climb a rock, use magic, etc. This means you can &lt;em class=&quot;markup--em markup--li-em&quot;&gt;get better at fighting&lt;/em&gt; as a player. &lt;em class=&quot;markup--em markup--li-em&quot;&gt;You&lt;/em&gt; have to fight. Therefore, you can experience, well, &lt;em class=&quot;markup--em markup--li-em&quot;&gt;fun&lt;/em&gt; (as it’s defined in books like &lt;a href=&quot;https://www.theoryoffun.com/&quot; data-href=&quot;https://www.theoryoffun.com/&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Theory of Fun for Game Design&lt;/a&gt;).&lt;/li&gt;
&lt;li name=&quot;2737&quot; id=&quot;2737&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Fights are highly unpredictable. You never know how each combat will turn out. You can do battle with the same group of enemies three times in a row and the course of the fight will be different each time. This means you get &lt;a href=&quot;https://www.amazon.com/dp/B00LMGLXTS/&quot; data-href=&quot;https://www.amazon.com/dp/B00LMGLXTS/&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;variable rewards&lt;/a&gt; — an important ingredient for getting you “hooked” on the game.&lt;/li&gt;
&lt;/ol&gt;&lt;p name=&quot;e32b&quot; id=&quot;e32b&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;Skyrim can throw similar combat at you over and over again, and not only does it not become boring, it’s great entertainment! This is not &lt;em class=&quot;markup--em markup--p-em&quot;&gt;despite&lt;/em&gt; the fact that it’s repetitive, it is &lt;em class=&quot;markup--em markup--p-em&quot;&gt;because&lt;/em&gt; of the cocktail of repetition, agency, and unpredictability.&lt;/p&gt;
&lt;p name=&quot;9aff&quot; id=&quot;9aff&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Therein lies the fundamental challenge of trying to marry videogames and text.&lt;/p&gt;
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;d35b&quot; id=&quot;d35b&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;Good videogames &lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;always have&lt;/strong&gt; some repetition. All gaming from Pong through Tetris to Portal to Skyrim is based on repeatedly throwing a similar problem at you, the player, so that you can get better at solving it.&lt;/li&gt;
&lt;li name=&quot;8ea7&quot; id=&quot;8ea7&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Good text &lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;cannot have&lt;/strong&gt; too much repetition. Repetitive text is boring. Try writing up everything that happens in the first five minutes of some &lt;a href=&quot;https://www.youtube.com/watch?v=0q5uDKtI1Go&quot; data-href=&quot;https://www.youtube.com/watch?v=0q5uDKtI1Go&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Super Mario gameplay&lt;/a&gt; with prose that is fun to read. It’s impossible.&lt;/li&gt;
&lt;/ul&gt;&lt;p name=&quot;429e&quot; id=&quot;429e&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;Text games normally solve this in two ways: they either stay away from repetition as much as possible (which makes the game more intellectual — all situations and solutions are unique) or they break up repetitive gameplay into a bunch of minigames. For great examples of the former approach, see almost any of the &lt;a href=&quot;http://ifdb.tads.org/search?sortby=rcu&amp;amp;newSortBy.x=0&amp;amp;newSortBy.y=0&amp;amp;searchfor=&amp;amp;browse=1&quot; data-href=&quot;http://ifdb.tads.org/search?sortby=rcu&amp;amp;newSortBy.x=0&amp;amp;newSortBy.y=0&amp;amp;searchfor=&amp;amp;browse=1&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;top traditional interactive fiction&lt;/a&gt;. For an example of the latter, see the brilliant &lt;a href=&quot;https://www.inklestudios.com/sorcery/&quot; data-href=&quot;https://www.inklestudios.com/sorcery/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Sorcery!&lt;/a&gt; series.&lt;/p&gt;
&lt;p name=&quot;1a09&quot; id=&quot;1a09&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Third try&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;6901&quot; id=&quot;6901&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Since we already understand that Skyrim’s gameplay is largely centered around combat, let’s try the minigame approach (and not the more intellectual, non-repetitive one).&lt;/p&gt;
&lt;blockquote name=&quot;cd16&quot; id=&quot;cd16&quot; class=&quot;graf graf--blockquote graf-after--p&quot;&gt;
&lt;p&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;Outside of Whiterun&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;ccdd&quot; id=&quot;ccdd&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;You arrive at Whiterun from the south. There’s a solitary bandit waiting for you, axe in hand and ready to fight.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;2e79&quot; id=&quot;2e79&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot;&gt;
&lt;p&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;&amp;gt; kill bandit&lt;br/&gt;&lt;/strong&gt;run&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;f693&quot; id=&quot;f693&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;&amp;lt;A graphical combat minigame.&amp;gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;fa9a&quot; id=&quot;fa9a&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;The bandit keels over and dies.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;0*KWIDOqkcgL5Uwm5a.&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;0*KWIDOqkcgL5Uwm5a.&quot; src=&quot;https://cdn-images-1.medium.com/max/1200/0*KWIDOqkcgL5Uwm5a.&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;9a04&quot; id=&quot;9a04&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The minigame can have some text in it, but the mechanics are not tied to the text. In Sorcery!, the combat minigame consists of a series of decisions about how hard to hit (on a scale of total defense to all-out attack). But there are no limits to the design of the minigames we can use. It can be a simple card game, a match 3 puzzle, etc.&lt;/p&gt;
&lt;p name=&quot;f79b&quot; id=&quot;f79b&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;This works. I, for one, would buy a mobile version of Skyrim done in the same fashion as Sorcery!.&lt;/p&gt;
&lt;p name=&quot;6cd8&quot; id=&quot;6cd8&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;But I also think we can do better. Much of my enjoyment of Skyrim stems from the fact that it’s an open world where anything can happen almost anywhere. There’s no switching between “exploration mode” and “combat mode”. An inn where you’re talking to an NPC can become a point of tactical cover moments later. You can maneuver around a patrol and hit them with something nasty at long range from the relative safety of a cliff. And so on.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*nRRqtVBwuP8JYWMLo3Eslw.jpeg&quot; data-width=&quot;3732&quot; data-height=&quot;1572&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*nRRqtVBwuP8JYWMLo3Eslw.jpeg&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*nRRqtVBwuP8JYWMLo3Eslw.jpeg&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;1c3c&quot; id=&quot;1c3c&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;None of this translates well into the text-with-minigame solution. So let’s go further.&lt;/p&gt;
&lt;p name=&quot;765d&quot; id=&quot;765d&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Fourth try&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;52c8&quot; id=&quot;52c8&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;We’re going to tweak the level of abstraction yet again. Going frame-by-frame in our naive start was obviously the wrong move. And going with “kill bandit” obviously made the level of abstraction too high, no matter whether the fight was described in text or represented through a minigame.&lt;/p&gt;
&lt;p name=&quot;8687&quot; id=&quot;8687&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Let’s descend just a little bit from “kill bandit” into a tactics-based approach. Something like this:&lt;/p&gt;
&lt;blockquote name=&quot;7d06&quot; id=&quot;7d06&quot; class=&quot;graf graf--blockquote graf-after--p&quot;&gt;
&lt;p&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;Outside of Whiterun&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;b96a&quot; id=&quot;b96a&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;You arrive at Whiterun from the south. There’s a solitary bandit waiting for you, axe in hand and ready to fight.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;facc&quot; id=&quot;facc&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot;&gt;
&lt;p&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;&amp;gt; kill bandit&lt;br/&gt;&lt;/strong&gt;run&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;a97c&quot; id=&quot;a97c&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;How exactly do you want to go about killing the bandit?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;b08e&quot; id=&quot;b08e&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;… with sword&lt;br/&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;&amp;gt; … with bow and arrow&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;5b47&quot; id=&quot;5b47&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;6&quot;&gt;
&lt;p&gt;You quickly string the bow and let it fly. The arrow whirls just past the bandit’s ear. &amp;lt;Rest of fight description.&amp;gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;156d&quot; id=&quot;156d&quot; class=&quot;graf graf--p graf-after--blockquote&quot;&gt;This approach creates more options than just “kill”, but after a while it’s not much fun. Each player gravitates toward a certain style of play (stealth, long range, melee, magic, etc.) and therefore there’s really not much choice involved. For an archer, the best tactic will almost always be “bow and arrow” or “sneak around”. Another player will see very different combat sequences, but that doesn’t necessarily make the experience fun.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*-Wf0d8uYp8DrHrHCvDv9ww.jpeg&quot; data-width=&quot;3732&quot; data-height=&quot;1572&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*-Wf0d8uYp8DrHrHCvDv9ww.jpeg&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*-Wf0d8uYp8DrHrHCvDv9ww.jpeg&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;1185&quot; id=&quot;1185&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;So we’ll need to offer players meaningful choices on a lower level.&lt;/p&gt;
&lt;p name=&quot;a8f2&quot; id=&quot;a8f2&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Fifth try&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;cc53&quot; id=&quot;cc53&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Alright, fine. Let’s go with action-by-action level combat, &lt;a href=&quot;https://roll20.net/compendium/dnd5e/Combat#toc_5&quot; data-href=&quot;https://roll20.net/compendium/dnd5e/Combat#toc_5&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;D&amp;amp;D-style&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote name=&quot;f918&quot; id=&quot;f918&quot; class=&quot;graf graf--blockquote graf-after--p&quot;&gt;
&lt;p&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;Outside of Whiterun&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;7759&quot; id=&quot;7759&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;You arrive at Whiterun from the south. There’s a solitary bandit waiting for you, axe in hand and ready to fight. You unsheathe your sword and approach.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;1419&quot; id=&quot;1419&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;&amp;gt; thrust sword&lt;br/&gt;&lt;/strong&gt;swing sword&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;c7d0&quot; id=&quot;c7d0&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;Your blade moves fast but misses, just left of the bandit’s chest. The bandit swings the axe and lightly cleaves your leather jerkin.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;7cbf&quot; id=&quot;7cbf&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;thrust sword&lt;br/&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;&amp;gt; swing sword&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;51e5&quot; id=&quot;51e5&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;The sword cuts the bandit’s thigh and he yells in pain. He tries to strike at you from above, but the axe goes wide.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;937b&quot; id=&quot;937b&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot;&gt;
&lt;p&gt;&amp;lt;Rest of fight.&amp;gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;df40&quot; id=&quot;df40&quot; class=&quot;graf graf--p graf-after--blockquote&quot;&gt;This starts to look like the player has actual agency. There are many possible moves that fit different playing styles (a swordsman can thrust, swing, tackle, decapitate; an archer can target different parts of the body) and those moves can be combined in a myriad of different ways. Players can develop their own tactics!&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*KglqnCOqHlgHBe_ecbxh7Q.jpeg&quot; data-width=&quot;3732&quot; data-height=&quot;1572&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*KglqnCOqHlgHBe_ecbxh7Q.jpeg&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*KglqnCOqHlgHBe_ecbxh7Q.jpeg&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;6106&quot; id=&quot;6106&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;But there are issues with this approach.&lt;/p&gt;
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;7739&quot; id=&quot;7739&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;The prose ends up being very mechanical. It’s not as bad as our first naive approach, but it’s still something a real author would never write. “You do X, opponent does Y.” Repeat. That gets boring quickly.&lt;/li&gt;
&lt;li name=&quot;15de&quot; id=&quot;15de&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;There is no easy way to deal with reactions. A strike with a sword either hits or misses. Sure, the receiving side can anticipate the strike and can be defending at that time (“assume defensive stance” or “raise shield”), but players generally hate doing that. It means trading your opportunity to strike (awesome) for &lt;em class=&quot;markup--em markup--li-em&quot;&gt;maybe&lt;/em&gt; taking less damage (meh). It might be the rational choice in some situations, but it definitely doesn’t feel like an adventure.&lt;/li&gt;
&lt;/ul&gt;&lt;p name=&quot;0b02&quot; id=&quot;0b02&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;So now it looks like we’re screwed. We covered the whole spectrum of abstraction and nothing quite makes the game we want work.&lt;/p&gt;
&lt;p name=&quot;1619&quot; id=&quot;1619&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;This is where I found myself after tinkering with the game for 5 years. I almost gave up, thinking that I proved — for myself at least — that there is no way to build a Skyrim-like open world game in text. Not even in theory.&lt;/p&gt;
&lt;p name=&quot;6e76&quot; id=&quot;6e76&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;I decided to take a break from this “Skyrim in text” project. Instead, I started building something completely different. The game was still text-based, but gamplay was more procedural and psychological. It was about survival in a group.&lt;/p&gt;
&lt;p name=&quot;7ef4&quot; id=&quot;7ef4&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;And during &lt;em class=&quot;markup--em markup--p-em&quot;&gt;that&lt;/em&gt; project, I realized that text stories are fractal. Unlinke in videogames (which need a steady update method), in text we don’t actually need to stick to a single level of abstraction.&lt;/p&gt;
&lt;p name=&quot;776f&quot; id=&quot;776f&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Sixth and final try&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;5ca8&quot; id=&quot;5ca8&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Let me show you what I mean:&lt;/p&gt;
&lt;blockquote name=&quot;fdb0&quot; id=&quot;fdb0&quot; class=&quot;graf graf--blockquote graf-after--p&quot;&gt;
&lt;p&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;Outside of Whiterun&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;eab3&quot; id=&quot;eab3&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;You arrive at Whiterun from the south. There’s a solitary bandit waiting for you, axe in hand and ready to fight.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;6af7&quot; id=&quot;6af7&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;&amp;gt; charge at the bandit&lt;br/&gt;&lt;/strong&gt;use bow and arrow&lt;br/&gt;run&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;ba92&quot; id=&quot;ba92&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;You raise your sword and dash at the bandit. He takes a few quick steps back and when you come in range, he swiftly swings at you.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;d322&quot; id=&quot;d322&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot;&gt;
&lt;p&gt;jump back&lt;br/&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;&amp;gt; dodge&lt;br/&gt;&lt;/strong&gt;block&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;2d3e&quot; id=&quot;2d3e&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;7&quot;&gt;
&lt;p&gt;You duck and the bandit’s sword swings an inch above your head. This briefly exposes the bandit’s side, which has no armor.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;c398&quot; id=&quot;c398&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;&lt;strong class=&quot;markup--strong markup--blockquote-strong&quot;&gt;&amp;gt; thrust sword&lt;br/&gt;&lt;/strong&gt;punch flank&lt;br/&gt;ignore&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;dcc6&quot; id=&quot;dcc6&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot;&gt;
&lt;p&gt;&amp;lt;Rest of fight.&amp;gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;d2fa&quot; id=&quot;d2fa&quot; class=&quot;graf graf--p graf-after--blockquote&quot;&gt;Notice how the “charge” command is a tactical one — it’s on a higher level of abstraction than slash or thrust. Then the bandit’s first swing is on the action-by-action level. But then, before the swing even finishes, we go one level deeper, and the player can react to that situation by dodging (or blocking or jumping back). And since that succeeds, we stay at that low level, and the player can insert a counter-thrust &lt;em class=&quot;markup--em markup--p-em&quot;&gt;during&lt;/em&gt; the bandit’s dodged swing.&lt;/p&gt;
&lt;p name=&quot;19c4&quot; id=&quot;19c4&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Now the fight includes many choices on all levels, from tactical positioning to split-second actions. Repetition is then more fun to the player, because the same action can be performed in vastly different contexts.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*VgFUxD7mhk9MHSSZvsKi4g.jpeg&quot; data-width=&quot;3732&quot; data-height=&quot;1572&quot; data-is-featured=&quot;true&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*VgFUxD7mhk9MHSSZvsKi4g.jpeg&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*VgFUxD7mhk9MHSSZvsKi4g.jpeg&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;785d&quot; id=&quot;785d&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The resulting prose reads like something a human would write. It’s not just two people exchanging damage, it’s an actual swordfight with dynamic potential outcomes every step of the way. And gameplay flows through different levels of abstraction, like a book would.&lt;/p&gt;
&lt;p name=&quot;0845&quot; id=&quot;0845&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Caveats&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;bb77&quot; id=&quot;bb77&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;So there you have it: Insignificant Little Vermin, my submission to IFCOMP, is working with the concept of fractal stories in an open-world simulation to produce a relatively fluid, readable text adventure. Some players have even assumed the text is &lt;em class=&quot;markup--em markup--p-em&quot;&gt;not&lt;/em&gt; procedurally generated in parts where I know it is. More importantly, though, players tend to have a lot of fun.&lt;/p&gt;

&lt;p name=&quot;2062&quot; id=&quot;2062&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;It’s not all fireworks and applause, though, of course. The short game made it clear to me what the limitations of such an approach are, and just how much more work there is to be done before I accomplish my vision of a completely open world rendered in text.&lt;/p&gt;
&lt;p name=&quot;b561&quot; id=&quot;b561&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;First of all, this game is really just the first stab in this direction. I discovered many of the insights above while developing Vermin. There is no playbook, no best practices, no industry events about fractal stories or “Skyrims in text”. There is, thankfully, an abundance of literature about interactive fiction, though most of it focuses on a very different aspect of fiction than what I’m after.&lt;/p&gt;
&lt;p name=&quot;4b22&quot; id=&quot;4b22&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;For one, there’s much less content in Vermin than I’d like it to have. It’s nowhere near an AAA game (like Skyrim) in scope. It currently only offers limited RPG progression (player character doesn’t learn new stuff, for example), and the combat could use more variability. Also, there is no mapping.&lt;/p&gt;
&lt;p name=&quot;adc3&quot; id=&quot;adc3&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;That’s all quite easily solved by putting more work in on this project, though.&lt;/p&gt;
&lt;p name=&quot;54c1&quot; id=&quot;54c1&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;What I find more interesting are the limitations that won’t go away so easily. The ones that stem from the nature of the system, and from text in general.&lt;/p&gt;
&lt;p name=&quot;0635&quot; id=&quot;0635&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;The need for clarity and (surface) simplicity&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;66f1&quot; id=&quot;66f1&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In text, it is very easy to give too much information. A completely chaotic engagement is fine when it develops in 3D graphics, but unreadable when it’s rendered in prose. A swordfight with 10 actors must be broken down or abstracted — a possible solution being something like Level of Detail (LoD) or Limelight, where the actions of actors who are distant from the player are either ignored or described only in broad strokes.&lt;/p&gt;
&lt;p name=&quot;0074&quot; id=&quot;0074&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In text, it’s hard (and boring) to explain spatial relations. Read any sword and sorcery novel and you’ll see how Conan/Fafhrd/Elric always swings at the pirate/swordsman/lich but never “forward and to the left”. Position is always described only in the most general way. No matter how clear you think you are, you’ll never beat 2D or 3D graphics in terms of quickly letting the player know where everyone is and what they’re doing.&lt;/p&gt;
&lt;p name=&quot;13c7&quot; id=&quot;13c7&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In text, you have to think about names, categories, and pronouns, too. It actually helps to have a variety of genders at play at any given time, because then it’s easier to use pronouns to let the player know who we’re focusing on. For example, when you have a scene with a male thief, a female warrior, and a spider, your natural language generation algorithm will get by with “he”, “she” and “it” for the most part. Which is great. Contrast that with a scene that contains 3 male orc warriors and you’ll probably have a lot of sentences like “The orc on the left makes a move.” Which is boring and difficult to read.&lt;/p&gt;
&lt;p name=&quot;d1ab&quot; id=&quot;d1ab&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In text, it’s harder to work with a myriad of characters’ states. You can’t expect the player to remember that, for example, the orc has his arm extended to the left. Fortunately, people have imagination (and love to use it!), so your underlying simulation &lt;em class=&quot;markup--em markup--p-em&quot;&gt;can&lt;/em&gt; be complex — you just need to make sure to only foreground the most important things, and let the reader get the rest from subtext.&lt;/p&gt;
&lt;p name=&quot;0675&quot; id=&quot;0675&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;The need for a casual feel&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;2ecc&quot; id=&quot;2ecc&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In text, you can’t have your mechanics too stats-based or competitive. If you go that path, sooner or later, your game becomes a spreadsheet with some text sprinkled on top to provide a narrative. Not that this is a bad thing — I like these kinds of games (see &lt;a href=&quot;http://philome.la/johnayliff/seedship/play&quot; data-href=&quot;http://philome.la/johnayliff/seedship/play&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Seedship&lt;/a&gt;). It’s just that this is not what we’re going for with fractal stories.&lt;/p&gt;
&lt;p name=&quot;2a9e&quot; id=&quot;2a9e&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Procedural content isn’t a silver bullet&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;1314&quot; id=&quot;1314&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In text, combat (and procedural content in general) works much better when intermixed with something else. Context, character, and narrative development are part of what makes a game fun. In Vermin, all fights have at least some flavor text (opponents talking to each other) and so do the sectiones in which you’re roaming the world. You can’t just defer to procedural generation — you should still have a lot of content (even if that content is, in the end, used procedurally). This follows the general rule, though, that procedural content is not for “lazy developers”. It’s not there to free you from creating content. It’s there to let content react to the player. You still need a lot of it.&lt;/p&gt;
&lt;p name=&quot;63e7&quot; id=&quot;63e7&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Writing is hard&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;ea11&quot; id=&quot;ea11&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;It’s easy to come up with “You hit orc for 15 HP”. It’s much harder to come up with many versions of “your blade misses the orc”.&lt;/p&gt;
&lt;p name=&quot;9f74&quot; id=&quot;9f74&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Natural language generation is hard&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;c528&quot; id=&quot;c528&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;It’s easier than modern 3D programming (at least in terms of person-hours), but not by much. Human language is rich and full of pitfalls. Fortunately, we can choose a subset of natural language that is quite easy to do (specifically, present-tense action). Still, for something like this to work, you can’t expect someone who is primarily a writer to successfully program the game. This is &lt;em class=&quot;markup--em markup--p-em&quot;&gt;not&lt;/em&gt; the next Twine or Inform. This is for a development team, not a single author.&lt;/p&gt;
&lt;p name=&quot;f6ca&quot; id=&quot;f6ca&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Not for just any genre&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;5361&quot; id=&quot;5361&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Lastly, not all types of content are a good fit for fractal stories. Or, at the very least, some genres are much better suited for this approach. Sword &amp;amp; sorcery is great because it’s:&lt;/p&gt;
&lt;ol class=&quot;postList&quot;&gt;&lt;li name=&quot;d107&quot; id=&quot;d107&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;A lot of action described straightforwardly, in present tense.&lt;/li&gt;
&lt;li name=&quot;5cb2&quot; id=&quot;5cb2&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Immediate, visceral conflict.&lt;/li&gt;
&lt;li name=&quot;68ef&quot; id=&quot;68ef&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Reactive (counter a slash by blocking it).&lt;/li&gt;
&lt;li name=&quot;4d12&quot; id=&quot;4d12&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Physical exploration.&lt;/li&gt;
&lt;/ol&gt;&lt;p name=&quot;4a88&quot; id=&quot;4a88&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;Contrast this to something like &lt;a href=&quot;http://store.steampowered.com/app/400/Portal/&quot; data-href=&quot;http://store.steampowered.com/app/400/Portal/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;Portal&lt;/a&gt;, which features actions that are hard to imagine occuring in spaces that are hard to describe, involving lots of skill on the player’s end. But we don’t need to go as far as 3D puzzlers to find subject matter that won’t work. Even other combat-oriented open world games are significantly harder to “port to text” once they involve relatively realistic modern-day shooting instead of fantasy combat. Maybe it’s just my lack of imagination (and almost no experience with literature involving gun fights), but I can’t see a good combat mechanic that would work with the immediacy of firearms.&lt;/p&gt;
&lt;p name=&quot;004f&quot; id=&quot;004f&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;To end on a high note&lt;/strong&gt;&lt;/p&gt;
&lt;p name=&quot;2c4b&quot; id=&quot;2c4b&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;So I’ve spent quite a lot of time here explaining the problems of fractal stories. Let’s end on a high note. I want to explain why I’m so enthusiastic about this.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*z1VRhWGR-_oU_ca3uM7o7Q.png&quot; data-width=&quot;1080&quot; data-height=&quot;1920&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*z1VRhWGR-_oU_ca3uM7o7Q.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1200/1*z1VRhWGR-_oU_ca3uM7o7Q.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;206e&quot; id=&quot;206e&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The form factor that Insignificant Little Vermin ended up with is a great fit for casual play, especially on mobile. It’s not taxing in terms of skill. It can be consumed in short bites of gameplay, or as a whole. It provides an element of chance. It lets you read a story that is unique to you, and lets you explore a world at your own pace. It doesn’t require you to squint to see what’s going on on the screen — it’s just text and a few static paintings.&lt;/p&gt;
&lt;p name=&quot;542e&quot; id=&quot;542e&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In short, it’s something that you can read &amp;amp; play while waiting on a bus.&lt;/p&gt;
&lt;p name=&quot;5386&quot; id=&quot;5386&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;I really, earnestly believe that this could be transformational for many indie game developers (some of whom are not even thinking of themselves as game developers yet). As I said before, this is not a system for a single writer-author-developer. Compared to Twine, the barrier to entry is very high. But compared to even the simplest 2D platformer, it’s relatively low.&lt;/p&gt;
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;6594&quot; id=&quot;6594&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;You can move you developers from graphics programming and 60fps performance optimization to core mechanics, simulation, and natural language generation. Remember, you don’t have a 16ms frame budget anymore, so your simulation and AI can take its time.&lt;/li&gt;
&lt;li name=&quot;7c61&quot; id=&quot;7c61&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Your artists can spend less time filling in the details of the environment (grass here, wall texture there) and more time imagining interesting worlds and characters. They are also unrestricted by capabilities of the rendering engine. If they can describe it, they can use it. An army of 500 thousand dwarves streaming down a valley? Sure. A Ringworld-type structure where you can see terrain millions of kilometers away? Go for it. A Lovecraftian/Escheresque/non-Euclidean architecture? No problem.&lt;/li&gt;
&lt;li name=&quot;5b11&quot; id=&quot;5b11&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Designers are less bound to a single level of detail. Instead, they can let the player be both a general and a soldier in one game, and it will never be a problem. There’s so much less you need to implement with text than there is with graphics. Adding any gameplay mechanic is suddenly much cheaper.&lt;/li&gt;
&lt;li name=&quot;cb5e&quot; id=&quot;cb5e&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;A small team can create something that meets Skyrim in scale.&lt;/li&gt;
&lt;/ul&gt;&lt;p name=&quot;f62d&quot; id=&quot;f62d&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;I can’t wait to see where this approach to game development goes. I’ll be updating this blog and my Twitter account with my progress. If you want less noise, you can &lt;a href=&quot;https://egamebook.com/signup/&quot; data-href=&quot;https://egamebook.com/signup/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;subscribe&lt;/a&gt; to the mailing list that I created to keep you updated on key moments.&lt;/p&gt;
&lt;p name=&quot;479b&quot; id=&quot;479b&quot; class=&quot;graf graf--p graf-after--p graf--trailing&quot;&gt;And if you missed the link, &lt;a href=&quot;https://egamebook.com/vermin&quot; data-href=&quot;https://egamebook.com/vermin&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;here’s Insignificant Little Vermin&lt;/a&gt; and &lt;a href=&quot;https://ifcomp.org/ballot/#entry-1669&quot; data-href=&quot;https://ifcomp.org/ballot/#entry-1669&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;nofollow noopener&quot; target=&quot;_blank&quot;&gt;here’s its submission&lt;/a&gt; at IFCOMP.&lt;/p&gt;
&lt;/div&gt;</description>
<pubDate>Thu, 09 Nov 2017 11:17:21 +0000</pubDate>
<dc:creator>ellinokon</dc:creator>
<og:title>Skyrim rendered in text – Filip Hracek – Medium</og:title>
<og:url>https://medium.com/@filiph/skyrim-rendered-in-text-1899548ab2c4</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*VgFUxD7mhk9MHSSZvsKi4g.jpeg</og:image>
<og:description>Fractal stories, or how to build an open-world text adventure</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://medium.com/@filiph/skyrim-rendered-in-text-1899548ab2c4</dc:identifier>
</item>
</channel>
</rss>