<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Facebook Lies</title>
<link>https://iain.learmonth.me/blog/2017/2017w402/</link>
<guid isPermaLink="true" >https://iain.learmonth.me/blog/2017/2017w402/</guid>
<description>&lt;div class=&quot;post-stamp&quot;&gt;&lt;time datetime=&quot;2017-10-03T12:00:00Z&quot;&gt;3 Oct 2017&lt;/time&gt;&lt;span class=&quot;taglist&quot;&gt;· &lt;a class=&quot;btn btn-light&quot; href=&quot;https://iain.learmonth.me/tags/security/&quot;&gt;security&lt;/a&gt; &lt;a class=&quot;btn btn-light&quot; href=&quot;https://iain.learmonth.me/tags/web/&quot;&gt;web&lt;/a&gt;&lt;/span&gt;&lt;/div&gt;&lt;p&gt;In the past, I had a Facebook account. Long ago I “deleted” this account through the procedure outlined &lt;a href=&quot;https://en-gb.facebook.com/help/250563911970368&quot;&gt;on their help pages&lt;/a&gt;. In theory, 14 days after I used this process my account would be irrevocably gone. This was all lies.&lt;/p&gt;
&lt;p&gt;My account was not deleted and yesterday I received an email:&lt;/p&gt;
&lt;img src=&quot;https://iain.learmonth.me/content/images/2017/10/facebookemail.png&quot;/&gt; Screenshot of the email I received from Facebook
&lt;p&gt;It took me a moment to figure it out, but what had happened here is someone had logged into my Facebook account using my email address and password. Facebook simply reactivated the account, which had not had its data deleted, as if I had logged in.&lt;/p&gt;
&lt;p&gt;This was possible because:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Facebook was clinging to the hope that I would like to return&lt;/li&gt;
&lt;li&gt;The last time I used Facebook I didn’t know what a password manager was and was using the same password for basically everything&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;When I logged back in, all I needed to provide to prove I was me was my date of birth. Given that old Facebook passwords are readily available from dumps (people think their accounts are gone, so why should they be changing their passwords?) and my date of birth is not secret either, this is not great.&lt;/p&gt;
&lt;p&gt;I followed the deletion procedure again and in 2 weeks (you can’t immediately request deletion apparently) I’ll check to see if the account is really gone. I’ve updated the password so at least the deletion process can’t be interrupted by whoever has that password (probably lots of people - it’ll be in a ton of dumps where databases have been hacked).&lt;/p&gt;
&lt;p&gt;If it’s still not gone, I hear you can just post obscene and offensive material until Facebook deletes you. I’d rather not have to take that route though.&lt;/p&gt;
&lt;p&gt;If you’re interested to see if you’ve turned up in a hacked database dump yourself, I would recommend &lt;a href=&quot;https://haveibeenpwned.com/&quot;&gt;hibp&lt;/a&gt;.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;em&gt;If you would like to contact me with comments, please &lt;a href=&quot;mailto:irl@fsfe.org?subject=Re:%20Facebook%20Lies&quot;&gt;send me an email&lt;/a&gt;.&lt;/em&gt;&lt;br/&gt;&lt;em&gt;If you would like to support my free software work, you can &lt;a href=&quot;https://www.paypal.me/fossirl/25&quot;&gt;donate via PayPal&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;This post was syndicated on:&lt;/p&gt;
</description>
<pubDate>Tue, 03 Oct 2017 12:33:40 +0000</pubDate>
<dc:creator>JoshTriplett</dc:creator>
<og:title>Facebook Lies · Iain R. Learmonth</og:title>
<og:url>https://iain.learmonth.me/blog/2017/2017w402/</og:url>
<og:type>article</og:type>
<dc:language>en-gb</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://iain.learmonth.me/blog/2017/2017w402/</dc:identifier>
</item>
<item>
<title>The Nobel Prize in Physics 2017</title>
<link>https://www.nobelprize.org/nobel_prizes/physics/laureates/2017/press.html</link>
<guid isPermaLink="true" >https://www.nobelprize.org/nobel_prizes/physics/laureates/2017/press.html</guid>
<description>&lt;img src=&quot;https://www.nobelprize.org/nobel_prizes/physics/laureates/kva_logo_09.gif&quot; alt=&quot;Logo&quot;/&gt;

&lt;p&gt;3 October 2017&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;/redirect/links_out/prizeawarder.php?from=/nobel_prizes/physics/laureates/2016/press.html&amp;amp;object=kva&amp;amp;to=http://www.kva.se/en/&quot; target=&quot;_blank&quot;&gt;The Royal Swedish Academy of Sciences&lt;/a&gt; has decided to award the Nobel Prize in Physics 2017 with one half to&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rainer Weiss&lt;/strong&gt;&lt;br/&gt;LIGO/VIRGO Collaboration&lt;/p&gt;
&lt;p&gt;and the other half jointly to&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Barry C. Barish&lt;/strong&gt;&lt;br/&gt;LIGO/VIRGO Collaboration&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kip S. Thorne&lt;/strong&gt;&lt;br/&gt;LIGO/VIRGO Collaboration&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&quot;for decisive contributions to the LIGO detector and the observation of gravitational waves&quot;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Gravitational waves finally captured&lt;/h2&gt;
&lt;p class=&quot;ingress&quot;&gt;On 14 September 2015, the universe's gravitational waves were observed for the very first time. The waves, which were predicted by Albert Einstein a hundred years ago, came from a collision between two black holes. It took 1.3 billion years for the waves to arrive at the LIGO detector in the USA.&lt;/p&gt;
&lt;p&gt;The signal was extremely weak when it reached Earth, but is already promising a revolution in astrophysics. Gravitational waves are an entirely new way of observing the most violent events in space and testing the limits of our knowledge.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LIGO, the Laser Interferometer Gravitational-Wave Observatory&lt;/strong&gt;, is a collaborative project with over one thousand researchers from more than twenty countries. Together, they have realised a vision that is almost fifty years old. The 2017 Nobel Laureates have, with their enthusiasm and determination, each been invaluable to the success of LIGO. Pioneers &lt;strong&gt;Rainer Weiss&lt;/strong&gt; and &lt;strong&gt;Kip S. Thorne&lt;/strong&gt;, together with &lt;strong&gt;Barry C. Barish&lt;/strong&gt;, the scientist and leader who brought the project to completion, ensured that four decades of effort led to gravitational waves finally being observed.&lt;/p&gt;
&lt;p&gt;In the mid-1970s, Rainer Weiss had already analysed possible sources of background noise that would disturb measurements, and had also designed a detector, a laser-based interferometer, which would overcome this noise. Early on, both Kip Thorne and Rainer Weiss were firmly convinced that gravitational waves could be detected and bring about a revolution in our knowledge of the universe.&lt;/p&gt;
&lt;p&gt;Gravitational waves spread at the speed of light, filling the universe, as Albert Einstein described in his general theory of relativity. They are always created when a mass accelerates, like when an ice-skater pirouettes or a pair of black holes rotate around each other. Einstein was convinced it would never be possible to measure them. The LIGO project's achievement was using a pair of gigantic laser interferometers to measure a change thousands of times smaller than an atomic nucleus, as the gravitational wave passed the Earth.&lt;/p&gt;
&lt;p&gt;So far all sorts of electromagnetic radiation and particles, such as cosmic rays or neutrinos, have been used to explore the universe. However, gravitational waves are direct testimony to disruptions in spacetime itself. This is something completely new and different, opening up unseen worlds. A wealth of discoveries awaits those who succeed in capturing the waves and interpreting their message.&lt;/p&gt;
&lt;h3&gt;Read more about this year's prize&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.nobelprize.org/nobel_prizes/physics/laureates/2017/popular-physicsprize2017.pdf&quot; target=&quot;_blank&quot; class=&quot;text_link&quot;&gt;Popular Science Background&lt;/a&gt;&lt;br/&gt;&lt;span class=&quot;copy&quot;&gt;Pdf 1811 kB&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.nobelprize.org/nobel_prizes/physics/laureates/2017/advanced-physicsprize2017.pdf&quot; target=&quot;_blank&quot; class=&quot;text_link&quot;&gt;Scientific Background&lt;/a&gt;&lt;br/&gt;&lt;span class=&quot;copy&quot;&gt;Pdf 2338 kB&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nobelprize.org/nobel_prizes/physics/laureates/2017/fig_fy_en_17_gravitationalwaves.pdf&quot; target=&quot;_blank&quot; class=&quot;text_link&quot; onclick=&quot;ga('nobel1.send', 'pageview', '/nobel_prizes/physics/laureates/2017/fig_fy_en_17_gravitationalwaves.pdf');&quot;&gt;Image - Gravitational waves (pdf 162 kB)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.nobelprize.org/nobel_prizes/physics/laureates/2017/fig_fy_en_17_twoblackholes.pdf&quot; target=&quot;_blank&quot; class=&quot;text_link&quot; onclick=&quot;ga('nobel1.send', 'pageview', '/nobel_prizes/physics/laureates/2017/fig_fy_en_17_twoblackholes.pdf');&quot;&gt;Image - Two black holes (pdf 508 kB)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.nobelprize.org/nobel_prizes/physics/laureates/2017/fig_fy_en_17_LIGO.pdf&quot; target=&quot;_blank&quot; class=&quot;text_link&quot; onclick=&quot;ga('nobel1.send', 'pageview', '/nobel_prizes/physics/laureates/2017/fig_fy_en_17_LIGO.pdf');&quot;&gt;Image - LIGO (pdf 1825 kB)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.nobelprize.org/nobel_prizes/physics/laureates/2017/fig_fy_en_17_LIGO%20in%20the%20USA.pdf&quot; target=&quot;_blank&quot; class=&quot;text_link&quot; onclick=&quot;ga('nobel1.send', 'pageview', '/nobel_prizes/physics/laureates/2017/fig_fy_sv_17_LIGO in the USA.pdf');&quot;&gt;Image - LIGO in the USA (pdf 1734 kB)&lt;/a&gt;&lt;/p&gt;
&lt;p class=&quot;copy&quot;&gt;All illustrations: Copyright © Johan Jarnestad/The Royal Swedish Academy of Sciences&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rainer Weiss&lt;/strong&gt;, born 1932 in Berlin, Germany. Ph.D. 1962 from Massachusetts Institute of Technology, MIT, Cambridge, MA, USA. Professor of Physics, Massachusetts Institute of Technology, MIT, Cambridge, MA, USA.&lt;br/&gt;&lt;a href=&quot;http://web.mit.edu/physics/people/faculty/weiss_rainer.html&quot; target=&quot;_blank&quot; class=&quot;text_link&quot;&gt;http://web.mit.edu/physics/people/faculty/weiss_rainer.html B&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Barry C. Barish&lt;/strong&gt;, born 1936 in Omaha, NE, USA. Ph.D. 1962 from University of California, Berkeley, CA, USA. Linde Professor of Physics, California Institute of Technology, Pasadena, CA, USA&lt;br/&gt;&lt;a href=&quot;https://labcit.ligo.caltech.edu/~BCBAct/&quot; target=&quot;_blank&quot; class=&quot;text_link&quot;&gt;https://labcit.ligo.caltech.edu/~BCBAct/ K&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kip S. Thorne&lt;/strong&gt;, born 1940 in Logan, UT, USA. Ph.D. 1965 from Princeton University, NJ, USA. Feynman Professor of Theoretical Physics, California Institute of Technology, Pasadena, CA, USA&lt;br/&gt;&lt;a href=&quot;https://www.its.caltech.edu/~kip/index.html/&quot; class=&quot;text_link&quot;&gt;https://www.its.caltech.edu/~kip/index.html/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;LIGO/VIRGO COLLABORATION&lt;/em&gt;&lt;/strong&gt;&lt;br/&gt;&lt;a href=&quot;http://ligo.org/&quot; class=&quot;text_link&quot;&gt;www.ligo.org&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Prize amount:&lt;/strong&gt; 9 million Swedish krona. &lt;strong&gt;&lt;br/&gt;Further information:&lt;/strong&gt; www.kva.se and http://nobelprize.org&lt;strong&gt;&lt;br/&gt;Press contact:&lt;/strong&gt; Jessica Balksjö Nannini, Press Officer, Phone +46 8 673 95 44, +46 70 673 96 50, jessica.balksjo@kva.se&lt;br/&gt;&lt;strong&gt;Experts:&lt;/strong&gt; Olga Botner, member of the Nobel Committee for Physics, Phone +46 73-390 86 50, olga.botner@physics.uu.se,&lt;br/&gt;Ulf Danielsson, member of the Nobel Committee for Physics, Phone +46 70-314 10 86, ulf.danielsson@physics.uu.se&lt;/p&gt;
&lt;br/&gt;&lt;hr/&gt;&lt;p class=&quot;smalltext&quot;&gt;The Royal Swedish Academy of Sciences, founded in 1739, is an independent organisation whose overall objective is to promote the sciences and strengthen their influence in society. The Academy takes special responsibility for the natural sciences and mathematics, but endeavours to promote the exchange of ideas between various disciplines.&lt;/p&gt;
&lt;p class=&quot;smalltext&quot;&gt;&lt;em&gt;Nobel Prize® är is a registered trademark of the Nobel Foundation.&lt;/em&gt;&lt;/p&gt;


&lt;div&gt;
&lt;p&gt;
&lt;h5&gt;Share this:&lt;/h5&gt;
&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;&lt;span class=&quot;row1&quot;&gt;To cite this page&lt;/span&gt;&lt;br/&gt;&lt;span class=&quot;row2&quot;&gt;MLA style: &quot;The 2017 Nobel Prize in Physics - Press Release&quot;. &lt;em&gt;Nobelprize.org.&lt;/em&gt; Nobel Media AB 2014. Web. 3 Oct 2017. &amp;lt;http://www.nobelprize.org/nobel_prizes/physics/laureates/2017/press.html&amp;gt;&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;Recommended:&lt;/h2&gt;

</description>
<pubDate>Tue, 03 Oct 2017 09:53:22 +0000</pubDate>
<dc:creator>runesoerensen</dc:creator>
<og:image>http://www.nobelprize.org/nobel_prizes/physics/laureates/2017/../kva_logo_09.gif</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nobelprize.org/nobel_prizes/physics/laureates/2017/press.html</dc:identifier>
</item>
<item>
<title>Guacamole – A clientless remote desktop gateway</title>
<link>https://guacamole.incubator.apache.org/</link>
<guid isPermaLink="true" >https://guacamole.incubator.apache.org/</guid>
<description>&lt;div class=&quot;hook&quot; readability=&quot;11&quot;&gt;
&lt;div class=&quot;demo&quot;&gt;&lt;img class=&quot;thumbnail&quot; src=&quot;https://guacamole.incubator.apache.org/images/demo-thumbnail.jpg&quot; alt=&quot;Screenshot of Guacamole 0.9.4&quot;/&gt;&lt;iframe src=&quot;//player.vimeo.com/video/116207678?title=0&amp;amp;byline=0&amp;amp;portrait=0&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/div&gt;
&lt;div class=&quot;description&quot; readability=&quot;17&quot;&gt;
&lt;p&gt;Apache Guacamole is a &lt;strong&gt;clientless remote desktop gateway&lt;/strong&gt;. It supports standard protocols like VNC, RDP, and SSH.&lt;/p&gt;
&lt;p&gt;We call it &lt;em&gt;clientless&lt;/em&gt; because no plugins or client software are required.&lt;/p&gt;
&lt;p&gt;Thanks to HTML5, once Guacamole is installed on a server, all you need to access your desktops is a web browser.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;ul class=&quot;features&quot; readability=&quot;10.616064565719&quot;&gt;&lt;li class=&quot;html5&quot; readability=&quot;3&quot;&gt;
&lt;h2&gt;Access your computers from &lt;em&gt;anywhere&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Because the Guacamole client is an HTML5 web application, use of your computers is not tied to any one device or location. &lt;strong&gt;As long as you have access to a web browser, you have access to your machines.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li class=&quot;cloud-desktop&quot; readability=&quot;2&quot;&gt;
&lt;h2&gt;Keep your desktop in the cloud&lt;/h2&gt;
&lt;p&gt;Desktops accessed through Guacamole need not physically exist. With both Guacamole and a desktop operating system hosted in the cloud, you can combine the convenience of Guacamole with the resilience and flexibility of cloud computing.&lt;/p&gt;
&lt;/li&gt;
&lt;li class=&quot;apache&quot; readability=&quot;5.2582417582418&quot;&gt;
&lt;h2&gt;Free and open source&lt;/h2&gt;
&lt;p&gt;Apache Guacamole is and will always be &lt;strong&gt;free and open source software&lt;/strong&gt;. It is licensed under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot;&gt;Apache License, Version 2.0&lt;/a&gt;, and is actively maintained by a community of developers that use Guacamole to access their own development environments.&lt;/p&gt;
&lt;p&gt;We feel this sets us apart from other remote desktop solutions, and gives us a &lt;a href=&quot;https://guacamole.incubator.apache.org/open-source/&quot;&gt;distinct advantage&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li class=&quot;documented-api&quot; readability=&quot;2.6741214057508&quot;&gt;
&lt;h2&gt;Built on a well-documented API&lt;/h2&gt;
&lt;p&gt;Apache Guacamole is built on its own stack of core APIs which are &lt;a href=&quot;https://guacamole.incubator.apache.org/api-documentation/&quot;&gt;thoroughly documented&lt;/a&gt;, including basic tutorials and conceptual overviews in the &lt;a href=&quot;https://guacamole.incubator.apache.org/doc/gug/&quot;&gt;online manual&lt;/a&gt;. These APIs allow Guacamole to be tightly integrated into other applications, whether they be open source or proprietary.&lt;/p&gt;
&lt;/li&gt;
&lt;li class=&quot;commercially-supported&quot; readability=&quot;0&quot;&gt;
&lt;h2&gt;Commercially supported&lt;/h2&gt;
&lt;p&gt;For enterprises, dedicated commercial support is also available through &lt;a href=&quot;https://guacamole.incubator.apache.org/support/#commercial-support&quot;&gt;third party companies&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
<pubDate>Tue, 03 Oct 2017 02:44:28 +0000</pubDate>
<dc:creator>mutin-sa</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://guacamole.incubator.apache.org/</dc:identifier>
</item>
<item>
<title>Google and Facebook Have Failed Us</title>
<link>https://www.theatlantic.com/amp/article/541794/?single_page=true</link>
<guid isPermaLink="true" >https://www.theatlantic.com/amp/article/541794/?single_page=true</guid>
<description>&lt;aside readability=&quot;0.94939759036145&quot;&gt;
&lt;/aside&gt;&lt;section id=&quot;article-section-1&quot; readability=&quot;46.01776384535&quot;&gt;&lt;p&gt;In the crucial early hours after the Las Vegas mass shooting, it happened again: Hoaxes, completely unverified rumors, failed witch hunts, and blatant falsehoods spread across the internet.&lt;/p&gt;
&lt;p&gt;But they did not do so by themselves: They used the infrastructure that Google and Facebook and YouTube have built to achieve wide distribution. These companies are the most powerful information gatekeepers that the world has ever known, and yet they refuse to take responsibility for their &lt;em&gt;active role&lt;/em&gt; in damaging the quality of information reaching the public.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;BuzzFeed&lt;/em&gt;’s &lt;a href=&quot;https://twitter.com/broderick/status/914807674025512961&quot;&gt;Ryan Broderick found&lt;/a&gt; that Google’s “top stories” results surfaced 4chan forum posts about a man that right-wing amateur sleuths had incorrectly identified as the Las Vegas shooter.&lt;/p&gt;
&lt;p&gt;4chan is a known source not just of racism, but hoaxes and deliberate misinformation. In any list a human might make of sites to exclude from being labeled as “news,” 4chan would be near the very top.&lt;/p&gt;
&lt;/section&gt;
&lt;section id=&quot;article-section-2&quot; readability=&quot;52.565313653137&quot;&gt;&lt;p&gt;Yet, there Google was surfacing 4chan as people desperately searched for information about this wrongly accused man, adding fuel to the fire, amplifying the rumor. This is playing an &lt;em&gt;active role&lt;/em&gt; in the spread of bad information, poisoning the news ecosystem.&lt;/p&gt;
&lt;p&gt;The problem can be traced back to &lt;a href=&quot;http://searchengineland.com/googles-news-listings-beyond-traditional-205213&quot;&gt;a change Google made in October 2014&lt;/a&gt; to include non-journalistic sites in the “In the News” box instead of pulling from Google News.&lt;/p&gt;
&lt;p&gt;But one might have imagined that not &lt;em&gt;every&lt;/em&gt; forum site could be included. The idea that 4chan would be within the universe that Google might scrape is horrifying.&lt;/p&gt;
&lt;p&gt;Worse, when I asked Google about this, and indicated why I thought it was a severe problem, they sent back boilerplate.&lt;/p&gt;
&lt;blockquote readability=&quot;11&quot;&gt;
&lt;p&gt;Unfortunately, early this morning we were briefly surfacing an inaccurate 4chan website in our Search results for a small number of queries. Within hours, the 4chan story was algorithmically replaced by relevant results. This should not have appeared for any queries, and we’ll continue to make algorithmic improvements to prevent this from happening in the future.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It’s no longer good enough to note that something was algorithmically surfaced and then replaced. It’s no longer good enough to shrug off (“briefly,” “for a small number of queries”) the problems in the system simply because &lt;em&gt;it has computers in the decision loop&lt;/em&gt;.&lt;/p&gt;
&lt;/section&gt;
&lt;section id=&quot;article-section-3&quot; readability=&quot;62.04851004851&quot;&gt;&lt;p&gt;After I followed up with Google, they sent a more detailed response, which I cannot directly quote, but can describe. It was primarily an attempt to minimize the mistake Google had made, while acknowledging that they had made a mistake.&lt;/p&gt;
&lt;p&gt;4chan results, they said, had not shown up for general searches about Las Vegas, but only for the name of the misidentified shooter. The reason the 4chan forum post showed up was that it was “fresh” and there were relatively few searches for the falsely accused man. Basically, the algorithms controlling what to show didn’t have a lot to go on, and when something new popped up as searches for the name were ramping up, it was happy to slot it as the first result.&lt;/p&gt;
&lt;p&gt;The note further explained that what shows up in “In the News” derives from the “authoritativeness” of a site as well as the “freshness” of the content on it. And Google acknowledged they’d made a mistake in this case.&lt;/p&gt;
&lt;p&gt;The thing is: This is a predictable problem. In fact, there is already a similar example in the extant record. After the Boston bombings, we saw a very similar “&lt;a href=&quot;https://www.theatlantic.com/technology/archive/2013/04/-bostonbombing-the-anatomy-of-a-misinformation-disaster/275155/&quot;&gt;misinformation disaster&lt;/a&gt;.”&lt;/p&gt;
&lt;p&gt;Gabe Rivera, who runs a tech-news service called Techmeme that uses humans and algorithms to identify important stories, &lt;a href=&quot;https://twitter.com/gaberivera/status/914916351323422720&quot;&gt;addressed the problem&lt;/a&gt; in a tweet. Google, he said, couldn’t be asked to hand-sift all content but “they do have the resources to moderate the head,” i.e., the most important searches.&lt;/p&gt;
&lt;/section&gt;
&lt;section id=&quot;article-section-4&quot; readability=&quot;62&quot;&gt;&lt;p&gt;The truth is that machines need many examples to learn from. That’s something we know from all the current artificial-intelligence research. They’re not good at “one-shot” learning. But humans are very good at dealing with new and unexpected situations. Why are there not more humans inside Google who are tasked with basic information filtering? How can this not be part of the system, given that we know the machines will struggle with rare, breaking-news situations?&lt;/p&gt;
&lt;p&gt;Google is too important, and from what I’ve seen reporting on them for 10 years, the company &lt;em&gt;does care&lt;/em&gt; about information quality. Even from a pure corporate-trust and brand perspective, wouldn’t it be worth it to have a large enough team to make sure they get these situations right across the globe?&lt;/p&gt;
&lt;p&gt;Of course, it is not just Google.&lt;/p&gt;
&lt;p&gt;On Facebook, a simple search for “Las Vegas” yields a Group called “Las Vegas Shooting /Massacre,” which sprung up after the shooting and already has more than 5,000 members.&lt;/p&gt;
&lt;p&gt;The group is run by Jonathan Lee Riches, who gained notoriety by filing 3,000 frivolous lawsuits while serving a 10 year prison sentence after being convicted for stealing money by impersonating people whose bank credentials had been phished. Now, he calls himself an “investigative journalist” with &lt;em&gt;Infowars&lt;/em&gt;, though there is no indication he’s been published on the site, and given that he also lists himself as a former male underwear model at Victoria’s Secret, a former nuclear scientist at Chernobyl, and a former bodyguard at Buckingham Palace, his work history may not be reliable.&lt;/p&gt;
&lt;/section&gt;
&lt;section id=&quot;article-section-5&quot; readability=&quot;75.010004168404&quot;&gt;&lt;p&gt;The problems with surfacing this man’s group to Facebook users is obvious to &lt;em&gt;literally any human.&lt;/em&gt; But to Facebook’s algorithms, it’s just a fast-growing group with an engaged community.&lt;/p&gt;
&lt;p&gt;Most people who joined the group looking for information presumably don’t know that the founder is notorious for legal and informational hijinks.&lt;/p&gt;
&lt;p&gt;Meanwhile, Kevin Roose of &lt;em&gt;The New York Times&lt;/em&gt; &lt;a href=&quot;https://twitter.com/kevinroose/status/914881599216521216&quot;&gt;pointed out&lt;/a&gt; that Facebook’s Trending Stories page was surfacing stories about the shooting from Sputnik, &lt;a href=&quot;http://www.politico.com/magazine/story/2017/08/21/russian-propaganda-sputnik-reporter-215511&quot;&gt;a known source of Russian propaganda&lt;/a&gt;. &lt;a href=&quot;https://twitter.com/kevinroose/status/914907992637747200&quot;&gt;Their statement&lt;/a&gt; was, like Google’s, designed to minimize what had happened.&lt;/p&gt;
&lt;p&gt;“Our Global Security Operations Center spotted these posts this morning and we have removed them. However, their removal was delayed, allowing them to be screen-captured and circulated online,” a spokesperson responded. “We are working to fix the issue that allowed this to happen in the first place and deeply regret the confusion this caused.”&lt;/p&gt;
&lt;p&gt;All across the information landscape, looking for news about the shooting within the dominant platforms delivered horrifying results. “Managing breaking news is an extremely difficult problem but it's incredible that asking the search box of *every major platform* returns raw toxic sewage,” wrote John Hermann, who covers the platforms for &lt;em&gt;The New York Times&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;For example, he noted that Google’s conglomerate mate at Alphabet, YouTube, was also surfacing absolutely wild things and no respected news organization.&lt;/p&gt;
&lt;p&gt;As news consumers, we can say this: &lt;em&gt;It does not have to be like this&lt;/em&gt;. Imagine a newspaper posting unverified rumors about a shooter from a bunch of readers who had been known to perpetuate hoaxes. There would be hell to pay—and for good reason. The standards of journalism are a set of tools for helping to make sense of chaotic situations, in which bad and good information about an event coexist. These technology companies need to borrow our tools—and hire the people to execute on the principles—or stop saying that they care about the quality of information that they deliver to people.&lt;/p&gt;
&lt;p&gt;There’s no hiding behind algorithms anymore. The problems cannot be minimized. The machines have shown they are not up to the task of dealing with rare, breaking news events, and it is unlikely that they will be in the near future. More humans must be added to the decision-making process, and the sooner the better.&lt;/p&gt;
&lt;/section&gt;</description>
<pubDate>Tue, 03 Oct 2017 00:31:33 +0000</pubDate>
<dc:creator>DLay</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.theatlantic.com/amp/article/541794/?single_page=true</dc:identifier>
</item>
<item>
<title>What&amp;#039;s been wrought using the Piece Table? (2014)</title>
<link>https://web.archive.org/web/20160308183811/http://1017.songtrellisopml.com/whatsbeenwroughtusingpiecetables</link>
<guid isPermaLink="true" >https://web.archive.org/web/20160308183811/http://1017.songtrellisopml.com/whatsbeenwroughtusingpiecetables</guid>
<description>&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;A few months before I decided to try to gain employment with Microsoft back in late 1983, I read about Microsoft Word, which was in its first version for MS-DOS back then.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;The feature I read about that fascinated me the most, even more than the idea that you could select text that you wanted to operate on with a mouse, or that text onscreen was laid out as it would look on the page, with bold, italics, underline and strikethough visible in the display, was it's ability to undo the last operation that you'd made to change your document. And then to redo the operation, if you decided you really needed to make that change after all.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;The first few weeks that I worked at Microsoft before I began to work on Mac Word 1.0 in June 1984, I was asked to review the manual for the soon-to-ship PC Word 2.0 to make sure it was describing what the application was actually doing, and as a way to become familiar with Word's technology in detail.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;When I was able to do experiments with Word's Undo on a copy of Word 2.0 that ran on the IBM PC I was issued, I became more intrigued. No matter how large a text file or Word document was, I found that one could select thousands of lines of a newly opened text file, and copy it into an empty document in a fraction of a second.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;No matter how enormous the copied text was, you could undo that change in a fraction of a second, and if you did redo the operation, it was equally fast.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; withallofthetextedit12970=&quot;&quot;&gt;&lt;a href=&quot;javascript:ec('withAllOfTheTextEdit12970','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;img class=&quot;expandIcon&quot; id=&quot;img_withAllOfTheTextEdit12970&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/dw.gif&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;a onclick=&quot;javascript:ec('withAllOfTheTextEdit12970','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;span class=&quot;spanOutlineText spanExpandableText&quot;&gt;With all of the text editors that I had used up to that point, including some that it been my job to modify and maintain, if you were copying an extremely large selection of text, you had to pay for executing that kind of operation by waiting longer than an eyeblink and sometimes for a good number of breaths for such an operation to complete.&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;show&quot; id=&quot;withAllOfTheTextEdit12970&quot; name=&quot;withAllOfTheTextEdit12970&quot; readability=&quot;33&quot;&gt;
&lt;div class=&quot;divOutlineList&quot; readability=&quot;11&quot;&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;In extreme cases, the wait would be so long, there was time to leave the room to get a soda, or a cup of coffee.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;When I used Word I never had to pay that cost.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;How did the Word team do that? I was really anxious to find out.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;It took several months before my curiousity was satisfied.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; atatimewhenweneededt5334=&quot;&quot;&gt;&lt;a href=&quot;javascript:ec('atATimeWhenWeNeededT5334','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;img class=&quot;expandIcon&quot; id=&quot;img_atATimeWhenWeNeededT5334&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/dw.gif&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;a onclick=&quot;javascript:ec('atATimeWhenWeNeededT5334','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;span class=&quot;spanOutlineText spanExpandableText&quot;&gt;At a time when we needed to know, Charles Simonyi diagrammed on his white board how Word edits worked for me and Rob Horowitz, another novice Microsoft engineer assigned to the Mac Word 1.0 (Sand Word) project.&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;Word's editing speed and ability to Undo and Redo depended on a data structure called a piece table.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;J Strother Moore, a computer scientist who worked at Xerox PARC when Simonyi was on staff in 1974, had invented the piece table as a side-consequence of work he did on representing logic clauses within software that proved theorems.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;He had realized that when one edited a text, it was not necessary to represent the edit result as one long string of characters recorded in a buffer.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; usingthatnaivetextre19949=&quot;&quot;&gt;&lt;a href=&quot;javascript:ec('usingThatNaiveTextRe19949','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;img class=&quot;expandIcon&quot; id=&quot;img_usingThatNaiveTextRe19949&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/dw.gif&quot; border=&quot;0&quot;/&gt;&lt;/a&gt; &lt;a onclick=&quot;javascript:ec('usingThatNaiveTextRe19949','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;span class=&quot;spanOutlineText spanExpandableText&quot;&gt;Using that naive text representation lead to the big waits that occurred when one inserted hundreds of thousands of characters into the middle of existing text.&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;show&quot; id=&quot;usingThatNaiveTextRe19949&quot; name=&quot;usingThatNaiveTextRe19949&quot; readability=&quot;36.5&quot;&gt;
&lt;div class=&quot;divOutlineList&quot; readability=&quot;18&quot;&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;In the naive method, you would have to first make room for the newly added characters in a text buffer by moving all characters at the insertion point to the right by the number of characters that were to be added and then copy the new text into the gap that was created.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;In 1970's software, a process that copied hundreds of thousands of characters, cost time that could be measured by watching a clock's second hand, whenever such an operation occured during a program's execution. This was especially noticeable when a large copy operation required that the copied bytes be reflected to a disk file as a measure to preserve space within a program's precious and limited-in-size internal memory allocation.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;Instead of moving text in a document into one contiguous string, one could, instead, write a small set of records, consisting of only a few bytes of data, that described how a text string was fragmented into pieces as it was edited.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;When text existed already in a file, when that file was opened, a type of record, call it a piece description, could be created that showed where text began in the file, and how many characters were contiguously recorded after that point in the file.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;If newly typed characters needed to be added to the middle of that text, that single piece description could be fragmented into two pieces. The descriptors for those pieces, would then point to the location of the first character of text up to the insertion point, and then to the location of the character after the insertion up to the end of the original text.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;Then a third record could be created that describes the text that was newly typed, which records where that new text was recorded, which could be in memory somelace or in a temporary scratch file rceorded on the computer's file system. That new record could be placed between the beginning and ending piece that bracketed the point of insertion.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; thissequenceofdescri31807=&quot;&quot;&gt;&lt;a href=&quot;javascript:ec('thisSequenceOfDescri31807','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;img class=&quot;expandIcon&quot; id=&quot;img_thisSequenceOfDescri31807&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/dw.gif&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;a onclick=&quot;javascript:ec('thisSequenceOfDescri31807','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;span class=&quot;spanOutlineText spanExpandableText&quot;&gt;This sequence of descriptive records that would summarize the state of an edited document could be called a piece table.&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;show&quot; id=&quot;thisSequenceOfDescri31807&quot; name=&quot;thisSequenceOfDescri31807&quot; readability=&quot;8.5&quot;&gt;
&lt;div class=&quot;divOutlineList&quot; readability=&quot;12&quot;&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;J Moore doesn't use that terminology in his original description of the idea in a Xerox technical report.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;It appears that Simonyi and his engineers who wrote the Bravo word processor were the first to coin the phrase 'piece table' to describe Moore's innovation. I saw that phrase already used to describe document editing data structures in the source code files of PC Word 1.0 and Mac Word 1.0.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;The resulting three record sequence would describe the new sequence of characters in the document.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;Any further editing would fragment the document further, which would require the addition of more piece descriptors to the piece table to describe each fragment that was created.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;If one recorded the original piece record list from before an edit, you could undo an edit by blessing the original piece description that existed before the edit, as the new specification of the document's state.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;If you also remembered the after-edit piece description, before doing an undo operation, one only needed to move that small piece table version around in computer memory, rather than moving all of the chararacters they point to, to accomplish a redo.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;So that was the answer that satisfied my how-did-they-do-that curiosity: shuffle tiny records that describe pieces in a few bytes of memory rather than shuffle actual buffer contents that could contain hundreds of thousand or millions of characters into a new order.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; id=&quot;i591&quot; werethereotherproper7449=&quot;&quot;&gt;&lt;a href=&quot;javascript:ec('wereThereOtherProper7449','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;img class=&quot;expandIcon&quot; id=&quot;img_wereThereOtherProper7449&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/dw.gif&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;a onclick=&quot;javascript:ec('wereThereOtherProper7449','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;span class=&quot;spanOutlineText spanExpandableText&quot;&gt;Were there other properties of the piece table that Microsoft developers were able to exploit for the user's benefit?&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;show&quot; id=&quot;wereThereOtherProper7449&quot; name=&quot;wereThereOtherProper7449&quot; readability=&quot;28.456705930977&quot;&gt;
&lt;div class=&quot;divOutlineList&quot; readability=&quot;52.323620582765&quot;&gt;
&lt;p class=&quot;divOutlineItem&quot; id=&quot;i592&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;One of the banes that early word processor users had to endure was the time that they had to wait whenever they decided to save their documents.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; id=&quot;i593&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;When MacWord ran on early Macs that saved fheir files to that slow variable speed floppy disk drive that shipped with those machines, users always had to wait seconds, sometimes minutes, for that operation to complete, while the disk drive sang and groaned to record the new document file that was being written. Unfortunately, in early versions of Word a save operation forced the abandonment of the piece table representation of a document's character order. Save had to follow the piece table entries and re-represent the document text as that long, long string of characters.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; id=&quot;i594&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;This was where the piper had to be paid. Every character in the document needed to be touched to construct that long string, and every character had to make that trip down to the document's disk image. Once a save ended, the previous piece table was thrown away and a new one consisting of a single piece was created to point to the entire extent of that new text string within he saved file.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; id=&quot;i595&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;Starting with Mac Word 3.0, the beginning of Win Word 1.1's lineage, it was possible to fast save a document. A fast save operation, recorded the piece table structure reflecting the document's current editing state into the document file that was being saved. It would locate any pieces that were newly typed or that were copied from another document, and flushed only those characters and their peroperties down to the saved file.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; id=&quot;i597&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;Imagine a million character document, where the user had typed a 'the' string to correct a grammar mistake in their text and copied one sentence into the document to add support to an argument that they were making in their document. If the document could be fast saved, only the new &quot;the&quot; and the copied sentence content plus properties needed to be copied into the saved file. These fast saved additions accumulated in layers at the end of the original Word document file.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; id=&quot;i596&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;Word preferred to save a document via fast save, if the document had already been completely saved once and if there was still space available in Word's internal memory to accomodate a sizeable piece table. If it could be performed, a small edit to an enormous file could be performed in an eye blink, the performance one expected when software used a piece table to keep track of editing state.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; id=&quot;i598&quot; ifthewordranoutofint4859=&quot;&quot;&gt;&lt;a href=&quot;javascript:ec('ifTheWordRanOutOfInt4859','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;img class=&quot;expandIcon&quot; id=&quot;img_ifTheWordRanOutOfInt4859&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/dw.gif&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;a onclick=&quot;javascript:ec('ifTheWordRanOutOfInt4859','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;span class=&quot;spanOutlineText spanExpandableText&quot;&gt;If the Word ran out of internal memory space, one of its go-to-the-lifeboat actions was to force a full save of fast saved documents that had accumulated large piece tables. By spending the time to reorganize the document's text stream so that its piece table could be discarded, it beacame possible to free up enough memory so that the user could continue to type and edit for a much longer time.&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;show&quot; id=&quot;ifTheWordRanOutOfInt4859&quot; name=&quot;ifTheWordRanOutOfInt4859&quot; readability=&quot;25&quot;&gt;
&lt;div class=&quot;divOutlineList&quot; readability=&quot;45&quot;&gt;
&lt;p class=&quot;divOutlineItem&quot; id=&quot;i599&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;A flag in a the header of a saved Word doc, fib.fComplex, was set to fTrue whenever that document had been fast saved. The arrangement of text in such documents WAS quite complex, because adjacent text characters recorded in the file may no longer be adjacent in the document's text stream. This caused competitors who wished to read Word documents fits, I had heard, because they had to get used to the unusual geometry of fast saved text.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; id=&quot;i600&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;It was also possible that characters that were still stored in the document file, had been deleted since those characters were first copied into the file. That meant if a user of the program had typed embarassing or incriminating text in a Word document, saved it and then thought better of publishing such thoughts, and erased them, that once saved text still was recorded in a Word file. If you used an operating system utility to dump the raw content of a fast saved Word file, this stuff became visible to the technically minded nosy person.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; id=&quot;i601&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;These were both disadvanatages of providing a fast save facility in Word.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; id=&quot;i602&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;The overriding advantage that made up for these detrments was that the feature could save an intensive user of Word hundreds of hours of otherwise wasted save time per year, which would have interrupted the flow of their writing and document production. I imagine it's effect, as Word gained large market share in the early 90's and before disk drives sped up massively, might been visible as a tiny sliver of a percent of benefit in econometrician's studies of Gross National Product and national productivity.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; id=&quot;i604&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;Even in the most braggadocious mood, you usually couldn't dream to make such a claim for any feature of a software program. Given the wide adoption of Word around the world, the large number of users involved, the necessity for frequent use of the save operation to preserve a user's word processor work, and the slowness of disk drives for an extended period of years, it's pretty certain that fast saving paid off for society and for Microsoft.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;As a Microsoft sofware developer, I could view the code that performed so marvelously, and I could make small changes to that process to improve its performance and add to the flexibility of its execution in different ways, but I could only show that code to other Microsoft employees, because the details of its operation was protected as a trade secret.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;Microsoft's piece table implementation remained a secret until last week, when the company allowed the Computer History Museum to publish the source code for Windows Word 1.1 on its website for the curious to view.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;Despite the fact that Moore's piece table invention was crucial to the development of Bravo and Bravo X at PARC and later Microsoft Word after Simonyi moved to Microsoft, the data structure is little noted in public literature.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; itisgivenanotveryspe8701=&quot;&quot;&gt;&lt;a href=&quot;javascript:ec('itIsGivenANotVerySpe8701','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;img class=&quot;expandIcon&quot; id=&quot;img_itIsGivenANotVerySpe8701&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/dw.gif&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;a onclick=&quot;javascript:ec('itIsGivenANotVerySpe8701','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;span class=&quot;spanOutlineText spanExpandableText&quot;&gt;It is given a not-very specific, not really adequate, two sentence description in Wikipedia. Moore's invention of the idea, and its use as foundational technology within Bravo and Word is not mentioned there.&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;show&quot; id=&quot;itIsGivenANotVerySpe8701&quot; name=&quot;itIsGivenANotVerySpe8701&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;divOutlineList&quot; readability=&quot;8&quot;&gt;
&lt;p class=&quot;divOutlineItem&quot; id=&quot;i605&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;It's not Wikipedia's fault. The utility of the piece table could not be verified until instances of these algorithms became available for technical review.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;The discussion page beneath this article indicates that it is rated Low Importance by the WikiProject Computing group within Wikipedia, which is tasked with improving the coverage of computers, computing, and information technology on Wikipedia.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;Given the importance of this data structure to the performance and flexibility of operation of Microsoft Word, a software application that has been used by billions, perhaps this article has been misrated.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; whatsbeenwroughtusin10831=&quot;&quot;&gt;&lt;a href=&quot;javascript:ec('whatSBeenWroughtUsin10831','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;img class=&quot;expandIcon&quot; id=&quot;img_whatSBeenWroughtUsin10831&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/dw.gif&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;a onclick=&quot;javascript:ec('whatSBeenWroughtUsin10831','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;span class=&quot;spanOutlineText spanExpandableText&quot;&gt;What's been wrought using piece tables? Answer: The Word formatted documents created by the billion or so peple who have used different version of Microsoft Word since it's introduction.&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;show&quot; id=&quot;whatSBeenWroughtUsin10831&quot; name=&quot;whatSBeenWroughtUsin10831&quot; readability=&quot;15.5&quot;&gt;
&lt;div class=&quot;divOutlineList&quot; readability=&quot;26&quot;&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;I haven't spoken yet about another aspect of piece tables. There's a field in a piece description, the pcd.prm, that records any formatting change that has been made to the character, paragraph or table properties within the run of text in the document described by that piece description, since the time that piece was first created.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;This 16 bit quantity, if it is non-zero, can point to a heap block in Word's internal heap that describes how the properties of that block differ from the default settings of those properties.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;Using this scheme, hundreds of bytes of property data for a piece could frequently be specified using only a few bytes of descriptive data.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;This whole aspect of Word's design is pretty amazing and deserves study. I promise another article that will describe how this part of the design worked.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p class=&quot;divOutlineItem&quot; id=&quot;i590&quot; atourthroughthepiece23461=&quot;&quot;&gt;&lt;a href=&quot;javascript:ec('aTourThroughThePiece23461','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;img class=&quot;expandIcon&quot; id=&quot;img_aTourThroughThePiece23461&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/dw.gif&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;a onclick=&quot;javascript:ec('aTourThroughThePiece23461','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;span class=&quot;spanOutlineText spanExpandableText&quot;&gt;A tour through the piece table related code for software developers&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;show&quot; id=&quot;aTourThroughThePiece23461&quot; name=&quot;aTourThroughThePiece23461&quot; readability=&quot;22.458195089582&quot;&gt;
&lt;div class=&quot;divOutlineList&quot; readability=&quot;40.757465162575&quot;&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;To see the definition of a single piece table descriptor in the Word 1.1 archive, look for the definition of PCD (piece descriptor) in Opus/Wordtech/doc.h.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; thecompletepiecetabl7536=&quot;&quot;&gt;&lt;a href=&quot;javascript:ec('theCompletePieceTabl7536','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;img class=&quot;expandIcon&quot; id=&quot;img_theCompletePieceTabl7536&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/dw.gif&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;a onclick=&quot;javascript:ec('theCompletePieceTabl7536','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;span class=&quot;spanOutlineText spanExpandableText&quot;&gt;The complete piece table for a document was stored in a structure called a PLCPCD. A PLC was a dynamic generalization of an array (dynamic means its size could grown and shrunk under program control during execution) that packaged into one heap block an array of character positions (CPs) in one-to-one correspondence with an array of PCDs, which was recorded immediately after the array of CPs in the heap block.&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;show&quot; id=&quot;theCompletePieceTabl7536&quot; name=&quot;theCompletePieceTabl7536&quot; readability=&quot;9&quot;&gt;
&lt;div class=&quot;divOutlineList&quot; readability=&quot;13&quot;&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;One could not write a single C expression which would give you access to the data strored past pcd.rgcp. One is not allowed to declare a variable length array that follows another variable length array inside of a C structure definition.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;Instead one could write generic get and set access routines, that would do calculations to reach below the rgcp allocation and grab or stuff a data payload in its own slot within the heap allocation.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;You'll find definition of the PLC also in Opus/Wordtech/doc.h. You'll see that a DOD (document descriptor structure defined in doc.h) defines a hplcpcd, which is a document's piece table. PLCs are created by HplcInit() in Opus/wordtech/clsplc.c. GetPlc() and PutPlc() are the getter and setter routines for that retrieve and send a payload data structure to a PLC.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;FOpenPlc(), which grows and shrinks PLCs, and FInsertInPlc(), which adds a new payload structure to a plc at a given CP coordinate, are found in Opus/wordtech/clsplc.c&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;The distance between two adjacent character positions in the array of CPs was the size of a piece.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;Because it takes n+1 points to determine the bounds of n intervals, the 1-to-1 correspondence maintained within a PLCPCD was between beginning CPs of pieces and the PCDs in the following PCD array.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;The final CP in the CP array was only an end of piece coordinate. Since it did not start a piece, unlike all of the earlier CPs in its array, it did not have a corresponding PCD structure recorded in the array of PCDs.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; mostoftheprimitivero28215=&quot;&quot;&gt;&lt;a href=&quot;javascript:ec('mostOfThePrimitiveRo28215','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;img class=&quot;expandIcon&quot; id=&quot;img_mostOfThePrimitiveRo28215&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/dw.gif&quot; border=&quot;0&quot;/&gt;&lt;/a&gt;&lt;a onclick=&quot;javascript:ec('mostOfThePrimitiveRo28215','show','https://web.archive.org/web/20160308183811/http://i.opml.org/rw.gif','https://web.archive.org/web/20160308183811/http://i.opml.org/dw.gif');&quot;&gt;&lt;span class=&quot;spanOutlineText spanExpandableText&quot;&gt;Most of the primitive routines that acted on a piece table can mostly be found in Opus/edit.c.&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;show&quot; id=&quot;mostOfThePrimitiveRo28215&quot; name=&quot;mostOfThePrimitiveRo28215&quot; readability=&quot;13.5&quot;&gt;
&lt;div class=&quot;divOutlineList&quot; readability=&quot;22&quot;&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;The routine that split a piece table at a specified CP position in preparation for insertion or deletion was called IpcdSplit().&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;The routine that created a new piece to describe text that was newly typed and inserted that new piece into the piece table, in order to replace a target CP range, was called FReplace().&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;Using FReplace() to insert a zero-length piece to replace a CP range, performed a text deletion operation.&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;The routine that was used to copy a run of characters from one document to another, perturbing the destination piece table to record the edit, was called FReplaceCps().&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;The routine that created a single entry piece table that described the entire range of text within a full saved document was FInitPlcpcd() in Opus/create2.c&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;divOutlineItem&quot; id=&quot;i603&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;The routine that performed a fast save operation was FQuickSave(), which can be found in Opus/wordtech/savefast.c&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p class=&quot;divOutlineItem&quot;&gt;&lt;img class=&quot;expandIcon&quot; src=&quot;https://web.archive.org/web/20160308183811im_/http://i.opml.org/bw.gif&quot; border=&quot;0&quot;/&gt;&lt;span class=&quot;spanOutlineText&quot;&gt;If you have any questions about any of this, leave them in the Disqus comment thread below. I'll answer as I notice them.&lt;/span&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 02 Oct 2017 20:32:12 +0000</pubDate>
<dc:creator>punnerud</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://web.archive.org/web/20160308183811/http://1017.songtrellisopml.com/whatsbeenwroughtusingpiecetables</dc:identifier>
</item>
<item>
<title>Y Combinator&amp;#039;s Founding Principles</title>
<link>http://www.ycombinator.com/principles/</link>
<guid isPermaLink="true" >http://www.ycombinator.com/principles/</guid>
<description>&lt;p&gt;Startups are on balance a good thing. Their founders and early employees can be much more productive than they'd be working for an established company. Y Combinator's goal is to cause there to be more startups, by helping founders to start them.&lt;/p&gt;&lt;p&gt;Y Combinator represents the union of two ideas that had not previously been combined: the application of mass production techniques to startup funding. Funding startups in batches is not only more efficient, but also better for founders.&lt;/p&gt;
&lt;p&gt;YC's value is the number of startups we help times how much we help them. Make both factors surprisingly big, and the product will be surprising squared.&lt;/p&gt;
&lt;p&gt;From the point YC funds a startup we should put the founders' interests first, before even our own. That may seem counterintuitive in a for-profit business, but in this business it works; it's more scalable (in much the same way telling the truth is), and empirically the benefits of benevolence are greater than the costs. And since the only way to be consistently benevolent is to actually be a good person, YC's employees must be.&lt;/p&gt;
&lt;p&gt;We must remember that we're investors, not bosses. We can advise and persuade, but not command. Good founders don't need more than advice anyway. And since you can't know what it's like to start a startup without having done it, those who advise the founders should be mostly people who have.&lt;/p&gt;
&lt;p&gt;YC is also a startup itself, and (what's more difficult) must remain one. Many of the startups we fund get their first injection of startup culture from YC, so it's critical that YC practice what it preaches. YC has to be fast, cheap, informal, and focused on essentials. If something seems like the sort of bullshit a big company would engage in, it's probably a mistake. As with benevolence, the first line of defense is hiring.&lt;/p&gt;
&lt;p&gt;The most successful founders are motivated less by money than by a consuming interest in what they're building. YC showed that this principle extends to investing too. What drove us in starting YC was that it seemed a cool hack: that if we helped founders in the earliest stages, there could be a lot more successful startups. That hypothesis turned out to be correct, and it has a long way to run. Focus on helping founders, and everything else will follow.&lt;/p&gt;
&lt;p&gt;Paul Graham&lt;br/&gt;Jessica Livingston&lt;br/&gt;Robert Morris&lt;br/&gt;Trevor Blackwell&lt;/p&gt;
</description>
<pubDate>Mon, 02 Oct 2017 18:13:03 +0000</pubDate>
<dc:creator>craigkerstiens</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.ycombinator.com/principles/</dc:identifier>
</item>
<item>
<title>Streams: a new general purpose data structure in Redis</title>
<link>http://antirez.com/news/114</link>
<guid isPermaLink="true" >http://antirez.com/news/114</guid>
<description>&lt;span class=&quot;info&quot;&gt;&lt;span class=&quot;username&quot;&gt;&lt;a href=&quot;http://antirez.com/user/antirez&quot;&gt;antirez&lt;/a&gt;&lt;/span&gt; 1 day ago. 56188 views.&lt;/span&gt;&lt;pre&gt;
Until a few months ago, for me streams were no more than an interesting and relatively straightforward concept in the context of messaging. After Kafka popularized the concept, I mostly investigated their usefulness in the case of Disque, a message queue that is now headed to be translated into a Redis 4.2 module. Later I decided that Disque was all about AP messaging, which is, fault tolerance and guarantees of delivery without much efforts from the client, so I decided that the concept of streams was not a good match in that case.

However, at the same time, there was a problem in Redis, that was not taking me relaxed about the data structures exported by default. There is some kind of gap between Redis lists, sorted sets, and Pub/Sub capabilities. You can kindly use all these tools in order to model a sequence of messages or events, but with different tradeoffs. Sorted sets are memory hungry, can’t model naturally the same message delivered again and again, clients can’t block for new messages. Because a sorted set is not a sequential data structure, it’s a set where elements can be moved around changing their scores: no wonder if it was not a good match for things like time series. Lists have different problems creating similar applicability issues in certain use cases: you cannot explore what is in the middle of a list because the access time in that case is linear. Moreover no fan-out is possible, blocking operations on list serve a single element to a single client. Nor there was a fixed element identifier in lists, in order to say: given me things starting from that element. For one-to-many workloads there is Pub/Sub, which is great in many cases, but for certain things you do not want fire-and-forget: to retain a history is important, not just to refetch messages after a disconnection, also because certain list of messages, like time series, are very important to explore with range queries: what were my temperature readings in this 10 seconds range?

The way I tried to address the above problems, was planning a generalization of sorted sets and lists into a unique more flexible data structure, however my design attempts ended almost always in making the resulting data structure ways more artificial than the current ones. One good thing about Redis is that the data structures exported resemble more the natural computer science data structures, than, “this API that Salvatore invented”. So in the end, I stopped my attempts, and said, ok that’s what we can provide so far, maybe I’ll add some history to Pub/Sub, or some more flexibility to lists access patterns in the future. However every time an user approached me during a conference saying “how would you model time series in Redis?” or similar related questions, my face turned green.

Genesis
=======

After the introduction of modules in Redis 4.0, users started to see how to fix this problem themselves. One of them, Timothy Downs, wrote me the following over IRC:

    &amp;lt;forkfork&amp;gt; the module I'm planning on doing is to add a transaction log style data type - meaning that a very large number of subscribers can do something like pub sub without a lot of redis memory growth
    &amp;lt;forkfork&amp;gt; subscribers keeping their position in a message queue rather than having redis maintain where each consumer is up to and duplicating messages per subscriber

This captured my imagination. I thought about it a few days, and realized that this could be the moment when we could solve all the above problems at once. What I needed was to re-imagine the concept of “log”. It is a basic programming element, everybody is used to it, because it’s just as simple as opening a file in append mode and writing data to it in some format. However Redis data structures must be abstract. They are in memory, and we use RAM not just because we are lazy, but because using a few pointers, we can conceptualize data structures and make them abstract, to allow them to break free from the obvious limits. For instance normally a log has several problems: the offset is not logical, but is an actual bytes offset, what if we want logical offsets that are related to the time an entry was inserted? We have range queries for free. Similarly, a log is often hard to garbage collect: how to remove old elements in an append only data structure? Well, in our idealized log, we just say we want at max this number of entries, and the old ones will go away, and so forth.

While I was trying to write a specification starting from the seed idea of Timothy, I was working to a radix tree implementation that I was using for Redis Cluster, to optimize certain parts of its internals. This provided the ground in order to implement a very space efficient log, that was still accessible in logarithmic time to get ranges. At the same time I started reading about Kafka streams to get other interesting ideas that could fit well into my design, and this resulted into getting the concept of Kafka consumer groups, and idealizing it again for Redis and the in-memory use case. However the specification remained just a specification for months, at the point that after some time I rewrote it almost from scratch in order to upgrade it with many hints that I accumulated talking with people about this upcoming addition to Redis. I wanted Redis streams to be a very good use case for time series especially, not just for other kind of events and messaging applications.

Let’s write some code
=====================

Back from Redis Conf, during the summertime, I was implementing a library called “listpack”.  This library is just the successor of ziplist.c, that is, a data structure that can represent a list of string elements inside a single allocation. It’s just a very specialized serialization format, with the peculiarity of being parsable also in reverse order, from right to left: something needed in order to substitute ziplists in all the use cases.

Mixing radix trees + listpacks, it is possible to easily build a log that is at the same time very space efficient, and indexed, that means, allowing for random access by IDs and time. Once this was ready, I started to write the code in order to implement the stream data structure. I’m still finishing the implementation, however at this point, inside the Redis “streams” branch at Github, there is enough to start playing and having fun. I don’t claim that the API is 100% final, but there are two interesting facts: one is that at this point, only the consumer groups are missing, plus a number of less important commands to manipulate the stream, but all the big things are implemented already. The second is the decision to backport all the stream work back into the 4.0 branch in about two months, once everything looks stable. It means that Redis users will not have to wait for Redis 4.2 in order to use streams, they will be available ASAP for production usage. This is possible because being a new data structure, almost all the code changes are self-contained into the new code. With the exception of the blocking list operations: the code was refactored so that we share the same code for streams and lists blocking operations, with a great simplification of the Redis internals.

Tutorial: welcome to Redis Streams
==================================

In some way, you can think at streams as a supercharged version of Redis lists. Streams elements are not just a single string, they are more objects composed of fields and values. Range queries are possible and fast. Each entry in a stream has an ID, which is a logical offset. Different clients can blocking-wait for elements with IDs greater than a specified one. A fundamental command of Redis streams is XADD. Yes, all the Redis stream commands are prefixed by an “X”.

&amp;gt; XADD mystream * sensor-id 1234 temperature 10.5
1506871964177.0

The XADD command will append the specified entry as a new element to the specified stream “mystream”. The entry, in the example above, has two fields: sensor-id and temperature, however each entry in the same stream can have different fields. Using the same field names will just lead to better memory usage. An interesting thing is also that the fields order is guaranteed to be retained. XADD returns the ID of the just inserted entry, because with the asterisk in the third argument, we asked the command to auto-generate the ID. This is almost always what you want, but it is possible also to force a specific ID, for instance in order to replicate the command to slaves and AOF files.

The ID is composed of two parts: a millisecond time and a sequence number. 1506871964177 is the millisecond time, and is just a Unix time with millisecond resolution. The number after the dot, 0, is the sequence number, and is used in order to distinguish entries added in the same millisecond. Both numbers are 64 bit unsigned integers. This means that we can add all the entries we want in a stream, even in the same millisecond. The millisecond part of the ID is obtained using the maximum between the current local time of the Redis server generating the ID, and the last entry inside the stream. So even if, for instance, the computer clock jumps backward, the IDs will continue to be incremental. In some way you can think stream entry IDs as whole 128 bit numbers. However the fact that they have a correlation with the local time of the instance where they are added, means that we have millisecond precision range queries for free.

As you can guess, adding two entries in a very fast way, will result in only the sequence number to be incremented. We can simulate the “fast insertion” simply with a MULTI/EXEC block:

&amp;gt; MULTI
OK
&amp;gt; XADD mystream * foo 10
QUEUED
&amp;gt; XADD mystream * bar 20
QUEUED
&amp;gt; EXEC
1) 1506872463535.0
2) 1506872463535.1

The above example also shows how we can use different fields for different entries without having to specifying any schema initially. What happens however is that every first message of every block (that usually contains something in the range of 50-150 messages) is used as reference, and successive entries having the same fields are compressed with a single flag saying “same fields of the first entry in this block”. So indeed using the same fields for successive messages saves a lot of memory, even when the set of fields slowly change over time.

In order to retrieve data from the stream there are two ways: range queries, that are implemented by the XRANGE command, and streaming, implemented by the XREAD command. XRANGE just fetches a range of items from start to stop, inclusive. So for instance I can fetch a single item, if I know its ID, with:

&amp;gt; XRANGE mystream 1506871964177.0 1506871964177.0
1) 1) 1506871964177.0
   2) 1) &quot;sensor-id&quot;
      2) &quot;1234&quot;
      3) &quot;temperature&quot;
      4) &quot;10.5&quot;

However you can use the special start symbol of “-“ and the special stop symbol of “+” to signify the minimum and maximum ID possible. It’s also possible to use the COUNT option in order to limit the amount of entries returned. A more complex XRANGE example is the following:

&amp;gt; XRANGE mystream - + COUNT 2
1) 1) 1506871964177.0
   2) 1) &quot;sensor-id&quot;
      2) &quot;1234&quot;
      3) &quot;temperature&quot;
      4) &quot;10.5&quot;
2) 1) 1506872463535.0
   2) 1) &quot;foo&quot;
      2) &quot;10&quot;

Here we are reasoning in terms of ranges of IDs, however you can use XRANGE in order to get a specific range of elements in a given time range, because you can omit the “sequence” part of the IDs. So what you can do is to just specify times in milliseconds. The following means: “Give me 10 entries starting from the Unix time 1506872463”:

127.0.0.1:6379&amp;gt; XRANGE mystream 1506872463000 + COUNT 10
1) 1) 1506872463535.0
   2) 1) &quot;foo&quot;
      2) &quot;10&quot;
2) 1) 1506872463535.1
   2) 1) &quot;bar&quot;
      2) &quot;20&quot;

A final important thing to note about XRANGE is that, given that we receive the IDs in the reply, and the immediately successive ID is trivially obtained just incrementing the sequence part of the ID, it is possible to use XRANGE to incrementally iterate the whole stream, receiving for every call the specified number of elements. After the *SCAN family of commands in Redis, that allowed iteration of Redis data structures *despite* the fact they were not designed for being iterated, I avoided to make the same error again.

Streaming with XREAD: blocking for new data
===========================================

XRANGE is perfect when we want to access our stream to get ranges by ID or time, or single elements by ID. However in the case of streams that different clients must consume as data arrives, this is not good enough and would require some form of pooling (that could be a good idea for *certain* applications that just connect from time to time to get data).

The XREAD command is designed in order to read, at the same time, from multiple streams just specifying the ID of the last entry in the stream we got. Moreover we can request to block if no data is available, to be unblocked when data arrives. Similarly to what happens with blocking list operations, but here data is not consumed from the stream, and multiple clients can access the same data at the same time.

This is a canonical example of XREAD call:

&amp;gt; XREAD BLOCK 5000 STREAMS mystream otherstream $ $

And it means: get data from “mystream” and “otherstream”. If no data is available, block the client, with a timeout of 5000 milliseconds. After the STREAMS option we specify the keys we want to listen for, and the last ID we have. However a special ID of “$” means: assume I’ve all the elements that there are in the stream right now, so give me just starting from the next element arriving.

If, from another client, I send the commnad:

&amp;gt; XADD otherstream * message “Hi There”

This is what happens on the XREAD side:

1) 1) &quot;otherstream&quot;
   2) 1) 1) 1506935385635.0
         2) 1) &quot;message&quot;
            2) &quot;Hi There&quot;

We get the key that received data, together with the data received. In the next call, we’ll likely use the ID of the last message received:

&amp;gt; XREAD BLOCK 5000 STREAMS mystream otherstream $ 1506935385635.0

And so forth. However note that with this usage pattern, it is possible that the client will connect again after a very big delay (because it took time to process messages, or for any other reason). In such a case, in the meantime, a lot of messages could pile up, so it is wise to always use the COUNT option with XREAD, in order to make sure the client will not be flooded with messages and the server will not have to lose too much time just serving tons of messages to a single client.

Capped streams
==============

So far so good… however streams at some point have to remove old messages. Fortunately this is possible with the MAXLEN option of the XADD command:

&amp;gt; XADD MAXLEN 1000000 * field1 value1 field2 value2

This basically means, if the stream, after adding the new element is found to have more than 1 million messages, remove old messages so that the length returns back to 1 million elements. It’s just like using RPUSH + LTRIM with lists, but this time we have a built-in mechanism to do so. However note that the above means that every time we add a new message, we have also to incur in the work needed in order to remove a message from the other side of the stream. This takes some CPU, so it is possible to use the “~” symbol before the count in MAXLEN, in order to specify that we are not really demanding *exactly* 1 million messages, but if there are a few more it’s not a big problem:

&amp;gt; XADD MAXLEN ~ 1000000 * foo bar

This way XADD will remove messages only when it can remove a whole node. This will make having the capped stream almost for free compared to vanilla XADD.

Consumer groups (work in progress)
==================================

This is the first of the features that is not already implemented in Redis, but is a work in progress. It is also the idea more clearly inspired by Kafka, even if implemented here in a pretty different way. The gist is that with XREAD, clients can also add a “GROUP &amp;lt;name&amp;gt;” option. Automatically all the clients in the same group will get *different* messages. Of course there could be multiple groups reading from the same stream, in such cases all groups will receive duplicates of the same messages arriving in the stream, but within each group, messages will not be repeated.

An extension to groups is that it will be possible to specify a “RETRY &amp;lt;milliseconds&amp;gt;” option when groups are specified: in this case, if messages are not acknowledged for processing with XACK, they will be delivered again after the specified amount of milliseconds. This provides some best effort reliability to the delivering of the messages, in case the client has no private means to mark messages as processed. This part is a work in progress as well.

Memory usage and saving loading times
=====================================

Because of the design used to model Redis streams, the memory usage is remarkably low. It depends on the number of fields, values, and their lengths, but for simple messages we are at a few millions of messages for every 100 MB of used memory. Moreover, the format is conceived to need very minimal serialization: the listpack blocks that are stored as radix tree nodes, have the same representation on disk and in memory, so they are trivially stored and read. For instance Redis can read 5 million entries from the RDB file in 0.3 seconds.
This makes replication and persistence of streams very efficient.

It is planned to also allow deletion of items in the middle. This is only partially implemented, but the strategy is to mark entries as deleted in the entry flag, and when a given ratio between entries and deleted entires is reached, the block is rewritten to collect the garbage, and if needed it is glued to another adjacent block in order to avoid fragmentation.

Conclusions end ETA
===================

Redis streams will be part of Redis stable in the 4.0 series before the end of the year. I think that this general purpose data structure is going to put a huge patch in order for Redis to cover a lot of use cases that were hard to cover: that means that you had to be creative in order to abuse the current data structures to fix certain problems. One very important use case is time series, but my feeling is that also streaming of messages for other use cases via TREAD is going to be very interesting both as replacement for Pub/Sub applications that need more reliability than fire-and-forget, and for completely new use cases. For now, if you want to start to evaluate the new capabilities in the context of your problems, just fetch the “streams” branch at Github and start playing. After all bug reports are welcome :-)

If you like videos, a real-time session showing streams is here: &lt;a rel=&quot;nofollow&quot; href=&quot;https://www.youtube.com/watch?v=ELDzy9lCFHQ&quot;&gt;https://www.youtube.com/watch?v=ELDzy9lCFHQ&lt;/a&gt;
&lt;/pre&gt;</description>
<pubDate>Mon, 02 Oct 2017 15:14:05 +0000</pubDate>
<dc:creator>darwhy</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>http://antirez.com/news/114</dc:identifier>
</item>
<item>
<title>Diminishing returns of static typing</title>
<link>https://blog.merovius.de/2017/09/12/diminishing-returns-of-static-typing.html</link>
<guid isPermaLink="true" >https://blog.merovius.de/2017/09/12/diminishing-returns-of-static-typing.html</guid>
<description>&lt;p&gt;I often get into discussions with people, where the matter of strictness and expressiveness of a static type system comes up. The most common one, by far, is Go's lack of generics and the resulting necessity to use &lt;code&gt;interface{}&lt;/code&gt; in container types (the &lt;a href=&quot;https://godoc.org/container&quot;&gt;container-subpackages&lt;/a&gt; are obvious cases, but also &lt;a href=&quot;https://godoc.org/context&quot;&gt;context&lt;/a&gt;). When I express my view, that the lack of static type-safety for containers isn't a problem, I am treated with condescending reactions ranging from disbelief to patronizing.&lt;/p&gt;&lt;p&gt;I also often take the &lt;em&gt;other&lt;/em&gt; side of the argument. This happens commonly, when talking to proponents of dynamically typed languages. In particular I got into debates of whether Python would be suitable for a certain use-case. When the lack of static type-safety is brought up, the proponents of Python defend it by pointing out that it now features optional type hints. Which they say make it possible, to reap the benefits of static typing even in a conventionally dynamically typed language.&lt;/p&gt;
&lt;p&gt;This is an attempt to write my thoughts on both of these (though they are not in any way novel or creative) down more thoroughly. Discussions usually don't provide the space for that. They are also often charged and parties are more interested in “winning the argument”, than finding consensus.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;I don't think it's particularly controversial, that static typing in general has advantages, even though actual data about those seems to be &lt;a href=&quot;https://danluu.com/empirical-pl/&quot;&gt;surprisingly hard to come by&lt;/a&gt;. &lt;em&gt;I&lt;/em&gt; certainly believe that, it is why I use Go in the first place. There is a difference of opinion though, in how large and important those benefits are and how much of the behavior of a program must be statically checked to reap those benefits.&lt;/p&gt;
&lt;p&gt;To understand this, we should first make explicit &lt;em&gt;what&lt;/em&gt; the benefits of static type checking are. The most commonly mentioned one is to catch bugs as early in the development process as possible. If a piece of code I write already contains a rigorous proof of correctness in the form of types, just writing it down and compiling it gives me assurance that it will work as intended in all circumstances. At the other end of the spectrum, in a fully dynamic language I will need to write tests exercising all of my code to find bugs. Running tests takes time. Writing &lt;em&gt;good&lt;/em&gt; tests that actually cover all intended behavior is hard. And as it's in general impossible to cover &lt;em&gt;all&lt;/em&gt; possible execution paths, there will always be the possibility of a rare edge-case that we didn't think of testing to trigger a bug in production.&lt;/p&gt;
&lt;p&gt;So, we can think of static typing as increasing the proportion of bug-free lines of code deployed to production. This is of course a simplification. In practice, we would still catch a lot of the bugs via more rigorous testing, QA, canarying and other practices. To a degree we can still subsume these in this simplification though. If we catch a buggy line of code in QA or the canary phase, we are going to roll it back. So in a sense, the proportion of code we wrote that makes it as bug-free into production will still go down. Thus:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;small&quot; src=&quot;https://blog.merovius.de/assets/static_typing_v_good_code.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;This is usually the understanding, that the “more static typing is always better” argument is based on. Checking more behavior at compile time means less bugs in production means more satisfied customers and less being woken up at night by your pager. Everybody's happy.&lt;/p&gt;
&lt;p&gt;Why then is it, that we don't all code in Idris, Agda or a similarly strict language? Sure, the graph above is suggestively drawn to taper off, but it's still monotonically increasing. You'd think that this implies more is better. The answer, of course, is that static typing has a cost and that there is no free lunch.&lt;/p&gt;
&lt;p&gt;The costs of static typing again come in many forms. It requires more upfront investment in thinking about the correct types. It increases compile times and thus the change-compile-test-repeat cycle. It makes for a steeper learning curve. And more often than we like to admit, the error messages a compiler will give us will decline in usefulness as the power of a type system increases. Again, we can oversimplify and subsume these effects in saying that it reduces our speed:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;small&quot; src=&quot;https://blog.merovius.de/assets/static_typing_v_speed.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;This is what we mean when we talk about dynamically typed languages being good for rapid prototyping. In the end, however, what we are usually interested in, is what I'd like to call &lt;em&gt;velocity&lt;/em&gt;: The speed with which we can deploy new features to our users. We can model that as the speed with which we can roll out bug-free code. Graphically, that is expressed as the product of the previous two graphs:&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;small&quot; src=&quot;https://blog.merovius.de/assets/static_typing_v_velocity.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;In practice, the product of these two functions will have a maximum, a sweet spot of maximum velocity. Designing a type system for a programming language is, at least in part, about finding that sweet spot&lt;a href=&quot;https://blog.merovius.de/2017/09/12/diminishing-returns-of-static-typing.html#footnote1&quot;&gt;¹&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now if we are to accept all of this, that opens up a different question: If we are indeed searching for that sweet spot, how do we explain the vast differences in strength of type systems that we use in practice? The answer of course is simple (and I'm sure many of you have already typed it up in an angry response). The curves I drew above are completely made up. Given how hard it is to do empirical research in this space and to actually quantify the measures I used here, it stands to reason that their shape is very much up for interpretation.&lt;/p&gt;
&lt;p&gt;A Python developer might very reasonably believe that optional type-annotations are more than enough to achieve most if not all the advantages of static typing. While a Haskell developer might be much better adapted to static typing and not be slowed down by it as much (or even at all). As a result, the perceived sweet spot can vary widely:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.merovius.de/assets/static_typing_pythonista_v_haskeller.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;What's more, the importance of these factors might vary a lot too. If you are writing avionics code or are programming the control unit for a space craft, you probably want to be pretty darn sure that the code you are deploying is correct. On the other hand, if you are a Silicon Valley startup in your growth-phase, user acquisition will be of a very high priority and you get users by deploying features quicker than your competitors. We can model that, by weighing the factors differently:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.merovius.de/assets/static_typing_startup_v_nasa.png&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Your use case will determine the sweet spot you are looking for and thus the language you will choose. But a language is also designed with a set of use cases in mind and will set its own sweet spot according to that.&lt;/p&gt;
&lt;p&gt;I think when we talk about how strict a type system should be, we need to acknowledge these subjective factors. And it is fine to believe that your perception of one of those curves or how they should be weighted is closer to a hypothetical objective reality than another persons. But you should make that belief explicit and provide a justification of &lt;em&gt;why&lt;/em&gt; your perception is more realistic. Don't just assume that other people view them the same way and then be confused that they do not come to the same conclusions as you.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;Back to Go's type system. In my opinion, Go manages to hit a good sweet spot (that is, its design agrees with my personal preferences on this). To me it seems that Go reaps probably upwards of 90% of the benefits you can get from static typing while still being not too impeding. And while I definitely agree static typing is beneficial, the &lt;em&gt;marginal&lt;/em&gt; benefit of making user-defined containers type-safe simply seems pretty low (even if it's positive). In the end, it would probably be less than 1% of Go code that would get this additional type-checking and it is probably pretty obvious code. And meanwhile, I perceive generics as a language feature pretty costly. So I find it hard to justify a large perceived cost with a small perceived benefit.&lt;/p&gt;
&lt;p&gt;Now, that is not to say I'm not open to be convinced. Just that simply saying “but more type-safety!” is only looking at one side of the equation and isn't enough. You need to acknowledge that there is no free lunch and that this is a tradeoff. You need to accept that your perceptions of how big the benefit of adding static typing is, how much it costs and how important it is are all subjective. If you want to convince me that my perception of their benefit is wrong, the best way would be to provide specific instances of bugs or production crashes caused by a type-assertion on an &lt;code&gt;interface{}&lt;/code&gt; taken out of a container. Or a refactoring you couldn't make because of the lack of type-safety with a specific container. Ideally, this takes the form of an &lt;a href=&quot;https://github.com/golang/go/wiki/ExperienceReports&quot;&gt;experience report&lt;/a&gt;, which I consider an excellent way to talk about engineered tradeoffs.&lt;/p&gt;
&lt;p&gt;Of course you can continue to roll your eyes whenever someone questions your perception of the value-curve of static typing. Or pretend that when I say the &lt;em&gt;marginal&lt;/em&gt; benefit of type-safe containers is small, I am implying that the &lt;em&gt;total&lt;/em&gt; benefit of static typing is small. It's an effective debate-tactic, if your goal is to shut up your opposition. But not if your goal is to convince them and build consensus.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;[1] There is a generous and broad exception for research languages here. If the point of your design is to explore the possibility space of type-systems, matters of practicality can of course often be ignored. &lt;a href=&quot;https://blog.merovius.de/2017/09/12/diminishing-returns-of-static-typing.html#footnote1_back&quot;&gt;⬆&lt;/a&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 02 Oct 2017 15:09:25 +0000</pubDate>
<dc:creator>robgering</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.merovius.de/2017/09/12/diminishing-returns-of-static-typing.html</dc:identifier>
</item>
<item>
<title>Ask HN: Who is hiring? (October 2017)</title>
<link>https://news.ycombinator.com/item?id=15384262</link>
<guid isPermaLink="true" >https://news.ycombinator.com/item?id=15384262</guid>
<description>&lt;tr readability=&quot;0.55737704918033&quot;&gt;&lt;td bgcolor=&quot;#FF6600&quot;&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;
&lt;/tr&gt;&lt;tr readability=&quot;6.5986928104575&quot;&gt;&lt;td&gt;
&lt;table class=&quot;fatitem&quot; border=&quot;0&quot; readability=&quot;4.9490196078431&quot;&gt;&lt;tr class=&quot;athing&quot; id=&quot;15384262&quot; readability=&quot;0&quot;&gt;&lt;td align=&quot;right&quot; valign=&quot;top&quot; class=&quot;title&quot;/&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;title&quot;&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=15384262&quot; class=&quot;storylink&quot;&gt;Ask HN: Who is hiring? (October 2017)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;0.73170731707317&quot;&gt;&lt;td colspan=&quot;2&quot;/&gt;
&lt;td class=&quot;subtext&quot;&gt;&lt;span class=&quot;score&quot; id=&quot;score_15384262&quot;&gt;605 points&lt;/span&gt; by &lt;a href=&quot;https://news.ycombinator.com/user?id=whoishiring&quot; class=&quot;hnuser&quot;&gt;whoishiring&lt;/a&gt; &lt;span class=&quot;age&quot;&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=15384262&quot;&gt;1 day ago&lt;/a&gt;&lt;/span&gt; &lt;span id=&quot;unv_15384262&quot;/&gt; | &lt;a href=&quot;https://news.ycombinator.com/hide?id=15384262&amp;amp;goto=item%3Fid%3D15384262&quot;&gt;hide&lt;/a&gt; | &lt;a href=&quot;https://hn.algolia.com/?query=Ask%20HN%3A%20Who%20is%20hiring%3F%20(October%202017)&amp;amp;sort=byDate&amp;amp;dateRange=all&amp;amp;type=story&amp;amp;storyText=false&amp;amp;prefix&amp;amp;page=0&quot; class=&quot;hnpast&quot;&gt;past&lt;/a&gt; | &lt;a href=&quot;https://www.google.com/search?q=Ask%20HN%3A%20Who%20is%20hiring%3F%20(October%202017)&quot;&gt;web&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/item?id=15384262&quot;&gt;886 comments&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/fave?id=15384262&amp;amp;auth=eabee9de12ef46c776ee5dba1fdf26d2b93ca573&quot;&gt;favorite&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;
&lt;/tr&gt;&lt;tr readability=&quot;11.626552795031&quot;&gt;&lt;td colspan=&quot;2&quot;/&gt;
&lt;td readability=&quot;11.16149068323&quot;&gt;Please lead with the location of the position and include the keywords REMOTE, INTERNS and/or VISA when the corresponding sort of candidate is welcome. When remote work is not an option, include ONSITE. If it isn't a household name, please explain what your company does.
&lt;p&gt;Submitters: please only post if you personally are part of the hiring company—no recruiting firms or job boards. One post per company please.&lt;/p&gt;
&lt;p&gt;Readers: please only email submitters if you personally are interested in the job—no recruiters or sales calls.&lt;/p&gt;
&lt;p&gt;You can also use kristopolous' console script to search the thread: &lt;a href=&quot;https://news.ycombinator.com/item?id=10313519&quot; rel=&quot;nofollow&quot;&gt;https://news.ycombinator.com/item?id=10313519&lt;/a&gt;.&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td colspan=&quot;2&quot;/&gt;
&lt;td&gt;

&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr readability=&quot;0.7448275862069&quot;&gt;&lt;td&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;10&quot; width=&quot;0&quot;/&gt;&lt;br/&gt;&lt;center&gt;&lt;a href=&quot;https://www.ycombinator.com/apply/&quot;&gt;Applications are open for YC Winter 2018&lt;/a&gt;&lt;/center&gt;
&lt;br/&gt;&lt;center&gt;&lt;span class=&quot;yclinks&quot;&gt;&lt;a href=&quot;https://news.ycombinator.com/newsguidelines.html&quot;&gt;Guidelines&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/newsfaq.html&quot;&gt;FAQ&lt;/a&gt; | &lt;a href=&quot;mailto:hn@ycombinator.com&quot;&gt;Support&lt;/a&gt; | &lt;a href=&quot;https://github.com/HackerNews/API&quot;&gt;API&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/security.html&quot;&gt;Security&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/lists&quot;&gt;Lists&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/bookmarklet.html&quot; rel=&quot;nofollow&quot;&gt;Bookmarklet&lt;/a&gt; | &lt;a href=&quot;https://news.ycombinator.com/dmca.html&quot;&gt;DMCA&lt;/a&gt; | &lt;a href=&quot;http://www.ycombinator.com/apply/&quot;&gt;Apply to YC&lt;/a&gt; | &lt;a href=&quot;mailto:hn@ycombinator.com&quot;&gt;Contact&lt;/a&gt;&lt;/span&gt;
&lt;/center&gt;
&lt;/td&gt;
&lt;/tr&gt;</description>
<pubDate>Mon, 02 Oct 2017 15:00:14 +0000</pubDate>
<dc:creator>whoishiring</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://news.ycombinator.com/item?id=15384262</dc:identifier>
</item>
<item>
<title>ZeroNet: Decentralized websites using Bitcoin crypto and the BitTorrent network</title>
<link>https://zeronet.io/</link>
<guid isPermaLink="true" >https://zeronet.io/</guid>
<description>&lt;head&gt;&lt;title&gt;ZeroNet: Decentralized websites using Bitcoin crypto and the BitTorrent network&lt;/title&gt;&lt;meta charset=&quot;utf-8&quot;/&gt;&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=utf-8&quot;/&gt;&lt;link rel=&quot;stylesheet&quot; href=&quot;media/fonts.css&quot; type=&quot;text/css&quot;/&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot;/&gt;&lt;link rel=&quot;preload&quot; href=&quot;media/fonts.css&quot; as=&quot;style&quot;/&gt;&lt;title&gt;facebook&lt;/title&gt;&lt;title&gt;reddit&lt;/title&gt;&lt;title&gt;github&lt;/title&gt;&lt;title&gt;twitter&lt;/title&gt;&lt;title&gt;ghost&lt;/title&gt;&lt;title&gt;lang&lt;/title&gt;&lt;/head&gt;&lt;body id=&quot;readabilityBody&quot; readability=&quot;31.299935358759&quot;&gt;

&lt;div class=&quot;block block-100 head-top&quot;&gt;

&lt;h2&gt;Open, free and uncensorable websites,&lt;br/&gt;using Bitcoin cryptography and BitTorrent network&lt;/h2&gt;





&lt;/div&gt;
&lt;p&gt;
&lt;h3&gt;Peer-to-Peer&lt;/h3&gt;
&lt;h4&gt;Your content distributed directly to other visitors without any central server.&lt;/h4&gt;
&lt;/p&gt;
&lt;div class=&quot;block block-34 block-detail arrow-left block-p2p-detail&quot;&gt;
&lt;ul&gt;&lt;li&gt;
&lt;h5&gt;Uncensored&lt;/h5&gt;
It's nowhere because it's everywhere!&lt;/li&gt;
&lt;li&gt;
&lt;h5&gt;No hosting costs&lt;/h5&gt;
Sites are served by visitors.&lt;/li&gt;
&lt;li&gt;
&lt;h5&gt;Always accessible&lt;/h5&gt;
No single point of failure.&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;
&lt;p&gt;
&lt;h3&gt;Simple&lt;/h3&gt;
&lt;h4&gt;No configuration needed:&lt;br/&gt;Download, unpack and start using it.&lt;/h4&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;h3&gt;.bit domains&lt;/h3&gt;
&lt;h4&gt;Decentralized domains using Namecoin cryptocurrency.&lt;/h4&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;h3&gt;No passwords&lt;/h3&gt;
&lt;h4&gt;Your account is protected by the same cryptography as your Bitcoin wallet.&lt;/h4&gt;
&lt;/p&gt;
&lt;p&gt;
&lt;h3&gt;Fast&lt;/h3&gt;
&lt;h4&gt;Page response time is not limited by your connection speed.&lt;/h4&gt;
&lt;/p&gt;

&lt;div class=&quot;block block-33 block-detail arrow-left block-dynamic-detail&quot;&gt;
&lt;h3&gt;Dynamic content&lt;/h3&gt;
&lt;h4&gt;Real-time updated, multi-user websites.&lt;/h4&gt;

&lt;/div&gt;
&lt;p&gt;
&lt;h3&gt;Works everywhere&lt;/h3&gt;
&lt;h4&gt;Supports any modern browser on&lt;br/&gt;Windows, Linux or Mac platforms.&lt;/h4&gt;
&lt;/p&gt;
&lt;div class=&quot;block block-34 block-anon&quot;&gt;
&lt;h3&gt;Anonymity&lt;/h3&gt;
&lt;h4&gt;You can easily hide your IP address using the Tor network.&lt;/h4&gt;

&lt;/div&gt;
&lt;p&gt;
&lt;h3&gt;Offline&lt;/h3&gt;
&lt;h4&gt;Browse the sites you're seeding even if your internet connection is down.&lt;/h4&gt;
&lt;/p&gt;
&lt;div class=&quot;block block-100 block-opensource&quot;&gt;
&lt;h3&gt;Open Source&lt;/h3&gt;
&lt;h4&gt;Developed by the community for the community.&lt;/h4&gt;

&lt;/div&gt;




&lt;h3 class=&quot;hosted&quot;&gt;Hosted by the wonderful &lt;a href=&quot;http://www.vultr.com/?ref=6955457-3B&quot; target=&quot;_blank&quot;&gt;Vultr.com&lt;/a&gt;&lt;/h3&gt;
&lt;/body&gt;</description>
<pubDate>Mon, 02 Oct 2017 14:27:58 +0000</pubDate>
<dc:creator>csantini</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://zeronet.io/</dc:identifier>
</item>
</channel>
</rss>