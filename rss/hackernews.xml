<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>The impossible dream of USB-C</title>
<link>https://marco.org/2017/10/14/impossible-dream-of-usb-c</link>
<guid isPermaLink="true" >https://marco.org/2017/10/14/impossible-dream-of-usb-c</guid>
<description>&lt;header readability=&quot;0.49514563106796&quot;&gt;
&lt;p&gt;&lt;time datetime=&quot;2017-10-14T15:31:55-04:00&quot; pubdate=&quot;pubdate&quot;&gt;October 14, 2017&lt;/time&gt;&lt;a class=&quot;permalink&quot; title=&quot;Permalink&quot; href=&quot;https://marco.org/2017/10/14/impossible-dream-of-usb-c&quot;&gt;&lt;span class=&quot;noprint&quot;&gt;∞&lt;/span&gt;&lt;span class=&quot;printonly&quot;&gt;https://marco.org/2017/10/14/impossible-dream-of-usb-c&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/header&gt;&lt;p&gt;I love the idea of USB-C: one port and one cable that can replace all other ports and cables. It sounds so simple, straightforward, and unified.&lt;/p&gt;&lt;p&gt;In practice, it’s not even close.&lt;/p&gt;
&lt;p&gt;USB-C normally transfers data by the USB protocol, but it also supports Thunderbolt… sometimes. The 12-inch MacBook has a USB-C port, but it doesn’t support Thunderbolt at all. All other modern MacBook models support Thunderbolt over their USB-C ports… but if you have a 13-inch model, and it has a Touch Bar, then the right-side ports don’t have full Thunderbolt bandwidth.&lt;/p&gt;
&lt;p&gt;If you bought a USB-C cable, it might support Thunderbolt, or it might not. There’s no way to tell by looking at it. There’s usually no way to tell whether a given USB-C device requires Thunderbolt, either — you just need to plug it in and see if it works.&lt;/p&gt;
&lt;p&gt;Much of USB-C’s awesome capability comes from Thunderbolt and other Alternate Modes. But due to their potential bandwidth demands, computers can’t have very many USB-C ports, making it especially wasteful to lose one to a laptop’s own power cable. The severe port shortage, along with the need to connect to non-USB-C devices, inevitably leads many people to need annoying, inelegant, and expensive dongles and hubs.&lt;/p&gt;
&lt;p&gt;While a wide variety of USB-C dongles are available, most use the same handful of unreliable, mediocre chips inside. Some USB-A dongles make Wi-Fi drop on MacBook Pros. Some USB-A devices don’t work properly when adapted to USB-C, or only work in certain ports. Some devices only work when plugged directly into a laptop’s precious few USB-C ports, rather than any hubs or dongles. And reliable HDMI output seems nearly impossible in practice.&lt;/p&gt;
&lt;p&gt;Very few hubs exist to add more USB-C ports, so if you have more than a few peripherals, you can’t just replace all of their cables with USB-C versions. You’ll need a hub that provides multiple USB-A ports instead, and you’ll need to keep your USB-A cables for when you’re plugged into the hub — but also keep USB-C cables or dongles around for everything you might ever need to plug directly into the computer’s ports.&lt;/p&gt;
&lt;p&gt;Hubs with additional USB-C ports might pass Thunderbolt through to them, but usually don’t. Sometimes, they add a USB-C port that can only be used for power passthrough. Many hubs with power passthrough have lower wattage limits than a 13-inch or 15-inch laptop needs.&lt;/p&gt;
&lt;p&gt;Fortunately, USB-C is a great charging standard. Well, it’s more of a collection of standards. USB-C devices can charge via the slow old USB rates, but for higher-powered devices or faster charging, that’s not enough current.&lt;/p&gt;
&lt;p&gt;Many Android phones support Qualcomm’s Quick Charge over USB-C, which is different — usually — from the official, better, newer USB-C Power Delivery (PD) standard. Apple products, some Android phones, and the Nintendo Switch use USB-C PD. Quick Charge devices don’t get any benefit — usually — from PD chargers, and vice versa.&lt;/p&gt;
&lt;p&gt;Your charger, cable, and any standalone batteries you want to use all must support the same charging standard for it to work at full speed.&lt;/p&gt;
&lt;p&gt;Some cables don’t support USB-C PD at all, and most don’t support laptop wattages. Apple’s cable supports USB-C PD charging at high wattages… unless you bought the earlier version that doesn’t. Most standalone batteries sold to date don’t support USB-C PD — there are only a handful on the market so far, and most of them can’t charge a laptop at full speed, unless it’s the 12-inch MacBook.&lt;/p&gt;
&lt;p&gt;You can use USB-C PD to fast-charge an iPhone 8 or iPad Pro with a USB-C to Lightning cable. But it doesn’t work with every USB-PD battery or charger, or every USB-C to Lightning cable, or every iPad Pro.&lt;/p&gt;
&lt;p&gt;And, of course, there’s usually no way to tell at a glance whether a given cable, charger, battery, or device supports USB-C PD or at what wattages.&lt;/p&gt;
&lt;p&gt;It’s comforting to think that over time, this will all settle down and we’ll finally achieve the dream of a single cable and port for everything. But that’s not how technology really works.&lt;/p&gt;
&lt;p&gt;Before today’s USB-C can become ubiquitous and homogeneous, the next protocol or port will come out. We’ll have new, faster USB 4.0 and Thunderbolt 4 standards over the same-looking USB-C ports. We’ll want to move to an even thinner USB-D port. The press will call it “the future” and Apple will celebrate its new laptops that only have a USB-D port — two, if we’re lucky.&lt;/p&gt;
&lt;p&gt;And we’ll have to start over again, buying all new cables, dongles, hubs, chargers, batteries, and displays to adapt it to what we really need.&lt;/p&gt;
&lt;p&gt;Maybe next time, we’ll get it right. But probably not.&lt;/p&gt;
</description>
<pubDate>Sat, 14 Oct 2017 19:32:53 +0000</pubDate>
<dc:creator>okket</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://marco.org/2017/10/14/impossible-dream-of-usb-c</dc:identifier>
</item>
<item>
<title>The software engineering notebook</title>
<link>http://winterflower.github.io/2017/08/17/software-engineering-notebook/</link>
<guid isPermaLink="true" >http://winterflower.github.io/2017/08/17/software-engineering-notebook/</guid>
<description>&lt;span class=&quot;post-date&quot;&gt;17 Aug 2017&lt;/span&gt;
&lt;p&gt;Fellow software engineers/hackers/devs/code gardeners, do you keep a notebook (digital or plain dead-tree version) to record things you learn?&lt;/p&gt;
&lt;p&gt;Since my days assembling glassware and synthesizing various chemicals in the organic chemistry lab, I’ve found keeping notes to be an indispensable tool at getting better and remembering important lessons learned. One of my professors recommended writing down, after every lab sessions, what had been accomplished and what needed to be done next time. When lab sessions are few and far apart (weekly instead of daily), it is easy to forget the details (for example, the mistakes that were made during weighing of chemicals ). A good quick summary helps with this!&lt;/p&gt;
&lt;p&gt;When I first started working for a software company, I was overwhelmed. Academic software development was indeed very different to large scale distributed software development. For example, the academic software I wrote was rarely version controlled and had few tests. I had never heard of a ‘build’ or DEV/QA/PROD environments, not to mention things like Gradle or Jenkins. The academic software I worked on was distributed in zip files and usually edited by only one person (usually the original author). The systems I started working on were simultaneously developed by tens of developers across the globe.&lt;/p&gt;
&lt;p&gt;To deal with the newbie developer info-flood, I went back to the concept of a ‘software engineering lab notebook’. At first, I jotted down commands needed to setup proper compilation flags for the dev environment and how to run the build locally to debug errors. A bit later, I started jotting down diagrams of the internals of the systems I was working on and summaries of code snippets that I had found particularly thorny to understand. Sometimes these notes proved indispensable in under-stress debug scenarios when I needed to quickly revisit what was happening in a particular area of the codebase without the luxury of a long debug.&lt;/p&gt;
&lt;p&gt;In addition to keeping a record of things that can make your development and debug life easier, a software engineering lab notebook can serve as a good way to learn from previous mistakes. When I revisit some of the code I wrote a year ago or even a few months ago, I often cringe. It’s the same feeling as when you read a draft of a hastily written essay or work of fiction and then approach it again with fresh eyes. All of the great ideas suddenly seem - well- less than great. For example, recently I was looking at a server side process that I wrote to perform computations on a stream of events (coming via ZeroMQ connection from another server ) and saw that for some reason I had included a logging functionality that looped through every single item in an update (potentially 100s ) and wrote a log statement with the data! Had the rate of events been higher, this could have caused some performance issues, though the exact quantification of the impact still remains an area where I need to improve. Things such as these go into the notebook to the ‘avoid-in-the-future-list’.&lt;/p&gt;
</description>
<pubDate>Sat, 14 Oct 2017 19:13:19 +0000</pubDate>
<dc:creator>Winterflow3r</dc:creator>
<dc:language>en-us</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://winterflower.github.io/2017/08/17/software-engineering-notebook/</dc:identifier>
</item>
<item>
<title>Some Insights from a Julia Developer</title>
<link>http://www.stochasticlifestyle.com/like-julia-scales-productive-insights-julia-developer/</link>
<guid isPermaLink="true" >http://www.stochasticlifestyle.com/like-julia-scales-productive-insights-julia-developer/</guid>
<description>&lt;p&gt;In this post I would like to reflect a bit on Julia. These are my personal views and I have had more than a year developing a lot of packages for the Julia programming language. &lt;a href=&quot;https://www.reddit.com/r/Julia/comments/71kkom/when_to_use_julia/dndsn4s/&quot;&gt;After roaming around many different languages including R, MATLAB, C, and Python&lt;/a&gt;; Julia is finally a language I am sticking to. In this post I would like to explain why. I want to go back through some thoughts about what the current state of the language is, who it's good for, and what changes I would like to see. My opinions changed a lot since first starting to work on Julia, so I'd just like to share the changed mindset one has after using the language deeply.&lt;/p&gt;
&lt;h2&gt;Quick Summary&lt;/h2&gt;
&lt;p&gt;Here's a quick summary of my views.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Julia is not only a fast language, but what makes it unique is how predictable the performance and the compilation process is.&lt;/li&gt;
&lt;li&gt;The language gives you lots of introspection tools to be able to easily isolate issues.&lt;/li&gt;
&lt;li&gt;The opt-in type checking and allowing many different architectures to be fast is a strong bonus for software development, especially when scaling to large software ecosystems, over other scripting languages since it allows you to write simple and self-documenting code.&lt;/li&gt;
&lt;li&gt;Julia's unique features make it easy to make packages which are type-generic and parallel.&lt;/li&gt;
&lt;li&gt;Most of these benefits will be seen by package developers. &quot;Users&quot; will probably not see as much of a difference in their own codes because the majority of their performance will be determined by the packages they use.&lt;/li&gt;
&lt;li&gt;To attract a new wave of users, Julia needs to start taking a &quot;package-first&quot; mentality and push package-level unique features rather than language-level features. Language level is what developers care about, but the majority of programmers are not developers.&lt;/li&gt;
&lt;li&gt;We have all of the basics in Julia, but we need to start showing off (and working towards) how we can be different. Every package should be picking some special features and types to support. Speed is just one feature.&lt;/li&gt;
&lt;li&gt;Julia can have a bright future, but we may need to start advertising and teaching it differently.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;I think that to start, I need to discuss a topic which might be new to newcomers.&lt;/p&gt;
&lt;h2&gt;Julia's JiT is not like other JiTs, and it helps package development&lt;/h2&gt;
&lt;p&gt;LuaJIT is a runtime that just-in-time compiles Lua, so is it the same thing or similar to Julia? What about MATLAB's JIT? The answer is no, Julia's compilation strategy is very different. Other JIT setups for scripting languages use something that's known as a tracing JIT. What these do is they track the commands you are running and then via some algorithm (possibly probabilistic in some setups like LuaJIT) it determines what parts of the code are repetitively ran enough that they warrant compilation, and then code for those specific sets of commands are compiled and when the parser hits those areas again it runs the JIT code.&lt;/p&gt;
&lt;p&gt;Julia on the other hand is almost static. When running any Julia function, it will first take your function calls and then auto-specialize it down to concrete types. So f(x::Number)=x^2 will specialize to Float64 when you call it with f(1.0). Using this type information, it has a compile-time stage where it propagates all type information as far as it can go. So if you internally do x^2, it will replace that with ^(::Float64,2) (the 2 is a compile-time constant), and then since it can determine the types of everything, it will push this all the way down to generate clean LLVM IR (and thus clean assembly code) for x^2 on Float64s. It will cache this compiled version of the function so that way f(2.0) uses the same compiled code (notice that the compilation process only needs the type and not the runtime values!). However, when you then call it with f(1), it compiles a separate version for Int.&lt;/p&gt;
&lt;p&gt;In some sense, this is very different. The obvious difference is that Julia's compilation process is deterministic and is open to introspection. If you do&lt;/p&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap5&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap4&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap3&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap2&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight&quot; readability=&quot;6&quot;&gt;
&lt;div class=&quot;julia&quot; readability=&quot;7&quot;&gt;
&lt;pre class=&quot;de1&quot;&gt;
@code_warntype f(1.0)
@code_llvm f(1.0)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Julia will spit back at you its internal AST of the method specialized on Float64, and then the LLVM IR. The nice thing about this is that you can check every step of the process. I never participated in programming language development conversations before working with Julia, but this makes it easy. You see this kick something out, you find out yourself how to optimize it, and then you can easily ask someone &quot;why doesn't the compiler optimize xyz away here?&quot; and then that leads to code changes (i.e. &quot;oh, array aliasing is preventing compiler optimizations!&quot;), new work on compiler optimizations, and better performance. A recent place where just happened was &lt;a href=&quot;https://discourse.julialang.org/t/help-speeding-up-a-test-progam-from-a-book/6383/16?u=chrisrackauckas&quot;&gt;in this Discourse discussion&lt;/a&gt; where a user points out that a separate faster LLVM instruction could be used than the one that is actually emitted.&lt;/p&gt;
&lt;p&gt;But what I really like about this process is that it's clear and deterministic. I can know exactly what my packages are doing and why. This makes it very easy to optimize packages because you always know what's going on. When trying to piece together 50+ packages like DifferentialEquations.jl, deterministic rules for how to optimize code and easy means of introspection are essential to know what's going on. This kind of transparency is something that's really easy to appreciate after working on something like MATLAB where the internals are opaque.&lt;/p&gt;
&lt;p&gt;There is a tradeoff though. In order for this process to work well it requires being able to propagate type information. This requires that the types of the variables inside of a function are what's known as &quot;type-stable&quot; or &quot;type-inferrable&quot;. For example,&lt;/p&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap5&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap4&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap3&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap2&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;julia&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;de1&quot;&gt;
function g(a,n)
  x = 0
  for i in 1:n
    x += a
  end
end
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;will add a n times. If you call this with g(1.0,10), you'll notice this is not as fast as you'd hope. The reason is because the type of x starts as a Int, and then Int+Float64 produces a Float64 and so x changes types. Think about writing a C-code for doing this: you can't just have a variable change types like that? So what Julia does is compile x in a way such that it is dynamically typed, and the lost assumptions associated with doing this is why the performance drops down to that of Python/MATLAB/R since now it has the same amount of dynamicness as a real scripting language: it's all just a game of how much information is known by the compiler. So the way to think about it is, if your code can be type-stable, Julia will take full advantage of this and make a completely static function and compile that, otherwise it will make the necessary parts dynamic in order to handle the extra features. Together, this is a pretty nifty way to mix ease of use and performance. At any time, you can @code_warntype your function call and it will explicitly highlight which variables are &quot;being dynamic&quot;, and that tells you exactly what to optimize. For example,&lt;/p&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap5&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap4&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap3&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap2&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight&quot; readability=&quot;16.5&quot;&gt;
&lt;div class=&quot;julia&quot; readability=&quot;28&quot;&gt;
&lt;pre class=&quot;de1&quot;&gt;
Variables:
  #self#::#g
  a::Float64
  n::Int64
  i::Int64
  #temp#@_5::Int64
  x::UNION{FLOAT64, INT64}
  #temp#@_7::Core.MethodInstance
  #temp#@_8::Float64
 
Body:
  begin
      x::UNION{FLOAT64, INT64} = 0 # line 3:
      SSAValue(2) = (Base.select_value)((Base.sle_int)(1, n::Int64)::Bool, n::Int64, (Base.sub_int)(1, 1)::Int64)::Int64
      #temp#@_5::Int64 = 1
      5:
      unless (Base.not_int)((#temp#@_5::Int64 === (Base.add_int)(SSAValue(2), 1)::Int64)::Bool)::Bool goto 29
      SSAValue(3) = #temp#@_5::Int64
      SSAValue(4) = (Base.add_int)(#temp#@_5::Int64, 1)::Int64
      #temp#@_5::Int64 = SSAValue(4) # line 4:
      unless (x::UNION{FLOAT64, INT64} isa Int64)::Bool goto 14
      #temp#@_7::Core.MethodInstance = MethodInstance for +(::Int64, ::Float64)
      goto 23
      14:
      unless (x::UNION{FLOAT64, INT64} isa Float64)::Bool goto 18
      #temp#@_7::Core.MethodInstance = MethodInstance for +(::Float64, ::Float64)
      goto 23
      18:
      goto 20
      20:
      #temp#@_8::Float64 = (x::UNION{FLOAT64, INT64} + a::Float64)::Float64
      goto 25
      23:
      #temp#@_8::Float64 = $(Expr(:invoke, :(#temp#@_7), :(Main.+), :(x), :(a)))
      25:
      x::UNION{FLOAT64, INT64} = #temp#@_8::Float64
      27:
      goto 5
      29:
      return
  end::Void
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;This is spitting back and telling me that x is typed as both Float64 and Int64 which is exactly the problem I identified. But have no fear, Julia is based around typing, so it has ways to handle this. For example, zero(a) gives you a 0 in the type of a, so&lt;/p&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap5&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap4&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap3&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap2&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight-wrap&quot;&gt;
&lt;div class=&quot;wp-geshi-highlight&quot; readability=&quot;6.5&quot;&gt;
&lt;div class=&quot;julia&quot; readability=&quot;8&quot;&gt;
&lt;pre class=&quot;de1&quot;&gt;
function g(a,n)
  x = zero(a)
  for i in 1:n
    x += a
  end
end
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;is a good generic type-stable function for any number which has zero and + defined. That's quite unique: I can put floating point numbers in here, symbolic expressions from &lt;a href=&quot;https://github.com/symengine/SymEngine.jl&quot;&gt;SymEngine.jl&lt;/a&gt;, ArbFloats from the user-defined &lt;a href=&quot;https://github.com/JuliaArbTypes/ArbFloats.jl&quot;&gt;ArbFloats.jl&lt;/a&gt; fast arbitrary precision library, etc. Since Julia always auto-specializes on the types, no matter what I give it, it will know how to replace zero with whatever it needs to and do so at compile-time since at compile-time there's enough type information to know what that constant should be, and then it will replace the + with calls to the correct version of +. Thus there is no runtime overhead for handling genericness. All you have to do is remember to write your code in a manner such that the types match what comes in (other helpful functions for this include typeof, eltype, and similar). So Julia is not only built to make generic functions performant, but it also has the right tools for handling types to make this actually easy to do.&lt;/p&gt;
&lt;p&gt;There is a little bit more of a cognitive burden here than in other languages. It's not much, &lt;a href=&quot;http://www.stochasticlifestyle.com/7-julia-gotchas-handle/&quot;&gt;but there are a few things to keep in mind&lt;/a&gt;. Tracing JITs will more automatically handle typing by using runtime information, but are harder to introspect and harder to determine when things are going wrong (also, it cannot fully statically compile ahead of time). With Julia, you do need to understand and think about your code in terms of types if you want full performance. But if you do so, then your code isn't &quot;close to C&quot;, it literally has enough information that it can create and compile a function which is as fast as C (and any speed difference from C is usually due to the fact that you're using the LLVM (clang) compiler instead of the more standard GCC, or Julia in some cases is missing the ability to add an optimization. Extra compiler optimizations are part of the 1.x milestones. In practice you'll find that most codes are &amp;lt;2x from C, with the vast majority being close to 1x and then a few being higher due to reasons which already have open issues and many times work in progress PRs scheduled for a 1.x). But this reduction to types and inference has one crucial advantage: it scales really well. This reliance on types (and optional type declarations) makes it easier to scale because Julia gives you lots of information if types don't line up like you think they should. And it's deterministic. While it doesn't do static analysis like C++, type declarations and method dispatch declarations allow your code to throw type warnings when necessary, and the stack trace gives you tons of information about missing methods (this kind of information could be used to give warnings in an IDE though, which is something I'd like to see). From this, it's easy to know when types are wrong. &lt;a href=&quot;https://realmensch.org/2016/05/28/goodbye-lua/&quot;&gt;The inability to handle types well&lt;/a&gt; is one of the main reasons I see developers having hard times scaling up programs in scripting languages, but it's not an issue in Julia. So while LuaJIT and other well-designed tracing JITs can in many cases match Julia's speed with enough work optimizing runtime heuristics, they won't have the same ease of understanding and predictability as Julia, and won't have as much of a robust type-checking system to plug into as your program and contributor base grows.&lt;/p&gt;
&lt;p&gt;Sure, Numba uses a similar strategy as Julia. If you stick to only using Float64s and a few other basic types (Float32, and some integer types?) you'll get something very similar to what's described here. But I've described before that &lt;a href=&quot;http://www.stochasticlifestyle.com/type-dispatch-design-post-object-oriented-programming-julia/&quot;&gt;making robust generic software with this type-inference approach&lt;/a&gt; requires a strong understanding and use of the type system, and I have never found Python+Numba close to matching Julia in its ability to let the user directly handle types, and it's this combination of the type system + the compilation strategy that makes Julia code scale well. This difference may not be appreciated in one-off scripts, but when building and maintaining robust software I have found it necessary.&lt;/p&gt;
&lt;p&gt;But lastly, using this strategy Julia actually can produce static binaries like compiled C or Fortran code, so I think this fact will come in handy for making Julia into a package development language. Basically, you can write packages in Julia, and someday it will be easy to generate a runtime-free binary (like C/Fortran) others can link to. That leads me to my next point.&lt;/p&gt;
&lt;p&gt;In some ways, I see Julia actually as a more productive C++ instead of a faster Python. At least as a package developer I tend to use it like that, and a lot of its features essentially give you full generic template programming that you'd attempt in C++, just with a lot less code. I think that for many developers, this is a more appropriate way to consider Julia. But that's not Julia's whole audience, which leads to my next point.&lt;/p&gt;
&lt;h2&gt;You Will See The Most Benefits in Package Development&lt;/h2&gt;
&lt;p&gt;How much does this language choice actually effect real use cases? Well, I think it depends on who you are. One thing that you'll notice is that, for the vast majority of users, the performance of your scripts is almost entirely determined by the speed of your packages. This is true in pretty much any scientific computing application that I know of, in Julia, R, MATLAB, Python, etc. If 90% of your time is spent calling functions from DataFrames or JuMP, then the speed of your analysis code really doesn't impact the final runtime all that much.&lt;/p&gt;
&lt;p&gt;So in some sense I don't believe that Julia's performance will directly effect most users of the language (and I think that this is true for R/Python/MATLAB as well). However, where it really manifests itself is indirectly through package development productivity. Simply put, a Python package written in pure Python is slow, like really really slow. Same with R and MATLAB. This impacts the development time and resources since all of the major projects like SciPy, NumPy, and Pandas are almost entirely C++ code underneath. And this also impacts features. It's really hard to make traditional languages handle things like arbitrary precision arithmetic well, and so you'll see that most functionality in things like SciPy do not support all of the glory that Python allows since it's not really Python code! Then Python packages are built on SciPy and its limitations, and that just further propagates the impossibility of extending the algorithms to something more generic. Python in fact has so many issues here that projects have had to internally develop their own JITs in order to get the performance, features, and syntax they want. &lt;a href=&quot;https://github.com/neurophysik/jitcode&quot;&gt;JitCODE&lt;/a&gt; and &lt;a href=&quot;https://github.com/robclewley/pydstool&quot;&gt;PyDSTool&lt;/a&gt; are two ODE solvers which developed DSLs and ways to compile their code. One nice example is &lt;a href=&quot;https://www.youtube.com/watch?v=DBVLcgq2Eg0&quot;&gt;this video on PyTorch&lt;/a&gt; where the developer describes the tracing JIT they built into their package. I seriously commend these individuals for building such amazing systems, but dear god I do not want to spend my own development time building a compilation scheme for each new scientific project I am involved in! To me, these are language level issues and should be addressed by the language. What the PyTorch developers had to build into their package is what Julia gives you package developers for free, and as a scientist who doesn't want to write compilers, I am super satisfied.&lt;/p&gt;
&lt;p&gt;In Julia, packages can be written in Julia and be performant. As demonstrated above and in &lt;a href=&quot;http://www.stochasticlifestyle.com/type-dispatch-design-post-object-oriented-programming-julia/&quot;&gt;this post&lt;/a&gt;, they can also be type-agnostic or type-generic and still be performant. This means that it's much easier/quicker to develop &quot;good&quot; libraries in Julia, and that it's easy for any of these libraries to support just about any type. An example for this is &lt;a href=&quot;https://github.com/JuliaMath/GenericSVD.jl&quot;&gt;GenericSVD.jl&lt;/a&gt; which just implements an SVD-factorization in Julia, and this compiles to fast code for things which are not supported by common Fortran bindings like BigFloats, ArbFloats, complex numbers, etc. and the code is simple. It's a great simple package which likely didn't take much development time but is already something very unique due to its type-genericness and speed.&lt;/p&gt;
&lt;p&gt;This makes me think that most users should look at Julia a little differently. To me, Julia is a language which is designed to make it easy to roll your own package and have them be performant. If you want to develop libraries, then Julia is great for you and the benchmarks (among other things like multiple dispatch and type-genericness) show why you should choose Julia over Python/R/MATLAB/etc. To me it's almost a no-brainer to choose Julia for your next methods project or package. But if you're a user who uses libraries, you cannot expect a blanket performance difference from your previous language &quot;just&quot; because it's Julia. For example, Pandas is in Python, but it's fast because it's written in C++. Just because you're switching to Julia you shouldn't expect functions on data table objects to be faster (in fact, they might not be since Pandas is pretty well-optimized. One place where the package ecosystem is currently not as well-optimized is plotting, where the plotting libraries need some restructuring to improve performance.). What you can expect is that the development of the associated Julia libraries is more transparent (it's in Julia) and generally performant as well. But also, all of the tools you'd use to help out your own scripts make it easy to find out what's wrong in a library and contribute some code to speed it up. Going from user to contributor and actually helping package development is super trivial and probably the most underappreciated &quot;feature&quot; (or more, side effect of the design) of Julia.&lt;/p&gt;
&lt;p&gt;But given the market for who actually chooses Julia, Julia packages do tend to be fast in the end Julia is really top-of-the-class in many disciplines, though that is not true all around. But the fact that development is fast and easy to write scalable packages in means that you can find a lot of odd unique packages: lots of small numerical linear algebra or new optimization algorithm packages which run performantly and just don't exist anywhere else make up a good chunk of what I find in Julia. To top it off, Julia makes it easy to setup continuous integration in Windows, Mac, and Linux to get your package off the ground and well-tested. &lt;a href=&quot;https://www.youtube.com/watch?v=tx8DRc7_c9I&amp;amp;t=4s&quot;&gt;Here's a video that goes through all of that&lt;/a&gt;. Again, package-development level features.&lt;/p&gt;
&lt;h2&gt;Fast Multiparadigms Makes Maintenance Easier&lt;/h2&gt;
&lt;p&gt;The big thing which has helped me scale packages is that Julia doesn't care how you program. Looping, vectorization, functional styles with lots of recursion, etc.: it doesn't care. You can just choose the form that is natural for your problem, write a code which uses that style, and know it's going to be performant. This is very different than the R/MATLAB/Python mentality of &quot;vectorize everything!&quot;, even if it's not natural to vectorize. I remember doing some very &quot;fun&quot; hacks in MATLAB and Python specifically in order to make good use of vectorized functions, and having to document inside of comments a step-by-step explanation of what the hell the code is actually doing. Many of those times, a 3 line loop which is almost copy-pasted from the paper's psudocode would've done the trick, but in the name of performance I mangled the code. Julia doesn't require this, and I have very much appreciated this when reviewing code from others.&lt;/p&gt;
&lt;p&gt;This is probably the biggest difference. If you have appropriately vectorized code in MATLAB/Python/R, the performance difference from Julia isn't actually too big. There's still a difference because vectorization is particularly wasteful with memory and isn't actually optimal in most cases, but it's within an order of magnitude. However, you are required to program in a specific way in these other languages to get here, and if you're doing anything more than scripting you've probably found that this doesn't scale because, well, sometimes code gets mangled when you force vectorization!&lt;/p&gt;
&lt;h2&gt;There is Beauty in Using the Full Language&lt;/h2&gt;
&lt;p&gt;Sure, Python isn't too bad if for performance you stick to arrays of 64-bit numbers as allowed in NumPy arrays, and you use pre-written functions (C-bindings) in SciPy. But what if you really want to create your own numbers as some object, and you want to build data structures using objects in order to more easily scale your software? This is where this setup begins to take its toll. When saying that a small part of the language is required to be used in order to get good performance, you get pidgeon-holed away from the most intuitive code and have to move towards big arrays of numbers. Instead of &quot;naming&quot; separate arrays, items are thrown into matrices with comments says &quot;1:3 is chemical A, 4:7 is chemical B, ...&quot;, which I did not find scales very well.&lt;/p&gt;
&lt;p&gt;But in Julia, using types is actually performant. Structs in Julia are value-types, which means that they create structures which inline the type-values instead of using indirection via pointers. This means that a Complex{Float64}, which is a struct of two values (the real and imaginary parts), is in memory as a 128-bit chunk with two numbers. This is as performant as if you implemented some kind of intrinsic type in C. You can use this without worry. And mutable structs, while including pointers, are much more performant than objects in most scripting languages because functions auto-specialize on them. The result is that you can make use of all of the language when building software that needs to be performant, once again making it easier to scale projects.&lt;/p&gt;
&lt;h2&gt;GPUs and Parallelism is Straightforward and Generic&lt;/h2&gt;
&lt;p&gt;Serial performance only gets you so far. Scaling up your project means two things: scaling up the size and features of the codebase, and scaling up the computing power. I discussed how Julia excels in the first part, but what about the second? What I have found really great about Julia is that its parallelization is dead simple. Add @threads in front of a loop and done, its multithreaded. @parallel and pmap do distributed parallelism via multiprocessing, and &lt;a href=&quot;http://www.stochasticlifestyle.com/multi-node-parallelism-in-julia-on-an-hpc/&quot;&gt;I have shown how in 5 minutes you can parallelize your code across a whole cluster&lt;/a&gt;. &lt;a href=&quot;http://www.stochasticlifestyle.com/julia-on-the-hpc-with-gpus/&quot;&gt;Explicitly writing GPU kernels&lt;/a&gt; and &lt;a href=&quot;http://www.stochasticlifestyle.com/multiple-gpu-on-the-hpc-with-julia/&quot;&gt;using multiple GPUs&lt;/a&gt; is easy.&lt;/p&gt;
&lt;p&gt;I cannot forget one of my favorite developments as of late: &lt;a href=&quot;https://github.com/JuliaGPU/GPUArrays.jl&quot;&gt;GPUArrays.jl&lt;/a&gt;. Essentially, when you write vectorized (broadcasted) code with a GPUArray, it automatically parallelizes on the GPU. Neat! But this means that any generic function written in this style is already compatible to automatically compile a GPU version. So things like OrdinaryDiffEq.jl (the ODE solvers of DifferentialEquations.jl), even though they don't have a single piece of GPU-specific code in them, compile performant versions of their solvers for GPUs, automatically, because Julia feels like being nice. Like seriously, once you get a strong understanding about how types and dispatching works, Julia is absolutely mindblowing.&lt;/p&gt;
&lt;h2&gt;Summary: Julia Is Great For Package Development, But Users Just See Packages&lt;/h2&gt;
&lt;p&gt;And this leads me to my conclusion. In each of the sections above I note why Julia is great for building packages in. In comparison to the other scripting languages, I find nothing comes close in terms of productivity, scalability, resulting performance, and resulting features. No other languages makes it so easy to make a function which is performant yet doesn't care what number types you use! And being allowed to use the whole language &quot;correctly&quot; means that your code is much easier to understand and grow. If you're looking to publish a package along with your algorithm, Julia is definitely the right place to be. In that sense, this group will see Julia as an easier or more productive C++.&lt;/p&gt;
&lt;p&gt;But for end users throwing together a 100 line script for a data analysis? I don't think that this crowd will actually see as much of a difference between other scripting languages if the packages in the other languages they are using are sufficiently performant (this isn't always true, but let's assume it is). To people who aren't &quot;pros&quot; in the language, it will probably look like it just has a different syntax. It will be a little faster than vectorized code in other languages if code is in type-stable functions, but most of the differences a user will notice will come from the mixture of features and performance of packages. Because of this, I am not sure if marketing the features of the language is actually the best way to approach the general audience. The general audience will be convinced Julia is worthwhile only by the package offering.&lt;/p&gt;
&lt;p&gt;I want to end though with the fact that, since Julia packages are written in Julia, a Julia user is qualified to write, debug, and contribute to packages. I myself never saw myself becoming a package dev until about a year ago, and this transition only was because Julia makes the change so easy (it wasn't any different than the Julia development I was already doing!). While this is a good highlight to people who would read a blog post about programming languages, I still think this is a small niche when considering the average programmer. I don't think that the average programmer sees this as an upside. Most don't have the time to invest in this kind of development, and see that push that &quot;you can do it all yourself!&quot; as a turn-off (even though it's &quot;can&quot; instead of &quot;have to&quot;!). It's a very different crowd to be catered to.&lt;/p&gt;
&lt;h2&gt;My Recommendations&lt;/h2&gt;
&lt;h3&gt;Package Accessibility and Discoverability&lt;/h3&gt;
&lt;p&gt;So in the end, I see Julia as in a state of transition. The way it was marketed before in terms of performance benchmarks, the type-system, etc. are all things which appeal to package developers, and Julia already has had great adoption by developers. But now Julia needs to start targeting general audiences by sharing its packages. There are a few things in the Base Julia language like adding noalias scopes that I would like to see, but pretty much everything (other than a few compiler optimizations) I put as &quot;not essential&quot; now. What I see as essential is making packages more accessible.&lt;/p&gt;
&lt;p&gt;As someone well-aware of the packages which are available, I can tell you that &quot;lack of packages&quot; isn't really a problem with the ecosystem: you can find a great package that does what you're looking for. In fact, this weird idea that Julia doesn't have packages yet leads to some silly chatroom discussions where a newcomer joins the Gitter channel and asks &quot;I wanna contribute to the language... has x been done yet?&quot;, give back 10 example, &quot;y?&quot;, give back 5 examples, &quot;z?&quot;, 5 examples, &quot;oh, I'm surprised how much is already done!&quot;.&lt;/p&gt;
&lt;p&gt;This chat happens quite often, which is fine, but it points to a problem. We really need to work more on &quot;discoverability&quot; and &quot;noob-friendly docs&quot;, both items which I'm not entirely sure what the easiest way to handle are. But, I think that this is what is required for Julia to grow (word choice is on purpose: grow instead of succeed, because Julia to me is clearly already successful enough to sustain a development community and thus invest your own time in it, though the community could grow much more). If we think of newcomers as coming in waves: wave 1 brought in language devs and was extremely successful (as evidenced by the over 500 committers to just the Base language, more than projects like cpython, Cython, or PyPy has ever had!), wave 2 brought in package devs and has been very successful as well, but now we need to gear up for wave 3: the package users. This means that package discoverability is what will bring in the third wave. Tools like &lt;a href=&quot;https://juliaobserver.com/&quot;&gt;JuliaObserver.com&lt;/a&gt; need to be better advertised as standard parts of the Julia-sphere.&lt;/p&gt;
&lt;h3&gt;Package Uniqueness&lt;/h3&gt;
&lt;p&gt;Julia packages should spend more time on their unique parts. Everyone has optimization packages, and people in Python are using things written in C/C++/Fortran like NLopt and IPOPT and so the performance difference is essentially nil in these well-developed cases. But a native Julia package can have other advantages as well. Working out of the box with complex and arbitrary precision numbers, using autodifferentiation, being compatible with DistributedArrays and GPUArrays to allow these forms of parallelism without having to transfer back to the CPU for the optimizer's parts. These features are huge for large classes of researchers (I know that quantum physicists want better complex number support everywhere!) and this is where Julia's packages shine: genericness. Supporting genericness and parallelism should be front and center with every big Julia package, and it should have unique examples to show it off.&lt;/p&gt;
&lt;p&gt;We already have very strong libraries in quite a few domains of scientific computing. Here's what Julia sounds like when, instead of saying &quot;Julia can do ______&quot;, you say some unique things about the libraries:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Constrainted optimization and mathematical programming. &lt;a href=&quot;https://jump.readthedocs.io/en/latest/&quot;&gt;JuMP&lt;/a&gt;'s DSL automatically defines Jacobians and Hessians via autodifferentiation. Its solver independence makes it easy to switch between libraries to choose the most efficient one for the problem.&lt;/li&gt;
&lt;li&gt;Numerical linear algebra. Not only does th standard library expose more of BLAS and LAPACK via special matrix types than other languages, there are many special-purpose libraries for linear solving. &lt;a href=&quot;https://github.com/JuliaMath/IterativeSolvers.jl&quot;&gt;IterativeSolvers.jl&lt;/a&gt; allows you to use any &lt;a href=&quot;https://github.com/Jutho/LinearMaps.jl&quot;&gt;LinearMap&lt;/a&gt;, meaning that by passing a lazy LinearMap it automatically works as matrix-free preconditioned GMRES etc., and there are many (and a growing number) of methods to choose from. This library also supported complex and arbitrary precision numbers. In case you were wondering, there does exist PETSc.jl is a binding to the infamous PETSc library, and it is compatible with MPI.&lt;/li&gt;
&lt;li&gt;Differential equations. &lt;a href=&quot;http://www.stochasticlifestyle.com/comparison-differential-equation-solver-suites-matlab-r-julia-python-c-fortran/&quot;&gt;I've already discussed the feature set of DifferentialEquations.jl&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Autodifferentiation. With &lt;a href=&quot;https://github.com/JuliaDiff/ForwardDiff.jl&quot;&gt;ForwardDiff.jl&lt;/a&gt; and its operator overloading approach, pretty much any Julia code can use forward-mode autodifferentiation without any modifications, including the Base library. This means there's almost no reason to use numerical differentiation of any pure-Julia code! Reverse-mode autodifferentiation exists as well, but I'll leave a note pointing to &lt;a href=&quot;https://www.youtube.com/watch?v=SXJ0ZfawzxA&quot;&gt;the future Cassette.jl&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;&lt;p&gt;I'm not an image processing guy but &lt;a href=&quot;https://juliaimages.github.io/latest/index.html&quot;&gt;Images.jl&lt;/a&gt; is a large library developed by Tim Holy which must have some unique points. A major contributor to Julia is Steven Johnson, the creator of &lt;a href=&quot;http://www.fftw.org/&quot;&gt;FFTW&lt;/a&gt; (and &lt;a href=&quot;https://nlopt.readthedocs.io/en/latest/&quot;&gt;NLopt&lt;/a&gt; which has a &lt;a href=&quot;https://github.com/JuliaOpt/NLopt.jl&quot;&gt;Julia binding&lt;/a&gt;), the widely used FFT library, is a big Julia contributor and there has been discussions about building a generic FFT library in Julia. In addition, &lt;a href=&quot;https://nickhigham.wordpress.com/2015/10/20/the-rise-of-mixed-precision-arithmetic/&quot;&gt;many applied mathematicians are pointing to mixed-precision algorithms&lt;/a&gt; as a big possibility for increasing the performance of scientific applications, and Julia's strong control over types and specialization on types is perfect for handling such algorithms.&lt;/p&gt;
&lt;p&gt;The main thing is that, if we only talk about &quot;Python does this, does Julia do _____?&quot;, then we can only talk about speed. We have lots of unique developments already, so we should change the conversation to highlight the things that packages in Julia can do that other packages cannot.&lt;/p&gt;
&lt;h3&gt;Package Distributions and Branding&lt;/h3&gt;
&lt;p&gt;Another thing I think we should be doing is bundling together packages as distributions. We somewhat do this with the organizations (JuliaStats, JuliaOpt, etc.), but it's not the same as having a single cohesive documentation. People know and respect SciPy. It's a hodgepodge of different things, but you know who's going to look at your bug reports and you've other parts of SciPy and that's why over time you respect it. It's very hard to get that kind of respect for a lone 5 star github package. I think cohesive documentation for orgs and metapackages, much like I have done with &lt;a href=&quot;http://docs.juliadiffeq.org/latest/&quot;&gt;DifferentialEquations.jl&lt;/a&gt; is required in order to get big &quot;mainstream&quot; packages that users can know and trust.&lt;/p&gt;
&lt;h3&gt;&quot;User&quot;-focused Tutorials&lt;/h3&gt;
&lt;p&gt;I think Julia can become big since it gives package developers so many tools to make big performant package ecosystems with ease. But this has made most people in the community focused on talking to other &quot;developers&quot;. We should reach out to &quot;users&quot; with simple examples and tutorials, and understand that most people don't want to contribute to a package or anything like that, but really just want to add a package and use a function to spit out a plot as fast as possible. For the next wave of Julia users, we should show should how the package ecosystem enables this kind of usage, and that's how Julia will grow.&lt;/p&gt;
&lt;p&gt;I believe that we should target teaching resources directly to them, similar to what's seen in other languages. I see workshops for &quot;learn how to do regression in R!&quot;, &quot;learn how to build websites with Shiny!&quot;, &quot;learn how to use Pandas!&quot;, but for Julia I only seem to see &quot;let's learn Julia in depth: how to write fast code and the type system&quot;. We should instead run workshops directly on Optim, JuMP, DifferentialEquations, etc. at various universities where we are already &quot;teaching Julia&quot;, and have it setup as &quot;direct to skill&quot; for specific disciplines instead of teaching fancy language-level features. Although it's hard because I too am a purist in &quot;learn the language and you can do it all!&quot;, I think we need to reach out more to those who only want to do a very specific thing, and train them in Julia for psychology research, Julia for climate models, etc. And honestly, I can't think off of the top of my head a good tutorial that says &quot;here's how to do scientific computing in Julia&quot;, and works through some specific issues and skills that piece together a few important libraries. We need to write some resources along these lines.&lt;/p&gt;
&lt;p&gt;At least, that's what I think. Feel free to agree/disagree in the comments below.&lt;/p&gt;
&lt;div class=&quot;sharedaddy sd-sharing-enabled&quot;&gt;
&lt;div class=&quot;robots-nocontent sd-block sd-social sd-social-icon sd-sharing&quot;&gt;
&lt;h3 class=&quot;sd-title&quot;&gt;Share this:&lt;/h3&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
&lt;h3 class=&quot;jp-relatedposts-headline&quot;&gt;&lt;em&gt;Related&lt;/em&gt;&lt;/h3&gt;
&lt;/p&gt;
</description>
<pubDate>Sat, 14 Oct 2017 11:40:09 +0000</pubDate>
<dc:creator>ptype</dc:creator>
<og:type>article</og:type>
<og:title>I Like Julia Because It Scales and Is Productive: Some Insights From A Julia Developer - Stochastic Lifestyle</og:title>
<og:description>In this post I would like to reflect a bit on Julia. These are my personal views and I have had more than a year developing a lot of packages for the Julia programming language. After roaming around many different languages including R, MATLAB, C, and Python; Julia is finally a language I am sticking to. In this post I would like to explain why. I want to go back through some thoughts about what the current state of the language is, who it's good for, and what changes I would like to see. My opinions changed a lot since first starting to work on Julia, so I'd just like to share the changed mindset one has after using the language deeply. Quick Summary Here's a quick summary of my views. Julia is not only a fast language, but what makes it ... READ MORE</og:description>
<og:url>http://www.stochasticlifestyle.com/like-julia-scales-productive-insights-julia-developer/</og:url>
<og:image>http://www.stochasticlifestyle.com/wp-content/themes/chrisrack/style/faviPic2.PNG</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.stochasticlifestyle.com/like-julia-scales-productive-insights-julia-developer/</dc:identifier>
</item>
<item>
<title>Using the Web Audio API to Make a Modem</title>
<link>https://martinmelhus.com/web-audio-modem/</link>
<guid isPermaLink="true" >https://martinmelhus.com/web-audio-modem/</guid>
<description>&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Lately, I've been working with a client where my development computer is not connected to the Internet. This is a huge inconvenience, as the unavailability of Google and Stack Overflow vastly impact my productivity. Only recently have I begun to grasp how much of my time is actually spent copy/pasting between Visual Studio and the browser.&lt;/p&gt;
&lt;p&gt;My office also features an Internet connected laptop and my development computer expose 3,5 mm jack sockets for audio devices. And thus my problems can be solved! Here's how I made a modem for closing the gap with Web Audio.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PS&lt;/strong&gt; If you just want to try the modem already, head over to the &lt;a href=&quot;https://martme.github.io/webaudio-modem&quot;&gt;live demo&lt;/a&gt;. Also check out the source code on &lt;a href=&quot;https://github.com/martme/webaudio-modem&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;html5tomodem&quot;&gt;HTML5 to Modem&lt;/h2&gt;
&lt;p&gt;Our modern era copy/paste implementation will be based on the &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API&quot;&gt;Web Audio API&lt;/a&gt; which &lt;a href=&quot;http://caniuse.com/#search=web%20audio&quot;&gt;is supported by all major browsers&lt;/a&gt;. Most notably we'll leverage instances of &lt;a href=&quot;https://developer.mozilla.org/en/docs/Web/API/OscillatorNode&quot;&gt;&lt;code&gt;OscillatorNode&lt;/code&gt;&lt;/a&gt; to encode data as an audio signal composed of sinusoids at preselected frequencies. The audio signal is decomposed using an &lt;a href=&quot;https://developer.mozilla.org/en/docs/Web/API/AnalyserNode&quot;&gt;&lt;code&gt;AnalyserNode&lt;/code&gt;&lt;/a&gt; in our gap-closing endeavor.&lt;/p&gt;
&lt;h3 id=&quot;modulation&quot;&gt;Modulation&lt;/h3&gt;
&lt;p&gt;We can convert characters to integer values using ASCII encoding. Our encoding problem can thus be reduced to encoding values in the range 0 through 127.&lt;/p&gt;
&lt;p&gt;With distinct frequencies, &lt;code&gt;f&lt;sub&gt;0&lt;/sub&gt;&lt;/code&gt; through &lt;code&gt;f&lt;/code&gt;6 we can encode the bits comprising a 7-bit signal. We let &lt;code&gt;f&lt;sub&gt;i&lt;/sub&gt;&lt;/code&gt; be included in the encoded number &lt;code&gt;n&lt;/code&gt; if &lt;code&gt;n &amp;amp; (1 &amp;lt;&amp;lt; i)&lt;/code&gt;, i.e. the bit at position &lt;code&gt;i&lt;/code&gt; from the least significant bit in the binary representation of &lt;code&gt;n&lt;/code&gt; equals &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://martinmelhus.com/content/images/2017/09/spectrum-1.png&quot; alt=&quot;Frequency spectrum resulting from encoding the character 'a' (0b1100001)&quot; title=&quot;Frequency spectrum resulting from encoding the character 'a' (0b1100001)&quot;/&gt;&lt;/p&gt;
&lt;h3 id=&quot;demodulation&quot;&gt;Demodulation&lt;/h3&gt;
&lt;p&gt;The decoding party knows which frequencies are used by encoder. Figuring out if these are present in a recorded signal is done by applying a &lt;a href=&quot;https://betterexplained.com/articles/an-interactive-guide-to-the-fourier-transform/&quot;&gt;Fourier transform&lt;/a&gt; to the input signal. Significant peaks in the resulting frequency bins indicates which frequencies are in fact present in the recorded signal, and we can map these bins to the known frequencies.&lt;/p&gt;
&lt;p&gt;Luckily, the Web Audio API handles all the complexities of digital signal processing for us via the &lt;code&gt;AnalyserNode&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&quot;parameters&quot;&gt;Parameters&lt;/h3&gt;
&lt;p&gt;We can find the sampling rate of our &lt;code&gt;AnalyserNode&lt;/code&gt; via the &lt;code&gt;sampleRate&lt;/code&gt; property on our &lt;code&gt;AudioContext&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-javascript&quot;&gt;var audioContext = new AudioContext();
var sampleRate = audioContext.sampleRate; // 44100
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;This means that we'll be able to analyze frequencies up to &lt;code&gt;22050 Hz&lt;/code&gt;, half of the sampling rate. Using a &lt;code&gt;fftwSize&lt;/code&gt; (window size in the Fourier transform) of &lt;code&gt;512&lt;/code&gt; yields &lt;code&gt;256&lt;/code&gt; frequency bins, each representing a range of approximately &lt;code&gt;86.1 Hz&lt;/code&gt;. The gap between our chosen frequencies should far exceed this value. We select the following frequencies &lt;code&gt;f&lt;sub&gt;0&lt;/sub&gt;&lt;/code&gt; through &lt;code&gt;f&lt;/code&gt;7, activated by the associated bitmask.&lt;/p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Bit&lt;/th&gt;
&lt;th&gt;Bit mask&lt;/th&gt;
&lt;th&gt;Frequency (Hz)&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0b0000000&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;392&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0b000000&lt;strong&gt;1&lt;/strong&gt;0&lt;/td&gt;
&lt;td&gt;784&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0b00000&lt;strong&gt;1&lt;/strong&gt;00&lt;/td&gt;
&lt;td&gt;1046.5&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0b0000&lt;strong&gt;1&lt;/strong&gt;000&lt;/td&gt;
&lt;td&gt;1318.5&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0b000&lt;strong&gt;1&lt;/strong&gt;0000&lt;/td&gt;
&lt;td&gt;1568&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0b00&lt;strong&gt;1&lt;/strong&gt;00000&lt;/td&gt;
&lt;td&gt;1864.7&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;0b0&lt;strong&gt;1&lt;/strong&gt;000000&lt;/td&gt;
&lt;td&gt;2093&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0b&lt;strong&gt;1&lt;/strong&gt;0000000&lt;/td&gt;
&lt;td&gt;2637&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;If you want to see the modem in action, check out this &lt;a href=&quot;https://martme.github.io/webaudio-modem/encoder.html&quot;&gt;live of the version encoder&lt;/a&gt;. The corresponding decoder can be found &lt;a href=&quot;https://martme.github.io/webaudio-modem/decoder.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The source code for the live version is available on &lt;a href=&quot;https://github.com/martme/webaudio-modem/&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&quot;encoder&quot;&gt;Encoder&lt;/h3&gt;
&lt;p&gt;First, we need to initialize an audio context. We also create a master gain node, and set the gain to 1.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-javascript&quot;&gt;let audioContext = new AudioContext();
let masterGain = audioContext.createGain();
masterGain.gain.value = 1;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Next up is setting up an oscillator for each of the frequencies, producing a sinusoid at the given frequency.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-javascript&quot;&gt;const frequencies = [392, 784, 1046.5, 1318.5, 1568, 1864.7, 2093, 2637];

let sinusiods = frequencies.map(f =&amp;gt; {
    let oscillator = audioContext.createOscillator();
    oscillator.type = 'sine';
    oscillator.frequency.value = f;
    oscillator.start();
    return oscillator;
});
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Now, we need some means to switch the oscillators on and off, depending on the input signal. We can do this by connecting each of the oscillators to a dedicated gain node, utilizing the &lt;code&gt;volume&lt;/code&gt; property of the gain node to control the oscillator.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-javascript&quot;&gt;let oscillators = frequencies.map(f =&amp;gt; {
    let volume = audioContext.createGain();
    volume.gain.value = 0;
    return volume;
});
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;To connect everything, we pass the oscillator's output &lt;code&gt;sinusoids[i]&lt;/code&gt; to the corresponding gain node &lt;code&gt;oscillators[i]&lt;/code&gt; and the send the output of all the gain nodes to the master gain node &lt;code&gt;masterGain&lt;/code&gt;. Lastly, we send the output from &lt;code&gt;masterGain&lt;/code&gt; to the system audio playback device, referenced by the &lt;code&gt;destination&lt;/code&gt; property of the &lt;code&gt;AudioContext&lt;/code&gt; instance.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-javascript&quot;&gt;sinusoids.forEach((sine, i) =&amp;gt; sine.connect(oscillators[i]));
oscillators.forEach((osc) =&amp;gt; osc.connect(masterGain));
masterGain.connect(audioContext.destination);
&lt;/code&gt;
&lt;/pre&gt;
&lt;h4 id=&quot;encoding&quot;&gt;Encoding&lt;/h4&gt;
&lt;p&gt;The character &lt;code&gt;a&lt;/code&gt; has character code 97 which equals &lt;code&gt;0b1100001&lt;/code&gt;. As for our encoder, this means that the oscillators at index 0, 1 and 6 should be active when encoding this value.&lt;/p&gt;
&lt;p&gt;To retrieve the active oscillators for a given value we have the implement the &lt;code&gt;char2oscillators&lt;/code&gt; function.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-javascript&quot;&gt;const char2oscillators = (char) =&amp;gt; {
    return oscillators.filter((_, i) =&amp;gt; {
        let charCode = char.charCodeAt(0);
        return charCode &amp;amp; (1 &amp;lt;&amp;lt; i);
    });
};
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Now that we've identified which oscillators to activate, we can create a function that encodes individual characters. The &lt;code&gt;mute&lt;/code&gt; function is used to silence all oscillators between each character transmission.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-javascript&quot;&gt;const mute = () =&amp;gt; {
    oscillators.forEach(osc =&amp;gt; {
        osc.gain.value = 0
    });
}

const encodeChar = (char, duration) =&amp;gt; {
    let activeOscillators = char2oscillators(char);
    activeOscillators.forEach(osc =&amp;gt; {
        osc.gain.value = 1;
    });
    window.setTimeout(mute, duration);
}
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Combining the pieces, we end up with the following function to encode strings of text.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-javascript&quot;&gt;const encode = (text) =&amp;gt; {
    const pause = 50;
    const duration = 150;
    // number of chars transmitted per second equals 1000/timeBetweenChars
    const timeBetweenChars = pause + duration;

    text.split('').forEach((char, i) =&amp;gt; {
        window.setTimeout(() =&amp;gt; {
            encodeChar(char, duration);
        }, i * timeBetweenChars);
    });
};
&lt;/code&gt;
&lt;/pre&gt;
&lt;h3 id=&quot;decoder&quot;&gt;Decoder&lt;/h3&gt;
&lt;p&gt;The decoder must use the same set of frequencies as the encoder. We start by initializing the audio context as well as an analyzer node. We set the &lt;code&gt;smoothingTimeConstant&lt;/code&gt; of the analyzer node to &lt;code&gt;0&lt;/code&gt; to ensure that the analyzer only carry information about the most recent window.&lt;br/&gt;The &lt;code&gt;minDecibels&lt;/code&gt; property is set to &lt;code&gt;-58&lt;/code&gt; which helps to improve the signal-to-noise ratio by reducing the amount of background noise accounted for by the analyzer.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-javascript&quot;&gt;const frequencies = [392, 784, 1046.5, 1318.5, 1568, 1864.7, 2093, 2637];

let audioContext = new AudioContext();
let analyser = audioContext.createAnalyser();
analyser.fftSize = 512;
analyser.smoothingTimeConstant = 0.0;
analyser.minDecibels = -58;
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Next up, connect the microphone input to the analyzer node.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-javascript&quot;&gt;navigator.getUserMedia({audio: true}, (stream) =&amp;gt; {
    var microphone = audioContext.createMediaStreamSource(stream);
    microphone.connect(analyser);
}, (err) =&amp;gt; { console.error('[error]', err); });
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The analyzer will require a buffer to store frequency bin data, so we introduce a buffer for the cause. The following utility function is used to decide which bin corresponds to a given frequency, and returns the value of the corresponding bin.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-javascript&quot;&gt;let buffer = new Uint8Array(analyser.frequencyBinCount);

const frequencyBinValue = (f) =&amp;gt; {
    const hzPerBin = (audioContext.sampleRate) / (2*analyser.frequencyBinCount);
    const index = parseInt((f + hzPerBin/2) / hzPerBin);
    return buffer[index];
};
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;We populate the buffer by allowing the Web Audio API work it's Fourier sorcery with the following single line of code.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-javascript&quot;&gt;analyser.getByteFrequencyData(buffer);
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;After having populated the bins, we can deduce which frequencies are active and thus converting this information to a numeric value. We call the value the decoder &lt;code&gt;state&lt;/code&gt;. The &lt;code&gt;isActive&lt;/code&gt; function is used to decide if a frequency bin value is above a chosen threshold, indicating that it was present in the recorded audio. We set the threshold to 124, with a theoretical maximum of 255.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-javascript&quot;&gt;const isActive = (value) =&amp;gt; {
    return value &amp;gt; 124;
};

const getState = () =&amp;gt; {
    return frequencies
        .map(frequencyBinValue)
        .reduce((acc, binValue, idx) =&amp;gt; {
            if (isActive(binValue)) {
                acc += (1 &amp;lt;&amp;lt; idx);
            }
            return acc;
        }, 0);
};
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;We require the state of the decoder to be consistent for multiple generations of frequency bins before we're confident that the state is due to receiving an encoded signal. Decoding is performed by continuously updating the frequency bin data followed by comparing the state output of the decoder to its previous state.&lt;/p&gt;
&lt;pre&gt;
&lt;code class=&quot;language-javascript&quot;&gt;const decode = () =&amp;gt; {
    let prevState = 0;
    let duplicates = 0;

    const iteration = () =&amp;gt; {
        analyser.getByteFrequencyData(buffer);
        let state = getState();

        if (state === prevState) {
            duplicates++;
        }
        else {
            prevState = state;
            duplicates = 0;
        }
        if (duplicates === 10) {
            // we are now confident, and the value can be outputted
            let decoded = String.fromCharCode(state);
            console.log('[output]', decoded);
        }
        // allow a small break before starting next iteration
        window.setTimeout(iteration, 1);
    };
    iteration();
};
decode();
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;And that's about it! Here's what it looks like in action (the microphone is next to the webcam on the top of the screen).&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://martinmelhus.com/content/images/2017/10/decoder.jpg&quot; alt=&quot;Decoder in action&quot; title=&quot;Headphones are placed above the built-in microphone on the PC acting as decoder.&quot;/&gt;&lt;/p&gt;
&lt;h3 id=&quot;disclaimer&quot;&gt;Disclaimer&lt;/h3&gt;
&lt;p&gt;I made this; it's not perfect. As a matter of fact, the signal-to-noise ratio is usually really bad. However, making it was fun, and that's what matters to me!&lt;/p&gt;
</description>
<pubDate>Sat, 14 Oct 2017 11:34:07 +0000</pubDate>
<dc:creator>maaaats</dc:creator>
<og:type>article</og:type>
<og:title>Web Audio Modem</og:title>
<og:description>What do you do when you cannot copy text between computers due to lack of internet connectivity? I built a modem using the Web Audio API, allowing data transfer via audio.</og:description>
<og:url>https://martinmelhus.com/web-audio-modem/</og:url>
<og:image>https://martinmelhus.com/content/images/2017/08/20170823-20170823_114735.jpg</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://martinmelhus.com/web-audio-modem/</dc:identifier>
</item>
<item>
<title>Call for help: fund GIMP development and Libre animation</title>
<link>https://girinstud.io/news/2017/10/call-for-help-fund-gimp-development-libre-animation/</link>
<guid isPermaLink="true" >https://girinstud.io/news/2017/10/call-for-help-fund-gimp-development-libre-animation/</guid>
<description>&lt;p&gt;&lt;em&gt;Too long, didn’t read?&lt;/em&gt; In a few words: our GIMP development + &lt;em&gt;ZeMarmot&lt;/em&gt; production is currently funded barely above 400 € per month, this doesn’t pay the bills, my main computer broke today and Aryeom’s graphics tablet has been working badly for some time now. We are a bit bummed out.&lt;/p&gt;
&lt;p&gt;&lt;img class=&quot;aligncenter size-full wp-image-1818&quot; src=&quot;http://girinstud.io/log/wp-content/uploads/2017/10/07.png&quot; alt=&quot;&quot; width=&quot;200&quot; height=&quot;200&quot; srcset=&quot;https://girinstud.io/log/wp-content/uploads/2017/10/07.png 200w, https://girinstud.io/log/wp-content/uploads/2017/10/07-150x150.png 150w, https://girinstud.io/log/wp-content/uploads/2017/10/07-100x100.png 100w&quot; sizes=&quot;(max-width: 200px) 100vw, 200px&quot;/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So we call for your help!&lt;br/&gt;You can fund GIMP development and ZeMarmot production on &lt;a href=&quot;https://www.patreon.com/zemarmot&quot;&gt;Patreon&lt;/a&gt; or &lt;a href=&quot;https://www.tipeee.com/zemarmot&quot;&gt;Tipeee&lt;/a&gt;!&lt;/strong&gt;&lt;strong&gt;&lt;br/&gt;&lt;/strong&gt;Read below for more.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;If you read us regularly, you know that I am hacking GIMP a lot. We are just a handful of regular developers  in GIMP, I am &lt;a href=&quot;https://www.openhub.net/p/gimp/contributors/summary&quot;&gt;one of them&lt;/a&gt;. My contributions go from &lt;a href=&quot;http://girinstud.io/news/2017/09/gimp-2-9-6-zemarmot/&quot;&gt;regular bug fixes to bigger features&lt;/a&gt;, maintenance of several pieces of code as well as regular code review from contributed patches. I do this in the context of &lt;em&gt;&lt;a href=&quot;http://film.zemarmot.net&quot;&gt;ZeMarmot&lt;/a&gt;&lt;/em&gt; project, with Aryeom Han, director and animator.  We draw on and hack GIMP because we believe in Free Software.&lt;br/&gt;On the side, I also &lt;a href=&quot;https://www.openhub.net/accounts/Jehan&quot;&gt;contribute to a lot of other Free Software&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Our absolutely-not-hidden goal is to be able, one day, to live from hacking Free Software and creating Libre Art. But clearly there is no denying that we are currently failing. With about 400€ a month for 2 people, association &lt;a href=&quot;http://libreart.info/en/&quot;&gt;LILA&lt;/a&gt; can barely pay a few days a month (by the rules, which means a good part of the sum even goes to non-wage labour costs). These 400€ are not even the monthly rent we pay for our 1-room flat (31 m², in the far suburb of Paris); so you would assume well that we don’t live from it. We mostly live off savings and other things to pay the bills. These “other things” also use time we would rather spend on drawing and coding.&lt;/p&gt;
&lt;p&gt;We would indeed enjoy working full-time on &lt;em&gt;ZeMarmot&lt;/em&gt;, creating Free Software and Libre Art for everyone to enjoy. But we are very far from this point.&lt;/p&gt;
&lt;p&gt;The main reason why we have not stopped the project already is that we promised we’d release the pilot. Funders are counting on us. Of course the other reason is that we still hope things will work out and that we will be able to live from what we love. Still the project is done at slow pace because we can’t afford to starve, right? So we are at times demoralized.&lt;/p&gt;
&lt;p&gt;This is why I am doing this call. If you can afford it and believe that improving GIMP is important, then I would propose to fund &lt;em&gt;ZeMarmot&lt;/em&gt; which supports paid development.&lt;br/&gt;Similarly if you want to see more Libre Art, and in particular cool animation films, and maybe later other movies under Libre licenses in professional quality, then I again propose to support &lt;em&gt;ZeMarmot&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://patreon.com/zemarmot&quot;&gt;» Patreon funding «&lt;/a&gt;&lt;br/&gt;&lt;a href=&quot;https://tipeee.com/zemarmot&quot;&gt;» Tipeee funding «&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Our material is dying&lt;/h2&gt;
&lt;p&gt;And so why is this post released today? The situation has been hard for months now, but today it is dire: my laptop just broke. It just won’t turn on. All my data are safe since I do regular backups (and I think the hard drive is still ok anyway), but I don’t have a computer anymore to work on (I am writing this on a 8-year old 32-bit netbook which barely stands opening a few browser tabs!).&lt;/p&gt;
&lt;p&gt;On her side, Aryeom’s graphics tablet has had issues for months. &lt;a href=&quot;http://girinstud.io/news/2016/09/can-you-save-a-wacom-tablet-with-broken-usb-port/&quot;&gt;As you may remember, we partly dealt with them&lt;/a&gt;, but the tablet regularly shuts down for no reason, we have to remove and put back the battery or similar annoying “workarounds”. And we fear that we have to buy a new one soon.&lt;/p&gt;
&lt;p&gt;So that’s what triggered this blog post because I realize how precarious is our situation. We barely get funding for living bills, we eat our savings and now we have (expensive) material issues. So we are calling you all who like Free Software and Libre Art. Do you believe &lt;em&gt;ZeMarmot&lt;/em&gt; is a good thing? Do you believe our project has any meaning and that it should continue for years and years? We believe this, and have believed it for the last 2 years where we have been trying. If you do too, maybe help us a bit, relatively to your means. If you really can’t afford it, at least you can spread the word.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;ZeMarmot&lt;/em&gt; is a wonderful experience for us, and we really don’t want it to have a bitter end (though we won’t regret a second of it).&lt;/p&gt;
&lt;p&gt;Thanks for reading!&lt;/p&gt;
</description>
<pubDate>Sat, 14 Oct 2017 10:46:17 +0000</pubDate>
<dc:creator>avaika</dc:creator>
<og:type>article</og:type>
<og:title>Call for help: fund GIMP development and Libre animation</og:title>
<og:url>https://girinstud.io/news/2017/10/call-for-help-fund-gimp-development-libre-animation/</og:url>
<og:description>Too long, didn’t read? In a few words: our GIMP development + ZeMarmot production is currently funded barely above 400 € per month, this doesn’t pay the bills, my main computer broke to…</og:description>
<og:image>http://girinstud.io/log/wp-content/uploads/2017/10/07.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://girinstud.io/news/2017/10/call-for-help-fund-gimp-development-libre-animation/</dc:identifier>
</item>
<item>
<title>How JavaScript works: Event loop and the rise of Async programming</title>
<link>https://blog.sessionstack.com/how-javascript-works-event-loop-and-the-rise-of-async-programming-5-ways-to-better-coding-with-2f077c4438b5</link>
<guid isPermaLink="true" >https://blog.sessionstack.com/how-javascript-works-event-loop-and-the-rise-of-async-programming-5-ways-to-better-coding-with-2f077c4438b5</guid>
<description>&lt;p name=&quot;f653&quot; id=&quot;f653&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Welcome to post # 4 of the series dedicated to exploring JavaScript and its building components. In the process of identifying and describing the core elements, we also share some rules of thumb we use when building &lt;a href=&quot;https://www.sessionstack.com/&quot; data-href=&quot;https://www.sessionstack.com/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;SessionStack&lt;/a&gt;, a JavaScript application that has to be robust and highly-performant in order to stay competitive.&lt;/p&gt;
&lt;p name=&quot;fbf6&quot; id=&quot;fbf6&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Did you miss the first three chapters? You can find them here:&lt;/p&gt;
&lt;ol class=&quot;postList&quot;&gt;&lt;li name=&quot;7709&quot; id=&quot;7709&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;&lt;a href=&quot;https://blog.sessionstack.com/how-does-javascript-actually-work-part-1-b0bacc073cf?source=collection_home---2------1----------------&quot; data-href=&quot;https://blog.sessionstack.com/how-does-javascript-actually-work-part-1-b0bacc073cf?source=collection_home---2------1----------------&quot; class=&quot;markup--anchor markup--li-anchor&quot; target=&quot;_blank&quot;&gt;An overview of the engine, the runtime, and the call stack&lt;/a&gt;&lt;/li&gt;
&lt;li name=&quot;3598&quot; id=&quot;3598&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;a href=&quot;https://blog.sessionstack.com/how-javascript-works-inside-the-v8-engine-5-tips-on-how-to-write-optimized-code-ac089e62b12e?source=collection_home---2------2----------------&quot; data-href=&quot;https://blog.sessionstack.com/how-javascript-works-inside-the-v8-engine-5-tips-on-how-to-write-optimized-code-ac089e62b12e?source=collection_home---2------2----------------&quot; class=&quot;markup--anchor markup--li-anchor&quot; target=&quot;_blank&quot;&gt;Inside Google’s V8 engine + 5 tips on how to write optimized code&lt;/a&gt;&lt;/li&gt;
&lt;li name=&quot;272a&quot; id=&quot;272a&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;a href=&quot;https://blog.sessionstack.com/how-javascript-works-memory-management-how-to-handle-4-common-memory-leaks-3f28b94cfbec?source=collection_home---2------0----------------&quot; data-href=&quot;https://blog.sessionstack.com/how-javascript-works-memory-management-how-to-handle-4-common-memory-leaks-3f28b94cfbec?source=collection_home---2------0----------------&quot; class=&quot;markup--anchor markup--li-anchor&quot; target=&quot;_blank&quot;&gt;Memory management + how to handle 4 common memory leaks&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;p name=&quot;f139&quot; id=&quot;f139&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;This time we’ll expand on our first post by reviewing the drawbacks to programming in a single-threaded environment and how to overcome them to build stunning JavaScript UIs. As the tradition goes, at the end of the article we’ll share 5 tips on how to write cleaner code with async/await.&lt;/p&gt;
&lt;h4 name=&quot;98f1&quot; id=&quot;98f1&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--h4-strong&quot;&gt;Why having a single thread is a limitation?&lt;/strong&gt;&lt;/h4&gt;
&lt;p name=&quot;7279&quot; id=&quot;7279&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;In the &lt;a href=&quot;https://blog.sessionstack.com/how-does-javascript-actually-work-part-1-b0bacc073cf&quot; data-href=&quot;https://blog.sessionstack.com/how-does-javascript-actually-work-part-1-b0bacc073cf&quot; class=&quot;markup--anchor markup--p-anchor&quot; target=&quot;_blank&quot;&gt;first post&lt;/a&gt; we launched, we pondered over the question &lt;em class=&quot;markup--em markup--p-em&quot;&gt;what happens when you have function calls in the Call Stack that take a huge amount of time to be processed&lt;/em&gt;.&lt;/p&gt;
&lt;p name=&quot;4203&quot; id=&quot;4203&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Imagine, for example, a complex image transformation algorithm that’s running in the browser.&lt;/p&gt;
&lt;p name=&quot;2239&quot; id=&quot;2239&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;While the Call Stack has functions to execute, the browser can’t do anything else — it’s being blocked. This means that the browser can’t render, it can’t run any other code, it’s just stuck. And here comes the problem — your app UI is no longer efficient and pleasing.&lt;/p&gt;
&lt;p name=&quot;7998&quot; id=&quot;7998&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Your app &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;is stuck&lt;/strong&gt;.&lt;/p&gt;
&lt;p name=&quot;2029&quot; id=&quot;2029&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In some cases, this might not be such a critical issue. But hey — here’s an even bigger problem. Once your browser starts processing too many tasks in the Call Stack, it may stop being responsive for a long time. At that point, a lot of browsers would take action by raising an error, asking whether they should terminate the page:&lt;/p&gt;
&lt;p name=&quot;cde0&quot; id=&quot;cde0&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;It’s ugly, and it completely ruins your UX:&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*MCt4ZC0dMVhJsgo1u6lpYw.jpeg&quot; data-width=&quot;461&quot; data-height=&quot;283&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*MCt4ZC0dMVhJsgo1u6lpYw.jpeg&quot;/&gt;&lt;/div&gt;
&lt;h4 name=&quot;5851&quot; id=&quot;5851&quot; class=&quot;graf graf--h4 graf-after--figure&quot;&gt;&lt;strong class=&quot;markup--strong markup--h4-strong&quot;&gt;The building blocks of a JavaScript program&lt;/strong&gt;&lt;/h4&gt;
&lt;p name=&quot;5b39&quot; id=&quot;5b39&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;You may be writing your JavaScript application in a single .js file, but your program is almost certainly comprised of several blocks, only one of which is going to execute &lt;em class=&quot;markup--em markup--p-em&quot;&gt;now&lt;/em&gt;, and the rest will execute &lt;em class=&quot;markup--em markup--p-em&quot;&gt;later&lt;/em&gt;. The most common block unit is the function.&lt;/p&gt;
&lt;p name=&quot;edb4&quot; id=&quot;edb4&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The problem most developers new to JavaScript seem to have is understanding that &lt;em class=&quot;markup--em markup--p-em&quot;&gt;later&lt;/em&gt; doesn’t necessarily happen strictly and immediately after &lt;em class=&quot;markup--em markup--p-em&quot;&gt;now&lt;/em&gt;. In other words, tasks that cannot complete &lt;em class=&quot;markup--em markup--p-em&quot;&gt;now&lt;/em&gt; are, by definition, going to complete asynchronously, which means you won’t have the above-mentioned blocking behavior as you might have subconsciously expected or hoped for.&lt;/p&gt;
&lt;p name=&quot;31f3&quot; id=&quot;31f3&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Let’s take a look at the following example:&lt;/p&gt;

&lt;p name=&quot;2292&quot; id=&quot;2292&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;You’re probably aware that standard Ajax requests don’t complete synchronously, which means that at the time of code execution the ajax(..) function does not yet have any value to return back to be assigned to a response variable.&lt;/p&gt;
&lt;p name=&quot;8886&quot; id=&quot;8886&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;A simple way of “waiting” for an asynchronous function to return its result is to use a function called &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;callback:&lt;/strong&gt;&lt;/p&gt;

&lt;p name=&quot;e0f4&quot; id=&quot;e0f4&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Just a note: you can actually make &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;synchronous&lt;/strong&gt; Ajax requests. Never, ever do that. If you make a synchronous Ajax request, the UI of your JavaScript app will be blocked — the user won’t be able to click, enter data, navigate, or scroll. This would prevent any user interaction. It’s a terrible practice.&lt;/p&gt;
&lt;p name=&quot;5588&quot; id=&quot;5588&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;This is how it looks like, but please, never do this — don’t ruin the web:&lt;/p&gt;

&lt;p name=&quot;2ba4&quot; id=&quot;2ba4&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;We used an Ajax request just as an example. You can have any chunk of code execute asynchronously.&lt;/p&gt;
&lt;p name=&quot;0264&quot; id=&quot;0264&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;This can be done with the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;setTimeout(callback, milliseconds)&lt;/code&gt; function. What the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;setTimeout&lt;/code&gt; function does is to set up an event (a timeout) to happen later. Let’s take a look:&lt;/p&gt;

&lt;p name=&quot;2317&quot; id=&quot;2317&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The output in the console will be the following:&lt;/p&gt;
&lt;pre name=&quot;a8dd&quot; id=&quot;a8dd&quot; class=&quot;graf graf--pre graf-after--p&quot;&gt;
first&lt;br/&gt;third&lt;br/&gt;second
&lt;/pre&gt;
&lt;h4 name=&quot;4156&quot; id=&quot;4156&quot; class=&quot;graf graf--h4 graf-after--pre&quot;&gt;&lt;strong class=&quot;markup--strong markup--h4-strong&quot;&gt;Dissecting the Event Loop&lt;/strong&gt;&lt;/h4&gt;
&lt;p name=&quot;572e&quot; id=&quot;572e&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;We’ll start with a somewhat of an odd claim — despite allowing async JavaScript code (like the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;setTimeout&lt;/code&gt; we just discussed), until ES6, JavaScript itself has actually never had any direct notion of asynchrony built into it. The JavaScript engine has never done anything more than executing a single chunk of your program at any given moment.&lt;/p&gt;
&lt;p name=&quot;a479&quot; id=&quot;a479&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;For more details on how JavaScript engines work (Google’s V8 specifically), check one of our &lt;a href=&quot;https://blog.sessionstack.com/how-javascript-works-inside-the-v8-engine-5-tips-on-how-to-write-optimized-code-ac089e62b12e&quot; data-href=&quot;https://blog.sessionstack.com/how-javascript-works-inside-the-v8-engine-5-tips-on-how-to-write-optimized-code-ac089e62b12e&quot; class=&quot;markup--anchor markup--p-anchor&quot; target=&quot;_blank&quot;&gt;previous articles&lt;/a&gt; on the topic.&lt;/p&gt;
&lt;p name=&quot;9e3c&quot; id=&quot;9e3c&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;So, who tells the JS Engine to execute chunks of your program? In reality, the JS Engine doesn’t run in isolation — it runs inside a &lt;em class=&quot;markup--em markup--p-em&quot;&gt;hosting&lt;/em&gt;environment, which for most developers is the typical web browser or Node.js. Actually, nowadays, JavaScript gets embedded into all kinds of devices, from robots to light bulbs. Every single device represents a different type of hosting environment for the JS Engine.&lt;/p&gt;
&lt;p name=&quot;ef5e&quot; id=&quot;ef5e&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The common denominator in all environments is a built-in mechanism called the &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;event loop,&lt;/strong&gt; which handles the execution of multiple chunks of your program over time, each time invoking the JS Engine.&lt;/p&gt;
&lt;p name=&quot;4e3d&quot; id=&quot;4e3d&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;This means that the JS Engine is just an on-demand execution environment for any arbitrary JS code. It’s the surrounding environment that schedules the events (the JS code executions).&lt;/p&gt;
&lt;p name=&quot;e4f0&quot; id=&quot;e4f0&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;So, for example, when your JavaScript program makes an Ajax request to fetch some data from the server, you set up the “response” code in a function (the “callback”), and the JS Engine tells the hosting environment:&lt;br/&gt;“Hey, I’m going to suspend execution for now, but whenever you finish with that network request, and you have some data, please &lt;em class=&quot;markup--em markup--p-em&quot;&gt;call&lt;/em&gt; this function &lt;em class=&quot;markup--em markup--p-em&quot;&gt;back&lt;/em&gt;.”&lt;/p&gt;
&lt;p name=&quot;1301&quot; id=&quot;1301&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The browser is then set up to listen for the response from the network, and when it has something to return to you, it will schedule the callback function to be executed by inserting it into the &lt;em class=&quot;markup--em markup--p-em&quot;&gt;event loop&lt;/em&gt;.&lt;/p&gt;
&lt;p name=&quot;3b0e&quot; id=&quot;3b0e&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Let’s look at the below diagram:&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*FA9NGxNB6-v1oI2qGEtlRQ.png&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*FA9NGxNB6-v1oI2qGEtlRQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*FA9NGxNB6-v1oI2qGEtlRQ.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;42b2&quot; id=&quot;42b2&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;You can read more about the Memory Heap and the Call Stack in our &lt;a href=&quot;https://blog.sessionstack.com/how-does-javascript-actually-work-part-1-b0bacc073cf&quot; data-href=&quot;https://blog.sessionstack.com/how-does-javascript-actually-work-part-1-b0bacc073cf&quot; class=&quot;markup--anchor markup--p-anchor&quot; target=&quot;_blank&quot;&gt;previous article&lt;/a&gt;.&lt;/p&gt;
&lt;p name=&quot;6630&quot; id=&quot;6630&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;And what are these Web APIs? In essence, they are threads that you can’t access, you can just make calls to them. They are the pieces of the browser in which concurrency kicks in. If you’re a Node.js developer, these are the C++ APIs.&lt;/p&gt;
&lt;p name=&quot;1698&quot; id=&quot;1698&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;So what is the &lt;em class=&quot;markup--em markup--p-em&quot;&gt;event loop after all&lt;/em&gt;?&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*KGBiAxjeD9JT2j6KDo0zUg.png&quot; data-width=&quot;478&quot; data-height=&quot;162&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*KGBiAxjeD9JT2j6KDo0zUg.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;2281&quot; id=&quot;2281&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The Event Loop has one simple job — to monitor the Call Stack and the Callback Queue. If the Call Stack is empty, it will take the first event from the queue and will push it to the Call Stack, which effectively runs it.&lt;/p&gt;
&lt;p name=&quot;d321&quot; id=&quot;d321&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Such an iteration is called a &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;tick&lt;/strong&gt; in the Event Loop. Each event is just a function callback.&lt;/p&gt;

&lt;p name=&quot;0b64&quot; id=&quot;0b64&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Let’s “execute” this code and see what happens:&lt;/p&gt;
&lt;ol class=&quot;postList&quot;&gt;&lt;li name=&quot;b3a1&quot; id=&quot;b3a1&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;The state is clear. The browser console is clear, and the Call Stack is empty.&lt;/li&gt;
&lt;/ol&gt;&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*9fbOuFXJHwhqa6ToCc_v2A.png&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*9fbOuFXJHwhqa6ToCc_v2A.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*9fbOuFXJHwhqa6ToCc_v2A.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;d56e&quot; id=&quot;d56e&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;2. &lt;code class=&quot;markup--code markup--p-code&quot;&gt;console.log('Hi')&lt;/code&gt; is added to the Call Stack.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*dvrghQCVQIZOfNC27Jrtlw.png&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*dvrghQCVQIZOfNC27Jrtlw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*dvrghQCVQIZOfNC27Jrtlw.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;4a06&quot; id=&quot;4a06&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;3. &lt;code class=&quot;markup--code markup--p-code&quot;&gt;console.log('Hi')&lt;/code&gt; is executed.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*yn9Y4PXNP8XTz6mtCAzDZQ.png&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*yn9Y4PXNP8XTz6mtCAzDZQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*yn9Y4PXNP8XTz6mtCAzDZQ.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;20b3&quot; id=&quot;20b3&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;4. &lt;code class=&quot;markup--code markup--p-code&quot;&gt;console.log('Hi')&lt;/code&gt; is removed from the Call Stack.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*iBedryNbqtixYTKviPC1tA.png&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*iBedryNbqtixYTKviPC1tA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*iBedryNbqtixYTKviPC1tA.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;5c37&quot; id=&quot;5c37&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;5. &lt;code class=&quot;markup--code markup--p-code&quot;&gt;setTimeout(function cb1() { ... })&lt;/code&gt; is added to the Call Stack.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*HIn-BxIP38X6mF_65snMKg.png&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*HIn-BxIP38X6mF_65snMKg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*HIn-BxIP38X6mF_65snMKg.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;d37c&quot; id=&quot;d37c&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;6. &lt;code class=&quot;markup--code markup--p-code&quot;&gt;setTimeout(function cb1() { ... })&lt;/code&gt; is executed. The browser creates a timer as part of the Web APIs. It is going to handle the countdown for you.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*vd3X2O_qRfqaEpW4AfZM4w.png&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*vd3X2O_qRfqaEpW4AfZM4w.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*vd3X2O_qRfqaEpW4AfZM4w.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;b724&quot; id=&quot;b724&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;7. The &lt;code class=&quot;markup--code markup--p-code&quot;&gt;setTimeout(function cb1() { ... })&lt;/code&gt; itself is complete and is removed from the Call Stack.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*_nYLhoZPKD_HPhpJtQeErA.png&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*_nYLhoZPKD_HPhpJtQeErA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*_nYLhoZPKD_HPhpJtQeErA.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;1a2a&quot; id=&quot;1a2a&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;8. &lt;code class=&quot;markup--code markup--p-code&quot;&gt;console.log('Bye')&lt;/code&gt; is added to the Call Stack.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*1NAeDnEv6DWFewX_C-L8mg.png&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*1NAeDnEv6DWFewX_C-L8mg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*1NAeDnEv6DWFewX_C-L8mg.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;9739&quot; id=&quot;9739&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;9. &lt;code class=&quot;markup--code markup--p-code&quot;&gt;console.log('Bye')&lt;/code&gt; is executed.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*UwtM7DmK1BmlBOUUYEopGQ.png&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*UwtM7DmK1BmlBOUUYEopGQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*UwtM7DmK1BmlBOUUYEopGQ.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;dc8c&quot; id=&quot;dc8c&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;10. &lt;code class=&quot;markup--code markup--p-code&quot;&gt;console.log('Bye')&lt;/code&gt; is removed from the Call Stack.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*-vHNuJsJVXvqq5dLHPt7cQ.png&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*-vHNuJsJVXvqq5dLHPt7cQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*-vHNuJsJVXvqq5dLHPt7cQ.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;09f7&quot; id=&quot;09f7&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;11. After at least 5000 ms, the timer completes and it pushes the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;cb1&lt;/code&gt; callback to the Callback Queue.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*eOj6NVwGI2N78onh6CuCbA.png&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*eOj6NVwGI2N78onh6CuCbA.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*eOj6NVwGI2N78onh6CuCbA.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;4a4c&quot; id=&quot;4a4c&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;12. The Event Loop takes &lt;code class=&quot;markup--code markup--p-code&quot;&gt;cb1&lt;/code&gt; from the Callback Queue and pushes it to the Call Stack.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*jQMQ9BEKPycs2wFC233aNg.png&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*jQMQ9BEKPycs2wFC233aNg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*jQMQ9BEKPycs2wFC233aNg.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;d154&quot; id=&quot;d154&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;13. &lt;code class=&quot;markup--code markup--p-code&quot;&gt;cb1&lt;/code&gt; is executed and adds &lt;code class=&quot;markup--code markup--p-code&quot;&gt;console.log('Inside cb1')&lt;/code&gt; to the Call Stack.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*hpyVeL1zsaeHaqS7mU4Qfw.png&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*hpyVeL1zsaeHaqS7mU4Qfw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*hpyVeL1zsaeHaqS7mU4Qfw.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;7e35&quot; id=&quot;7e35&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;14. &lt;code class=&quot;markup--code markup--p-code&quot;&gt;console.log('cb1')&lt;/code&gt; is executed.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*lvOtCg75ObmUTOxIS6anEQ.png&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*lvOtCg75ObmUTOxIS6anEQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*lvOtCg75ObmUTOxIS6anEQ.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;2fd9&quot; id=&quot;2fd9&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;15. &lt;code class=&quot;markup--code markup--p-code&quot;&gt;console.log('cb1')&lt;/code&gt; is removed from the Call Stack.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*Jyyot22aRkKMF3LN1bgE-w.png&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*Jyyot22aRkKMF3LN1bgE-w.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*Jyyot22aRkKMF3LN1bgE-w.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;259d&quot; id=&quot;259d&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;16. &lt;code class=&quot;markup--code markup--p-code&quot;&gt;cb1&lt;/code&gt; is removed from the Call Stack.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*t2Btfb_tBbBxTvyVgKX0Qg.png&quot; data-width=&quot;1024&quot; data-height=&quot;768&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*t2Btfb_tBbBxTvyVgKX0Qg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*t2Btfb_tBbBxTvyVgKX0Qg.png&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;ff5a&quot; id=&quot;ff5a&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;A quick recap:&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*TozSrkk92l8ho6d8JxqF_w.gif&quot; data-width=&quot;808&quot; data-height=&quot;606&quot; data-is-featured=&quot;true&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*TozSrkk92l8ho6d8JxqF_w.gif&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*TozSrkk92l8ho6d8JxqF_w.gif&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;aec2&quot; id=&quot;aec2&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;It’s interesting to note that ES6 specifies how the event loop should work, meaning that technically it’s within the scope of the JS engine’s responsibilities, which is no longer playing just a hosting environment role. One main reason for this change is the introduction of Promises in ES6 because the latter require access to a direct, fine-grained control over scheduling operations on the event loop queue (we’ll discuss them in a greater detail later).&lt;/p&gt;
&lt;h4 name=&quot;ca8b&quot; id=&quot;ca8b&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;How setTimeout(…) works&lt;/h4&gt;
&lt;p name=&quot;7c23&quot; id=&quot;7c23&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;It’s important to note that &lt;code class=&quot;markup--code markup--p-code&quot;&gt;setTimeout(…)&lt;/code&gt; doesn’t automatically put your callback on the event loop queue. It sets up a timer. When the timer expires, the environment places your callback into the event loop, so that some future tick will pick it up and execute it.Take a look at this code:&lt;/p&gt;

&lt;p name=&quot;4a63&quot; id=&quot;4a63&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;That doesn’t mean that &lt;code class=&quot;markup--code markup--p-code&quot;&gt;myCallback&lt;/code&gt; will be executed in 1,000 ms but rather that, in 1,000 ms, &lt;code class=&quot;markup--code markup--p-code&quot;&gt;myCallback&lt;/code&gt; will be added to the queue. The queue, however, might have other events that have been added earlier — your callback will have to wait.&lt;/p&gt;
&lt;p name=&quot;aa5f&quot; id=&quot;aa5f&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;There are quite a few articles and tutorials on getting started with async code in JavaScript that suggest doing a setTimeout(callback, 0). Well, now you know what the Event Loop does and how setTimeout works: calling setTimeout with 0 as a second argument just defers the callback until the Call Stack is clear.&lt;/p&gt;
&lt;p name=&quot;61e6&quot; id=&quot;61e6&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Take a look at the following code:&lt;/p&gt;

&lt;p name=&quot;3454&quot; id=&quot;3454&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Although the wait time is set to 0 ms, the result in the browser console will be the following:&lt;/p&gt;
&lt;pre name=&quot;d68c&quot; id=&quot;d68c&quot; class=&quot;graf graf--pre graf-after--p&quot;&gt;
Hi&lt;br/&gt;Bye&lt;br/&gt;callback
&lt;/pre&gt;
&lt;h4 name=&quot;8cd8&quot; id=&quot;8cd8&quot; class=&quot;graf graf--h4 graf-after--pre&quot;&gt;What are Jobs in ES6 ?&lt;/h4&gt;
&lt;p name=&quot;3293&quot; id=&quot;3293&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;A new concept called the “Job Queue” was introduced in ES6. It’s a layer on top of the Event Loop queue. You are most likely to bump into it when dealing with the asynchronous behavior of Promises (we’ll talk about them too).&lt;/p&gt;
&lt;p name=&quot;a3af&quot; id=&quot;a3af&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;We’ll just touch on the concept now so that when we discuss async behavior with Promises, later on, you understand how those actions are being scheduled and processed.&lt;/p&gt;
&lt;p name=&quot;1773&quot; id=&quot;1773&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Imagine it like this: the Job Queue is a queue that’s attached to the end of every tick in the Event Loop queue. Certain async actions that may occur during a tick of the event loop will not cause a whole new event to be added to the event loop queue, but will instead add an item (aka Job) to the end of the current tick’s Job queue.&lt;/p&gt;
&lt;p name=&quot;d32d&quot; id=&quot;d32d&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;This means that you can add another functionality to be executed later, and you can rest assured that it will be executed right after, before anything else.&lt;/p&gt;
&lt;p name=&quot;235f&quot; id=&quot;235f&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;A Job can also cause more Jobs to be added to the end of the same queue. In theory, it’s possible for a Job “loop” (a Job that keeps adding other Jobs, etc.) to spin indefinitely, thus starving the program of the necessary resources needed to move on to the next event loop tick. Conceptually, this would be similar to just expressing a long-running or infinite loop (like &lt;code class=&quot;markup--code markup--p-code&quot;&gt;while (true)&lt;/code&gt; ..) in your code.&lt;/p&gt;
&lt;p name=&quot;3e53&quot; id=&quot;3e53&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Jobs are kind of like the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;setTimeout(callback, 0)&lt;/code&gt; “hack” but implemented in such a way that they introduce a much more well-defined and guaranteed ordering: later, but as soon as possible.&lt;/p&gt;
&lt;h4 name=&quot;5382&quot; id=&quot;5382&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--h4-strong&quot;&gt;Callbacks&lt;/strong&gt;&lt;/h4&gt;
&lt;p name=&quot;76e2&quot; id=&quot;76e2&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;As you already know, callbacks are by far the most common way to express and manage asynchrony in JavaScript programs. Indeed, the callback is the most fundamental async pattern in the JavaScript language. Countless JS programs, even very sophisticated and complex ones, have been written on top of no other async foundation than the callback.&lt;/p&gt;
&lt;p name=&quot;3518&quot; id=&quot;3518&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Except that callbacks don’t come with no shortcomings. Many developers are trying to find better async patterns. It’s impossible, however, to effectively use any abstraction if you don’t understand what’s actually under the hood.&lt;/p&gt;
&lt;p name=&quot;f5aa&quot; id=&quot;f5aa&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In the following chapter, we’ll explore couple of these abstractions in depth to show why more sophisticated async patterns (that will be discussed in subsequent posts) are necessary and even recommended.&lt;/p&gt;
&lt;h4 name=&quot;1ec9&quot; id=&quot;1ec9&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Nested Callbacks&lt;/h4&gt;
&lt;p name=&quot;8021&quot; id=&quot;8021&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Look at the following code:&lt;/p&gt;

&lt;p name=&quot;9f58&quot; id=&quot;9f58&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;We’ve got a chain of three functions nested together, each one representing a step in an asynchronous series.&lt;/p&gt;
&lt;p name=&quot;0f81&quot; id=&quot;0f81&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;This kind of code is often called a “callback hell”. But the “callback hell” actually has almost nothing to do with the nesting/indentation. It’s a much deeper problem than that.&lt;/p&gt;
&lt;p name=&quot;d052&quot; id=&quot;d052&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;First, we’re waiting for the “click” event, then we’re waiting for the timer to fire, then we’re waiting for the Ajax response to come back, at which point it might get all repeated again.&lt;/p&gt;
&lt;p name=&quot;7fb0&quot; id=&quot;7fb0&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;At first glance, this code may seem to map its asynchrony naturally to sequential steps like:&lt;/p&gt;

&lt;p name=&quot;4b1d&quot; id=&quot;4b1d&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Then we have:&lt;/p&gt;

&lt;p name=&quot;1189&quot; id=&quot;1189&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Then later we have:&lt;/p&gt;

&lt;p name=&quot;3c3b&quot; id=&quot;3c3b&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;And finally:&lt;/p&gt;

&lt;p name=&quot;25ef&quot; id=&quot;25ef&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;So, such a sequential way of expressing your async code seems a lot more natural, doesn’t it? There must be such a way, right?&lt;/p&gt;
&lt;h4 name=&quot;066f&quot; id=&quot;066f&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Promises&lt;/h4&gt;
&lt;p name=&quot;aad8&quot; id=&quot;aad8&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Take a look at the following code:&lt;/p&gt;

&lt;p name=&quot;77cd&quot; id=&quot;77cd&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;It’s all very straightforward: it sums the values of &lt;code class=&quot;markup--code markup--p-code&quot;&gt;x&lt;/code&gt; and &lt;code class=&quot;markup--code markup--p-code&quot;&gt;y&lt;/code&gt; and prints it to the console. What if, however, the value of &lt;code class=&quot;markup--code markup--p-code&quot;&gt;x&lt;/code&gt; or &lt;code class=&quot;markup--code markup--p-code&quot;&gt;y&lt;/code&gt; was missing and was still to be determined? Say, we need to retrieve the values of both &lt;code class=&quot;markup--code markup--p-code&quot;&gt;x&lt;/code&gt; and &lt;code class=&quot;markup--code markup--p-code&quot;&gt;y&lt;/code&gt; from the server, before they can be used in the expression. Let’s imagine that we have a function &lt;code class=&quot;markup--code markup--p-code&quot;&gt;loadX&lt;/code&gt; and &lt;code class=&quot;markup--code markup--p-code&quot;&gt;loadY&lt;/code&gt; that respectively load the values of &lt;code class=&quot;markup--code markup--p-code&quot;&gt;x&lt;/code&gt; and &lt;code class=&quot;markup--code markup--p-code&quot;&gt;y&lt;/code&gt; from the server. Then, imagine that we have a function &lt;code class=&quot;markup--code markup--p-code&quot;&gt;sum&lt;/code&gt; that sums the values of &lt;code class=&quot;markup--code markup--p-code&quot;&gt;x&lt;/code&gt; and &lt;code class=&quot;markup--code markup--p-code&quot;&gt;y&lt;/code&gt; once both of them are loaded.&lt;/p&gt;
&lt;p name=&quot;da15&quot; id=&quot;da15&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;It could look like this (quite ugly, isn’t it):&lt;/p&gt;

&lt;p name=&quot;33e5&quot; id=&quot;33e5&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;There is something very important here — in that snippet, we treated &lt;code class=&quot;markup--code markup--p-code&quot;&gt;x&lt;/code&gt; and &lt;code class=&quot;markup--code markup--p-code&quot;&gt;y&lt;/code&gt; as &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;future&lt;/strong&gt; values, and we expressed an operation &lt;code class=&quot;markup--code markup--p-code&quot;&gt;sum(…)&lt;/code&gt; that (from the outside) did not care whether &lt;code class=&quot;markup--code markup--p-code&quot;&gt;x&lt;/code&gt; or &lt;code class=&quot;markup--code markup--p-code&quot;&gt;y&lt;/code&gt; or both were or weren’t available right away.&lt;/p&gt;
&lt;p name=&quot;e237&quot; id=&quot;e237&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Of course, this rough callbacks-based approach leaves much to be desired. It’s just a first tiny step towards understanding the benefits of reasoning about &lt;em class=&quot;markup--em markup--p-em&quot;&gt;future values&lt;/em&gt; without worrying about the time aspect of when they will be available.&lt;/p&gt;
&lt;h4 name=&quot;8f53&quot; id=&quot;8f53&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Promise Value&lt;/h4&gt;
&lt;p name=&quot;8878&quot; id=&quot;8878&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Let’s just briefly glimpse at how we can express the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;x + y&lt;/code&gt; example with Promises:&lt;/p&gt;

&lt;p name=&quot;410b&quot; id=&quot;410b&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;There are two layers of Promises in this snippet.&lt;/p&gt;
&lt;p name=&quot;29b5&quot; id=&quot;29b5&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;code class=&quot;markup--code markup--p-code&quot;&gt;fetchX()&lt;/code&gt; and &lt;code class=&quot;markup--code markup--p-code&quot;&gt;fetchY()&lt;/code&gt; are called directly, and the values they return (promises!) are passed to &lt;code class=&quot;markup--code markup--p-code&quot;&gt;sum(...)&lt;/code&gt;. The underlying values these promises represent may be ready &lt;em class=&quot;markup--em markup--p-em&quot;&gt;now&lt;/em&gt; or &lt;em class=&quot;markup--em markup--p-em&quot;&gt;later&lt;/em&gt;, but each promise normalizes its behavior to be the same regardless. We reason about &lt;code class=&quot;markup--code markup--p-code&quot;&gt;x&lt;/code&gt; and &lt;code class=&quot;markup--code markup--p-code&quot;&gt;y&lt;/code&gt; values in a time-independent way. They are &lt;em class=&quot;markup--em markup--p-em&quot;&gt;future values&lt;/em&gt;, period.&lt;/p&gt;
&lt;p name=&quot;78f0&quot; id=&quot;78f0&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The second layer is the promise that &lt;code class=&quot;markup--code markup--p-code&quot;&gt;sum(...)&lt;/code&gt; creates &lt;br/&gt;(via &lt;code class=&quot;markup--code markup--p-code&quot;&gt;Promise.all([ ... ])&lt;/code&gt;) and returns, which we wait on by calling &lt;code class=&quot;markup--code markup--p-code&quot;&gt;then(...)&lt;/code&gt;. When the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;sum(...)&lt;/code&gt;operation completes, our sum &lt;em class=&quot;markup--em markup--p-em&quot;&gt;future value&lt;/em&gt; is ready and we can print it out. We hide the logic for waiting on the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;x&lt;/code&gt; and &lt;code class=&quot;markup--code markup--p-code&quot;&gt;y&lt;/code&gt; &lt;em class=&quot;markup--em markup--p-em&quot;&gt;future values&lt;/em&gt; inside of &lt;code class=&quot;markup--code markup--p-code&quot;&gt;sum(...)&lt;/code&gt; .&lt;/p&gt;
&lt;p name=&quot;98a8&quot; id=&quot;98a8&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Note&lt;/strong&gt;: Inside &lt;code class=&quot;markup--code markup--p-code&quot;&gt;sum(…)&lt;/code&gt;, the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;Promise.all([ … ])&lt;/code&gt; call creates a promise (which is waiting on &lt;code class=&quot;markup--code markup--p-code&quot;&gt;promiseX&lt;/code&gt; and &lt;code class=&quot;markup--code markup--p-code&quot;&gt;promiseY&lt;/code&gt; to resolve). The chained call to &lt;code class=&quot;markup--code markup--p-code&quot;&gt;.then(...)&lt;/code&gt; creates another promise, which the return&lt;br/&gt;&lt;code class=&quot;markup--code markup--p-code&quot;&gt;values[0] + values[1]&lt;/code&gt; line immediately resolves (with the result of the addition). Thus, the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;then(...)&lt;/code&gt; call we chain off the end of the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;sum(...)&lt;/code&gt; call — at the end of the snippet — is actually operating on that second promise returned, rather than the first one created by &lt;code class=&quot;markup--code markup--p-code&quot;&gt;Promise.all([ ... ])&lt;/code&gt;. Also, although we are not chaining off the end of that second &lt;code class=&quot;markup--code markup--p-code&quot;&gt;then(...)&lt;/code&gt;, it too has created another promise, had we chosen to observe/use it. This Promise chaining stuff will be explained in much greater detail later in this chapter.&lt;/p&gt;
&lt;p name=&quot;49f5&quot; id=&quot;49f5&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;With Promises, the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;then(...)&lt;/code&gt; call can actually take two functions, the first for fulfillment (as shown earlier), and the second for rejection:&lt;/p&gt;

&lt;p name=&quot;6423&quot; id=&quot;6423&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;If something went wrong when getting &lt;code class=&quot;markup--code markup--p-code&quot;&gt;x&lt;/code&gt; or &lt;code class=&quot;markup--code markup--p-code&quot;&gt;y&lt;/code&gt;, or something somehow failed during the addition, the promise that &lt;code class=&quot;markup--code markup--p-code&quot;&gt;sum(...)&lt;/code&gt; returns would be rejected, and the second callback error handler passed to &lt;code class=&quot;markup--code markup--p-code&quot;&gt;then(...)&lt;/code&gt; would receive the rejection value from the promise.&lt;/p&gt;
&lt;p name=&quot;6e87&quot; id=&quot;6e87&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Because Promises encapsulate the time-dependent state — waiting on the fulfillment or rejection of the underlying value — from the outside, the Promise itself is time-independent, and thus Promises can be composed (combined) in predictable ways regardless of the timing or outcome underneath.&lt;/p&gt;
&lt;p name=&quot;2e20&quot; id=&quot;2e20&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Moreover, once a Promise is resolved, it stays that way forever — it becomes an &lt;em class=&quot;markup--em markup--p-em&quot;&gt;immutable value&lt;/em&gt; at that point — and can then be &lt;em class=&quot;markup--em markup--p-em&quot;&gt;observed&lt;/em&gt; as many times as necessary.&lt;/p&gt;
&lt;p name=&quot;706d&quot; id=&quot;706d&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;It’s really useful that you can actually chain promises:&lt;/p&gt;

&lt;p name=&quot;8f23&quot; id=&quot;8f23&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Calling &lt;code class=&quot;markup--code markup--p-code&quot;&gt;delay(2000)&lt;/code&gt; creates a promise that will fulfill in 2000ms, and then we return that from the first &lt;code class=&quot;markup--code markup--p-code&quot;&gt;then(...)&lt;/code&gt; fulfillment callback, which causes the second &lt;code class=&quot;markup--code markup--p-code&quot;&gt;then(...)&lt;/code&gt;'s promise to wait on that 2000ms promise.&lt;/p&gt;
&lt;p name=&quot;f92e&quot; id=&quot;f92e&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Note&lt;/strong&gt;: Because a Promise is externally immutable once resolved, it’s now safe to pass that value around to any party, knowing that it cannot be modified accidentally or maliciously. This is especially true in relation to multiple parties observing the resolution of a Promise. It’s not possible for one party to affect another party’s ability to observe Promise resolution. Immutability may sound like an academic topic, but it’s actually one of the most fundamental and important aspects of Promise design, and shouldn’t be casually passed over.&lt;/p&gt;
&lt;h4 name=&quot;918a&quot; id=&quot;918a&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--h4-strong&quot;&gt;To Promise or not to Promise?&lt;/strong&gt;&lt;/h4&gt;
&lt;p name=&quot;4f1d&quot; id=&quot;4f1d&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;An important detail about Promises is knowing for sure if some value is an actual Promise or not. In other words, is it a value that will behave like a Promise?&lt;/p&gt;
&lt;p name=&quot;7397&quot; id=&quot;7397&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;We know that Promises are constructed by the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;new Promise(…)&lt;/code&gt; syntax, and you might think that &lt;code class=&quot;markup--code markup--p-code&quot;&gt;p instanceof Promise&lt;/code&gt; would be a sufficient check. Well, not quite.&lt;/p&gt;
&lt;p name=&quot;27a3&quot; id=&quot;27a3&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Mainly because you can receive a Promise value from another browser window (e.g. iframe), which would have its own Promise, different from the one in the current window or frame, and that check would fail to identify the Promise instance.&lt;/p&gt;
&lt;p name=&quot;9bbd&quot; id=&quot;9bbd&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Moreover, a library or framework may choose to vend its own Promises and not use the native ES6 Promise implementation to do so. In fact, you may very well be using Promises with libraries in older browsers that have no Promise at all.&lt;/p&gt;
&lt;h4 name=&quot;d75e&quot; id=&quot;d75e&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Swallowing exceptions&lt;/h4&gt;
&lt;p name=&quot;4ca1&quot; id=&quot;4ca1&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;If at any point in the creation of a Promise, or in the observation of its resolution, a JavaScript exception error occurs, such as a &lt;code class=&quot;markup--code markup--p-code&quot;&gt;TypeError&lt;/code&gt; or &lt;code class=&quot;markup--code markup--p-code&quot;&gt;ReferenceError&lt;/code&gt;, that exception will be caught, and it will force the Promise in question to become rejected.&lt;/p&gt;
&lt;p name=&quot;79e4&quot; id=&quot;79e4&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;For example:&lt;/p&gt;

&lt;p name=&quot;723b&quot; id=&quot;723b&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;But what happens if a Promise is fulfilled yet there was a JS exception error during the observation (in a &lt;code class=&quot;markup--code markup--p-code&quot;&gt;then(…)&lt;/code&gt; registered callback)? Even though it won’t be lost, you may find the way they’re handled a bit surprising. Until you dig a little deeper:&lt;/p&gt;

&lt;p name=&quot;c1cb&quot; id=&quot;c1cb&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;It looks like the exception from &lt;code class=&quot;markup--code markup--p-code&quot;&gt;foo.bar()&lt;/code&gt; really did get swallowed. It wasn’t, though. There was something deeper that went wrong, however, which we failed to listen for. The &lt;code class=&quot;markup--code markup--p-code&quot;&gt;p.then(…)&lt;/code&gt; call itself returns another promise, and it’s that promise that will be rejected with the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;TypeError&lt;/code&gt; exception.&lt;/p&gt;
&lt;h4 name=&quot;bf3f&quot; id=&quot;bf3f&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--h4-strong&quot;&gt;Handling uncaught exceptions&lt;/strong&gt;&lt;/h4&gt;
&lt;p name=&quot;ce62&quot; id=&quot;ce62&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;There are other approaches which many would say are &lt;em class=&quot;markup--em markup--p-em&quot;&gt;better&lt;/em&gt;.&lt;/p&gt;
&lt;p name=&quot;a8f8&quot; id=&quot;a8f8&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;A common suggestion is that Promises should have a &lt;code class=&quot;markup--code markup--p-code&quot;&gt;done(…)&lt;/code&gt; added to them, which essentially marks the Promise chain as “done.” &lt;code class=&quot;markup--code markup--p-code&quot;&gt;done(…)&lt;/code&gt;doesn’t create and return a Promise, so the callbacks passed to &lt;code class=&quot;markup--code markup--p-code&quot;&gt;done(..)&lt;/code&gt; are obviously not wired up to report problems to a chained Promise that doesn’t exist.&lt;/p&gt;
&lt;p name=&quot;cac9&quot; id=&quot;cac9&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;It’s treated as you might usually expect in uncaught error conditions: any exception inside a &lt;code class=&quot;markup--code markup--p-code&quot;&gt;done(..)&lt;/code&gt; rejection handler would be thrown as a global uncaught error (in the developer console, basically):&lt;/p&gt;

&lt;h4 name=&quot;ee15&quot; id=&quot;ee15&quot; class=&quot;graf graf--h4 graf-after--figure&quot;&gt;&lt;strong class=&quot;markup--strong markup--h4-strong&quot;&gt;What’s happening in ES8? Async/await&lt;/strong&gt;&lt;/h4&gt;
&lt;p name=&quot;e408&quot; id=&quot;e408&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;JavaScript ES8 introduced &lt;code class=&quot;markup--code markup--p-code&quot;&gt;async/await&lt;/code&gt; that makes the job of working with Promises easier. We’ll briefly go through the possibilities &lt;code class=&quot;markup--code markup--p-code&quot;&gt;async/await&lt;/code&gt; offers and how to leverage them to write async code.&lt;/p&gt;
&lt;p name=&quot;2de6&quot; id=&quot;2de6&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;So, let’s see how async/await works.&lt;/p&gt;
&lt;p name=&quot;aa2d&quot; id=&quot;aa2d&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;You define an asynchronous function using the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;async&lt;/code&gt; function declaration. Such functions return an &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/AsyncFunction&quot; data-href=&quot;https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/AsyncFunction&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;AsyncFunction&lt;/a&gt; object. The &lt;code class=&quot;markup--code markup--p-code&quot;&gt;AsyncFunction&lt;/code&gt; object represents the asynchronous function which executes the code, contained within that function.&lt;/p&gt;
&lt;p name=&quot;bc29&quot; id=&quot;bc29&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;When an async function is called, it returns a &lt;code class=&quot;markup--code markup--p-code&quot;&gt;Promise&lt;/code&gt; . When the async function returns a value, that’s not a &lt;code class=&quot;markup--code markup--p-code&quot;&gt;Promise&lt;/code&gt; , a &lt;code class=&quot;markup--code markup--p-code&quot;&gt;Promise&lt;/code&gt; will be automatically created and it will be resolved with the returned value from the function. When the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;async&lt;/code&gt; function throws an exception, the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;Promise&lt;/code&gt; will be rejected with the thrown value.&lt;/p&gt;
&lt;p name=&quot;68d2&quot; id=&quot;68d2&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;An &lt;code class=&quot;markup--code markup--p-code&quot;&gt;async&lt;/code&gt; function can contain an &lt;code class=&quot;markup--code markup--p-code&quot;&gt;await&lt;/code&gt; expression, that pauses the execution of the function and waits for the passed Promise’s resolution, and then resumes the async function’s execution and returns the resolved value.&lt;/p&gt;
&lt;p name=&quot;aa75&quot; id=&quot;aa75&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;You can think of a &lt;code class=&quot;markup--code markup--p-code&quot;&gt;Promise&lt;/code&gt; in JavaScript as the equivalent of Java’s &lt;code class=&quot;markup--code markup--p-code&quot;&gt;Future&lt;/code&gt; or &lt;code class=&quot;markup--code markup--p-code&quot;&gt;C#&lt;/code&gt;'s Task.&lt;/p&gt;
&lt;blockquote name=&quot;2941&quot; id=&quot;2941&quot; class=&quot;graf graf--blockquote graf-after--p&quot; readability=&quot;5&quot;&gt;
&lt;p&gt;The purpose of &lt;code class=&quot;markup--code markup--blockquote-code&quot;&gt;async/await&lt;/code&gt; is to simplify the behavior of using promises.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;4c11&quot; id=&quot;4c11&quot; class=&quot;graf graf--p graf-after--blockquote&quot;&gt;Let’s take a look at the following example:&lt;/p&gt;

&lt;p name=&quot;57a5&quot; id=&quot;57a5&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Similarly, functions that are throwing exceptions are equivalent to functions which return promises that have been rejected:&lt;/p&gt;

&lt;p name=&quot;1f8e&quot; id=&quot;1f8e&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The &lt;code class=&quot;markup--code markup--p-code&quot;&gt;await&lt;/code&gt; keyword can only be used in &lt;code class=&quot;markup--code markup--p-code&quot;&gt;async&lt;/code&gt; functions and allows you to synchronously wait on a Promise. If we use promises outside of an &lt;code class=&quot;markup--code markup--p-code&quot;&gt;async&lt;/code&gt; function, we’ll still have to use &lt;code class=&quot;markup--code markup--p-code&quot;&gt;then&lt;/code&gt; callbacks:&lt;/p&gt;

&lt;p name=&quot;b9df&quot; id=&quot;b9df&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;You can also define async functions using an “async function expression”. An async function expression is very similar to and has almost the same syntax as, an async function statement. The main difference between an async function expression and an async function statement is the function name, which can be omitted in async function expressions to create anonymous functions. An async function expression can be used as an IIFE (Immediately Invoked Function Expression) which runs as soon as it is defined.&lt;/p&gt;
&lt;p name=&quot;1fd0&quot; id=&quot;1fd0&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;It looks like this:&lt;/p&gt;

&lt;p name=&quot;f40a&quot; id=&quot;f40a&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;More importantly, async/await is supported in all major browsers:&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;0*z-A-JIe5OWFtgyd2.&quot; data-width=&quot;1536&quot; data-height=&quot;500&quot; data-action=&quot;zoom&quot; data-action-value=&quot;0*z-A-JIe5OWFtgyd2.&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/0*z-A-JIe5OWFtgyd2.&quot;/&gt;&lt;/div&gt;
If this compatibility is not what you are after, there are also several JS transpilers like &lt;a href=&quot;https://babeljs.io/docs/plugins/transform-async-to-generator/&quot; data-href=&quot;https://babeljs.io/docs/plugins/transform-async-to-generator/&quot; class=&quot;markup--anchor markup--figure-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Babel&lt;/a&gt; and &lt;a href=&quot;https://www.typescriptlang.org/docs/handbook/release-notes/typescript-2-3.html&quot; data-href=&quot;https://www.typescriptlang.org/docs/handbook/release-notes/typescript-2-3.html&quot; class=&quot;markup--anchor markup--figure-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;TypeScript.&lt;/a&gt;
&lt;p name=&quot;5aec&quot; id=&quot;5aec&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;At the end of the day, the important thing is not to blindly choose the “latest” approach to writing async code. It’s essential to understand the internals of async JavaScript, learn why it’s so critical and comprehend in-depth the internals of the method you have chosen. Every approach has pros and cons as with everything else in programming.&lt;/p&gt;
&lt;h3 name=&quot;2872&quot; id=&quot;2872&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;5 Tips on writing highly maintainable, non-brittle async code&lt;/h3&gt;
&lt;ol class=&quot;postList&quot;&gt;&lt;li name=&quot;c2aa&quot; id=&quot;c2aa&quot; class=&quot;graf graf--li graf-after--h3&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Clean code:&lt;/strong&gt; Using async/await allows you to write a lot less code. Every time you use async/await you skip a few unnecessary steps: write .then, create an anonymous function to handle the response, name the response from that callback e.g.&lt;/li&gt;
&lt;/ol&gt;
&lt;p name=&quot;0ae5&quot; id=&quot;0ae5&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Versus:&lt;/p&gt;

&lt;p name=&quot;9f30&quot; id=&quot;9f30&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;2. &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Error handling:&lt;/strong&gt; Async/await makes it possible to handle both sync and async errors with the same code construct — the well-known try/catch statements. Let’s see how it looks with Promises:&lt;/p&gt;

&lt;p name=&quot;7569&quot; id=&quot;7569&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Versus:&lt;/p&gt;

&lt;p name=&quot;1703&quot; id=&quot;1703&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;3. &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Conditionals:&lt;/strong&gt; Writing conditional code with &lt;code class=&quot;markup--code markup--p-code&quot;&gt;async/await&lt;/code&gt; is a lot more straightforward:&lt;/p&gt;

&lt;p name=&quot;f59b&quot; id=&quot;f59b&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Versus:&lt;/p&gt;

&lt;p name=&quot;cdde&quot; id=&quot;cdde&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;4. &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Stack Frames:&lt;/strong&gt; Unlike with &lt;code class=&quot;markup--code markup--p-code&quot;&gt;async/await&lt;/code&gt;, the error stack returned from a promise chain gives no clue of where the error happened. Look at the following:&lt;/p&gt;

&lt;p name=&quot;dd18&quot; id=&quot;dd18&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Versus:&lt;/p&gt;

&lt;p name=&quot;7be4&quot; id=&quot;7be4&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;5. &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Debugging:&lt;/strong&gt; If you have used promises, you know that debugging them is a nightmare. For example, if you set a breakpoint inside a .then block and use debug shortcuts like “stop-over”, the debugger will not move to the following .then because it only “steps” through synchronous code.&lt;br/&gt;With &lt;code class=&quot;markup--code markup--p-code&quot;&gt;async/await&lt;/code&gt; you can step through await calls exactly as if they were normal synchronous functions.&lt;/p&gt;
&lt;p name=&quot;0d35&quot; id=&quot;0d35&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Writing &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;async JavaScript code is important&lt;/strong&gt; not only for the apps themselves but &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;for libraries as well&lt;/strong&gt;.&lt;/p&gt;
&lt;p name=&quot;bb09&quot; id=&quot;bb09&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;For example, the &lt;a href=&quot;https://www.sessionstack.com&quot; data-href=&quot;https://www.sessionstack.com&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;SessionStack&lt;/a&gt; library records everything in your web app/website: all DOM changes, user interactions, JavaScript exceptions, stack traces, failed network requests, and debug messages.&lt;/p&gt;
&lt;p name=&quot;5b89&quot; id=&quot;5b89&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;And this all has to happen in your production environment without impacting any of the UX. We need to heavily optimize our code and make it asynchronous as much as possible so that we can increase the number of events that are being processed by the Event Loop.&lt;/p&gt;
&lt;p name=&quot;f0b5&quot; id=&quot;f0b5&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;And not just the library! When you replay a user session in SessionStack, we have to render everything that happened in your user’s browser at the time the problem occurred, and we have to reconstruct the whole state, allowing you to jump back and forth in the session timeline. In order to make this possible, we’re heavily employing the async opportunities that JavaScript provides.&lt;/p&gt;
&lt;p name=&quot;038b&quot; id=&quot;038b&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;There is a free plan that allows you to &lt;a href=&quot;https://www.sessionstack.com/signup/&quot; data-href=&quot;https://www.sessionstack.com/signup/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;get started for free&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;0*xSEaWHGqqlcF8g5H.&quot; data-width=&quot;800&quot; data-height=&quot;444&quot; data-action=&quot;zoom&quot; data-action-value=&quot;0*xSEaWHGqqlcF8g5H.&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/0*xSEaWHGqqlcF8g5H.&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;b00b&quot; id=&quot;b00b&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Resources:&lt;/p&gt;
</description>
<pubDate>Sat, 14 Oct 2017 09:27:02 +0000</pubDate>
<dc:creator>kiyanwang</dc:creator>
<og:title>How JavaScript works: Event loop and the rise of Async programming + 5 ways to better coding with…</og:title>
<og:url>https://blog.sessionstack.com/how-javascript-works-event-loop-and-the-rise-of-async-programming-5-ways-to-better-coding-with-2f077c4438b5</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*TozSrkk92l8ho6d8JxqF_w.gif</og:image>
<og:description>Welcome to post # 4 of the series dedicated to exploring JavaScript and its building components. In the process of identifying and…</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.sessionstack.com/how-javascript-works-event-loop-and-the-rise-of-async-programming-5-ways-to-better-coding-with-2f077c4438b5?gi=bf2c4aededf8</dc:identifier>
</item>
<item>
<title>Coding Interview Cheatsheet</title>
<link>https://github.com/yangshun/tech-interview-handbook/blob/master/preparing/cheatsheet.md</link>
<guid isPermaLink="true" >https://github.com/yangshun/tech-interview-handbook/blob/master/preparing/cheatsheet.md</guid>
<description>
&lt;p&gt;This is a straight-to-the-point distilled list of technical interview Do's and Don'ts, mainly for algorithmic interviews. Some of these may apply to only phone screens on whiteboard interviews but most will apply to both. I revise this list before each of my interviews to remind myself of them and eventually internalized all of them to the point I do not have to rely on it anymore.&lt;/p&gt;
&lt;p&gt;For a detailed walkthrough of interview preparation, refer to the &lt;a href=&quot;https://github.com/yangshun/tech-interview-handbook/blob/master/preparing&quot;&gt;&quot;Preparing for a Coding Interview&quot;&lt;/a&gt; section.&lt;/p&gt;
&lt;h3&gt;1. Before Interview&lt;/h3&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th/&gt;
&lt;th&gt;Things&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Prepare pen, paper and earphones/headphones.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Find a quiet environment with good Internet connection.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Ensure webcam and audio are working. There were times I had to restart Chrome to get Hangouts to work again.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Request for the option to interview over Hangouts/Skype instead of a phone call; it is easier to send links or text across.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Decide on and be familiar with a programming language.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Familiarize yourself with the coding environment (CoderPad/CodePen). Set up the coding shortcuts, turn on autocompletion, tab spacing, etc.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Prepare answers to the &lt;a href=&quot;https://github.com/yangshun/tech-interview-handbook/blob/master/non-technical/behavioral.md&quot;&gt;frequently-asked questions&lt;/a&gt; in an interview.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Prepare some &lt;a href=&quot;https://github.com/yangshun/tech-interview-handbook/blob/master/non-technical/questions-to-ask.md&quot;&gt;questions to ask&lt;/a&gt; at the end of the interview.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Dress comfortably. Usually you do not need to wear smart clothes, casual should be fine. T-shirts and jeans are acceptable at most places.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Stay calm and composed.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;⚠️&lt;/td&gt;
&lt;td&gt;Turn off the webcam if possible. Most remote interviews will not require video chat and leaving it on only serves as a distraction.&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3&gt;2. Introduction&lt;/h3&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th/&gt;
&lt;th&gt;Things&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Introduce yourself in a few sentences under a minute or two.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Mention interesting points that are relevant to the role you are applying for.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Sound enthusiastic! Speak with a smile and you will naturally sound more engaging.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;Spend too long introducing yourself. The more time you spend talk the less time you have to code.&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3&gt;3. Upon Getting the Question&lt;/h3&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th/&gt;
&lt;th&gt;Things&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Repeat the question back at the interviewer.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Clarify any assumptions you made subconsciously. Many questions are under-specified on purpose. A tree-like diagram could very well be a graph that allows for cycles and a naive recursive solution would not work.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Clarify input format and range. Ask whether input can be assumed to be well-formed and non-null.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Work through a small example to ensure you understood the question.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Explain a high level approach even if it is a brute force one.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Improve upon the approach and optimize. Reduce duplicated work and cache repeated computations.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Think carefully, then state and explain the time and space complexity of your approaches.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;If stuck, think about related problems you have seen before and how they were solved. Check out the &lt;a href=&quot;https://github.com/yangshun/tech-interview-handbook/blob/master/algorithms&quot;&gt;tips&lt;/a&gt; in this section.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;Ignore information given to you. Every piece is important.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;Jump into coding straightaway.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;Start coding without interviewer's green light.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;Appear too unsure about your approach or analysis.&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3&gt;4. During Coding&lt;/h3&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th/&gt;
&lt;th&gt;Things&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Explain what you are coding/typing to the interviewer, what you are trying to achieve.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Practice good coding style. Clear variable names, consistent operator spacing, proper indentation, etc.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Type/write at a reasonable speed.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;As much as possible, write actual compilable code, not pseudocode.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Write in a modular fashion. Extract out chunks of repeated code into functions.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Ask for permission to use trivial functions without having to implement them; saves you some time.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Use the hints given by the interviewer.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Demonstrate mastery of your chosen programming language.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Demonstrate technical knowledge in data structures and algorithms.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;If you are cutting corners in your code, state that out loud to your interviewer and say what you would do in a non-interview setting (no time constraints). E.g., I would write a regex to parse this string rather than using &lt;code&gt;split()&lt;/code&gt; which may not cover all cases.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Practice whiteboard space-management skills.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;⚠️&lt;/td&gt;
&lt;td&gt;Reasonable defensive coding. Check for nulls, empty collections, etc. Can omit if input validity has been clarified with the interviewer.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;Remain quiet the whole time.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;Spend too much time writing comments.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;Use extremely verbose variable names.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;Copy and paste code without checking.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;Interrupt your interviewer when they are talking. Usually if they speak, they are trying to give you hints or steer you in the right direction.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;Write too big (takes up too much space) or too small (illegible) if on a whiteboard.&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3&gt;5. After Coding&lt;/h3&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th/&gt;
&lt;th&gt;Things&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Scan through your code for mistakes as if it was your first time seeing code written by someone else.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Check for off-by-one errors.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Come up with more test cases. Try extreme test cases.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Step through your code with those test cases.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Look out for places where you can refactor.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Reiterate the time and space complexity of your code.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Explain trade-offs and how the code/approach can be improved if given more time.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;Immediately announce that you are done coding. Do the above first!&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;Argue with the interviewer. They may be wrong but that is very unlikely given that they are familiar with the question.&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3&gt;6. Wrap Up&lt;/h3&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th/&gt;
&lt;th&gt;Things&lt;/th&gt;
&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Ask questions. More importantly, ask good and engaging questions that are tailored to the company! Pick some questions from &lt;a href=&quot;https://github.com/yangshun/tech-interview-handbook/blob/master/non-technical/questions-to-ask.md&quot;&gt;this list&lt;/a&gt;.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;✅&lt;/td&gt;
&lt;td&gt;Thank the interviewer.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;End the interview without asking any questions.&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;❌&lt;/td&gt;
&lt;td&gt;Ask about your interview performance. It can get awkward.&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h6&gt;References&lt;/h6&gt;
</description>
<pubDate>Sat, 14 Oct 2017 06:35:29 +0000</pubDate>
<dc:creator>yangshun</dc:creator>
<og:image>https://avatars0.githubusercontent.com/u/1315101?s=400&amp;v=4</og:image>
<og:type>object</og:type>
<og:title>yangshun/tech-interview-handbook</og:title>
<og:url>https://github.com/yangshun/tech-interview-handbook</og:url>
<og:description>tech-interview-handbook - 💯 Algorithms, front end and behavioral content for rocking your coding interview. 🆕 Interview Cheatsheet! 🆕</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://github.com/yangshun/tech-interview-handbook/blob/master/preparing/cheatsheet.md</dc:identifier>
</item>
<item>
<title>There&amp;#039;s No Fire Alarm for Artificial General Intelligence</title>
<link>https://intelligence.org/2017/10/13/fire-alarm/</link>
<guid isPermaLink="true" >https://intelligence.org/2017/10/13/fire-alarm/</guid>
<description>&lt;hr/&gt;&lt;p&gt;What is the function of a fire alarm?&lt;/p&gt;

&lt;p&gt;One might think that the function of a fire alarm is to provide you with important evidence about a fire existing, allowing you to change your policy accordingly and exit the building.&lt;/p&gt;
&lt;p&gt;In the classic experiment by Latane and Darley in 1968, eight groups of three students each were asked to fill out a questionnaire in a room that shortly after began filling up with smoke. Five out of the eight groups didn’t react or report the smoke, even as it became dense enough to make them start coughing. Subsequent manipulations showed that a lone student will respond 75% of the time; while a student accompanied by two actors told to feign apathy will respond only 10% of the time. This and other experiments seemed to pin down that what’s happening is pluralistic ignorance. We don’t want to look panicky by being afraid of what isn’t an emergency, so we try to look calm while glancing out of the corners of our eyes to see how others are reacting, but of course they are also trying to look calm.&lt;/p&gt;
&lt;p&gt;(I’ve read a number of replications and variations on this research, and the effect size is blatant. I would not expect this to be one of the results that dies to the replication crisis, and I haven’t yet heard about the replication crisis touching it. But we have to put a maybe-not marker on everything now.)&lt;/p&gt;
&lt;p&gt;A fire alarm creates common knowledge, in the you-know-I-know sense, that there is a fire; after which it is socially safe to react. When the fire alarm goes off, you know that everyone else knows there is a fire, you know you won’t lose face if you proceed to exit the building.&lt;/p&gt;
&lt;p&gt;The fire alarm doesn’t tell us with certainty that a fire is there. In fact, I can’t recall one time in my life when, exiting a building on a fire alarm, there was an actual fire. Really, a fire alarm is &lt;em&gt;weaker&lt;/em&gt; evidence of fire than smoke coming from under a door.&lt;/p&gt;
&lt;p&gt;But the fire alarm tells us that it’s socially okay to react to the fire. It promises us with certainty that we won’t be embarrassed if we now proceed to exit in an orderly fashion.&lt;/p&gt;
&lt;p&gt;It seems to me that this is one of the cases where people have mistaken beliefs about what they believe, like when somebody loudly endorsing their city’s team to win the big game will back down as soon as asked to bet. They haven’t consciously distinguished the rewarding exhilaration of shouting that the team will win, from the feeling of anticipating the team will win.&lt;/p&gt;
&lt;p&gt;When people look at the smoke coming from under the door, I think they think their uncertain wobbling feeling comes from not assigning the fire a high-enough probability of really being there, and that they’re reluctant to act for fear of wasting effort and time. If so, I think they’re interpreting their own feelings mistakenly. If that was so, they’d get the same wobbly feeling on hearing the fire alarm, or even more so, because fire alarms correlate to fire less than does smoke coming from under a door. The uncertain wobbling feeling comes from the worry that others believe differently, not the worry that the fire isn’t there. The reluctance to act is the reluctance to be seen looking foolish, not the reluctance to waste effort. That’s why the student alone in the room does something about the fire 75% of the time, and why people have no trouble reacting to the much weaker evidence presented by fire alarms.&lt;/p&gt;

&lt;hr/&gt;
&lt;p&gt;It’s now and then proposed that we ought to start reacting later to the issues of Artificial General Intelligence (&lt;a href=&quot;http://econlog.econlib.org/archives/2016/03/so_far_unfriend.html&quot;&gt;background here&lt;/a&gt;), because, it is said, we are so far away from it that it just isn’t possible to do productive work on it today.&lt;/p&gt;
&lt;p&gt;(For direct argument about there being things doable today, see: Soares and Fallenstein (&lt;a href=&quot;https://intelligence.org/files/TechnicalAgenda.pdf&quot;&gt;2014/2017&lt;/a&gt;); Amodei, Olah, Steinhardt, Christiano, Schulman, and Mané (&lt;a href=&quot;https://arxiv.org/abs/1606.06565&quot;&gt;2016&lt;/a&gt;); or Taylor, Yudkowsky, LaVictoire, and Critch (&lt;a href=&quot;https://intelligence.org/2017/02/28/using-machine-learning/&quot;&gt;2016&lt;/a&gt;).)&lt;/p&gt;
&lt;p&gt;(If none of those papers existed or if you were an AI researcher who’d read them but thought they were all garbage, and you wished you could work on alignment but knew of nothing you could do, the wise next step would be to sit down and spend two hours by the clock sincerely trying to think of possible approaches. Preferably without self-sabotage that makes sure you don’t come up with anything plausible; as might happen if, hypothetically speaking, you would actually find it much more comfortable to believe there was nothing you ought to be working on today, because e.g. then you could work on other things that interested you more.)&lt;/p&gt;
&lt;p&gt;(But never mind.)&lt;/p&gt;
&lt;p&gt;So if AGI seems far-ish away, and you think the conclusion licensed by this is that you can’t do any productive work on AGI alignment yet, then the implicit alternative strategy on offer is: Wait for some unspecified future event that tells us AGI is coming near; and &lt;em&gt;then&lt;/em&gt; we’ll all know that it’s okay to start working on AGI alignment.&lt;/p&gt;
&lt;p&gt;This seems to me to be wrong on a number of grounds. Here are some of them.&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;One:&lt;/strong&gt; As Stuart Russell observed, if you get radio signals from space and spot a spaceship there with your telescopes and you know the aliens are landing in thirty years, you still start thinking about that today.&lt;/p&gt;
&lt;p&gt;You’re not like, “Meh, that’s thirty years off, whatever.” You certainly don’t casually say “Well, there’s nothing we can do until they’re closer.” Not without spending two hours, or at least &lt;a href=&quot;http://www.readthesequences.com/MotivatedStoppingAndMotivatedContinuation&quot;&gt;five minutes&lt;/a&gt; by the clock, brainstorming about whether there is anything you ought to be starting now.&lt;/p&gt;
&lt;p&gt;If you said the aliens were coming in thirty years and you were therefore going to do nothing today… well, if these were &lt;a href=&quot;https://www.facebook.com/yudkowsky/posts/10155616782514228&quot;&gt;more effective time&lt;/a&gt;s, somebody would ask for a schedule of what you thought ought to be done, starting when, how long before the aliens arrive. If you didn’t have that schedule ready, they’d know that you weren’t operating according to a worked table of timed responses, but just procrastinating and doing nothing; and they’d correctly infer that you probably hadn’t searched very hard for things that could be done today.&lt;/p&gt;
&lt;p&gt;In Bryan Caplan’s terms, anyone who seems quite casual about the fact that “nothing can be done now to prepare” about the aliens is &lt;a href=&quot;http://econlog.econlib.org/archives/2016/01/the_invisible_t.html&quot;&gt;missing a mood&lt;/a&gt;; they should be much more alarmed at not being able to think of any way to prepare. And maybe ask if somebody else has come up with any ideas? But never mind.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Two:&lt;/strong&gt; History shows that for the general public, and even for scientists not in a key inner circle, and even for scientists &lt;em&gt;in&lt;/em&gt; that key circle, it is very often the case that key technological developments still seem decades away, five years before they show up.&lt;/p&gt;
&lt;p&gt;In 1901, two years before helping build the first heavier-than-air flyer, Wilbur Wright told his brother that powered flight was &lt;a href=&quot;https://books.google.com/books?id=ldxfLyNIk9wC&amp;amp;pg=PA91&amp;amp;dq=&amp;quot;i+said+to+my+brother+orville&amp;quot;&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ved=0ahUKEwioiseChcnWAhWL-VQKHab6AqMQ6AEIJjAA#v=onepage&amp;amp;q=%22i%20said%20to%20my%20brother%20orville%22&amp;amp;f=false&quot;&gt;fifty years away&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In 1939, three years before he personally oversaw the first critical chain reaction in a pile of uranium bricks, Enrico Fermi voiced &lt;a href=&quot;https://books.google.com/books?id=aSgFMMNQ6G4C&amp;amp;pg=PA813&amp;amp;lpg=PA813&amp;amp;dq=weart+fermi&amp;amp;source=bl&amp;amp;ots=Jy1pBOUL10&amp;amp;sig=c9wK_yLHbXZS_GFIv0K3bgpmE58&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ved=0ahUKEwjNofKsisnWAhXGlFQKHbOSB1QQ6AEIKTAA#v=onepage&amp;amp;q=%22ten%20per%20cent%22&amp;amp;f=false&quot;&gt;90% confidence&lt;/a&gt; that it was &lt;a href=&quot;http://lesswrong.com/lw/h8m/being_halfrational_about_pascals_wager_is_even/&quot;&gt;impossible&lt;/a&gt; to use uranium to sustain a fission chain reaction. I believe Fermi also said a year after that, aka two years before the denouement, that &lt;em&gt;if&lt;/em&gt; net power from fission was even possible (as he then granted some greater plausibility) then it would be fifty years off; but for this I neglected to keep the citation.&lt;/p&gt;
&lt;p&gt;And of course if you’re not the Wright Brothers or Enrico Fermi, you will be even more surprised. Most of the world learned that atomic weapons were now a thing when they woke up to the headlines about Hiroshima. There were esteemed intellectuals saying &lt;a href=&quot;https://www.xaprb.com/blog/flight-is-impossible/&quot;&gt;four years &lt;em&gt;after&lt;/em&gt; the Wright Flyer&lt;/a&gt; that heavier-than-air flight was impossible, because knowledge propagated more slowly back then.&lt;/p&gt;
&lt;p&gt;Were there events that, in &lt;a href=&quot;https://www.readthesequences.com/Hindsight-Devalues-Science&quot;&gt;hindsight&lt;/a&gt;, today, we can see as signs that heavier-than-air flight or nuclear energy were nearing? Sure, but if you go back and read the actual newspapers from that time and see what people actually said about it then, you’ll see that they did not know that these were signs, or that they were very uncertain that these might be signs. Some playing the part of Excited Futurists proclaimed that big changes were imminent, I expect, and others playing the part of Sober Scientists tried to pour cold water on all that childish enthusiasm; I expect that part was more or less exactly the same decades earlier. If somewhere in that din was a superforecaster who said “decades” when it was decades and “5 years” when it was five, good luck noticing them amid all the noise. More likely, the superforecasters were the ones who said “Could be tomorrow, could be decades” both when the big development was a day away and when it was decades away.&lt;/p&gt;
&lt;p&gt;One of the major modes by which hindsight bias makes us feel that the past was more predictable than anyone was actually able to predict at the time, is that in hindsight we know what we ought to notice, and we fixate on only one thought as to what each piece of evidence indicates. If you look at what people actually say at the time, historically, they’ve usually got no clue what’s about to happen three months before it happens, because they don’t know which signs are which.&lt;/p&gt;
&lt;p&gt;I mean, you &lt;em&gt;could&lt;/em&gt; say the words “AGI is 50 years away” and have those words happen to be true. People were also saying that powered flight was decades away when it was in fact decades away, and those people happened to be right. The problem is that everything looks the same to you either way, if you are actually living history instead of reading about it afterwards.&lt;/p&gt;
&lt;p&gt;It’s not that whenever somebody says “fifty years” the thing always happens in two years. It’s that this confident prediction of things being far away corresponds to an epistemic state about the technology that feels the same way internally until you are very very close to the big development. It’s the epistemic state of “Well, I don’t see how to do the thing” and sometimes you say that fifty years off from the big development, and sometimes you say it two years away, and sometimes you say it while the Wright Flyer is flying somewhere out of your sight.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Three:&lt;/strong&gt; Progress is driven by peak knowledge, not average knowledge.&lt;/p&gt;
&lt;p&gt;If Fermi and the Wrights couldn’t see it coming three years out, imagine how hard it must be for anyone else to see it.&lt;/p&gt;
&lt;p&gt;If you’re not at the global peak of knowledge of how to do the thing, and looped in on all the progress being made at what will turn out to be the leading project, you aren’t going to be able to see of your own knowledge &lt;em&gt;at all&lt;/em&gt; that the big development is imminent. Unless you are very good at perspective-taking in a way that wasn’t necessary in a hunter-gatherer tribe, and very good at realizing that other people may know techniques and ideas of which you have no inkling even that you do not know them. If you don’t consciously compensate for the lessons of history in this regard; then you will promptly say the decades-off thing. Fermi wasn’t still thinking that net nuclear energy was impossible or decades away by the time he got to 3 months before he built the first pile, because at that point Fermi was looped in on everything and saw how to do it. But anyone not looped in probably still felt like it was fifty years away while the actual pile was fizzing away in a squash court at the University of Chicago.&lt;/p&gt;
&lt;p&gt;People don’t seem to automatically compensate for the fact that the timing of the big development is a function of the peak knowledge in the field, a threshold touched by the people who know the most and have the best ideas; while they themselves have average knowledge; and therefore what they themselves know is not strong evidence about when the big development happens. I think they aren’t thinking about that at all, and they just eyeball it using their own sense of difficulty. If they are thinking anything more deliberate and reflective than that, and incorporating real work into correcting for the factors that might bias their lenses, they haven’t bothered writing down their reasoning anywhere I can read it.&lt;/p&gt;
&lt;p&gt;To know that AGI is decades away, we would need enough understanding of AGI to know what pieces of the puzzle are missing, and how hard these pieces are to obtain; and that kind of insight is unlikely to be available until the puzzle is complete. Which is also to say that to anyone outside the leading edge, the puzzle will look more incomplete than it looks on the edge. That project may publish their theories in advance of proving them, although I hope not. But there are unproven theories now too.&lt;/p&gt;
&lt;p&gt;And again, that’s not to say that people saying “fifty years” is a certain sign that something is happening in a squash court; they were saying “fifty years” sixty years ago too. It’s saying that anyone who thinks technological &lt;em&gt;timelines&lt;/em&gt; are actually forecastable, in advance, by people who are not looped in to the leading project’s progress reports and who don’t share all the best ideas about exactly how to do the thing and how much effort is required for that, is learning the wrong lesson from history. In particular, from reading history books that neatly lay out lines of progress and their visible signs that we all know &lt;em&gt;now&lt;/em&gt; were important and evidential. It’s sometimes possible to say useful conditional things about the consequences of the big development whenever it happens, but it’s rarely possible to make confident predictions about the &lt;em&gt;timing&lt;/em&gt; of those developments, beyond a one- or two-year horizon. And if you are one of the rare people who can call the timing, if people like that even exist, nobody else knows to pay attention to you and not to the Excited Futurists or Sober Skeptics.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Four:&lt;/strong&gt; The future uses different tools, and can therefore easily do things that are very hard now, or do with difficulty things that are impossible now.&lt;/p&gt;
&lt;p&gt;Why do we know that AGI is decades away? In popular articles penned by heads of AI research labs and the like, there are typically three prominent reasons given:&lt;/p&gt;
&lt;p&gt;(A) The author does not know how to build AGI using present technology. The author does not know where to start.&lt;/p&gt;
&lt;p&gt;(B) The author thinks it is really very hard to do the impressive things that modern AI technology does, they have to slave long hours over a hot GPU farm tweaking hyperparameters to get it done. They think that the public does not appreciate how hard it is to get anything done right now, and is panicking prematurely because the public thinks anyone can just fire up Tensorflow and build a robotic car.&lt;/p&gt;
&lt;p&gt;(C) The author spends a lot of time interacting with AI systems and therefore is able to personally appreciate all the ways in which they are still stupid and lack common sense.&lt;/p&gt;
&lt;p&gt;We’ve now considered some aspects of argument A. Let’s consider argument B for a moment.&lt;/p&gt;
&lt;p&gt;Suppose I say: “It is now possible for one comp-sci grad to do in a week anything that N+ years ago the research community could do with neural networks &lt;em&gt;at all&lt;/em&gt;.” How large is N?&lt;/p&gt;
&lt;p&gt;I got some answers to this on Twitter from people whose credentials I don’t know, but the most common answer was five, which sounds about right to me based on my own acquaintance with machine learning. (Though obviously not as a literal universal, because reality is never that neat.) If you could do something in 2012 period, you can probably do it fairly straightforwardly with modern GPUs, Tensorflow, Xavier initialization, batch normalization, ReLUs, and Adam or RMSprop or just stochastic gradient descent with momentum. The modern techniques are just that much better. To be sure, there are things we can’t do now with just those simple methods, things that require tons more work, but those things were not possible at all in 2012.&lt;/p&gt;
&lt;p&gt;In machine learning, when you can do something at all, you are probably at most a few years away from being able to do it easily using the future’s much superior tools. From this standpoint, argument B, “You don’t understand how hard it is to do what we do,” is something of a non-sequitur when it comes to timing.&lt;/p&gt;
&lt;p&gt;Statement B sounds to me like the same sentiment voiced by Rutherford &lt;a href=&quot;https://www.edge.org/conversation/the-myth-of-ai#26015&quot;&gt;in 1933&lt;/a&gt; when he called net energy from atomic fission “moonshine”. If you were a nuclear physicist in 1933 then you had to split all your atoms by hand, by bombarding them with other particles, and it was a laborious business. If somebody talked about getting net energy from atoms, maybe it made you feel that you were unappreciated, that people thought your job was easy.&lt;/p&gt;
&lt;p&gt;But of course this will always be the lived experience for AI engineers on serious frontier projects. You don’t get paid big bucks to do what a grad student can do in a week (unless you’re working for a bureaucracy with no clue about AI; but that’s not Google or FB). Your personal experience will &lt;em&gt;always&lt;/em&gt; be that what you are paid to spend months doing is difficult. A change in this personal experience is therefore not something you can use as a fire alarm.&lt;/p&gt;
&lt;p&gt;Those playing the part of wiser sober skeptical scientists would obviously agree in the abstract that our tools will improve; but in the popular articles they pen, they just talk about the painstaking difficulty of this year’s tools. I think that when they’re in that mode they are not even trying to forecast what the tools will be like in 5 years; they haven’t written down any such arguments as part of the articles I’ve read. I think that when they tell you that AGI is decades off, they are literally giving an estimate of &lt;a href=&quot;https://www.readthesequences.com/UnboundedScalesHugeJuryAwardsAndFuturism&quot;&gt;how long it feels to them&lt;/a&gt; like it would take to build AGI using their current tools and knowledge. Which is why they emphasize how hard it is to stir the heap of linear algebra until it spits out good answers; I think they are not imagining, at all, into how this experience may change over considerably less than fifty years. If they’ve explicitly considered the bias of estimating future tech timelines based on their present subjective sense of difficulty, and tried to compensate for that bias, they haven’t written that reasoning down anywhere I’ve read it. Nor have I ever heard of that forecasting method giving good results historically.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Five:&lt;/strong&gt; Okay, let’s be blunt here. I don’t think most of the discourse about AGI being far away (&lt;em&gt;or&lt;/em&gt; that it’s near) is being generated by models of future progress in machine learning. I don’t think we’re looking at wrong models; I think we’re looking at no models.&lt;/p&gt;
&lt;p&gt;I was once at a conference where there was a panel full of famous AI luminaries, and most of the luminaries were nodding and agreeing with each other that of course AGI was very far off, except for two famous AI luminaries who stayed quiet and let others take the microphone.&lt;/p&gt;
&lt;p&gt;I got up in Q&amp;amp;A and said, “Okay, you’ve all told us that progress won’t be all that fast. But let’s be more concrete and specific. I’d like to know what’s the &lt;em&gt;least&lt;/em&gt; impressive accomplishment that you are very confident &lt;em&gt;cannot&lt;/em&gt; be done in the next two years.”&lt;/p&gt;
&lt;p&gt;There was a silence.&lt;/p&gt;
&lt;p&gt;Eventually, two people on the panel ventured replies, spoken in a rather more tentative tone than they’d been using to pronounce that AGI was decades out. They named “A robot puts away the dishes from a dishwasher without breaking them”, and &lt;a href=&quot;http://www.cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html&quot;&gt;Winograd schemas&lt;/a&gt;. Specifically, “I feel quite confident that the Winograd schemas—where we recently had a result that was in the 50, 60% range—in the next two years, we will not get 80, 90% on that regardless of the techniques people use.”&lt;/p&gt;
&lt;p&gt;A few months after that panel, there was unexpectedly a big breakthrough on Winograd schemas. The breakthrough didn’t crack 80%, so three cheers for wide credibility intervals with error margin, but I expect the predictor might be feeling slightly more nervous now with one year left to go. (I don’t think it was the breakthrough I remember reading about, but Rob turned up &lt;a href=&quot;https://l.facebook.com/l.php?u=https%3A%2F%2Fwww.ijcai.org%2Fproceedings%2F2017%2F0326.pdf&amp;amp;h=ATMiIliuWNyZbf0ezht51f12W7gL1Gw1AgwfGsF2MUCMMNa_sw9vB1iS6etZYPeiaJuKbxZR92VAbn7uJAZwUHkXm59JK0pBI4cB2ve9rKRpz0vKGXozkvegWE7gbiUWuoP8BwLo0_0mhpnIdbfiO9X9Dpw&quot;&gt;this paper&lt;/a&gt; as an example of one that could have been submitted at most 44 days after the above conference and gets up to 70%.)&lt;/p&gt;
&lt;p&gt;But that’s not the point. The point is the silence that fell after my question, and that eventually I only got two replies, spoken in tentative tones. When I asked for concrete feats that were impossible in the next two years, I think that that’s when the luminaries on that panel switched to trying to build a mental model of future progress in machine learning, asking themselves what they could or couldn’t predict, what they knew or didn’t know. And to their credit, most of them did know their profession well enough to realize that forecasting future boundaries around a rapidly moving field is actually &lt;em&gt;really hard&lt;/em&gt;, that nobody knows what will appear on arXiv next month, and that they needed to put wide credibility intervals with very generous upper bounds on how much progress might take place twenty-four months’ worth of arXiv papers later.&lt;/p&gt;
&lt;p&gt;(Also, Demis Hassabis was present, so they all knew that if they named something insufficiently impossible, Demis would have DeepMind go and do it.)&lt;/p&gt;
&lt;p&gt;The question I asked was in a completely different genre from the panel discussion, requiring a mental context switch: the assembled luminaries actually had to try to consult their rough, scarce-formed intuitive models of progress in machine learning and figure out what future experiences, if any, their model of the field definitely prohibited within a two-year time horizon. Instead of, well, emitting socially desirable verbal behavior meant to kill that darned hype about AGI and get some predictable applause from the audience.&lt;/p&gt;
&lt;p&gt;I’ll be blunt: I don’t think the confident long-termism has been thought out at all. If your model has the extraordinary power to say what will be impossible in ten years after another one hundred and twenty months of arXiv papers, then you ought to be able to say much weaker things that are impossible in two years, and you should have those predictions queued up and ready to go rather than falling into nervous silence after being asked.&lt;/p&gt;
&lt;p&gt;In reality, the two-year problem is hard and the ten-year problem is laughably hard. The future is hard to predict in general, our predictive grasp on a rapidly changing and advancing field of science and engineering is very weak indeed, and it doesn’t permit narrow credible intervals on what can’t be done.&lt;/p&gt;
&lt;p&gt;Grace et al. (&lt;a href=&quot;https://arxiv.org/abs/1705.08807&quot;&gt;2017&lt;/a&gt;) surveyed the predictions of 352 presenters at ICML and NIPS 2015. Respondents’ aggregate forecast was that the proposition “all occupations are fully automatable” (in the sense that “for any occupation, machines could be built to carry out the task better and more cheaply than human workers”) will not reach 50% probability until 121 years hence. Except that a randomized subset of respondents were instead asked the slightly different question of “when unaided machines can accomplish every task better and more cheaply than human workers”, and in this case held that this was 50% likely to occur &lt;a href=&quot;http://www.bayesianinvestor.com/blog/index.php/2017/06/01/do-ai-experts-exist/&quot;&gt;within 44 years&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;That’s what happens when you ask people to produce an estimate they can’t estimate, and there’s a social sense of what the desirable verbal behavior is supposed to be.&lt;/p&gt;

&lt;hr/&gt;
&lt;p&gt;When I observe that there’s no fire alarm for AGI, I’m not saying that there’s no possible equivalent of smoke appearing from under a door.&lt;/p&gt;
&lt;p&gt;What I’m saying rather is that the smoke under the door is always going to be arguable; it is not going to be a clear and undeniable and absolute sign of fire; and so there is never going to be a fire alarm producing common knowledge that action is now due and socially acceptable.&lt;/p&gt;
&lt;p&gt;There’s an old trope saying that as soon as something is actually done, it ceases to be called AI. People who work in AI and are in a broad sense pro-accelerationist and techno-enthusiast, what you might call the Kurzweilian camp (of which I am not a member), will sometimes rail against this as unfairness in judgment, as moving goalposts.&lt;/p&gt;
&lt;p&gt;This overlooks a real and important phenomenon of adverse selection against AI accomplishments: If you can do something impressive-sounding with AI in 1974, then that is because that thing turned out to be doable in some cheap cheaty way, not because 1974 was so amazingly great at AI. We are uncertain about how much cognitive effort it takes to perform tasks, and how easy it is to cheat at them, and the first “impressive” tasks to be accomplished will be those where we were most wrong about how much effort was required. There was a time when some people thought that a computer winning the world chess championship would require progress in the direction of AGI, and that this would count as a sign that AGI was getting closer. When Deep Blue beat Kasparov in 1997, in a Bayesian sense we did learn something about progress in AI, but we also learned something about chess being easy. Considering the techniques used to construct Deep Blue, most of what we learned was “It is surprisingly possible to play chess without easy-to-generalize techniques” and not much “A surprising amount of progress has been made toward AGI.”&lt;/p&gt;
&lt;p&gt;Was AlphaGo smoke under the door, a sign of AGI in 10 years or less? People had previously given Go as an example of What You See Before The End.&lt;/p&gt;
&lt;p&gt;Looking over the paper describing AlphaGo’s architecture, it seemed to me that we &lt;em&gt;were&lt;/em&gt; mostly learning that available AI techniques were likely to go further towards generality than expected, rather than about Go being surprisingly easy to achieve with fairly narrow and ad-hoc approaches. Not that the method scales to AGI, obviously; but AlphaGo did look like a product of &lt;em&gt;relatively&lt;/em&gt; general insights and techniques being turned on the special case of Go, in a way that Deep Blue wasn’t. I also updated significantly on “The general learning capabilities of the human cortical algorithm are less impressive, less difficult to capture with a ton of gradient descent and a zillion GPUs, than I thought,” because if there were anywhere we expected an impressive hard-to-match highly-natural-selected but-still-general cortical algorithm to come into play, it would be in humans playing Go.&lt;/p&gt;
&lt;p&gt;Maybe if we’d seen a thousand Earths undergoing similar events, we’d gather the statistics and find that a computer winning the planetary Go championship is a reliable ten-year-harbinger of AGI. But I don’t actually know that. Neither do you. Certainly, anyone can publicly argue that we just learned Go was easier to achieve with strictly narrow techniques than expected, as was true many times in the past. There’s no possible sign short of actual AGI, no case of smoke from under the door, for which we know that this is definitely serious fire and now AGI is 10, 5, or 2 years away. Let alone a sign where we know everyone else will believe it.&lt;/p&gt;
&lt;p&gt;And in any case, multiple leading scientists in machine learning have already published articles telling us their criterion for a fire alarm. They will believe Artificial General Intelligence is imminent:&lt;/p&gt;
&lt;p&gt;(A) When they personally see how to construct AGI using their current tools. This is what they are always saying is not currently true in order to castigate the folly of those who think AGI might be near.&lt;/p&gt;
&lt;p&gt;(B) When their personal jobs do not give them a sense of everything being difficult. This, they are at pains to say, is a key piece of knowledge not possessed by the ignorant layfolk who think AGI might be near, who only believe that because they have never stayed up until 2AM trying to get a generative adversarial network to stabilize.&lt;/p&gt;
&lt;p&gt;(C) When they are very impressed by how smart their AI is relative to a human being in respects that still feel magical to them; as opposed to the parts they do know how to engineer, which no longer seem magical to them; aka the AI seeming pretty smart in interaction and conversation; aka the AI actually being an AGI already.&lt;/p&gt;
&lt;p&gt;So there isn’t going to be a fire alarm. Period.&lt;/p&gt;
&lt;p&gt;There is never going to be a time before the end when you can look around nervously, and see that it is now clearly common knowledge that you can talk about AGI being imminent, and take action and exit the building in an orderly fashion, without fear of looking stupid or frightened.&lt;/p&gt;

&lt;hr/&gt;
&lt;p&gt;So far as I can presently estimate, now that we’ve had AlphaGo and a couple of other maybe/maybe-not shots across the bow, and seen a huge explosion of effort invested into machine learning and an enormous flood of papers, we are probably going to occupy our present epistemic state until very near the end.&lt;/p&gt;
&lt;p&gt;By saying we’re probably going to be in roughly this epistemic state until almost the end, I &lt;em&gt;don’t&lt;/em&gt; mean to say we know that AGI is imminent, or that there won’t be important new breakthroughs in AI in the intervening time. I mean that it’s hard to guess how many further insights are needed for AGI, or how long it will take to reach those insights. After the next breakthrough, we still won’t know how many more breakthroughs are needed, leaving us in pretty much the same epistemic state as before. Whatever discoveries and milestones come next, it will probably continue to be hard to guess how many further insights are needed, and timelines will continue to be similarly murky. Maybe researcher enthusiasm and funding will rise further, and we’ll be able to say that timelines are shortening; or maybe we’ll hit another AI winter, and we’ll know that’s a sign indicating that things will take longer than they would otherwise; but we still won’t know &lt;em&gt;how long.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;At some point we might see a sudden flood of arXiv papers in which really interesting and fundamental and scary cognitive challenges seem to be getting done at an increasing pace. Whereupon, as this flood accelerates, even some who imagine themselves sober and skeptical will be unnerved to the point that they venture that perhaps AGI is only 15 years away now, maybe, possibly. The signs might become so blatant, very soon before the end, that people start thinking it is socially acceptable to say that maybe AGI is 10 years off. Though the signs would have to be pretty darned blatant, if they’re to overcome the social barrier posed by luminaries who are estimating arrival times to AGI using their personal knowledge and personal difficulties, as well as all the historical bad feelings about AI winters caused by hype.&lt;/p&gt;
&lt;p&gt;But even if it becomes socially acceptable to say that AGI is 15 years out, in those last couple of years or months, I would still expect there to be disagreement. There will still be others protesting that, as much as associative memory and human-equivalent cerebellar coordination (or whatever) are now solved problems, they still don’t know how to construct AGI. They will note that there are no AIs writing computer science papers, or holding a truly sensible conversation with a human, and castigate the senseless alarmism of those who talk as if we already knew how to do that. They will explain that foolish laypeople don’t realize how much pain and tweaking it takes to get the current systems to work. (Although those modern methods can easily do almost anything that was possible in 2017, and any grad student knows how to roll a stable GAN on the first try using the tf.unsupervised module in Tensorflow 5.3.1.)&lt;/p&gt;
&lt;p&gt;When all the pieces are ready and in place, lacking only the last piece to be assembled by the very peak of knowledge and creativity across the whole world, it will still seem to the average ML person that AGI is an enormous challenge looming in the distance, because they still won’t personally know how to construct an AGI system. Prestigious heads of major AI research groups will still be writing &lt;a href=&quot;https://www.technologyreview.com/s/608986/forget-killer-robotsbias-is-the-real-ai-danger/&quot;&gt;articles&lt;/a&gt; decrying the folly of fretting about the total destruction of all Earthly life and all future value it could have achieved, and saying that we should not let this distract us from &lt;em&gt;real, respectable concerns&lt;/em&gt; like loan-approval systems accidentally absorbing human biases.&lt;/p&gt;
&lt;p&gt;Of course, the future is very hard to predict in detail. It’s so hard that not only do I confess my own inability, I make the far stronger positive statement that nobody else can do it either. The “flood of groundbreaking arXiv papers” scenario is one way things could maybe possibly go, but it’s an implausibly specific scenario that I made up for the sake of concreteness. It’s certainly not based on my extensive experience watching other Earthlike civilizations develop AGI. I do put a significant chunk of probability mass on “There’s not much sign visible outside a Manhattan Project until Hiroshima,” because that scenario is simple. Anything more complex is just one more story full of &lt;a href=&quot;https://www.readthesequences.com/Burdensome-Details&quot;&gt;burdensome details&lt;/a&gt; that aren’t likely to all be true.&lt;/p&gt;
&lt;p&gt;But no matter how the details play out, I do predict in a very general sense that there will be no fire alarm that is not an actual running AGI—no unmistakable sign before then that everyone knows and agrees on, that lets people act without feeling nervous about whether they’re worrying too early. That’s just not how the history of technology has usually played out in much simpler cases like flight and nuclear engineering, let alone a case like this one where all the signs and models are disputed. We already know enough about the uncertainty and low quality of discussion surrounding this topic to be able to say with confidence that there will be no unarguable socially accepted sign of AGI arriving 10 years, 5 years, or 2 years beforehand. If there’s any general social panic it will be by coincidence, based on terrible reasoning, uncorrelated with real timelines except by total coincidence, set off by a Hollywood movie, and focused on relatively trivial dangers.&lt;/p&gt;
&lt;p&gt;It’s no coincidence that nobody has given any actual account of such a fire alarm, and argued convincingly about how much time it means we have left, and what projects we should only then start. If anyone does write that proposal, the next person to write one will say something completely different. And probably neither of them will succeed at convincing me that they know anything prophetic about timelines, or that they’ve identified any sensible angle of attack that is (a) worth pursuing at all and (b) not worth starting to work on right now.&lt;/p&gt;

&lt;hr/&gt;
&lt;p&gt;It seems to me that the decision to delay all action until a nebulous totally unspecified future alarm goes off, implies an order of recklessness great enough that the law of continued failure comes into play.&lt;/p&gt;
&lt;p&gt;The law of continued failure is the rule that says that if your country is incompetent enough to use a plaintext 9-numeric-digit password on all of your bank accounts and credit applications, your country is not competent enough to correct course after the next disaster in which a hundred million passwords are revealed. A civilization competent enough to correct course in response to that prod, to react to it the way you’d want them to react, is competent enough not to make the mistake in the first place. When a system fails massively and obviously, rather than subtly and at the very edges of competence, the next prod is not going to cause the system to suddenly snap into doing things intelligently.&lt;/p&gt;
&lt;p&gt;The law of continued failure is especially important to keep in mind when you are dealing with big powerful systems or high-status people that you might feel nervous about derogating, because you may be tempted to say, “Well, it’s flawed now, but as soon as a future prod comes along, everything will snap into place and everything will be all right.” The systems about which this fond hope is actually warranted look like they are mostly doing all the important things right already, and only failing in one or two steps of cognition. The fond hope is almost never warranted when a person or organization or government or social subsystem is currently falling massively short.&lt;/p&gt;
&lt;p&gt;The folly required to ignore the prospect of aliens landing in thirty years is already great enough that the other flawed elements of the debate should come as no surprise.&lt;/p&gt;
&lt;p&gt;And with all of that going wrong simultaneously today, we should predict that the same system and incentives won’t produce correct outputs after receiving an uncertain sign that maybe the aliens are landing in five years instead. The law of continued failure suggests that if existing authorities failed in enough different ways at once to think that it makes sense to try to derail a conversation about existential risk by saying the real problem is the security on self-driving cars, the default expectation is that they will still be saying silly things later.&lt;/p&gt;
&lt;p&gt;People who make large numbers of simultaneous mistakes don’t generally have all of the incorrect thoughts subconsciously labeled as “incorrect” in their heads. Even when motivated, they can’t suddenly flip to skillfully executing all-correct reasoning steps instead. Yes, we have various experiments showing that monetary incentives can reduce overconfidence and political bias, but (a) that’s reduction rather than elimination, (b) it’s with extremely clear short-term direct incentives, not the nebulous and politicizable incentive of “a lot being at stake”, and (c) that doesn’t mean a switch is flipping all the way to “carry out complicated correct reasoning”. If someone’s brain contains a switch that can flip to enable complicated correct reasoning at all, it’s got enough internal precision and skill to think mostly-correct thoughts now instead of later—at least to the degree that some conservatism and double-checking gets built into examining the conclusions that people know will get them killed if they’re wrong about them.&lt;/p&gt;
&lt;p&gt;There is no sign and portent, &lt;a href=&quot;http://lesswrong.com/lw/hp5/after_critical_event_w_happens_they_still_wont/&quot;&gt;no threshold crossed&lt;/a&gt;, that suddenly causes people to wake up and start doing things systematically correctly. People who can react that competently to any sign at all, let alone a less-than-perfectly-certain not-totally-agreed item of evidence that is &lt;em&gt;likely&lt;/em&gt; a wakeup call, have probably already done the timebinding thing. They’ve already imagined the future sign coming, and gone ahead and thought sensible thoughts earlier, like Stuart Russell saying, “If you know the aliens are landing in thirty years, it’s still a big deal now.”&lt;/p&gt;

&lt;hr/&gt;
&lt;p&gt;Back in the funding-starved early days of what is now MIRI, I learned that people who donated last year were likely to donate this year, and people who last year were planning to donate “next year” would quite often this year be planning to donate “next year”. Of course there were genuine transitions from zero to one; everything that happens needs to happen for a first time. There were college students who said “later” and gave nothing for a long time in a genuinely strategically wise way, and went on to get nice jobs and start donating. But I also learned well that, like many cheap and easy solaces, saying the word “later” is addictive; and that this luxury is available to the rich as well as the poor.&lt;/p&gt;
&lt;p&gt;I don’t expect it to be any different with AGI alignment work. People who are trying to get what grasp they can on the alignment problem will, in the next year, be doing a little (or a lot) better with whatever they grasped in the previous year (plus, yes, any general-field advances that have taken place in the meantime). People who want to defer that until after there’s a better understanding of AI and AGI will, after the next year’s worth of advancements in AI and AGI, want to defer work until a better future understanding of AI and AGI.&lt;/p&gt;
&lt;p&gt;Some people really &lt;em&gt;want&lt;/em&gt; alignment to &lt;em&gt;get done&lt;/em&gt; and are therefore &lt;em&gt;now&lt;/em&gt; trying to wrack their brains about how to get something like a reinforcement learner to &lt;a href=&quot;https://arbital.com/p/pointing_finger/&quot;&gt;reliably identify a utility function over particular elements in a model of the causal environment instead of a sensory reward term&lt;/a&gt; or &lt;a href=&quot;https://arbital.com/p/updated_deference/&quot;&gt;defeat the seeming tautologicalness of updated (non-)deference&lt;/a&gt;. Others would rather be working on other things, and will therefore declare that there is no work that can possibly be done today, &lt;em&gt;not&lt;/em&gt; spending two hours quietly thinking about it first before making that declaration. And this will not change tomorrow, unless perhaps tomorrow is when we wake up to some interesting newspaper headlines, and probably not even then. The luxury of saying “later” is not available only to the truly poor-in-available-options.&lt;/p&gt;
&lt;p&gt;After a while, I started telling effective altruists in college: “If you’re planning to earn-to-give later, then for now, give around $5 every three months. And never give exactly the same amount twice in a row, or give to the same organization twice in a row, so that you practice the mental habit of re-evaluating causes and re-evaluating your donation amounts on a regular basis. &lt;em&gt;Don’t&lt;/em&gt; learn the mental habit of just always saying ‘later’.”&lt;/p&gt;
&lt;p&gt;Similarly, if somebody was &lt;em&gt;actually&lt;/em&gt; going to work on AGI alignment “later”, I’d tell them to, every six months, spend a couple of hours coming up with the best current scheme they can devise for aligning AGI and doing useful work on that scheme. Assuming, if they must, that AGI were somehow done with technology resembling current technology. And publishing their best-current-scheme-that-isn’t-good-enough, at least in the sense of posting it to Facebook; so that they will have a sense of embarrassment about naming a scheme that does not look like somebody actually spent two hours trying to think of the best bad approach.&lt;/p&gt;
&lt;p&gt;There are things we’ll better understand about AI in the future, and things we’ll learn that might give us more confidence that particular research approaches will be relevant to AGI. There may be more future sociological developments akin to Nick Bostrom publishing &lt;em&gt;Superintelligence&lt;/em&gt;, Elon Musk tweeting about it and thereby heaving a rock through the Overton Window, or more respectable luminaries like Stuart Russell openly coming on board. The future will hold more AlphaGo-like events to publicly and privately highlight new ground-level advances in ML technique; and it may somehow be that this does &lt;em&gt;not&lt;/em&gt; leave us in the same epistemic state as having already seen AlphaGo and GANs and the like. It could happen! I can’t see exactly how, but the future does have the capacity to pull surprises in that regard.&lt;/p&gt;
&lt;p&gt;But before waiting on that surprise, you should ask whether your uncertainty about AGI timelines is really uncertainty at all. If it feels to you that guessing AGI might have a 50% probability in N years is not enough knowledge to act upon, if that feels scarily uncertain and you want to wait for more evidence before making any decisions… then ask yourself how you’d feel if you believed the probability was 50% in N years, and everyone else on Earth also believed it was 50% in N years, and everyone believed it was right and proper to carry out policy P when AGI has a 50% probability of arriving in N years. If that visualization feels very different, then any nervous “uncertainty” you feel about doing P is not really about whether AGI takes much longer than N years to arrive.&lt;/p&gt;
&lt;p&gt;And you are almost surely going to be stuck with that feeling of “uncertainty” no matter how close AGI gets; because no matter how close AGI gets, whatever signs appear will almost surely not produce common, shared, agreed-on public knowledge that AGI has a 50% chance of arriving in N years, nor any agreement that it is therefore right and proper to react by doing P.&lt;/p&gt;
&lt;p&gt;And if all that did become common knowledge, then P is unlikely to still be a neglected intervention, or AI alignment a neglected issue; so you will have waited until sadly late to help.&lt;/p&gt;
&lt;p&gt;But far more likely is that the common knowledge just isn’t going to be there, and so it will always feel nervously “uncertain” to consider acting.&lt;/p&gt;
&lt;p&gt;You can either act despite that, or not act. Not act until it’s too late to help much, in the best case; not act at all until after it’s essentially over, in the average case.&lt;/p&gt;
&lt;p&gt;I don’t think it’s wise to wait on an unspecified epistemic miracle to change how we feel. In all probability, you’re going to be in this mental state for a while—including any nervous-feeling “uncertainty”. If you handle this mental state by saying “later”, that general policy is not likely to have good results for Earth.&lt;/p&gt;

&lt;hr/&gt;
&lt;p&gt;Further resources:&lt;/p&gt;

&lt;hr/&gt;&lt;div id=&quot;DidYouLike&quot; class=&quot;well remove-bottom&quot;&gt;
&lt;p&gt;&lt;strong&gt;Did you like this post?&lt;/strong&gt; You may enjoy our other &lt;a href=&quot;https://intelligence.org/category/analysis/&quot; title=&quot;View all posts in Analysis&quot;&gt;Analysis&lt;/a&gt; posts, including:&lt;/p&gt;
&lt;/div&gt;
</description>
<pubDate>Sat, 14 Oct 2017 05:26:14 +0000</pubDate>
<dc:creator>MBlume</dc:creator>
<og:type>article</og:type>
<og:title>There's No Fire Alarm for Artificial General Intelligence - Machine Intelligence Research Institute</og:title>
<og:description>  What is the function of a fire alarm?   One might think that the function of a fire alarm is to provide you with important evidence about a fire existing, allowing you to change your policy accordingly and exit the building. In the classic experiment by Latane and Darley in 1968, eight groups of... Read more »</og:description>
<og:url>https://intelligence.org/2017/10/13/fire-alarm/</og:url>
<og:image>https://intelligence.org/files/mirilogofb.jpg</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://intelligence.org/2017/10/13/fire-alarm/</dc:identifier>
</item>
<item>
<title>IGN acquires Humble Bundle</title>
<link>https://techcrunch.com/2017/10/13/ign-acquires-pay-what-you-want-game-shop-humble-bundle/</link>
<guid isPermaLink="true" >https://techcrunch.com/2017/10/13/ign-acquires-pay-what-you-want-game-shop-humble-bundle/</guid>
<description>&lt;img src=&quot;https://tctechcrunch2011.files.wordpress.com/2017/10/humblebundle.jpg?w=738&quot; class=&quot;&quot;/&gt;&lt;p id=&quot;speakable-summary&quot;&gt;Online media giant IGN has &lt;a target=&quot;_blank&quot; href=&quot;http://blog.humblebundle.com/post/166366386976/humble-bundle-is-joining-forces-with-ign&quot; rel=&quot;noopener&quot;&gt;acquired Humble Bundle&lt;/a&gt;, the indie games storefront best known for its pay-what-you-want bundles that raise money for charity. The company says nothing will change for users and that “we will keep our own office, culture, and amazing team with IGN helping us further our plans.”&lt;/p&gt;&lt;p&gt;Humble Bundle &lt;a target=&quot;_blank&quot; href=&quot;https://techcrunch.com/2010/05/04/humble-indie-bundle-lets-you-pay-what-you-want-for-a-stellar-lineup/&quot; rel=&quot;noopener&quot;&gt;started in 2010&lt;/a&gt;, its first bundle (the Humble Indie Bundle) comprising World of Goo, Gish, Aquaria and several other notable indie games, for which users could pay anything from a nickel to well over retail price. With 138,000 purchases paying an average of just over $9, it raised $1.27 million, with a significant portion going to charity (Child’s Play, if I remember correctly).&lt;/p&gt;
&lt;div id=&quot;attachment_352185&quot; class=&quot;wp-caption aligncenter&quot; readability=&quot;32&quot;&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://tctechcrunch2011.files.wordpress.com/2010/05/humble.jpg&quot; rel=&quot;noopener&quot;&gt;&lt;img class=&quot;wp-image-352185 size-full&quot; src=&quot;https://tctechcrunch2011.files.wordpress.com/2010/05/humble.jpg?w=620&amp;amp;h=315&quot; alt=&quot;&quot; width=&quot;620&quot; height=&quot;315&quot; srcset=&quot;https://tctechcrunch2011.files.wordpress.com/2010/05/humble.jpg 620w, https://tctechcrunch2011.files.wordpress.com/2010/05/humble.jpg?w=150&amp;amp;h=76 150w, https://tctechcrunch2011.files.wordpress.com/2010/05/humble.jpg?w=300&amp;amp;h=152 300w&quot; sizes=&quot;(max-width: 620px) 100vw, 620px&quot;/&gt;&lt;/a&gt;
&lt;p class=&quot;wp-caption-text&quot;&gt;I uploaded this picture to CrunchGear in 2010.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Since then there have been dozens upon dozens of bundles and sales, with the company branching out to e-books and music as well as games. Over $100 million has been raised for charity since the company started up, and a similar amount for the developers involved — an optional small percentage could be designated as a “Humble tip” to cover operations.&lt;/p&gt;
&lt;p&gt;There’s a separate, permanent store for games not currently in bundles, focused on making the sales process easy for indie developers (with 10 percent going to charity).&lt;/p&gt;
&lt;p&gt;The launch in 2015 of the “Humble Monthly Bundle” gave users who paid $12 a month a new set of games to download regularly; this was originally viewed with some skepticism, but seems to have panned out well, even if the main result is probably a deeply overpopulated Steam library.&lt;/p&gt;

&lt;p&gt;Joining IGN, a media conglomerate, doesn’t really seem like a natural move, I’d say, but it’s more than a little possible that Humble Bundle has aspirations to move beyond a variety of storefronts — aspirations that require more resources than a young 60-person company has access to.&lt;/p&gt;

&lt;p&gt;I asked about this and co-founder John Graham offered the following:&lt;/p&gt;
&lt;blockquote readability=&quot;11&quot;&gt;
&lt;p&gt;&lt;span class=&quot;m_-4785067566945669621m_4767227354693258655m_-1800593470615293916gmail-m_7977801432699554997gmail-im&quot;&gt;Jeff and I started Humble Bundle in 2010 living in our parents’ homes, and never dreamed that we would be where we are today. We were not looking to sell the company, we were just focused on what we’ve always been doing: creating great games for our community while supporting charitable organizations around the world. We made a business we were passionate about without an exit in mind, and found a partner who understands who we are and what we do. IGN wants to give us the support and resources to help us do all of the things we already do better and faster.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;IGN has a rough reputation among gamers, some of whom think of it as being too friendly with the greater gaming industry — trading access for favorable reviews is a common accusation, though almost certainly an untrue one. But the idea that the same parent company that owns a game store also owns a major game review site does raise some pretty obvious (although far from unique) conflict of interest concerns.&lt;/p&gt;
&lt;p&gt;However, I (as a reader of IGN and a customer of Humble Bundle) have faith in both companies to keep a reasonable amount of separation between the two. It would only undermine both to mix them together in some unsavory way.&lt;/p&gt;
&lt;p&gt;I’ve contacted Humble Bundle for more details on the deal and will update this post if I hear back.&lt;/p&gt;
</description>
<pubDate>Sat, 14 Oct 2017 03:49:08 +0000</pubDate>
<dc:creator>doppp</dc:creator>
<og:title>IGN acquires pay-what-you-want game shop Humble Bundle</og:title>
<og:description>Online media giant IGN has acquired Humble Bundle, the indie games storefront best known for its pay-what-you-want bundles that raise money for charity. The..</og:description>
<og:image>https://tctechcrunch2011.files.wordpress.com/2017/10/humblebundle.jpg</og:image>
<og:url>http://social.techcrunch.com/2017/10/13/ign-acquires-pay-what-you-want-game-shop-humble-bundle/</og:url>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://techcrunch.com/2017/10/13/ign-acquires-pay-what-you-want-game-shop-humble-bundle/</dc:identifier>
</item>
<item>
<title>Psilocybin for treatment-resistant depression: fMRI-measured brain mechanisms [pdf]</title>
<link>https://www.nature.com/articles/s41598-017-13282-7.pdf</link>
<guid isPermaLink="true" >https://www.nature.com/articles/s41598-017-13282-7.pdf</guid>
<description>&lt;a href=&quot;https://www.nature.com/articles/s41598-017-13282-7.pdf&quot;&gt;Download PDF&lt;/a&gt;</description>
<pubDate>Fri, 13 Oct 2017 23:23:58 +0000</pubDate>
<dc:creator>dtawfik1</dc:creator>
<og:title>IGN acquires pay-what-you-want game shop Humble Bundle</og:title>
<og:description>Online media giant IGN has acquired Humble Bundle, the indie games storefront best known for its pay-what-you-want bundles that raise money for charity. The..</og:description>
<og:image>https://tctechcrunch2011.files.wordpress.com/2017/10/humblebundle.jpg</og:image>
<og:url>http://social.techcrunch.com/2017/10/13/ign-acquires-pay-what-you-want-game-shop-humble-bundle/</og:url>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>application/pdf</dc:format>
<dc:identifier>https://www.nature.com/articles/s41598-017-13282-7.pdf</dc:identifier>
</item>
</channel>
</rss>