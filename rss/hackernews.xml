<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Ask HN: Best way to get started with AI?</title>
<link>https://news.ycombinator.com/item?id=15689399</link>
<guid isPermaLink="true" >https://news.ycombinator.com/item?id=15689399</guid>
<description>&lt;tr class=&quot;athing comtr&quot; id=&quot;15689936&quot; readability=&quot;6.4624390243902&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;3.2312195121951&quot;&gt;&lt;tr readability=&quot;6.4624390243902&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;9.6936585365854&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;18.941176470588&quot;&gt;&lt;span class=&quot;c00&quot;&gt;I'm in the same boat. For long time, I was interested in AI but at the same time intimidated by math. I'm relatively comfortable with discrete mathematics and classical algorithms and at the same time calculus and linear algebra is completely foreign to me. Also, I do not accept way to learn ML without good understanding of core principles behind it. So math is a must.&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;A few months ago, I stumbled upon very amazing YouTube Channel &lt;em&gt;3Blue1Brown&lt;/em&gt; which explains math in very accessible way and at the same time I got feeling that I finally started understanding core ideas behind linear algebra and calculus.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;Just recently he published 4 videos about deep neural networks:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=aircAruvnKk&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=aircAruvnKk&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=IHZwWFHWa-w&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=IHZwWFHWa-w&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Ilg3gGewQ5U&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=Ilg3gGewQ5U&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=tIeHLnjs5U8&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/watch?v=tIeHLnjs5U8&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;So my fear of ML was gone away and I'm very &lt;em&gt;excited&lt;/em&gt; to explore whole new world for neural networks and other things like support vector machines etc &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690235&quot; readability=&quot;5.4730392156863&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690347&quot; readability=&quot;4.4964028776978&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.2482014388489&quot;&gt;&lt;tr readability=&quot;4.4964028776978&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;80&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;3Blue1Brown is a treasure. The production value is excellent, and he's great at taking seemingly uninteresting ideas and painting a beautiful picture to connect them in twenty minutes. I used to go through a video before falling asleep each night.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690846&quot; readability=&quot;6.4615384615385&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;3.2307692307692&quot;&gt;&lt;tr readability=&quot;6.4615384615385&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Having watched the third one out of sequence, seeing the first two and then watching the third again helped me get a good understanding of the fundamentals. 3blue1brown as a narrator does a excellent job of allowing a rather tricky subject be more approachable, and inspired me to buy a course to allow a deeper dive into the math behind ML+NNs.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690544&quot; readability=&quot;2.5376344086022&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690274&quot; readability=&quot;3.6382978723404&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690185&quot; readability=&quot;2.4782608695652&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;1.2391304347826&quot;&gt;&lt;tr readability=&quot;2.4782608695652&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;I came here to write a similar comment. Really make sure to watch the playlists in the correct order on the above YouTube channel.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690257&quot; readability=&quot;1.5294117647059&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690901&quot; readability=&quot;3.5164835164835&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;1.7582417582418&quot;&gt;&lt;tr readability=&quot;3.5164835164835&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;80&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Classification and regression. Given examples, predict labels or values for new data. SVM used to be more «hot» than neural nets and are still very useful.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689532&quot; readability=&quot;4.0617283950617&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689615&quot; readability=&quot;4.8031496062992&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.4015748031496&quot;&gt;&lt;tr readability=&quot;4.8031496062992&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Andrew Ng's courses are excellent. Another pretty good Coursera course is Machine Learning Foundations from the University of Washington. It is very high level and novice friendly. While it covers non of the math and very little programming it does give a nice quick introduction to the most popular ML techniques out there and when to use them. It all depends on what level you are interested in starting. They also have follow up courses that go deeper into the different techniques.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689841&quot; readability=&quot;4.5756457564576&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.2878228782288&quot;&gt;&lt;tr readability=&quot;4.5756457564576&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;80&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;5.4907749077491&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;10.780876494024&quot;&gt;&lt;span class=&quot;c00&quot;&gt;+1 for this recommendation.&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;I would specifically recommend Machine Learning Foundations: A Case Study Approach - It is fantastic and helped me greatly start my ML journey last year.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;Turi is awesome, I hope Apple is doing something great with it. &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689599&quot; readability=&quot;1.3563218390805&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690065&quot; readability=&quot;6.8146279949559&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;3.4073139974779&quot;&gt;&lt;tr readability=&quot;6.8146279949559&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;80&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;9.2484237074401&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;17.883870967742&quot;&gt;&lt;span class=&quot;c00&quot;&gt;From my experience Andrew Ng wiped the floor with every other lecturer I've had. Both the ML and his new Deep Learning course.&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;If the lecturers aren't very interesting Coursera can be as hard as any other lectures. I gave up on the Scala functional programming and disappointingly have stalled with Geoffrey Hinton's Neural Networks courses.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;But I really can't understate how good Andrew Ng is, he has a very relaxed manner and manages to make some very complex topics seem almost trivial.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;The worst of the mathematics is derivatives and matrix multiplication. You can even avoid matrix multiplication mostly in the ML course, but in his Deep Learning course he takes you through the 300x performance benefit you get from using NumPy and matrix multiplication vs loops. &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689625&quot; readability=&quot;9.3279569892473&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;4.6639784946237&quot;&gt;&lt;tr readability=&quot;9.3279569892473&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;80&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;6.5295698924731&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;12.814285714286&quot;&gt;&lt;span class=&quot;c00&quot;&gt;At your level yes, I would recommend starting with the ML course. It is really beneficial to understanding how the mathematics work.&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;The two most important things to remember, since the courses are challenging: 1) don't be in a hurry, and 2) don't give up! Take the time to learn every detail presented, do the optional exercises, and dig deep. &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689663&quot; readability=&quot;4.5168067226891&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.2584033613445&quot;&gt;&lt;tr readability=&quot;4.5168067226891&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;120&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;It's definitely challenging. The math and just seeing the complicated formulas really push me, but the reward is good too. I'm tired of pushing pixels and doing some meaty stuffy like ML is a nice change of pace.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690412&quot; readability=&quot;4.6511627906977&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.3255813953488&quot;&gt;&lt;tr readability=&quot;4.6511627906977&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;80&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;I would recommend starting with deep learning first since that's what you are interested in and it covers all the ML principles you need to be familiar with. If you want to go deeper and get familiar with other ML techniques too you can easily follow the old course afterwards.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15691302&quot; readability=&quot;0.125&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689953&quot; readability=&quot;4.9057971014493&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.4528985507246&quot;&gt;&lt;tr readability=&quot;4.9057971014493&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;8.1763285024155&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;15.89219330855&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Personally I recommend Stanford CSI 231n &lt;a href=&quot;http://cs231n.stanford.edu/&quot; rel=&quot;nofollow&quot;&gt;http://cs231n.stanford.edu/&lt;/a&gt;&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;Its specifically geared towards visual recognition, but it starts with the basics of machine learning and moves on to feed forward nets and covnets and covers RNNs and attention towards the end.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;The assignments are a great set of jupyter notebooks that really get your hands on the material and you can find a number of peoples complete assignments on github just by searching.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;The lectures are available online as well &lt;a href=&quot;https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&quot; rel=&quot;nofollow&quot;&gt;https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-z...&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;I've done hinton's and Ngs courses and as someone who already has a non-ai development background I found this to be the best introduction. Its really an extension of Andrej Karpathy's Neural Nets for Hackers (&lt;a href=&quot;http://karpathy.github.io/neuralnets/&quot; rel=&quot;nofollow&quot;&gt;http://karpathy.github.io/neuralnets/&lt;/a&gt;) &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689681&quot; readability=&quot;10.395&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;5.1975&quot;&gt;&lt;tr readability=&quot;10.395&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;11.8125&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;23.004233700254&quot;&gt;&lt;span class=&quot;c00&quot;&gt;It somewhat depends on if you are looking to build AI to address business problems or if you are more interested in the type of AI work you see companies like Google discussing.&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;I can speak to what &quot;AI&quot; means for most businesses outside Top Tech which more frequently work with tabular, relational, or log data rather than image and text. For these companies, this is what you need to learn how to do&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;
&lt;span class=&quot;c00&quot;&gt;&lt;code&gt;   1. Define a prediction problem and extract labels

   2. Organize and clean the data for prediction

   3. Perform feature engineering by applying domain expertise

   4. Apply an off-the-shelf open source machine learning algorithm like a random forest
&lt;/code&gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;span class=&quot;c00&quot;&gt;Assuming you have access to data and programming skills to clean your data, defining prediction problems and performing feature engineering are the most important skills you have to pick up. For machine learning you can you use open source libraries like scikit-learn or tensorflow.&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;At my company, we've noticed a lot of programmers are intimated by the feature engineering step in particular, so we tried to make it easier by creating an open source library called Featuretools [0].&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;[0] &lt;a href=&quot;https://github.com/featuretools/featuretools&quot; rel=&quot;nofollow&quot;&gt;https://github.com/featuretools/featuretools&lt;/a&gt; &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689840&quot; readability=&quot;3.2380952380952&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;1.6190476190476&quot;&gt;&lt;tr readability=&quot;3.2380952380952&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Thanks for the featuretools project, it sounds really useful. Is it Python 2 only?&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689887&quot; readability=&quot;2.6296296296296&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15691311&quot; readability=&quot;7.651632970451&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;3.8258164852255&quot;&gt;&lt;tr readability=&quot;7.651632970451&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;9.0863141524106&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;17.854368932039&quot;&gt;&lt;span class=&quot;c00&quot;&gt;I think being effective in ML requires both theory, and practical knowledge you only get by doing.&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;Andrew Ng's ML course quickly provides a base in theory.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;Ideally you couple that with some empirical work.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;For that, I think sklearn is the best starting point (assuming you go down the python path). Modify some sample code and make a few simple models. Sklearn provides an excellent framework across all kinds of models (including deep learning if you use say keras.wrappers.scikit_learn), and can play well with pandas.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;There are lots of practical concerns that come up that are not covered in intro ML courses. &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15691213&quot; readability=&quot;7.3369565217391&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690183&quot; readability=&quot;9.0088607594937&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;4.5044303797468&quot;&gt;&lt;tr readability=&quot;9.0088607594937&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;8.5993670886076&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;16.775032509753&quot;&gt;&lt;span class=&quot;c00&quot;&gt;I'm involved with a startup that's specifically tackling this very problem -- how do you learn the theory &amp;amp; application of machine learning quickly (especially if you already know programming well). We teach using diagrams and interactive coding exercises in the browser: www.dataquest.io&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;If you already know Python, you could dive straight into machine learning (&lt;a href=&quot;https://www.dataquest.io/course/machine-learning-fundamentals&quot; rel=&quot;nofollow&quot;&gt;https://www.dataquest.io/course/machine-learning-fundamental...&lt;/a&gt;) and work your way upto calc / lin al, linear regression, decision trees, neural nets, etc.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;If you want to get a taste without signing up, you can check out our blog posts that preview the course (like this one: &lt;a href=&quot;https://www.dataquest.io/blog/machine-learning-tutorial/&quot; rel=&quot;nofollow&quot;&gt;https://www.dataquest.io/blog/machine-learning-tutorial/&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;Happy to answer any questions over DM or email (srini@ourdomain). &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690299&quot; readability=&quot;14.30866807611&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;7.154334038055&quot;&gt;&lt;tr readability=&quot;14.30866807611&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;11.178646934461&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;22.304635761589&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Great resources in the replies. If you want an environment to run code in w/o much setup, try our free service:&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;&lt;a href=&quot;https://notebooks.azure.com&quot; rel=&quot;nofollow&quot;&gt;https://notebooks.azure.com&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;it has Py2, Py3, R, F#, anaconda, TF, CNTK, etc. pre-installed.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;There are some ML tutorials on it already + you can use the &quot;load from github&quot; feature to load, run, edit, ... many of the great tutorials already on github.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;Other similar environments include colab by google and cocalc.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;#Disclaimer: Microsoft &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690385&quot; readability=&quot;5.6526315789474&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.8263157894737&quot;&gt;&lt;tr readability=&quot;5.6526315789474&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;In my opinion the best way to get started is first study statistical inference and modelling, in particular linear regression and the method of maximum likelihood. This will give you a critical eye later on for discerning when it's a good idea to actually use ML and when it's not (an important skill that apparently is in very short supply these days ;).&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690161&quot; readability=&quot;9.3300248138958&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;4.6650124069479&quot;&gt;&lt;tr readability=&quot;9.3300248138958&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;9.7965260545906&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;19.736147757256&quot;&gt;&lt;span class=&quot;c00&quot;&gt;AI != ML&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;For AI, I would take the Udacity AI courses.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;For ML, I would take the Udacity ML courses.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;I take a lot of different online courses, I have no affiliation with Udacity, but their courses are just too good.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;I studied AI (focused on ML) in a decent grad school (and I like to think I had the best teachers there), and I think the quality of the courses is comparable. &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690478&quot; readability=&quot;6.4661016949153&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;3.2330508474576&quot;&gt;&lt;tr readability=&quot;6.4661016949153&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;6.4661016949153&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;12.80303030303&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Isn't AI just applied ML?&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;Or is it an operant/classical conditioning sort of thing, where AI is specifically about training programs to act rather than to perceive/categorize things?&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;I suppose you can have AI that incorporates no ML (like most video game AI), but I'd imagine that will become vanishingly rare in the future. &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15691198&quot; readability=&quot;22.495941558442&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;11.247970779221&quot;&gt;&lt;tr readability=&quot;22.495941558442&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;80&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;15.160308441558&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;29.87582781457&quot;&gt;&lt;span class=&quot;c00&quot;&gt;In brief, AI &lt;em&gt;uses&lt;/em&gt; existing knowledge and/or heuristics (to solve problems that lack a closed-form solution), while ML &lt;em&gt;acquires&lt;/em&gt; knowledge and heuristics toward the same end, with the added goal of improving performance as it learns and adapting to changing conditions.&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;Traditionally, AI has been divided into distinct subfields (e.g. search, planning, natural language and speech processing, game playing, computer vision, robotics, knowledge representation, expert systems, logic, and ML). Today, ML is employed in all AI subfields, but until recently, most subject matter in each AI subfield had been unrelated to ML. In the past decade especially, that's changed as deep learning and probabilistic methods have gained mindshare and now are largely unavoidable when tackling AI-related problems.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;In general, AI's subfields have focused on identifying fundamental obstacles and important features in their own problem domain and developing appropriate techniques that operate on those features when solving problems (like using object recognition and localization to solve vision problems like autonomous driving). I suspect AI's past emphasis on feature engineering has faded as NN-based ML has risen. &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690521&quot; readability=&quot;4.2090395480226&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.1045197740113&quot;&gt;&lt;tr readability=&quot;4.2090395480226&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Eh, I think most people use them somewhat synonymously nowadays. ML is a subset of AI, which has become buzzwordy enough to lost most meaning IMO.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690165&quot; readability=&quot;3.6336336336336&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690912&quot; readability=&quot;5.2987012987013&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690434&quot; readability=&quot;13.627906976744&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;6.8139534883721&quot;&gt;&lt;tr readability=&quot;13.627906976744&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;10.902325581395&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;21.272727272727&quot;&gt;&lt;span class=&quot;c00&quot;&gt;It's really important not to skip the math. As a friend once said to me, doing deep learning without understanding the math is like gambling. It's fine to initially take a more practical, project-based approach for the sake of staying motivated, and you'll retain things better if you have project goals in mind, but, the math is that important.&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;The good news is that compared to other technical fields, the math is also relatively shallow. Here are some good resources that you don't need more than calculus/linalg for (I've used all of them and they got me off the ground):&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;&lt;a href=&quot;http://cs231n.stanford.edu/&quot; rel=&quot;nofollow&quot;&gt;http://cs231n.stanford.edu/&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;&lt;a href=&quot;http://neuralnetworksanddeeplearning.com/&quot; rel=&quot;nofollow&quot;&gt;http://neuralnetworksanddeeplearning.com/&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;&lt;a href=&quot;http://course.fast.ai/&quot; rel=&quot;nofollow&quot;&gt;http://course.fast.ai/&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;Once you feel confident, the &lt;em&gt;Deep Learning&lt;/em&gt; book is more math-heavy, but it is really very good. The authors are more or less deep learning gods. It'll teach you a tremendous amount about how/why neural nets work and the principles used to discover new architectures, and gain a strong intuition for how to use neural nets as a tool. Read it slowly---unless you're already good at math, it takes a while to get through. Don't skip the first five chapters. Use Google and Wikipedia to pick up concepts you don't understand along the way instead of skipping over them (it will bite you later). &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690487&quot; readability=&quot;3.59375&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689776&quot; readability=&quot;3.1775700934579&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690109&quot; readability=&quot;1.8285714285714&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689870&quot; readability=&quot;3.3846153846154&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;1.6923076923077&quot;&gt;&lt;tr readability=&quot;3.3846153846154&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;I'd second this. I'm not a &quot;math person&quot; so to speak, and they provide a great education that doesn't dip too deep into the math.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690393&quot; readability=&quot;6.4489795918367&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;3.2244897959184&quot;&gt;&lt;tr readability=&quot;6.4489795918367&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;4.4336734693878&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;8.5558583106267&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Possibly not what you're looking for (certainly not the cheapest option), but we (Lambda School - YC S17) just announced a live, remote class that trains engineers in AI &amp;amp; ML during weekday evenings for six months.&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;The next one starts in January, and is taught by an MIT grad that taught a similar course at MIT.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;&lt;a href=&quot;https://lambdaschool.com/artificial-intelligence&quot; rel=&quot;nofollow&quot;&gt;https://lambdaschool.com/artificial-intelligence&lt;/a&gt; &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690599&quot; readability=&quot;2.275&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690683&quot; readability=&quot;6.1923076923077&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;3.0961538461538&quot;&gt;&lt;tr readability=&quot;6.1923076923077&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;80&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;3.5384615384615&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;6.8333333333333&quot;&gt;&lt;span class=&quot;c00&quot;&gt;It's still/always 17%, but it would cap out at $20,000 if above ~117k, so it would effectively be less than 17% on an annualized basis.&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;Or paying up-front/in monthly payments is $1041/month for 12 months. &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689592&quot; readability=&quot;2.4086021505376&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689984&quot; readability=&quot;2.4709897610922&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690098&quot; readability=&quot;3.3394833948339&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689563&quot; readability=&quot;3.6299559471366&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;1.8149779735683&quot;&gt;&lt;tr readability=&quot;3.6299559471366&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Just hijacking this question for my benefit as well. I am a product manager in enterprise focused software. I want to transition to the world of AI. Is Udacity's $600 Deep Learning Nano degree worth it ?&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689755&quot; readability=&quot;8.4529411764706&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;4.2264705882353&quot;&gt;&lt;tr readability=&quot;8.4529411764706&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;I don't know about Deep Learning Nanodegree, but I took Machine Learning and AI Nanodegrees at Udacity and they are definitely worth it. I would not recommend them to total beginners in the field. You need to have at least some experience with data science and Python to be able to follow along. Do some free courses on Udacity, Coursera, EdX and other platforms, try to implement these algorithms with your own data and problems and then take the Nanodegree to fill the gaps.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15691090&quot; readability=&quot;1.037037037037&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689485&quot; readability=&quot;3.4736842105263&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;1.7368421052632&quot;&gt;&lt;tr readability=&quot;3.4736842105263&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;As a follow up: I want to pursue a math degree study. What course titles and textbooks starting at the calculus level do you guys recommend? I want enough math chomps to then go onto a PhD in ML.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689825&quot; readability=&quot;8.0230263157895&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;4.0115131578947&quot;&gt;&lt;tr readability=&quot;8.0230263157895&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;4.4572368421053&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;8.3505154639175&quot;&gt;&lt;span class=&quot;c00&quot;&gt;For self-study, you don't necessarily need to follow the usual progression of math classes that start at calculus. It's more important to get comfortable with linear algebra than calculus, especially the way a lot of intro calculus courses focus on calculating integrals and derivatives. Maybe it's not the best message, but the worst grades in my math degree were in the intro calculus classes.&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;I don't remember what intro linear algebra books I used, but my college uses this: &lt;a href=&quot;https://www.math.ucdavis.edu/~linear/&quot; rel=&quot;nofollow&quot;&gt;https://www.math.ucdavis.edu/~linear/&lt;/a&gt; (I took the class before this free textbook was developed). &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689652&quot; readability=&quot;8.6186440677966&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;4.3093220338983&quot;&gt;&lt;tr readability=&quot;8.6186440677966&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;8.6186440677966&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;16.813186813187&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Calculus: Thomas, Weir &amp;amp; Hass - Thomas' Calculus&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;Linear Algebra (which is what you &lt;em&gt;really&lt;/em&gt; need): Gilbert Strang - Linear Algebra and its applications&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;These two are all you need, with which you'll get a solid base. Then you're good to go on your own. These two combined are about 4 semesters worth of work. But if you really focus, I think you can get them done in a little less than 6 months.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;If you want a 'just what I need' approach, Khan Academy. &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689884&quot; readability=&quot;1.5659340659341&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689791&quot; readability=&quot;3.232&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;1.616&quot;&gt;&lt;tr readability=&quot;3.232&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Spivak calculus. There is nothing like it. You will learn how to think in math, not just calculus.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690709&quot; readability=&quot;1.5586592178771&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690269&quot; readability=&quot;4.2307692307692&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.1153846153846&quot;&gt;&lt;tr readability=&quot;4.2307692307692&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;5.5&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;10.474137931034&quot;&gt;&lt;span class=&quot;c00&quot;&gt;I was at the same point as you until I discovered the new Andrew Ng course on deep learning [1]&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;It's a good structured way to learn the core of ML while learning about Neural Networks and without having to become and linear algebra expert which for most people including like me was a deal breaker with other courses. The timing is great too as ML now is so much different than it was 2-3 years ago.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;[1] &lt;a href=&quot;https://www.coursera.org/specializations/deep-learning&quot; rel=&quot;nofollow&quot;&gt;https://www.coursera.org/specializations/deep-learning&lt;/a&gt; &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690346&quot; readability=&quot;8.2271468144044&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;4.1135734072022&quot;&gt;&lt;tr readability=&quot;8.2271468144044&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;I know you say you'd like to learn online, but I highly recommend picking up Duda and Hart's &lt;em&gt;Pattern Classification&lt;/em&gt; to have a theoretical complement to the &quot;hands on&quot;, programming type introductions. It's a very accessible intro to the topic, but also covers a lot of material in depth -- in particular, the topics you mention.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689816&quot; readability=&quot;5.3647058823529&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.6823529411765&quot;&gt;&lt;tr readability=&quot;5.3647058823529&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;5.3647058823529&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;10.761904761905&quot;&gt;&lt;span class=&quot;c00&quot;&gt;For deep learning, my two favorite nominees are:&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;1. Hugo Larochelle's Deep Learning course available on YouTube&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;2. Depending on how much math you like, Nando de Freitas's Deep Learning course (also on YouTube) is also superb. &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690027&quot; readability=&quot;4.808362369338&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.404181184669&quot;&gt;&lt;tr readability=&quot;4.808362369338&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;4.4076655052265&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;8.4758364312268&quot;&gt;&lt;span class=&quot;c00&quot;&gt;I took a grad ML course based on this book: &lt;a href=&quot;https://www.amazon.com/dp/B0759M2D9H&quot; rel=&quot;nofollow&quot;&gt;https://www.amazon.com/dp/B0759M2D9H&lt;/a&gt;&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;It teaches you the foundational theory behind ML, and shows how the fancier stuff is built on it. Good to know the foundations, so you can branch outside of predefined ML techniques. &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689495&quot; readability=&quot;6.5283018867925&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;3.2641509433962&quot;&gt;&lt;tr readability=&quot;6.5283018867925&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;If you’re genuinely a novice programmer/lesser background in linear algebra, AI should be the &lt;em&gt;last&lt;/em&gt; thing on your mind. Any attempts at a shortcut will enhance the difficulty in learning AI, and being able to code things besides simple examples. (which is why I am annoyed by many of the ML MOOCs which are targeted toward novice programmers)&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689653&quot; readability=&quot;4.6296296296296&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.3148148148148&quot;&gt;&lt;tr readability=&quot;4.6296296296296&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;I disagree. There is nothing magic or hard about basic ML. You can do real work with only some basic linear algebra and programming skills. Sure you won't be doing novel deep learning on 100 terabyte datasets, but most problems aren't that anyway.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690166&quot; readability=&quot;6.3657718120805&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;3.1828859060403&quot;&gt;&lt;tr readability=&quot;6.3657718120805&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;80&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Yeah, and to be honest while it's always important to be able to understand the math behind what you're doing, you can easily get started with just understanding what these algorithms do and why, and then work to expand your knowledge of the math over time from there.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689567&quot; readability=&quot;4.5941558441558&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.2970779220779&quot;&gt;&lt;tr readability=&quot;4.5941558441558&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Why does this annoy you? Some of the MOOCs are amazing and they certainly don't shortcut. I had many hard-learned lessons and realizations when going through the MOOCs I have taken. They also help in &quot;knowing what you don't know&quot;...which is essential in breaking out of ignorance.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689973&quot; readability=&quot;1.4343434343434&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690252&quot; readability=&quot;9.5928338762215&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;4.7964169381107&quot;&gt;&lt;tr readability=&quot;9.5928338762215&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;80&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;8.6335504885993&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;16.856418918919&quot;&gt;&lt;span class=&quot;c00&quot;&gt;I say it for the &lt;em&gt;opposite&lt;/em&gt; reason of elitism (and I’ve spoken out many time against elitism in ML/AI).&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;As other posts note, there are many resources available for teaching the concepts. But they don’t teach the &lt;em&gt;limits&lt;/em&gt; of AI, and the rise of MOOCs is setting novice programmers up for a shock when they encounter real world data that is not as nice as the Titanic dataset, and requires making smart decisions to handle the data, and handle it in a way that does not invalidate the results.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;Many &lt;em&gt;romanticize&lt;/em&gt; ML/AI as something that can solve any problem, which is a dangerous approach. &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15689595&quot; readability=&quot;5.2789699570815&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.6394849785408&quot;&gt;&lt;tr readability=&quot;5.2789699570815&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;I've learned computer science in college, so I do have _some_ linear algebra background. I'm by no means an expert coder, but maybe 'intermediary' would have been a better description than 'novice'. :-)&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690014&quot; readability=&quot;4.3225806451613&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.1612903225806&quot;&gt;&lt;tr readability=&quot;4.3225806451613&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;40&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot;&gt;&lt;span class=&quot;c00&quot;&gt;ML is just stats with non-linear systems. No need for programming to do stats, and while linear algebra helps, it is not essential.&lt;/span&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690809&quot; readability=&quot;4.9459459459459&quot;&gt;&lt;td&gt;
&lt;table border=&quot;0&quot; readability=&quot;2.472972972973&quot;&gt;&lt;tr readability=&quot;4.9459459459459&quot;&gt;&lt;td class=&quot;ind&quot;&gt;&lt;img src=&quot;https://news.ycombinator.com/s.gif&quot; height=&quot;1&quot; width=&quot;0&quot;/&gt;&lt;/td&gt;
&lt;td valign=&quot;top&quot; class=&quot;votelinks&quot;&gt;
&lt;center&gt;

&lt;/center&gt;
&lt;/td&gt;
&lt;td class=&quot;default&quot; readability=&quot;3.2972972972973&quot;&gt;

&lt;br/&gt;&lt;div class=&quot;comment&quot; readability=&quot;6.72&quot;&gt;&lt;span class=&quot;c00&quot;&gt;Berkeley has an free videos/slides, combined with exams, projects, and homework.&lt;/span&gt;
&lt;p&gt;&lt;span class=&quot;c00&quot;&gt;Link:http: //ai.berkeley.edu/home.html &lt;/span&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;&lt;/td&gt;
&lt;/tr&gt;&lt;tr class=&quot;athing comtr&quot; id=&quot;15690307&quot; readability=&quot;1.1785714285714&quot;&gt;&lt;td&gt;
&lt;/td&gt;
&lt;/tr&gt;</description>
<pubDate>Mon, 13 Nov 2017 19:31:54 +0000</pubDate>
<dc:creator>hackathonguy</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://news.ycombinator.com/item?id=15689399</dc:identifier>
</item>
<item>
<title>Brilliant Jerks in Engineering</title>
<link>http://www.brendangregg.com/blog/2017-11-13/brilliant-jerks.html</link>
<guid isPermaLink="true" >http://www.brendangregg.com/blog/2017-11-13/brilliant-jerks.html</guid>
<description>&lt;div readability=&quot;470.79360453183&quot;&gt;
&lt;div&gt;&lt;a href=&quot;http://www.brendangregg.com/blog/images/2017/brilliantjerks.jpg&quot;&gt;&lt;img src=&quot;http://www.brendangregg.com/blog/images/2017/brilliantjerks.jpg&quot; border=&quot;0&quot; width=&quot;380&quot;/&gt;&lt;br/&gt;&lt;/a&gt;
&lt;center&gt;&lt;span&gt;&lt;em&gt;Notice board at Ericsson, Stockholm (pic by &lt;a href=&quot;https://twitter.com/DeirdreS&quot;&gt;DeirdreS&lt;/a&gt;)&lt;/em&gt;&lt;/span&gt;&lt;/center&gt;
&lt;/div&gt;
&lt;p&gt;Many of us have worked with them: the engineering jerk who is brilliant at what they do, but treats others like trash. Some companies have a policy not to hire them (eg, Netflix's &quot;&lt;a href=&quot;https://www.slideshare.net/reed2001/culture-1798664/36-Brilliant_Jerks_Some_companies_tolerate&quot;&gt;No Brilliant Jerks&lt;/a&gt;&quot;, which was one of the many reasons I joined the company). There's also the &quot;&lt;a href=&quot;http://amzn.to/2zitvVd&quot;&gt;No Asshole Rule&lt;/a&gt;&quot;, popularized by a bestselling book of this title, which provides the following &lt;a href=&quot;http://en.wikipedia.org/wiki/The_No_Asshole_Rule&quot;&gt;test&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;1. After encountering the person, do people feel oppressed, humiliated or otherwise worse about themselves?&lt;br/&gt;2. Does the person target people who are less powerful than him/her?&lt;/p&gt;
&lt;p&gt;Here's a test for you or your company: Would you tolerate a brilliant engineer who is also an asshole? (Or the more company-polite version: would you tolerate a brilliant jerk?)&lt;/p&gt;
&lt;p&gt;This are numerous articles and opinions on the topic, including &lt;a href=&quot;https://retrospective.co/brilliant-jerks-cost-more-than-they-are-worth/&quot;&gt;Brilliant Jerks Cost More Than They Are Worth&lt;/a&gt;, and &lt;a href=&quot;https://hbr.org/2015/12/its-better-to-avoid-a-toxic-employee-than-hire-a-superstar&quot;&gt;It's Better to Avoid a Toxic Employee than Hire a Superstar&lt;/a&gt;. My colleague Justin Becker is also giving a talk at QConSF 2017 on the topic: &lt;a href=&quot;https://qconsf.com/sf2017/presentation/am-i-brilliant-jerk&quot;&gt;Am I a Brilliant Jerk?&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It may help to clarify that &quot;brilliant jerk&quot; can mean different things to different people. To illustrate, I'll describe two types of brilliant jerks: the selfless and the selfish, and their behavior in detail. I'll then describe the damage caused by these jerks, and ways to deal with them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The following are fictional characters.&lt;/strong&gt; These are not two actual engineers, but are collections of related traits to help examine this behavior beyond the simple &quot;no asshole rule.&quot; These are engineers who by default act like jerks, not engineers who sometimes act that way.&lt;/p&gt;
&lt;h2&gt;Fictional Alice, the selfless brilliant jerk&lt;/h2&gt;
&lt;p&gt;Alice is a brilliant engineer. Alice cares about the company.&lt;/p&gt;
&lt;p&gt;She is direct and honest. If she believes that an unpopular position is right for the company, Alice does not hesitate to voice it. She will even browbeat others to make her point, often coming across as mean-spirited. Alice would point out that she wasn't being mean, she was just stating what is right, and that being mean shouldn't hurt the company anyway. She has little empathy for the feelings of others, and sees little business value in having it.&lt;/p&gt;
&lt;p&gt;Alice is great at working individually on hard engineering problems. She gets along fine with her immediate team and manager, who understand her personality. She doesn't get on well with others whom she only meets occasionally. Outside of her team, Alice is known as a jerk, and people try to avoid working with her.&lt;/p&gt;
&lt;p&gt;Alice is great at fixing hard bugs, writing test suites, doing code merges, and other unglamorous work. If the company needs it done, she's happy to do it, and doesn't care much whether it furthers her own career.&lt;/p&gt;
&lt;p&gt;While selfless jerks can be a net positive for the company, they can become more effective if they learn that being kind results in greater productivity. This topic was covered in the &lt;a href=&quot;http://boz.com/articles/be-kind.html&quot;&gt;Be Kind&lt;/a&gt; post by boz. Different companies may have different attitudes towards Alice: whether to tolerate her behavior or not (most reviewers of this post said &quot;no,&quot; one said &quot;it's a grey area&quot;). Startups may tolerate Alice, for example, since the company is so small that everyone knows Alice and understands her personality.&lt;/p&gt;
&lt;p&gt;But I've described selfless jerks primarily for contrast with &lt;em&gt;selfish&lt;/em&gt; jerks.&lt;/p&gt;
&lt;h2&gt;Fictional Bob, the selfish brilliant jerk&lt;/h2&gt;
&lt;p&gt;Bob is a brilliant engineer. Bob cares about Bob.&lt;/p&gt;
&lt;p&gt;He is selfish, lacks empathy, and has delusions of grandeur. He believes that any behavior is justified that benefits himself, including abusing and exploiting others, for which he shows no guilt or remorse. He can be charming and charismatic to get his way, causing people to ignore or excuse his bad behavior.&lt;/p&gt;
&lt;p&gt;Below is a list of attributes that describe Bob, an extreme example of a brilliant engineering jerk. Not every brilliant jerk exhibits all of these behaviors, but I've seen each and every one of them firsthand.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bob interrupts others, and ignores their opinions&lt;/strong&gt;. He believes that he is the most important person in the room, and has no interest in what others have to say, frequently interrupting them. He can monopolize conversations with long, exaggerated stories that flatter himself. Less-assertive engineers are effectively silenced, even if their opinions on the topic are the most valuable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bob only does work that benefits himself&lt;/strong&gt;. He can work well on hard engineering problems, but only works on those he enjoys, or that help his career or promote his own earlier work. He creates new projects and immediately claims credit, but leaves the dirty work of finishing them to others, and avoids responsibility if they fail. He is brilliant at convincing the company to let him do what he wants, even when that ignores market demand or his own past performance. He never seriously mentors or trains other staff – he does not see that as useful to his own career.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bob bullies, humiliates, and oppresses individuals&lt;/strong&gt;. With non-technical people, he wins arguments by bamboozling them with irrelevant technical detail, making them feel dumb. With junior technical people, Bob likes to ridicule their ideas, letting everyone know how stupid they are, and how much smarter he is. When his technical specialties are needed, he makes people beg and grovel for his help, as another way to humiliate them. When others make mistakes, he enjoys shaming them using biting sarcasm and witty insults. He uses similar rhetoric in arguments, where he must always win, no matter the cost.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bob engages in displays of dominance in front of groups&lt;/strong&gt;. Bob likes to show everyone how important he is by how much he can get away with, including sheer rudeness. He is late to meetings, puts his feet up on the table, then looks at his phone or laptop while ignoring everyone around him. He sometimes makes obscene remarks in the office, bragging: &quot;If anyone else said that, they'd be fired!&quot;. He also insists on having a better laptop/desktop/monitors/etc than anyone else, to display his status.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bob tries to assert authority over all areas of the company&lt;/strong&gt;, including those where he has no expertise at all. Areas he cannot control, he denigrates: eg, as an engineer, he will claim that marketing is stupid, useless, unnecessary, and that &quot;anyone could do it.&quot;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bob is negative&lt;/strong&gt;. He trash-talks other technologies, companies, and people behind their backs, always finding something negative to say. He elevates his own status by slamming other people. He also attacks technologies that either don't leverage his own prior work, or don't conform to his own beliefs. Other engineers avoid new technologies, for fear of damaging ridicule from Bob.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bob manipulates and misleads&lt;/strong&gt;. Sometimes he misleads subtly, by presenting facts that are literally true in a way that is intentionally misleading. At other times he will simply lie, and do it with such confidence and assertiveness that he is almost always believed. He states his own preferences and opinions as facts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bob uses physical intimidation&lt;/strong&gt;. Bob glares at those he doesn't like, and may invade people's personal space. He may also use violent gestures such as slamming fists on desks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A string of good employees have quit because of Bob&lt;/strong&gt;. Some engineers become fed up with Bob and quit. Talented engineers are driven out by Bob on purpose, to eliminate threats to his own status. Bob demonizes those who left, attributing past failures to them, and their successes to others who stayed, especially himself. In this way, he convinces management that losing those staff was good for the company, and stops them from realizing that the real problem is Bob. Some who have left, if asked, will cite other reasons for quitting, hoping to avoid becoming victims of Bob's smear campaigns.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bob gives great talks – about himself&lt;/strong&gt;. Because he is a brilliant engineer and a great public orator, he is a popular speaker at technical events. In talks, he narrates self-enhancing stories and rewrites history to flatter himself, taking credit for other people's work – if not blatantly, then tacitly or by implication. He has a group of spellbound followers outside of the company who hero-worship and idolize him, and would love to work with him. He is well liked &lt;em&gt;from afar&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bob exploits junior engineers&lt;/strong&gt;: Bob finds junior engineers who admire his brilliance, and encourages them to do work that elevates Bob's ideas and projects, reflecting glory back onto Bob. They become so invested in helping Bob's career growth that they have none of their own.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bob is a negative role model&lt;/strong&gt;. Bob can drag down the workplace or community by becoming a negative role model and having others imitate his behavior. Those who admire Bob become negative, bully others, and engage in similar personal attacks, hoping for his approval and to become Bob themselves. Others simply use Bob to excuse their own pre-existing bad behavior: if Bob can do it, so can I.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Some of Bob's coworkers become accomplices, and gaslight his abuse&lt;/strong&gt;. They were there when he attacked and humiliated others, and they did nothing – or laughed along, encouraging Bob to continue. Bob likes to surround himself with such enablers. They may be otherwise reasonable people who have yet to understand that what they are witnessing is abuse. They may publicly defend Bob and deny that abuse happened, or minimize it, gaslighting Bob's victims (&quot;everyone's a jerk sometimes&quot;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bob refuses to change&lt;/strong&gt;. Bob knows that his behavior hurts people, but &quot;that's their problem.&quot;&lt;/p&gt;
&lt;p&gt;It bears repeating: I have seen each of these behaviors firsthand, from multiple brilliant jerks. Bob is an extreme fictional case who exhibits &lt;em&gt;all&lt;/em&gt; of these behaviors. One particular person may exhibit only some, without necessarily being a jerk. But, if you recognize many of these traits in a colleague – or in yourself – then, yes, you're probably dealing with a major-league jerk.&lt;/p&gt;
&lt;p&gt;Some reviewers of this post have said that Bob seems unbelievable: no one could be anywhere near that bad! They are fortunate not to have experienced a Bob, which is the lesson here: your understanding of a &quot;brilliant jerk&quot; may differ from that of someone who has actually worked with a Bob, or another severe type of jerk.&lt;/p&gt;
&lt;h2&gt;The problems caused by brilliant jerks&lt;/h2&gt;
&lt;p&gt;Problems caused by Alice, the selfless jerk, may include:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Alice hurts or offends some employees with her attitude.&lt;/li&gt;
&lt;li&gt;Alice causes her team and manager to spend energy mending fences with others.&lt;/li&gt;
&lt;li&gt;Alice's projects may be less successful, as others avoid working with her.&lt;/li&gt;
&lt;li&gt;Alice discourages others from asking her questions, so her technical expertise is often wasted.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;But Bob, the selfish jerk, can cause these additional problems:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Bob silences many technical opinions, lowering the company's technical IQ.&lt;/li&gt;
&lt;li&gt;Bob creates extra work for others who must fix his abandoned projects.&lt;/li&gt;
&lt;li&gt;Bob demoralizes many staff, which hurts productivity.&lt;/li&gt;
&lt;li&gt;Bob causes stress-related psychological and physical illness in his victims.&lt;/li&gt;
&lt;li&gt;Bob causes some staff to occasionally skip work: increasing absenteeism.&lt;/li&gt;
&lt;li&gt;Bob drives staff to quit, who will never come back.&lt;/li&gt;
&lt;li&gt;Bob may strengthen the company's competition, who hire those who quit.&lt;/li&gt;
&lt;li&gt;Bob makes it difficult to hire other good staff (word gets around).&lt;/li&gt;
&lt;li&gt;Bob discourages customers and investors (word gets around).&lt;/li&gt;
&lt;li&gt;Other staff devise processes to work around Bob, reducing the company's efficiency.&lt;/li&gt;
&lt;li&gt;Other staff may sabotage Bob's work, which sabotages the company.&lt;/li&gt;
&lt;li&gt;Bob inspires other staff to imitate his behavior, multiplying the problem.&lt;/li&gt;
&lt;li&gt;Bob creates a hostile workplace environment: an invitation to lawsuits.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Chapter 2 in &quot;The No Asshole Rule&quot; covers more details (although for general staff, not just engineers), and has instructions for calculating your TCA: the Total Cost of Assholes for your organization.&lt;/p&gt;
&lt;h2&gt;Dealing with brilliant jerks&lt;/h2&gt;
&lt;p&gt;There are two parts to this: helping the victims and staff who witness the behavior, and dealing with the jerks themselves. Both are big topics that I'll discuss here only briefly.&lt;/p&gt;
&lt;p&gt;At some companies, no one is telling Alice or Bob that their behavior is inappropriate. Everyone sees the bad behavior, but thinks it must be tolerated because Alice and Bob are so brilliant and valuable. Wrong! One important step a company can take is to explicitly adopt a &quot;no brilliant jerks&quot; policy. Netflix has such a policy as part of the culture slide deck, now a &lt;a href=&quot;https://jobs.netflix.com/culture&quot;&gt;memo&lt;/a&gt;, which reads:&lt;/p&gt;
&lt;blockquote readability=&quot;12&quot;&gt;
&lt;p&gt;On a dream team, there are no “brilliant jerks.” The cost to teamwork is just too high. Our view is that brilliant people are also capable of decent human interactions, and we insist upon that. When highly capable people work together in a collaborative context, they inspire each other to be more creative, more productive and ultimately more successful as a team than they could be as a collection of individuals.&lt;br/&gt;– Netflix culture memo&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This policy isn't some useless feel-good text from a nameless source: it was originally published on CEO Reed Hastings' &lt;a href=&quot;https://www.slideshare.net/reed2001/culture-1798664/36-Brilliant_Jerks_Some_companies_tolerate&quot;&gt;slideshare account&lt;/a&gt;. To be effective, such a policy for your company may also need to come from your CEO. All Netflix candidates are told to read the culture deck (memo) when interviewing, and are told that, yes, we take it seriously. While this helps people realize that jerks should not be tolerated, it doesn't necessarily stop jerks from being hired in the first place: Bob is brilliant and charismatic and would probably pass the interview. However, he would then find himself at a company where his colleagues recognize his bad behavior as unacceptable, and are empowered to speak up about it.&lt;/p&gt;
&lt;p&gt;In over three years at Netflix, I've worked with zero brilliant jerks. The &quot;no brilliant jerks&quot; policy works, and it's been great. If we hired any in that time, they either changed their ways or left the company before I could interact with them.&lt;/p&gt;
&lt;p&gt;Some brilliant jerks can mend their ways: Alice might be motivated to change if she can be made to understand that her behavior is hurting the company, which she cares about. She should be encouraged to exercise empathy, and to leave others feeling positive and motivated to work harder, rather than demotivated. My colleague, Justin Becker, explores this in detail in his QCon talk, including the topic of emotional intelligence (EQ). In the next section I'll share an example.&lt;/p&gt;
&lt;p&gt;As for Bob: he should be told that his behavior hurts people and the company, and given the opportunity to change – but the reality is that he probably doesn't care. He firmly believes that &quot;nice guys finish last,&quot; and, so far, being a jerk has worked well for him. His managers have the power to change that equation, because they control things that Bob wants: they allow him to work on his pet projects and to speak at events, they give him promotions and bonuses, and ultimately they let him keep his job.&lt;/p&gt;
&lt;blockquote readability=&quot;6.5242718446602&quot;&gt;
&lt;p&gt;&quot;I'd rather have a &lt;em&gt;hole&lt;/em&gt; in my organization than an &lt;em&gt;asshole&lt;/em&gt;.&quot;&lt;br/&gt;– Fred Wilson, Velocity NY 2013 &lt;a href=&quot;https://www.youtube.com/watch?v=fJOSX-W0yHA&amp;amp;feature=youtu.be&amp;amp;t=10m53s&quot;&gt;keynote&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a colleague/victim/witness, you should report Bob's behavior to management, but you probably shouldn't ask them outright to fire Bob (among other reasons, what if someday you were thought to be a Bob?). Give management information, but let them decide how to act. Actually firing a brilliant jerk is a complicated topic for a separate post, ideally written by a manager who has dealt with this. There's usually a process to follow, which unfortunately Bob may exploit to his advantage, showing improvement when needed to keep his job, but then reverting back to his bad old ways. He may also have convinced management that his technical skills and fame are so important that the company would fail without him. This isn't true, but fear may cause management to hesitate.&lt;/p&gt;
&lt;p&gt;For management to deal effectively with Bob, they must themselves be convinced that his behavior should not be tolerated, regardless of his brilliance. For some companies, that will require truly understanding the damage that Bob causes (listed above), to justify taking action. For companies like Netflix with an explicit &quot;no brilliant jerks&quot; policy, it's much easier for management to take action, as they don't need to convince anyone that jerks are a problem: that's already covered in company policy.&lt;/p&gt;
&lt;p&gt;Regular one-on-one meetings with staff, and scheduled skip-level meetings, should also help inform management about the damage jerks are causing. Netflix does this well: I have scheduled one-on-one meetings with my manager once every two weeks, their manager once a month, and their manager at least once a year. That's three levels of management I talk directly and in private with, without even having to ask for a meeting. We're also encouraged to give other employees direct and honest feedback, intervene if we see harassment, and escalate up to and including the CEO.&lt;/p&gt;
&lt;p&gt;As for public speaking: Bob draws power from being a public face of the company. Speaking events should be shared among staff who want to speak, and training can be made available to improve their skills (various companies offer this), so that Bob isn't the only good speaker. Conference organizers can also adopt a &quot;no brilliant jerks&quot; policy (some already do), and attendees can avoid conferences that host known jerks. If Alice needs to learn empathy, Bob needs to learn both empathy and to stop being selfish, and sharing public speaking or other rewarding projects is an example of the latter.&lt;/p&gt;
&lt;h2&gt;When I acted like a jerk&lt;/h2&gt;
&lt;p&gt;Many people sometimes act like Alice, and it can be easy to talk them out of it (Alice herself is harder, since it's her by-default behavior). I'll explain this with a story, this time of a moment when I acted like a jerk.&lt;/p&gt;
&lt;p&gt;Early in my career, an engineer at my company made a big mistake in my area of expertise, and sent an email that dodged responsibility and showed no path to fix it. I was furious and phoned the engineer: my intent was to make him realize that he'd made a big mistake, and put him on the right path. I was blunt, and told him off. I didn't enjoying doing so, but I felt I was doing a Good Thing for the company, and fixing a problem.&lt;/p&gt;
&lt;p&gt;A week later, his manager phoned me unexpectedly. He told me that he was aware of my phone call, and didn't think I was technically wrong, but did I know that the engineer has been demotivated and unproductive since I talked to him, and was it my intent to make his staff unproductive? No, of course not. The manager continued: do you think you could have told my engineer what you needed to, in a way that left them feeling positive and motivated to fix it? Sure, I probably can. Good. Always do that in the future, please. I did.&lt;/p&gt;
&lt;p&gt;The phone call lasted less than two minutes, and was immediately effective. I suspect the manager had done this before. Notice that he did not accuse me of being a jerk, rather, he posed two questions, which were basically: 1) are you intending to hurt the company?, and 2) are you able to act decently? There's only really one right answer to those questions. If he had just said &quot;you are a jerk&quot; I may have just replied &quot;no, I'm not&quot;, but by asking questions instead, it put the onus on me to think about the answer, and triggered a moment of self reflection.&lt;/p&gt;
&lt;h2&gt;Additional topics&lt;/h2&gt;
&lt;p&gt;There are some additional topics I have not covered in detail here, but should mention:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;What about jerks in open source communities?&lt;/strong&gt; A good reference for this is the &lt;a href=&quot;https://hypatia.ca/2016/06/21/no-more-rock-stars/&quot;&gt;no more rock stars&lt;/a&gt; post, where the rock star described is pretty much Bob. See that post for the section on: How do we as a community prevent rock stars?.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;What about Bob as a manager?&lt;/strong&gt; Bob may seek and be offered promotions into management, and become even more damaging to the company. Bob the manager exploits and threatens his subordinates. That's a topic for another post.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Does Bob sexually assault others?&lt;/strong&gt; Is Bob more likely to be a harasser due to delusions of grandeur and a sense of entitlement (&lt;a href=&quot;https://hypatia.ca/2017/07/18/the-al-capone-theory-of-sexual-harassment/&quot;&gt;Al Capone theory&lt;/a&gt;), or is he smart enough not to go that far, or, is he simply not that kind of jerk? I don't know. That's outside of my firsthand experience, so I didn't include it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;What about those junior engineers?&lt;/strong&gt; The ones Bob exploits for his own gain. That's another big topic. Some related reading &lt;a href=&quot;https://medium.com/@marlenac/the-blood-bag-co-narcissists-and-narcissists-in-tech-b6c78ae2aa5e&quot;&gt;here&lt;/a&gt;, &lt;a href=&quot;https://medium.com/@marlenac/the-blood-bag-cutting-the-iv-line-e05496871f8a&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;https://medium.com/@marlenac/the-blood-bag-patterns-of-blood-bags-and-narcissists-in-tech-c41854605103&quot;&gt;here&lt;/a&gt; (from which I borrowed the words &quot;reflecting glory&quot;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Is Bob actually brilliant?&lt;/strong&gt; Bob the brilliant jerk creates enemies in the industry, and some staff even sabotage Bob's work. In the long run, it hurts Bob's career. Not a brilliant result, really.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;What if Bob is pretending to be brilliant?&lt;/strong&gt; (updated) The consequences for the company can be worse. I didn't explore this topic here, but it would be a character similar to Bob who isn't actually brilliant, but pretends to be, and has most people believing him.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Does anyone exist who is really as bad as fictional Bob?&lt;/strong&gt; Yes. Fortunately they are rare. One reviewer thinks that my post will not be effective unless I name such a real-life Bob as a concrete example. Maybe they are right, but I've avoided that here. This isn't about one Bob, it's about all selfish brilliant jerks.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Should we publicly call out brilliant jerks in tech?&lt;/strong&gt; It's a complex topic. The book &lt;a href=&quot;https://www.amazon.com/Shame-Necessary-New-Uses-Tool/dp/0307950131&quot;&gt;Is Shame Necessary&lt;/a&gt; does make the point that shame and humiliation have a legitimate place in society when they are natural consequences to abusive behavior, which is also discussed in &lt;a href=&quot;https://blog.valerieaurora.org/2016/10/24/when-is-naming-abuse-itself-abusive/&quot;&gt;this post&lt;/a&gt;. Calling out abusers may save future victims of abuse, so long as the calling out is proportional and not abusive itself. For victims of abuse, I could not make a blanket recommendation: I don't know your specific situation and how safe it is for you to speak up. I've been in this situation myself, and was facing an extreme threat to myself and my family, and I understand how risky it can be.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;How do I know if I'm the jerk?&lt;/strong&gt; If you always think that being right is all that matters, and don't consider your impact on teamwork or relationships, you might be an Alice or a Bob. If you think that hurting other people is simply doing what it takes to get ahead, then you might be a Bob. See the earlier sections for more characteristics, and my colleague's QCon presentation: &lt;a href=&quot;https://qconsf.com/sf2017/presentation/am-i-brilliant-jerk&quot;&gt;Am I a Brilliant Jerk?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Should brilliant jerks be tolerated? To explore this, I described two fictional brilliant jerks: Alice, who is selfless, and Bob, who is selfish. This makes it clear that the behavior of selfish jerks, like Bob, should definitely not be tolerated. Bob can kill companies. When CEOs and VCs sometimes say that brilliant jerks may be worth it, I imagine they are thinking of Alice, a selfless jerk, and not Bob. (Alice is debatable.)&lt;/p&gt;
&lt;p&gt;Early on in my career, I supported brilliant jerks of any type and thought they were worth it. I was wrong. People had warned me about them, that their behavior was &quot;not ok,&quot; but they never went into much detail as to why. I've shared many details here. I didn't figure this all out until seeing the behavior and damage firsthand. (I've not only experienced it, but I may have reached my lifetime dosage of asshole-rads.)&lt;/p&gt;
&lt;p&gt;Companies can adopt a &quot;no asshole rule&quot;, or more politely, a &quot;no brilliant jerks&quot; policy. Colleagues may be genuinely conflicted about how to deal with Bob: on the one hand, he is a real jerk, but on the other he is a &quot;high performer,&quot; so isn't it in the company's best interest to tolerate his behavior? A policy helps you decide, and it can be as simple as three words: no brilliant jerks.&lt;/p&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;Picture and editing by Deirdré Straughan. Thanks to review feedback and suggestions from Alice Goldfuss, Baron Schwartz, Ed Hunter, Justin Becker, David Blank-Edelman, Valerie Aurora, and others. References and related reading:&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;span&gt;Sutton, R. &lt;em&gt;The No Asshole Rule&lt;/em&gt;. Grand Central Publishing, 2010.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;Babiak, P., Hare, R. D. &lt;em&gt;Snakes in Suits&lt;/em&gt;. Harper Business, 2007.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;Jacquet, J. &lt;em&gt;Is Shame Necessary&lt;/em&gt;. Random House, 2015.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;Sutton, R. &lt;em&gt;The Asshole Survival Guide&lt;/em&gt;. Houghton Mifflin Harcourt, 2017.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;&lt;a href=&quot;https://hbr.org/2015/12/its-better-to-avoid-a-toxic-employee-than-hire-a-superstar&quot;&gt;https://hbr.org/2015/12/its-better-to-avoid-a-toxic-employee-than-hire-a-superstar&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;&lt;a href=&quot;https://retrospective.co/brilliant-jerks-cost-more-than-they-are-worth/&quot;&gt;https://retrospective.co/brilliant-jerks-cost-more-than-they-are-worth/&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;&lt;a href=&quot;http://boz.com/articles/be-kind.html&quot;&gt;http://boz.com/articles/be-kind.html&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;&lt;a href=&quot;https://hypatia.ca/2016/06/21/no-more-rock-stars/&quot;&gt;https://hypatia.ca/2016/06/21/no-more-rock-stars/&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=fJOSX-W0yHA&amp;amp;feature=youtu.be&amp;amp;t=10m53s&quot;&gt;https://www.youtube.com/watch?v=fJOSX-W0yHA&amp;amp;feature=youtu.be&amp;amp;t=10m53s&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;&lt;a href=&quot;https://www.slideshare.net/reed2001/culture-1798664/36-Brilliant_Jerks_Some_companies_tolerate&quot;&gt;https://www.slideshare.net/reed2001/culture-1798664/36-Brilliant_Jerks_Some_companies_tolerate&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;&lt;a href=&quot;https://jobs.netflix.com/culture&quot;&gt;https://jobs.netflix.com/culture&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;&lt;a href=&quot;https://qconsf.com/sf2017/presentation/am-i-brilliant-jerk&quot;&gt;https://qconsf.com/sf2017/presentation/am-i-brilliant-jerk&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;&lt;a href=&quot;https://hypatia.ca/2017/07/18/the-al-capone-theory-of-sexual-harassment/&quot;&gt;https://hypatia.ca/2017/07/18/the-al-capone-theory-of-sexual-harassment/&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span&gt;The blood bag series: &lt;a href=&quot;https://medium.com/@marlenac/the-blood-bag-co-narcissists-and-narcissists-in-tech-b6c78ae2aa5e&quot;&gt;part 1&lt;/a&gt;, &lt;a href=&quot;https://medium.com/@marlenac/the-blood-bag-cutting-the-iv-line-e05496871f8a&quot;&gt;part 2&lt;/a&gt;, &lt;a href=&quot;https://medium.com/@marlenac/the-blood-bag-patterns-of-blood-bags-and-narcissists-in-tech-c41854605103&quot;&gt;part 3&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;&lt;p&gt;&lt;em&gt;You can comment here, but I can't guarantee your comment will remain here forever: I might switch comment systems at some point (eg, if disqus add advertisements).&lt;/em&gt;&lt;/p&gt;</description>
<pubDate>Mon, 13 Nov 2017 17:24:16 +0000</pubDate>
<dc:creator>dmit</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.brendangregg.com/blog/2017-11-13/brilliant-jerks.html</dc:identifier>
</item>
<item>
<title>How Firefox Got Fast Again</title>
<link>https://hacks.mozilla.org/2017/11/entering-the-quantum-era-how-firefox-got-fast-again-and-where-its-going-to-get-faster/</link>
<guid isPermaLink="true" >https://hacks.mozilla.org/2017/11/entering-the-quantum-era-how-firefox-got-fast-again-and-where-its-going-to-get-faster/</guid>
<description>&lt;p&gt;People have noticed that Firefox is fast again.&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/tweet3.png&quot;&gt;&lt;img class=&quot;alignnone size-large wp-image-31583&quot; src=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/tweet3-500x272.png&quot; alt=&quot;Tweet from Sara Soueidan about Firefox Nightly being fast&quot; width=&quot;500&quot; height=&quot;272&quot; srcset=&quot;https://hacks.mozilla.org/files/2017/11/tweet3-500x272.png 500w, https://hacks.mozilla.org/files/2017/11/tweet3-250x136.png 250w, https://hacks.mozilla.org/files/2017/11/tweet3-768x418.png 768w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Over the past seven months, we’ve been rapidly replacing major parts of the engine, introducing Rust and parts of Servo to Firefox. Plus, we’ve had a browser performance strike force scouring the codebase for performance issues, both obvious and non-obvious.&lt;/p&gt;
&lt;p&gt;We call this Project Quantum, and the first general release of the reborn Firefox Quantum comes out tomorrow.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/engine01.png&quot;&gt;&lt;img class=&quot;alignnone size-large wp-image-31559&quot; src=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/engine01-500x333.png&quot; alt=&quot;orthographic drawing of jet engine&quot; width=&quot;500&quot; height=&quot;333&quot; srcset=&quot;https://hacks.mozilla.org/files/2017/11/engine01-500x333.png 500w, https://hacks.mozilla.org/files/2017/11/engine01-250x167.png 250w, https://hacks.mozilla.org/files/2017/11/engine01-768x512.png 768w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;But this doesn’t mean that our work is done. It doesn’t mean that today’s Firefox is as fast and responsive as it’s going to be.&lt;/p&gt;
&lt;p&gt;So, let’s look at how Firefox got fast again and where it’s going to get faster.&lt;/p&gt;
&lt;h3&gt;Laying the foundation with coarse-grained parallelism&lt;/h3&gt;
&lt;p&gt;To get faster, we needed to take advantage of the way hardware has changed over the past 10 years.&lt;/p&gt;
&lt;p&gt;We aren’t the first to do this. Chrome was faster and more responsive than Firefox when it was first introduced. One of the reasons was that the Chrome engineers saw that a change was happening in hardware and they started making better use of that new hardware.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/scope02.png&quot;&gt;&lt;img class=&quot;alignnone size-large wp-image-31561&quot; src=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/scope02-500x172.png&quot; alt=&quot;Chrome looking to the future of coarse-grained parallelism&quot; width=&quot;500&quot; height=&quot;172&quot; srcset=&quot;https://hacks.mozilla.org/files/2017/11/scope02-500x172.png 500w, https://hacks.mozilla.org/files/2017/11/scope02-250x86.png 250w, https://hacks.mozilla.org/files/2017/11/scope02-768x264.png 768w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A new style of CPU was becoming popular. These CPUs had multiple cores which meant that they could do tasks independently of each other, but at the same time—in parallel.&lt;/p&gt;
&lt;p&gt;This can be tricky though. With parallelism, you can introduce subtle bugs that are hard to see and hard to debug. For example, if two cores need to add 1 to the same number in memory, one is likely to overwrite the other if you don’t take special care.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/race_condition_atomic18.png&quot;&gt;&lt;img class=&quot;alignnone size-large wp-image-31562&quot; src=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/race_condition_atomic18-500x1066.png&quot; alt=&quot;diagram showing data race between two cores&quot; width=&quot;500&quot; height=&quot;1066&quot; srcset=&quot;https://hacks.mozilla.org/files/2017/11/race_condition_atomic18-500x1066.png 500w, https://hacks.mozilla.org/files/2017/11/race_condition_atomic18-250x533.png 250w, https://hacks.mozilla.org/files/2017/11/race_condition_atomic18-768x1637.png 768w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A pretty straightforward way to avoid these kinds of bugs is just to make sure that the two things you’re working on don’t have to share memory — to split up your program into pretty large tasks that don’t have to cooperate much. This is what coarse-grained parallelism is.&lt;/p&gt;
&lt;p&gt;In the browser, it’s pretty easy to find these coarse grains. Have each tab as its own separate bit of work. There’s also the stuff around that webpage—the browser chrome—and that can be handled separately.&lt;/p&gt;
&lt;p&gt;This way, the pages can work at their own speed, simultaneously, without blocking each other. If you have a long-running script in a background tab, it doesn’t block work in the foreground tab.&lt;/p&gt;
&lt;p&gt;This is the opportunity that the Chrome engineers foresaw. We saw it too, but we had a bumpier path to get there. Since we had an existing code base we needed to plan for how to split up that code base to take advantage of multiple cores.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/scope04.png&quot;&gt;&lt;img class=&quot;alignnone size-large wp-image-31563&quot; src=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/scope04-500x172.png&quot; alt=&quot;Firefox looking to coarse-parallelism future&quot; width=&quot;500&quot; height=&quot;172&quot; srcset=&quot;https://hacks.mozilla.org/files/2017/11/scope04-500x172.png 500w, https://hacks.mozilla.org/files/2017/11/scope04-250x86.png 250w, https://hacks.mozilla.org/files/2017/11/scope04-768x264.png 768w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It took a while, but we got there. With the Electrolysis project, we finally made multiprocess the default for all users. And Quantum has been making our use of coarse-grained parallelism even better with a few other projects.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/graphs-draft01.png&quot;&gt;&lt;img class=&quot;alignnone size-large wp-image-31578&quot; src=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/graphs-draft01-500x105.png&quot; alt=&quot;timeline for coarse grained parallelism, with Electrolysis and Quantum Compositor before initial Quantum release and Quantum DOM after&quot; width=&quot;500&quot; height=&quot;105&quot; srcset=&quot;https://hacks.mozilla.org/files/2017/11/graphs-draft01-500x105.png 500w, https://hacks.mozilla.org/files/2017/11/graphs-draft01-250x52.png 250w, https://hacks.mozilla.org/files/2017/11/graphs-draft01-768x161.png 768w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Electrolysis&lt;/h4&gt;
&lt;p&gt;Electrolysis laid the groundwork for Project Quantum. It introduced a kind of multi-process architecture similar to the one that Chrome introduced. Because it was such a big change, we introduced it slowly, testing it with small groups of users starting in 2016 before rolling it out to all Firefox users in mid-2017.&lt;/p&gt;
&lt;h4&gt;Quantum Compositor&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/16.png&quot;&gt;&lt;img class=&quot;alignnone size-large wp-image-31565&quot; src=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/16-500x418.png&quot; alt=&quot;GPU process&quot; width=&quot;500&quot; height=&quot;418&quot; srcset=&quot;https://hacks.mozilla.org/files/2017/11/16-500x418.png 500w, https://hacks.mozilla.org/files/2017/11/16-250x209.png 250w, https://hacks.mozilla.org/files/2017/11/16-768x642.png 768w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Quantum Compositor moved the compositor to its own process. The biggest win here was that it made Firefox more stable. Having a separate process means that if the graphics driver crashes, it won’t crash all of Firefox. But having this separate process also makes Firefox more responsive.&lt;/p&gt;
&lt;h4&gt;Quantum DOM&lt;/h4&gt;
&lt;p&gt;Even when you split up the content windows between cores and have a separate main thread for each one, there are still a lot of tasks that main thread needs to do. And some of them are more important than others. For example, responding to a keypress is more important than running garbage collection. Quantum DOM gives us a way to prioritize these tasks. This makes Firefox more responsive. Most of this work has landed, but we still plan to take this further with something called pre-emptive scheduling.&lt;/p&gt;
&lt;h3&gt;Making best use of the hardware with fine-grained parallelism&lt;/h3&gt;
&lt;p&gt;When we looked out to the future, though, we need to go further than coarse-grained parallelism.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/scope05.png&quot;&gt;&lt;img class=&quot;alignnone size-large wp-image-31566&quot; src=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/scope05-500x172.png&quot; alt=&quot;Firefox looking towards the future of fine-grained parallelism&quot; width=&quot;500&quot; height=&quot;172&quot; srcset=&quot;https://hacks.mozilla.org/files/2017/11/scope05-500x172.png 500w, https://hacks.mozilla.org/files/2017/11/scope05-250x86.png 250w, https://hacks.mozilla.org/files/2017/11/scope05-768x264.png 768w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Coarse-grained parallelism makes better use of the hardware… but it doesn’t make the best use of it. When you split up these web pages across different cores, some of them don’t have work to do. So those cores will sit idle. At the same time, a new page being fired up on a new core takes just as long as it would if the CPU were single core.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/core_splitting06.png&quot;&gt;&lt;img class=&quot;alignnone size-large wp-image-31567&quot; src=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/core_splitting06-500x146.png&quot; alt=&quot;Splitting content windows across different cores&quot; width=&quot;500&quot; height=&quot;146&quot; srcset=&quot;https://hacks.mozilla.org/files/2017/11/core_splitting06-500x146.png 500w, https://hacks.mozilla.org/files/2017/11/core_splitting06-250x73.png 250w, https://hacks.mozilla.org/files/2017/11/core_splitting06-768x224.png 768w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It would be great to be able to use all of those cores to process the new page as it’s loading. Then you could get that work done faster.&lt;/p&gt;
&lt;p&gt;But with coarse-grained parallelism, you can’t split off any of the work from one core to the other cores. There are no boundaries between the work.&lt;/p&gt;
&lt;p&gt;With fine-grained parallelism, you break up this larger task into smaller units that can then be sent to different cores. For example, if you have something like the Pinterest website, you can split up the different pinned items and send those to be processed by different cores.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/core_splitting_fine03.png&quot;&gt;&lt;img class=&quot;alignnone size-large wp-image-31568&quot; src=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/core_splitting_fine03-500x278.png&quot; alt=&quot;Splitting work across cores fine-grained&quot; width=&quot;500&quot; height=&quot;278&quot; srcset=&quot;https://hacks.mozilla.org/files/2017/11/core_splitting_fine03-500x278.png 500w, https://hacks.mozilla.org/files/2017/11/core_splitting_fine03-250x139.png 250w, https://hacks.mozilla.org/files/2017/11/core_splitting_fine03-768x426.png 768w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This doesn’t just help with latency like the coarse-grained parallelism did. It also helps with pure speed. The page loads faster because the work is split up across all the cores. And as you add more cores, your page load keeps getting faster the more cores you add.&lt;/p&gt;
&lt;p&gt;So we saw that this was the future, but it wasn’t entirely clear how to get there. Because to make this fine-grained parallelism fast, you usually need to share memory between the cores. But that gives you &lt;a href=&quot;https://hacks.mozilla.org/2017/06/avoiding-race-conditions-in-sharedarraybuffers-with-atomics/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;those data races that I talked about before&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But we knew that the browser had to make this shift, so we started investing in research. We created a language that was free of these data races — Rust. Then we created a browser engine— Servo — that made full use of this fine-grained parallelism. Through that, we proved that this could work and that you could actually have fewer bugs while going faster.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/graphs-draft02.png&quot;&gt;&lt;img class=&quot;alignnone size-large wp-image-31579&quot; src=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/graphs-draft02-500x125.png&quot; alt=&quot;timeline of fine grained parallelism, with Quantum CSS before initial Qunatum release, and Quantum Render and possibly more after&quot; width=&quot;500&quot; height=&quot;125&quot; srcset=&quot;https://hacks.mozilla.org/files/2017/11/graphs-draft02-500x125.png 500w, https://hacks.mozilla.org/files/2017/11/graphs-draft02-250x62.png 250w, https://hacks.mozilla.org/files/2017/11/graphs-draft02-768x191.png 768w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Quantum CSS (aka Stylo)&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/08/18.png&quot;&gt;&lt;img class=&quot;alignnone size-large wp-image-31197&quot; src=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/08/18-500x356.png&quot; alt=&quot;Cores that have finished their work stealing from the core with more work&quot; width=&quot;500&quot; height=&quot;356&quot; srcset=&quot;https://hacks.mozilla.org/files/2017/08/18-500x356.png 500w, https://hacks.mozilla.org/files/2017/08/18-250x178.png 250w, https://hacks.mozilla.org/files/2017/08/18-768x547.png 768w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;With &lt;a href=&quot;https://hacks.mozilla.org/2017/08/inside-a-super-fast-css-engine-quantum-css-aka-stylo/&quot;&gt;Stylo&lt;/a&gt;, the work of CSS style computation is fully parallelized across all of the CPU cores. Stylo uses a technique called work stealing to efficiently split up the work between the cores so that they all stay busy. With this, you get a linear speed-up. You divide the time it takes to do CSS style computation by however many cores you have.&lt;/p&gt;
&lt;h4&gt;Quantum Render (featuring WebRender)&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/10/32.png&quot;&gt;&lt;img class=&quot;alignnone size-large wp-image-31418&quot; src=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/10/32-500x560.png&quot; alt=&quot;Diagram of the 4 different threads, with a RenderBackend thread between the main thread and compositor thread. The RenderBackend thread translates the display list into batched draw calls&quot; width=&quot;500&quot; height=&quot;560&quot; srcset=&quot;https://hacks.mozilla.org/files/2017/10/32-500x560.png 500w, https://hacks.mozilla.org/files/2017/10/32-250x280.png 250w, https://hacks.mozilla.org/files/2017/10/32-768x860.png 768w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Another part of the hardware that is highly parallelized is the GPU. It has hundreds or thousands of cores. You have to do a lot of planning to make sure these cores stay as busy as they can, though. That’s &lt;a href=&quot;https://hacks.mozilla.org/2017/10/the-whole-web-at-maximum-fps-how-webrender-gets-rid-of-jank/&quot;&gt;what WebRender does&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;WebRender will land in 2018, and will take advantage of modern GPUs. In the meantime, we’ve also attacked this problem from another angle. The Advanced Layers project modifies Firefox’s existing layer system to support batch rendering. It gives us immediate wins by optimizing Firefox’s current GPU usage patterns.&lt;/p&gt;
&lt;h4&gt;???&lt;/h4&gt;
&lt;p&gt;We think other parts of the rendering pipeline can benefit from this kind of fine-grained parallelism, too. Over the coming months, we’ll be taking a closer look to see where else we can use these techniques.&lt;/p&gt;
&lt;h3&gt;Making sure we keep getting faster and never get slow again&lt;/h3&gt;
&lt;p&gt;Beyond these major architectural changes that we knew we were going to have to make, a number of performance bugs also just slipped into the code base when we weren’t looking.&lt;/p&gt;
&lt;p&gt;So we created another part of Quantum to fix this… basically a browser performance strike force that would find these problems and mobilize teams to fix them.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/graphs-draft03.png&quot;&gt;&lt;img class=&quot;alignnone size-large wp-image-31580&quot; src=&quot;https://2r4s9p1yi1fa2jd7j43zph8r-wpengine.netdna-ssl.com/files/2017/11/graphs-draft03-500x165.png&quot; alt=&quot;timeline of Quantum Flow, with an upward sloping arc&quot; width=&quot;500&quot; height=&quot;165&quot; srcset=&quot;https://hacks.mozilla.org/files/2017/11/graphs-draft03-500x165.png 500w, https://hacks.mozilla.org/files/2017/11/graphs-draft03-250x82.png 250w, https://hacks.mozilla.org/files/2017/11/graphs-draft03-768x253.png 768w&quot; sizes=&quot;(max-width: 500px) 100vw, 500px&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Quantum Flow&lt;/h4&gt;
&lt;p&gt;The Quantum Flow team was this strike force. Rather than focusing on overall performance of a particular subsystem, they zero-ed in on some very specific, important use cases — for example, loading your social media feed — and worked across teams to figure out why it was less responsive in Firefox than other browsers.&lt;/p&gt;
&lt;p&gt;Quantum Flow brought us lots of big performance wins. Along the way, we also developed tools and processes to make it easier to find and track these types of issues.&lt;/p&gt;
&lt;p&gt;So what happens to Quantum Flow now?&lt;/p&gt;
&lt;p&gt;We’re taking this process that was so successful—identifying and focusing on one key use case at a time — and turning it into a regular part of our workflow. To do this, we’re improving our tools so we don’t need a strike force of experts to search for the issues, but instead can empower more engineers across the organization to find them.&lt;/p&gt;
&lt;p&gt;But there’s one problem with this approach. When we optimize one use case, we could deoptimize another. To prevent this, we’re adding lots of new tracking, including improvements to CI automation running performance tests, telemetry to track what users experience, and regression management inside of bugs. With this, we expect Firefox Quantum to keep getting better.&lt;/p&gt;
&lt;h3&gt;Tomorrow is just the beginning&lt;/h3&gt;
&lt;p&gt;Tomorrow is a big day for us at Mozilla. We’ve been driving hard over the past year to make Firefox fast. But it’s also just the beginning.&lt;/p&gt;
&lt;p&gt;We’ll be continuously delivering new performance improvements throughout the next year. We look forward to sharing them with you!&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.mozilla.org/en-US/firefox/quantum/&quot;&gt;Try Firefox Quantum in Release&lt;/a&gt; or in &lt;a href=&quot;https://www.mozilla.org/en-US/firefox/channel/desktop/&quot;&gt;Developer Edition&lt;/a&gt; to make sure you get the latest updates as they come out.&lt;/p&gt;
&lt;section class=&quot;about&quot; readability=&quot;6.5970873786408&quot;&gt;
&lt;p&gt;Lin is an engineer on the Mozilla Developer Relations team. She tinkers with JavaScript, WebAssembly, Rust, and Servo, and also draws code cartoons.&lt;/p&gt;
&lt;p&gt;&lt;a class=&quot;url&quot; href=&quot;https://hacks.mozilla.org/author/lclarkmozilla-com/&quot;&gt;More articles by Lin Clark…&lt;/a&gt;&lt;/p&gt;
&lt;/section&gt;</description>
<pubDate>Mon, 13 Nov 2017 14:18:26 +0000</pubDate>
<dc:creator>bpierre</dc:creator>
<og:url>https://hacks.mozilla.org/2017/11/entering-the-quantum-era-how-firefox-got-fast-again-and-where-its-going-to-get-faster</og:url>
<og:title>Entering the Quantum Era—How Firefox got fast again and where it’s going to get faster – Mozilla Hacks - the Web developer blog</og:title>
<og:description>Over the past seven months, we’ve been rapidly replacing major parts of the engine, introducing Rust and parts of Servo to Firefox. Plus, we’ve had a browser performance strike force ...</og:description>
<og:image>https://hacks.mozilla.org/files/2017/11/title_image.png</og:image>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://hacks.mozilla.org/2017/11/entering-the-quantum-era-how-firefox-got-fast-again-and-where-its-going-to-get-faster/</dc:identifier>
</item>
<item>
<title>Qualcomm rejects Broadcom&amp;#039;s $103B takeover proposal</title>
<link>http://www.reuters.com/article/us-qualcomm-m-a-broadcom/qualcomm-rejects-broadcoms-103-billion-takeover-proposal-idUSKBN1DD1NU?feedType=RSS&amp;feedName=topNews&amp;utm_source=twitter&amp;utm_medium=Social</link>
<guid isPermaLink="true" >http://www.reuters.com/article/us-qualcomm-m-a-broadcom/qualcomm-rejects-broadcoms-103-billion-takeover-proposal-idUSKBN1DD1NU?feedType=RSS&amp;feedName=topNews&amp;utm_source=twitter&amp;utm_medium=Social</guid>
<description>&lt;p data-reactid=&quot;38&quot;&gt;(Reuters) - Mobile chipmaker Qualcomm Inc on Monday rejected rival Broadcom Ltd’s $103-billion takeover bid, saying the offer undervalued the company and would face regulatory hurdles.&lt;/p&gt;

&lt;p data-reactid=&quot;42&quot;&gt;Shares of Qualcomm were up 1.8 percent at $65.74 in early afternoon trading, while those of Broadcom were down 0.4 percent at $263.95.&lt;/p&gt;
&lt;p data-reactid=&quot;43&quot;&gt;Broadcom said it would seek to engage with Qualcomm’s board and management, adding that it had received positive feedback from key customers and stockholders.&lt;/p&gt;
&lt;p data-reactid=&quot;44&quot;&gt;“We continue to believe our proposal represents the most attractive, value-enhancing alternative available to Qualcomm stockholders and we are encouraged by their reaction,” the company said.&lt;/p&gt;
&lt;p data-reactid=&quot;45&quot;&gt;Both companies count Apple among their top customers. Analysts have said a deal between the two would help Qualcomm settle its legal battle with the iPhone maker as Broadcom has a closer relationship with Apple.&lt;/p&gt;
&lt;p data-reactid=&quot;46&quot;&gt;Broadcom made an unsolicited bid last week to buy Qualcomm in an effort to become the dominant supplier of chips used in the 1.5 billion or so smartphones expected to be sold around the world this year.&lt;/p&gt;
&lt;p data-reactid=&quot;47&quot;&gt;Analysts said Broadcom can now raise its bid, go for a proxy fight or launch a hostile exchange offer.&lt;/p&gt;
&lt;p data-reactid=&quot;48&quot;&gt;“Qualcomm’s ‘thanks, but no thanks’ response to the unsolicited bid by Broadcom isn’t surprising and we would be surprised if at this point, Broadcom didn’t move forward with a proxy fight,” Loop Capital analyst Betsy Van Hees told Reuters.&lt;/p&gt;
&lt;div class=&quot;Image_container_1tVQo&quot; data-reactid=&quot;49&quot;&gt;

&lt;span class=&quot;Image_caption_KoNH1&quot; data-reactid=&quot;54&quot;&gt;A sign on the Qualcomm campus is seen in San Diego, California, U.S. November 6, 2017. REUTERS/Mike Blake&lt;/span&gt;&lt;/div&gt;
&lt;p data-reactid=&quot;55&quot;&gt;If Broadcom makes a hostile bid, Qualcomm’s governance rules would allow the rival to submit its own slate for the entire 11-member board by the Dec. 8 nomination deadline.&lt;/p&gt;
&lt;p data-reactid=&quot;56&quot;&gt;The easiest option, however, would be to talk to Qualcomm’s board and agree on a higher price.&lt;/p&gt;
&lt;div class=&quot;Image_container_1tVQo&quot; data-reactid=&quot;57&quot;&gt;

&lt;span class=&quot;Image_caption_KoNH1&quot; data-reactid=&quot;62&quot;&gt;A sign to the campus offices of chip maker Broadcom Ltd, is shown in Irvine, California, U.S., November 6, 2017. REUTERS/Mike Blake&lt;/span&gt;&lt;/div&gt;
&lt;p data-reactid=&quot;63&quot;&gt;“We are well-advised and know what our options are, and we have not eliminated any of those options,” Broadcom Chief Executive Hock Tan told Reuters last week.&lt;/p&gt;
&lt;p data-reactid=&quot;64&quot;&gt;The right price for Qualcomm could be between $80 and $85 per share, and Broadcom could go up to $90, Susquehanna analyst Christopher Rolland told Reuters.&lt;/p&gt;
&lt;p data-reactid=&quot;65&quot;&gt;Any deal would face scrutiny from the antitrust regulators as the combined company would own the high-end WiFi business globally, analysts said.&lt;/p&gt;
&lt;p data-reactid=&quot;66&quot;&gt;Regulators are already scrutinizing Qualcomm’s $38-billion acquisition of automotive chipmaker NXP Semiconductors NV.&lt;/p&gt;
&lt;p data-reactid=&quot;67&quot;&gt;Broadcom has indicated it is willing to buy Qualcomm irrespective of whether it closes the NXP deal.&lt;/p&gt;
&lt;p data-reactid=&quot;68&quot;&gt;Qualcomm now needs to convince investors that they can create more shareholder value independently, Raymond James analyst Chris Caso said.&lt;/p&gt;
&lt;div class=&quot;Attribution_attribution_o4ojT&quot; data-reactid=&quot;69&quot; readability=&quot;8&quot;&gt;
&lt;p class=&quot;Attribution_content_27_rw&quot; data-reactid=&quot;70&quot;&gt;Reporting by Supantha Mukherjee in Bengaluru; additional reporting by Sonam Rai; Editing by Sriraj Kalluvila and Arun Koyyur&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ArticleBody_trustBadgeContainer_1_iEv&quot; data-reactid=&quot;71&quot;&gt;&lt;span class=&quot;ArticleBody_trustBadgeTitle_3xFqc&quot; data-reactid=&quot;72&quot;&gt;Our Standards:&lt;/span&gt;&lt;span class=&quot;trustBadgeUrl&quot; data-reactid=&quot;73&quot;&gt;&lt;a href=&quot;http://thomsonreuters.com/en/about-us/trust-principles.html&quot; data-reactid=&quot;74&quot;&gt;The Thomson Reuters Trust Principles.&lt;/a&gt;&lt;/span&gt;&lt;/div&gt;
</description>
<pubDate>Mon, 13 Nov 2017 14:08:32 +0000</pubDate>
<dc:creator>rbanffy</dc:creator>
<og:title>Qualcomm rejects Broadcom's $103-billion takeover bid</og:title>
<og:url>https://www.reuters.com/article/us-qualcomm-m-a-broadcom/qualcomm-rejects-broadcoms-103-billion-takeover-proposal-idUSKBN1DD1NU</og:url>
<og:type>article</og:type>
<og:description>Mobile chipmaker Qualcomm Inc on Monday rejected rival Broadcom Ltd's $103-billion takeover bid, saying the offer undervalued the company and would face regulatory hurdles.</og:description>
<og:image>https://s3.reutersmedia.net/resources/r/?m=02&amp;d=20171113&amp;t=2&amp;i=1209613427&amp;w=&amp;fh=545px&amp;fw=&amp;ll=&amp;pl=&amp;sq=&amp;r=LYNXMPEDAC157</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.reuters.com/article/us-qualcomm-m-a-broadcom/qualcomm-rejects-broadcoms-103-billion-takeover-proposal-idUSKBN1DD1NU?feedType=RSS&amp;feedName=topNews</dc:identifier>
</item>
<item>
<title>Don’t Tax Options and RSUs Upon Vesting</title>
<link>http://avc.com/2017/11/dont-tax-options-and-rsus-upon-vesting/</link>
<guid isPermaLink="true" >http://avc.com/2017/11/dont-tax-options-and-rsus-upon-vesting/</guid>
<description>&lt;p&gt;The current draft of the Senate Tax Reform Bill would tax stock options and RSUs upon &lt;em&gt;vesting&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Currently, stock options are taxed upon exercise and RSUs are taxed upon release of the underlying shares.&lt;/p&gt;
&lt;p&gt;This is a HUGE deal to everyone who works in companies that partially compensate their employees with these two equity instruments.&lt;/p&gt;
&lt;p&gt;What this would mean is every month, when your equity compensation vests a little bit, you will owe taxes on it even though you can’t do anything with that equity compensation.&lt;/p&gt;
&lt;p&gt;You can’t spend it, you can’t save it, you can’t invest it. Because you don’t have it yet.&lt;/p&gt;
&lt;p&gt;Taxing equity compensation upon vesting makes no sense.&lt;/p&gt;
&lt;p&gt;I have seen many employees leave companies and not exercise their vested stock options. It happens all of the time.&lt;/p&gt;
&lt;p&gt;That should be a clear enough example to the lawmakers that vesting should not be a taxable event.&lt;/p&gt;
&lt;p&gt;But, sadly, I don’t think this is really about what makes sense. It is about politics.&lt;/p&gt;
&lt;p&gt;The US Senate, particularly the Republican leadership, needs to hear from you, the employees who will feel the pain of this change, that it is wrong.&lt;/p&gt;
&lt;p&gt;Otherwise, I think this provision could become law.&lt;/p&gt;
&lt;p&gt;And that would be the end of equity compensation in startups as we know it.&lt;/p&gt;
&lt;p&gt;If this provision becomes law, startup and growth tech companies will not be able to offer equity compensation to their employees. We will see equity compensation replaced with cash compensation and the ability to share in the wealth creation at your employer will be taken away. This has profound implications for those who work in tech companies and equally profound implications for the competitiveness of the US tech sector.&lt;/p&gt;
&lt;p&gt;So, what can we do about this?&lt;/p&gt;
&lt;p&gt;First, we have to move fast. The tax reform bill is moving quickly with a goal of getting it done before year end.&lt;/p&gt;
&lt;p&gt;This particular provision, which was in the House bill and was taken out last week, will be considered by the Senate as soon as TODAY.&lt;/p&gt;
&lt;p&gt;So, please reach out to your Senators and let them know that they “must remove&lt;em&gt; &lt;/em&gt;Section III(H)(1) from the Senate Tax Cuts And Jobs Act”.&lt;/p&gt;
&lt;p&gt;The best way to do that is to call their office and speak to the staffer who handles tax reform for them.&lt;/p&gt;
&lt;p&gt;Here’s &lt;a href=&quot;http://www.ucsusa.org/action/phone-calls.html#.Wgl44rA-dGw&quot;&gt;a short explanation of how to do that&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Please do it today. This is really very important to everyone who works in tech.&lt;/p&gt;
</description>
<pubDate>Mon, 13 Nov 2017 13:41:32 +0000</pubDate>
<dc:creator>mooreds</dc:creator>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://avc.com/2017/11/dont-tax-options-and-rsus-upon-vesting/</dc:identifier>
</item>
<item>
<title>The most downvoted comment in Reddit&amp;#039;s history</title>
<link>https://www.reddit.com/r/StarWarsBattlefront/comments/7cff0b/seriously_i_paid_80_to_have_vader_locked/dppum98/</link>
<guid isPermaLink="true" >https://www.reddit.com/r/StarWarsBattlefront/comments/7cff0b/seriously_i_paid_80_to_have_vader_locked/dppum98/</guid>
<description>&lt;p&gt;&lt;span class=&quot;score&quot; title=&quot;10330&quot;&gt;10.3k&lt;/span&gt;  · &lt;/p&gt;
&lt;div class=&quot;read-next-thumbnail&quot;&gt;&lt;img src=&quot;https://b.thumbs.redditmedia.com/t5Gz83TcjlkbJLCiYwrczM78BnsruT1-2_DA8UvZgPs.jpg&quot; width=&quot;70&quot; height=&quot;39&quot; alt=&quot;&quot;/&gt;&lt;/div&gt;
&lt;p&gt;Gamespot purchases $100 worth of loot crates, ends up with less than half the amount of credits needed to unlock Darth Vader and Luke. 40 hours or $260 to unlock one of the main characters in Star Wars.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;score&quot; title=&quot;18515&quot;&gt;18.5k&lt;/span&gt;  · &lt;/p&gt;
&lt;div class=&quot;read-next-thumbnail&quot;&gt;&lt;img src=&quot;https://b.thumbs.redditmedia.com/7kHuyRn7def4g8vEqdBq7O5ifMWw40hoU5_MuMXXuKY.jpg&quot; width=&quot;70&quot; height=&quot;52&quot; alt=&quot;&quot;/&gt;&lt;/div&gt;
&lt;p&gt;THIS JUST APPEARED IN THE BF2 STORE&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;score&quot; title=&quot;3067&quot;&gt;3067&lt;/span&gt;  · &lt;/p&gt;
&lt;div class=&quot;read-next-thumbnail&quot;&gt;&lt;img src=&quot;https://a.thumbs.redditmedia.com/C2TUHfoHED554AMrhKronZOpwmic-EUR2SOEaFm5KM0.jpg&quot; width=&quot;70&quot; height=&quot;29&quot; alt=&quot;&quot;/&gt;&lt;/div&gt;
&lt;p&gt;Whoever did this, hats off to you!&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;score&quot; title=&quot;1879&quot;&gt;1879&lt;/span&gt;  · &lt;/p&gt;
&lt;p&gt;Change will be a Constant in Star Wars Battlefront II&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;score&quot; title=&quot;2548&quot;&gt;2548&lt;/span&gt;  · &lt;/p&gt;
&lt;div class=&quot;read-next-thumbnail&quot;&gt;&lt;img src=&quot;https://b.thumbs.redditmedia.com/18cOGpiQSXqn5Sn8FJycorTUEPb4XC9BB-46Zp1Ag6I.jpg&quot; width=&quot;70&quot; height=&quot;51&quot; alt=&quot;&quot;/&gt;&lt;/div&gt;
&lt;p&gt;Dunkey called it in June&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;score&quot; title=&quot;15732&quot;&gt;15.7k&lt;/span&gt;  · &lt;/p&gt;
&lt;div class=&quot;read-next-thumbnail&quot;&gt;&lt;img src=&quot;https://b.thumbs.redditmedia.com/yvVsHT7tx4UxbEBBzcxwmIm3pcbofAlYaabgl6jLVXs.jpg&quot; width=&quot;70&quot; height=&quot;63&quot; alt=&quot;&quot;/&gt;&lt;/div&gt;
&lt;p&gt;While the Loot Box system is horrible, don't send death threats to the devs.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;score&quot; title=&quot;3805&quot;&gt;3805&lt;/span&gt;  · &lt;/p&gt;
&lt;p&gt;Their tagline is &quot;Heroes are born&quot;. More like Heroes are bought.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;score&quot; title=&quot;3676&quot;&gt;3676&lt;/span&gt;  · &lt;/p&gt;
&lt;div class=&quot;read-next-thumbnail&quot;&gt;&lt;img src=&quot;https://a.thumbs.redditmedia.com/8mb3fOJvuc23YnvkmI9I6_n6fhqoQ7fyL2V2RiNjWl0.jpg&quot; width=&quot;70&quot; height=&quot;70&quot; alt=&quot;&quot;/&gt;&lt;/div&gt;
&lt;p&gt;Looks like everyone’s jumping on board.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;score&quot; title=&quot;4681&quot;&gt;4681&lt;/span&gt;  · &lt;/p&gt;
&lt;p&gt;I just emailed Disney's video game department about SWBFII and I urge you to do the same.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;score&quot; title=&quot;2038&quot;&gt;2038&lt;/span&gt;  · &lt;/p&gt;
&lt;p&gt;Tomorrow I'll quit my job so I can dedicate my 40 hours per week to unlock a hero!&lt;/p&gt;
</description>
<pubDate>Mon, 13 Nov 2017 13:05:56 +0000</pubDate>
<dc:creator>edem</dc:creator>
<og:image>https://www.redditstatic.com/icon.png</og:image>
<og:description>The intent is to provide players with a sense of pride and accomplishment for unlocking different heroes. As for cost, we selected initial...</og:description>
<og:title>Seriously? I paid 80$ to have Vader locked? • r/StarWarsBattlefront</og:title>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.reddit.com/r/StarWarsBattlefront/comments/7cff0b/seriously_i_paid_80_to_have_vader_locked/dppum98/</dc:identifier>
</item>
<item>
<title>Face ID beaten by mask</title>
<link>http://www.bkav.com/d/top-news/-/view_content/content/103968/face-id-beaten-by-mask-not-an-effective-security-measure</link>
<guid isPermaLink="true" >http://www.bkav.com/d/top-news/-/view_content/content/103968/face-id-beaten-by-mask-not-an-effective-security-measure</guid>
<description>&lt;div id=&quot;language-id&quot;&gt;&lt;a href=&quot;http://bkav.com.vn/tin_tuc_noi_bat/-/chi_tiet/496291/bi-%C4%91anh-bai-boi-mat-na-cua-bkav-face-id-khong-%C4%91u-muc-%C4%91o-an-ninh&quot;&gt;&lt;img alt=&quot;Tiếng Việt (Việt Nam) - Beta&quot; class=&quot;icon&quot; id=&quot;aui_3_4_0_1_1044&quot; src=&quot;http://www.bkav.com/html/themes/classic/images/spacer.png&quot; title=&quot;Tiếng Việt&quot;/&gt;&lt;/a&gt;&lt;a href=&quot;http://www.bkav.com/top-news/-/view_content/content/103968/face-id-beaten-by-mask-not-an-effective-security-measure&quot;&gt;&lt;img alt=&quot;English (United States)&quot; class=&quot;icon&quot; src=&quot;http://www.bkav.com/html/themes/classic/images/spacer.png&quot; title=&quot;English (United States)&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;Questions &amp;amp; Answers&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Updated November 11, 2017) &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Q: How Face ID was set up?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; It learns from human face, just like normal.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Q: Were you able to use the mask to unlock the iPhone immediately after freshly enrolling the real face? The reason I ask is that, according to Apple's whitepaper, Face ID will take additional captures over time and augment its enrolled Face ID data with the newly calculated mathematical representation. Can you describe precisely how you went about conducting this experiment?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; It does not matter whether Apple Face ID &quot;learns&quot; new images of the face, since it will not affect the truth that Apple Face ID is not an effective security measure. However, we knew about this &quot;learning&quot;, thus, to give a more persuasive result, we applied the strict rule of &quot;absolutely no passcode&quot; when crafting the mask.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Q: Can you explain why your hack worked but similar attempts (like &lt;a href=&quot;https://www.wired.com/story/tried-to-beat-face-id-and-failed-so-far/&quot;&gt;Wired magazine's&lt;/a&gt;) failed?  &lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Because... we are the leading cyber security firm ;) It is quite hard to make the &quot;correct&quot; mask without certain knowledge of security. We were able to trick Apple's AI, as mentioned in the writing, because we understood how their AI worked and how to bypass it. As in 2008, we were the first to show that face recognition was not an effective security measure for laptops (related links can be found at the end of this writing).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Q: How did Bkav develop the mask (for example why you use silicone for the nose, why 3D printing for some areas while special processing for others, etc.)?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; You are right. Many people in the world have tried different kinds of masks but all failed. It is because we understand how AI of Face ID works and how to bypass it. As stated above, we were the first in the world to show that face recognition was not an effective security measure for laptops.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Q: Is 3D creation and printing difficult?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Not at all. It is quite simple, will be even more simple in the future. We might use smartphones with 3D scanning capabilities (like &lt;a href=&quot;https://www.sonymobile.com/global-en/products/phones/xperia-xz1/3d-creator/&quot;&gt;Sony XZ1&lt;/a&gt;); or set up a room with a 3D scanner, a few seconds is enough for the scanning (here's an example of a &lt;a href=&quot;https://www.aniwaa.com/product/3d-scanners/staramba-3d-instagraph/&quot;&gt;3D scanning booth&lt;/a&gt;). &lt;/p&gt;
&lt;p&gt;An easier way is photograph-based, artists craft a thing from its photos. Take the nose of our mask for example, its creation is not complicated at all. We had an artist make it by silicone first. Then, when we found that the nose did not perfectly meet our demand, we fixed it on our own, then the hack worked. That's why there's a part on the nose's left side that is of a different color (photo attached). So, it's easy to make the mask and beat Face ID. Here, I want to repeat that our experiment is a kind of Proof of Concept, the purpose of which is to prove a principle, other issues will be researched later.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Q: Are the dimensions of a person's face needed? How would those be obtained without a target sitting for them?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; The 1st point is, everything went much more easily than you expect. You can try it out with your own iPhone X, the phone shall recognize you even when you cover a half of your face. It means the recognition mechanism is not as strict as you think, Apple seems to rely too much on Face ID's AI. We just need a half face to create the mask. It was even simpler than we ourselves had thought.&lt;/p&gt;
&lt;p&gt;Apple has done this not so well. I remember reading an article on Mashable, in which Apple told that iPhone X had been planned to be rolled out in 2018, but the company then decided to release it one year earlier. This shows that they haven't carried out scientific and serious estimation before deciding to replace Touch ID with Face ID.&lt;/p&gt;
&lt;p&gt;The 2nd point is, in cyber security, we call it Proof of Concept, which is useful for both sides, the hackers and the users. The hackers, they can find out a simpler way to exploit users' device based on such PoC. While with users, if they know about such possibility, they will not use the feature to keep themselves safe. Just like the KRACK attack, it is not easy to be successfully exploited but users are urged to update the patch ASAP, because the threats are real. With Face ID's being beaten by our mask, FBI, CIA, country leaders, leaders of major corporations, ect. are the ones that need to know about the issue, because their devices are worth illegal unlock attempts. Exploitation is difficult for normal users, but simple for professional ones.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Q: What technologies and techniques were employed to make the 3D model associated with the 3D-printed portions of the mask?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A: &lt;/strong&gt;We used a popular 3D printer. Nose was made by a handmade artist. We use 2D printing for other parts (similar to how we tricked Face Recognition 9 years ago). The skin was also hand-made to trick Apple's AI.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Q: What's the approximate cost of the mask?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; ~ 150 USD&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Q: How long did it take to construct the mask, including the time to develop 3D models and other assets associated with its production?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; We started working on it, including 3D models and other assets, right after receiving iPhone X on Nov 5.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Q: Who would be the target for this kind of attack?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Potential targets shall not be regular users, but billionaires, leaders of major corporations, nation leaders and agents like FBI need to understand the Face ID's issue. Security units' competitors, commercial rivals of corporations, and even nations might benefit from our PoC.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Q: The clip is not clear enough. Your clip and answers have not included all details. Can you record another clip of this experiement with more details?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; I want to repeat that our experiment is a kind of Proof of Concept, the purpose of which is to prove a principle, other issues will be researched later.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Q: Is this a kind of deframing your competitor?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This is the work of our cyber security domain. As you have learnt from above answers, from 2008 when Bkav had not developed Bphone (&lt;a href=&quot;http://www.bkav.com/bphone&quot;&gt;Bkav.com/Bphone&lt;/a&gt;), we were the first company in the world to show that face recognition was not an effective security measure for laptops, right after Toshiba, Lenovo, Asus, etc. used this technology for their products (related links can be found at the end of this writing).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Q: According to Bkav, up to now which security measure is the most secure?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; As for biometric security, fingerprint is the best.  &lt;/p&gt;
&lt;p&gt;-------------------------------------&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;strong&gt;Face ID beaten by mask, not an effective security measure&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;Vietnam, November 10, 2017 -&lt;/strong&gt; At iPhone X launch event, Apple's Senior Vice President Phil Schiller claimed that Face ID can distinguish human's real face from masks thanks to its artificial intelligence (AI). &quot;&lt;/span&gt;&lt;em&gt;They (Apple engineering teams) have even gone and worked with professional mask makers and makeup artists in Hollywood to protect against these attempts to beat Face ID. These are actual masks used by the engineering team to train the neural network to protect against them in Face ID. It's incredible!&quot;,&lt;/em&gt; &lt;span&gt;Phil Schiller said (&lt;/span&gt;&lt;a href=&quot;https://www.apple.com/vn/apple-events/september-2017/&quot;&gt;Apple's Keynote September 2017&lt;/a&gt;&lt;span&gt;, from 1:27:10 to 1:27:26). However, one week after iPhone X officially went on sale,&lt;/span&gt; &lt;span&gt;Bkav security experts from Vietnam show that Face ID can be fooled by mask, which means it is not an effective security measure.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;img alt=&quot;&quot; src=&quot;http://www.bkav.com/documents/10192/0/Apple-FaceID.png?t=1510491939352&quot;/&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&quot;These are actual masks used by the engineering team to train the neural network to protect against them in Face ID&quot; &lt;/em&gt;&lt;em&gt;(Apple)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Below is the video that demonstrates how Face ID is beaten:&lt;/p&gt;
&lt;p&gt;(You can as well watch the video on YouTube here: &lt;a href=&quot;https://www.youtube.com/watch?v=i4YQRLQVixM&quot; target=&quot;_blank&quot;&gt;How Face ID be beaten by a mask&lt;/a&gt;, or download the video &lt;a href=&quot;http://www.bkav.com/documents/10180/103905/Mask-beats-FaceID.mp4&quot;&gt;here&lt;/a&gt;) &lt;/p&gt;
&lt;p&gt;&lt;iframe allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; height=&quot;315&quot; id=&quot;iframe-video-youtube&quot; src=&quot;https://www.youtube.com/embed/i4YQRLQVixM&quot; width=&quot;560&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;Mr. Ngo Tuan Anh, Bkav's Vice President of Cyber Security, said: &quot;&lt;em&gt;The mask is crafted by combining 3D printing with makeup and 2D images, besides some special processing on the cheeks and around the face, where there are large skin areas, to fool AI of Face ID&lt;/em&gt;&quot;.&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;img alt=&quot;&quot; src=&quot;http://www.bkav.com/documents/10192/0/FaceID.png?t=1510304099801&quot;/&gt;&lt;/p&gt;
&lt;p&gt;So, after nearly 10 years of development, face recognition is not mature enough to guarantee security for computers and smartphones. In 2008, Bkav was the first company in the world to show that face recognition was not an effective security measure for laptops, right after Toshiba, Lenovo, Asus, etc. used this technology for their products. You can find out more details in the links below (note: Bkis is the former name of Bkav):&lt;/p&gt;
&lt;p&gt;1. &lt;a href=&quot;https://www.computerworld.com/article/2531298/windows-pcs/laptop-face-recognition-tech-easy-to-hack--warns-black-hat-researcher.html&quot;&gt;Laptop face-recognition tech easy to hack, warns Black Hat researcher&lt;/a&gt; (Computerworld)&lt;/p&gt;
&lt;p&gt;2. &lt;a href=&quot;https://www.youtube.com/watch?v=OAdbbEKzUBo&quot;&gt;Face recognition bypass demonstration clip by Bkis at Black Hat Conference&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;3. &lt;a href=&quot;https://www.cnet.com/news/vietnamese-security-firm-your-face-is-easy-to-fake/&quot;&gt;Vietnamese security firm: Your face is easy to fake&lt;/a&gt; (CNET)&lt;/p&gt;
&lt;p&gt;We will publish the research which helps us to craft the mask that beats Face ID in the next writing.&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&lt;strong&gt;Bkav&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;About Bkav:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Established in 1995, Bkav Corporation (&lt;a href=&quot;http://www.bkav.com/&quot;&gt;www.bkav.com&lt;/a&gt;) is the leading firm in network security, software, smartphone manufacturing (&lt;a href=&quot;http://bkav.com/Bphone&quot; target=&quot;_blank&quot;&gt;Bkav.com/Bphone&lt;/a&gt;) and smarthome. Bkav is the first company in the world to discover the vulnerability in face recognition log-on in laptops right after this technology started to be popular. Before, the technology was trusted to be highly accurate and was used in products of many technology companies around the world like Toshiba, Lenovo, Asus, and so on.&lt;/p&gt;
&lt;p&gt;Bkav is known as the security firm to discover the first critical flaw in Google Chrome just days after its launch in 2008. Bkav was also the firm to trace the master server in Britain of unprecedentedly massive DDoS attacks targeting US and Korean governments' websites in July, 2009.&lt;/p&gt;
</description>
<pubDate>Mon, 13 Nov 2017 10:40:28 +0000</pubDate>
<dc:creator>scribu</dc:creator>
<og:title>Face ID beaten by mask, not an effective security measure</og:title>
<og:description>Bkav security experts from Vietnam show that Face ID can be fooled by mask, which means it is not an effective security measure.</og:description>
<og:url>http://www.bkav.com/d/top-news/-/view_content/content/103968/face-id-beaten-by-mask-not-an-effective-security-measure</og:url>
<og:image>http://www.bkav.com/documents/10192/0/set.png</og:image>
<og:type>article</og:type>
<dc:language>en-US</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://www.bkav.com/d/top-news/-/view_content/content/103968/face-id-beaten-by-mask-not-an-effective-security-measure</dc:identifier>
</item>
<item>
<title>Go, don&amp;#039;t collect my garbage</title>
<link>https://blog.cloudflare.com/go-dont-collect-my-garbage/</link>
<guid isPermaLink="true" >https://blog.cloudflare.com/go-dont-collect-my-garbage/</guid>
<description>&lt;p&gt;Not long ago I needed to benchmark the performance of Golang on a many-core machine. I took several of the benchmarks that are bundled with the Go source code, copied them, and modified them to run on all available threads. In that case the machine has 24 cores and 48 threads.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.cloudflare.com/content/images/2017/11/36963798223_b4da5151aa_k.jpg&quot; alt=&quot;&quot; title=&quot;&quot;/&gt;&lt;small&gt;&lt;a href=&quot;https://creativecommons.org/licenses/by-sa/2.0/&quot;&gt;CC BY-SA 2.0&lt;/a&gt; &lt;a href=&quot;https://www.flickr.com/photos/147079914@N03/36963798223/in/photolist-ZhPe1A-Yjn8pV-ZhPcm3-YfT3xL-ZkG9kP-qLEURD-4rPJbB-uwsT5-9aWgeA-92n4h-5LWz68-92n6p-5gE7TJ-3Scj6p-duNYgz-4rTMKJ-8P3YZ3-8QLKYc-CHFrLH-MuQT2E&quot;&gt;image&lt;/a&gt; by &lt;a href=&quot;https://www.flickr.com/photos/147079914@N03/&quot;&gt;sponki25&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;I started with ECDSA P256 Sign, probably because I have warm feeling for that function, since I &lt;a href=&quot;https://blog.cloudflare.com/go-crypto-bridging-the-performance-gap/&quot;&gt;optimized it for amd64&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, I ran the benchmark on a single goroutine: &lt;code&gt;ECDSA-P256 Sign,30618.50, op/s&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;That looks good; next I ran it on 48 goroutines: &lt;code&gt;ECDSA-P256 Sign,78940.67, op/s&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;OK, that is not what I expected. Just over 2X speedup, from 24 physical cores? I must be doing something wrong. Maybe Go only uses two cores? I ran &lt;code&gt;top&lt;/code&gt;, it showed 2,266% utilization. That is not the 4,800% I expected, but it is also way above 400%.&lt;/p&gt;
&lt;p&gt;How about taking a step back, and running the benchmark on two goroutines? &lt;code&gt;ECDSA-P256 Sign,55966.40, op/s&lt;/code&gt;. Almost double, so pretty good. How about four goroutines? &lt;code&gt;ECDSA-P256 Sign,108731.00, op/s.&lt;/code&gt; That is actually faster than 48 goroutines, what is going on?&lt;/p&gt;
&lt;p&gt;I ran the benchmark for every number of goroutines from 1 to 48:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.cloudflare.com/content/images/2017/11/goroutines2.png&quot; alt=&quot;alt&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Looks like the number of signatures per second peaks at 274,622, with 17 goroutines. And starts dropping rapidly after that.&lt;/p&gt;
&lt;p&gt;Time to do some profiling.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;(pprof) top 10
Showing nodes accounting for 47.53s, 50.83% of 93.50s total  
Dropped 224 nodes (cum &amp;lt;= 0.47s)  
Showing top 10 nodes out of 138  
      flat  flat%   sum%        cum   cum%
     9.45s 10.11% 10.11%      9.45s 10.11%  runtime.procyield /state/home/vlad/go/src/runtime/asm_amd64.s
     7.55s  8.07% 18.18%      7.55s  8.07%  runtime.futex /state/home/vlad/go/src/runtime/sys_linux_amd64.s
     6.77s  7.24% 25.42%     19.18s 20.51%  runtime.sweepone /state/home/vlad/go/src/runtime/mgcsweep.go
     4.20s  4.49% 29.91%     16.28s 17.41%  runtime.lock /state/home/vlad/go/src/runtime/lock_futex.go
     3.92s  4.19% 34.11%     12.58s 13.45%  runtime.(*mspan).sweep /state/home/vlad/go/src/runtime/mgcsweep.go
     3.50s  3.74% 37.85%     15.92s 17.03%  runtime.gcDrain /state/home/vlad/go/src/runtime/mgcmark.go
     3.20s  3.42% 41.27%      4.62s  4.94%  runtime.gcmarknewobject /state/home/vlad/go/src/runtime/mgcmark.go
     3.09s  3.30% 44.58%      3.09s  3.30%  crypto/elliptic.p256OrdSqr /state/home/vlad/go/src/crypto/elliptic/p256_asm_amd64.s
     3.09s  3.30% 47.88%      3.09s  3.30%  runtime.(*lfstack).pop /state/home/vlad/go/src/runtime/lfstack.go
     2.76s  2.95% 50.83%      2.76s  2.95%  runtime.(*gcSweepBuf).push /state/home/vlad/go/src/runtime/mgcsweepbuf.go
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;Clearly Go spends a disproportionate amount of time collecting garbage. All my benchmark does is generates signatures and then dumps them.&lt;/p&gt;
&lt;p&gt;So what are our options? The Go runtime states the following:&lt;/p&gt;
&lt;blockquote readability=&quot;10.081803005008&quot;&gt;
&lt;p&gt;The GOGC variable sets the initial garbage collection target percentage. A collection is triggered when the ratio of freshly allocated data to live data remaining after the previous collection reaches this percentage. The default is GOGC=100. Setting GOGC=off disables the garbage collector entirely. The runtime/debug package's SetGCPercent function allows changing this percentage at run time. See &lt;a href=&quot;https://golang.org/pkg/runtime/debug/#SetGCPercent&quot;&gt;https://golang.org/pkg/runtime/debug/#SetGCPercent&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The GODEBUG variable controls debugging variables within the runtime. It is a comma-separated list of name=val pairs setting these named variables:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let’s see what setting &lt;code&gt;GODEBUG&lt;/code&gt; to &lt;code&gt;gctrace=1&lt;/code&gt; does.&lt;/p&gt;
&lt;pre&gt;
&lt;code&gt;gc 1 @0.021s 0%: 0.15+0.37+0.25 ms clock, 3.0+0.19/0.39/0.60+5.0 ms cpu, 4-&amp;gt;4-&amp;gt;0 MB, 5 MB goal, 48 P  
gc 2 @0.024s 0%: 0.097+0.94+0.16 ms clock, 0.29+0.21/1.3/0+0.49 ms cpu, 4-&amp;gt;4-&amp;gt;1 MB, 5 MB goal, 48 P  
gc 3 @0.027s 1%: 0.10+0.43+0.17 ms clock, 0.60+0.48/1.5/0+1.0 ms cpu, 4-&amp;gt;4-&amp;gt;0 MB, 5 MB goal, 48 P  
gc 4 @0.028s 1%: 0.18+0.41+0.28 ms clock, 0.18+0.69/2.0/0+0.28 ms cpu, 4-&amp;gt;4-&amp;gt;0 MB, 5 MB goal, 48 P  
gc 5 @0.031s 1%: 0.078+0.35+0.29 ms clock, 1.1+0.26/2.0/0+4.4 ms cpu, 4-&amp;gt;4-&amp;gt;0 MB, 5 MB goal, 48 P  
gc 6 @0.032s 1%: 0.11+0.50+0.32 ms clock, 0.22+0.99/2.3/0+0.64 ms cpu, 4-&amp;gt;4-&amp;gt;0 MB, 5 MB goal, 48 P  
gc 7 @0.034s 1%: 0.18+0.39+0.27 ms clock, 0.18+0.56/2.2/0+0.27 ms cpu, 4-&amp;gt;4-&amp;gt;0 MB, 5 MB goal, 48 P  
gc 8 @0.035s 2%: 0.12+0.40+0.27 ms clock, 0.12+0.63/2.2/0+0.27 ms cpu, 4-&amp;gt;4-&amp;gt;0 MB, 5 MB goal, 48 P  
gc 9 @0.036s 2%: 0.13+0.41+0.26 ms clock, 0.13+0.52/2.2/0+0.26 ms cpu, 4-&amp;gt;4-&amp;gt;0 MB, 5 MB goal, 48 P  
gc 10 @0.038s 2%: 0.099+0.51+0.20 ms clock, 0.19+0.56/1.9/0+0.40 ms cpu, 4-&amp;gt;5-&amp;gt;0 MB, 5 MB goal, 48 P  
gc 11 @0.039s 2%: 0.10+0.46+0.20 ms clock, 0.10+0.23/1.3/0.005+0.20 ms cpu, 4-&amp;gt;4-&amp;gt;0 MB, 5 MB goal, 48 P  
gc 12 @0.040s 2%: 0.066+0.46+0.24 ms clock, 0.93+0.40/1.7/0+3.4 ms cpu, 4-&amp;gt;4-&amp;gt;0 MB, 5 MB goal, 48 P  
gc 13 @0.041s 2%: 0.099+0.30+0.20 ms clock, 0.099+0.60/1.7/0+0.20 ms cpu, 4-&amp;gt;4-&amp;gt;0 MB, 5 MB goal, 48 P  
gc 14 @0.042s 2%: 0.095+0.45+0.24 ms clock, 0.38+0.58/2.0/0+0.98 ms cpu, 4-&amp;gt;5-&amp;gt;0 MB, 5 MB goal, 48 P  
gc 15 @0.044s 2%: 0.095+0.45+0.21 ms clock, 1.0+0.78/1.9/0+2.3 ms cpu, 4-&amp;gt;4-&amp;gt;0 MB, 5 MB goal, 48 P  
gc 16 @0.045s 3%: 0.10+0.45+0.23 ms clock, 0.10+0.70/2.1/0+0.23 ms cpu, 4-&amp;gt;5-&amp;gt;0 MB, 5 MB goal, 48 P  
gc 17 @0.046s 3%: 0.088+0.40+0.17 ms clock, 0.088+0.45/1.9/0+0.17 ms cpu, 4-&amp;gt;4-&amp;gt;0 MB, 5 MB goal, 48 P  
.
.
.
.
gc 6789 @9.998s 12%: 0.17+0.91+0.24 ms clock, 0.85+1.8/5.0/0+1.2 ms cpu, 4-&amp;gt;6-&amp;gt;1 MB, 6 MB goal, 48 P  
gc 6790 @10.000s 12%: 0.086+0.55+0.24 ms clock, 0.78+0.30/4.2/0.043+2.2 ms cpu, 4-&amp;gt;5-&amp;gt;1 MB, 6 MB goal, 48 P
&lt;/code&gt;
&lt;/pre&gt;
&lt;p&gt;The first round of GC kicks in at 0.021s, then it starts collecting every 3ms and then every 1ms. That is insane, the benchmark runs for 10 seconds, and I saw 6,790 rounds of GC. The number that starts with @ is the time since program start, followed by a percentage that supposedly states the amount of time spent collecting garbage. This number is clearly misleading, because the performance indicates at least 90% of the time is wasted (indirectly) on GC, not 12%. The synchronization overhead is not taken into account. What really is interesting are the three numbers separated by arrows. They show the size of the heap at GC start, GC end, and the live heap size. Remember that a collection is triggered when the ratio of freshly allocated data to live data remaining after the previous collection reaches this percentage, and defaults to 100%.&lt;/p&gt;
&lt;p&gt;I am running a benchmark, where all allocated data is immediately discarded, and collected at the next GC cycle. The only live heap is fixed to the Go runtime, and having more goroutines does not add to the live heap. In contrast the freshly allocated data grows much faster with each additional goroutine, triggering increasingly frequent, and expensive GC cycles.&lt;/p&gt;
&lt;p&gt;Clearly what I needed to do next was to run the benchmark with the GC disabled, by setting &lt;code&gt;GOGC=off&lt;/code&gt;. This lead to a dramatic improvement: &lt;code&gt;ECDSA-P256 Sign,413740.30, op/s&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But still not the number I was looking for, and running an application without garbage collection is unsustainable in the long run. I started playing with the &lt;code&gt;GOGC&lt;/code&gt; variable. First I set it to 2,400, which made sense since we have 24 cores, perhaps collecting garbage 24 times less frequently will do the trick: &lt;code&gt;ECDSA-P256 Sign,671538.90, op/s&lt;/code&gt;, oh my that is getting better.&lt;/p&gt;
&lt;p&gt;What if I tried 4,800, for the number of threads? &lt;code&gt;ECDSA-P256 Sign,685810.90, op/s&lt;/code&gt;. Getting warmer.&lt;/p&gt;
&lt;p&gt;I ran a script to find the best value, from 100 to 20,000, in increments of 100. This is what I got:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.cloudflare.com/content/images/2017/11/gogc.png&quot; alt=&quot;alt&quot;/&gt;&lt;/p&gt;
&lt;p&gt;Looks like the optimal value for &lt;code&gt;GOGC&lt;/code&gt; in that case is 11,300 and it gets us 691,054 signatures/second. That is 22.56X times faster than the single core score, and overall pretty good for a 24 core processor. Remember that when running on a single core, the CPU frequency is 3.0GHz, and only 2.1GHz when running on all cores.&lt;/p&gt;
&lt;p&gt;Per goroutine performance when running with &lt;code&gt;GOGC=11330&lt;/code&gt; now looks like that:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.cloudflare.com/content/images/2017/11/goroutines3.png&quot; alt=&quot;&quot;/&gt;&lt;/p&gt;
&lt;p&gt;The scaling looks much better, and even past 24 goroutines, when we run out of physical cores, and start sharing cores with hyper-threading, the overall performance improves.&lt;/p&gt;
&lt;p&gt;The bottom line here is that although this type of benchmarking is definitely an edge case for garbage collection, where 48 threads allocate large amounts of short lived data, this situation can occur in real world scenarios. As many-core CPUs become a commodity, one should be aware of the pitfalls.&lt;/p&gt;
&lt;p&gt;Most languages with garbage collection offer some sort of garbage collection control. Go has the GOGC variable, that can also be controlled with the SetGCPercent function in the runtime/debug package. Don't be afraid to tune the GC to suit your needs.&lt;/p&gt;
&lt;hr/&gt;&lt;p&gt;&lt;small&gt;We're always looking for Go programmers, so if you found this blog post interesting, why not check out our &lt;a href=&quot;https://www.cloudflare.com/join-our-team&quot;&gt;jobs page&lt;/a&gt;?&lt;/small&gt;&lt;/p&gt;
</description>
<pubDate>Mon, 13 Nov 2017 10:31:24 +0000</pubDate>
<dc:creator>jgrahamc</dc:creator>
<og:type>article</og:type>
<og:title>Go, don't collect my garbage</og:title>
<og:description>Not long ago I needed to benchmark the performance of Golang on a many-core machine. I took several of the benchmarks that are bundled with the Go source code, copied them, and modified them to run on all available threads. In that case the machine has 24 cores and 48</og:description>
<og:url>http://blog.cloudflare.com/go-dont-collect-my-garbage/</og:url>
<og:image>http://blog.cloudflare.com/content/images/2017/11/36963798223_b4da5151aa_k-1.jpg</og:image>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.cloudflare.com/go-dont-collect-my-garbage/</dc:identifier>
</item>
<item>
<title>Why I’m Digging Deep Into Alzheimer’s</title>
<link>https://www.gatesnotes.com/Health/Digging-Deep-Into-Alzheimers?WT.mc_id=11_13_2017_00_Alzheimers_BG-EM_&amp;WT.tsrc=BGEM</link>
<guid isPermaLink="true" >https://www.gatesnotes.com/Health/Digging-Deep-Into-Alzheimers?WT.mc_id=11_13_2017_00_Alzheimers_BG-EM_&amp;WT.tsrc=BGEM</guid>
<description>&lt;p&gt;In every part of the world, people are living longer than they used to. Thanks to scientific advancements, fewer people die young from heart disease, cancer, and infectious diseases. It’s no longer unusual for a person to live well into their 80s and beyond. My dad will celebrate his 92&lt;sup&gt;nd&lt;/sup&gt; birthday in a couple weeks, a milestone that was practically unimaginable when he was born.&lt;/p&gt;
&lt;p&gt;This fact—that people are living longer than ever before—should always be a wonderful thing. But what happens when it’s not?&lt;/p&gt;
&lt;p&gt;&lt;iframe width=&quot;800&quot; height=&quot;450&quot; frameborder=&quot;0&quot; src=&quot;https://www.youtube.com/embed/tMpsn6wDS5E?rel=0&quot;&gt;[embedded content]&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;The longer you live, the more likely you are to develop a chronic condition. Your risk of getting arthritis, Parkinson’s, or another non-infectious disease that diminishes your quality of life increases with each year. But of all the disorders that plague us late in life, one stands out as a particularly big threat to society: Alzheimer’s disease.&lt;/p&gt;
&lt;p&gt;You have a nearly &lt;a href=&quot;https://www.alz.org/documents_custom/2016-facts-and-figures.pdf&quot; target=&quot;_blank&quot;&gt;50 percent chance&lt;/a&gt; of developing the disease if you live into your mid-80s. In the United States, it is the only cause of death in the top 10 without any meaningful treatments that becomes more prevalent each year. That trend will likely continue as baby boomers age, which means that more families will watch their loved ones suffer from cognitive decline and slowly disappear. Despite this growing burden, scientists have yet to figure out what exactly causes Alzheimer’s or how to stop the disease from destroying the brain.&lt;/p&gt;
&lt;p&gt;I first became interested in Alzheimer’s because of its costs—both emotional and economic—to families and healthcare systems. The financial burden of the disease is much easier to quantify. A person with Alzheimer’s or another form of dementia spends &lt;a href=&quot;https://www.alz.org/facts/#cost&quot; target=&quot;_blank&quot;&gt;five times more&lt;/a&gt; every year out-of-pocket on healthcare than a senior without a neurodegenerative condition. Unlike those with many chronic diseases, people with Alzheimer’s incur long-term care costs as well as direct medical expenses. If you get the disease in your 60s or 70s, you might require expensive care for decades.&lt;/p&gt;
&lt;p&gt;These costs represent one of the fastest growing burdens on healthcare systems in developed countries. According to the Alzheimer’s Association, Americans will spend &lt;a href=&quot;https://www.alz.org/documents_custom/2017-facts-and-figures.pdf&quot; target=&quot;_blank&quot;&gt;$259 billion&lt;/a&gt; caring for those with Alzheimer’s and other dementias in 2017. Absent a major breakthrough, expenditures will continue to squeeze healthcare budgets in the years and decades to come. This is something that governments all over the world need to be thinking about, including in low- and middle-income countries where life expectancies are catching up to the global average and the number of people with dementia is on the rise.&lt;/p&gt;
&lt;img alt=&quot;A growing health crisis: The projected number of people with dementia from 2015 to 2050, millions&quot; src=&quot;https://www.gatesnotes.com/-/media/Images/Articles/Health/Digging-Deep-Into-Alzheimers/alzheimers_2017_inline_dementia-graph_800x600_v2.jpg?h=600&amp;amp;w=800&amp;amp;la=en&amp;amp;hash=2506D8E81D10B7E920CDB99C25AADA3A75903064&quot;/&gt;&lt;p&gt;The human cost of Alzheimer’s is much more difficult to put into numbers. It’s a terrible disease that devastates both those who have it and their loved ones. This is something I know a lot about, because men in my family have suffered from Alzheimer’s. I know how awful it is to watch people you love struggle as the disease robs them of their mental capacity, and there is nothing you can do about it. It feels a lot like you’re experiencing a gradual death of the person that you knew.&lt;/p&gt;
&lt;p&gt;My family history isn’t the sole reason behind my interest in Alzheimer’s. But my personal experience has exposed me to how hopeless it feels when you or a loved one gets the disease. We’ve seen scientific innovation turn once-guaranteed killers like HIV into chronic illnesses that can be held in check with medication. I believe we can do the same (or better) with Alzheimer’s.&lt;/p&gt;
&lt;p&gt;I’ve spent considerable time over the last year learning about the disease and the progress made to date. There’s a lot of amazing work being done in this field to delay Alzheimer’s and reduce its cognitive impact. What I’ve heard from researchers, academics, funders, and industry experts makes me hopeful that we can substantially alter the course of Alzheimer’s if we make progress in five areas:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;We need to better understand how Alzheimer’s unfolds.&lt;/strong&gt; The brain is a complicated organ. Because it’s so difficult to study while patients are alive, we know very little about how it ages normally and how Alzheimer’s disrupts that process. Our understanding of what happens in the brain is based largely on autopsies, which show only the late stages of the disease and don’t explain many of its lingering mysteries. For example, we don’t fully understand why &lt;a href=&quot;https://www.alz.org/documents_custom/2017-facts-and-figures.pdf&quot;&gt;you are more likely to get Alzheimer’s if you’re African American&lt;/a&gt; or &lt;a href=&quot;https://www.usagainstalzheimers.org/sites/default/files/Latinos-and-AD_USC_UsA2-Impact-Report.pdf&quot; target=&quot;_blank&quot;&gt;Latino&lt;/a&gt; than if you’re white. If we’re going to make progress, we need a better grasp on its underlying causes and biology.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;We need to detect and diagnose Alzheimer’s earlier.&lt;/strong&gt; Since the only way to diagnose Alzheimer’s definitively is through an autopsy after death, it’s difficult to identify the disease definitively early in its progression. Cognitive tests exist but often have a high variance. If you didn’t sleep well the night before, that might skew your results. A more reliable, affordable, and accessible diagnostic—such as a blood test—would make it easier to see how Alzheimer’s progresses and track how effective new drugs are.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;We need more approaches to stopping the disease.&lt;/strong&gt; There are many ways an Alzheimer’s drug might help prevent or slow down the disease. Most drug trials to date have targeted amyloid and tau, two proteins that cause plaques and tangles in the brain. I hope those approaches succeed, but we need to back scientists with different, less mainstream ideas in case they don’t. A more diverse drug pipeline increases our odds of discovering a breakthrough.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;We need to make it easier to get people enrolled in clinical trials.&lt;/strong&gt; The pace of innovation is partly determined by how quickly we can do clinical trials. Since we don’t yet have a good understanding of the disease or a reliable diagnostic, it’s difficult to find qualified people early enough in the disease’s progression willing to participate. It can sometimes take years to enroll enough patients. If we could develop a process to pre-qualify participants and create efficient registries, we could start new trials more quickly.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;We need to use data better.&lt;/strong&gt; Every time a pharmaceutical company or a research lab does a study, they gather lots of information. We should compile this data in a common form, so that we get a better sense of how the disease progresses, how that progression is determined by gender and age, and how genetics determines your likelihood of getting Alzheimer’s. This would make it easier for researchers to look for patterns and identify new pathways for treatment.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;By improving in each of these areas, I think we can develop an intervention that drastically reduces the impact of Alzheimer’s. There are plenty of reasons to be optimistic about our chances: our understanding of the brain and the disease is advancing a great deal. We’re already making progress—but we need to do more.&lt;/p&gt;
&lt;p&gt;I want to support the brilliant minds doing this work. As a first step, I’ve invested $50 million in the Dementia Discovery Fund—a private fund working to diversify the clinical pipeline and identify new targets for treatment. Most of the major pharmaceutical companies continue to pursue the amyloid and tau pathways. DDF complements their work by supporting startups as they explore less mainstream approaches to treating dementia.&lt;/p&gt;
&lt;p&gt;I’m making this investment on my own, not through the foundation. The first Alzheimer’s treatments might not come to fruition for another decade or more, and they will be very expensive at first. Once that day comes, our foundation might look at how we can expand access in poor countries.&lt;/p&gt;
&lt;p&gt;But before we can even begin to think about how we do that, we need lots of scientific breakthroughs. With all of the new tools and theories in development, I believe we are at a turning point in Alzheimer’s R&amp;amp;D. Now is the right time to accelerate that progress before the major costs hit countries that can’t afford high priced therapies and where exposure to the kind of budget implications of an Alzheimer’s epidemic could bankrupt health systems.&lt;/p&gt;
&lt;p&gt;This is a frontier where we can dramatically improve human life. It’s a miracle that people are living so much longer, but longer life expectancies alone are not enough. People should be able to enjoy their later years—and we need a breakthrough in Alzheimer’s to fulfill that. I’m excited to join the fight and can’t wait to see what happens next.&lt;/p&gt;
&lt;div class=&quot;InlineHTML_Holder&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;&lt;span class=&quot;InlineHTML_SU_Link&quot;&gt;If you want to receive updates on how I’m getting involved in the fight to stop Alzheimer’s disease,&lt;/span&gt; &lt;span class=&quot;InlineHTML_SU_LinkB&quot;&gt;sign up to become a Gates Notes Insider.&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;Alz_SU_Insert_Holder&quot;&gt;&lt;img src=&quot;https://www.gatesnotes.com/media/GNIN/Bill_990.jpg&quot; class=&quot;Alz_SU_Insert_Img&quot;/&gt;&lt;span class=&quot;Alz_SU_Insert_Title&quot;&gt;Get updates on the fight to stop Alzheimer’s.&lt;/span&gt; &lt;span class=&quot;Alz_SU_Insert_Description&quot;&gt;Become a Gates Notes Insider to stay up to date on how I’m getting involved. You’ll be able to access exclusive content, comment on stories, and more.&lt;/span&gt;&lt;/div&gt;
&lt;div id=&quot;PrevNextTabsHolder&quot; class=&quot;PrevNextTabsHolder&quot;&gt;
&lt;div id=&quot;PrevNextTabs_PrevTab&quot;&gt;

&lt;p&gt;Previous Article&lt;/p&gt;
&lt;/div&gt;

&lt;div id=&quot;PrevNextTabs_NextTab&quot;&gt;
&lt;p&gt;Next Article&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Mon, 13 Nov 2017 10:15:19 +0000</pubDate>
<dc:creator>tuxguy</dc:creator>
<og:description>Bill Gates shares his thoughts on Alzheimer’s disease and his hopes for accelerating progress to find a breakthrough.</og:description>
<og:image>https://www.gatesnotes.com/-/media/Images/Articles/Health/Digging-Deep-Into-Alzheimers/alzheimers_2017_article_1200px_v2.jpg</og:image>
<og:title>Why I’m Digging Deep Into Alzheimer’s</og:title>
<og:type>article</og:type>
<og:url>https://www.gatesnotes.com/Health/Digging-Deep-Into-Alzheimers</og:url>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.gatesnotes.com/Health/Digging-Deep-Into-Alzheimers?WT.mc_id=11_13_2017_00_Alzheimers_BG-EM_&amp;WT.tsrc=BGEM</dc:identifier>
</item>
<item>
<title>Target=&quot;_blank&quot; – An underestimated vulnerability (2016)</title>
<link>https://www.jitbit.com/alexblog/256-targetblank---the-most-underestimated-vulnerability-ever/</link>
<guid isPermaLink="true" >https://www.jitbit.com/alexblog/256-targetblank---the-most-underestimated-vulnerability-ever/</guid>
<description>&lt;h4 class=&quot;hidden-phone&quot;&gt;&lt;a href=&quot;https://www.jitbit.com/alexblog/&quot; title=&quot;Founder's blog&quot;&gt;Founder's blog&lt;/a&gt;&lt;/h4&gt;

&lt;p&gt;May 4 2016 :: by Alex&lt;/p&gt;
&lt;p&gt;People using &lt;code&gt;target='_blank'&lt;/code&gt; links usually have no idea about this curious fact:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The page we're linking to &lt;em&gt;gains partial access to the linking page&lt;/em&gt; via the &lt;code&gt;window.opener&lt;/code&gt; object.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The newly opened tab can, say, change the &lt;code&gt;window.opener.location&lt;/code&gt; to some phishing page. Or execute some JavaScript on the opener-page on your behalf... Users &lt;em&gt;trust&lt;/em&gt; the page that is already opened, they won't get suspicious.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example attack&lt;/strong&gt;: create a fake &quot;viral&quot; page with cute cat pictures, jokes or whatever, get it shared on Facebook (which is known for opening links via _blank) and every time someone clicks the link - execute&lt;/p&gt;
&lt;pre&gt;
window.opener.location = 'https://fakewebsite/facebook.com/PHISHING-PAGE.html';
&lt;/pre&gt;
…redirecting to a page that asks the user to re-enter her Facebook password.
&lt;h2&gt;How to fix&lt;/h2&gt;
&lt;p&gt;Add this to your outgoing links.&lt;/p&gt;
&lt;pre&gt;
rel=&quot;noopener&quot;
&lt;/pre&gt;
&lt;p&gt;Update: FF does not support &quot;noopener&quot; so add this.&lt;/p&gt;
&lt;pre&gt;
rel=&quot;noopener noreferrer&quot;
&lt;/pre&gt;
&lt;p&gt;Remember, that every time you open a new window via &lt;code&gt;window.open();&lt;/code&gt; you're also &quot;vulnerable&quot; to this, so always reset the &quot;opener&quot; property&lt;/p&gt;
&lt;pre&gt;
var newWnd = window.open();
newWnd.opener = null;
&lt;/pre&gt;
&lt;p&gt;PS. Interestingly, Google &lt;a href=&quot;https://sites.google.com/site/bughunteruniversity/nonvuln/phishing-with-window-opener&quot; rel=&quot;nofollow&quot;&gt;doesn't seem to care&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.jitbit.com/images/alex.jpg&quot; class=&quot;userpic&quot; width=&quot;50&quot; alt=&quot;'Target=&amp;quot;_blank&amp;quot; - the most underestimated vulnerability ever' was written by Alex&quot;/&gt; by &lt;strong&gt;Alex.&lt;/strong&gt; CEO, founder&lt;/p&gt;
&lt;br/&gt;</description>
<pubDate>Mon, 13 Nov 2017 09:29:04 +0000</pubDate>
<dc:creator>handpickednames</dc:creator>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.jitbit.com/alexblog/256-targetblank---the-most-underestimated-vulnerability-ever/</dc:identifier>
</item>
</channel>
</rss>