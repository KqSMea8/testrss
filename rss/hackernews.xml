<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="css/feed.xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:media="http://search.yahoo.com/mrss/" xmlns:og="http://ogp.me/ns#">
<channel>
<atom:link rel="self" href="http://192.168.1.4/fivefilters/makefulltextfeed.php?url=hnrss.org%2Fnewest%3Fpoints%3D200&amp;max=10&amp;links=preserve&amp;exc=" />
<atom:link rel="alternate" title="Source URL" href="http://hnrss.org/newest?points=200" />
<atom:link rel="related" title="Subscribe to feed" href="http://www.subtome.com/#/subscribe?feeds=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D&amp;back=http%3A%2F%2F192.168.1.4%2Ffivefilters%2Fmakefulltextfeed.php%3Furl%3Dhnrss.org%252Fnewest%253Fpoints%253D200%26max%3D10%26links%3Dpreserve%26exc%3D" />
<title>Hacker News: Newest</title>
<link>https://news.ycombinator.com/newest</link>
<description>Hacker News RSS</description>
<item>
<title>Texans say voting machines changing straight-ticket choices</title>
<link>https://apnews.com/a8825810d10441f2ad828e95d6851d55</link>
<guid isPermaLink="true" >https://apnews.com/a8825810d10441f2ad828e95d6851d55</guid>
<description>&lt;p&gt;AUSTIN, Texas (AP) — Some Texas voters are complaining that machines flipped their straight-ticket selections to the other party in key races during early voting, especially the much-watched Senate battle between Republican incumbent Ted Cruz and Democrat Beto O’Rourke.&lt;/p&gt;
&lt;p&gt;The secretary of state’s office said Friday that there have been reported issues with Hart eSlate voting machines, which are used in around 30 percent of counties statewide and feature a wheel for selecting candidates and buttons to move from screen to screen. But it says they are caused by voters themselves and often occur when they complete and submit ballots too quickly.&lt;/p&gt;

&lt;p&gt;“The Hart eSlate machines are not malfunctioning, the problems being reported are a result of user error — usually voters hitting a button or using the selection wheel before the screen is finished rendering,” said Sam Taylor, spokesman for the office of Secretary of State Rolando Pablos, who was appointed by Republican Gov. Greg Abbott.&lt;/p&gt;
&lt;p&gt;The machines are used in around 80 counties, including the state’s largest, Harris, which is home to Houston, as well as Travis, which includes Austin, and Tarrant, encompassing Fort Worth. Early voting in Texas began Monday and has featured strong turnout and long lines. It runs through Nov. 2, ahead of Election Day on Nov. 6.&lt;/p&gt;
&lt;p&gt;Many Hart eSlate machines used in Texas don’t provide receipts or other forms of paper trail to voters, but those casting ballots do see a screen that shows their choices before final submission — and can go back and make changes. Similar machines are used in parts of Indiana, Kentucky, Pennsylvania, Tennessee and Virginia, according to Verified Voting, a nonprofit group focused on ensuring the accuracy of elections.&lt;/p&gt;
&lt;p&gt;The machine’s manufacturer, Hart InterCivic, attributed the Texas issues to 16-year-old technology.&lt;/p&gt;
&lt;p&gt;“The same story has happened in multiple elections,” Steven Sockwell, the company’s vice president of marketing, said Friday. “There was no flipping then and there’s not any now.”&lt;/p&gt;
&lt;p&gt;Instead, Sockwell said, what typically happens in cases where someone believes his or her vote has been changed is a voter will select a straight-party ticket, then unintentionally change votes in individual races without realizing it.&lt;/p&gt;
&lt;p&gt;Still, in a statement to supporters, Cruz cited “multiple reports” of race selections changing and added “once you select the Republican party ticket, please be patient and do not select ‘next’ until the ballot has populated all of the selections.”&lt;/p&gt;

&lt;p&gt;An advisory to county clerks and elections administrators issued earlier this week by Keith Ingram, the secretary of state’s office’s director of elections, said, “We have heard from a number of people voting on Hart eSlate machines that when they voted straight ticket, it appeared to them that the machine had changed one or more of their selections to a candidate from a different party.”&lt;/p&gt;
&lt;p&gt;The Texas Democratic Party called the issue “a malfunction,” said it was causing Democrats to inadvertently vote for Cruz and accused the secretary of state’s office of not doing enough to warn voters of potential issues.&lt;/p&gt;
&lt;p&gt;Party chairman Gilberto Hinojosa said in a statement that “Texas’ Republican government blamed voters and did nothing.” He called for a statewide public service announcement to warn voters, training for poll workers on the issue and removal of “all malfunctioning machines.”&lt;/p&gt;
&lt;p&gt;Taylor said Friday that his office “has already trained election officials across the state” while also instructing “election administrators to post additional signage in multiple languages” and requiring county officials to keep “a detailed, meticulous log of any malfunctioning machines, and remove any machines that are malfunctioning.”&lt;/p&gt;
&lt;p&gt;Taylor also said his office “has no legal authority whatsoever to force any” voting machine vendors “to make upgrades if their voting systems are otherwise in compliance with federal and state law,” and that Hart eSlate’s system was certified in 2009. He said counties are responsible for purchasing their own new voting equipment.&lt;/p&gt;
&lt;p&gt;“We will continue to educate Texas voters using existing resources,” Taylor said, “and urge all Texans casting a ballot to take their time, slow down, and carefully review their ballot before casting one.”&lt;/p&gt;
&lt;p&gt;___&lt;/p&gt;
&lt;p&gt;Associated Press Writer Frank Bajak contributed to this report from Boston.&lt;/p&gt;
</description>
<pubDate>Fri, 26 Oct 2018 18:59:43 +0000</pubDate>
<dc:creator>threatofrain</dc:creator>
<og:title>Texans say voting machines changing straight-ticket choices</og:title>
<og:type>article</og:type>
<og:url>https://apnews.com/a8825810d10441f2ad828e95d6851d55</og:url>
<og:image>https://storage.googleapis.com/afs-prod/media/media:0a29fa8cc01c4cce8ae6a82b940548ad/3000.jpeg</og:image>
<og:description>AUSTIN, Texas (AP) — Some Texas voters are complaining that machines flipped their straight-ticket selections to the other party in key races during early voting, especially the much-watched Senate battle between Republican incumbent Ted Cruz and Democrat Beto O'Rourke. The secretary of state's office said Friday that there have been reported issues with Hart eSlate voting machines, which are used in around 30 percent of counties statewide and feature a wheel for selecting candidates and buttons to move from screen to screen. But it says they are caused by voters themselves and often occur when they complete and submit ballots too quickly.</og:description>
<dc:format>text/html</dc:format>
<dc:identifier>https://apnews.com/a8825810d10441f2ad828e95d6851d55</dc:identifier>
</item>
<item>
<title>One Windows Kernel</title>
<link>https://techcommunity.microsoft.com/t5/Windows-Kernel-Internals/One-Windows-Kernel/ba-p/267142</link>
<guid isPermaLink="true" >https://techcommunity.microsoft.com/t5/Windows-Kernel-Internals/One-Windows-Kernel/ba-p/267142</guid>
<description>&lt;p&gt;Windows is one of the most versatile and flexible operating systems out there, running on a variety of machine architectures and available in multiple SKUs. It currently supports x86, x64, ARM and ARM64 architectures. Windows used to support Itanium, PowerPC, DEC Alpha, and MIPS (&lt;span&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Windows_NT&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;wiki entry&lt;/a&gt;&lt;/span&gt;). In addition, Windows supports a variety of SKUs that run in a multitude of environments; from data centers, laptops, Xbox, phones to embedded IOT devices such as ATM machines.&lt;/p&gt;

&lt;p&gt;The most amazing aspect of all this is that the core of Windows, its kernel, remains virtually unchanged on all these architectures and SKUs. The Windows kernel scales dynamically depending on the architecture and the processor that it’s run on to exploit the full power of the hardware. There is of course some architecture specific code in the Windows kernel, however this is kept to a minimum to allow Windows to run on a variety of architectures.&lt;/p&gt;

&lt;p&gt;In this blog post, I will talk about the evolution of the core pieces of the Windows kernel that allows it to transparently scale across a low power NVidia Tegra chip on the &lt;span&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Surface_2&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Surface RT&lt;/a&gt;&lt;/span&gt; from 2012, to the giant &lt;span&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;behemoths&lt;/a&gt;&lt;/span&gt; that power Azure data centers today.&lt;/p&gt;

&lt;p&gt;This is a picture of Windows taskmgr running on a pre-release Windows DataCenter class machine with 896 cores supporting 1792 logical processors and 2TB of RAM!&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;lia-inline-image-display-wrapper lia-image-align-inline&quot;&gt;&lt;img src=&quot;https://gxcuf89792.i.lithium.com/t5/image/serverpage/image-id/55551i89F6A2C912C5C448/image-size/large?v=1.0&amp;amp;px=999&quot; alt=&quot;TaskMgr.png&quot; title=&quot;TaskMgr.png&quot; li-image-url=&quot;https://gxcuf89792.i.lithium.com/t5/image/serverpage/image-id/55551i89F6A2C912C5C448?v=1.0&quot; li-image-display-id=&quot;'55551i89F6A2C912C5C448'&quot; li-message-uid=&quot;'267142'&quot; li-messages-message-image=&quot;true&quot; li-bindable=&quot;&quot; class=&quot;lia-media-image&quot; tabindex=&quot;0&quot; li-bypass-lightbox-when-linked=&quot;true&quot; li-use-hover-links=&quot;false&quot;/&gt;&lt;span class=&quot;lia-inline-image-caption&quot; onclick=&quot;event.preventDefault();&quot;&gt;Task Manager showing 1792 logical processors&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Before we talk about the details of the Windows kernel, I am going to take a small detour to talk about something called Windows refactoring. Windows refactoring plays a key part in increasing the reuse of Windows components across different SKUs, and platforms (e.g. client, server and phone). The basic idea of Windows refactoring is to allow the same DLL to be reused in different SKUs but support minor modifications tailored to the SKU without renaming the DLL and breaking apps.&lt;/p&gt;

&lt;p&gt;The base technology used for Windows refactoring are a lightly documented technology (entirely by design) called &lt;span&gt;&lt;a href=&quot;http://msdn.microsoft.com/en-us/library/windows/desktop/Hh802935(v=vs.85).aspx&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;API sets&lt;/a&gt;&lt;/span&gt;. API sets are a mechanism that allows Windows to decouple the DLL from where its implementation is located. For example, API sets allow win32 apps to continue to use kernel32.dll but, the implementation of all the APIs are in a different DLL. These implementation DLLs can also be different depending on your SKU. You can see API sets in action if you launch dependency walker on a traditional Windows DLL; e.g. kernel32.dll.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;lia-inline-image-display-wrapper lia-image-align-inline&quot;&gt;&lt;img src=&quot;https://gxcuf89792.i.lithium.com/t5/image/serverpage/image-id/55556i8BF228D9318A85CC/image-size/large?v=1.0&amp;amp;px=999&quot; alt=&quot;depends.png&quot; title=&quot;depends.png&quot; li-image-url=&quot;https://gxcuf89792.i.lithium.com/t5/image/serverpage/image-id/55556i8BF228D9318A85CC?v=1.0&quot; li-image-display-id=&quot;'55556i8BF228D9318A85CC'&quot; li-message-uid=&quot;'267142'&quot; li-messages-message-image=&quot;true&quot; li-bindable=&quot;&quot; class=&quot;lia-media-image&quot; tabindex=&quot;0&quot; li-bypass-lightbox-when-linked=&quot;true&quot; li-use-hover-links=&quot;false&quot;/&gt;&lt;span class=&quot;lia-inline-image-caption&quot; onclick=&quot;event.preventDefault();&quot;&gt;Dependency walker&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With that detour into how Windows is built to maximize code reuse and sharing, let’s go into the technical depths of the kernel starting with the scheduler which is key to the scaling of Windows.&lt;/p&gt;


&lt;p&gt;Windows NT is like a &lt;a href=&quot;https://en.wikipedia.org/wiki/Microkernel&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;microkernel&lt;/a&gt; in the sense that it has a core Kernel (KE) that does very little and uses the Executive layer (Ex) to perform all the higher-level policy. Note that EX is still kernel mode, so it's not a true microkernel. The kernel is responsible for thread dispatching, multiprocessor synchronization, hardware exception handling, and the implementation of low-level machine dependent functions. The EX layer contains various subsystems which provide the bulk of the functionality traditionally thought of as kernel such as IO, Object Manager, Memory Manager, Process Subsystem, etc.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;lia-inline-image-display-wrapper lia-image-align-inline&quot;&gt;&lt;img src=&quot;https://gxcuf89792.i.lithium.com/t5/image/serverpage/image-id/55558i85CBB1B2E72B3E88/image-size/large?v=1.0&amp;amp;px=999&quot; alt=&quot;arch.png&quot; title=&quot;arch.png&quot; li-image-url=&quot;https://gxcuf89792.i.lithium.com/t5/image/serverpage/image-id/55558i85CBB1B2E72B3E88?v=1.0&quot; li-image-display-id=&quot;'55558i85CBB1B2E72B3E88'&quot; li-message-uid=&quot;'267142'&quot; li-messages-message-image=&quot;true&quot; li-bindable=&quot;&quot; class=&quot;lia-media-image&quot; tabindex=&quot;0&quot; li-bypass-lightbox-when-linked=&quot;true&quot; li-use-hover-links=&quot;false&quot;/&gt;&lt;/span&gt; &lt;/p&gt;

&lt;p&gt;To get a better idea of the size of the components, here is a rough breakdown on the number of lines of code in a few key directories in the Windows kernel source tree (counting comments). There is a lot more to the Kernel not shown in this table. &lt;/p&gt;

&lt;table width=&quot;395&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td width=&quot;171&quot;&gt;
&lt;p&gt;&lt;strong&gt;Kernel subsystems&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;223&quot;&gt;
&lt;p&gt;&lt;strong&gt;Lines of code&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;171&quot;&gt;
&lt;p&gt;&lt;strong&gt;Memory Manager&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;223&quot;&gt;
&lt;p&gt;501, 000&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;171&quot;&gt;
&lt;p&gt;&lt;strong&gt;Registry&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;223&quot;&gt;
&lt;p&gt;211,000&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;171&quot;&gt;
&lt;p&gt;&lt;strong&gt;Power&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;223&quot;&gt;
&lt;p&gt;238,000&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;171&quot;&gt;
&lt;p&gt;&lt;strong&gt;Executive&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;223&quot;&gt;
&lt;p&gt;157,000&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;171&quot;&gt;
&lt;p&gt;&lt;strong&gt;Security&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;223&quot;&gt;
&lt;p&gt;135,000&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;171&quot;&gt;
&lt;p&gt;&lt;strong&gt;Kernel&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;223&quot;&gt;
&lt;p&gt;339,000&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;&lt;td width=&quot;171&quot;&gt;
&lt;p&gt;&lt;strong&gt;Process sub-system&lt;/strong&gt;&lt;/p&gt;
&lt;/td&gt;
&lt;td width=&quot;223&quot;&gt;
&lt;p&gt;116,000&lt;/p&gt;
&lt;/td&gt;
&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
&lt;p&gt;For more information on the architecture of Windows, the “&lt;a href=&quot;https://docs.microsoft.com/en-us/sysinternals/learn/windows-internals&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Windows Internals&lt;/a&gt;” series of books are a good reference.&lt;/p&gt;


&lt;p&gt;With that background, let's talk a little bit about the scheduler, its evolution and how Windows kernel can scale across so many different architectures with so many processors.&lt;/p&gt;

&lt;p&gt;A thread is the basic unit that runs program code and it is this unit that is scheduled by the Window scheduler. The Windows scheduler uses the thread priority to decide which thread to run and in theory the highest priority thread on the system always gets to run even if that entails preempting a lower priority thread.&lt;/p&gt;

&lt;p&gt;As a thread runs and experiences quantum end (minimum amount of time a thread gets to run), its dynamic priority decays, so that a high priority CPU bound thread doesn’t run forever starving everyone else. When another waiting thread is awakened to run, it is given a priority boost based on the importance of the event that caused the wait to be satisfied (e.g. a large boost is for a foreground UI thread vs. a smaller one for completing disk I/O). A thread therefore runs at a high priority as long as it’s interactive. When it becomes CPU (compute) bound, its priority decays, and it is considered only after other, higher priority threads get their time on the CPU. In addition, the kernel arbitrarily boosts the priority of ready threads that haven't received any processor time for a given period of time to prevent starvation and correct priority inversions.&lt;/p&gt;

&lt;p&gt;The Windows scheduler initially had a single ready queue from where it picked up the next highest priority thread to run on the processor. However, as Windows started supporting more and more processors the single ready queue turned out to be a bottleneck and around &lt;strong&gt;Windows Server 2003,&lt;/strong&gt; the scheduler changed to one ready queue per processor. As Windows moved to multiple per processor queues, it avoided having a single global lock protecting all the queues and allowed the scheduler to make locally optimum decisions. This means that any point the single highest priority thread in the system runs but that doesn’t necessarily mean that the top N (N is number of cores) priority threads on the system are running. This proved to be good enough until Windows started moving to low power CPUs, e.g. in laptops and tablets. On these systems, not running a high priority thread (such as the foreground UI thread) caused the system to have noticeable glitches in UI. And so, in &lt;strong&gt;Windows 8.1&lt;/strong&gt;, the scheduler changed to a hybrid model with per processor ready queues for affinitized (tied to a processor) work and shared ready queues between processors. This did not cause a noticeable impact on performance because of other architectural changes in the scheduler such as the dispatcher database lock refactoring which we will talk about later.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Windows 7&lt;/strong&gt; introduced something called the Dynamic Fair Share Scheduler; this feature was introduced primarily for terminal servers. The problem that this feature tried to solve was that one terminal server session which had a CPU intensive workload could impact the threads in other terminal server sessions. Since the scheduler didn’t consider sessions and simply used the priority as the key to schedule threads, users in different sessions could impact the user experience of others by starving their threads. It also unfairly advantages the sessions (users) who has a lot of threads because the sessions with more threads get more opportunity to be scheduled and received CPU time. This feature tried to add policy to the scheduler such that each session was treated fairly and roughly the same amount of CPU was available to each session. &lt;span&gt;Similar functionality is available in Linux as well, with its &lt;a href=&quot;https://en.wikipedia.org/wiki/Completely_Fair_Scheduler&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Completely Fair Scheduler&lt;/a&gt;&lt;/span&gt;. In &lt;strong&gt;Windows 8&lt;/strong&gt;, this concept was generalized as a scheduler group and added to the Windows Scheduler with each session in an independent scheduler group. In addition to the thread priority, the scheduler uses the scheduler groups as a second level index to decide which thread should run next. In a terminal server, all the scheduler groups are weighted equally and hence all sessions (scheduler groups) receive the same amount of CPU regardless of the number or priorities of the threads in the scheduler groups. In addition to its utility in a terminal server session, scheduler groups are also used to have fine grained control on a process at runtime. In Windows 8, &lt;strong&gt;Job objects&lt;/strong&gt; were enhanced to support &lt;span&gt;&lt;a href=&quot;http://msdn.microsoft.com/en-us/library/windows/desktop/hh448384(v=vs.85).aspx&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;CPU rate control&lt;/a&gt;&lt;/span&gt;. Using the CPU rate control APIs, one can decide how much CPU a process can use, whether it should be a hard cap or a soft cap and receive notifications when a process meets those CPU limits. This is like the resource controls features available in &lt;span&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Cgroups&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;cgroups&lt;/a&gt;&lt;/span&gt; on Linux.&lt;/p&gt;

&lt;p&gt;Starting with &lt;strong&gt;Windows 7&lt;/strong&gt;, Windows Server started supporting greater than &lt;span&gt;&lt;a href=&quot;http://msdn.microsoft.com/en-us/library/windows/hardware/gg463349.aspx&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;64 logical processors&lt;/a&gt;&lt;/span&gt; in a single machine. To add support for so many processors, Windows internally introduced a new entity called “processor group”. A group is a static set of up to 64 logical processors that is treated as a single scheduling entity.  The kernel determines at boot time which processor belongs to which group and for machines with less than 64 cores, with the overhead of the group structure indirection is mostly not noticeable. While a single process can span groups (such as a SQL server instance), and individual thread could only execute within a single scheduling group at a time.&lt;/p&gt;

&lt;p&gt;However, on machines with greater than 64 cores, Windows started showing some bottlenecks that prevented high performance applications such as SQL server from scaling their performance linearly with the number of processor cores. Thus, even if you added more cores and memory, the benchmarks wouldn’t show much increase in performance. And one of the main problems that caused this lack of performance was the contention around the Dispatcher database lock. The dispatcher database lock protected access to those objects that needed to be dispatched; i.e. scheduled. Examples of objects that were protected by this lock included threads, timers, I/O completion ports, and other waitable kernel objects (events, semaphores, mutants, etc.). Thus, in Windows 7 due to the impetus provided by the greater than 64 processor support, work was done to eliminate the dispatcher database lock and replace it with fine grained locks such as per object locks. This allowed benchmarks such as the SQL &lt;span&gt;&lt;a href=&quot;http://www.tpc.org/tpcc/&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;TPC-C&lt;/a&gt;&lt;/span&gt; to show a &lt;strong&gt;290%&lt;/strong&gt; improvement when compared to Windows 7 with a dispatcher database lock on certain machine configurations. This was one of the biggest performance boosts seen in Windows history, due to a single feature.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Windows 10&lt;/strong&gt; brought us another innovation in the scheduler space with &lt;span&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/windows/desktop/ProcThread/cpu-sets&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;CPU Sets&lt;/a&gt;&lt;/span&gt;. CPU Sets allow a process to partition the system such that its process can take over a group of processors and not allow any other process or system to run their threads on those processors. Windows Kernel even steers Interrupts from devices away from the processors that are part of your CPU set. This ensures that even devices cannot target their code on the processors which have been partitioned off by CPU sets for your app or process. Think of this as a low-tech Virtual Machine. As you can imagine this is a powerful capability and hence there are a lot of safeguards built-in to prevent an app developer from making the wrong choice within the API. CPU sets functionality are used by the customer when they use &lt;span&gt;&lt;a href=&quot;https://www.windowscentral.com/windows-10-game-mode&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;Game Mode&lt;/a&gt;&lt;/span&gt; to run their games.&lt;/p&gt;

&lt;p&gt;Finally, this brings us to &lt;strong&gt;ARM64&lt;/strong&gt; support with &lt;span&gt;&lt;a href=&quot;https://channel9.msdn.com/Events/Build/2018/BRK2438&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Windows 10 on ARM&lt;/a&gt;&lt;/span&gt;.  The ARM architecture supports a &lt;span&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/ARM_big.LITTLE&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener noreferrer&quot;&gt;big.LITTLE&lt;/a&gt;&lt;/span&gt; architecture, big.LITTLE is a heterogenous architecture where the “big” core runs fast, consuming more power and the “LITTLE” core runs slow consuming less power. The idea here is that you run unimportant tasks on the little core saving battery. To support big.LITTLE architecture and provide great battery life on Windows 10 on ARM, the Windows scheduler added support for &lt;strong&gt;heterogenous scheduling&lt;/strong&gt; which took into account the app intent for scheduling on big.LITTLE architectures.&lt;/p&gt;

&lt;p&gt;By app intent, I mean Windows tries to provide a quality of service for apps by tracking threads which are running in the foreground (or starved of CPU) and ensuring those threads always run on the big core. Whereas the background tasks, services, and other ancillary threads in the system run on the little cores. (As an aside, you can also &lt;span&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/windows/desktop/api/processthreadsapi/nf-processthreadsapi-setthreadinformation&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;programmatically&lt;/a&gt;&lt;/span&gt; mark your thread as unimportant which will make it run on the LITTLE core.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Work on Behalf:&lt;/strong&gt; In Windows, a lot of work for the foreground is done by other services running in the background. E.g. In Outlook, when you search for a mail, the search is conducted by a background service (Indexer). If we simply, run all the services on the little core, then the experience and performance of the foreground app will be affected. To ensure, that these scenarios are not slow on big.LITTLE architectures, Windows actually tracks when an app calls into another process to do work on its behalf. When this happens, we donate the foreground priority to the service thread and force run the thread in the service on the big core.&lt;/p&gt;

&lt;p&gt;That concludes our first (huge?) One Windows Kernel post, giving you an overview of the Windows Kernel Scheduler. We will have more similarly technical posts about the internals of the Windows Kernel. &lt;/p&gt;

&lt;p&gt;Hari Pulapaka&lt;/p&gt;
&lt;p&gt;(Windows Kernel Team)&lt;/p&gt;
</description>
<pubDate>Fri, 26 Oct 2018 15:18:47 +0000</pubDate>
<dc:creator>MikusR</dc:creator>
<og:image>https://gxcuf89792.i.lithium.com/t5/image/serverpage/image-id/55551i89F6A2C912C5C448?v=1.0</og:image>
<og:type>article</og:type>
<og:description>Windows is one of the most versatile and flexible operating systems out there, running on a variety of machine architectures and available in multiple SKUs. It currently supports x86, x64, ARM and ARM64 architectures. Windows used to support Itanium, PowerPC, DEC Alpha, and MIPS (wiki entry). In add...</og:description>
<og:title>One Windows Kernel</og:title>
<og:url>https://techcommunity.microsoft.com/t5/Windows-Kernel-Internals/One-Windows-Kernel/ba-p/267142</og:url>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://techcommunity.microsoft.com/t5/Windows-Kernel-Internals/One-Windows-Kernel/ba-p/267142</dc:identifier>
</item>
<item>
<title>The Architecture of Git (2012)</title>
<link>http://aosabook.org/en/git.html</link>
<guid isPermaLink="true" >http://aosabook.org/en/git.html</guid>
<description>&lt;section readability=&quot;11&quot;&gt;&lt;h2&gt;6.1. Git in a Nutshell&lt;/h2&gt;
&lt;p&gt;Git enables the maintenance of a digital body of work (often, but not limited to, code) by many collaborators using a peer-to-peer network of repositories. It supports distributed workflows, allowing a body of work to either eventually converge or temporarily diverge.&lt;/p&gt;
&lt;p&gt;This chapter will show how various aspects of Git work under the covers to enable this, and how it differs from other version control systems (VCSs).&lt;/p&gt;
&lt;/section&gt;&lt;section readability=&quot;72.829538820782&quot;&gt;&lt;h2&gt;6.2. Git's Origin&lt;/h2&gt;
&lt;p&gt;To understand Git's design philosophy better it is helpful to understand the circumstances in which the Git project was started in the Linux Kernel Community.&lt;/p&gt;
&lt;p&gt;The Linux kernel was unusual, compared to most commercial software projects at that time, because of the large number of committers and the high variance of contributor involvement and knowledge of the existing codebase. The kernel had been maintained via tarballs and patches for years, and the core development community struggled to find a VCS that satisfied most of their needs.&lt;/p&gt;
&lt;p&gt;Git is an open source project that was born out of those needs and frustrations in 2005. At that time the Linux kernel codebase was managed across two VCSs, BitKeeper and CVS, by different core developers. BitKeeper offered a different view of VCS history lineage than that offered by the popular open source VCSs at this time.&lt;/p&gt;
&lt;p&gt;Days after BitMover, the maker of BitKeeper, announced it would revoke the licenses of some core Linux kernel developers, Linus Torvalds began development, in haste, of what was to become Git. He began by writing a collection of scripts to help him manage email patches to apply one after the other. The aim of this initial collection of scripts was to be able to abort merges quickly so the maintainer could modify the codebase mid-patch-stream to manually merge, then continue merging subsequent patches.&lt;/p&gt;
&lt;p&gt;From the outset, Torvalds had one philosophical goal for Git—to be the anti-CVS—plus three usability design goals:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Support distributed workflows similar to those enabled by BitKeeper&lt;/li&gt;
&lt;li&gt;Offer safeguards against content corruption&lt;/li&gt;
&lt;li&gt;Offer high performance&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;These design goals have been accomplished and maintained, to a degree, as I will attempt to show by dissecting Git's use of directed acyclic graphs (DAGs) for content storage, reference pointers for heads, object model representation, and remote protocol; and finally how Git tracks the merging of trees.&lt;/p&gt;
&lt;p&gt;Despite BitKeeper influencing the original design of Git, it is implemented in fundamentally different ways and allows even more distributed plus local-only workflows, which were not possible with BitKeeper. &lt;a href=&quot;http://www.monotone.ca/&quot;&gt;Monotone&lt;/a&gt;, an open source distributed VCS started in 2003, was likely another inspiration during Git's early development.&lt;/p&gt;
&lt;p&gt;Distributed version control systems offer great workflow flexibility, often at the expense of simplicity. Specific benefits of a distributed model include:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Providing the ability for collaborators to work offline and commit incrementally.&lt;/li&gt;
&lt;li&gt;Allowing a collaborator to determine when his/her work is ready to share.&lt;/li&gt;
&lt;li&gt;Offering the collaborator access to the repository history when offline.&lt;/li&gt;
&lt;li&gt;Allowing the managed work to be published to multiple repositories, potentially with different branches or granularity of changes visible.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Around the time the Git project started, three other open source distributed VCS projects were initiated. (One of them, Mercurial, is discussed in Volume 1 of &lt;em&gt;The Architecture of Open Source Applications&lt;/em&gt;.) All of these dVCS tools offer slightly different ways to enable highly flexible workflows, which centralized VCSs before them were not capable of handling directly. Note: Subversion has an extension named SVK maintained by different developers to support server-to-server synchronization.&lt;/p&gt;
&lt;p&gt;Today popular and actively maintained open source dVCS projects include Bazaar, Darcs, Fossil, Git, Mercurial, and Veracity.&lt;/p&gt;
&lt;/section&gt;&lt;section readability=&quot;75.5&quot;&gt;&lt;h2&gt;6.3. Version Control System Design&lt;/h2&gt;
&lt;p&gt;Now is a good time to take a step back and look at the alternative VCS solutions to Git. Understanding their differences will allow us to explore the architectural choices faced while developing Git.&lt;/p&gt;
&lt;p&gt;A version control system usually has three core functional requirements, namely:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Storing content&lt;/li&gt;
&lt;li&gt;Tracking changes to the content (history including merge metadata)&lt;/li&gt;
&lt;li&gt;Distributing the content and history with collaborators&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Note: The third requirement above is not a functional requirement for all VCSs.&lt;/p&gt;
&lt;section readability=&quot;10&quot;&gt;&lt;h3&gt;Content Storage&lt;/h3&gt;
&lt;p&gt;The most common design choices for storing content in the VCS world are with a delta-based changeset, or with directed acyclic graph (DAG) content representation.&lt;/p&gt;
&lt;p&gt;Delta-based changesets encapsulate the differences between two versions of the flattened content, plus some metadata. Representing content as a directed acyclic graph involves objects forming a hierarchy which mirrors the content's filesystem tree as a snapshot of the commit (reusing the unchanged objects inside the tree where possible). Git stores content as a directed acyclic graph using different types of objects. The &quot;Object Database&quot; section later in this chapter describes the different types of objects that can form DAGs inside the Git repository.&lt;/p&gt;
&lt;/section&gt;&lt;section readability=&quot;20&quot;&gt;&lt;h3&gt;Commit and Merge Histories&lt;/h3&gt;
On the history and change-tracking front most VCS software uses one of the following approaches:
&lt;ul&gt;&lt;li&gt;Linear history&lt;/li&gt;
&lt;li&gt;Directed acyclic graph for history&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Again Git uses a DAG, this time to store its history. Each commit contains metadata about its ancestors; a commit in Git can have zero or many (theoretically unlimited) parent commits. For example, the first commit in a Git repository would have zero parents, while the result of a three-way merge would have three parents.&lt;/p&gt;
&lt;p&gt;Another primary difference between Git and Subversion and its linear history ancestors is its ability to directly support branching that will record most merge history cases.&lt;/p&gt;
&lt;img src=&quot;http://aosabook.org/images/git/dag-example.png&quot;/&gt; Figure 6.1: Example of a DAG representation in Git
&lt;p&gt;Git enables full branching capability using directed acyclic graphs to store content. The history of a file is linked all the way up its directory structure (via nodes representing directories) to the root directory, which is then linked to a commit node. This commit node, in turn, can have one or more parents. This affords Git two properties that allow us to reason about history and content in more definite ways than the family of VCSs derived from RCS do, namely:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;When a content (i.e., file or directory) node in the graph has the same reference identity (the SHA in Git) as that in a different commit, the two nodes are guaranteed to contain the same content, allowing Git to short-circuit content diffing efficiently.&lt;/li&gt;
&lt;li&gt;When merging two branches we are merging the content of two nodes in a DAG. The DAG allows Git to &quot;efficiently&quot; (as compared to the RCS family of VCS) determine common ancestors.&lt;/li&gt;
&lt;/ul&gt;&lt;/section&gt;&lt;section readability=&quot;3&quot;&gt;&lt;h3&gt;Distribution&lt;/h3&gt;
&lt;p&gt;VCS solutions have handled content distribution of a working copy to collaborators on a project in one of three ways:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Local-only: for VCS solutions that do not have the third functional requirement above.&lt;/li&gt;
&lt;li&gt;Central server: where all changes to the repository must transact via one specific repository for it to be recorded in history at all.&lt;/li&gt;
&lt;li&gt;Distributed model: where there will often be publicly accessible repositories for collaborators to &quot;push&quot; to, but commits can be made locally and pushed to these public nodes later, allowing offline work.&lt;/li&gt;
&lt;/ul&gt;&lt;/section&gt;&lt;p&gt;To demonstrate the benefits and limitations of each major design choice, we will consider a Subversion repository and a Git repository (on a server), with equivalent content (i.e., the HEAD of the default branch in the Git repository has the same content as the Subversion repository's latest revision on trunk). A developer, named Alex, has a local checkout of the Subversion repository and a local clone of the Git repository.&lt;/p&gt;
&lt;p&gt;Let us say Alex makes a change to a 1 MB file in the local Subversion checkout, then commits the change. Locally, the checkout of the file mimics the latest change and local metadata is updated. During Alex's commit in the centralized Subversion repository, a diff is generated between the previous snapshot of the files and the new changes, and this diff is stored in the repository.&lt;/p&gt;
&lt;p&gt;Contrast this with the way Git works. When Alex makes the same modification to the equivalent file in the local Git clone, the change will be recorded locally first, then Alex can &quot;push&quot; the local pending commits to a public repository so the work can be shared with other collaborators on the project. The content changes are stored identically for each Git repository that the commit exists in. Upon the local commit (the simplest case), the local Git repository will create a new object representing a file for the changed file (with all its content inside). For each directory above the changed file (plus the repository root directory), a new tree object is created with a new identifier. A DAG is created starting from the newly created root tree object pointing to blobs (reusing existing blob references where the files content has not changed in this commit) and referencing the newly created blob in place of that file's previous blob object in the previous tree hierarchy. (A &lt;em&gt;blob&lt;/em&gt; represents a file stored in the repository.)&lt;/p&gt;
&lt;p&gt;At this point the commit is still local to the current Git clone on Alex's local device. When Alex &quot;pushes&quot; the commit to a publicly accessible Git repository this commit gets sent to that repository. After the public repository verifies that the commit can apply to the branch, the same objects are stored in the public repository as were originally created in the local Git repository.&lt;/p&gt;
&lt;p&gt;There are a lot more moving parts in the Git scenario, both under the covers and for the user, requiring them to explicitly express intent to share changes with the remote repository separately from tracking the change as a commit locally. However, both levels of added complexity offer the team greater flexibility in terms of their workflow and publishing capabilities, as described in the &quot;Git's Origin&quot; section above.&lt;/p&gt;
&lt;p&gt;In the Subversion scenario, the collaborator did not have to remember to push to the public remote repository when ready for others to view the changes made. When a small modification to a larger file is sent to the central Subversion repository the delta stored is much more efficient than storing the complete file contents for each version. However, as we will see later, there is a workaround for this that Git takes advantage of in certain scenarios.&lt;/p&gt;
&lt;/section&gt;&lt;section readability=&quot;28&quot;&gt;&lt;h2&gt;6.4. The Toolkit&lt;/h2&gt;
&lt;p&gt;Today the Git ecosystem includes many command-line and UI tools on a number of operating systems (including Windows, which was originally barely supported). Most of these tools are mostly built on top of the Git core toolkit.&lt;/p&gt;
&lt;p&gt;Due to the way Git was originally written by Linus, and its inception within the Linux community, it was written with a toolkit design philosophy very much in the Unix tradition of command line tools.&lt;/p&gt;
&lt;p&gt;The Git toolkit is divided into two parts: the plumbing and the porcelain. The plumbing consists of low-level commands that enable basic content tracking and the manipulation of directed acyclic graphs (DAG). The porcelain is the smaller subset of &lt;code&gt;git&lt;/code&gt; commands that most Git end users are likely to need to use for maintaining repositories and communicating between repositories for collaboration.&lt;/p&gt;
&lt;p&gt;While the toolkit design has provided enough commands to offer fine-grained access to functionality for many scripters, application developers complained about the lack of a linkable library for Git. Since the Git binary calls &lt;code&gt;die()&lt;/code&gt;, it is not reentrant and GUIs, web interfaces or longer running services would have to fork/exec a call to the Git binary, which can be slow.&lt;/p&gt;
&lt;p&gt;Work is being done to improve the situation for application developers; see the &quot;Current And Future Work&quot; section for more information.&lt;/p&gt;
&lt;/section&gt;&lt;section readability=&quot;79&quot;&gt;&lt;h2&gt;6.5. The Repository, Index and Working Areas&lt;/h2&gt;
&lt;p&gt;Let's get our hands dirty and dive into using Git locally, if only to understand a few fundamental concepts.&lt;/p&gt;
&lt;p&gt;First to create a new initialized Git repository on our local filesystem (using a Unix inspired operating system) we can do:&lt;/p&gt;
&lt;pre&gt;
  $ mkdir testgit
  $ cd testgit
  $ git init
&lt;/pre&gt;
&lt;p&gt;Now we have an empty, but initialized, Git repository sitting in our testgit directory. We can branch, commit, tag and even communicate with other local and remote Git repositories. Even communication with other types of VCS repositories is possible with just a handful of &lt;code&gt;git&lt;/code&gt; commands.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;git init&lt;/code&gt; command creates a .git subdirectory inside of testgit. Let's have a peek inside it:&lt;/p&gt;
&lt;pre&gt;
tree .git/
.git/
|-- HEAD
|-- config
|-- description
|-- hooks
|   |-- applypatch-msg.sample
|   |-- commit-msg.sample
|   |-- post-commit.sample
|   |-- post-receive.sample
|   |-- post-update.sample
|   |-- pre-applypatch.sample
|   |-- pre-commit.sample
|   |-- pre-rebase.sample
|   |-- prepare-commit-msg.sample
|   |-- update.sample
|-- info
|   |-- exclude
|-- objects
|   |-- info
|   |-- pack
|-- refs
    |-- heads
    |-- tags
&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;.git&lt;/code&gt; directory above is, by default, a subdirectory of the root working directory, &lt;code&gt;testgit&lt;/code&gt;. It contains a few different types of files and directories:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;em&gt;Configuration&lt;/em&gt;: the &lt;code&gt;.git/config&lt;/code&gt;, &lt;code&gt;.git/description&lt;/code&gt; and &lt;code&gt;.git/info/exclude&lt;/code&gt; files essentially help configure the local repository.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Hooks&lt;/em&gt;: the &lt;code&gt;.git/hooks&lt;/code&gt; directory contains scripts that can be run on certain lifecycle events of the repository.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Staging Area&lt;/em&gt;: the &lt;code&gt;.git/index&lt;/code&gt; file (which is not yet present in our tree listing above) will provide a staging area for our working directory.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Object Database&lt;/em&gt;: the &lt;code&gt;.git/objects&lt;/code&gt; directory is the default Git object database, which contains all content or pointers to local content. All objects are immutable once created.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;References&lt;/em&gt;: the &lt;code&gt;.git/refs&lt;/code&gt; directory is the default location for storing reference pointers for both local and remote branches, tags and heads. A reference is a pointer to an object, usually of type &lt;code&gt;tag&lt;/code&gt; or &lt;code&gt;commit&lt;/code&gt;. References are managed outside of the Object Database to allow the references to change where they point to as the repository evolves. Special cases of references may point to other references, e.g. &lt;code&gt;HEAD&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The &lt;code&gt;.git&lt;/code&gt; directory is the actual repository. The directory that contains the working set of files is the &lt;em&gt;working directory&lt;/em&gt;, which is typically the parent of the &lt;code&gt;.git&lt;/code&gt; directory (or &lt;em&gt;repository&lt;/em&gt;). If you were creating a Git remote repository that would not have a working directory, you could initialize it using the &lt;code&gt;git init --bare&lt;/code&gt; command. This would create just the pared-down repository files at the root, instead of creating the repository as a subdirectory under the working tree.&lt;/p&gt;
&lt;p&gt;Another file of great importance is the &lt;em&gt;Git index&lt;/em&gt;: &lt;code&gt;.git/index&lt;/code&gt;. It provides the staging area between the local working directory and the local repository. The index is used to stage specific changes within one file (or more), to be committed all together. Even if you make changes related to various types of features, the commits can be made with like changes together, to more logically describe them in the commit message. To selectively stage specific changes in a file or set of files you can using &lt;code&gt;git add -p&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The Git index, by default, is stored as a single file inside the repository directory. The paths to these three areas can be customized using environment variables.&lt;/p&gt;
&lt;p&gt;It is helpful to understand the interactions that take place between these three areas (the repository, index and working areas) during the execution of a few core Git commands:&lt;/p&gt;
&lt;ul readability=&quot;1.5&quot;&gt;&lt;li readability=&quot;2&quot;&gt;&lt;code&gt;git checkout [branch]&lt;/code&gt;
&lt;p&gt;This will move the HEAD reference of the local repository to branch reference path (e.g. &lt;code&gt;refs/heads/master&lt;/code&gt;), populate the index with this head data and refresh the working directory to represent the tree at that head.&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;1&quot;&gt;&lt;code&gt;git add [files]&lt;/code&gt;
&lt;p&gt;This will cross reference the checksums of the &lt;em&gt;files&lt;/em&gt; specified with the corresponding entries in the Git index to see if the index for staged files needs updating with the working directory's version. Nothing changes in the Git directory (or repository).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Let us explore what this means more concretely by inspecting the contents of files under the &lt;code&gt;.git&lt;/code&gt; directory (or repository).&lt;/p&gt;
&lt;pre&gt;
  $ GIT_DIR=$PWD/.git
  $ cat $GIT_DIR/HEAD

  ref: refs/heads/master

  $ MY_CURRENT_BRANCH=$(cat .git/HEAD | sed 's/ref: //g')
  $ cat $GIT_DIR/$MY_CURRENT_BRANCH

  cat: .git/refs/heads/master: No such file or directory
&lt;/pre&gt;
&lt;p&gt;We get an error because, before making any commits to a Git repository at all, no branches exist except the default branch in Git which is &lt;code&gt;master&lt;/code&gt;, whether it exists yet or not.&lt;/p&gt;
&lt;p&gt;Now if we make a new commit, the master branch is created by default for this commit. Let us do this (continuing in the same shell, retaining history and context):&lt;/p&gt;
&lt;pre&gt;
  $ git commit -m &quot;Initial empty commit&quot; --allow-empty
  $ git branch

  * master

  $ cat $GIT_DIR/$MY_CURRENT_BRANCH

  3bce5b130b17b7ce2f98d17b2998e32b1bc29d68

  $ git cat-file -p $(cat $GIT_DIR/$MY_CURRENT_BRANCH)
&lt;/pre&gt;
&lt;p&gt;What we are starting to see here is the content representation inside Git's object database.&lt;/p&gt;
&lt;/section&gt;&lt;section readability=&quot;20&quot;&gt;&lt;h2&gt;6.6. The Object Database&lt;/h2&gt;
&lt;img src=&quot;http://aosabook.org/images/git/object-hierarchy.png&quot;/&gt; Figure 6.2: Git objects
&lt;p&gt;Git has four basic primitive objects that every type of content in the local repository is built around. Each object type has the following attributes: &lt;em&gt;type&lt;/em&gt;, &lt;em&gt;size&lt;/em&gt; and &lt;em&gt;content&lt;/em&gt;. The primitive object types are:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;em&gt;Tree&lt;/em&gt;: an element in a tree can be another tree or a blob, when representing a content directory.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Blob&lt;/em&gt;: a blob represents a file stored in the repository.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Commit&lt;/em&gt;: a commit points to a tree representing the top-level directory for that commit as well as parent commits and standard attributes.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Tag&lt;/em&gt;: a tag has a name and points to a commit at the point in the repository history that the tag represents.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;All object primitives are referenced by a SHA, a 40-digit object identity, which has the following properties:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;If two objects are identical they will have the same SHA.&lt;/li&gt;
&lt;li&gt;if two objects are different they will have different SHAs.&lt;/li&gt;
&lt;li&gt;If an object was only copied partially or another form of data corruption occurred, recalculating the SHA of the current object will identify such corruption.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;The first two properties of the SHA, relating to identity of the objects, is most useful in enabling Git's distributed model (the second goal of Git). The latter property enables some safeguards against corruption (the third goal of Git).&lt;/p&gt;
&lt;p&gt;Despite the desirable results of using DAG-based storage for content storage and merge histories, for many repositories delta storage will be more space-efficient than using &lt;em&gt;loose&lt;/em&gt; DAG objects.&lt;/p&gt;
&lt;/section&gt;&lt;section readability=&quot;23&quot;&gt;&lt;h2&gt;6.7. Storage and Compression Techniques&lt;/h2&gt;
&lt;p&gt;Git tackles the storage space problem by packing objects in a compressed format, using an index file which points to offsets to locate specific objects in the corresponding &lt;em&gt;packed&lt;/em&gt; file.&lt;/p&gt;
&lt;img src=&quot;http://aosabook.org/images/git/packed-format.png&quot;/&gt; Figure 6.3: Diagram of a pack file with corresponding index file
&lt;p&gt;We can count the number of loose (or unpacked) objects in the local Git repository using &lt;code&gt;git count-objects&lt;/code&gt;. Now we can have Git pack loose objects in the object database, remove loose objects already packed, and find redundant pack files with Git plumbing commands if desired.&lt;/p&gt;
&lt;p&gt;The pack file format in Git has evolved, with the initial format storing CRC checksums for the pack file and index file in the index file itself. However, this meant there was the possibility of undetectable corruption in the compressed data since the repacking phase did not involve any further checks. Version 2 of the pack file format overcomes this problem by including the CRC checksums of each compressed object in the pack index file. Version 2 also allows packfiles larger than 4 GB, which the initial format did not support. As a way to quickly detect pack file corruption the end of the pack file contains a 20-byte SHA1 sum of the ordered list of all the SHAs in that file. The emphasis of the newer pack file format is on helping fulfill Git's second usability design goal of safeguarding against data corruption.&lt;/p&gt;
&lt;p&gt;For remote communication Git calculates the commits and content that need to be sent over the wire to synchronize repositories (or just a branch), and generates the pack file format on the fly to send back using the desired protocol of the client.&lt;/p&gt;
&lt;/section&gt;&lt;section readability=&quot;43&quot;&gt;&lt;h2&gt;6.8. Merge Histories&lt;/h2&gt;
&lt;p&gt;As mentioned previously, Git differs fundamentally in merge history approach than the RCS family of VCSs. Subversion, for example, represents file or tree history in a linear progression; whatever has a higher revision number will supercede anything before it. Branching is not supported directly, only through an unenforced directory structure within the repository.&lt;/p&gt;
&lt;img src=&quot;http://aosabook.org/images/git/merge-history.png&quot;/&gt; Figure 6.4: Diagram showing merge history lineage
&lt;p&gt;Let us first use an example to show how this can be problematic when maintaining multiple branches of a work. Then we will look at a scenario to show its limitations.&lt;/p&gt;
&lt;p&gt;When working on a &quot;branch&quot; in Subversion at the typical root &lt;code&gt;branches/branch-name&lt;/code&gt;, we are working on directory subtree adjacent to the &lt;code&gt;trunk&lt;/code&gt; (typically where the live or &lt;em&gt;master&lt;/em&gt; equivalent code resides within). Let us say this branch is to represent parallel development of the &lt;code&gt;trunk&lt;/code&gt; tree.&lt;/p&gt;
&lt;p&gt;For example, we might be rewriting a codebase to use a different database. Part of the way through our rewrite we wish to merge in upstream changes from another branch subtree (not trunk). We merge in these changes, manually if necessary, and proceed with our rewrite. Later that day we finish our database vendor migration code changes on our &lt;code&gt;branches/branch-name&lt;/code&gt; branch and merge our changes into &lt;code&gt;trunk&lt;/code&gt;. The problem with the way linear-history VCSs like Subversion handle this is that there is no way to know that the changesets from the other branch are now contained within the trunk.&lt;/p&gt;
&lt;p&gt;DAG-based merge history VCSs, like Git, handle this case reasonably well. Assuming the other branch does not contain commits that have not been merged into our database vendor migration branch (say, &lt;code&gt;db-migration&lt;/code&gt; in our Git repository), we can determine—from the commit object parent relationships—that a commit on the &lt;code&gt;db-migration&lt;/code&gt; branch contained the &lt;em&gt;tip&lt;/em&gt; (or HEAD) of the other upstream branch. Note that a commit object can have zero or more (bounded by only the abilities of the merger) parents. Therefore the merge commit on the &lt;code&gt;db-migration&lt;/code&gt; branch &lt;em&gt;knows&lt;/em&gt; it merged in the current HEAD of the current branch and the HEAD of the other upstream branch through the SHA hashes of the parents. The same is true of the merge commit in the &lt;code&gt;master&lt;/code&gt; (the &lt;code&gt;trunk&lt;/code&gt; equivalent in Git).&lt;/p&gt;
&lt;p&gt;A question that is hard to answer definitively using DAG-based (and linear-based) merge histories is which commits are contained within each branch. For example, in the above scenario we assumed we merged into each branch all the changes from both branches. This may not be the case.&lt;/p&gt;
&lt;p&gt;For simpler cases Git has the ability to cherry pick commits from other branches in to the current branch, assuming the commit can cleanly be applied to the branch.&lt;/p&gt;
&lt;/section&gt;&lt;section readability=&quot;40.162259615385&quot;&gt;&lt;h2&gt;6.9. What's Next?&lt;/h2&gt;
&lt;p&gt;As mentioned previously, Git core as we know it today is based on a toolkit design philosophy from the Unix world, which is very handy for scripting but less useful for embedding inside or linking with longer running applications or services. While there is Git support in many popular Integrated Development Environments today, adding this support and maintaining it has been more challenging than integrating support for VCSs that provide an easy-to-link-and-share library for multiple platforms.&lt;/p&gt;
&lt;p&gt;To combat this, Shawn Pearce (of Google's Open Source Programs Office) spearheaded an effort to create a linkable Git library with more permissive licensing that did not inhibit use of the library. This was called &lt;a href=&quot;https://github.com/libgit2/libgit2&quot;&gt;libgit2&lt;/a&gt;. It did not find much traction until a student named Vincent Marti chose it for his Google Summer of Code project last year. Since then Vincent and Github engineers have continued contributing to the libgit2 project, and created bindings for numerous other popular languages such as Ruby, Python, PHP, .NET languages, Lua, and Objective-C.&lt;/p&gt;
&lt;p&gt;Shawn Pearce also started a BSD-licensed pure Java library called &lt;a href=&quot;https://github.com/eclipse/jgit&quot;&gt;JGit&lt;/a&gt; that supports many common operations on Git repositories. It is now maintained by the Eclipse Foundation for use in the Eclipse IDE Git integration.&lt;/p&gt;
&lt;p&gt;Other interesting and experimental open source endeavours outside of the Git core project are a number of implementations using alternative datastores as backends for the Git object database such as:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://github.com/spearce/jgit_cassandra&quot;&gt;jgit_cassandra&lt;/a&gt;, which offers Git object persistence using Apache Cassandra, a hybrid datastore using Dynamo-style distribution with BigTable column family data model semantics.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/spearce/jgit_hbase&quot;&gt;jgit_hbase&lt;/a&gt;, which enables read and write operations to Git objects stored in HBase, a distributed key-value datastore.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/libgit2/libgit2-backends&quot;&gt;libgit2-backends&lt;/a&gt;, which emerged from the libgit2 effort to create Git object database backends for multiple popular datastores such as Memcached, Redis, SQLite, and MySQL.&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;All of these open source projects are maintained independently of the Git core project.&lt;/p&gt;
&lt;p&gt;As you can see, today there are a large number of ways to use the Git format. The face of Git is no longer just the toolkit command line interface of the Git Core project; rather it is the repository format and protocol to share between repositories.&lt;/p&gt;
&lt;p&gt;As of this writing, most of these projects, according to their developers, have not reached a stable release, so work in the area still needs to be done but the future of Git appears bright.&lt;/p&gt;
&lt;/section&gt;&lt;section readability=&quot;30&quot;&gt;&lt;h2&gt;6.10. Lessons Learned&lt;/h2&gt;
&lt;p&gt;In software, every design decision is ultimately a trade-off. As a power user of Git for version control and as someone who has developed software around the Git object database model, I have a deep fondness for Git in its present form. Therefore, these lessons learned are more of a reflection of common recurring complaints about Git that are due to design decisions and focus of the Git core developers.&lt;/p&gt;
&lt;p&gt;One of the most common complaints by developers and managers who evaluate Git has been the lack of IDE integration on par with other VCS tools. The toolkit design of Git has made this more challenging than integrating other modern VCS tools into IDEs and related tools.&lt;/p&gt;
&lt;p&gt;Earlier in Git's history some of the commands were implemented as shell scripts. These shell script command implementations made Git less portable, especially to Windows. I am sure the Git core developers did not lose sleep over this fact, but it has negatively impacted adoption of Git in larger organizations due to portability issues that were prevalent in the early days of Git's development. Today a project named Git for Windows has been started by volunteers to ensure new versions of Git are ported to Windows in a timely manner.&lt;/p&gt;
&lt;p&gt;An indirect consequence of designing Git around a toolkit design with a lot of plumbing commands is that new users get lost quickly; from confusion about all the available subcommands to not being able to understand error messages because a low level plumbing task failed, there are many places for new users to go astray. This has made adopting Git harder for some developer teams.&lt;/p&gt;
&lt;p&gt;Even with these complaints about Git, I am excited about the possibilities of future development on the Git Core project, plus all the related open source projects that have been launched from it.&lt;/p&gt;
&lt;/section&gt;</description>
<pubDate>Fri, 26 Oct 2018 14:39:34 +0000</pubDate>
<dc:creator>wheresvic1</dc:creator>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>http://aosabook.org/en/git.html</dc:identifier>
</item>
<item>
<title>Heavy multitaskers have reduced memory</title>
<link>https://news.stanford.edu/2018/10/25/decade-data-reveals-heavy-multitaskers-reduced-memory-psychologist-says/</link>
<guid isPermaLink="true" >https://news.stanford.edu/2018/10/25/decade-data-reveals-heavy-multitaskers-reduced-memory-psychologist-says/</guid>
<description>&lt;p&gt;The smartphones that are now ubiquitous were just gaining popularity when &lt;a href=&quot;https://profiles.stanford.edu/anthony-wagner&quot;&gt;Anthony Wagner&lt;/a&gt; became interested in the research of his Stanford colleague, Clifford Nass, on the effects of media multitasking and attention. Though Wagner, a professor of psychology at Stanford University and director of the &lt;a href=&quot;https://memorylab.stanford.edu/&quot;&gt;Stanford Memory Laboratory&lt;/a&gt;, wasn’t convinced by the early data, he recommended some cognitive tests for Nass to use in subsequent experiments. More than 11 years later, Wagner was intrigued enough to write a review on past research findings, published in &lt;em&gt;Proceedings of the National Academy of Sciences&lt;/em&gt;, and contribute some of his own.&lt;/p&gt;
&lt;div class=&quot;pull-right pull-right-wide&quot; readability=&quot;8&quot;&gt;&lt;img class=&quot;wp-image-24130 size-full img-responsive&quot; src=&quot;https://news-media.stanford.edu/wp-content/uploads/2018/10/24142623/multitasking.jpg&quot; alt=&quot;Woman holding phone in one hand, tablet in another, at a laptop computer.&quot; srcset=&quot;https://news-media.stanford.edu/wp-content/uploads/2018/10/24142623/multitasking.jpg 1500w, https://news-media.stanford.edu/wp-content/uploads/2018/10/24142623/multitasking-555x370.jpg 555w, https://news-media.stanford.edu/wp-content/uploads/2018/10/24142623/multitasking-795x530.jpg 795w, https://news-media.stanford.edu/wp-content/uploads/2018/10/24142623/multitasking-960x640.jpg 960w, https://news-media.stanford.edu/wp-content/uploads/2018/10/24142623/multitasking-705x470.jpg 705w, https://news-media.stanford.edu/wp-content/uploads/2018/10/24142623/multitasking-345x230.jpg 345w, https://news-media.stanford.edu/wp-content/uploads/2018/10/24142623/multitasking-375x250.jpg 375w&quot; sizes=&quot;(max-width: 1500px) 100vw, 1500px&quot;/&gt;&lt;p class=&quot;media-caption&quot;&gt;A decade’s worth of research has shown that people who frequently use many types of media at once performed significantly worse on simple memory tasks. &lt;span class=&quot;media-attrib&quot;&gt;(Image credit: Getty Images)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;a href=&quot;http://www.pnas.org/content/early/2018/09/26/1611612115.short&quot;&gt;paper&lt;/a&gt;, co-authored with neuroscientist Melina Uncapher of the University of California, San Francisco, summarizes a decade’s worth of research on the relationship between media multitasking and various domains of cognition, including working memory and attention. In doing that analysis, Wagner noticed a trend emerging in the literature: People who frequently use many types of media at once, or heavy media multitaskers, performed significantly worse on simple memory tasks.&lt;/p&gt;
&lt;p&gt;Wagner spoke with &lt;em&gt;Stanford Report&lt;/em&gt; to explain the findings from his review on media multitasking and cognition, and discuss why it’s premature to determine the impact of these results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How did you become interested in researching media multitasking and memory?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I was brought into a collaboration with Cliff Nass, a Stanford faculty member in communication who passed away a few years ago, and his master’s student, Eyal Ophir. They had this question: With the explosion of media technologies that has resulted in there being multiple simultaneous channels available that we can switch between, how might this relate to human cognition? Eyal and Cliff would come chat with me about their early findings and – I have to say – I thought it was complete hooey. I was skeptical. But, after a few experiments, the data were increasingly pointing to a link between media multitasking and attention. Their findings struck me as potentially important given the way we’re living as humans in this attention economy. Years later, as a memory scientist my interests continued to grow. Given that attention and cognitive control are so fundamental for memory, I wanted to see if there was a relationship between media multitasking and memory.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How do you define media multitasking, and can you give hypothetical examples of people that would be “heavy” and “light” media multitaskers?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Well, we don’t multitask. We task switch. The word “multitasking” implies that you can do two or more things at once, but in reality our brains only allow us to do one thing at a time and we have to switch back and forth.&lt;/p&gt;
&lt;p&gt;Heavy media multitaskers have many media channels open at once and they switch between them. A heavy media multitasker might be writing an academic paper on their laptop, occasionally checking the Stanford basketball game on TV, responding to texts and Facebook messages, then getting back to writing – but then an email pops up and they check it. A light media multitasker would only be writing the academic paper or may only switch between a couple of media. They may turn off Wi-Fi, put away their phone or change their settings so they only get notified every hour. Those are some extreme examples, but they provide a sense of how people differ in their media use. Moreover, because our media landscape has continued to accelerate and change, those who are considered a heavy or light media multitasker today may not be the same as those a decade ago.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How do scientists assess someone’s memory?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are many forms of memory, and thus many ways of probing memory in the lab. For working memory – the ability to keep a limited amount of information active in mind – we often use simple short-delay memory tasks. For example, in one test we show a set of oriented blue rectangles, then remove them from the screen and ask the subject to retain that information in mind. Then we’ll show them another set of rectangles and ask if any have changed orientation. To measure memory capacity, we do this task with a different number of rectangles and determine how performance changes with increasing memory loads. To measure the ability to filter out distraction, sometimes we add distractors, like red rectangles that the subjects are told to ignore.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What overall trends did you notice when you were looking through the literature to write this review?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In about half of the studies, the heavy media multitaskers are significantly underperforming on tasks of working memory and sustained attention. The other half are null results; there’s no significant difference. It strikes me as pretty clear that there is a negative relationship between media multitasking and memory performance – that high media multitasking is associated with poor performance on cognitive memory tasks. There’s not a single published paper that shows a significant positive relationship between working memory capacity and multitasking.&lt;/p&gt;
&lt;p&gt;In the review we noticed an interesting potential emerging story. One possibility is that reduced working memory occurs in heavy media multitaskers because they have a higher probability of experiencing lapses of attention. When demands are low, they underperform. But, when the task demands are high, such as when the working memory tasks are harder, there’s no difference between the heavy and light media multitaskers. This observation, combined with the negative relationship between multitasking and performance on sustained attention tasks, prompted us to start looking at intrasubject variability and moment-to-moment fluctuations in a person’s ability to use task goals to direct attention in a sustained manner.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How do these findings affect how people should engage with media, or should they at all?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I would never tell anyone that the data unambiguously show that media multitasking causes a change in attention and memory. That would be premature. It’s too early to definitively determine cause and effect.&lt;/p&gt;
&lt;p&gt;One could choose to be cautious, however. Many of us have felt like our technology and media are controlling us – that email chime or text tone demands our attention. But we can control that by adopting approaches that minimize habitual multitasking; we can decide to be more thoughtful and reflective users of media.&lt;/p&gt;
&lt;p&gt;That said, multitasking isn’t efficient. We know there are costs of task switching. So that might be an argument to do less media multitasking – at least when working on a project that matters academically or professionally. If you’re multitasking while doing something significant, like an academic paper or work project, you’ll be slower to complete it and you might be less successful.&lt;/p&gt;
</description>
<pubDate>Fri, 26 Oct 2018 14:05:37 +0000</pubDate>
<dc:creator>cabinguy</dc:creator>
<og:type>article</og:type>
<og:title>Heavy multitaskers have reduced memory</og:title>
<og:description>People who frequently engage with multiple types of media at once performed worse on simple memory tasks, according to the last decade of research. However, it’s still too soon to determine cause and effect, says psychology Professor Anthony Wagner.</og:description>
<og:url>https://news.stanford.edu/2018/10/25/decade-data-reveals-heavy-multitaskers-reduced-memory-psychologist-says/</og:url>
<og:image>https://news-media.stanford.edu/wp-content/uploads/2018/10/24142623/multitasking.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://news.stanford.edu/2018/10/25/decade-data-reveals-heavy-multitaskers-reduced-memory-psychologist-says/</dc:identifier>
</item>
<item>
<title>A Dark Consensus About Screens and Kids Begins to Emerge in Silicon Valley</title>
<link>https://www.nytimes.com/2018/10/26/style/phones-children-silicon-valley.html</link>
<guid isPermaLink="true" >https://www.nytimes.com/2018/10/26/style/phones-children-silicon-valley.html</guid>
<description>&lt;div class=&quot;css-18sbwfn StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-4w7y5l&quot;&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;SAN FRANCISCO — The people who are closest to a thing are often the most wary of it. Technologists know how phones really work, and many have decided &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.nytimes.com/2018/10/26/style/silicon-valley-nannies.html?module=inline&quot; title=&quot;&quot;&gt;they don’t want their own children anywhere near them&lt;/a&gt;.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;A wariness that has been slowly brewing is turning into a regionwide consensus: The benefits of screens as a learning tool are overblown, and the risks for addiction and stunting development seem high. The debate in Silicon Valley now is about how much exposure to phones is O.K.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;“Doing no screen time is almost easier than doing a little,” said Kristin Stecher, a former social computing researcher married to a Facebook engineer. “If my kids do get it at all, they just want it more.”&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;Ms. Stecher, 37, and her husband, Rushabh Doshi, researched screen time and came to a simple conclusion: they wanted almost none of it in their house. Their daughters, ages 5 and 3, have no screen time “budget,” no regular hours they are allowed to be on screens. The only time a screen can be used is during the travel portion of a long car ride (the four-hour drive to Tahoe counts) or during a plane trip.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-14jsv4e&quot;/&gt;&lt;/div&gt;&lt;div class=&quot;css-18sbwfn StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-4w7y5l&quot;&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;Recently she has softened this approach. Every Friday evening the family watches one movie.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;There is a looming issue Ms. Stecher sees in the future: Her husband, who is 39, loves video games and thinks they can be educational and entertaining. She does not.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;“We’ll cross that when we come to it,” said Ms. Stecher, who is due soon with a boy.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;Some of the people who built video programs are now horrified by how many places a child can now watch a video.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;Asked about limiting screen time for children, Hunter Walk, a venture capitalist who for years directed product for YouTube at Google, sent a photo of a potty training toilet with an iPad attached and wrote: “Hashtag ‘products we didn’t buy.’”&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;Athena Chavarria, who worked as an executive assistant at Facebook and is now at Mark Zuckerberg’s philanthropic arm, the Chan Zuckerberg Initiative, said: “I am convinced the devil lives in our phones and is wreaking havoc on our children.”&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;Ms. Chavarria did not let her children have cellphones until high school, and even now bans phone use in the car and severely limits it at home.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-14jsv4e&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;css-18sbwfn StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-4w7y5l&quot;&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;She said she lives by the mantra that the last child in the class to get a phone wins. Her daughter did not get a phone until she started ninth grade.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;“Other parents are like, ‘Aren’t you worried you don’t know where your kids are when you can’t find them?’” Ms. Chavarria said. “And I’m like, ‘No, I do not need to know where my kids are every second of the day.’”&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-14jsv4e&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;css-1ur45q4 e5d2tgl3&quot;&gt;
&lt;div class=&quot;css-1ukm2ij e5d2tgl0&quot;&gt;More about kids and screens&lt;/div&gt;
&lt;div class=&quot;css-15g2oxy e5d2tgl2&quot;&gt;
&lt;div class=&quot;css-1s7gosn ezm5mny6&quot;&gt;
&lt;div class=&quot;css-i9gxme ezm5mny4&quot;&gt;
&lt;div class=&quot;css-oyr3ly ezm5mny2&quot;&gt;Silicon Valley Nannies Are Phone Police for Kids&lt;/div&gt;
&lt;div class=&quot;css-14shocx ezm5mny3&quot;&gt;Child care contracts now demand that nannies hide phones, tablets, computers and TVs from their charges.&lt;/div&gt;
&lt;time class=&quot;css-tnzxe9 eqgapgq0&quot; datetime=&quot;2018-10-26&quot;&gt;Oct. 26, 2018&lt;/time&gt;&lt;/div&gt;
&lt;div class=&quot;css-wexrmg ezm5mny0&quot;&gt;&lt;img src=&quot;https://static01.nyt.com/images/2018/10/26/style/26SiliconNannies-1/26SiliconNannies-threeByTwoSmallAt2X.jpg&quot; class=&quot;css-1g9kf2h ezm5mny1&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;css-1s7gosn ezm5mny6&quot;&gt;
&lt;div class=&quot;css-i9gxme ezm5mny4&quot;&gt;
&lt;div class=&quot;css-oyr3ly ezm5mny2&quot;&gt;The Digital Gap Between Rich and Poor Kids Is Not What We Expected&lt;/div&gt;
&lt;div class=&quot;css-14shocx ezm5mny3&quot;&gt;America’s public schools are still touting devices with screens — even offering digital-only preschools. The rich are banning screens from class altogether.&lt;/div&gt;
&lt;time class=&quot;css-tnzxe9 eqgapgq0&quot; datetime=&quot;2018-10-26&quot;&gt;Oct. 26, 2018&lt;/time&gt;&lt;/div&gt;
&lt;div class=&quot;css-wexrmg ezm5mny0&quot;&gt;&lt;img src=&quot;https://static01.nyt.com/images/2018/10/26/style/26DigitalDivide-PROMO/26DigitalDivide-PROMO-threeByTwoSmallAt2X.jpg&quot; class=&quot;css-1g9kf2h ezm5mny1&quot;/&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;css-18sbwfn StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-4w7y5l&quot;&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;For longtime tech leaders, watching how the tools they built affect their children has felt like a reckoning on their life and work.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;Among those is Chris Anderson, the former editor of Wired and now the chief executive of a robotics and drone company. He is also the founder of &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://geekdad.com/&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;GeekDad.com&lt;/a&gt;.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;“On the scale between candy and crack cocaine, it’s closer to crack cocaine,” Mr. Anderson said of screens.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-14jsv4e&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;css-18sbwfn StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-4w7y5l&quot;&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;Technologists building these products and writers observing the tech revolution were naïve, he said.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;“We thought we could control it,” Mr. Anderson said. “And this is beyond our power to control. This is going straight to the pleasure centers of the developing brain. This is beyond our capacity as regular parents to understand.”&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;He has five children and 12 tech rules. They include: no phones until the summer before high school, no screens in bedrooms, network-level content blocking, no social media until age 13, no iPads at all and screen time schedules enforced by Google Wifi that he controls from his phone. Bad behavior? The child goes offline for 24 hours.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;“I didn’t know what we were doing to their brains until I started to observe the symptoms and the consequences,” Mr. Anderson said.&lt;/p&gt;
&lt;div class=&quot;css-zgakxe e1vv25i80&quot;&gt;&lt;span class=&quot;css-1ly73wi e1afaoz0&quot;&gt;Image&lt;/span&gt;&lt;img alt=&quot;&quot; class=&quot;css-1m50asq&quot; src=&quot;https://static01.nyt.com/images/2018/10/11/style/oakImage-1539293138789/oakImage-1539293138789-articleLarge.png?quality=75&amp;amp;auto=webp&amp;amp;disable=upscale&quot; srcset=&quot;https://static01.nyt.com/images/2018/10/11/style/oakImage-1539293138789/oakImage-1539293138789-articleLarge.png?quality=90&amp;amp;auto=webp 600w,https://static01.nyt.com/images/2018/10/11/style/oakImage-1539293138789/oakImage-1539293138789-jumbo.png?quality=90&amp;amp;auto=webp 576w,https://static01.nyt.com/images/2018/10/11/style/oakImage-1539293138789/oakImage-1539293138789-superJumbo.png?quality=90&amp;amp;auto=webp 1080w&quot; sizes=&quot;50vw&quot; itemprop=&quot;url&quot; itemid=&quot;https://static01.nyt.com/images/2018/10/11/style/oakImage-1539293138789/oakImage-1539293138789-articleLarge.png?quality=75&amp;amp;auto=webp&amp;amp;disable=upscale&quot;/&gt;&lt;/div&gt;
&lt;span class=&quot;css-1wp6toh e1olku6u0&quot;&gt;A view of the Anderson family schedule.&lt;/span&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;“This is scar tissue talking. We’ve made every mistake in the book, and I think we got it wrong with some of my kids,” Mr. Anderson said. “We glimpsed into the chasm of addiction, and there were some lost years, which we feel bad about.”&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-14jsv4e&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;css-18sbwfn StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-4w7y5l&quot;&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;His children attended private elementary school, where he saw the administration introduce iPads and smart whiteboards, only to “descend into chaos and then pull back from it all.”&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;This idea that Silicon Valley parents are wary about tech is not new. The godfathers of tech expressed these concerns years ago, and concern has been loudest from the top.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;Tim Cook, the C.E.O. of Apple, &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.theguardian.com/technology/2018/jan/19/tim-cook-i-dont-want-my-nephew-on-a-social-network&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;said earlier this year&lt;/a&gt; that he would not let his nephew join social networks. Bill Gates banned cellphones until his children were teenagers, and Melinda Gates wrote that &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.washingtonpost.com/news/parenting/wp/2017/08/24/melinda-gates-i-spent-my-career-in-technology-i-wasnt-prepared-for-its-effect-on-my-kids/?utm_term=.a462ac452c51&quot; title=&quot;&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;she wished they had waited even longer&lt;/a&gt;. Steve Jobs &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.nytimes.com/2014/09/11/fashion/steve-jobs-apple-was-a-low-tech-parent.html?module=inline&quot; title=&quot;&quot;&gt;would not let his young children near iPads&lt;/a&gt;.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;But in the last year, a fleet of high-profile Silicon Valley defectors have been sounding alarms in increasingly dire terms about what these gadgets do to the human brain. Suddenly rank-and-file Silicon Valley workers are obsessed. No-tech homes are cropping up across the region. &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.nytimes.com/2018/10/26/style/silicon-valley-nannies.html?module=inline&quot; title=&quot;&quot;&gt;Nannies are being asked to sign no-phone&lt;/a&gt; &lt;a class=&quot;css-1g7m0tk&quot; href=&quot;https://www.nytimes.com/2018/10/26/style/silicon-valley-nannies.html?module=inline&quot; title=&quot;&quot;&gt;contracts&lt;/a&gt;.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;Those who have exposed their children to screens try to talk them out of addiction by explaining how the tech works.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;John Lilly, a Silicon Valley-based venture capitalist with Greylock Partners and the former C.E.O. of Mozilla, said he tries to help his 13-year-old son understand that he is being manipulated by those who built the technology.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-14jsv4e&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;css-18sbwfn StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-4w7y5l&quot;&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;“I try to tell him somebody wrote code to make you feel this way — I’m trying to help him understand how things are made, the values that are going into things and what people are doing to create that feeling,” Mr. Lilly said. “And he’s like, ‘I just want to spend my 20 bucks to get my Fortnite skins.’”&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;And there are those in tech who disagree that screens are dangerous. Jason Toff, 32, who ran the video platform Vine and now works for Google, lets his 3-year-old play on an iPad, which he believes is no better or worse than a book. This opinion is unpopular enough with his fellow tech workers that he feels there is now “a stigma.”&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;“One reaction I got just yesterday was, ‘Doesn’t it worry you that all the major tech execs are limiting screen time?’” Mr. Toff said. “And I was like, ‘Maybe it should, but I guess I’ve always been skeptical of norms.’ People are just scared of the unknown.”&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;“It’s contrarian,” Mr. Toff said. “But I feel like I’m speaking for a lot of parents that are afraid of speaking out loud for fear of judgment.”&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;He said he thinks back to his own childhood growing up watching a lot of TV. “I think I turned out O.K.,” Mr. Toff said.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;Other Silicon Valley parents say there are ways to make some limited screen time slightly less toxic.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-14jsv4e&quot;/&gt;&lt;/div&gt;
&lt;div class=&quot;css-18sbwfn StoryBodyCompanionColumn&quot;&gt;
&lt;div class=&quot;css-4w7y5l&quot;&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;Renee DiResta, a security researcher on the board of the Center for Humane Tech, won’t allow passive screen time, but will allow short amounts of time on challenging games.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;She wants her 2- and 4-year-old children to learn how to code young, so she embraces their awareness of gadgets. But she distinguishes between these types of screen use. Playing a building game is allowed, but watching a YouTube video is not, unless it is as a family.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;And Frank Barbieri, a San Francisco-based executive at the start-up PebblePost that tracks online activity to send direct mail advertising, tries to limit his 5-year-old daughter’s screen time to Italian language content.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;“We have friends who are screen abolitionists, and we have friends who are screen liberalists,” Mr. Barbieri said.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;He had read studies on how learning a second language at a young age is good for the developing mind, so his daughter watches Italian-language movies and TV shows.&lt;/p&gt;
&lt;p class=&quot;css-1xl4flh e2kc3sl0&quot;&gt;“For us, honestly, me and my wife were like, ‘Where would we like to visit?’” Mr. Barbieri said.&lt;/p&gt;
&lt;/div&gt;
&lt;aside class=&quot;css-14jsv4e&quot;/&gt;&lt;/div&gt;
</description>
<pubDate>Fri, 26 Oct 2018 14:05:18 +0000</pubDate>
<dc:creator>extraterra</dc:creator>
<og:url>https://www.nytimes.com/2018/10/26/style/phones-children-silicon-valley.html</og:url>
<og:type>article</og:type>
<og:title>A Dark Consensus About Screens and Kids Begins to Emerge in Silicon Valley</og:title>
<og:image>https://static01.nyt.com/images/2018/10/28/fashion/26NoTech-1/26NoTech-1-facebookJumbo.jpg</og:image>
<og:description>“I am convinced the devil lives in our phones.”</og:description>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://www.nytimes.com/2018/10/26/style/phones-children-silicon-valley.html</dc:identifier>
</item>
<item>
<title>Generating custom photo-realistic faces using AI</title>
<link>https://blog.insightdatascience.com/generating-custom-photo-realistic-faces-using-ai-d170b1b59255</link>
<guid isPermaLink="true" >https://blog.insightdatascience.com/generating-custom-photo-realistic-faces-using-ai-d170b1b59255</guid>
<description>&lt;p name=&quot;604f&quot; id=&quot;604f&quot; class=&quot;graf graf--p graf--leading&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;All the code and online demo are available at the&lt;/em&gt; &lt;a href=&quot;https://github.com/SummitKwan/transparent_latent_gan&quot; data-href=&quot;https://github.com/SummitKwan/transparent_latent_gan&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;project page&lt;/em&gt;&lt;/a&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h3 name=&quot;f1b3&quot; id=&quot;f1b3&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Teaching computers to draw photos according to descriptions&lt;/h3&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*lCAeDPpTkfvDqzZXiQoc1g.png&quot; data-width=&quot;705&quot; data-height=&quot;212&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*lCAeDPpTkfvDqzZXiQoc1g.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*lCAeDPpTkfvDqzZXiQoc1g.png&quot;/&gt;&lt;/div&gt;
Discriminative vs. generative tasks
&lt;p name=&quot;0f24&quot; id=&quot;0f24&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;&lt;em class=&quot;markup--em markup--p-em&quot;&gt;Describing&lt;/em&gt; an image is easy for humans, and we are able to do it from a very young age. In machine learning, this task is a &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;discriminative&lt;/strong&gt; classification/regression problem, i.e. predicting feature labels from input images. Recent advancements in ML/AI techniques, especially deep learning models, are beginning to excel in these tasks, sometimes reaching or exceeding human performance, as is demonstrated in scenarios like visual object recognition (e.g. from AlexNet to ResNet on ImageNet classification) and object detection/segmentation (e.g. from RCNN to YOLO on COCO dataset), etc.&lt;/p&gt;
&lt;p name=&quot;b375&quot; id=&quot;b375&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;However, the other way around, g&lt;em class=&quot;markup--em markup--p-em&quot;&gt;enerating&lt;/em&gt; realistic images based on descriptions, is much harder, and takes years of graphic design training. In machine learning this is a &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;generative&lt;/strong&gt; task, which is also much more challenging than discriminative tasks, as a generative model has to produce much richer information (like a full image at some level of detail and variation) based on a smaller seed input.&lt;/p&gt;
&lt;p name=&quot;85ec&quot; id=&quot;85ec&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Despite the difficulty in creating such types of applications, &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;generative models&lt;/strong&gt; (with some control) can be extremely useful in many cases:&lt;/p&gt;
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;1d9e&quot; id=&quot;1d9e&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Content creation&lt;/strong&gt;: Imagine if an advertisement company could automatically generate attractive product images that match the content and style of the webpage where these images are inserted; a fashion designer could get inspiration by asking an algorithm to produce 20 examples of shoe designs that are related to “leisure”, “canvas”, “summer” and “passionate”; and a new game could allow players to create realistic avatars based simple descriptions.&lt;/li&gt;
&lt;li name=&quot;8d4c&quot; id=&quot;8d4c&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Content-aware smart editing&lt;/strong&gt;: We could allow a photographer to change the facial expression, amount of wrinkles and hair style of a profile photo with several clicks; and a Hollywood studio artist could transform the footage shot on a cloudy evening to look like it was shot on a sunny morning, with sunshine shedding light from the left side of the screen.&lt;/li&gt;
&lt;li name=&quot;8875&quot; id=&quot;8875&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Data augmentation&lt;/strong&gt;: An autonomous driving car company could synthesize realistic videos of a particular type of accident scenario to augment the training dataset; and a credit card company could synthesize data of a particular type of fraud data that is underrepresented in the dataset to improve their fraud detection system.&lt;/li&gt;
&lt;/ul&gt;&lt;p name=&quot;5032&quot; id=&quot;5032&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;In this post, we will describe our recent work called &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Transparent Latent-space GAN (TL-GAN)&lt;/strong&gt;, which extends current cutting edge models to provide a new interface. We are currently working on a paper, that will have more technical details.&lt;/p&gt;
&lt;h3 name=&quot;261d&quot; id=&quot;261d&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Overview of generative models&lt;/h3&gt;
&lt;p name=&quot;5d9b&quot; id=&quot;5d9b&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;The deep learning community is making rapid progress on generative models. Among them are three promising types of models: &lt;a href=&quot;https://arxiv.org/abs/1601.06759&quot; data-href=&quot;https://arxiv.org/abs/1601.06759&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;autoregressive models&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1312.6114&quot; data-href=&quot;https://arxiv.org/abs/1312.6114&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;variational autoencoders (VAE)&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1406.2661&quot; data-href=&quot;https://arxiv.org/abs/1406.2661&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;generative adversarial networks (GAN)&lt;/a&gt;, illustrated as the figure below. If you are interested in the details, please check out this awesome OpenAI blog &lt;a href=&quot;https://blog.openai.com/generative-models/&quot; data-href=&quot;https://blog.openai.com/generative-models/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;post&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*JFg3SRN0Uf-A7YWajQ6HOw.png&quot; data-width=&quot;1632&quot; data-height=&quot;509&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*JFg3SRN0Uf-A7YWajQ6HOw.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*JFg3SRN0Uf-A7YWajQ6HOw.png&quot;/&gt;&lt;/div&gt;
&lt;em class=&quot;markup--em markup--figure-em&quot;&gt;Figure: comparison of generator networks. Image from&lt;/em&gt; &lt;a href=&quot;https://wiki.math.uwaterloo.ca/statwiki/index.php?title=STAT946F17/Conditional_Image_Generation_with_PixelCNN_Decoders&quot; data-href=&quot;https://wiki.math.uwaterloo.ca/statwiki/index.php?title=STAT946F17/Conditional_Image_Generation_with_PixelCNN_Decoders&quot; class=&quot;markup--anchor markup--figure-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;&lt;em class=&quot;markup--em markup--figure-em&quot;&gt;University of Waterloo STAT946F17 course&lt;/em&gt;&lt;/a&gt;
&lt;p name=&quot;ef16&quot; id=&quot;ef16&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;So far, GANs produce images of the &lt;a href=&quot;https://blog.openai.com/generative-models/&quot; data-href=&quot;https://blog.openai.com/generative-models/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;highest quality&lt;/a&gt; (photo-realistic and diverse, with convincing details in high resolution). Look at the stunning images generated by Nvidia’s recent work with pg-GAN (&lt;a href=&quot;https://github.com/tkarras/progressive_growing_of_gans&quot; data-href=&quot;https://github.com/tkarras/progressive_growing_of_gans&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;progressively-growing GAN&lt;/a&gt;). For this reason, this blog post will focus on GAN models.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*F1HFYYfHgkjaEcGPrW5FDw.png&quot; data-width=&quot;700&quot; data-height=&quot;350&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*F1HFYYfHgkjaEcGPrW5FDw.png&quot;/&gt;&lt;/div&gt;
&lt;em class=&quot;markup--em markup--figure-em&quot;&gt;Figure: synthetic images generated by&lt;/em&gt; &lt;a href=&quot;https://github.com/tkarras/progressive_growing_of_gans&quot; data-href=&quot;https://github.com/tkarras/progressive_growing_of_gans&quot; class=&quot;markup--anchor markup--figure-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;&lt;em class=&quot;markup--em markup--figure-em&quot;&gt;pg-GAN&lt;/em&gt;&lt;/a&gt; &lt;em class=&quot;markup--em markup--figure-em&quot;&gt;from Nvidia. None of these images are real!&lt;/em&gt;
&lt;h3 name=&quot;1ab2&quot; id=&quot;1ab2&quot; class=&quot;graf graf--h3 graf-after--figure&quot;&gt;Controlling the output of GAN models&lt;/h3&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*lzQ-xViUbiJSA0Ytu9YTQQ.png&quot; data-width=&quot;899&quot; data-height=&quot;430&quot; data-is-featured=&quot;true&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*lzQ-xViUbiJSA0Ytu9YTQQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*lzQ-xViUbiJSA0Ytu9YTQQ.png&quot;/&gt;&lt;/div&gt;
Figure: random image generation vs. controlled image generation
&lt;p name=&quot;09f4&quot; id=&quot;09f4&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The &lt;a href=&quot;https://arxiv.org/abs/1406.2661&quot; data-href=&quot;https://arxiv.org/abs/1406.2661&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;original version of GAN&lt;/a&gt; and many popular successors (like &lt;a href=&quot;https://arxiv.org/abs/1511.06434&quot; data-href=&quot;https://arxiv.org/abs/1511.06434&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;DC-GAN&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1710.10196&quot; data-href=&quot;https://arxiv.org/abs/1710.10196&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;pg-GAN&lt;/a&gt;) are &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;unsupervised&lt;/strong&gt; learning models. After training, the generator network takes random noise as input and produces a photo-realistic image that is barely distinguishable from the training dataset. However, we cannot further control the features of the generated images. In most applications (such as the scenarios described in the first section), users would like to generate samples with &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;custom features&lt;/strong&gt; (like age, hair color, facial expression, etc), and ideally, tuning each feature continuously.&lt;/p&gt;
&lt;p name=&quot;af59&quot; id=&quot;af59&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;To achieve controlled synthesis, numerous variants of GAN have been created. They can be roughly divided into two types: style-transfer networks and conditional generators.&lt;/p&gt;
&lt;h4 name=&quot;a761&quot; id=&quot;a761&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Style-transfer networks&lt;/h4&gt;
&lt;p name=&quot;986e&quot; id=&quot;986e&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Style-transfer networks, represented by &lt;a href=&quot;https://junyanz.github.io/CycleGAN/&quot; data-href=&quot;https://junyanz.github.io/CycleGAN/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;CycleGAN&lt;/a&gt; and &lt;a href=&quot;https://phillipi.github.io/pix2pix/&quot; data-href=&quot;https://phillipi.github.io/pix2pix/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;pix2pix&lt;/a&gt;, are models trained to translate image from one domain to another (e.g. from horse to zebra, from sketch to colored images). As a result, we cannot continuously tune one feature gradually between two discrete states (eg. add slightly more beard on the face). Also, one network is dedicated to one type of transfer, so it requires ten different neural networks to tune 10 features.&lt;/p&gt;
&lt;h4 name=&quot;1595&quot; id=&quot;1595&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Conditional generators&lt;/h4&gt;
&lt;p name=&quot;211a&quot; id=&quot;211a&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Conditional generators, represented by &lt;a href=&quot;https://arxiv.org/abs/1411.1784&quot; data-href=&quot;https://arxiv.org/abs/1411.1784&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;conditional GAN&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1610.09585&quot; data-href=&quot;https://arxiv.org/abs/1610.09585&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;AC-GAN&lt;/a&gt;, and &lt;a href=&quot;https://github.com/hanzhanggit/StackGAN&quot; data-href=&quot;https://github.com/hanzhanggit/StackGAN&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Stack-GAN&lt;/a&gt;, are models that jointly learn images with feature labels during training time, enabling the image generation to be conditioned on custom features. Therefore, when you want to add new tunable features to the generation process, you have to retrain the whole GAN model, which takes an enormous amount of computing resources and time (e.g. days to weeks on a single K80 GPU with the perfect set of hyper-parameters). In addition, you have to rely on a single dataset that contains all the custom feature labels to perform the training, instead of leveraging different labels from multiple datasets.&lt;/p&gt;
&lt;p name=&quot;3131&quot; id=&quot;3131&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Our model, which we call &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Transparent Latent-space GAN&lt;/strong&gt; (TL-GAN), addresses these problems of existing methods by approaching controlled generation task from a novel angle. It offers users the ability to &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;gradually tune one or multiple features using a single network&lt;/strong&gt;. Besides, adding new tunable features can be done very efficiently in less than one hour.&lt;/p&gt;
&lt;h3 name=&quot;29f4&quot; id=&quot;29f4&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;TL-GAN: a novel and efficient approach for controlled synthesis and editing&lt;/h3&gt;
&lt;h4 name=&quot;c19d&quot; id=&quot;c19d&quot; class=&quot;graf graf--h4 graf-after--h3&quot;&gt;Making the mysterious latent space transparent&lt;/h4&gt;
&lt;p name=&quot;77c6&quot; id=&quot;77c6&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;We will leverage NVIDIA’s &lt;a href=&quot;https://arxiv.org/abs/1710.10196&quot; data-href=&quot;https://arxiv.org/abs/1710.10196&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;pg-GAN&lt;/a&gt;, the model that generates the photo-realistic high resolution face images as shown in the the previous section. All the features of a generated 1024px*1024px image are determined solely by a 512-dimentional noise vector in the latent space (as a low-dimensional representation of the image content). Therefore, &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;if we could understand what the latent space represents (i.e., making it transparent), we could completely control the generation process&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*ChmB0i6NQGCpCyseOULISw.png&quot; data-width=&quot;700&quot; data-height=&quot;375&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*ChmB0i6NQGCpCyseOULISw.png&quot;/&gt;&lt;/div&gt;
Motivation of TL-GAN: understand latent space to control generation process
&lt;p name=&quot;75d2&quot; id=&quot;75d2&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;By experimenting with the pre-trained pg-GAN, I found that the latent space actually has two good properties:&lt;/p&gt;
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;230d&quot; id=&quot;230d&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;It is well populated, meaning that most points in the space will generate reasonable images&lt;/li&gt;
&lt;li name=&quot;fbe8&quot; id=&quot;fbe8&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;It is quite continuous, meaning the interpolation between two points in the latent space usually leads to a smooth transition of corresponding images.&lt;/li&gt;
&lt;/ul&gt;&lt;p name=&quot;7074&quot; id=&quot;7074&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;With this in mind, my intuition was that it is possible to find directions in the latent space that are predictive of features that we care about (eg. male-female). If so, we can use unit vectors of these directions as the feature axes for controlling the generation process (more male-like or more female-like).&lt;/p&gt;
&lt;h4 name=&quot;3ed5&quot; id=&quot;3ed5&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Approach: uncovering the feature axes&lt;/h4&gt;
&lt;p name=&quot;2e36&quot; id=&quot;2e36&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;To find these feature axes in the latent space, we will &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;build a link between a latent vector &lt;em class=&quot;markup--em markup--p-em&quot;&gt;z&lt;/em&gt; and the feature labels &lt;em class=&quot;markup--em markup--p-em&quot;&gt;y&lt;/em&gt;&lt;/strong&gt; through supervised learning methods trained on paired &lt;em class=&quot;markup--em markup--p-em&quot;&gt;(z,y)&lt;/em&gt; data. Now the problem becomes how to get such paired data, since existing datasets only contain images &lt;em class=&quot;markup--em markup--p-em&quot;&gt;x&lt;/em&gt; and their corresponding feature labels &lt;em class=&quot;markup--em markup--p-em&quot;&gt;y&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*-VLgZiCEkwPROEtZaWxb4w.png&quot; data-width=&quot;960&quot; data-height=&quot;540&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*-VLgZiCEkwPROEtZaWxb4w.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*-VLgZiCEkwPROEtZaWxb4w.png&quot;/&gt;&lt;/div&gt;
Figure: approaches to link latent vector z with feature label y
&lt;p name=&quot;2738&quot; id=&quot;2738&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;&lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;Potential approaches:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote name=&quot;1467&quot; id=&quot;1467&quot; class=&quot;graf graf--blockquote graf-after--p&quot; readability=&quot;9&quot;&gt;
&lt;p&gt;One potential approach is to compute the corresponding latent vectors &lt;em class=&quot;markup--em markup--blockquote-em&quot;&gt;z&lt;/em&gt; of images &lt;em class=&quot;markup--em markup--blockquote-em&quot;&gt;x_real&lt;/em&gt; from an existing dataset labeled with features of interest &lt;em class=&quot;markup--em markup--blockquote-em&quot;&gt;y_real&lt;/em&gt;. However, the GAN network does not offer an easy way to compute &lt;em class=&quot;markup--em markup--blockquote-em&quot;&gt;z_encode=G^(−1)(x_real)&lt;/em&gt;, making this idea difficult to implement.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote name=&quot;cb9e&quot; id=&quot;cb9e&quot; class=&quot;graf graf--blockquote graf-after--blockquote&quot; readability=&quot;8&quot;&gt;
&lt;p&gt;A second potential approach is to generate synthetic images &lt;em class=&quot;markup--em markup--blockquote-em&quot;&gt;x_gen&lt;/em&gt; using GAN from a random latent vector &lt;em class=&quot;markup--em markup--blockquote-em&quot;&gt;z&lt;/em&gt; as &lt;em class=&quot;markup--em markup--blockquote-em&quot;&gt;x_gen=G(z)&lt;/em&gt;. The problem here is that the synthetic images are unlabeled, and we can not easily leverage the available labeled dataset.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p name=&quot;baf7&quot; id=&quot;baf7&quot; class=&quot;graf graf--p graf-after--blockquote&quot;&gt;To solve this problem, the key innovation of our TL-GAN model is to &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;train a separate feature extractor&lt;/strong&gt; (a classifier for discrete label or regressor for continuous label) model &lt;em class=&quot;markup--em markup--p-em&quot;&gt;y=F(x)&lt;/em&gt; using an existing labelled image dataset &lt;em class=&quot;markup--em markup--p-em&quot;&gt;(x_real, y_real)&lt;/em&gt;, and then couple the trained GAN generator G with the feature extractor network F. Once this is done, we can predict the feature labels &lt;em class=&quot;markup--em markup--p-em&quot;&gt;y_pred&lt;/em&gt; of the synthetic images &lt;em class=&quot;markup--em markup--p-em&quot;&gt;x_gen&lt;/em&gt; using the trained feature extractor network, and thus establish the link between z and y through synthetic images as &lt;em class=&quot;markup--em markup--p-em&quot;&gt;x_gen=G(z)&lt;/em&gt; and &lt;em class=&quot;markup--em markup--p-em&quot;&gt;y_pred=F(x_gen)&lt;/em&gt;.&lt;/p&gt;
&lt;p name=&quot;8665&quot; id=&quot;8665&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Now that we have the paired latent vector and features, we can train a regressor model &lt;em class=&quot;markup--em markup--p-em&quot;&gt;y=A(z)&lt;/em&gt; to uncover all feature axes for controlling the image generation process.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*l6ug8cLOpd_TtxxgWlnw1Q.png&quot; data-width=&quot;1180&quot; data-height=&quot;479&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*l6ug8cLOpd_TtxxgWlnw1Q.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*l6ug8cLOpd_TtxxgWlnw1Q.png&quot;/&gt;&lt;/div&gt;
Figure: architecture of our TL-GAN model
&lt;p name=&quot;bdf0&quot; id=&quot;bdf0&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;The above figure shows the architecture of the TL-GAN model, which contains five steps:&lt;/p&gt;
&lt;ol class=&quot;postList&quot;&gt;&lt;li name=&quot;2da9&quot; id=&quot;2da9&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Learning the distribution:&lt;/strong&gt; Choose a well-trained GAN model and take the generator network. I chose the well-trained pg-GAN (provided by Nvidia), which offers the best face generation quality.&lt;/li&gt;
&lt;li name=&quot;ca62&quot; id=&quot;ca62&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Classification:&lt;/strong&gt; Choose a pre-trained feature extractor model (could be a convolutional neural network or other computer vision models), or train your own feature extractor network using a labelled dataset. I trained a simple convolutional neural network on the CelebA dataset (which contains 30,000+ face images with 40 labels each).&lt;/li&gt;
&lt;li name=&quot;e5da&quot; id=&quot;e5da&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Generation:&lt;/strong&gt; Generate a number of random latent vectors, pass through the trained GAN generator to produce synthetic images, then use a trained feature extractor to produce features for every image.&lt;/li&gt;
&lt;li name=&quot;f2cc&quot; id=&quot;f2cc&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Correlation:&lt;/strong&gt; Use a Generalized Linear Model (GLM) to perform regression between latent vectors and features. &lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;The regression slope becomes the feature axes.&lt;/strong&gt;&lt;/li&gt;
&lt;li name=&quot;6a3b&quot; id=&quot;6a3b&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;strong class=&quot;markup--strong markup--li-strong&quot;&gt;Exploration:&lt;/strong&gt; Start from one latent vector, move it along one or more feature axes, and examine how this affects the generated images.&lt;/li&gt;
&lt;/ol&gt;&lt;p name=&quot;19d4&quot; id=&quot;19d4&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;I made this process very efficient; once we have a pre-trained GAN model, identifying feature axes &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;only takes one hour&lt;/strong&gt; on a single-GPU machine. This is achieved by several engineering tricks including transfer learning, downsampling image size, pre-cache synthetic images, etc.&lt;/p&gt;
&lt;h3 name=&quot;7c7b&quot; id=&quot;7c7b&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Results&lt;/h3&gt;
&lt;p name=&quot;7304&quot; id=&quot;7304&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Let us see how this simple idea works.&lt;/p&gt;
&lt;h4 name=&quot;5994&quot; id=&quot;5994&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Moving latent vector along feature axes&lt;/h4&gt;
&lt;p name=&quot;2be7&quot; id=&quot;2be7&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;First, I tested whether the discovered feature axes can be used to control the corresponding feature of the generated image. To do this, I generated a random vector &lt;em class=&quot;markup--em markup--p-em&quot;&gt;z_0&lt;/em&gt; in the latent space of GAN, and produced a synthetic image &lt;em class=&quot;markup--em markup--p-em&quot;&gt;x_0&lt;/em&gt; by passing it through the generator network &lt;em class=&quot;markup--em markup--p-em&quot;&gt;x_0=G(z_0)&lt;/em&gt;. Next, I moved the latent vector along one feature axis &lt;em class=&quot;markup--em markup--p-em&quot;&gt;u&lt;/em&gt; (a unit vector in the latent space, say, corresponding to the gender of the face) by distance &lt;em class=&quot;markup--em markup--p-em&quot;&gt;λ&lt;/em&gt;, to a new location &lt;em class=&quot;markup--em markup--p-em&quot;&gt;x_1=x_0+λu&lt;/em&gt;, and generated a new image &lt;em class=&quot;markup--em markup--p-em&quot;&gt;x1=G(z1)&lt;/em&gt;. Ideally, the corresponding feature of the new image would be modified toward the expected direction.&lt;/p&gt;
&lt;p name=&quot;f52d&quot; id=&quot;f52d&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;The results of moving the latent space vector along several example feature axes (gender, age, etc) are shown below. This works surprisingly well! We can &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;smoothly morph&lt;/strong&gt; the image between male ←→ female, young←→old, etc.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*ow2h4j_v8jdnIJZzLcktlQ.png&quot; data-width=&quot;1176&quot; data-height=&quot;869&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*ow2h4j_v8jdnIJZzLcktlQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*ow2h4j_v8jdnIJZzLcktlQ.png&quot;/&gt;&lt;/div&gt;
Figure: Initial result of moving latent vector along example entangled feature axes
&lt;h4 name=&quot;431c&quot; id=&quot;431c&quot; class=&quot;graf graf--h4 graf-after--figure&quot;&gt;Disentangling correlated feature axes&lt;/h4&gt;
&lt;p name=&quot;0068&quot; id=&quot;0068&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;The examples above shows a shortcoming of the initial method, i.e., the &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;entangled feature axes&lt;/strong&gt;. For instance, when I intended to reduce the amount of beard, the generated faces became more female-like, which is not what a user would expect. This problem is due to the fact that the gender feature and the beard feature are by nature &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;correlated&lt;/strong&gt;, modifying one leading to the corollary change of the other. Similar things happened to other features like hairline and wavy hair. As is illustrated in the figure below, the original beard axis is not perpendicular to the gender axis in the latent space.&lt;/p&gt;
&lt;p name=&quot;1501&quot; id=&quot;1501&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;To solve this problem, I used straight-forward linear algebra tricks. Specifically, I projected the beard axis to a new direction that is orthogonal to the gender axis, which effectively removes their correlation, and thus could potentially disentangle these two features of generated face images.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*0RPW3Rvqy_uDAoUpVzhfOA.png&quot; data-width=&quot;660&quot; data-height=&quot;390&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*0RPW3Rvqy_uDAoUpVzhfOA.png&quot;/&gt;&lt;/div&gt;
Figure: disentangle correlated feature axes using linear algebra tricks
&lt;p name=&quot;b367&quot; id=&quot;b367&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;I applied this feature disentangling method to the same example face. This time I used gender axis and age axis as reference features, projected all other feature axes so that they became orthogonal to gender and age, and examined the generated images when the latent vector moved along the newly generated feature axes (shown in the figure below). As we expected, the features including hairline, wavy hair and beard now modify the gender of face anymore, which behaves as expected.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*3OlC0FsszJlkL9uYDV5haQ.png&quot; data-width=&quot;1176&quot; data-height=&quot;869&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*3OlC0FsszJlkL9uYDV5haQ.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*3OlC0FsszJlkL9uYDV5haQ.png&quot;/&gt;&lt;/div&gt;
Figure: Improved result of moving latent vector along example disentangled feature axes
&lt;h4 name=&quot;46d1&quot; id=&quot;46d1&quot; class=&quot;graf graf--h4 graf-after--figure&quot;&gt;Flexible interactive editing&lt;/h4&gt;
&lt;p name=&quot;a451&quot; id=&quot;a451&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;To see how flexibly our TL-GAN model can control the image generation process, I built an interactive GUI to explore the effect of gradually tuning feature values along different feature axes, as shown below:&lt;/p&gt;

Video: interactive editing of face images using TL-GAN
&lt;p name=&quot;86a0&quot; id=&quot;86a0&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Again, the model works surprisingly well when I use the features axes to control the generated images!&lt;/p&gt;
&lt;h3 name=&quot;c7eb&quot; id=&quot;c7eb&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Recap&lt;/h3&gt;
&lt;p name=&quot;a855&quot; id=&quot;a855&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;This project provides a novel method to control the generation process of an unsupervised generative model like GAN (generative adversarial network). Using an already well-trained GAN generator (Nvidia’s pg-GAN), I made its latent space transparent by discovering the meaningful feature axes in it. When a vector moves along a feature axis in the latent space, the corresponding image morphs over this feature, which enables controllable synthesis and edit.&lt;/p&gt;
&lt;p name=&quot;656c&quot; id=&quot;656c&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;This method has clear advantages:&lt;/p&gt;
&lt;ol class=&quot;postList&quot;&gt;&lt;li name=&quot;d361&quot; id=&quot;d361&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;Efficiency: To add a new feature tuner for the generator, you do not need to re-train the GAN model, thus it only takes &amp;lt;1h to add 40 feature tuners with our method.&lt;/li&gt;
&lt;li name=&quot;6efb&quot; id=&quot;6efb&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Flexibility: You can use any feature extractor trained on any dataset to add more feature tuners to the well-trained GAN.&lt;/li&gt;
&lt;/ol&gt;&lt;h4 name=&quot;b380&quot; id=&quot;b380&quot; class=&quot;graf graf--h4 graf-after--li&quot;&gt;A word about ethics&lt;/h4&gt;
&lt;p name=&quot;5fb8&quot; id=&quot;5fb8&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;This work allows us to have fine grained control over image generation, but it still relies heavily on the features of our dataset. Training on photos of celebrities of Hollywood means that our model will be very good at generating photos of a predominantly white and attractive demographic. In turn, this would lead to users only being able to generate these specific kind of faces which are only representative of a small subset of people. If we were to deploy this as an application, we would want to make sure that we have augmented our initial dataset to take into account the diversity of our users.&lt;/p&gt;
&lt;p name=&quot;5609&quot; id=&quot;5609&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;In addition, while this tool could be a huge creative help, we should ask ourselves how this model could be used for nefarious purposes. If we can generate realistic looking faces of any type, what are the implications for our ability to trust in what we see. These sort of issues are important to tackle today. As we have seen with the recent applications of &lt;a href=&quot;https://en.wikipedia.org/wiki/Deepfake&quot; data-href=&quot;https://en.wikipedia.org/wiki/Deepfake&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;deepfakes&lt;/a&gt;, the capability of AI methods is increasing at a fast pace, so it is vital for us to start conversations about how to best deploy them.&lt;/p&gt;
&lt;h3 name=&quot;1472&quot; id=&quot;1472&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Online demo and code&lt;/h3&gt;
&lt;p name=&quot;1cc4&quot; id=&quot;1cc4&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;All the code and online demo of this work are available at &lt;a href=&quot;https://github.com/SummitKwan/transparent_latent_gan&quot; data-href=&quot;https://github.com/SummitKwan/transparent_latent_gan&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;this Github project page&lt;/a&gt;.&lt;/p&gt;
&lt;h4 name=&quot;08b7&quot; id=&quot;08b7&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;If you want to play with the model in your web-browser&lt;/h4&gt;
&lt;p name=&quot;27e4&quot; id=&quot;27e4&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;You do not have to download any code, model or data. Just follow the instructions on &lt;a href=&quot;https://github.com/SummitKwan/transparent_latent_gan#1-instructions-on-the-online-demo&quot; data-href=&quot;https://github.com/SummitKwan/transparent_latent_gan#1-instructions-on-the-online-demo&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;this section&lt;/a&gt; of the GitHub README page. You can tweak faces in your web-browser as is demonstrated in the the above Video.&lt;/p&gt;
&lt;h4 name=&quot;58b3&quot; id=&quot;58b3&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;If you want to try my code&lt;/h4&gt;
&lt;p name=&quot;fb1e&quot; id=&quot;fb1e&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Just follow the README page of the GitHub repository. The code is built on Anaconda Python 3.6 with Tensorflow and Keras.&lt;/p&gt;
&lt;h4 name=&quot;dcbd&quot; id=&quot;dcbd&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;If you want to contribute&lt;/h4&gt;
&lt;p name=&quot;a7c9&quot; id=&quot;a7c9&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;You are very welcome! Feel free to submit a Pull Request or file an issue on Github&lt;/p&gt;
&lt;h4 name=&quot;dab4&quot; id=&quot;dab4&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;&lt;strong class=&quot;markup--strong markup--h4-strong&quot;&gt;About me&lt;/strong&gt;&lt;/h4&gt;
&lt;p name=&quot;9bf0&quot; id=&quot;9bf0&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;I recently finished PhD in computational and cognitive neuroscience from Brown University, with a concurrent master degree in computer science focusing on machine learning. In the past, I studied how neurons in the brain collectively process information to achieve high-level functions like visual perception. I am enthusiastic about the algorithmic approach to understand, mimic, and implement intelligence, as well as apply them to solve challenging real-world problems. I am actively looking for ML/AI researcher positions in the tech industry.&lt;/p&gt;
&lt;h4 name=&quot;af04&quot; id=&quot;af04&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Acknowledgements&lt;/h4&gt;
&lt;p name=&quot;0388&quot; id=&quot;0388&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;This work was done as the project for the &lt;a href=&quot;https://www.insightdata.ai/&quot; data-href=&quot;https://www.insightdata.ai/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Insight AI fellow program&lt;/a&gt; over three weeks. I give my thanks to the program director &lt;a href=&quot;https://twitter.com/EmmanuelAmeisen&quot; data-href=&quot;https://twitter.com/EmmanuelAmeisen&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Emmanuel Ameisen&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/mrubash1&quot; data-href=&quot;https://twitter.com/mrubash1&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Matt Rubashkin&lt;/a&gt; for their general guidance, especially &lt;a href=&quot;https://twitter.com/EmmanuelAmeisen&quot; data-href=&quot;https://twitter.com/EmmanuelAmeisen&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Emmanuel Ameisen&lt;/a&gt; for his suggestions and editing work on this blog post. I also give my thanks to all Insight staff for providing a great learning environment, and to Insight AI fellows from whom I learned a lot. Special thanks to &lt;a href=&quot;https://www.linkedin.com/in/ruobing-xia/&quot; data-href=&quot;https://www.linkedin.com/in/ruobing-xia/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Ruobing Xia&lt;/a&gt; for providing numerous inspirations as I decided on project directions, and for the enormous help structuring and editing this blog post.&lt;/p&gt;
&lt;p name=&quot;f25d&quot; id=&quot;f25d&quot; class=&quot;graf graf--p graf-after--p graf--trailing&quot;&gt;This passage was originally posted on my personal &lt;a href=&quot;http://summit.metaneuro.com/index.php/2018/10/10/draw-as-you-can-tell-controlled-image-synthesis-an/&quot; data-href=&quot;http://summit.metaneuro.com/index.php/2018/10/10/draw-as-you-can-tell-controlled-image-synthesis-an/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;
</description>
<pubDate>Fri, 26 Oct 2018 13:26:19 +0000</pubDate>
<dc:creator>homarp</dc:creator>
<og:title>Generating custom photo-realistic faces using AI – Insight Data</og:title>
<og:url>https://blog.insightdatascience.com/generating-custom-photo-realistic-faces-using-ai-d170b1b59255</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*lzQ-xViUbiJSA0Ytu9YTQQ.png</og:image>
<og:description>Controlled image synthesis and editing using a novel TL-GAN model</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.insightdatascience.com/generating-custom-photo-realistic-faces-using-ai-d170b1b59255?gi=f9480fd22db2</dc:identifier>
</item>
<item>
<title>Microsoft completes GitHub acquisition</title>
<link>https://blog.github.com/2018-10-26-github-and-microsoft/</link>
<guid isPermaLink="true" >https://blog.github.com/2018-10-26-github-and-microsoft/</guid>
<description>&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/98681/40890924-4bad5ce0-6732-11e8-9648-192aa71f0830.png&quot; alt=&quot;GitHub and Microsoft&quot;/&gt;&lt;/p&gt;
&lt;p&gt;I’m thrilled to share that the Microsoft acquisition of GitHub is complete. 🎉 Monday is my first day as CEO. Millions of people rely on GitHub every day, and I am honored by the opportunity to lead this company.&lt;/p&gt;
&lt;p&gt;When we announced the acquisition in June, I &lt;a href=&quot;https://natfriedman.github.io/hello/&quot;&gt;shared two principles&lt;/a&gt; for GitHub that are worth repeating:&lt;/p&gt;
&lt;ul readability=&quot;6.5&quot;&gt;&lt;li readability=&quot;9&quot;&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;GitHub will operate independently as a community, platform, and business.&lt;/strong&gt; This means that GitHub will retain its developer-first values, distinctive spirit, and open extensibility. We will always support developers in their choice of any language, license, tool, platform, or cloud.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li readability=&quot;4&quot;&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;GitHub will retain its product philosophy.&lt;/strong&gt; We love GitHub because of the deep care and thoughtfulness that goes into every facet of the developer’s experience. I understand and respect this, and know that we will continue to build tasteful, snappy, polished tools that developers love.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;Ultimately, my job is to make GitHub better for you.&lt;/p&gt;
&lt;p&gt;I’ve spent the past few months meeting with hundreds of developers as I prepared for this role, from maintainers to startups to large businesses. The passion for GitHub is amazing—both in the areas where we excel and in the areas where you want us to do more. I’ve learned a lot from these conversations, and listening to our customers will be a core part of how GitHub operates as a company.&lt;/p&gt;
&lt;p&gt;Three objectives will be top of mind for us as we build the future of GitHub:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Ensuring GitHub is the best place to run productive communities and teams&lt;/li&gt;
&lt;li&gt;Making GitHub accessible to more developers around the world&lt;/li&gt;
&lt;li&gt;Reliability, security, and performance&lt;/li&gt;
&lt;/ul&gt;&lt;p&gt;We will start by focusing on the daily experience of using GitHub and will double down on our &lt;a href=&quot;https://blog.github.com/2018-08-28-announcing-paper-cuts/&quot;&gt;paper cuts project&lt;/a&gt;. We will improve core scenarios like search, notifications, issues/projects, and our mobile experience. And of course we are excited to make GitHub Actions broadly available.&lt;/p&gt;
&lt;p&gt;We believe in the power of communities—that we can all achieve more when we collaborate with others. As the world’s largest developer community, GitHub brings together over 31 million developers to create, collaborate, share, and build on each other’s work.&lt;/p&gt;
&lt;p&gt;Our vision is to serve every developer on the planet, by being the best place to build software. This is a dream opportunity for all of us at GitHub, and we couldn’t be more excited to roll up our sleeves and start this next chapter.&lt;/p&gt;
</description>
<pubDate>Fri, 26 Oct 2018 13:05:28 +0000</pubDate>
<dc:creator>moritzplassnig</dc:creator>
<og:title>Pull request successfully merged. Starting build…</og:title>
<og:description>With the Microsoft acquisition of GitHub complete, Nat Friedman joins as CEO.</og:description>
<og:url>https://blog.github.com/2018-10-26-github-and-microsoft/</og:url>
<og:image>https://user-images.githubusercontent.com/98681/40890924-4bad5ce0-6732-11e8-9648-192aa71f0830.png</og:image>
<og:type>article</og:type>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.github.com/2018-10-26-github-and-microsoft/</dc:identifier>
</item>
<item>
<title>Not Lisp again (2009)</title>
<link>https://funcall.blogspot.com/2009/03/not-lisp-again.html</link>
<guid isPermaLink="true" >https://funcall.blogspot.com/2009/03/not-lisp-again.html</guid>
<description>It was 1983 and I was taking my first real computer course at MIT. Hal Abelson and Bill Siebert were co-lecturers. Hal was delivering the bad news:
&lt;blockquote&gt;
&lt;p&gt;“If you already know how to program, you may be at a disadvantage because you will have to unlearn some bad habits.”&lt;/p&gt;
&lt;/blockquote&gt;
Wonderful. I knew how to program (after all, I knew Macro-11, Z80 assembly and BASIC), and I could guess what the bad habits were. Violating the un-cuteness principle was probably one of them. Nothing fun is going to be allowed. (I rarely take notes at lectures. I'm too busy keeping an internal running commentary.)
&lt;blockquote&gt;
&lt;p&gt;“In this course we will be using the programming language Lisp...”&lt;/p&gt;
&lt;/blockquote&gt;
Argh! Not &lt;em&gt;that&lt;/em&gt; again! What is it with Lisp? Ok, maybe at Harvard they do that sort of thing, but this was MIT! Don't they hack computers here?&lt;p&gt;I was wondering what I had gotten myself in to. I held on to two small hopes. First, that the little amount of Lisp I had learned at Harvard would give me a head start over the other 250 students in the room. Second, that I only had to take a couple of computer classes to fulfill that part of the EECS requirement. I could stomach a bit of Lisp for a while, toe the company line, and be completely un-cute long enough to pass the course.&lt;/p&gt;&lt;p&gt;Hal put the archetypical Lisp program on the blackboard:
&lt;/p&gt;&lt;pre&gt;
(define (fact x)
  (if (zero? x)
      1
      (* x (fact (- x 1)))))
&lt;/pre&gt;
He walked the class through the substitution model:
&lt;pre&gt;
(fact 5)
(* 5 (fact 4))
(* 5 (* 4 (fact 3)))
(* 5 (* 4 (* 3 (fact 2))))
(* 5 (* 4 (* 3 (* 2 (fact 1)))))
(* 5 (* 4 (* 3 (* 2 (* 1 (fact 0))))))
(* 5 (* 4 (* 3 (* 2 (* 1 1)))))
(* 5 (* 4 (* 3 (* 2 1))))
(* 5 (* 4 (* 3 2)))
(* 5 (* 4 6))
(* 5 24)
120
&lt;/pre&gt;&lt;blockquote&gt;
&lt;p&gt;“This is a recursive process. At each step in the recursion we break the problem into smaller parts, solve each part separately, then combine the answers. In this example, each recursive call to fact uses a smaller number. Eventually we get to fact 0 which we know is 1. At this point we can start the multiplication.”&lt;/p&gt;
&lt;/blockquote&gt;
And then he said “This isn't very efficient.”&lt;p&gt;I thought that was an odd thing to say about a Lisp program. At Harvard, the obvious inefficiency of Lisp was downplayed. Here the professor was pointing it out and was going to do something about it.
&lt;/p&gt;&lt;blockquote&gt;
&lt;p&gt;“Look how the size of the problem depends upon the value of X. When X is five, there are five pending multiplications that have to be kept track of. If X were ten, then there would be ten pending multiplications. The computer is going to have to use up resources to keep track of these.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“What we want to do is to keep track of the product of the numbers we've seen so far. Like this:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;
  (define (fact x)

    (define (iter n accum)
      (if (zero? n)
          accum
          (iter (- n 1) (* n accum))))

    (iter x 1))

  (fact 5)
  (iter 5 1)
  (iter 4 5)
  (iter 3 20)
  (iter 2 60)
  (iter 1 120)
  (iter 0 120)
  120
&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;“This is an iterative process. Instead of breaking the problem into parts that we solve separately, we convert the problem into a simpler problem that has the same solution. The solution to (iter 5 1) is the same as the solution to (iter 4 5), which is the same as the solution to (iter 3 20) and so forth.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“Notice how in this case we aren't building a chain of pending multiplications. At each iteration, the value of n is being mutiplied by the accumulated product and this becomes the new accumulated product in the next iteration. When n reaches zero, we have in the accumulator the product of all numbers from n down to zero.&lt;/p&gt;
&lt;/blockquote&gt;
This was pretty obvious to me. However, I thought Hal was omitting an important point. Clearly each recursive call to iter had to be matched by a corresponding (implicit) return. Otherwise, how would the program know how to get back to the caller? Granted, there were no pending multiplications, but that doesn't mean we're not using up stack frames.&lt;p&gt;But then Hal said, “With an iterative process, the computer doesn't need to use up resources for each iteration. If you know a bit about programming, you might have expected that each iterative call, like each recursive call, would consume a small amount of memory and eventually run out. Our system is smart enough to notice the iteration and avoid doing that.”&lt;/p&gt;&lt;p&gt;Now &lt;em&gt;that&lt;/em&gt; caught my attention. I was impressed first by the fact that whoever designed this particular Lisp system cared about efficiency. I was impressed second by the fact that he was able to program the computer to automatically figure it out. I was impressed third that however it was that it worked, it had to be &lt;em&gt;really&lt;/em&gt; fast at it (or the cure would be worse than the problem).&lt;/p&gt;&lt;p&gt;Frankly, I had my doubts. I thought that &lt;em&gt;maybe&lt;/em&gt; the computer was able to figure this out &lt;em&gt;some&lt;/em&gt; of the time, or &lt;em&gt;maybe&lt;/em&gt; it was able to substantially reduce, but not totally eliminate, stack allocation. Or maybe this was some sleight of hand: the resources are allocated somewhere else. I tried to figure out how I'd do this in assembly code, but I was baffled. I made a note to myself to check into this.&lt;/p&gt;&lt;p&gt;Hal went on to introduce first-class procedures. I didn't see the rationale for these. I mean, you can't call a function that hasn't been written, and if it has been written then why not just call it directly and avoid the run-around?&lt;/p&gt;&lt;p&gt;Hal said “You're all familiar with the derivative operator.” and he wrote it on the blackboard:
&lt;/p&gt;&lt;pre&gt;
                               f (x + dx) - f (x)
       D f(x) = f'(x) = lim  --------------------
                                      dx
&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;“We can program this as follows:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;
    (define dx .0001)

    (define (deriv f)
   
       (define (f-prime x)
         (/ (- (f (+ x dx)) (f x))
            dx))

       f-prime)
&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;“Notice how our formula for the derivate translates directly into the lisp code.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“Let's take the derivative of the cube function:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;
    (define (cube x) (* x x x))

    (define cube-deriv (deriv cube))
&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;“And this new function should act like the derivative of the cube function.”&lt;/p&gt;
&lt;/blockquote&gt;
He put up a slide on the overhead project that showed the above code and the output from the computer:
&lt;pre&gt;
(cube-deriv 2)
;Value: 12.000600010022566

(cube-deriv 3)
;Value: 27.00090001006572

(cube-deriv 4)
;Value: 48.00120000993502
&lt;/pre&gt;
Sure enough, that's reasonably close to 3x^2.&lt;p&gt;I was floored. Here we take the simple, straightforward definition of the derivative of a function. Type it in with essentially no modification (we're a little cavalier about dropping the limit and just defining dx to be a ‘reasonably small number’) and suddenly the computer knew how to do calculus! This was serious magic. I had never seen anything like it before.&lt;/p&gt;&lt;p&gt;Hal went on to explain how the substitution model of evaluation worked in this example and I carefully watched. It seemed simple. It couldn't be &lt;em&gt;that&lt;/em&gt; simple, though, could it? Something must be missing. I had to find out.&lt;/p&gt;&lt;p&gt;As soon as the lecture ended I ran to the Scheme lab. Hewlett Packard had donated several dozen HP 9836 Chipmunks to MIT. Each one was equiped with a blindingly fast 8-MHz 68000 processor. On top of each workstation sat a fairly sizable expansion box that held an additional 16 megabytes of RAM. A student had sole use of an entire machine while working on problem sets. No time-sharing! It was an incredibly lavish setup.&lt;/p&gt;&lt;p&gt;The lab was empty (go figure, everyone had just gotten out of the lecture). I carefully typed in the examples from the lecture and tried them out. I tried the recursive factorial and managed to blow out the stack with a fairly big number, so I knew there was indeed a stack limit and how the machine behaved when you hit it. I removed the multiply from the iterative factorial to avoid making huge numbers and gave it some ridiculously large argument. The machine churned and churned and eventually returned an answer. I gave it a bigger number. A lot more churning, but no sign of stack overflow. I guess that really worked.&lt;/p&gt;&lt;p&gt;Then I typed in the derivative example from the lecture. Sure enough it worked just like Hal said. I tried passing in a few other functions. They worked, too. I did the homework exercises for fun. They just worked.&lt;/p&gt;&lt;p&gt;This was computing on a whole new level. I thought I knew how to program. I didn't know &lt;em&gt;anything&lt;/em&gt; like this. I needed to find out whether there was any more of this magic, and exactly how it worked.&lt;/p&gt;&lt;p&gt;My prior objections to Lisp were suddenly far less relevant.
&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Slower than assembly? Maybe for table lookups, but who cares about something as mundane as that? I want to do magic. If I have to look up something in a table, maybe I'll use assembly code.&lt;/li&gt;
&lt;li&gt;Harder to understand? No way. Look at the derivative example. Five lines of code to express the basis of differential calculus. Five lines of assembly code can barely call a subroutine. (As the proud owner of an HP calculator, I had no problem whatsoever with prefix notation.)&lt;/li&gt;
&lt;li&gt;Less efficient? I really didn't know, but it was clear that someone had put some serious thought into it and found some interesting solutions.&lt;/li&gt;
&lt;li&gt;Less capable? Dude, it's doing calculus!&lt;/li&gt;
&lt;/ul&gt;
Well, the initial lecture in computer science was a lot more fun and interesting than I expected, but of course I knew they were trying to ‘sell’ comp. sci. as a career. I was pretty sure that this example was a hook and that the tedium would soon follow, perhaps even in the next lecture.

</description>
<pubDate>Fri, 26 Oct 2018 12:53:45 +0000</pubDate>
<dc:creator>kamaal</dc:creator>
<og:url>http://funcall.blogspot.com/2009/03/not-lisp-again.html</og:url>
<og:title>Not Lisp again....</og:title>
<og:description>A blog about computers, functional languages, Lisp, and Scheme.</og:description>
<dc:format>text/html</dc:format>
<dc:identifier>https://funcall.blogspot.com/2009/03/not-lisp-again.html</dc:identifier>
</item>
<item>
<title>WebRender is in beta</title>
<link>https://mozillagfx.wordpress.com/2018/10/26/webrender-newsletter-27/</link>
<guid isPermaLink="true" >https://mozillagfx.wordpress.com/2018/10/26/webrender-newsletter-27/</guid>
<description>&lt;p&gt;🎉 WebRender is in beta 🎉! There are still a number of blocking bugs so WebRender will stay on beta for a few trains until it has received enough polish to hit the release population. This is an important milestone for everyone working on the project and the main piece of news outside of the bullet points below.&lt;/p&gt;
&lt;p&gt;I’m increasingly  running out of ideas to write intros without repeating the same thing each week. So instead I’ll start the next few newsletter with a piece of WebRender history. Here is one:&lt;/p&gt;
&lt;p&gt;Towards the beginning, WebRender’s overall architecture really felt centered around attempt at answering the question “Can we implement CSS rendering logic directly on the GPU?”. By that I mean that WebRender had a collection of shaders that &lt;em&gt;very&lt;/em&gt; closely matched CSS properties. For example a single image shader was able to handle all of the image and background-image properties, and a single border shader was able to do all of the different border styles, parameters being provided in layout space instead of device space.&lt;br/&gt;This maybe doesn’t sound like much, but for someone who’s been used to seeing layers upon layers of abstractions between the output of layout and the final pieces of graphics code that writes into the window, this idea of implementing the CSS specification directly into shaders in a fairly straightforward way was quite remarkable and novel.&lt;br/&gt;In today’s WebRender the shader system isn’t as close to a verbatim implementation of CSS specifications as it used to be. A lot of this “low level CSS” vibe remains but we also split and combine shaders in ways that better take advantage of the characteristics of modern GPUs.&lt;br/&gt;To me, this ability to solve specific web rendering challenges in the high and low level layers alike without having to conform to old rendering models is one of WebRender’s greatest strengths.&lt;/p&gt;
&lt;h2&gt;Notable WebRender and Gecko changes&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;Bobby implemented a more efficient sizing logic for render targets to save GPU memory.&lt;/li&gt;
&lt;li&gt;Bobby fixed a crash.&lt;/li&gt;
&lt;li&gt;Dan &lt;a href=&quot;https://bugzil.la/1490007&quot;&gt;prevented&lt;/a&gt; enormous box shadows from crashing the renderer.&lt;/li&gt;
&lt;li&gt;Kats improved the tooling to synchronize between the WebRender and Gecko repositories.&lt;/li&gt;
&lt;li&gt;Kats and Jamie got WebRender in Firefox for Android to a point where it doesn’t just crash at startup.&lt;/li&gt;
&lt;li&gt;Kvark &lt;a href=&quot;https://github.com/servo/webrender/pull/3131&quot;&gt;fixed&lt;/a&gt; the backface visibility bugs.&lt;/li&gt;
&lt;li&gt;Kvark &lt;a href=&quot;https://github.com/servo/plane-split/pull/24&quot;&gt;fixed&lt;/a&gt; z-fighting glitch with 3d transforms.&lt;/li&gt;
&lt;li&gt;Matt fixed some tiled blob image bugs.&lt;/li&gt;
&lt;li&gt;Andrew landed the first half of the animated image recycling work.&lt;/li&gt;
&lt;li&gt;Andrew &lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=1471671&quot;&gt;fixed&lt;/a&gt; a crash&lt;/li&gt;
&lt;li&gt;Lee &lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=1495919&quot;&gt;fixed&lt;/a&gt; a font leak on Windows.&lt;/li&gt;
&lt;li&gt;Glenn &lt;a href=&quot;https://github.com/servo/webrender/pull/3219&quot;&gt;worked around&lt;/a&gt; a clip mask bug with blob images.&lt;/li&gt;
&lt;li&gt;Glenn landed a series of incremental changes towards picture caching: &lt;a href=&quot;https://github.com/servo/webrender/pull/3213&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://github.com/servo/webrender/pull/3218&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;https://github.com/servo/webrender/pull/3221&quot;&gt;3&lt;/a&gt;, &lt;a href=&quot;https://github.com/servo/webrender/pull/3224&quot;&gt;4&lt;/a&gt;, &lt;a href=&quot;https://github.com/servo/webrender/pull/3228&quot;&gt;5&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Emilio &lt;a href=&quot;https://github.com/servo/webrender/pull/3220&quot;&gt;implemented&lt;/a&gt; proper support for using tiled images as clip masks.&lt;/li&gt;
&lt;li&gt;Sotaro &lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=1500520&quot;&gt;fixed&lt;/a&gt; a texture corruption issue after resuming from sleep on Linux with proprietary nVidia drivers.&lt;/li&gt;
&lt;li&gt;Sotaro &lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=1499002&quot;&gt;fixed&lt;/a&gt; a flickering issue at startup on Windows.&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Ongoing work&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;Glenn is making progress towards picture caching.&lt;/li&gt;
&lt;li&gt;Doug is getting close to completing the document splitting work.&lt;/li&gt;
&lt;li&gt;Nical is getting a subset of SVG filters to run on the GPU instead of the CPU fallback.&lt;/li&gt;
&lt;li&gt;Bobby never stops improving memory usage.&lt;/li&gt;
&lt;li&gt;Matt and Gankro are improving the interaction between blob images and scrolling.&lt;/li&gt;
&lt;li&gt;Kats is standing up WebRender in Firefox for Android.&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Enabling WebRender in Firefox Nightly&lt;/h2&gt;
&lt;ul&gt;&lt;li&gt;In about:config set “gfx.webrender.all” to true,&lt;/li&gt;
&lt;li&gt;restart Firefox.&lt;/li&gt;
&lt;/ul&gt;&lt;div id=&quot;jp-post-flair&quot; class=&quot;sharedaddy sd-like-enabled sd-sharing-enabled&quot;&gt;
&lt;div class=&quot;sharedaddy sd-sharing-enabled&quot;&gt;
&lt;div class=&quot;robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing&quot;&gt;
&lt;h3 class=&quot;sd-title&quot;&gt;Share this:&lt;/h3&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded&quot; id=&quot;like-post-wrapper-40065927-1085-5bd41b613e916&quot; data-src=&quot;//widgets.wp.com/likes/index.html?ver=20180319#blog_id=40065927&amp;amp;post_id=1085&amp;amp;origin=mozillagfx.wordpress.com&amp;amp;obj_id=40065927-1085-5bd41b613e916&quot; data-name=&quot;like-post-frame-40065927-1085-5bd41b613e916&quot;&gt;
&lt;h3 class=&quot;sd-title&quot;&gt;Like this:&lt;/h3&gt;
&lt;div class=&quot;likes-widget-placeholder post-likes-widget-placeholder&quot;&gt;&lt;span class=&quot;button&quot;&gt;&lt;span&gt;Like&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;loading&quot;&gt;Loading...&lt;/span&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
<pubDate>Fri, 26 Oct 2018 12:34:57 +0000</pubDate>
<dc:creator>bpierre</dc:creator>
<og:type>article</og:type>
<og:title>WebRender newsletter #27</og:title>
<og:url>https://mozillagfx.wordpress.com/2018/10/26/webrender-newsletter-27/</og:url>
<og:description>🎉 WebRender is in beta 🎉! There are still a number of blocking bugs so WebRender will stay on beta for a few trains until it has received enough polish to hit the release population. This is an imp…</og:description>
<og:image>https://s0.wp.com/i/blank.jpg</og:image>
<dc:language>en</dc:language>
<dc:format>text/html</dc:format>
<dc:identifier>https://mozillagfx.wordpress.com/2018/10/26/webrender-newsletter-27/</dc:identifier>
</item>
<item>
<title>Two years of Elixir at The Outline</title>
<link>https://blog.usejournal.com/two-years-of-elixir-at-the-outline-ad671a56c9ce</link>
<guid isPermaLink="true" >https://blog.usejournal.com/two-years-of-elixir-at-the-outline-ad671a56c9ce</guid>
<description>&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*VKigG-7FHOXNy3l5yfLggg.png&quot; data-width=&quot;1098&quot; data-height=&quot;531&quot; data-action=&quot;zoom&quot; data-action-value=&quot;1*VKigG-7FHOXNy3l5yfLggg.png&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*VKigG-7FHOXNy3l5yfLggg.png&quot;/&gt;&lt;/div&gt;
23k LOC of Elixir in Production: What We Learned

&lt;p name=&quot;4e7b&quot; id=&quot;4e7b&quot; class=&quot;graf graf--p graf-after--h3&quot;&gt;Two years ago this month, I started as a developer at &lt;a href=&quot;https://theoutline.com/post/420/welcome-to-the-outline?zd=1&amp;amp;zi=adibmhqc&quot; data-href=&quot;https://theoutline.com/post/420/welcome-to-the-outline?zd=1&amp;amp;zi=adibmhqc&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;The Outline&lt;/a&gt;. At the time, the site was just an idea that existed as a series of design mock ups and small prototypes. We had just three months to build a news website with some pretty ambitious design goals, as well as a CMS to create the visually expressive content that the designs demanded. We chose Elixir and Phoenix as the foundation of our website after being attracted to its concurrency model, reliability, and ergonomics.&lt;/p&gt;
&lt;p name=&quot;007f&quot; id=&quot;007f&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Over this time, I have gained a major appreciation for Elixir, not only for the productivity it affords me, but of the business opportunities it has opened up for us. In these past two years, Elixir has gone from 1.3 to 1.7, and great improvements have been introduced by the core team:&lt;/p&gt;
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;2d90&quot; id=&quot;2d90&quot; class=&quot;graf graf--li graf-after--p&quot;&gt;GenStage / Flow&lt;/li&gt;
&lt;li name=&quot;27b4&quot; id=&quot;27b4&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;mix format&lt;/li&gt;
&lt;li name=&quot;251f&quot; id=&quot;251f&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Registry&lt;/li&gt;
&lt;li name=&quot;6b25&quot; id=&quot;6b25&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Syntax highlighting&lt;/li&gt;
&lt;li name=&quot;6519&quot; id=&quot;6519&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;IEx debugging enhancements&lt;/li&gt;
&lt;li name=&quot;cad9&quot; id=&quot;cad9&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Exception.Blame and other stack trace improvements&lt;/li&gt;
&lt;li name=&quot;a7fa&quot; id=&quot;a7fa&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;Dynamic Supervisor&lt;/li&gt;
&lt;/ul&gt;&lt;p name=&quot;7c8b&quot; id=&quot;7c8b&quot; class=&quot;graf graf--p graf-after--li&quot;&gt;As I reach this two year mark, I thought others might benefit from an explanation of why I love Elixir so much after two years, what I still struggle with, and some beginner mistakes that I made early on.&lt;/p&gt;
&lt;h3 name=&quot;7a2a&quot; id=&quot;7a2a&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Highlights&lt;/h3&gt;
&lt;h4 name=&quot;5843&quot; id=&quot;5843&quot; class=&quot;graf graf--h4 graf-after--h3&quot;&gt;Elixir is really fast.&lt;/h4&gt;
&lt;p name=&quot;ae78&quot; id=&quot;ae78&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;90ms is a 90th percentile response time on The Outline. Our post page route is even faster! We got this performance out of the box, without really any tuning or fine-grained optimizations. For other routes that do not hit the database, we see response times measures in microseconds. This speed allows us to build features that I wouldn’t have &lt;a href=&quot;https://robots.thoughtbot.com/how-we-replaced-react-with-phoenix&quot; data-href=&quot;https://robots.thoughtbot.com/how-we-replaced-react-with-phoenix&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;even considered possible&lt;/a&gt; in other languages.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;0*F12Dx8SdXo1N1Jan&quot; data-width=&quot;1030&quot; data-height=&quot;192&quot; data-action=&quot;zoom&quot; data-action-value=&quot;0*F12Dx8SdXo1N1Jan&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/0*F12Dx8SdXo1N1Jan&quot;/&gt;&lt;/div&gt;
&lt;h4 name=&quot;8e3c&quot; id=&quot;8e3c&quot; class=&quot;graf graf--h4 graf-after--figure&quot;&gt;Caching? Who needs it!&lt;/h4&gt;
&lt;p name=&quot;0e75&quot; id=&quot;0e75&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Elixir is so fast that we haven’t had much need for CDN or service level caching. It’s been a luxury to not have to spend time debugging caching issues between Redis and memcached, which are issues that have kept me up into the wee hours of the morning in past roles. The lack of public cache opens up the path for dynamic content and user-based personalization on initial page load.&lt;/p&gt;
&lt;p name=&quot;5de7&quot; id=&quot;5de7&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;While we don’t cache routes at the CDN, we do cache some expensive database queries. For that we use light in-memory caching via &lt;a href=&quot;https://github.com/sasa1977/con_cache&quot; data-href=&quot;https://github.com/sasa1977/con_cache&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;ConCache&lt;/a&gt;, a wonderful library by Saša Jurić.&lt;/p&gt;
&lt;h4 name=&quot;9db3&quot; id=&quot;9db3&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Server render your HTML with Phoenix&lt;/h4&gt;
&lt;p name=&quot;00eb&quot; id=&quot;00eb&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;It seems like people get started with Phoenix writing JSON apis, and leave the HTML to Preact and other front end frameworks. A lot of the raw site performance we get from Elixir and Phoenix is from its ability to render HTML extremely quickly, on the order of microseconds. Phoenix allows us to have really fast server-rendered pages, and then we let Javascript kick in to add dynamic features. Before reaching for Vue.js or Svelte, consider going old school and rendering your HTML on the server; you might be delighted.&lt;/p&gt;
&lt;h4 name=&quot;534d&quot; id=&quot;534d&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;ExUnit is great&lt;/h4&gt;
&lt;p name=&quot;ae79&quot; id=&quot;ae79&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;ExUnit gives you so much out of the box. In most of the other languages that I’ve used, testing frameworks are third-party, and setup is often a pain. ExUnit comes bundled with a code coverage tool, and its &lt;a href=&quot;https://github.com/elixir-lang/elixir/pull/4703&quot; data-href=&quot;https://github.com/elixir-lang/elixir/pull/4703&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;assertion diffs&lt;/a&gt; keep &lt;a href=&quot;https://elixir-lang.org/blog/2018/07/25/elixir-v1-7-0-released/&quot; data-href=&quot;https://elixir-lang.org/blog/2018/07/25/elixir-v1-7-0-released/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;improving&lt;/a&gt;! Not only that, you can &lt;code class=&quot;markup--code markup--p-code&quot;&gt;mix test --slowest&lt;/code&gt;to find your slowest tests, or &lt;code class=&quot;markup--code markup--p-code&quot;&gt;mix test --failed&lt;/code&gt;to rerun only the tests that failed the last run.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;img class=&quot;graf-image&quot; data-image-id=&quot;0*eGz2tI05wKDX6cd-&quot; data-width=&quot;648&quot; data-height=&quot;566&quot; data-action=&quot;zoom&quot; data-action-value=&quot;0*eGz2tI05wKDX6cd-&quot; src=&quot;https://cdn-images-1.medium.com/max/1200/0*eGz2tI05wKDX6cd-&quot;/&gt;&lt;/div&gt;
&lt;p name=&quot;1ee6&quot; id=&quot;1ee6&quot; class=&quot;graf graf--p graf-after--figure&quot;&gt;Doctests are easily my favorite part of ExUnit. For the uninitiated, &lt;a href=&quot;https://hexdocs.pm/ex_unit/ExUnit.DocTest.html#content&quot; data-href=&quot;https://hexdocs.pm/ex_unit/ExUnit.DocTest.html#content&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;doctests&lt;/a&gt; are tests that you write inline in your documentation. They get compiled and run when you do mix test. The power here is two-fold; you get code examples right next to the definition of your code and you know that the examples work.&lt;/p&gt;
&lt;h4 name=&quot;545f&quot; id=&quot;545f&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;First class documentation is something I take for granted&lt;/h4&gt;
&lt;p name=&quot;0371&quot; id=&quot;0371&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Having a consistent way to read docs across packages makes things really easy to find. I spent some time taking a data science and machine learning course in Python last month, and I realized exactly how spoiled I’ve been with Elixir documentation. It’s hard to measure the value of a consistent, familiar, and pervasive documentation system. The latest distillery release excepted, every Elixir library’s documentation has the same look and feel. My favorite part about Elixir documentation is the link right back to the source code. The way I usually read documentation is by trying to understand it through the text, and the if something isn’t clear I click the link to the source code and follow the code directly.&lt;/p&gt;
&lt;h4 name=&quot;877a&quot; id=&quot;877a&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Phoenix Channels are such a great abstraction over WebSockets&lt;/h4&gt;
&lt;p name=&quot;cd5f&quot; id=&quot;cd5f&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Think of Phoenix Channels as controllers for Websockets. The socket registers topics which are analogous to a router. At The Outline, we were able to remove thousands of lines of JavaScript by moving code into the Channel. Moving mutable JavaScript into Elixir was a great feeling. It’s always been our goal to ship as little code to the client as possible, and keeping user state in Channels facilitates that in a way that I would not have considered if I was using Node.js or Ruby. The memory overhead of channels has been relatively low, and we didn’t need to make any changes to our infrastructure to support them.&lt;/p&gt;
&lt;h4 name=&quot;d2ca&quot; id=&quot;d2ca&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;The Community is wonderful!&lt;/h4&gt;
&lt;p name=&quot;bcd1&quot; id=&quot;bcd1&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Elixir has been a friendly and helpful community these past two years. I’ve received a ton of advice on the Elixir Slack channel when I’ve asked for help. I’ve also enjoyed attending and speaking at the &lt;a href=&quot;https://www.meetup.com/NYC-Elixir&quot; data-href=&quot;https://www.meetup.com/NYC-Elixir&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;NYC Elixir Meetup&lt;/a&gt;, as well as the &lt;a href=&quot;http://empex.co/&quot; data-href=&quot;http://empex.co/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Empex and Empex West conferences&lt;/a&gt;. I’ve met some great people through these events, including several leaders in the community, and I hope to meet more passionate people in the future!&lt;/p&gt;
&lt;p name=&quot;0dc5&quot; id=&quot;0dc5&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;I’d like to also call out both the &lt;a href=&quot;https://soundcloud.com/elixirtalk&quot; data-href=&quot;https://soundcloud.com/elixirtalk&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;ElixirTalk&lt;/a&gt; (hi Chris and Desmond) and &lt;a href=&quot;https://elixiroutlaws.com/&quot; data-href=&quot;https://elixiroutlaws.com/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Elixir Outlaws&lt;/a&gt; podcasts, which are fantastic and do a really great job of breaking down interesting problems in the ecosystem.&lt;/p&gt;
&lt;h3 name=&quot;bce3&quot; id=&quot;bce3&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Things I still struggle with&lt;/h3&gt;
&lt;h4 name=&quot;da07&quot; id=&quot;da07&quot; class=&quot;graf graf--h4 graf-after--h3&quot;&gt;Stack traces are not always the best&lt;/h4&gt;
&lt;p name=&quot;2c97&quot; id=&quot;2c97&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Sometimes you change a line in a controller or a view, and you end up with a stack trace in your 1000 line module that starts at line 1. The problem? Meta-programming! Despite all the great things that meta-programming gives us in terms of ergonomics, its makes certain types of exceptions really hard to pinpoint. Luckily, not all stack traces are this way, but it can be extremely frustrating when the stack trace leaves you empty handed.&lt;/p&gt;
&lt;h4 name=&quot;3c44&quot; id=&quot;3c44&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Sometimes tests throw random warnings that I don’t know how to trace&lt;/h4&gt;
&lt;p name=&quot;b559&quot; id=&quot;b559&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Asynchronous and concurrent code is notoriously hard to debug. What’s harder to debug is asynchronous and concurrent code that you haven’t written. We have some lingering error messages that get printed during random test runs. Attempts to debug them have been futile, so they appear to be &lt;a href=&quot;https://en.wikipedia.org/wiki/Heisenbug&quot; data-href=&quot;https://en.wikipedia.org/wiki/Heisenbug&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;heisenbugs&lt;/a&gt;. I have a suspicion that our particular issue is with Phoenix Channels and Ecto Sandbox mode, but I haven’t quite narrowed it down. Please let me know if you have!&lt;/p&gt;
&lt;h4 name=&quot;e096&quot; id=&quot;e096&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Working with Ecto Associations are still hard&lt;/h4&gt;
&lt;p name=&quot;ba33&quot; id=&quot;ba33&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;While I’m really comfortable working with changesets and writing join queries in Ecto, breaking down my code for associations is still hard. Its pretty straightforward when dealing with simple associations, but when you have a data model that involves multiple entities, and you want to create new entities while associating them to existing entities, some things break down for me.&lt;/p&gt;
&lt;p name=&quot;05fe&quot; id=&quot;05fe&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;What still does not feel natural to me is where to place code that deals with the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;put_assoc&lt;/code&gt; and &lt;code class=&quot;markup--code markup--p-code&quot;&gt;cast_assoc&lt;/code&gt; family of functions. My first tendency would be to put it in the &lt;code class=&quot;markup--code markup--p-code&quot;&gt;changeset/2&lt;/code&gt; function in the schema, but you do not always want that logic. Of course, you can have multiple changeset functions, but I haven’t found the right balance for that either. What I’ve started doing is moving association code outside of the schema and changeset, and into the bounded context thats building the association.&lt;/p&gt;
&lt;div class=&quot;aspectRatioPlaceholder is-locked&quot;&gt;

&lt;a href=&quot;https://blog.usejournal.com/meet-journal-d222fce8db1d&quot; data-href=&quot;https://blog.usejournal.com/meet-journal-d222fce8db1d&quot; class=&quot;graf-imageAnchor&quot; data-action=&quot;image-link&quot; data-action-observe-only=&quot;true&quot;&gt;&lt;img class=&quot;graf-image&quot; data-image-id=&quot;1*PMBwoU5bWkso8M2A1ocS9Q.png&quot; data-width=&quot;2000&quot; data-height=&quot;192&quot; src=&quot;https://cdn-images-1.medium.com/max/1600/1*PMBwoU5bWkso8M2A1ocS9Q.png&quot;/&gt;&lt;/a&gt;&lt;/div&gt;
&lt;h3 name=&quot;56bc&quot; id=&quot;56bc&quot; class=&quot;graf graf--h3 graf-after--figure&quot;&gt;Beginner mistakes I made early on&lt;/h3&gt;
&lt;h4 name=&quot;a61e&quot; id=&quot;a61e&quot; class=&quot;graf graf--h4 graf-after--h3&quot;&gt;Pattern matching all the things&lt;/h4&gt;
&lt;p name=&quot;45ca&quot; id=&quot;45ca&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;What really drew me into Elixir at first was how wonderful it felt to pattern match in function heads. The utility of multiple function heads, &lt;code class=&quot;markup--code markup--p-code&quot;&gt;if&lt;/code&gt; as an expression rather than a statement, and immutable data structures had me hooked really fast (especially coming from Javascript).&lt;/p&gt;
&lt;p name=&quot;f3d9&quot; id=&quot;f3d9&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;What ended up happening is that I would pattern match at &lt;em class=&quot;markup--em markup--p-em&quot;&gt;every single opportunity.&lt;/em&gt; Without a static type system, pattern matching felt like a friendlier replacement, and I wanted to make use of it at every corner. The problem is that it’s &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;not&lt;/strong&gt; a type system, and using it as such has drawbacks that are not immediately obvious until you write a certain amount of Elixir code. When you pattern-match gratuitously, you over-specify your code, and you miss opportunities to apply generic code to wider domains, and make that code more difficult to refactor in the future.&lt;/p&gt;
&lt;p name=&quot;2c99&quot; id=&quot;2c99&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;While my love of pattern-matching has not gone away, it has become clearer to me &lt;strong class=&quot;markup--strong markup--p-strong&quot;&gt;when&lt;/strong&gt; to pattern-match, and more importantly, what level of specificity should I pattern match on. Do I need to pattern-match on this struct, or will a map suffice? Does this private function need to pattern match it arguments when the shape is already clear in its only caller? These nuances become clearer as you write more code, and deciding when and when not to pattern-match is a matter of preference and style.&lt;/p&gt;
&lt;h4 name=&quot;76e1&quot; id=&quot;76e1&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Trimming data too early in Phoenix Templates.&lt;/h4&gt;
&lt;p name=&quot;72b5&quot; id=&quot;72b5&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;This is a problem that’s closely related with the desire to pattern-match. Once you start rendering more than the Hello World example of Phoenix, you’re gonna have to start passing data through nested views and templates to fully render a page. When you start passing data down, tend towards being additive rather than regressive.&lt;/p&gt;
&lt;pre name=&quot;44bb&quot; id=&quot;44bb&quot; class=&quot;graf graf--pre graf-after--p&quot;&gt;
# Here we’re possibly over pattern matching and over specifying.&lt;br/&gt;# If we want to pass more data down in the future, we have to&lt;br/&gt;# change this function in addition to its caller
&lt;/pre&gt;
&lt;pre name=&quot;75b2&quot; id=&quot;75b2&quot; class=&quot;graf graf--pre graf-after--pre&quot;&gt;
def render(“parent.html”, %{content: content}) do&lt;br/&gt;render(“child.html”, %{content: content, extra: data})&lt;br/&gt;end
&lt;/pre&gt;
&lt;pre name=&quot;4d03&quot; id=&quot;4d03&quot; class=&quot;graf graf--pre graf-after--pre&quot;&gt;
# This way is less restrictive, and makes maintenance easier&lt;br/&gt;# in the future if we decide to pass more data
&lt;/pre&gt;
&lt;pre name=&quot;1598&quot; id=&quot;1598&quot; class=&quot;graf graf--pre graf-after--pre&quot;&gt;
def render(“parent.html”, params) do&lt;br/&gt;render(“child.html”, Map.put(params, :extra, :data))&lt;br/&gt;end
&lt;/pre&gt;
&lt;h4 name=&quot;28f4&quot; id=&quot;28f4&quot; class=&quot;graf graf--h4 graf-after--pre&quot;&gt;You usually don’t need a GenServer!&lt;/h4&gt;
&lt;p name=&quot;26cc&quot; id=&quot;26cc&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;When starting to learn about Elixir / Erlang, it’s so tempting to start writing GenServers, Tasks, processes, etc for the problem at hand. Before you do, please read Saša’s &lt;a href=&quot;https://www.theerlangelist.com/article/spawn_or_not&quot; data-href=&quot;https://www.theerlangelist.com/article/spawn_or_not&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;To spawn, or not to spawn?&lt;/a&gt;, which breaks down when you should reach for processes and when modules / functions are good enough.&lt;/p&gt;
&lt;h4 name=&quot;7f1d&quot; id=&quot;7f1d&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Know when to leverage protocols&lt;/h4&gt;
&lt;p name=&quot;738b&quot; id=&quot;738b&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;Knowing when to implement a protocol, such as Phoenix’s &lt;a href=&quot;https://hexdocs.pm/phoenix_html/Phoenix.HTML.Safe.html&quot; data-href=&quot;https://hexdocs.pm/phoenix_html/Phoenix.HTML.Safe.html&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;HTML.Safe protocol&lt;/a&gt;, can be extremely powerful. I wrote a bit about protocols in my last blog post, &lt;a href=&quot;https://blog.usejournal.com/beyond-functions-in-elixir-refactoring-for-maintainability-5c73daba77f3&quot; data-href=&quot;https://blog.usejournal.com/beyond-functions-in-elixir-refactoring-for-maintainability-5c73daba77f3&quot; class=&quot;markup--anchor markup--p-anchor&quot; target=&quot;_blank&quot;&gt;Beyond Functions in Elixir: Refactoring for Maintainability&lt;/a&gt;. In that post, I walk through implementing a custom Ecto.Type for Markdown, and then automatically converting it to HTML in your templates via protocols.&lt;/p&gt;
&lt;h4 name=&quot;d4a0&quot; id=&quot;d4a0&quot; class=&quot;graf graf--h4 graf-after--p&quot;&gt;Not converting user data to well known shapes early enough&lt;/h4&gt;
&lt;p name=&quot;80eb&quot; id=&quot;80eb&quot; class=&quot;graf graf--p graf-after--h4&quot;&gt;As soon as you get data from the external world, cast it into a well known shape. For this, &lt;a href=&quot;http://ecto.changeset&quot; data-href=&quot;http://ecto.changeset&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Ecto.Changeset&lt;/a&gt; is your best friend. When I first started out, I resisted using changesets, as there is a bit of a learning curve, and it seemed easier to shove data right into the database. Don’t do this.&lt;/p&gt;
&lt;p name=&quot;7f4b&quot; id=&quot;7f4b&quot; class=&quot;graf graf--p graf-after--p&quot;&gt;Ecto.Changeset is such a wonderful tool that will save you so much time, and there are many ways to learn it. I haven’t read the Ecto book, but I do recommend reading through the &lt;a href=&quot;https://hexdocs.pm/ecto/Ecto.html&quot; data-href=&quot;https://hexdocs.pm/ecto/Ecto.html&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;documentation&lt;/a&gt; as well as the free &lt;a href=&quot;http://pages.plataformatec.com.br/ebook-whats-new-in-ecto-2-0&quot; data-href=&quot;http://pages.plataformatec.com.br/ebook-whats-new-in-ecto-2-0&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;What’s new in Ecto 2.1?&lt;/a&gt;. José Valim also wrote &lt;a href=&quot;http://blog.plataformatec.com.br/2016/05/ectos-insert_all-and-schemaless-queries/&quot; data-href=&quot;http://blog.plataformatec.com.br/2016/05/ectos-insert_all-and-schemaless-queries/&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;an excellent blog post&lt;/a&gt; describing how to use Ecto Schemas and Changesets to map data between different domains, without those domains necessarily being backed by a database.&lt;/p&gt;
&lt;h3 name=&quot;87f8&quot; id=&quot;87f8&quot; class=&quot;graf graf--h3 graf-after--p&quot;&gt;Things I’m interested in learning more about&lt;/h3&gt;
&lt;ul class=&quot;postList&quot;&gt;&lt;li name=&quot;d55d&quot; id=&quot;d55d&quot; class=&quot;graf graf--li graf-after--h3&quot;&gt;&lt;a href=&quot;https://nerves-project.org/&quot; data-href=&quot;https://nerves-project.org/&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;nerves&lt;/a&gt; — Craft and deploy bulletproof embedded software in &lt;a href=&quot;http://elixir-lang.org/&quot; data-href=&quot;http://elixir-lang.org/&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Elixir&lt;/a&gt;&lt;/li&gt;
&lt;li name=&quot;c237&quot; id=&quot;c237&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;a href=&quot;https://github.com/toniqsystems/raft&quot; data-href=&quot;https://github.com/toniqsystems/raft&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;raft&lt;/a&gt; — An Elixir implementation of the raft consensus protocol&lt;/li&gt;
&lt;li name=&quot;1daa&quot; id=&quot;1daa&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;a href=&quot;https://pragprog.com/book/fhproper/property-based-testing-with-proper-erlang-and-elixir&quot; data-href=&quot;https://pragprog.com/book/fhproper/property-based-testing-with-proper-erlang-and-elixir&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;Property testing&lt;/a&gt; — via &lt;a href=&quot;https://github.com/proper-testing/proper&quot; data-href=&quot;https://github.com/proper-testing/proper&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;PropEr&lt;/a&gt; and &lt;a href=&quot;https://github.com/whatyouhide/stream_data&quot; data-href=&quot;https://github.com/whatyouhide/stream_data&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;StreamData&lt;/a&gt;&lt;/li&gt;
&lt;li name=&quot;df08&quot; id=&quot;df08&quot; class=&quot;graf graf--li graf-after--li&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Z2DU0qLfPIY&quot; data-href=&quot;https://www.youtube.com/watch?v=Z2DU0qLfPIY&quot; class=&quot;markup--anchor markup--li-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;LiveView&lt;/a&gt; — Upcoming Phoenix compatible library from Chris McCord that blends Phoenix Channels and reactive html&lt;/li&gt;
&lt;/ul&gt;&lt;p name=&quot;2e4b&quot; id=&quot;2e4b&quot; class=&quot;graf graf--p graf-after--li graf--trailing&quot;&gt;Well, thank you for reading this far! These past two years have been a wonderful time. I’m excited to get more involved in the community, and to write more! Say hi on twitter &lt;a href=&quot;https://twitter.com/davydog187&quot; data-href=&quot;https://twitter.com/davydog187&quot; class=&quot;markup--anchor markup--p-anchor&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;https://twitter.com/davydog187&lt;/a&gt; and let me know what else you’d like to hear about!&lt;/p&gt;
</description>
<pubDate>Fri, 26 Oct 2018 12:26:56 +0000</pubDate>
<dc:creator>davydog187</dc:creator>
<og:title>Two years of Elixir at The Outline – Noteworthy - The Journal Blog</og:title>
<og:url>https://blog.usejournal.com/two-years-of-elixir-at-the-outline-ad671a56c9ce</og:url>
<og:image>https://cdn-images-1.medium.com/max/1200/1*VKigG-7FHOXNy3l5yfLggg.png</og:image>
<og:description>Two years ago this month, I started as a developer at The Outline. At the time, the site was just an idea that existed as a series of…</og:description>
<og:type>article</og:type>
<dc:format>text/html</dc:format>
<dc:identifier>https://blog.usejournal.com/two-years-of-elixir-at-the-outline-ad671a56c9ce?gi=4508edd36e6d</dc:identifier>
</item>
</channel>
</rss>